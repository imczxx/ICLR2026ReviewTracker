{"id": "z28PLIEj6l", "number": 6940, "cdate": 1758003002498, "mdate": 1759897882721, "content": {"title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "abstract": "Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce FutureX, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents’ adaptive reasoning and performance in dynamic environments. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.", "tldr": "FutureX, a dynamic, live and contamination-free benchmark for evaluating LLM agents predicting the future.", "keywords": ["Benchmark", "Future Prediction", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40bce15c91b8fe5e7256e4c35ab12aab99e75b13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FutureX, a large-scale live benchmark designed to evaluate LLM agents on future-prediction tasks that require reasoning, information gathering, and decision-making. Unlike prior static benchmarks, FutureX uses a semi-automated pipeline that continuously crawls, curates, and resolves questions from 195 websites across 11 domains. The system automatically collects future-oriented questions, gathers LLM predictions at event start dates, and scores them after resolution, enabling a contamination-free, real-time evaluation. The benchmark assesses 25 models, from base LLMs to research agents (e.g., Grok-4, Gemini Deep Research), with human-expert comparisons."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Contamination-free design: The design of the closed environment to prevent information contamination is well-executed and addresses a critical challenge in LLM evaluation.\n- Comprehensive model evaluation: The study includes 25 models spanning reasoning, search, and deep-research agents with human expert baseline, providing a comprehensive evaluation landscape."}, "weaknesses": {"value": "- The pipeline's automated expansion might favor scale over fidelity. No quantitative validation is provided to ensure the \"extra\" questions truly add value.\n-  The paper lists 195 websites but will be good to include the full list of the selected websitesl. More details on domain balance, filtering reliability would help.\n- The authors may consider including Brier score as most prior works in forecasting adopt this metric, which would facilitate comparison with existing literature."}, "questions": {"value": "- What are the daily compute and maintenance costs for running FutureX, given the live crawling and 25-model evaluation pipeline?\n- Could future versions compare LLM predictions with crowd forecasts (e.g. prediction-market aggregates) to test alignment with collective prediction?\n- Will the full list of websites, question templates, and answer-extraction code be released for transparency and reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rJivP363gu", "forum": "z28PLIEj6l", "replyto": "z28PLIEj6l", "signatures": ["ICLR.cc/2026/Conference/Submission6940/Reviewer_S6CR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6940/Reviewer_S6CR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937802932, "cdate": 1761937802932, "tmdate": 1762919172278, "mdate": 1762919172278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FutureX, a large-scale, dynamic benchmark for evaluating the future prediction capabilities of LLM agents. The authors argue that existing benchmarks fail to test complex reasoning and decision-making under real-world uncertainty.\n\nFutureX uses a fully automated pipeline that continuously gathers future-oriented questions from 195 diverse websites. It runs 25 different LLM agent models to get their predictions on a \"start date.\" After the event's \"resolution date,\" the system automatically finds the ground-truth outcome and scores the predictions. This live-updating design inherently eliminates data contamination, as the answers do not exist at the time of prediction.\n\nThe benchmark includes four difficulty tiers, ranging from simple single-choice questions (Level 1) to high-volatility, open-ended numerical and ranking tasks (Level 4). The paper's findings show that while agents with search tools (like Grok-4) perform best, all models struggle significantly with the harder tiers. Furthermore, all evaluated agents still perform substantially worse than a baseline of 40 human experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of FutureX is its novel \"live\" evaluation design. By focusing on future events whose outcomes are not yet known, it provides a robust and scalable solution to the critical problem of data contamination that plagues static benchmarks.\n\nThe automated pipeline for question collection and answer verification is a significant technical achievement, ensuring the benchmark remains current and challenging.\n\nAnother strength is the logical stratification of tasks into four difficulty tiers. The results validate this structure, showing a clear performance drop as tasks move from simple retrieval (Level 1/2) to complex, open-ended reasoning under uncertainty (Level 3/4).\n\nI like the the inclusion of a human expert baseline. It provides crucial context, grounding the model scores and highlighting the significant gap that still exists between current AI agents and human-level analytical reasoning."}, "weaknesses": {"value": "As far as I understand, the benchmark's \"prediction window.\" is only \"one-week\". This restricts the evaluation to short-term predictions, not long-term forecasting. It fails to test an agent's ability to reason about events months or years in the future, which is a different and critical skill for human analysts. I think this heavily limited this benchmark.\n\nThe evaluation metrics, while appropriate for correctness, do not capture the probabilistic nature of forecasting. The benchmark does not assess an agent's ability to express calibrated confidence (e.g., providing a probability or a confidence interval). It only scores the accuracy of a single-point answer unlike some recent works that assess the confidence too.\n\nThe comparison to human experts is a bit ambiguous. It is not specified whether the 40 human experts and the 25 LLM agents had access to the exact same information retrieval tools (I think this is very important as the author pointed out about potential leakage of data to models; such leakage could happen to human experts too), making the performance gap difficult to interpret fairly.\n\nthe benchmark tests the entire agent system at once. This makes it difficult to isolate the point of failure. When an agent performs poorly, it's unclear whether the fault lies with the underlying LLM's reasoning or with the agent's planning and tool-use framework."}, "questions": {"value": "What are the computational and financial costs associated with maintaining this live benchmark? Its complexity might make it difficult for other researchers to reproduce or build upon.\n\n\n\nHow does the benchmark disentangle the performance of the base LLM from the performance of the agent framework? For example, are the failures of open-source agents due to poor planning logic or the base model's inability to follow the plan?\n\nAre there plans to expand the benchmark's scope beyond the one-week prediction window to include long-term forecasting tasks, which require different reasoning skills than short-term information synthesis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "luCTEWkr5N", "forum": "z28PLIEj6l", "replyto": "z28PLIEj6l", "signatures": ["ICLR.cc/2026/Conference/Submission6940/Reviewer_Kgce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6940/Reviewer_Kgce"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939209804, "cdate": 1761939209804, "tmdate": 1762919171895, "mdate": 1762919171895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FutureX, a large-scale, live benchmark for evaluating base LLMs and LLM agents on real-world future prediction tasks. It features a four-stage pipeline for event database construction, future event curation, agent prediction, and answer acquisition, with all stages executed daily to ensure dynamic, contamination-free evaluation across diverse domains and difficulty levels. Experiments with 25 models show that search- and tool-augmented agents outperform base LLMs but still lag behind humans on complex tasks, establishing FutureX as a scalable framework for assessing LLM reasoning in uncertain, real-world environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Future prediction is a good testbed for evaluating the capabilities of LLMs and agents in information gathering, reasoning, and predictive analysis, while naturally mitigating data contamination since ground-truth answers are not available yet at prediction time.\n\n2. FutureX provides an automated and scalable pipeline for data construction, future event curation, and answer verification, offering good practical value with minimal manual effort.\n\n3. The tiered event categorization in FutureX (Basic, Wide Search, Deep Search, Super Agent) enables systematic assessment of reasoning depth and tool usage in LLM agents.\n\n4. The evaluation spans 25 models covering base, search-augmented, and tool-augmented agents, which offers comprehensive and comparative insights across different model classes.\n\n5. The paper is clearly written and well-organized, with informative figures and tables."}, "weaknesses": {"value": "1. Some details on human annotation are unclear. For example, the number of questions per category, whether human experts had access to all information sources or relied solely on their own knowledge, and the consistency of their answers on the same question. Such information would clarify the human–agent performance comparison and also serve as an indicator of the robustness and quality of the questions in the benchmark.\n\n2. While the performance analysis across 25 models is comprehensive, the paper offers limited discussion on the diagnostic utility of FutureX. It is not clear whether the benchmark can help identify specific weaknesses in agents or provide actionable insights for improving their reasoning and prediction capabilities. Such discussion will further benefit the benchmark users and the development of new agents."}, "questions": {"value": "1. In Line 252, it is mentioned that \"the answer acquisition success rate exceeds 97%\". How is this success rate calculated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "obUT1RaF7u", "forum": "z28PLIEj6l", "replyto": "z28PLIEj6l", "signatures": ["ICLR.cc/2026/Conference/Submission6940/Reviewer_diMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6940/Reviewer_diMG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110317571, "cdate": 1762110317571, "tmdate": 1762919171565, "mdate": 1762919171565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}