{"id": "PDNpRLxDlI", "number": 19360, "cdate": 1758295677712, "mdate": 1763697418664, "content": {"title": "Influence-Preserving Proxies for Gradient-Based Data Selection in LLM FineTuning", "abstract": "Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce IProX, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model’s influence. Experimental results across diverse LLM families and evaluation tasks show that IProX consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with IProX achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, IProX achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that IProX provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.", "tldr": "", "keywords": ["Data Selection", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0258f7d84a6034158e2bfa23828cc9fcc663dbc.pdf", "supplementary_material": "/attachment/17ec49488ed5d45f82a07f46dadb9cab9b5deebf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes IPROX, a framework for building smaller, influence-preserving proxy models to make gradient-based data selection feasible for large language models. Instead of using fixed off-the-shelf proxies, IPROX derives them directly from the target model through a two-stage process: an Influence-Preserving SVD that retains gradient-relevant components via second-moment reweighting, followed by a gradient-alignment stage that matches the proxy’s gradients and logits to the target model for stability. The authors provide theoretical justification that this compression preserves influence consistency and validate the method empirically across multiple model families (Llama3, Gemma3, Qwen2/3) and benchmarks (MMLU, BBH, TyDiQA). Results show that IPROX consistently outperforms larger off-the-shelf proxies while cutting computation by more than half, establishing a principled and efficient approach for scalable data selection in LLM fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Presents a novel, well-motivated framework (IPROX) that bridges model compression and gradient-based influence estimation for LLM data selection.\n2. Offers strong theoretical grounding through Proposition 4.1, connecting layer perturbations to bounded influence deviation.\n3. Demonstrates consistent empirical improvements across multiple LLM families (Llama3, Gemma3, Qwen2/3) and tasks (MMLU, BBH, TyDiQA) and achieves significant computational efficiency, reducing cost by over 50% while maintaining or improving performance."}, "weaknesses": {"value": "1. The theory relies on strong smoothness and bounded shift assumptions that may not fully hold for deep transformer landscapes.\n2. The alignment stage introduces extra hyperparameters (e.g., λ_KL, τ) that may require dataset-specific tuning.\n3. Limited exploration of extreme compression regimes (beyond 70% sparsity), where influence preservation may degrade.\n4. Experiments focus solely on text-based fine-tuning, leaving open how the method generalizes to multimodal or retrieval-augmented LLMs."}, "questions": {"value": "1. The paper primarily evaluates with TracIn and Influence Functions. How would IPROX behave with more recent estimators such as Fisher-based or curvature-aware influence methods? Is IPSVD theoretically compatible with these variants?\n2. What is the sensitivity of the extra hyperparameter (e.g., λ_KL, τ)?\n3. The experiments cap at 70 % sparsity. What happens under more aggressive compression (e.g., 80–90 %)—does influence preservation collapse gradually or sharply?\n4. The reported efficiency is impressive for 3B–7B targets. Could the authors provide projected or preliminary numbers for larger models, such as 70B, to demonstrate scalability under realistic industrial constraints?\n5. Two highly relevant recent works should be included: (a) LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection and (b) EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xbw0w8tF4G", "forum": "PDNpRLxDlI", "replyto": "PDNpRLxDlI", "signatures": ["ICLR.cc/2026/Conference/Submission19360/Reviewer_U9aH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19360/Reviewer_U9aH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760813397739, "cdate": 1760813397739, "tmdate": 1762931295398, "mdate": 1762931295398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IPROX, a two-stage framework for constructing influence-preserving proxy models to enable scalable gradient-based data selection in the fine-tuning of LLMs. The approach first compresses the target LLM using a low-rank, influence-aware SVD (IPSVD) and then further aligns the proxy with the target via internal gradient alignment and output anchoring losses. Empirical results are presented across several LLM families, demonstrating improved performance and efficiency compared to existing off-the-shelf proxy models and two baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Principled Influence-Preserving Compression**: The paper develops a well-justified influence-preserving low-rank SVD technique for model compression, addressing the misalignment between traditional reconstruction objectives and the needs of influence-based data selection.\n- **Comprehensive Theoretical and Implementation Details**: The paper carefully connects theoretical derivations to practical considerations, including efficient computation using “skinny” SVDs and probe-based approximations."}, "weaknesses": {"value": "- **Theoretical Limitations and Empirical Edge Cases**: While the influence-retention bound is elegant, its assumptions (local smoothness, geometric coherence, bounded covariate shift) could be restrictive in practice. Furthermore, the treatment of the embedding and LM head as incompressible is not systematically evaluated in ablation.\n- **Performance Gains need more explanation**: The paper acknowledges task-type sensitivity but does not deliver deeper insight into when/why influence preservation matters most versus simple SVD or other alternatives."}, "questions": {"value": "- Is there empirical evidence on how sensitive the effectiveness of IPROX is to the choice and size of the probe set N? Can an ablation show how influence approximation degrades (or not) as N varies, especially in large model regimes?\n- Regarding the incompressible layers (embedding, LM head): Are there empirical ablation studies quantifying their effect on overall model or proxy performance?\n- Could the authors clarify the mathematical assumptions required for the influence preservation bounds, and discuss scenarios where these may not hold (e.g., high non-linearity, significant distribution shift)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NojFn8k9s0", "forum": "PDNpRLxDlI", "replyto": "PDNpRLxDlI", "signatures": ["ICLR.cc/2026/Conference/Submission19360/Reviewer_npPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19360/Reviewer_npPH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627099936, "cdate": 1761627099936, "tmdate": 1762931294955, "mdate": 1762931294955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational inefficiency of gradient-based data selection methods for large language models (LLMs), which are critical for SFT but impractical for multi-billion-parameter models due to high costs. It introduces IPROX, a two-stage framework that constructs influence-preserving proxy models directly from the target LLM: first, an Influence-Preserving SVD stage compresses the target model’s weight matrices to retain gradient-based influence information (unlike standard SVD, which prioritizes reconstruction loss), and second, an alignment stage refines the proxy by matching its gradients to the target model in low-rank space and anchoring output logits via KL divergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Targeted solution to an important problem**: The paper directly addresses the limitation of gradient-based data selection, which is the  poor scalability with LLM size. The authors focus on proxy design, which is an orthogonal approach to simplifying influence computation itself. This fills a gap in existing work, which either uses suboptimal off-the-shelf proxies or reduces influence calculation cost without preserving alignment to the target model.\n-  **Theoretically Grounded Proxy Construction**: IPROX is not heuristic: Proposition 4.1 provides a theoretical bound linking the expected squared error of layer perturbations to influence preservation, and IPSVD is designed to minimize this error via second-moment reweighting of inputs and upstream gradients.\n- **Flexibility and Efficiency**: IPROX allows flexible control of proxy size via the rank parameter in IPSVD, enabling trade-offs between computational cost and performance."}, "weaknesses": {"value": "- **Dependence on Probe Set Quality**: IPROX relies on a small probe set to approximate second-moment matrices for IPSVD. The paper mentions using token sampling to avoid length bias but does not evaluate how probe set size, diversity, or representativeness affects performance.\n- **Narrow Task and Data Scope**: While the paper uses three diverse tasks, all training data is drawn from DOLLY (instruction-response pairs). It does not test IPROX on other data types (e.g., code, scientific text) or tasks with larger distributional shifts from DOLLY (e.g., low-resource language QA). This limits conclusions about IPROX’s performance in more heterogeneous fine-tuning scenarios.\n- **Lack of Ablation for Second-Moment Reweighting**: While IPSVD’s second-moment reweighting is core to its design, the paper does not ablate this component (e.g., comparing IPSVD to unweighted SVD) to isolate its impact on influence preservation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MASkvaYybK", "forum": "PDNpRLxDlI", "replyto": "PDNpRLxDlI", "signatures": ["ICLR.cc/2026/Conference/Submission19360/Reviewer_PeGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19360/Reviewer_PeGn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820338029, "cdate": 1761820338029, "tmdate": 1762931294584, "mdate": 1762931294584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IPROX addresses the computational cost of gradient-based data selection for LLM fine-tuning by constructing smaller proxy models that preserve the target model's influence characteristics. The framework uses two stages: (1) Influence-Preserving SVD (IPSVD) that compresses the target model while retaining influence-relevant components via second-moment reweighting, and (2) alignment that matches gradients in low-rank space and anchors output logits."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Flexible efficiency-performance trade-off**: Enables controllable proxy size through sparsity parameter ρ, allowing practitioners to balance computational cost and selection quality.\n\n2. **Practical scalability with minimal overhead**: Achieves >50% cost reduction on Llama3.2 with proxy construction adding only ~5-7 minutes overhead, using efficient probe-based approximation that avoids forming large  matrices."}, "weaknesses": {"value": "**Missing comparisons to established data selection methods**: The paper lacks empirical comparison with core-set and generalization-based selection methods like GLISTER[1], Coresets, and gradient-based selection frameworks. These are only briefly mentioned in related work but not evaluated. The absence of comparison to methods like LESS[3] and DsDm[2] weakens the argument for IPROX's advantage over the broader data selection literature beyond just gradient-based influence methods.\n\n\n\n\n[1] GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning\n[2] DsDm: Model-Aware Dataset Selection with Datamodels\n[3] Less: Selecting influential data for targeted instruction tuning"}, "questions": {"value": "How does IPROX compare to non-gradient baselines like GLISTER, LESS, or coreset methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BIHNdxyOTf", "forum": "PDNpRLxDlI", "replyto": "PDNpRLxDlI", "signatures": ["ICLR.cc/2026/Conference/Submission19360/Reviewer_fpV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19360/Reviewer_fpV6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937874562, "cdate": 1761937874562, "tmdate": 1762931293810, "mdate": 1762931293810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}