{"id": "PyXp0b5hcn", "number": 10955, "cdate": 1758185524304, "mdate": 1759897618448, "content": {"title": "Exploiting Client Heterogeneity for Forgetting Mitigation in Federated Continual Learning: A Spatio-Temporal Gradient Alignment Approach", "abstract": "Federated Continual Learning (FCL) has recently emerged as a crucial research area, as data from distributed clients typically arrives as a stream, requiring sequential learning. This paper explores a more practical and challenging FCL setting, where clients may have unrelated or even heterogeneous tasks, leading to gradient conflicts where local updates point in divergent directions. In such scenario, statistical heterogeneity and data noise can create spurious correlations, leading to biased feature learning and catastrophic forgetting. Existing FCL approaches often use generative replay to create pseudo-datasets of previous tasks. However, generative replay itself suffers from catastrophic forgetting and task divergence among clients, leading to overfitting in FCL. To address these challenges, we propose a novel approach called \\textbf{\\underline{S}}patio-\\textbf{\\underline{T}}emporal gr\\textbf{\\underline{A}}dient \\textbf{\\underline{M}}atching with \\textbf{\\underline{P}}rototypical Coreset (STAMP). Our contributions are threefold: 1) We develop a model-agnostic method to determine subset of samples that effectively form prototypes when using a prototypical network, making it resilient to continual learning challenges; 2) We introduce a spatio-temporal gradient matching approach, applied at both the client-side (temporal) and server-side (spatio), to mitigate catastrophic forgetting and data heterogeneity; 3) We leverage prototypes to approximate task-wise gradients, improving gradient matching on the client-side. Extensive experiments demonstrate our method's superiority over existing baselines, particularly in scenarios with a large number of sequential tasks, highlighting its effectiveness in addressing the complexities of real-world FCL.", "tldr": "This paper introduces a gradient matching approach to leverage client-specific knowledge to mitigate forgetting in heterogeneous federated continual learning", "keywords": ["Federated Continual Learning", "Gradient Matching", "Heterogeneous task"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0939407657562d018b2404b5d5a8cce20377e151.pdf", "supplementary_material": "/attachment/d296198f359ee4e326427b574af1abf9b579eaf4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes STAMP for FCL. It introduces temporal gradient alignment on clients and spatial gradient alignment on the server to handle task and client heterogeneity. At the same time, a prototypical coreset mechanism provides efficient replay without heavy memory usage. The method aims to align gradients across both temporal and spatial dimensions, improving generalization and reducing forgetting. Extensive experiments on diverse datasets demonstrate that STAMP outperforms existing FCL methods in accuracy, stability, and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Treating spatio-temporal gradient alignment jointly is a creative and well-motivated way to handle heterogeneity in FCL.\n- The prototypical coreset approach is lightweight and avoids the heavy cost of generative replay, which is appealing for real-world edge deployments.\n- The paper conducts comprehensive evaluations across multiple datasets and metrics, showing clear and consistent improvements.\n- The inclusion of a theoretical generalization bound adds some rigor and helps contextualize the method’s stability and plasticity claims."}, "weaknesses": {"value": "- The theoretical analysis remains largely qualitative and does not clearly demonstrate why STAMP achieves better gradient alignment in practice.\n- While the proposed approach is efficient, the algorithmic pipeline (temporal + spatial alignment + coreset) can be conceptually heavy and may limit reproducibility.\n- The improvements, although consistent, are relatively modest on some benchmarks, and the scalability on extremely large client pools is not deeply analyzed."}, "questions": {"value": "- How sensitive is STAMP to the size of the coreset, and what is the trade-off between memory footprint and accuracy?\n- Can STAMP handle asynchronous or partially participating clients in more realistic federated settings?\n- How does STAMP perform when the degree of heterogeneity is extreme, e.g., when client label spaces are disjoint?\n- Some related baselines like FedSSI and references should be added and compared [1-2].\n\n[1] FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence. The Thirteenth International Conference on Learning Representations, ICML 2025\n\n[2] Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey. IEEE Communications Surveys & Tutorials, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yXzvXTncli", "forum": "PyXp0b5hcn", "replyto": "PyXp0b5hcn", "signatures": ["ICLR.cc/2026/Conference/Submission10955/Reviewer_frwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10955/Reviewer_frwb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561482239, "cdate": 1761561482239, "tmdate": 1762922153011, "mdate": 1762922153011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the use of gradient alignment technique across tasks and clients computed from a prototype coreset of samples (proposed in this paper) at each client to improve generalization in federated continual learning setting. The paper also provides bounds on the generalization gap between seen and unseen tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides theoretical justification for gradient alignment techniques and specifically for the coreset selection technique being used\n- Generalization bound is derived and proposed technique is shown to have reduced the generalization gap.\n- Performance is shown to be on par with popular prior work such as FedWeIT, while reducing the disk usage significantly.\n- Ablation studies are quite extensive, the results shown are convincing to show that the method work reliably and provides the claimed gains. Datasets however are somewhat still more traditional datasets used in federated learning and does not contain natural continual learning datasets."}, "weaknesses": {"value": "- The method relies on some assumptions such as requiring replay buffer at clients.\n- Gradient alignment might slow the convergence, especially when updating the global model.\n- The paper should at least in the appendix comment on how the coreset selection (combinatorial problem) is solved and what’s the additional worst-case complexity (O(n^2) or O(n log n), etc).\n- The claims are a bit overstated as the generalization over tasks is not really the goal of the coreset selection. For example, assume a large volume of samples from a certain task/client that dominates a few training rounds. In those cases, the coreset will start to get dominated by samples from those tasks. As a result, the gradient alignment favors those tasks containing the most samples. Instead of task level, I suppose the claim is that the proposed technique will help in reducing generalization gap across the diverse data."}, "questions": {"value": "- How is the coreset selection done? Even if this is a well-known and solved problem, it should clearly be stated including the computational complexity of it.\n- Is the paper's goal really the generalization over all data or tasks? Check comments above, but the question is what happens when a single task has 1000x more samples compared to another task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AA8nYAM6t2", "forum": "PyXp0b5hcn", "replyto": "PyXp0b5hcn", "signatures": ["ICLR.cc/2026/Conference/Submission10955/Reviewer_Ya9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10955/Reviewer_Ya9R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676648565, "cdate": 1761676648565, "tmdate": 1762922152426, "mdate": 1762922152426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STAMP, a federated continual learning (FCL) approach incorporating spatio-temporal gradient alignment across clients and tasks, along with a prototypical coreset to mitigate catastrophic forgetting with reduced memory usage. The idea of aligning gradients across the temporal (intra-client) and spatial (inter-client) dimensions is interesting, and the paper provides both theoretical motivation and empirical evaluations showing improvement over existing baselines on several vision datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel idea of performing both temporal and spatio gradient alignment in FCL.\n2.Prototypical coreset is a good alternative to generative replay and full memory buffers."}, "weaknesses": {"value": "- Handling catastrophic forgetting: Although this work is rehearsal based, the work does not explicitly handle catastrophic forgetting (most of the rehearsal methods do, such as AGEM/GEM). However, the paper repeatedly claims that STAMP reduces catastrophic forgetting, but this is not explicitly demonstrated experimentally or theoretically. The provided plots focus only on temporal and spatio gradient alignment metrics. Good alignment does not necessarily guarantee reduced forgetting. More direct evidence such as forgetting curves and class/task-level retention is needed.\n- The claim of Fig 1 is misleading. The difference indeed decreases, but at the cost of global accuracy in STAMP. How is this a good case? This also contradicts the claim of improved intra-client retention. The paper needs to explain this discrepancy.\n- It is unclear how spatio and temporal gradient alignment preserve the gradient direction, especially in coparison with the memory data. The paper states that alignment prevents negative transfer, but lacks intuition or analysis on how this specifically preserves directionality of task gradients over time.\n- It is not clear how the theoretical results show the impact of using coresets - a typical generalization result must encompass the effect of coresets. For instance, if one performed random sampling of points instead of using a coreset, how would the effect be reflected in Thm 2. \n- Storage cost concern for storing gradients. Calculating gradient alignment implicitly requires storing gradients from previous tasks and clients. This may become costly for large models.\n- Ablation studies are insufficient. The method introduces multiple components (temporal GA, spatio GA, prototypical coreset, ProtoNet, MixStyle), yet ablations are limited. Add more like varying number of tasks, effect of different epochs per task, impact of removing the prototypical network, varying coreset sizes.\n- \\gamma used in the gradient alignment formulation is not defined in the main text.\n- Possible error in Section 2.1. The description states that r is the current round of task t, but it should logically refer to the current round of task t+1.\n- Figure 3 does not support the claim that heterogeneity helps generalization. The results do not show a clear benefit as heterogeneity increases. This contradicts the core hypothesis. The chosen values of \\alpha are quite high, and does not indicate highly heterogeneous cases. \nMissing plots in Figures 5 and 6. No results for CIFAR100 with 2 classes/task for temporal gradient alignment in Fig. 5. No results for CIFAR100 with 20 classes/task for spatio gradient alignment Fig. 6. Some recent baselines with theoretical guarantees (CFLAG, AISTATS 2025) is not cited."}, "questions": {"value": "Please clarify the points raised in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P4R0B9xRpu", "forum": "PyXp0b5hcn", "replyto": "PyXp0b5hcn", "signatures": ["ICLR.cc/2026/Conference/Submission10955/Reviewer_CrLV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10955/Reviewer_CrLV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899652731, "cdate": 1761899652731, "tmdate": 1762922151666, "mdate": 1762922151666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the federated continual learning (FCL) scenarios where heterogeneous tasks assigned to clients. The authors propose Spatio-Temporal grAdient alignMent with Prototypical coreset (STAMP), which uses gradient alignment method and prototypes for mitigating the biased feature learning and severe catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper focuses on an important research problem of CFL with heterogeneous clients."}, "weaknesses": {"value": "* Gradient Alignment method is not something new\n* The theoretical results are either trivial (Lemma 1) or too complex to get meaningful insight (Theorems 1,2). Especially, it was hard to find some connection between the theoretical results and the empirical observations.\n* It is unclear when/why STAMP works well."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "icQQJiMh5m", "forum": "PyXp0b5hcn", "replyto": "PyXp0b5hcn", "signatures": ["ICLR.cc/2026/Conference/Submission10955/Reviewer_vU55"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10955/Reviewer_vU55"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997613130, "cdate": 1761997613130, "tmdate": 1762922150908, "mdate": 1762922150908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}