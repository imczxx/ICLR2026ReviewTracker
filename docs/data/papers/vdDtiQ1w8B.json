{"id": "vdDtiQ1w8B", "number": 15605, "cdate": 1758253062992, "mdate": 1763769955044, "content": {"title": "VideoSearch Reasoner: Boosting Multimodal Reward Models through Think with Image Reasoning", "abstract": "Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: **(1)** visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and **(2)** all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce **VideoSearch Reasoner**, a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: **(i)** Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; **(ii)** select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and **(iii)** apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VideoSearch Reasoner achieves 80.5\\% on VideoGen Reward, 82.3\\% on GenAI-Bench, and 75.6\\% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.", "tldr": "We propose a Multimodal Reward Model utilizing Thinking-with-Image framework", "keywords": ["Multimodal Reward Model", "Multimodal LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ee33161bd2563e4e4427e0628fc1db532b495a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "(I tend to write shorter reviews and the length of the review does not reflect the quality of paper or the time spend on reviewing it).\n\nNote that I am not an expert in Video modelling nor Reward Modelling and the review is from a perspective of a general Deep Learning researcher.\n\nThis paper proposes a pretty intersting and well thought pipeline for improving multimodal reward models. The pipeline is called Thinking with Image reasoning. This paper expands on the details of training an effective reward model that can pick and choose frames to enable accurate yet efficienct processing. The reward models improve peroformance on the public benchmarks and are SOTA. \n\noverall this is a very well executed paper and is of value to the community to build and improve open models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I am not an expert, but from the PoV of a deep learning researcher this paper brings in so much knowledge to the table and help the community build better and more efficienct reward and reasoning models."}, "weaknesses": {"value": "The paper has quite a few typos and it would be great if the authors can fix that before the camera ready."}, "questions": {"value": "I am not an expert on this and would defer to other reviewers for more nuanced questions on the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KlGrGQdK5c", "forum": "vdDtiQ1w8B", "replyto": "vdDtiQ1w8B", "signatures": ["ICLR.cc/2026/Conference/Submission15605/Reviewer_gqKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15605/Reviewer_gqKX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760732321142, "cdate": 1760732321142, "tmdate": 1762925876206, "mdate": 1762925876206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors try to train a reward model initialized from Qwen2.5VL 7B to achieve thinking and reasoning abilities to select the informative frames from video and configure a Windows memory by RL post-training. Specifically, they follow a common pipeline to first curate a cold-start CoT dataset to install the basic reasoning skill necessitated by the video frame selection and scoring functions, then do a fine-grained rejection sampling stage to fine-tune the model, and finally apply GRPO RL training to the model trained on the curated data. This paper obtains competitive performances on several video preference benchmarks, like VideoGen Reward,  GenAI-Bench, and MJ-Bench-Video."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The application of GRPO to LLM RM as an informative frame selection.\n\n2. The final evaluations on several video preferences benchmarks are competitive even with SoTA."}, "weaknesses": {"value": "1. Though the fame selection by incentivizing the MLLM model sounds like a meaningful exploration, the inference cost, such as the computational overhead and also the inference latency, seems very heavy to obtain a result from this RM. Do the authors analyze and compare these? If the computational overhead and resource consumption take a lot, how can we demonstrate the applicability of this RM model for other training objectives?\n\n2. The overall framework for this paper sounds a bit complicated, since it proposes lots of components to get the final thinking-with-image multimodal reward model. But how do the authors choose the strategic details and also the hyperparameters, such as the combination of reward functions and the corresponding weightings?\n\n3. A main concern of this paper is that they choose the in-distribution benchmarks and datasets to train and evaluate the model, how to demonstrate the RM's generalization, and also some OOD evaluations.\n\n4. Does this paper also try to apply the final model to train a Video LLM or Video Generation model to validate the improvements of the video RM model, which shall maintain the fair settings with other models?\n\n5. When facing the long-duration or long-context video scenarios, will the window memory design still handle well with the forgotten issue?"}, "questions": {"value": "The main paper organization and writing confuse me a bit and look a bit chaotic, while I understand the great effort for the overall framework, but I do need to understand the main differences, comparisons, advantages, and demonstrations of the pipeline of this paper's contributions. And what if skipping the SFT and applying the RL directly? Did this paper conduct an exploration of this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6OAYvxj0WD", "forum": "vdDtiQ1w8B", "replyto": "vdDtiQ1w8B", "signatures": ["ICLR.cc/2026/Conference/Submission15605/Reviewer_XqJ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15605/Reviewer_XqJ6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104770430, "cdate": 1762104770430, "tmdate": 1762925875753, "mdate": 1762925875753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework for multimodal reward models (RMs) designed to overcome key limitations in video processing. The authors identify two main problems with existing RMs: 1) visual inputs consume large context budgets, forcing aggressive downsampling and loss of detail, and 2) packing all visual information into the initial prompt causes the model to \"forget\" or hallucinate during subsequent text-only Chain-of-Thought (CoT) reasoning. The authors demonstrate state-of-the-art results for open-source models on benchmarks like VideoGen Reward, GenAI-Bench, and MJ-Bench-Video, with particular advantages in long-video scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies critical bottlenecks in VLM-based RMs (context limits, text-only CoT) and proposes a direct and intuitive solution.\n2. The model achieves SOTA results on multiple standard benchmarks. Crucially, the authors go further to show in Table 2 that the model's advantage is most pronounced on \"hard\" subsets, namely long videos and complex prompts, which directly validates the hypothesis of the paper."}, "weaknesses": {"value": "1. The main weakness is the limited conceptual novelty of the \"Thinking-with-Image\" framework. This framework is functionally equivalent to an agentic RAG system that operates on a video's frame index. While this is a new application for RMs, the paper would be stronger if it explicitly contextualized itself against VideoRAG and agentic video analysis literature and clarified its novel contributions beyond the application domain.\n2. The paper repeatedly refers to \"visual reasoning operations\", but the only operation implemented and discussed appears to be \"select frames\". This is a retrieval or search operation that provides new context for reasoning; it is not a reasoning operation in itself. This framing feels like an overstatement of the technical mechanism.\n3. The iterative nature of the framework (reason -> retrieve -> reason) will necessarily be much slower and more computationally expensive than single-pass RMs. The authors acknowledge this as a limitation but do not provide any quantification. This trade-off between accuracy and latency is a significant practical consideration that is left unanalyzed.\n4. The entire pipeline is bootstrapped by \"high-quality, long CoT trajectories\" generated by GPT-4o. The success of the \"Cold Start\" stage, and thus the entire model, is highly dependent on access to this powerful, proprietary model for data curation."}, "questions": {"value": "1. The paper emphasizes \"visual reasoning operations\". Besides the select_frames tool, are there any other visual operations implemented or envisioned? If not, could the authors justify why this tool call is classified as a \"reasoning operation\" rather than a \"visual retrieval/search operation\" that feeds new context to the model's textual reasoning process?\n2. The \"Thinking-with-Image\" framework appears to be a well-executed RAG system for video, enabling the model to agentically search for relevant frames. What do the authors view as the core conceptual difference or advantage of this framework compared to existing work on VideoRAG or retrieval-augmented video understanding? The core capability of agentic frame search seems highly similar, even if the end-task (RM) is different.\n3. Could the authors provide a practical comparison of the inference latency (e.g., time per evaluation) with the baselines (like UnifiedReward-Think) on an average-length video from one of the test benchmarks? This would be crucial for understanding the practical accuracy/cost trade-off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TZpnKMLyth", "forum": "vdDtiQ1w8B", "replyto": "vdDtiQ1w8B", "signatures": ["ICLR.cc/2026/Conference/Submission15605/Reviewer_Whd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15605/Reviewer_Whd2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154105107, "cdate": 1762154105107, "tmdate": 1762925874874, "mdate": 1762925874874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VideoSearch Reasoner, a novel multimodal reward model designed to address limitations in current VLM-based reward models for video preference tasks. The key innovation is a \"thinking-with-image\" framework that allows the model to actively retrieve visual information through tool invocation (frame selection) and maintain a configurable visual memory window. The authors propose a three-stage training pipeline: (1) Cold Start with curated visual CoT data, (2) Rejection sampling Fine-Tuning on high-quality traces, and (3) Group Relative Policy Optimization (GRPO) with novel reward signals. The approach achieves state-of-the-art results on video preference benchmarks, particularly excelling on longer videos."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The model achieves impressive performance improvements across multiple benchmarks, with particularly notable gains on complex/long video subsets\n\n- The context budget constraint from visual inputs and the information forgetting problem during pure textual reasoning. The motivation is clear and compelling.\n\n- The paper includes extensive ablation studies examining visual reasoning, training pipeline components, auxiliary rewards, and accuracy reward design."}, "weaknesses": {"value": "- While the paper demonstrates effectiveness on videos with ~128 frames, there's insufficient analysis of how the approach scales to truly long videos. \n\n- The paper acknowledges increased latency in the limitations section but provides no concrete measurements. How much slower is inference compared to baselines? What is the memory overhead? \n\n- The paper doesn't compare against some relevant work in visual reasoning, such as methods that use explicit visual programs or other tool-use paradigms beyond the baselines mentioned.\n\n- The paper lacks qualitative analysis of when and why the model fails. What types of videos or queries are problematic?"}, "questions": {"value": "- How does the approach perform on long videos. \n\n- Does the model learn meaningful frame selection strategies, or is it mostly random? Can you provide analysis of which frames are selected and why?\n\n- Can you provide concrete numbers on inference time and memory usage compared to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uMlCKhetjE", "forum": "vdDtiQ1w8B", "replyto": "vdDtiQ1w8B", "signatures": ["ICLR.cc/2026/Conference/Submission15605/Reviewer_juW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15605/Reviewer_juW2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163312380, "cdate": 1762163312380, "tmdate": 1762925873993, "mdate": 1762925873993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}