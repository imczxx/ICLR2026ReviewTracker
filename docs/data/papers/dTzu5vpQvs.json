{"id": "dTzu5vpQvs", "number": 7933, "cdate": 1758044402385, "mdate": 1763561979955, "content": {"title": "Does Your 3D Encoder Really Work? A simple yet effective pathway to real 3D scene understanding", "abstract": "Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest in extending them to 3D settings. Based on their encoder design, this paper categorizes recent 3D VLMs into 3D scene-centric, 3D object-centric and 2D image-based approaches. Despite their architectural similarity to 2D counterparts, 3D scene-centric VLMs have exhibited comparatively lower performance compared with the latest 3D object-centric and 2D image-based approaches. To understand this gap, we conduct an in-depth analysis and find that these models show unstable data scaling capabilities and limited reliance on the 3D scene encoder, instead overfitting to linguistic cues and frequent answers. Although data balancing of under-sampling offers partial improvements, it fails to address the fundamental problem, as the model continues to largely ignore the 3D scene input. To address these limitations and encourage genuine 3D scene understanding, we introduce a simple yet effective training strategy: __rearranging the input sequence__. By positioning the 3D scene between the question and the answer, we prevent the model from learning shortcuts from linguistic cues alone and compel it to ground its comprehension in the visual context. Our experiments show this method not only improves the model's genuine understanding, but also restores the effectiveness of standard pre-training and supervised fine-tuning stages. Crucially, our approach ensures the 3D encoder plays an essential role, laying a more robust foundation for future 3D VLM research.", "tldr": "", "keywords": ["Spatial intelligence", "3D scene understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b45afffe82b8ce267f465e3d4c5e320d6783012e.pdf", "supplementary_material": "/attachment/cbf8307c11c0ebc67bb71aab4005068ea1b94196.zip"}, "replies": [{"content": {"summary": {"value": "The paper analyzes why 3D scene-centric vision-language models (VLMs) underperform compared to 3D object-centric and 2D-based models, finding that they rely too much on language cues and neglect 3D scene input. To address this, the authors propose rearranging the input sequence to place the 3D scene between the question and answer, forcing the model to use visual information. This simple strategy improves genuine 3D understanding and model performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors report several interesting findings on why 3D scene-centric models underperforms to 3D object-centric and 2D-based models. These findings are worth exploration.\n\n2. The authors conduct comprehensive experiments on studying the phenomenons.\n\n3. The proposed sequence rearrangement is simple and seems to be effective in Table 11."}, "weaknesses": {"value": "1. Recent relevant works, such as VG-LLM [1] and 3DRS [2], are not cited or compared in the paper. Including these comparisons would help contextualize the contributions.\n\n2. The paper’s presentation is unsatisfactory. While several subsections are used to introduce findings, there is little explanation of why these phenomena occur or how the proposed method addresses them. Sections 3 and 4, in particular, are tedious and detract from the reader’s engagement and interest.\n\n3. The effectiveness of the proposed sequence rearrangement is not fully validated. The authors should conduct experiments to specifically verify whether this approach addresses the data scaling issue. If such experiments are already included, please clarify or direct me to the relevant section.\n\n4. The authors attribute the observed phenomena to the order of scene, question, and answer tokens. However, in the Video-3D-LLM training set, I found that the token order is also [scene, question, answer]. What accounts for the difference in results—why does this arrangement work for Video-3D-LLM?\n\n[1] Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors. NeurIPS 2025.\n\n[2] MLLMs Need 3D-Aware Representation Supervision for Scene Understanding. NeurIPS 2025."}, "questions": {"value": "1. Please cite and discuss the relevant recent papers.\n\n2. Please provide more detailed explanations for the observed phenomena.\n\n3. Please report the results of the missing experiments.\n\n4. Please clarify why the [scene, question, answer] token order is effective in Video-3D-LLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XmY9esIA3k", "forum": "dTzu5vpQvs", "replyto": "dTzu5vpQvs", "signatures": ["ICLR.cc/2026/Conference/Submission7933/Reviewer_egbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7933/Reviewer_egbm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731876229, "cdate": 1761731876229, "tmdate": 1762919955793, "mdate": 1762919955793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper carries out a study that analysis the usefulness of the 3D encoder in 3D VLMs. The analysis is performed in the aspects of the scaling capabilities of 3D VLMs, the effectiveness of the pretraining stage, and the ablation on the 3D VLMs completely without the 3D encoder. The analysis shows that the 3D encoders in the current scene-centric 3D VLMs are not working properly, and the model might overfit to the pure text question-answer data. To solve this issue, the paper proposes to rearrange the sequence of 3D features and text tokens during training, to force the 3D VLMs to focus on the information from the 3D encoder. Experimental results demonstrate that sequence rearrangement can help with the 3D understanding capability of 3D VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a pretty interesting finding that 3D tokens in current 3D VLMs are not functioning properly, resulting in the 3D VLMs even achieving similar results without the presence of 3D features. This is an important finding and can raise the awareness of this issue for the community.\n\n- The remedy of rearranging the input sequence is a simple but effective solution to solve the spotted issue, ensuring the 3D encoder to properly function in 3D reasoning."}, "weaknesses": {"value": "- As the paper acknowledges on this point, there is performance gap of the model studied in the paper and the current state-of-the-arts. I think this could be a valid concern because the current state-of-the-art models may already have mitigated issues in these points. If this is the case, the significance of the issue studied in the paper will be downplayed.\n\n- This paper follows a problem analysis followed by proposed method workflow, which is often great for understanding the paper. However, I feel the motivation of the analysis part is not very natural. For example, it is unclear why the study will start from inspecting whether 3D VLMs have scaling capabilities, as it seems to have loose relationship with the issue spotted in the paper.\n\n- Typo: The title of Section 3.3 \"Dose\" should be \"Does\"."}, "questions": {"value": "- Is the studied issue in the paper still being a severe issue in more recent 3D VLMs? Basically for the 3D VLMs that the authors refer as state-of-the-arts which has large performance gap with the more primitive models studied in the paper.\n\n- The issue studied in the paper is most severe for 3D scene-centric 3D VLMs, with mitigated effects on 3D object-centric 3D VLMs and 2D image-based 3D VLMs. The experiment with the proposed rearranging strategy is also carried out in 3D scene-centric 3D VLMs. How is the performance comparison between the 3D scene-centric VLMs after applying the rearranging strategy and the other types of 3D VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hCQVEdprHx", "forum": "dTzu5vpQvs", "replyto": "dTzu5vpQvs", "signatures": ["ICLR.cc/2026/Conference/Submission7933/Reviewer_4oGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7933/Reviewer_4oGj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791521561, "cdate": 1761791521561, "tmdate": 1762919955294, "mdate": 1762919955294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first analyzes the limitations of the 3D scene-centric VLMs in the 3D scene QA task. The authors find that the model scales poorly with more data, and has limited reliance on the 3D scene information, as the performance is on par with pure LLM taking only the question as input. The authors attribute this to the imbalance of the answer distribution, and propose to solve it by rearranging the input sequence (positioning the 3D scene between the question and the answer)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an important problem of scene-level 3D understanding using VLMs. It systematically analyzes the shortcuts existing in baselines due to the imbalance in the answer distribution, and has an interesting finding that question-only LLMs can do as well as VLMs with scene input. \n2. The paper is easy to follow. It is well-written in general despite some minor typos. The experiment protocols and datasets are clearly described and seem reproducible."}, "weaknesses": {"value": "The analysis of the problem makes sense to me, but my main concern of this paper is that it doesn't propose a valid solution, and the contribution is not enough. The data-balancing helps, but that is a well-known technique. The position swapping between the scene and the questions is more interesting, but the results are not promising. See below:\n1. One of the main contributions that this paper claims is swapping the positions of 3D scene and question, which the authors claim to help the model \"achieving genuine visual understanding\". However, the performance of the final model is worse than the model without swapping (Compare Tab. 11 to Tab. 3). It is unclear what does genuine visual understanding actually mean, and why does it matter to us if the model's performance is even worse. One experiment the authors could do is to evaluate the zero-shot or few-shot generalization capability of both models on other benchmarks, where hopefully the baseline will fail due to overfitting, whereas the proposed model could generalize better. \n2. It is unclear what is the 3D encoder architecture, and how much does the capacity of the encoder matters to the conclusion the authors draw. It would be helpful to compare the results across different encoder backbones, such as Point Transformer v2 and v3.\n3. I'm confused by Tab 7 (left). All trained models are worse than the random guess by frequency baseline, even the full model trained by the authors. What is the takeaway from this table?"}, "questions": {"value": "1. In theory, why would the order of the question and the 3D scene tokens matter to the VLM, since the VLM is an attention-based architecture that models all-pairs relationships? Doesn't the shortcut from text to answer still exist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tivAn67g0g", "forum": "dTzu5vpQvs", "replyto": "dTzu5vpQvs", "signatures": ["ICLR.cc/2026/Conference/Submission7933/Reviewer_8U3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7933/Reviewer_8U3K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877128925, "cdate": 1761877128925, "tmdate": 1762919954829, "mdate": 1762919954829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the phenomenon of limited reliance on the 3D scene encoder in 3D scene-centric vision-language models. The authors analyze how these models scale with pre-training data and identify that the 3D encoder is often underutilized. To address this, they propose a sequence rearrangement strategy for the input, demonstrating its effectiveness on the ScanQA benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides a thorough investigation into the underutilization of the 3D scene encoder, a timely problem for the field.\n2. The extensive experiments on data scaling offer diagnostic insights into 3D VLMs."}, "weaknesses": {"value": "1. The proposed solution, sequence rearrangement, is introduced only near the end of the paper. Its connection to the earlier analysis is not clearly justified, and the experiments on ScanQA alone are insufficient to convincingly validate its effectiveness.\n2. The rearrangement strategy itself is relatively simple and does not constitute a strong methodological contribution."}, "questions": {"value": "1. As noted in Weakness 1, could the authors better explain the reasoning behind why sequence rearrangement helps mitigate the observed issue?\n2. It is unclear which datasets were used for the scaling experiments, could the authors clarify this?\n3. Could the authors elaborate on how the lack of improvement from data scaling supports the hypothesis that the 3D scene encoder is underutilized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "29qix1Usz2", "forum": "dTzu5vpQvs", "replyto": "dTzu5vpQvs", "signatures": ["ICLR.cc/2026/Conference/Submission7933/Reviewer_xho9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7933/Reviewer_xho9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962891418, "cdate": 1761962891418, "tmdate": 1762919954444, "mdate": 1762919954444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response – Thanks to All Reviewers for Constructive and Insightful Feedback"}, "comment": {"value": "We greatly appreciate the insightful and constructive feedback provided by all reviewers. We are particularly encouraged by the consensus recognizing the timeliness and significance of the problem we address—the underutilization of 3D scene encoders in current Vision-Language Models (VLMs)—and the diagnostic value of our systematic analysis.\n\nReviewers xho9, 8U3K, and egbm acknowledged that our paper provides a thorough investigation into an important and timely problem for the 3D VLM community: the failure of models to properly leverage 3D scene features. Reviewer 8U3K found our systematic analysis of linguistic shortcuts to be valuable, noting the intriguing finding that a question-only LLM can achieve performance comparable to 3D scene-input VLMs. Reviewer xho9 emphasized the quality of our extensive experiments, which offer diagnostic insights into the issue.\n\nReviewers 4oGj and egbm specifically praised our solution. Reviewer 4oGj highlighted that the discovery of 3D tokens \"not functioning properly\" is an important finding and appreciated that our proposed remedy of rearranging the input sequence is a simple but effective solution for ensuring the 3D encoder functions as intended. Reviewer egbm also recognized that the proposed sequence rearrangement is simple and effective.\n\nWe believe that we have been able to thoroughly address all reviewers’ comments by clarifying certain sections of the paper, providing additional experimental verifications, and strengthening the discussion around the novelty and generality of our proposed solution. Details on these changes and specific responses to individual concerns can be found in the reply to each reviewer."}}, "id": "jEtSi2Vf7u", "forum": "dTzu5vpQvs", "replyto": "dTzu5vpQvs", "signatures": ["ICLR.cc/2026/Conference/Submission7933/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7933/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission7933/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763561479621, "cdate": 1763561479621, "tmdate": 1763561479621, "mdate": 1763561479621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}