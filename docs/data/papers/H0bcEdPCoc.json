{"id": "H0bcEdPCoc", "number": 8016, "cdate": 1758051940505, "mdate": 1763613939528, "content": {"title": "Let's (not) just put things in Context: Test-time Training for Long-context LLMs", "abstract": "Advances in training and architectural design have enabled LLMs with million-token context windows, yet in practice these models often read far more than they can reliably use. While inference-time compute scaling—typically via “thinking tokens”—can help on short multi-step reasoning tasks, our controlled long-context experiments show rapidly diminishing returns that collapse as context grows. We trace this to score dilution in static self-attention and prove that, in such regimes, decoding more tokens cannot reliably recover buried evidence. We propose query-only test-time training (qTTT): a cache-preserving adaptation that performs a single prefill to fix keys/values and then applies a handful of gradient updates to the query projections. qTTT provably increases the target–distractor margin and, empirically, delivers consistent gains across model sizes and benchmarks. On Qwen3-4B, qTTT improves average accuracy by +12.6 and +14.1 absolute points on LongBench-v2 and ZeroSCROLLS, respectively. The practical takeaway is simple: for long contexts, spending a small inference-time budget on context-specific adaptation is a more effective use of compute than generating additional thinking tokens.", "tldr": "We study the limitations of vanilla inference-time scaling approaches for long-context large language models and study test-time training as a promising alternative.", "keywords": ["long-context language models", "test-time training", "inference-time scaling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8144171bb2b5ee8d3278c172bbd6842c41fe0978.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that long-context failures in static self-attention stem from score dilution: as distractors accumulate, attention mass on the true “needle” vanishes unless the target–distractor logit gap grows with context length. The authors prove a logarithmic margin requirement and show that decoding-based inference-time scaling (e.g., “thinking” tokens) cannot reliably fix this. They propose query-only test-time training (qTTT): perform a single prefill to cache K/V, then run a few lightweight gradient steps updating only the query projections over short spans while reusing the KV cache."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a compute-aware design. The proposed prefill-once, KV-cache reuse, and FLOP matching to thinking tokens make for a fair comparison and a practical recipe.\n\n2. The benchmarks in the paper are sufficient. It evaluates the proposed method across model sizes and multiple long-context benchmarks.\n\n3. The idea is really cute. The inference-time updating the parameters is novel in the community."}, "weaknesses": {"value": "1. The theoretical analysis is too naive to capture the main motivation. Concretely, it cannot prove that the score dilution is the main reason for the poor performance. \n\n* For example, whether the poor performance comes from the small number of training samples. Usually, learning more complex abilities, i.e., solving problems with longer context, requires more training samples than learning the simple ability. The poor performance can simply come from the relatively small number of samples. \n\n* Even we assume that the poor performance comes from the score dilution. The current analysis is oversimplified. For example, Lemma 2.3 requires a $log T$ scaling of the logits. Such can be achieved by RoPE, which is designed for the long-range decay. In addition, adapting RoPE for the proper score decaying is the cornerstone of most long-context papers. The existing analysis just ignores the role of RoPE.\n\n2. The paper claims that the proposed method solves the score dilution problem. However, no evaluation or visualization of the attention scores are presented. \n\n3. Test-time scaling baseline is missing. The proposed method achieves better performance with more computation. However, no such baseline is included. For example, whether the beam search, BoN achieves the comparable performance with the same budget.\n\n4. Ablation is missing. The learning rate for the test-time training is important. Whether this depends on the context length, context semantic is not known."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Puqk2I4d6I", "forum": "H0bcEdPCoc", "replyto": "H0bcEdPCoc", "signatures": ["ICLR.cc/2026/Conference/Submission8016/Reviewer_SSXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8016/Reviewer_SSXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675294531, "cdate": 1761675294531, "tmdate": 1762920018709, "mdate": 1762920018709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of long-context Large Language Models (LLMs): while modern LLMs support context windows of millions of tokens, they often fail to reliably use information buried in long texts. Existing inference-time strategies (e.g., chain-of-thought \"thinking tokens\") show diminishing returns as context length grows, due to a phenomenon the authors term score dilution—static self-attention cannot sufficiently separate the \"target\" (relevant information) from \"distractor\" (irrelevant) tokens, leading to vanishing target probability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*  It formally introduces \"score dilution\" to explain long-context LLM failures, turning vague issues (e.g., missed key info) into a quantifiable, solvable problem—filling a gap in prior research that lacked clear theoretical grounding for such limitations.\n\n* The proposed query-only Test-Time Training (qTTT) is innovative in its frugality: it reuses frozen KV caches and only updates query projections, avoiding the high compute of full-model fine-tuning or ineffective \"thinking tokens\" for long texts.\n\n* qTTT offers a low-overhead, drop-in fix for real-world long-context use cases (code analysis, EHR review), and its \"score dilution\" framework guides future research on improving long-context LLMs beyond incremental tweaks."}, "weaknesses": {"value": "* While it highlights compute efficiency, it does not measure inference latency (critical for production) when qTTT is added—leaving unclear if its small compute overhead translates to acceptable delays for time-sensitive tasks (e.g., real-time code debugging).\n\n* It does not explore how qTTT performs with noisy or low-quality long texts (e.g., unstructured logs, messy code), where distractors are more prevalent—limiting understanding of its robustness beyond clean benchmark datasets."}, "questions": {"value": "Does this paper provide a performance comparison with alternative test-time scaling methods, given an equivalent computational budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pmBbF1pQE1", "forum": "H0bcEdPCoc", "replyto": "H0bcEdPCoc", "signatures": ["ICLR.cc/2026/Conference/Submission8016/Reviewer_Yzb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8016/Reviewer_Yzb3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880414635, "cdate": 1761880414635, "tmdate": 1762920017950, "mdate": 1762920017950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes a test-time learning method for long context handling with ICL examples."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clever idea to learn only the test time \"decoder\", not the \"encoder\"\n- Extremely strong performance improvement"}, "weaknesses": {"value": "- Training required (during decoding)\n  - No detailed efficiency study has happened.\n- No large model is tested"}, "questions": {"value": "- What if training the whole model from scratch using this method? (Including encoder, meta learning approach)\n- Why do you need to update the query parameters? No need to finetune the MLP?\n- How can we serve different query weight parameters in a real-world serving framework, such as vLLM?\n  - This could be challenging due to the CUDA graph capturing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cupbvDAkSx", "forum": "H0bcEdPCoc", "replyto": "H0bcEdPCoc", "signatures": ["ICLR.cc/2026/Conference/Submission8016/Reviewer_BjzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8016/Reviewer_BjzK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919051204, "cdate": 1761919051204, "tmdate": 1762920017459, "mdate": 1762920017459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}