{"id": "Rj2tP4eXI1", "number": 14090, "cdate": 1758228399224, "mdate": 1759897390583, "content": {"title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "abstract": "Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.", "tldr": "", "keywords": ["self-recognition", "AI bias", "AI safety", "self-awareness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d64a3c4e0219df5165027167a09d0f8626087ec9.pdf", "supplementary_material": "/attachment/20fc8c1727fe97f28072af375ae8c3257e21d06c.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims to measure whether current frontier LLMs are able to recognize their own text generations. The authors devise two tasks on which to evaluate this, and test a range of leading open- and closed-weights models. They report that models perform poorly on these tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The ability of models to recognize their own generations is relevant for a variety of applications, as well as AI safety, so it's an important topic of study. The ability is a moving target, so it's good to continually assess the latest models, as this work does."}, "weaknesses": {"value": "In motivating the research, the authors state without further elaboration that models \"cannot be held responsible for outputs they do not recognize as their own\", which seems dubious. The relationship to personality testing, and the relevance of that to AI safety, is also unclear.\n\nFigure 1A is unserious and adds nothing to the paper.\n\nFigures lack error bars.\n\nNit: \"Self-Awarenss\" on line 122\n\nMerely accessing all the models through the OpenRouter API does not \"eliminate implementation-specific variations\" as stated on line 155, as the open-weight models are generally hosted by a range of providers that OpenRouter routes to dynamically based on availability and cost, unless specifically requested otherwise.\n\nMost critically, I have concerns about whether the two tasks chosen are well suited to evaluating self-generated-text recognition ability. \n\nOn the technical side, the binary task is implemented such that there's a 9:1 class imbalance, making overall accuracy an inappropriate metric of success. (F1 scores are appropriate, but they are confusingly presented as \"F1 Rate\" in Figure 3b; also unclear is why \"30%\" is identified as the success criterion.) More fundamentally, simply asking the models whether they wrote a random chunk of text is not a sensitive metric of text-recognition ability. Obviously models have no memory of writing the text, so all they have to go on is textual cues. Outside of obvious tells (e.g., the infamous em dash) the signatures of authorship will be subtle, and will have to be weighed against things like prior probability (\"out of all the texts in the world, how many were generated by me\") and response bias (propensity towards saying Yes or No, or claiming/disclaiming responsibility generally). Because of this, researchers generally use other metrics, such as asking models which of two presented texts is its own, or examining the probabilities assigned to authorship when texts are presented individually as here.\n\nThe exact model prediction task seems even less well motivated. How is DeepSeek Chat V3.1, released March 24, 2025, supposed to be able to identify text generated by GPT-5, released August 7, 2025? Little wonder that models mostly seem to go by model family notoriety, choosing Claude and GPT models most frequently.\n\nNit: Capitalization of \"Effects\" on line 317.\n\nNit: Appendix D tables are interspersed amidst Appendices E and F."}, "questions": {"value": "What were the recall and precision for self-recognition in task 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G89RUMXZIr", "forum": "Rj2tP4eXI1", "replyto": "Rj2tP4eXI1", "signatures": ["ICLR.cc/2026/Conference/Submission14090/Reviewer_btz5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14090/Reviewer_btz5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875434814, "cdate": 1761875434814, "tmdate": 1762924566940, "mdate": 1762924566940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether LLMs can recognize their own generated text—a capacity the authors term self-recognition, which they argue is foundational for AI accountability and trust. Using a systematic cross-evaluation framework, they test ten major LLMs on two tasks: binary self-recognition and exact model prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The cross-evaluation framework across 10 frontier models and two text lengths is diverse.\n\n2. The reasoning-trace examination (e.g., hierarchical bias toward GPT/Claude) adds depth and interpretability to quantitative results.\n\n3. The paper studies a provocative question (“Can LLMs recognize themselves?”) that bridges metacognition, AI safety, and model interpretability."}, "weaknesses": {"value": "1. The number of generated samples may be too small, only 100 generated by each model.\n\n2. The self-recognition task does not discuss how a sample generated by one LLM cannot be generated by others, leaving some doubts on the evaluation. Also the paper does not discuss why we should expect the LLM to have self-recognition. The LLM only gets its ability from training data and the data usually donnot include the name of the model, making it hard to acquire \"self-recognition\".\n\n\n3. It is unclear how to improve the model's self-recognition performance or general performance based on the proposed benchmark."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6V4iKrcU7D", "forum": "Rj2tP4eXI1", "replyto": "Rj2tP4eXI1", "signatures": ["ICLR.cc/2026/Conference/Submission14090/Reviewer_neDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14090/Reviewer_neDd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908840851, "cdate": 1761908840851, "tmdate": 1762924566552, "mdate": 1762924566552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether Large Language Models (LLMs) possess self-awareness. To test this, the authors designed an experiment to evaluate if models can recognize \"content generated by themselves\". The experiment's results demonstrated that most models consistently fail at this self-recognition task, with performance often near random chance. The authors then proceeded to investigate why this phenomenon occurs, hypothesizing that it might stem from a limited awareness of their own or other models' existence. However, further analysis revealed that most models are aware of their own existence and can correctly identify their own model family. Therefore, the authors conclude that the failure in self-recognition is not a simple lack of awareness but is instead caused by deep systematic and hierarchical biases, such as associating high-quality text only with perceived \"top-tier\" models like GPT and Claude rather than themselves."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a highly interesting and novel problem. Instead of focusing on the common task of distinguishing AI-generated text from human-written text , it investigates the models' own intrinsic metacognitive capability for self-recognition.\n\n- The study proposes a novel evaluation framework. The authors designed two specific tasks, binary self-recognition (a yes/no question) and exact model prediction (choosing from a list of 10 models), to test this capability. This design led to the insightful conclusion that models exhibit a \"consistent failure in self-recognition\" , with performance rarely exceeding random chance.\n\n- The authors do not just present the results; they actively explore the underlying reasons for the models' failure. They hypothesized that the failure might be due to a limited awareness of their own existence. They then conducted further experiments (an \"existence test\") to investigate this , ultimately finding that most models do possess family-level awareness. This allowed them to conclude that the failure is instead rooted in deep, systematic \"hierarchical biases\"."}, "weaknesses": {"value": "1. While the paper's introduction promises a \"systematic evaluation framework,\" the methodology section in the main body (Section 3) is somewhat brief.\n- Data Generation: The paper states that it uses prompts across three domains: \"creative writing, technical explanation, and opinion essays\". However, it does not provide a strong justification for why these three specific categories were chosen. Are they considered in terms of completeness? Are they designed to be orthogonal to test different model capabilities? A more detailed rationale for this \"task design\" would strengthen the \"systematic\" claim.\n- Result Stability:  It's not specified if the main evaluation tasks (binary and exact prediction) were run multiple times to check the stability of any single model's prediction patterns. Discussing this potential for variance would add to the robustness of the findings.\n\n2. The paper's most interesting finding is the contradiction: models possess \"family-level awareness\" but fail at self-recognition due to a \"hierarchical bias\". The attribution of this failure to \"hierarchical bias\" feels more like a description of the behavior rather than a deep causal explanation.\n\nTo summarize, my most significant concern is that I feel this paper's evaluation methodology seems to fall short of being 'systematic' and scientifically rigorous. Furthermore, although the experimental results are interesting, the analysis lacks sufficient depth."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0D8qhDR44t", "forum": "Rj2tP4eXI1", "replyto": "Rj2tP4eXI1", "signatures": ["ICLR.cc/2026/Conference/Submission14090/Reviewer_nsB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14090/Reviewer_nsB7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175089785, "cdate": 1762175089785, "tmdate": 1762924566136, "mdate": 1762924566136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}