{"id": "d0zxXMd0k4", "number": 20746, "cdate": 1758309617980, "mdate": 1763751932987, "content": {"title": "Performative Policy Gradient: Ascent to Optimality in Performative Reinforcement Learning", "abstract": "Post-deployment machine learning algorithms often influence the environments they act in, and thus *shift* the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this *performative* setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the **Performative Policy Gradient** algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to *performatively optimal policies*, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves  *performative stability* but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.", "tldr": "First performative policy gradient algorithm designed to provably attain performative optimality with softmax parameterisation.", "keywords": ["Reinforcement Learning", "Performative Reinforcement Learning", "Markov Decision Process", "Policy Gradient", "Convergence", "Optimality"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f86eff2eea77a4a49210d75d7bfcba671ba2dc9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies performative reinforcement learning, where an agent's deployed policy influences the environment's reward and transition dynamics. This work applies policy gradient methods in this performative RL setting. It provides the performative RL version of the performance difference lemma and the policy gradient theorem. It provides a rigorous convergence analysis in the tabular setting. Experiments are also provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper successfully applies policy gradients in the performative RL setting and provides theoretical analysis. The derivation of the Performative Performance Difference Lemma (Lemma 1) and the Performative Policy Gradient Theorem (Theorem 2) provides general-purpose tools for future research. Theorem 3 provides an analysis of the rate of convergence.\n- The authors argue that the prevailing focus in PeRL on achieving performative stability is insufficient, as such policies can be suboptimal. Instead, this work aims to achieve performative optimality, finding a policy that maximizes the expected return within the environment it induces."}, "weaknesses": {"value": "- The Performative Policy Gradient Theorem (Theorem 2) and the resulting gradient estimator (Equation 10) explicitly include the gradient terms (for policy, transition, and reward). To compute these terms, one must know the exact, differentiable functional form that maps the policy parameters to the environment's transition and reward functions. For instance, in the specific case of the softmax PeMDP (Equation 11), one must have access to the feature map $\\psi(\\cdot)$ and the reward coefficient $\\xi$. This makes PePG fundamentally different from, and less generally applicable than, standard policy gradient methods.\n- The application of policy gradients in performative RL also seems not to be theoretically challenging. The proof structure follows quite consistently with the seminal works."}, "questions": {"value": "How is your Cov different from that in the standard RL setting? Would it be unbounded?\nTypos: Line 97 and Line 480 have a typo on (1-\\gamma). Typo also appears on line 1415."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HIatP22XOP", "forum": "d0zxXMd0k4", "replyto": "d0zxXMd0k4", "signatures": ["ICLR.cc/2026/Conference/Submission20746/Reviewer_UpXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20746/Reviewer_UpXT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973661448, "cdate": 1761973661448, "tmdate": 1762934170911, "mdate": 1762934170911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the performative reinforcement learning (PRL) setting, where the transition kernel and reward function depend on the deployed policy. In this setting, the important solution concepts are performatively optimal points (PO) and performatively stable points (PS). The algorithms in the PRL literature find PS rather than PO points. The authors propose the first PRL algorithm that converges to PO rather than converge to PS as the algorithms in the literature do. The main contributions of the paper are to derive performance difference lemma for the PRL setting and to derive the policy gradient theorem for the performative and entropy-regularized performative settings. They propose the performative policy gradient algorithm (PePG), which is the standard policy gradient (PG) algorithm with the gradient estimation replaced by the performative counterparts. They analyze the convergence of this algorithm in the softmax policy class for softmax PeMDPs and showcase the difference between algorithms in the experiments. A summary of the theoretical contributions are as follows:\n- Lemma 1: Performance difference lemma in performative setting\n- Lemma 2: A bound on the performative shift around PO following from the performative performance difference lemma\n- Theorem 2: Policy gradient derivation for performative and entropy-regularized performative settings\n- Lemma 3: Gradient domination in softmax PeMDPs for the unregularized and entropy-regularized value function\n- Theorem 3: Convergence of PePG in softmax MDPs\n\nNote: softmax PeMDPs refer to the setting where there exists a feature map $\\psi: \\mathcal{S} \\to [0, \\psi_{\\max}]$ such that the transition kernel satisfies $P_{\\pi_\\theta}(s' | s, a) \\propto \\exp (\\theta_{s, a} \\psi(s'))$ and the rewards are linear in policy parameters $r_{\\pi_{\\theta}}(s, a) = \\mathcal{P}_{-[R_{\\max}, R_{\\max}]}[\\xi \\theta_{s, a}]$."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The focus in the PRL literature is on finding PS. The question in this paper of whether we can find performatively optimal points is important.\n- The results are new in PRL literature although the techniques does appear to be standard in RL/PRL literature.\n- The first two chapters are well-written and enjoyable to read. I comment on the rest further on the weaknesses and questions sections.\n- The authors contribute the first algorithm that finds $\\epsilon$-PO in the softmax PeMDPs.\n- The experiments look promising for PePG."}, "weaknesses": {"value": "The analysis is only for softmax PeMDPs. It would be helpful to understand how restrictive this class of problems is. It would make the paper clearer to compare and contrast this class with other common classes and possibly give some examples of what can be represented in this class or not. I also find references to some related works on softmax PeMDPs neccessary.\n- It is also unclear why the authors avoid the sensitivity assumption and opt for the softmax PeMDP assumption. The softmax PeMDP assumption seems to be much restrictive. It looks like it is a theoretical convenience as there is no discussion about this choice. In addition, the statement \"This is the only assumption needed through the paper\" (line 241) looks deceptive due to the softmax PeMDP assumption.\n- The algorithm only works for softmax PeMDPs as the gradient estimation uses the specific structure of softmax PeMDPs, which appears to be a very strong assumption necessary for gradient estimation.\n- Theorem 3 does not work for $\\epsilon \\le (\\star)$ where the $(\\star)$ is the term in line 1623 for the unregularized case and the term in line 1886 for the regularized case. It is not obvious whether this is a proof artifact or the true behavior. Given Theorem 3 and the experiments in [1], this might not be a proof artifact but the true behavior. The authors should consider adding a related discussion.\n- The presentation of the results could be improved. It is not obvious what is assumed for results. The authors claim that only bounded rewards is assumed. However, in Theorem 2, the gradient of reward and transition functions are taken w.r.t. the policy. What ensures the differentiability? If it is the softmax PeMDP setting, then this theorem are misplaced in the Section 3 and should be in Section 4 (or the softmax PeMDPs) should be introduced and assumed earlier.\n- The implication for Lemma 2 could be stated clearer, possibly by connecting the results to the later sections.\n- There are no values that quantifies the performative strength. This implies the results do not depend on any such constant and it is not obvious how one should interpret the results in the sense of performativity.\n- The experiments discussion are not sufficient. I elaborate on this in questions section.\n\nMinor issues:\n- The authors note that \"propose the first provably converging and computationally efficient PG algorithm.\" While this is true for the standard PRL problem, [1] proves that the standard policy gradient ascent and natural policy gradient algorithms converge in the general setting of performative Markov potential games. The authors should consider mentioning this in the paper.\n- For the theoretical results, it would be helpful to refer to the related proofs in the appendix.\n- How are the sample-based estimator part (between lines 1323-1329) related to the proof? If it is not related, the authors could consider placing it to somewhere else.\n- Line 323: kernesls -> kernels\n- Line 1919: optimale -> optimal\n- The latex equations in y-axes of Figure 2 are not rendered.\n- The sentence \"As we develop a PG-type algorithm, it will be interesting to see how much can we reduce variance (Wu et al., 2018; Papini et al., 2018) while achieving optimality.\" (line 482) could be more clear.\n\n[1] Sahitaj, R., Sasnauskas, P., Yalın, Y., Mandal, D. and Radanovic, G., 2025, April. Independent Learning in Performative Markov Potential Games. In _International Conference on Artificial Intelligence and Statistics_ (pp. 3304-3312). PMLR."}, "questions": {"value": "- The authors note that the only assumption needed is the bounded rewards. However, in this case, what guarantees the existence of the performative gradients? Doesn't one at least need to assume the differentiability of $P_{\\pi}$ and $r_{\\pi}$ w.r.t. the policy? Doesn't gradient estimation require some further assumptions such as boundedness in this case?\n- In the PRL literature, the strength of the performative effects are quantified through the sensitivity assumption and the related constants, e.g., $\\omega_r$, $\\omega_p$ in [1] and  $\\varepsilon_r$, $\\varepsilon_p$ in [2]. However, the authors refrain from having such an assumption (although softmax MDP assumption seems to be stronger). In this case, how does one quantify the performative strength in this setting? It looks like it depends on $\\xi$ and $\\psi$ but it is not obvious how. Moreover, the regret bounds does not depend on the performative strength so it is unclear how the performance changes w.r.t. the performative strength.\n- Similar to the previous question, it is not clear how one should pick the performative strength for the regularization term. In the papers referenced in Table 1, the regularization strength depends on the performative strength. Although there is an ablation study in Appendix H, the discussion is insufficient. Could you elaborate on how to pick the term?\n- Could you also compare the standard PGA and NPG methods in this setting (e.g. as described in [1])? They also provide meaningful baselines as the other baselines in the work is repeated retraining algorithms.\n- Why is policy stability measured w.r.t. the consecutive occupancy measures instead of policies? How can one conclude policy stability from this difference? Why is this better than measuring the difference between consecutive policies?\n- What are the stopping criteria for the experiments? Are they run for fixed 100 iterations? The distance between consecutive occupancy measures seems to have converged but the value function keeps increasing on average for PePG and Reg PePG. What is the optimal policy and value? How suboptimal are the policies in the graphs? What does the theory suggests about the value function gap in this case? How many iterations are necessary to achieve a PO? Could you also plot the consecutive policy differences as well as the occupancy differences?\n- What do the authors mean by \"However, this stability comes at the cost of solution quality, as MDRR becomes trapped in a suboptimal point\" (line 468)? The average difference between consecutive occupancy measures are the same as PePG algorithm. Could you elaborate on how one can interpret the same average behavior as \"prioritizing finding any stable point over finding an optimal solution\" (line 468) for the baseline MDRR algorithm and as \"actively exploring for better solutions\" (line 469) for the PePG algorithm. In addition, could you elaborate on why one would not want policy stability in this case?\n\n[1] Sahitaj, R., Sasnauskas, P., Yalın, Y., Mandal, D. and Radanovic, G., 2025, April. Independent Learning in Performative Markov Potential Games. In _International Conference on Artificial Intelligence and Statistics_ (pp. 3304-3312). PMLR.\n\n[2] Mandal, D., Triantafyllou, S. and Radanovic, G., 2023, July. Performative reinforcement learning. In _International Conference on Machine Learning_ (pp. 23642-23680). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HwEkf1340V", "forum": "d0zxXMd0k4", "replyto": "d0zxXMd0k4", "signatures": ["ICLR.cc/2026/Conference/Submission20746/Reviewer_D3Xq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20746/Reviewer_D3Xq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979677813, "cdate": 1761979677813, "tmdate": 1762934170574, "mdate": 1762934170574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL. Based on these, they introduce the Performative Policy Gradient algorithm (PePG). The proposed method is evlauated on standard performative RL environments and outperforms s standard policy gradient algorithms and the existing performative RL algorithms aiming for stability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strong theorical anylisis and formulation. \n\nSufficient baseline and ablations."}, "weaknesses": {"value": "1. Incorrect statement: At introduction, it says \"Some examples of successful and popular policy gradient methods include TRPO, PPO, DDPG, SAC \". But DDPG and SAC are Q-learning algorithms not policy gradient methods.\n\n2. Misleading plots: The left plot in Figure 2 marked as \"Value Function Evolution\". Shouldn't evaluation average return be the accurate plot here?"}, "questions": {"value": "Since evaluation is limited to one environment, would the results generalize to environments with different mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VCGoFjpwrz", "forum": "d0zxXMd0k4", "replyto": "d0zxXMd0k4", "signatures": ["ICLR.cc/2026/Conference/Submission20746/Reviewer_izdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20746/Reviewer_izdx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995574720, "cdate": 1761995574720, "tmdate": 1762934170179, "mdate": 1762934170179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}