{"id": "tIVCfVnIHo", "number": 1354, "cdate": 1756875009521, "mdate": 1759898213413, "content": {"title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "abstract": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation. Video results: https://anonlyra.github.io/anonlyra", "tldr": "Feed-forward 3D and 4D with camera-controlled video diffusion models", "keywords": ["3d", "video diffusion", "gaussian splatting"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baaae7062100b48bbd82b388591323d6b9d3f874.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a self-distillation strategy based on a camera-controlled video diffusion model and only needs to train a 3DGS decoder directly operating in the latent space of the video model. In this way, the 3DGS decoder is supervised by the original RGB decoder of the video model, which only needs an input image and a sampled camera trajectory instead of captured real-world multi-view images. 4D generation can be easily extended through time conditioning modules within the 3DGS decoder."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is clear and important. Using synthetic data instead of real multi-view data can obviously provide greater diversity.\n2. The overall paper is easy to read, with a very clear logic and structure.\n3. The paper provides extensive experiments to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. I have some doubts about the efficiency of the proposed approach. It is generally acknowledged that video generation models are still quite time-consuming, and this paper’s method additionally requires multiple trajectories. However, I did not see any reported statistics on the dataset generation time, nor whether additional manual screening or automated filtering is needed.\n\n2. Another key concern lies in multi-view consistency. Are the videos for different trajectories generated independently? If so, how is multi-view consistency ensured? If not, wouldn’t that further reduce efficiency and make the approach difficult to apply in practice? I have the same question regarding the *video-to-4D* setting.\n\n3. In the quantitative experiments, I believe that monocular reconstruction or single image-to-3d generation tasks are difficult to evaluate using reconstruction metrics, and the authors did not provide corresponding qualitative comparisons. Moreover, since datasets like DL3DV contain complex and diverse trajectories, while Lyra generates 3D scenes along fixed trajectories, how does Lyra achieve the best quantitative results under such conditions?"}, "questions": {"value": "Please see the weaknesses.\n\nSince the motivation of the paper is very important, my initial score is 6. If my concerns can be properly addressed, I will actively consider increasing the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5AB5p5bD7E", "forum": "tIVCfVnIHo", "replyto": "tIVCfVnIHo", "signatures": ["ICLR.cc/2026/Conference/Submission1354/Reviewer_J3MU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1354/Reviewer_J3MU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555577993, "cdate": 1761555577993, "tmdate": 1762915746230, "mdate": 1762915746230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lyra, a self-distillation framework designed to distill the implicit 3D knowledge from video diffusion models into an explicit 3D Generative Scene (3DGS) representation. The process begins by collecting video latent data, which includes multiple camera trajectories obtained from a pretrained camera-controlled video diffusion model. Using this dataset, the authors build upon long-LRM to transform the video latent representations into 3DGS. Additionally, the approach can be extended to dynamic content by generating 3DGS on a per-frame basis. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance in both static and dynamic 3D scene generation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, providing sufficient implementation details.\n2. The method is straightforward, and the implementation is well executed. It allows for the immediate transformation of the generated video into a 3DGS representation and enables dynamic scene reconstruction from the video generation model."}, "weaknesses": {"value": "1. The technical contribution is somewhat limited. It presents a straightforward combination of video diffusion and long-LRM. While long-LRM can accept raw images as input, the proposed method specifically utilizes video latent representations. This technique has already been implemented in Wonderland [1]. \n\n2. For dynamic scene reconstruction, utilizing frame-wise 3DGS may result in temporal inconsistencies, as demonstrated in the provided video.\n\n[1] Wonderland: Navigating 3D Scenes from a Single Image"}, "questions": {"value": "1. What is the main difference between this method and Wonderland, which also generates video latent representations first and uses latent-LRM for 3DGS reconstruction?\n   \n2. Compared to previous methods, what accounts for the performance improvements? Is it due to the camera-controlled video diffusion model being superior to earlier versions or denser camera viewpoints? \n\n3. What is the result of directly feeding the generated pixels from Gen3C into long-LRM, despite their efficiency? This could be a baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jev5TxvsJv", "forum": "tIVCfVnIHo", "replyto": "tIVCfVnIHo", "signatures": ["ICLR.cc/2026/Conference/Submission1354/Reviewer_tRus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1354/Reviewer_tRus"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925883916, "cdate": 1761925883916, "tmdate": 1762915746104, "mdate": 1762915746104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lyra, a pipeline for generative 3D scene reconstruction and text-driven scene editing under sparse and noisy real-world mobile captures. The method jointly optimizes camera, geometry, and appearance and supports object removal, replacement, and global stylistic edits. It integrates latent diffusion priors with real-world scene scale and spatial consistency constraints, demonstrated on a diverse dataset including indoor and outdoor environments (e.g., apartment, temple, garden scenes) where real mobile scans contain artifacts such as railings and scaffolds."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method targets real mobile scans with imperfect geometry, lighting, and camera poses, a valuable problem space. Example scenes include cluttered apartments and outdoor temple/garden scans with occluders (e.g., railings, scaffolding).\n\n2. The method can recover missing background structure when occluded (e.g., walls behind furniture) and maintain accurate room layout.\n\n3. Lyra enforces realistic scale and physical spatial consistency across generations."}, "weaknesses": {"value": "1. While results demonstrate quality, more specifics on training/inference time would help adoption clarity.\n\n2. Some failure modes still visible. E.g., subtle texture drift in larger structural edits, though still competitive and reasonable under sparse-view settings."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eq80w4guNu", "forum": "tIVCfVnIHo", "replyto": "tIVCfVnIHo", "signatures": ["ICLR.cc/2026/Conference/Submission1354/Reviewer_Yx9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1354/Reviewer_Yx9E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930003965, "cdate": 1761930003965, "tmdate": 1762915745823, "mdate": 1762915745823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lyra, a novel framework for generating explicit 3D (and dynamic 4D) scene representations from a single image or video. The key contribution is a self-distillation method that distills the implicit 3D knowledge from a pre-trained, camera-controlled video diffusion model (the \"teacher\") into a feed-forward 3D Gaussian Splatting (3DGS) decoder (the \"student\"). This approach cleverly avoids the need for large-scale, real-world multi-view 3D datasets by using the video model's RGB output to supervise the 3DGS decoder, both operating from a shared video latent. The framework is also effectively extended to dynamic 4D scene generation from a single monocular video, demonstrating strong performance and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Self-Distillation: The core idea of using a pre-trained video diffusion model as a \"teacher\" to supervise a 3DGS \"student\" decoder is highly novel. This latent-space distillation is an elegant solution to the 3D training data bottleneck, effectively bridging the gap between powerful 2D generative models and explicit 3D representations.\n\n2. SOTA Performance and Representation: The method achieves state-of-the-art quantitative results on standard benchmarks (RealEstate10K, DL3DV, Tanks-and-Temples) for single-image-to-3D generation. By directly outputting a 3DGS representation, the model produces a geometrically consistent, explicit 3D scene that supports real-time rendering, which is a significant advantage over 2D-only video models.\n\n3. Efficient Latent-Space Operation and 4D Extension: Operating the 3DGS decoder in the VAE's latent space allows the model to efficiently process a very large number of synthesized views (726 views per scene), overcoming the memory and compute bottlenecks of pixel-space reconstruction methods. The extension to 4D dynamic scenes from a single video, supported by a clever \"dynamic data augmentation\" strategy, is a substantial and well-executed addition."}, "weaknesses": {"value": "1. Dependency on Teacher Model Quality: The paper's conclusion rightfully states that the output quality is \"bounded by the capacity of our camera-controlled video diffusion model.\" This is a strong dependency. Any artifacts, flickering, or 3D inconsistencies generated by the teacher (GEN3C) will be directly distilled into the final 3DGS representation. The paper does not fully explore how robust the framework is to such failures in the teacher model.\n\n2. Reliance on External Depth Estimation: The paper notes that using only RGB loss \"often produces flattened geometry,\" and thus adds a scale-invariant depth loss ($\\mathcal{L}_{depth}$) supervised by an off-the-shelf depth estimator (ViPE). While effective, this means the method is not purely self-distilled from the video model; it relies on another pre-trained model for geometric guidance. This slightly weakens the claim of distilling 3D knowledge solely from the video model's multi-view consistency.\n\n3. Minor Terminology Nitpick: The term \"self-distillation\" is used, but the framework is more accurately a form of cross-modal distillation: from a 2D decoder (the VAE's RGB head) to a 3D decoder (the new 3DGS head). In classic self-distillation, a large model teaches a smaller model of the same architecture. This is a minor semantic point and does not detract from the method's novelty."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6tTAd1uMgO", "forum": "tIVCfVnIHo", "replyto": "tIVCfVnIHo", "signatures": ["ICLR.cc/2026/Conference/Submission1354/Reviewer_hhrs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1354/Reviewer_hhrs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996270304, "cdate": 1761996270304, "tmdate": 1762915745636, "mdate": 1762915745636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}