{"id": "ITeuGb2bYg", "number": 1378, "cdate": 1756877563737, "mdate": 1759898211837, "content": {"title": "Policy Likelihood-based Query Sampling and Critic-Exploited Reset for Efficient Preference-based Reinforcement Learning", "abstract": "Preference-based reinforcement learning (PbRL) enables agent training without explicit reward design by leveraging human feedback. Although various query sampling strategies have been proposed to improve feedback efficiency, many fail to enhance performance because they select queries from outdated experiences with low likelihood under the current policy. Such queries may no longer represent the agent's evolving behavior patterns, reducing the informativeness of human feedback. To address this issue, we propose a policy likelihood-based query sampling and critic-exploited reset (PoLiCER). Our approach uses policy likelihood-based query sampling to ensure that queries remain aligned with the agent’s evolving behavior. However, relying solely on policy-aligned sampling can result in overly localized guidance, leading to overestimation bias, as the model tends to overfit to early feedback experiences. To mitigate this, PoLiCER incorporates a dynamic resetting mechanism that selectively resets the reward estimator and its associated Q-function based on critic outputs. Experimental evaluation across diverse locomotion and robotic manipulation tasks demonstrates that PoLiCER consistently outperforms existing PbRL methods.", "tldr": "We present an efficient preference-based reinforcement learning method which selects informative queries using policy likelihoods and mitigates reward overestimation caused by primacy bias.", "keywords": ["preference-based reinforcement learning", "robotic manipulation", "locomotion"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/481c34bcd4c8e20abc9b87ea4a877eca7be1b5c8.pdf", "supplementary_material": "/attachment/37e807e4802e6fc16c0365b4e503b0bba486c591.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a new method called PoLiCER to improve query efficiency in previous PbRL work. It proposes two improvements: policy likelihood sampling (PLS) aims to make the reward model focus on the trajectories that are likely under the current policy; critic exploited reset forces the reward estimator to reset occasionally to reduce overestimation bias that emerges from repeated feedback over the same region of behaviour. Experiments involved in locomotion and manipulation show that PoLiCER outperforms prior PbRL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Preference-based RL is a growing area with practical relevance. This work provides a new perspective to improve the algorithms, promoting the development of the field.\n\n2. The motivation is clear, and the authors use experiments to demonstrate that these issues are indeed important, which enhances the readability and coherence of the paper.\n\n3. The two mechanisms the authors propose are beneficial to each other: reset the critic and reward model to make them more plastic to adapt to preference data that is more consistent with the current strategy.\n\n4. The authors provide empirical evaluation across multiple tasks (locomotion + robot manipulation), which helps test generality.\n\n5. The code is included in the Supplementary Material, demonstrating the reproducibility of this work."}, "weaknesses": {"value": "1. PLS seems to lack some theoretical insight to support. It is not clear why directly maximizing the policy likelihood can find more informative trajectories.\n\n2. Human-in-the-loop experiments are relatively simple.\n\n3. In the main experiments, using TA as another trick to the algorithm may cause SURF to completely become an ablation of this method, and the additional setting of the ratio may lead to unfairness in the experiment."}, "questions": {"value": "1. What are the details in human-in-the-loop experiments? How did the volunteers provide feedback? I hope the authors can provide some examples to prove that their experiments are fair, i.e., this feedback may show personalized preferences or mixed preferences, and PoLiCER can handle them. Meanwhile, explain what factor will affect the process.\n\n2. In the main experiments, why is QPA better than PoLiCER in Hammer? PLS improves QPA's sampling method, so PoLiCER should definitely be better than QPA in all experiments, but the results are not.\n\n3. What is the time consumption for pixel-based tasks? Since these tasks generally take more time, it is important to report your time consumption and compare it with previous methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4VH0slCQlp", "forum": "ITeuGb2bYg", "replyto": "ITeuGb2bYg", "signatures": ["ICLR.cc/2026/Conference/Submission1378/Reviewer_SQmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1378/Reviewer_SQmv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431755890, "cdate": 1761431755890, "tmdate": 1762915753766, "mdate": 1762915753766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PoLiCER addresses two central challenges in preference-based RL: query-policy misalignment and reward overestimation caused by primacy bias. It proposes Policy Likelihood-based Sampling (PLS) to select queries aligned with the current policy and Critic-Exploited Reset (CER) to dynamically reset the reward model and critic when overestimation is detected."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper focuses on (1) query–policy misalignment in query sampling and (2) reward overestimation caused by primacy bias in reward learning, which are key issues of high importance in PBRL.\n\nThe design of the approach is reasonable, including the likelihood computation in PLS and the CRITIC-EXPLOITED RESET mechanism; it is simple and low-cost.\n\nIt shows clear performance advantages on both vector-observation and pixel-observation tasks, and provides human-in-the-loop validation along with fairly comprehensive ablations."}, "weaknesses": {"value": "PLS is sensitive to policy entropy/scale and may prefer low-entropy trajectories, reducing diversity.\n\nThe discussion of related work could be more complete. PPE~[1] advocates proximal policy exploration to expand the coverage of the preference buffer and improve reward model quality; like this paper’s PLS, it aims to make labeled data/queries closer to the current policy, but PPE emphasizes active generation/exploration, whereas PLS passively selects from the replay buffer and ranks by likelihood. The difference and advantage of PLS over PPE should be discussed in the manuscript.\n\nMinor issues such as typos: “primary bias” vs. “primacy bias”; a unified wording is recommended.\n\n\n[1] Zhu, Y., ... . (2024). Optimizing reward models with proximal policy exploration in preference-based reinforcement learning. In NeurIPS 2024 Workshop on Behavioral Machine Learning."}, "questions": {"value": "Does the PLS likelihood score bias toward low-entropy policies? It is recommended to add sensitivity experiments on policy entropy/temperature.\n\nIs a diversity constraint necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "37AtCEXjna", "forum": "ITeuGb2bYg", "replyto": "ITeuGb2bYg", "signatures": ["ICLR.cc/2026/Conference/Submission1378/Reviewer_vBkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1378/Reviewer_vBkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807616157, "cdate": 1761807616157, "tmdate": 1762915753638, "mdate": 1762915753638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PoLiCER, which introduces two mechanisms to improve preference-based reinforcement learning. 1) Policy Likelihood-based Sampling (PLS) selects feedback queries most aligned with the current policy. 2) Critic-Exploited Reset (CER) prevents reward overestimation from primacy bias by adaptively resetting the reward and critic networks. The goal of PLS is to ensure feedback queries remain representative of the current policy, avoiding outdated or irrelevant samples. Compared to recency-based methods, it directly measures alignment between data and current policy. It is more computationally efficient than disagreement sampling, requiring only $2 \\times L \\times K$ forward passes than $N$ for disagreement sampling. And it does not increase training cost. The goal of CER is to counteract primacy bias, where the reward estimator overfits to early feedback and inflates Q-values, leading to overoptimistic policies. It dynamically stabilizes reward learning by resetting networks only when critic overestimation is detected. Experiments are conducted on Meta-World and DMControl tasks. Authors compared to several existing baselines and show improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "PoLiCER offers several strengths over prior preference-based reinforcement learning methods such as disagreement sampling and recency-based query selection. Its Policy Likelihood-based Sampling (PLS) improves data–policy alignment by measuring how representative each trajectory is under the current policy, rather than assuming recency implies relevance. This allows the model to select feedback that directly reflects its current behavior, improving sample efficiency and policy convergence.\n\nAnother advantage is computational efficiency. Unlike ensemble-based disagreement sampling, which requires multiple forward passes per query, PLS operates with a fixed, small number of policy evaluations. Its inverse-rank likelihood weighting also provides robustness to outliers, enabling stable performance across diverse continuous control tasks. Together, these factors make PoLiCER both efficient and reliable in selecting informative feedback.\n\nPoLiCER’s second component, Critic-Exploited Reset (CER), effectively mitigates reward overestimation caused by primacy bias. Instead of using fixed reset intervals, CER dynamically resets the reward estimator and critic only when critic outputs exceed an adaptive threshold. This approach reduces overestimation while allowing normal learning to continue when stable, leading to better long-term returns and less training disruption.\n\nOverall the writing is clear and easy to follow. Experimental setups are well explained, together with baseline methods. In the DMControl suite, it achieved performance comparable to QPA and clearly outperformed earlier PbRL methods like PEBBLE, SURF, RUNE, and MRN. In the more challenging Meta-World benchmarks, PoLiCER further distinguished itself by achieving over 80% average success, significantly higher and more consistent than competing methods."}, "weaknesses": {"value": "1. PLS depends on accurate policy likelihood estimation, which may be unreliable in highly stochastic or multimodal policies. \n2. The rank-based weighting, though robust, can blur distinctions between highly relevant and moderately relevant samples. \n3. CER’s adaptive resets introduce temporary instability as networks reinitialize, requiring careful tuning of replay ratios. \n4. Because PoLiCER omits ensemble-based uncertainty modeling, it may handle noisy or inconsistent human feedback less effectively than Bayesian or disagreement-based methods."}, "questions": {"value": "It seems that PoLiCER has more empirical performance gain in pixel-based environments, is there any intuition on why this happens compared to state-based environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k6Yqh54Qiz", "forum": "ITeuGb2bYg", "replyto": "ITeuGb2bYg", "signatures": ["ICLR.cc/2026/Conference/Submission1378/Reviewer_i7YB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1378/Reviewer_i7YB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098228717, "cdate": 1762098228717, "tmdate": 1762915753506, "mdate": 1762915753506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}