{"id": "YxVebk7kIw", "number": 9510, "cdate": 1758125458819, "mdate": 1763720825480, "content": {"title": "MAPLE: Masked Adapter Prototype Learning for OOD generalization", "abstract": "Parameter-efficient fine-tuning with adapters (e.g., LoRA) equips LLMs with task-specific skills. However, utilizing multiple pretrained adapters for out-of-distribution (OOD) generalization remains challenging. Existing techniques for OOD generalization using multiple pretrained LoRAs, route inputs using LoRA representations (prototypes) obtained independently, assuming these representations capture complementary information. However, we observe that for existing methods, in-distribution and OOD routing entropies are often comparable, thus bringing the complementarity assumption into question. We derive the theoretical conditions that could lead to a violation of such assumptions, distilling the cause down to the presence of shared, noisy prototype subspaces. Based on this, we introduce $\\textbf{MAPLE (Masked-Adapter Prototype LEarning)}$, a simple learning framework that refines LoRA prototypes by masking the target task’s LoRA during prototype learning. In doing so, it encourages prototypes to discard noisy attributes, which improves routing and strengthens OOD generalization. Extensive experiments on language models of varying size, such as Phi-2 (2.7B) and LLaMA-3 (8B) equipped with heterogeneous pools of pretrained LoRAs, show that MAPLE improves the LoRA representation and thus achieves state-of-the-art performance across multiple benchmarks.", "tldr": "This paper proposes a masked-adapter prototype learning scheme that fixes unreliable routing among multiple pretrained LoRAs by learning complementary, denoised prototypes, boosting OOD generalization.", "keywords": ["Mixture of LoRAs", "OOD generalization", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1565ca6c8870077f10e3a2aa608d5792323f03af.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to improve OOD methods that use multiple pre-trained LoRAs with routing. The authors utilize routing entropy to demonstrate that routing may not provide complementary information across different tasks. Furthermore, they present a theoretical analysis to explain the noisiness in routing and the resulting degradation in OOD performance. To address this issue, the authors propose removing the target-task LoRA from the forward pass during training, positing that this will help reduce spurious, task-specific noise in the learned representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses an interesting and important problem. I like the comprehensive overview of prior works and their connection to the current study. The results also demonstrate good improvements over previous state-of-the-art methods."}, "weaknesses": {"value": "From the problem formulation, it is not entirely clear what type of Out-of-Distribution (OOD) setting is used by the authors. Some parts of the paper would benefit from clearer sentences and a more precise problem formulation (see Questions for details). Additionally, certain ideas in the paper could be explained more clearly (see Questions for details). \n\nThe proposed method is based on removing the target-task LoRA from the forward pass during training. The authors posit that this will reduce noisiness; however, this assumption is not supported by prior work or analysis."}, "questions": {"value": "It would be helpful to begin the methodology section with a brief introduction to the problem and the notations used. For instance, the authors introduce Equation 1 without defining $e$ and $x$, which makes it difficult to understand the purpose of routing entropy. Similarly, the section would benefit from first introducing the problem setting (e.g., multitask learning with LoRA and routing) and clearly explaining what prototypes and routing entropy are before using them in the analysis.\n\nOn line 164, authors mention following:\n\n> We start by formalizing our argument that for a set of experts to generalize OOD, it must be accompanied by an increased entropy, relative to ID samples, when applied to OOD samples.\n\nEntropy represents the expected information over a distribution. For a set of experts to generalize to OOD data, the entropy for in-distribution (ID) and out-of-distribution (OOD) samples should ideally be similar. In other words, the expected information for OOD samples should be comparable to that of ID samples. If OOD samples are highly unexpected (i.e., yield much higher information), the model may struggle to generalize. Perhaps the authors are instead referring to routing entropy? Please correct me if this interpretation is inaccurate.\n\nIn the line 143 and line 160, authors reason about routing entropy for IID and OOD samples.  \n\n> Ideally, for in-distribution (ID) tasks (i.e., LoRA trained on the input task exists in the LoRA pool), the routing entropy should be lower. While for out-ofdistribution (OOD) tasks, the routing entropy is expected to be higher, reflecting greater uncertainty.\n\nAnd then explain observed similarity in entropy across ID and OOD samples.\n\n> This could be due to representations encoding certain noisy attributes, which blur inter-adapter distinctions and thus yield high routing uncertainty.\n\nHowever, similarity in routing entropy could also result from an effective router that performs well on OOD tasks. It would be helpful if the authors provided an intuitive explanation of this aspect.\n\nOn line 294, authors mention target-task.\n\n> We hypothesize that this failure stems from including the target-task LoRA in the routing during training. \n\nHowever, it is not quite clear what target-task in this setting means."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ovhWicdhST", "forum": "YxVebk7kIw", "replyto": "YxVebk7kIw", "signatures": ["ICLR.cc/2026/Conference/Submission9510/Reviewer_8KQz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9510/Reviewer_8KQz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851316080, "cdate": 1761851316080, "tmdate": 1762921082056, "mdate": 1762921082056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to improve out-of-distribution generalization in prototype-based routing for LLM adapter selection. During prototype learning, the target task’s adapter is masked to prevent the router from learning noisy features. The authors theoretically show that noisy subspaces cause in- and out-of-distribution samples to become indistinguishable. Experimental results demonstrate that the proposed framework improves the performance of two prototype-based routing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a solid theoretical analysis, including proofs, to explain the inadequacy of existing methods.\n- The proposed method is conceptually simple and easy to implement."}, "weaknesses": {"value": "- The paper assumes that the entropy difference between in- and out-of-distribution samples adequately captures OOD generalization. However, this assumption overlooks other well-established metrics that can better capture distributional separability, such as the energy score [1].\n- The theoretical analysis is not clearly presented. For example, the meaning of the equation in Definition 1 is unclear, and its connection to the rest of the analysis is not well explained. As another example, it is unclear why theorem one indicates that \"ensuring that the routing plan induces a low source and high target entropy is a necessary condition for generalization\".\n-  There is a gap between the theoretically claims and the empirical method. It is unclear why masking the target task's adapter leads a less noisy representation. Some theoretical or empirical analysis is expected.\n- The experimental evaluation is not robust and comprehensive.\n  - Only the overall task performance is reported. Routing or OOD detection performance is missing.\n  - The reported improvements over the baselines are marginal, and the absence of error bars or confidence intervals makes it difficult to assess statistical significance.\n- The approach is narrowly tailored to the prototype-based adapter selection setting, which limits its general applicability.\n\n[1] Liu, Weitang, et al. \"Energy-based out-of-distribution detection.\" Advances in neural information processing systems 33 (2020): 21464-21475."}, "questions": {"value": "- In Definition 1, is $f$ a classifier or an encoder?\n- In the experiments, what examples are considered as ID and what are considered as OOD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yNWXAte19N", "forum": "YxVebk7kIw", "replyto": "YxVebk7kIw", "signatures": ["ICLR.cc/2026/Conference/Submission9510/Reviewer_Q9e8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9510/Reviewer_Q9e8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860689328, "cdate": 1761860689328, "tmdate": 1762921081846, "mdate": 1762921081846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAPLE (Masked Adapter Prototype LEarning), a method to improve OOD generalization when combining multiple LoRA adapters. It is motivated by observing that the current prototype-based routing strategies often yield similar routing entropy for in-distribution and OOD samples, implying poor distinction between the two. The authors provide a theoretical analysis showing that shared noisy subspaces in LoRA prototypes lead to this problem. MAPLE mitigates it by masking the target adapter during prototype learning, forcing the model to refine prototypes that rely on less noisy signals. Empirical results performance gains (~0.5%) over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear motivation, backing theory, and straight-forward mitigation algorithm. The paper connects empirical entropy observations to a formal theoretical framework explaining why simple approaches fail.\n* Simple and practical. MAPLE’s masking approach is straightforward, easy to integrate, and doesn’t require modifying base LLMs or LoRAs."}, "weaknesses": {"value": "* The empirical gains appear modest. Compared to other baselines, the improvements are very small, raising the question of whether they might simply result from differences in their initializations?\n* The theoretical analysis feels somewhat loose and not fully integrated into a cohesive understanding of the problem.\n* The observations seem restricted to text data, limiting the generality of the findings."}, "questions": {"value": "* Figure 1 appears overly stylized and may not reflect real data. Could the authors share the raw observations and clarify how the plot was generated?\n* How sensitive is MAPLE to the selection of top-k?\n* Given the relatively small absolute gains, is it possible that MAPLE’s improvements stem from random initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lVP7zGZecp", "forum": "YxVebk7kIw", "replyto": "YxVebk7kIw", "signatures": ["ICLR.cc/2026/Conference/Submission9510/Reviewer_HVHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9510/Reviewer_HVHh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871693506, "cdate": 1761871693506, "tmdate": 1762921081587, "mdate": 1762921081587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}