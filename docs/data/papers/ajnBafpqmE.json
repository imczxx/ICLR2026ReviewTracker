{"id": "ajnBafpqmE", "number": 1165, "cdate": 1756855730953, "mdate": 1759898224356, "content": {"title": "Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models", "abstract": "In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders.\nWe introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality.\nThis alignment yields semantically rich image tokenizers that benefit diffusion models.\nOn ImageNet 256$\\times$256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.", "tldr": "We propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation.", "keywords": ["Visual Tokenizer", "Latent Diffusion Model", "Foundation Encoder"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f71cdb12eedabcbfcba37898ef59c88a23823c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes replacing the tokenizer VAE in LDM with pretrained visual encoders, specifically DINOv2, and designs a three-stage training framework. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach is simple yet effective which leverages a pretrained DINO model as the encoder and introduces a three-stage training scheme that balances semantic representation and reconstruction quality.\n2. Comprehensive experiments verify the effectiveness of the method."}, "weaknesses": {"value": "1. The motivation, *“Our intuition is that learning semantics is inherently more difficult than learning reconstruction”*,  requires further clarification and supporting evidence.\n\n2. My main concern is that all training and evaluation are conducted on ImageNet without any OOD testing. It is possible that the low gFID results stem from overfitting rather than genuine improvements in generalization, since introducing semantic latent distributions may inherently simplify the latent structure compared to pixel-level ones [1,2].\n\n3. Moreover, the authors use DINOv2 as a frozen encoder, which is pretrained on hundreds of millions of images[3]. This model may already include exposure to datasets similar to ImageNet, making it unclear whether the observed gFID improvement truly arises from the proposed architecture itself, or simply from the pretrained encoder’s prior knowledge of similar data.\n\n4. Although the proposed three-stage training improves reconstruction performance as much as possible, the reconstruction FID still degrades (see Table 5), which limits the applicability of the method as a general-purpose encoder for tasks requiring accurate reconstruction, such as image editing and inpainting.\n\n5. The code and evaluation scripts should be released to ensure reproducibility.\n\n[1] Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models\n\n[2] Masked Autoencoders Are Effective Tokenizers for Diffusion Models\n\n[3] DINOv2: Learning Robust Visual Features without Supervision"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sTLFsCssRi", "forum": "ajnBafpqmE", "replyto": "ajnBafpqmE", "signatures": ["ICLR.cc/2026/Conference/Submission1165/Reviewer_LVhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1165/Reviewer_LVhM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530359531, "cdate": 1761530359531, "tmdate": 1762915694704, "mdate": 1762915694704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Generation models (e.g., Diffusion models, flow matching model) employ an autoencoder to reduce the dimension of the image dataset for computational efficiency and generation ability.\n- The authors claim that the latent space of the encoder should be semantically meaningful.\n- By introducing DINO-v2 initialized encoder and semantic preservation regularization, they achieve faster convergence while training and better fid with the same NFE while inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The author first proposed to directly employ the pretrained vision foundation model, followed by an adaptor as an encoder for training a generative model.\n- The proposed architecture is simple but effective.\n- The experiments in Table 1 support the effects of each configuration. Specifically, in the Semantic preservation loss table, there is a trade-off relationship between the reconstruction and generation performance\n- Compared to the VA-VAE, the proposed method achieves better gFID along various sampling steps and CFG scale. In addition, the proposed latent space makes the generative models converge faster than the latent space of VA-VAE\n- Compared to FLUX VAE, the proposed methods achieve better generation performance qualitatively and quantitatively."}, "weaknesses": {"value": "- The overall training strategy seems heuristic, falling short of scalability. Combining multiple stages into a simple pipeline or providing principled methods to measure when to stop each stage would be helpful for applicability.\n- Why DINO-v2 works better than other foundation models? An analysis or theoretical explanation would be helpful\n- Lack of analysis of the semantics of the latent space.\n- [REPA-E] also incorporates the vision foundation model for training VAE. What is the advantage of the proposed method over the [REPA-E]? \n\n[REPA-E]: REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers"}, "questions": {"value": "- What is the author's thought about the “generative-friendly latent space”? According to the claim of the paper, it seems that preservation of the latent space of DINOv2 is helpful for training generative models. If so, would it be an optimal and promising direction of constructing latent space to only get closer to the DINO space? Are there any other components that should be considered?\n- What  ‘linear probing’ exactly did you use for evaluation, and why? Do all the measurements for linear probing give the same correlation and conclusion?\n- The vision foundation model could be huge and computationally expensive. Can you compare the size of the proposed tokneizer with the others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FDM6twWynB", "forum": "ajnBafpqmE", "replyto": "ajnBafpqmE", "signatures": ["ICLR.cc/2026/Conference/Submission1165/Reviewer_GRL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1165/Reviewer_GRL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969156920, "cdate": 1761969156920, "tmdate": 1762915694406, "mdate": 1762915694406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a three stage framework to transform a pre-trained semantic encoder into an image tokenizer. In the first stage, they employ an adaptor that maps features from the frozen encoder into a lower-dimensional latent space (32 dimensions in their experiments). The adaptor and decoder are jointly trained using a reconstruction loss. In the second stage, the encoder is fine-tuned to improve reconstruction quality. However, since this process can degrade semantic capacity, an L2 regularization term is applied on the low-dimensional latent to keep the updated encoder close to its previous representation. Experiments conducted mainly on ImageNet 256 show a 0.46 improvement in FID when using the LightningDiT framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation, though sometimes not self-contained, is overall clear.\n\nThe empirical performance, evaluated on ImageNet, shows 3.4% improvements on linear probing, on ImageNet 256."}, "weaknesses": {"value": "1. It's not clear what the overall computational cost is for all stages, when compared to other methods.\n\n2. I am not sure how sensitive the dimensionality of the adaptor is. If we want to generalize to ImageNet 512, does it mean that we only need to double the latent?\n\n3. The use of an L2 loss to preserve semantic consistency may not be robust, especially when the latent space lacks proper normalization.\n\n4. While semantic preservation appears improved, the drop in linear probing accuracy during training raises concerns about the trade-off between reconstruction fidelity and semantic capacity.\n\n5. The decrease with rFID shows that the constraint on the adaptor and the stage-wise framework might impact the reconstruction accuracy."}, "questions": {"value": "What's the performance before CFG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "039k1PwHy9", "forum": "ajnBafpqmE", "replyto": "ajnBafpqmE", "signatures": ["ICLR.cc/2026/Conference/Submission1165/Reviewer_svcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1165/Reviewer_svcV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145714262, "cdate": 1762145714262, "tmdate": 1762915694112, "mdate": 1762915694112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new way of image tokenizer training for image generation. It targets the weak semantics of the latent space of traditional VAE tokenizers, which can be caused by the reconstruction-only training supervision. This method builds an image tokenizer based on pre-trained discriminative vision foundation models such as DINO. It introduces a progressive three-stage training method that trains an adaptor and a decoder to reconstruct the images, while maintaining the semantically rich latent space. Experiments on ImageNet and text-to-image generation prove the effectiveness. Extensive ablations justify the design of each training stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper targets an important problem of semantic-rich image tokenizer traning. The solution shown is simple and effective. The idea is well presented and illustrated. \n\n- The extensive ablation and detailed reconstruction / semantic capacity tracking along the training process are very helpful and insightful. \n\n- The final quantivative evaluation results are strong."}, "weaknesses": {"value": "- Not technically a weakness, but the final linear probing performance is only ~35%, dropping by a lot compared to the original DINO latent space. What could be the potential ways of further bridging the gap? \n\n- In the stage 2 training, applying the regularization to the output of $E_p$ instead of $A$ seems to be a more intuitive way and can better preserve the semantics. Could the authors provide some insights on why that does not work well?\n\n- In summary, my main concern is that, although the idea is motivated by leveraging a semantically rich encoder to obtain a more diffusable latent space, the substantial linear-probing performance drop and the observed trade-offs between reconstruction fidelity and semantic preservation make it unclear whether the improved results stem from a genuinely richer semantic space or merely from initializing the tokenizer with a well-pretrained encoder that can also be some other models."}, "questions": {"value": "- I didn't find detailed comparisons for the costs. Will using the pretrained vision encoder introduce increased memory consumption and encoding latency? \n\n- Could the authors provide GenEval score comparisons in Table 5? It will provide more comprehensive evaluations on alignment and better justify the core contribution of this paper. \n\n- Figure 4 right. The baseline VA-VAE fails to train in full training epochs in both setting due to numerical issues. Would it be possible to add another baseline for full-scale comparisons? This is optional as it might require prohibitive time during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QPwz6NKOwm", "forum": "ajnBafpqmE", "replyto": "ajnBafpqmE", "signatures": ["ICLR.cc/2026/Conference/Submission1165/Reviewer_XEQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1165/Reviewer_XEQ8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245703899, "cdate": 1762245703899, "tmdate": 1762915693987, "mdate": 1762915693987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}