{"id": "bLasjdl4dE", "number": 14575, "cdate": 1758239266704, "mdate": 1759897361604, "content": {"title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "abstract": "Although Large Language Models (LLMs) have become  capable  reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs.\nTo address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning.\nSSRMs generate reasoning traces in a *non-executable* Pythonic syntax that names each reasoning step and marks its inputs and outputs. \nThis structure allows SSRM traces to be automatically *audited* to identify reasoning flaws.\nWe evaluate three types of audits: hand-crafted *structured reasoning audits*, written in a domain-specific language (DSL) implemented in Python; LLM-generated *structured reasoning audits*; and learned *typicality audits*, which apply probabilistic models over reasoning traces. \nWe show that all of these methods can be used to effectively flag probable reasoning errors.\nImportantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.", "tldr": "", "keywords": ["LLM", "Generation Auditing", "Verifiability"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8044cf46c66989223f8d78281935ca1ab858225.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical problem of faithfulness and error detection in the reasoning of Large Language Models (LLMs) by proposing Semi-Structured Reasoning Models (SSRMs), which are trained to generate reasoning traces in a non-executable, Python-like syntax that explicitly names each step and its inputs/outputs for programmatic analysis. The core contribution is the introduction of three distinct auditing methods enabled by this structure: hand-crafted audits (DSL-based unit tests), LLM-generated audits (automating test creation), and typicality audits (a novel statistical approach flagging atypical reasoning step sequences). The authors provide strong empirical validation, primarily on a new MedCalcV2 benchmark, demonstrating that these audits effectively flag reasoning flaws that correlate with incorrect answers, and that training for this structured output (SSRM) does not compromise, but in fact improves overall task accuracy compared to identically-trained unstructured baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The SSRM approach can avoid the rigidity of fully executable code (Program-of-Thought) but provides far more verifiability than unstructured free-text (Chain-of-Thought). The idea to parse the trace into a DataFrame and run unit-test-like audits is highly practical and interpretable.\n2. The paper explores three different ways to perform audits, addressing rigor (hand-crafted), scalability (LLM-generated), and statistical novelty (typicality audits). The typicality audit, in particular, is a novel contribution for reasoning verification."}, "weaknesses": {"value": "1. It is not clear to understand the correlation between %failed and Failing/Passing of the audits. Why sometimes passing is higher than failing with high delta, but we see %failed is also high?\n\n2. The hand-crafted and LLM-generated audits seem highly effective for MedCalc, which is an inherently structured, rule-based task. It is less clear how this approach would scale to other reasoning tasks where the \"correct\" reasoning steps are not as clearly defined (in table 6). The \"partial program\" (the set of available functions) seems to require significant manual, task-specific design.\n\n3. The paper mentions in Appendix E.5 and Table 11 that the SSRM traces are significantly longer than unstructured CoT traces (e.g., 4-5x more tokens for MedCalcV2). This implies a substantial increase in inference cost (time, memory, and computation) which is a major practical drawback, but this point is somewhat buried in the appendix."}, "questions": {"value": "- Questions:\n  - Page 6, Table 1 and 2: do the P values without * mean the result is not significant? \n  - The manual effort to define the \"partial program\" (the vocabulary of reasoning functions like analyze_input, convert_units, evaluate_rule) seems like a potential bottleneck. How much effort was this for the MedCalcV2 task, and how do you see this step being managed for new, unseen domains? Do you have example how this can be generalized to non-MedCalcV2 task?\n- Typos:\n  - Page 6, Table 2: \"sstep 2 output feeds into step 3 input\" -> \"step 2...\" (double 's')."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FvHAGIlRzb", "forum": "bLasjdl4dE", "replyto": "bLasjdl4dE", "signatures": ["ICLR.cc/2026/Conference/Submission14575/Reviewer_oGJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14575/Reviewer_oGJj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864528973, "cdate": 1761864528973, "tmdate": 1762924963021, "mdate": 1762924963021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Semi-Structured Reasoning Models (SSRMs), a method aimed at addressing the problem of faithfulness in LLM reasoning. The core idea is to train LLMs to produce reasoning traces in a semi-structured, non-executable Pythonic syntax. This structure, unlike free-form text, allows for automated audits to detect reasoning flaws. The authors propose and evaluate three auditing methods: hand-crafted audits, LLM-generated audits, and probabilistic typicality audits. They demonstrate that SSRMs can be effectively audited for errors while maintaining strong performance on several reasoning benchmarks compared to models of similar size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the highly significant and timely problem of ensuring the faithfulness of LLM reasoning. As LLMs are increasingly deployed in high-stakes domains, developing methods for auditing their reasoning processes is of critical importance, and this work makes a valuable contribution in this direction.\n- The central idea of leveraging semi-structured representations for automated auditing is both novel and elegant. It offers a promising paradigm for moving beyond opaque, free-form text outputs towards more scrutable and verifiable reasoning chains.\n- The authors provide compelling empirical evidence that their method does not compromise reasoning performance. Showing that the added structure for auditability comes at no cost to accuracy on a range of benchmarks is a strong and important result."}, "weaknesses": {"value": "- The paper's clarity could be significantly improved, particularly concerning the methodological details. As it stands, some aspects of the implementation are challenging to fully understand, which may hinder reproducibility.\n- The motivation behind choosing a \"Pythonic syntax\" would benefit from a more thorough discussion. Providing a comparison with alternative formalisms and explaining the trade-offs would help justify this specific design choice.\n- The paper builds heavily on Program Trace Prompting (PTP) but would be more self-contained with a more detailed explanation of this prior work. This would help readers better appreciate the novel contributions of the current paper.\n- The process for generating the semi-structured training data could be described in more detail. Clarifying how the reasoning traces are created and validated would strengthen the paper's claims.\n- More information on the creation of function \"stubs\" would be helpful. Understanding whether these are pre-defined or model-generated, and how their quality is assured, is important for evaluating the auditing framework.\n- The description of the audit creation process could be expanded. Elucidating how this process generalizes beyond the provided examples would bolster confidence in the method's broad applicability.\n\nWhile the paper's central claim is compelling, the description of the reasoning generation and auditing process could be further developed. Answering some of the open questions about the framework—such as the generation of function stubs and training data—would substantiate the claims of rigorous auditability and strengthen the overall contribution."}, "questions": {"value": "1.  Could you elaborate on the motivation for using a \"Pythonic syntax\"? The paper would be strengthened by a discussion of the alternatives that were considered and a clearer justification for this design choice.\n2.  To improve the paper's self-containedness, could you provide a more detailed explanation of Program Trace Prompting (PTP) and clarify your specific modifications and extensions?\n3.  Could you clarify the generation process for function stubs? It would be helpful to understand if they are manually defined or generated by an LLM, and how their quality and appropriateness for a given task are ensured.\n4.  The paper would benefit from more details on the training data generation process. Could you elaborate on how semi-structured traces are created from existing datasets and what steps are taken to ensure their correctness?\n5.  Could you expand on the audit creation process? Clarification on whether audits are generic or task-specific, and what the \"minimal guidance\" for LLM-generated audits entails, would be valuable for the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4eoo3nNBT", "forum": "bLasjdl4dE", "replyto": "bLasjdl4dE", "signatures": ["ICLR.cc/2026/Conference/Submission14575/Reviewer_KMTn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14575/Reviewer_KMTn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896547807, "cdate": 1761896547807, "tmdate": 1762924962535, "mdate": 1762924962535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to train semi-structured reasoning models (SSRMs), reasoning language models that output reasoning traces in a structured, Python-like format, whereas execution is done primarily via a language model itself. The main advantage is that the structure enables users to do \"audits\" of the reasoning process automatically -- applying rules that can verify certain properties of the reasoning process, rather than just the outcome. The authors propose a pipeline to train these models by inferring semi-structured traces from existing CoT traces. Experiments on domains ranging from bigbench-hard tasks, MATH and a medical domain show that SSRMs can be audited via hand-crafted and learned (typicality) rules."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a significant, well-motivated problem of how to test the validity of LLM reasoning beyond looking at their final answer. The general idea of proposing a loose structure makes sense, and it not being fully symbolic gives it flexibility to work across domains.\n\nWhile this has broadly been explored, the idea of allowing users to write programmatic audits is novel as far as I'm aware."}, "weaknesses": {"value": "The paper lacks important details about most of the method. Central to SSRMs is the format that is enforced, but the format itself is only vaguely described (there's the example in Figure 1, but the format is barely mentioned in Section 3). I'm confused by the fact that the representation is a Pandas DataFrame, since programs are hierarchical (and even if the trace is just linear, each function call has a variable number of arguments, which I would assume map to columns in the data frame). Thus, I don't really understand what is the structure that the paper is proposing.\n\nBroadly, the idea of structuring chain-of-thought reasoning has been explored before (e.g., see Natural Programs [1], from NeurIPS '23). Thus, this is not really new. The new angle seems to be the programmatic audits. But understanding what kinds of audits are easy/hard to specify crucially requires seeing the format described in more detail. For instance, Table 1 mentions this \"solve formula math is correct\" audit -- obviously checking math in general is extremely hard, and the paper only mentions that it \"uses Python’s eval function\". Thus, I'm also unclear on how these audits are implemented, and what range of interesting errors do they indeed end up catching.\n\nFor the evaluation, the main results are about the audits themselves. The audits in Tables 1, 2 and 4 seem very ad hoc -- why these specifically? It seems like they were written by the authors themselves. It would be more reassuring if existing audits that we know people are already interested in could be demonstrated in your framework. Otherwise, it's hard to interpret the results. At the very least, the paper should show interesting examples of errors caught by these audits and why they are important to detect.\n\nThus, since the focus is on the ability to audit reasoning, I believe the paper should (1) describe the format in much more detail, (2) either find well-motivated audits from somewhere else, or show more evidence that the author's hand-crafted (or typicality) audits can capture interesting and important errors in LLM reasoning across the benchmarks.\n\n[1] https://arxiv.org/pdf/2306.03872"}, "questions": {"value": "- Is the format flat, or is it hierarchical? If it's the latter, how does hierarchy get represented in the data frame?\n- In Supervised Fine-Tuning (Section 3), the paper mentions that traces are extracted from existing datasets, but only \"aces that yield correct final answer are retained\". How is this check implemented, since you cannot just execute the traces using Python?\n- What rate of structured traces were discarded by this rule?\n- Does your full pipeline include SFT and GRPO? What fraction of compute is allocated to both? What's the delta from just doing SFT?\n- Are there good examples of interesting errors your audits catch in MedCalcV2, where auditing seems to be the most important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5P6SMKMVF", "forum": "bLasjdl4dE", "replyto": "bLasjdl4dE", "signatures": ["ICLR.cc/2026/Conference/Submission14575/Reviewer_sE1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14575/Reviewer_sE1b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945550066, "cdate": 1761945550066, "tmdate": 1762924961966, "mdate": 1762924961966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces semi structured reasoning as a way to audit the reasoning process of llms. The semi structured reasoning allows the authors to audit the reasoning in 3 ways: human audits, automated llm audits, and probabilistic audits based on reasoning patterns. The authors show how lllms trained to produce reasoning in this DSL can perform well with SFT and RL on medcalc bench while also producing auditable reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is clearly motivated and well presented.\n- I particularly liked the typicality audits based on reasoning patterns\n- The results on MedCalc are quite comprehensive"}, "weaknesses": {"value": "- My main concern: Why not generate certifiably correct reasoning? ****If the reasoning process follows a DSL, why not constrain generation to produce provably valid traces by construction (similar to Poesia et al.'s certified reasoning with LLMs)? The paper audits **after** generation, but doesn't explain why generation-time constraints aren't preferable. This would eliminate many errors rather than just detecting them.\n\nCertified Deductive Reasoning with Language Models (https://arxiv.org/abs/2306.04031)\n- There might be some circularity: The audits are generated with a stronger llm. How do we ensure that the audits themselves are correct?\n- The results in table 6 are a little difficult to parse. It would be great if the table could split up into tables or figures to make different points more clear.\n- 363 apply apply → apply both (?)\n- **analysis of when audits fail:** The paper shows correlations between audit failures and errors but it would be great to see some more analysis:\n    - FP rates\n    - FN rates\n    - What types of reasoning errors evade all three audit types"}, "questions": {"value": "Have you experimented with constrained generation to enforce valid DSL syntax during sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y2GHfAFi35", "forum": "bLasjdl4dE", "replyto": "bLasjdl4dE", "signatures": ["ICLR.cc/2026/Conference/Submission14575/Reviewer_KRyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14575/Reviewer_KRyG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949594862, "cdate": 1761949594862, "tmdate": 1762924961595, "mdate": 1762924961595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}