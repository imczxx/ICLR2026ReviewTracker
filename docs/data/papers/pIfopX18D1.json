{"id": "pIfopX18D1", "number": 23156, "cdate": 1758340313066, "mdate": 1759896829886, "content": {"title": "Item Response Scaling Laws: A Measurement Theory Approach to Generalizable Neural Performance Prediction", "abstract": "Classical neural scaling laws describe how the performance of large language models (LLMs) improves with increased compute, but they are typically estimated in aggregate across all questions in a benchmark, overlooking the information carried by individual questions. Item Response Theory (IRT) offers a principled way to address this by modeling per-question characteristics, though traditional IRT is limited to binary data with a Bernoulli loss. In pre-training downstream scaling, probabilities of producing the correct answer over the entire vocabulary yield more informative laws, while in test-time scaling, repeated sampling naturally gives rise to empirical probabilities. Empirical probability responses do not arise in human testing or LLM leaderboard evaluations, settings where IRT has shown success. To bridge this gap, we propose extending IRT with a Beta loss on empirical probability responses, naturally yielding Item Response Scaling Laws. We validate our framework in two large-scale studies: (1) pre-training downstream scaling, using 25 models from 6 families with up to 359 checkpoints on 15 NLP datasets; and (2) test-time scaling, using 15 models on 10 NLP datasets with up to 10,000 samples per question. In both cases, IRT-based approaches provide reliable and efficient estimates of scaling behavior while remaining interpretable and generalizable.", "tldr": "We propose Item Response Scaling Laws, yielding efficiently estimated, interpretable, and generalizable scaling laws validated on large-scale pre-training and test-time studies.", "keywords": ["Item Response Theory", "scaling law", "LLM evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/991d4c65bb3ef2d3d2dfadb6aaea5d30afa04d81.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Item Response Scaling Law, an Item Response Theory (IRT)-based method for generalized performance prediction of neural networks. The proposed method evaluates models on a small set of samples (easy ones) to estimate the model parameters, such as LLM ability and question difficulty. Then, it predicts model performance on the second set of samples (hard ones). Experimental results demonstrate IRT's potential to serve as an interpretable and generalizable tool for predicting model performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using IRT is interesting and may have high potential in model performance prediction.\n2. The main idea is clear."}, "weaknesses": {"value": "1. This paper is hard to follow. The section and paraphrase arrangement can be improved:\n\n    (1) Contents in Section 3.1, where you introduce IRT, and contents in Section 2, where you introduce the prediction formulation of test-time scaling, should move to Section 2. \n\n    (2) The 2nd and 3rd paragraphs of Section1 are a bit ambiguous. I assume the 2nd is about experimental scale; the 3rd is too poorly structured, so I don't understand the main argument. I suggest you rewrite the two sections by explicitly listing down ((1), (2),...) challenges. In the 4th paragraph, you can thus make it clearer on why IRT addresses them (e.g., we address issue (1) by ...; (2) by...).\n\n2. Given that your method builds on the Rasch model, it will be clearer to introduce its formulation in Section 2. You may also argue why other IRT models, like 3PL, were not chosen.\n\n3. Line 177 seems to lack \",\" between two equations, and the subscript \"new\" in lines 177-178 is redundant.\n\n4. In line 213, you write p^Vocab (Correct Choice); but from line 215 it becomes p_{Correct Choice}.\n\n5. In lines 255 and 263, I don't recommend using subscripts \"1\" and \"2\" to split between two sets of responses. \"1\" and \"2\" do not convey information and do not clearly differentiate between the two sets.\n\n6. In Figure 3, the y-axis label overlaps with the legend.\n\n7. I don't recommend explaining Figures in the appendix (Figures 7~9) as the main paper's analysis. The main paper should be self-contained. Reviewers, though encouraged, are not obliged to review the appendix."}, "questions": {"value": "1. [a] measures the difficulty of each question (later splitting them into different question groups based on difficulty) using the Brier score instead of IRT. I believe IRT has a more fine-grained depiction of difficulty, and thus it might be worth discussing as a related work.\n\n2. In the 2nd paragraph of Section 1, you mention that previous work struggles at the experimental scale—How does IRT address this issue? My understanding is that IRT addresses the issue of granularity instead of scale.\n\n3. In Figure 2's subset-based prediction, how did you do subset sampling and plot the final prediction line? If I didn't misunderstand, the reason why the prediction line matches the full set's curve is simply because the sample mean is an unbiased estimator. For example, if you have 10 samples [a, b,..., i, j] and at each time you uniformly sample 3 of them to calculate the average. Doing this 100 times leads to a number close to the average of [a, b,..., i, j].\n\n4. I am confused about Figures 1 and 3. Why is the right y-axis IRT theta? Isn't it a fixed estimated value? How to compare it with the left axis (Mean score)? Left axis is the Mean score of what?\n\n5. What are baseline methods for Beta-IRT to compare with?\n\na. [U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models](https://openreview.net/forum?id=jjfve2gIXe)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q7mKss4FtC", "forum": "pIfopX18D1", "replyto": "pIfopX18D1", "signatures": ["ICLR.cc/2026/Conference/Submission23156/Reviewer_dyjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23156/Reviewer_dyjt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197246507, "cdate": 1761197246507, "tmdate": 1762942537072, "mdate": 1762942537072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation of classical neural scaling laws for large language models (LLMs): their reliance on aggregate metrics (e.g., mean accuracy) across benchmark questions, which overlooks individual question characteristics and leads to poor generalizability and interpretability. To solve this, the authors propose Item Response Scaling Laws (IRSL), an extension of Item Response Theory (IRT) from psychology/educational testing to model LLM performance scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "IRSL establishes a new paradigm for scaling laws—rooted in measurement theory—instead of empirical curve-fitting. This provides a theoretical basis for generalizability, which classical laws lack."}, "weaknesses": {"value": "The Beta loss uses a precision parameter $\\(\\phi\\)$, but the paper does not explain how $\\(\\phi\\)$ is selected (e.g., cross-validation, fixed value). Sensitivity to $\\(\\phi\\)$ is untested, which raises questions about whether $\\(\\phi\\)$ choices drive results."}, "questions": {"value": "1. You filtered questions with pass@1 ≤0.01 to mitigate IRT’s zero-probability limitation. Why 0.01 specifically? Did you test other thresholds (e.g., 0.005, 0.02) and, if so, how did they affect MAE and the number of retained questions? Is there a theoretical basis (e.g., IRT discriminatory power) for choosing this threshold?\n2. How did you select the value of $\\(\\phi\\)$ for the Beta loss? Did you perform a sensitivity analysis to test how different $\\(\\phi\\)$ (e.g., 0.01, 1, 100) impact calibration accuracy (Spearman $\\(\\rho\\)$) and scaling law fit (MAE)? If $\\(\\phi\\)$ is optimized via cross-validation, what was the validation procedure (e.g., leave-one-dataset-out)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7okgwsiSKU", "forum": "pIfopX18D1", "replyto": "pIfopX18D1", "signatures": ["ICLR.cc/2026/Conference/Submission23156/Reviewer_rLDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23156/Reviewer_rLDS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974421762, "cdate": 1761974421762, "tmdate": 1762942536865, "mdate": 1762942536865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Item Response Theory (IRT) scaling law, which models per-question difficulty and per-LLM ability, to predict downstream performance. It conducts two main experiments, one on pre-training downstream scaling and another on test-time scaling. The authors conclude from both experiments that IRT scaling laws are a useful framework to predict downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation and theory part is written well. The introduction of Item Response Theory to scaling law is natural and novel. I agree that modelling per-question characteristics is an interesting next-step that deepens our understanding of the scaling law."}, "weaknesses": {"value": "W1: The organization and presentation of the experiment part can be strengthen. Upon reading it multiple times, I feel it is challenging for me (and potentially other readers as well) to grasp (1) does the method work well, (2) how does it compare to and whether it improves upon existing methods. I feel many experiments lack a clear take-away and I find it challenging to grasp what is the main finding/result for every experiment.\n\nW2. The authors should strengthen the presentation of evidence regarding  effectiveness of their method. In several experiments, the authors conclude effectiveness from comparing IRT's curve and the ground truth and saying there is a good match, for example Figure 1, Figure 4, and Figure 5. But this seems subjective and readers might not agree (for example, there are mismatches in Figure 4 and 5). Could the authors present more objective criteria for effectiveness of their method?\n\nW3. It would be very helpful if the authors could include more baseline comparisons. In most experiments, the author direct compare their method to ground truth and with some level of match, the authors argue their method predicts well. It lacks a situational context for readers to interpret such level of match. Presenting more baseline comparisons would help solve W2 as well.\n\nDirection for improvement: I feel it would strengthen the paper greatly if the authors could go beyond analysis and demonstrate downstream application of IRT scaling laws as well. Does it give new insights on scaling LLMs and could the authors test it?"}, "questions": {"value": "1. Line 298: The authors said \"As shown in Figure 1, IRT [...] closely tracks the ground-truth curve\". I could not understand how it is supported from Figure 1. binary-IRT (red) reports CAT \\theta and full set Acc is the ground-truth. These two are two different metrics. How to support the finding that \"IRTlosely tracks the ground-truth curve\"? I would encourage the authors to clarify in paper as well.\n\n2. In line 268 ~ 307 and Figure 1, could the authors compare to more baselines, such as the Classical linear approximation model, Equation 2 in paper. As the \"random subset mean\" baseline does not leverage any information.\n\n3. I wonder why in Figure 1, experiments are conducted in a much narrower FLOP range, not on a logarithm scale such as in Figure 2?\n\n4. Figure 3 has a similar problem, that while the authors said \"Beta-IRT effectively captures pCorrect Choice, which is more predictable and less emergent\", Beta-IRT and average pCorrect Choice report two different metrics and I am not sure how the figure supports this conclusion. \n\n5. From Figure 3, it appears that subset p_vocab is the mostly close with the fullest p_vocab, which is the ground truth. Can I interpret that as subset p_vocab gives better productivity than the proposed subset beta-IRT p_vocab?\n\n6. I have some questions over the selection of models & datasets, which the authors are encouraged to justify. For example, model sizes are much narrower in Figure 1; a specific subset of MMLU, MMLU anatomy, is used in Figure 3;\n\n7. I think Figure 1 & Figure 4 are better plotted as a scatter plot rather than as a pure line plot, as the most important aspect is the points representing models.\n\n8. The train-test split in the paragraph from Line 343 to 365 is decided by the calibrated difficulties, which themselves are a factor in the IRT model. I have concerns that this split might be problematic. Could the authors justify or consider using an external factor that split the dataset into an easy and difficult part?\n\n9. I encourage the authors to include the classic scaling law as a baseline comparison in the paragraph from line 343 to 365, and Figure 4, to provide evidence that IRT is superior. \n\n10. In Figure 4, isn't that the Test IRT became systematically higher than Test GT, which seems contradicting line 354 \"As shown in Figure 4, the IRT model closely matches the ground truth curve on both the training and test sets\"? The authors are encouraged to discuss this.\n\n11. I encourage the authors to include other methods and compare in Figure 5 experiment.\n\n12. I'm less sure about how to interpret the Spearman correlation reported in line 421, and whether \"IRT-predicted probabilities correlate closely with pass@1\" (line 422). Visually in Figure 5, it is also questionable to me IRT probabilities predict pass@1 well. While I understand that scaling law might inherently be a challenging task to predict, the authors should present more context & baselines to aid readers in understanding their method's improvement and superiority. The experiment shown in Figure 6 has the same problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "84GaLV3COS", "forum": "pIfopX18D1", "replyto": "pIfopX18D1", "signatures": ["ICLR.cc/2026/Conference/Submission23156/Reviewer_uUtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23156/Reviewer_uUtS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987129960, "cdate": 1761987129960, "tmdate": 1762942536384, "mdate": 1762942536384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Item Response Scaling Laws (IRSL), a framework that recasts neural scaling laws within the formalism of Item Response Theory (IRT) from psychometrics.\nInstead of modeling dataset-level performance (e.g., accuracy vs. FLOPs), the authors treat each model as a “test taker” and each benchmark question as an “item” with latent difficulty $z_j$.\nModel ability \\theta_i and item difficulty $z_j$ are jointly modeled through a Rasch or Beta-IRT formulation, where $p(y_{ij}=1) = \\sigma(\\theta_i - z_j)$.\nThis allows the authors to interpret scaling behavior in terms of latent ability estimation, leading to smoother, more stable scaling curves and improved sample efficiency via Computerized Adaptive Testing (CAT).\nExtensive experiments across pre-training and test-time scaling settings show that IRSL provides smoother, lower-variance trends compared to traditional aggregate fitting, and the latent ability \\theta generalizes across question subsets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work introduces a theoretically principled bridge between scaling laws and measurement theory.\nFraming scaling as the interaction between model ability and item difficulty is conceptually elegant and potentially influential for future interpretability-driven scaling research.\n- The paper evaluates both pre-training downstream scaling and test-time scaling, covering multiple model families, FLOP ranges, and benchmarks. The results (e.g., Figs. 1–4, 6–10) consistently demonstrate that IRT-based estimation yields smoother curves and reduced variance (e.g., 33% TV reduction in Fig. 1).\n- The latent parameters \\theta and z provide an interpretable decomposition of model performance that could, in principle, generalize across benchmarks."}, "weaknesses": {"value": "1. Based on my understanding, the framework critically relies on calibrated item difficulties $z_j$, either imported from prior leaderboards (e.g., HELM, LM-Eval) or newly estimated using model response data through Beta loss.\nThis reliance means IRSL cannot operate in a truly predictive regime: for a new benchmark without prior model responses, the difficulty parameters are unidentifiable, and thus the scaling curve cannot be extrapolated.\nIn other words, IRSL functions as a measurement and efficiency framework rather than a predictive scaling law.\nThe paper’s claim of “generalizable prediction” holds only when item-level calibration is already available, not when facing unseen tasks, which is a key difference from conventional scaling laws that aim for zero-shot extrapolation.\nEmpirical discussion of cross-benchmark transfer (Fig. 8) supports this concern, showing that calibrated difficulties are not stable across decoding setups.\n2. Modeling the empirical probabilities with a Beta likelihood implicitly assumes unimodal, symmetric response uncertainty.\nIn practice, LLM confidence distributions can be heavy-tailed or multi-modal due to sampling temperature, decoding randomness, or calibration bias.\nThe robustness of this assumption is not analyzed."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FCle7T7nW1", "forum": "pIfopX18D1", "replyto": "pIfopX18D1", "signatures": ["ICLR.cc/2026/Conference/Submission23156/Reviewer_nMND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23156/Reviewer_nMND"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130932502, "cdate": 1762130932502, "tmdate": 1762942536102, "mdate": 1762942536102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}