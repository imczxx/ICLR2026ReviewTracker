{"id": "80oiymEbaO", "number": 21312, "cdate": 1758316158280, "mdate": 1759896929147, "content": {"title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems", "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 163 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective robustness score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.", "tldr": "", "keywords": ["LLM-based agents", "LLM-based multi-agent systems", "Robustness", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c3526a7cb3b0b0c011ba0345a2f160755cb9ec7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TAMAS, the first benchmark for evaluating the safety and robustness of multi-agent LLM systems. It argues that existing benchmarks focus only on single agents, failing to capture unique risks from multi-agent collaboration. TAMAS tests systems against six attack types (including prompt injections, impersonation, and multi-agent-specific attacks like colluding or contradicting agents) across five high-stakes domains. The key finding is that multi-agent systems are \"highly vulnerable\" to these attacks, revealing \"new, systemic risks\" not seen in single-agent setups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces TAMAS, the first benchmark to systematically evaluate the safety of multi-agent LLM systems. Its key innovation is defining and testing \"multi-agent-specific risks\" (like Byzantine, Colluding, and Contradicting agents) , which \"have no analog in single-agent setups\".\n\n2. The work is methodologically rigorous. The TAMAS benchmark is comprehensive, spanning 300 adversarial instances across five domains and six attack types. The evaluation is thorough, testing 10 LLM backbones across three agent configurations and two frameworks. The introduction of the Effective Robustness Score (ERS) provides a thoughtful metric for the safety-utility trade-off.\n\n3. The paper is well-written and clearly motivated, identifying that multi-agent safety is \"largely under-explored\". The attack framework is clearly explained with text and an excellent overview figure (Figure 1). The results are presented accessibly in tables and figures, with \"Illustrative Cases\" in the appendix making the risks concrete."}, "weaknesses": {"value": "The core weakness is the paper's focus on demonstrating failure without providing actionable steps for mitigation or a deep root cause analysis.\n\n* Missing Defenses: The paper does not test the effectiveness of simple, common defenses, like providing agents with explicit refusal instructions (safety guardrails) in their prompts, which limits its practical use.\n\n* Shallow Analysis: It needs a deeper root cause analysis to distinguish whether failures are due to: Model-Level Compliance (LLM ignoring safety training) and Orchestration Failure (the multi-agent framework logic breaking down).\n\n* Scalability Issue: The reliance on human evaluation for labeling the outcomes is slow and unscalable. Reporting Inter-Rater Reliability (IRR) scores and validating against an LLM-as-a-Judge method are needed for community adoption."}, "questions": {"value": "Please refer to the weaknesses part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "74SwZR1lwh", "forum": "80oiymEbaO", "replyto": "80oiymEbaO", "signatures": ["ICLR.cc/2026/Conference/Submission21312/Reviewer_nmYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21312/Reviewer_nmYQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997424961, "cdate": 1761997424961, "tmdate": 1762941688859, "mdate": 1762941688859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the TAMAS benchmark to evaluate the safety of multi-agent system against adversarial attacks. The benchmark tests systems across 5 scenarios with 300 adversarial instances across six attack types. By evaluating ten different LLMs across three agent interaction architectures, the authors demonstrate that current multi-agent systems are highly vulnerable to these attacks, introducing a new metric called the Effective Robustness Score (ERS) to measure the trade-off between safety and task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality and Significance: to the best of my knowledge, this is  the first benchmark to evaluate the safety and robustness of multi-agent LLM systems, especially for >= 3 agents. Also, some attacks specified for MAS, such as Byzantine, Colluding, and Contradicting are also tested.  The topic is also an important topic that the community would be interested in, as it addresses under-explored and systemic risks of MAS.\n\n2. The benchmark provides extensive evaluation, spanning five domains, 300 adversarial instances, 100 harmless tasks and three distinct agent interaction configurations. The analysis as well as the proposed metric provides a nuanced understanding of how architectural choices impact system safety, and can benefit follow up studies."}, "weaknesses": {"value": "Weakness 1: Limited Scope of Adversarial Goals (Disruption vs. Misuse)\n\nA limitation of the benchmark is its focus on attacks that disrupt a given task (e.g., Byzantine, Contradicting agents) or manipulate the immediate output (e.g., prompt injection), rather than testing for more severe, exploitative misuse. The safety community is increasingly concerned with threat actors instrumentalizing systems for inherently harmful, multi-step goals. The current benchmark does not appear to evaluate scenarios such as 1) using a multi-agent system (e.g., with a code agent) to collaboratively generate malware or find exploits. 2) attack a system to post or modifying harmful content on social media. 3) exploiting agent-to-tool interactions to exfiltrate private data. While derailing a task is a valid security concern, these \"malicious use\" scenarios represent a higher-stakes, more complex class of threats. \n\nWeakness 2: Missing of Defense methods\n\nThe paper's central finding is the high vulnerability of multi-agent systems, but this conclusion is drawn in the absence of any standard defensive measures. Many of the tested attacks, particularly prompt injections, are well-known vulnerabilities, and it is common practice to deploy LLM systems with system-level safety filters, input/output sanitization, or \"guardrail\" models (such as Prompt-Guard). By evaluating the systems in an undefended \"vacuum,\" the paper's results, while striking, may be overstated. The lack of a defensive baseline makes it difficult to assess the benchmark's true, practical difficulty. \n\nOverall, I like the paper and did not find any serious weakness. However, evaluating a benchmark paper is more difficult and I may adjust my review after reading other reviewers comments."}, "questions": {"value": "1. In Equation [4-7], what are the optimized parts to maximize the goal. Are they $\\delta_i$? If yes, how to optimize them?\n\n2. When testing the Qwen3 model, is the thinking mode on or off?\n\n2. Would like to see the performance on Claude if resources permits. Claude series should be the safest LLMs for both non-agent and agent use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P23ouCXtya", "forum": "80oiymEbaO", "replyto": "80oiymEbaO", "signatures": ["ICLR.cc/2026/Conference/Submission21312/Reviewer_okgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21312/Reviewer_okgF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241512693, "cdate": 1762241512693, "tmdate": 1762941688615, "mdate": 1762941688615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on vulnerabilities of multi-agent dynamics and co-ordination, propose a benchmark Threats and Attacks in Multi-Agent Systems (TAMAS), including 5 scenarios, comprising 300 adversarial instances across six attack types and 163 tools, along with\n100 harmless tasks. And evaluated 10 models, and 3 MAS frameworks from Autogen and CrewAI. They also propose an Effective robustness score (ERS) for assessing the tradeoff between safety and effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1, The topic of assessing multi-agent system safety is timely and important.\n\n2, The benchmark includes multiple tasks, multiple constructed prompts, and the corresponding metric. And the evaluation includes multiple agentic structures."}, "weaknesses": {"value": "1, Lack of comparison with other agent safey benchmarks [1,2,3], what's the difference and main contribution compared to these benchmarks?\n\n[1] AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents.\n\n[2] Agent-SafetyBench: Evaluating the Safety of LLM Agents.\n\n[3] Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents\n\n2, Lack of attack scenarios. For example, the MAS jailbreak [4], malicious coding behavior [5], and agent conversation behavior [6] are less considered.\n\n[4] Infecting llm agents via generalizable adversarial attack.\n\n[5] Multi-agent systems execute arbitrary malicious code.\n\n[6] Red-teaming llm multi-agent systems via communication attacks."}, "questions": {"value": "1, As described in Appendix C.4, the ERS metric is defined as the harmonic mean of the safety score and performance score, which is a reasonable approach to balance these two objectives. However, the paper would benefit from additional experimental validation of this design choice. Specifically, it would be valuable to see ablation studies or comparative analyses demonstrating that the ERS metric more effectively captures the trade-off between safety and performance compared to alternative formulations that also consider both factors.\n\n2, The current safety evaluation relies on ARIA and LLM-as-a-judge as criteria. While this is a common and practical setup, in certain webpage attack scenarios, there exist more direct and reliable criteria — for instance, whether the agent executes a specific unsafe action (e.g., transferring money to a malicious actor). The paper does not discuss why such criteria were not considered, nor does it provide results using these more concrete safety indicators. Including such analyses could strengthen the validity and interpretability of the safety evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AElGUTYkoq", "forum": "80oiymEbaO", "replyto": "80oiymEbaO", "signatures": ["ICLR.cc/2026/Conference/Submission21312/Reviewer_EZRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21312/Reviewer_EZRz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283983811, "cdate": 1762283983811, "tmdate": 1762941688100, "mdate": 1762941688100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TAMAS, a benchmark designed to evaluate the adversarial robustness and safety of multi-agent LLM systems. The authors define a threat model encompassing six attack types, including prompt-level injections and attacks described as multi-agent-specific: Byzantine, Colluding, and Contradicting agents. The benchmark dataset spans five domains and is used to evaluate ten different LLM backbones across three distinct agent interaction configurations derived from the Autogen and CrewAI frameworks. The principal finding of the study is that current multi-agent LLM systems exhibit significant vulnerabilities to these adversarial attacks, often failing to refuse explicitly malicious instructions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is interesting. \n- Its studied problem of adversarial vulerabilities in multi-agent systems is interesting."}, "weaknesses": {"value": "- The paper's claims to originality are overstated, it fails to properly clarift what is fundamentally new about their evaluation of adversarial attacks compared against the attacks under the LLM or single agent context. \n- The tasks and tools are partly synthetic, so it is unclear how well results transfer to real systems with live APIs and true side effects. \n- The quality of the benchmark execution is also questionable; the dataset of 300 adversarial instances seems small for the scope of the claims. \n- The safety judging leans on LLMs, yet the paper does not report agreement with humans or how consistent the labels are. \n- The combined score is not fully defined and lacks sensitivity checks, and the tables do not report uncertainty or repeat runs."}, "questions": {"value": "- What is the exact formula for your combined score (including weights and any normalization)? \n- How does that score change if the mix of benign vs. attack tasks changes, or if you use a different threshold for “attack success”?\n- What is the agreement rate between humans and the LLM judge on safety labels, and how did you resolve any conflicts? \n- How stable are the results across different random seeds, decoding settings (e.g., temperature), and prompt or role templates, and can you report confidence intervals from repeat runs? \n- Can you add defenses suited for multi-agent systems (message checks, permission gates, or consensus filters) and show their effect on both safety and normal task success? \n- How do attack success rates change when you vary team size and the communication graph, and when you use another agent framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KLu5GVXV8Z", "forum": "80oiymEbaO", "replyto": "80oiymEbaO", "signatures": ["ICLR.cc/2026/Conference/Submission21312/Reviewer_4dV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21312/Reviewer_4dV7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762298359850, "cdate": 1762298359850, "tmdate": 1762941687452, "mdate": 1762941687452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}