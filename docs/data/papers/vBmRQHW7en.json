{"id": "vBmRQHW7en", "number": 18384, "cdate": 1758287072081, "mdate": 1759897107195, "content": {"title": "Sampling-aware Adversarial Attacks Against Large Language Models", "abstract": "To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point greedy generations, overlooking the inherently stochastic nature of LLMs and overestimating robustness. We show that for the goal of eliciting harmful responses, repeated sampling of model outputs during the attack complements prompt optimization and serves as a strong and efficient attack vector. By casting attacks as a resource allocation problem between optimization and sampling, we determine compute-optimal trade-offs and show that integrating sampling into existing attacks boosts success rates by up to 37\\% and improves efficiency by up to two orders of magnitude. We further analyze how distributions of output harmfulness evolve during an adversarial attack, discovering that many common optimization strategies have little effect on output harmfulness. Finally, we introduce a label-free proof-of-concept objective based on entropy maximization, demonstrating how our sampling-aware perspective enables new optimization targets. Overall, our findings establish the importance of sampling in attacks to accurately assess and strengthen LLM safety at scale.", "tldr": "We make adversarial attacks more efficient and effective by integrating sampling as an attack vector.", "keywords": ["llms", "adversarial attacks", "jailbreak", "sampling", "efficiency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e63babe223b6a5778f7233664abead3eb2aad42.pdf", "supplementary_material": "/attachment/a48ea4c6c2ebd9ff2f71978590771aa123f14f9a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a sampling-aware framework for adversarial attacks against large language models (LLMs). The key idea is to integrate sampling explicitly into the optimization process, treating attack design as a compute-budget allocation problem between optimization and sampling. The authors show that repeated sampling during attack iterations can improve attack success rate (ASR) and efficiency, introducing a label-free entropy-maximization objective as a proof-of-concept."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[1] The paper correctly identifies that most current attacks evaluate only greedy generations and ignore stochastic sampling, which can underestimate real-world risk.\t\n[2] Experiments include multiple baselines (GCG, PAIR, BEAST, AutoDAN, etc.) and diverse open-weight models.\n[3] The entropy-maximization loss is interesting as a model-agnostic, label-free alternative that naturally leverages sampling variance."}, "weaknesses": {"value": "[1] The paper claims an “optimal trade-off” between optimization and sampling but provides no analytical derivation or formal proof.\n[2] The harm function h(y) is mentioned but not precisely defined; the dependency on the judge model (StrongReject) is underexplained.\n[3] Only sub-10B open-weight models are used; conclusions about “LLM safety at scale” are overgeneralized.\n[4] It maximizes first-token entropy only; rationale for focusing on the first token rather than full sequence entropy is missing. A short experiment or discussion on multi-token entropy or conditional entropy over first k tokens would strengthen Section 6.4.\n[5] The categories “refusal,” “incomplete,” and “harmful” appear visually distinct, yet thresholds (0.1, 0.3, 0.5) seem arbitrary. Adding a short justification or sensitivity test for these cut-offs would substantiate the observation."}, "questions": {"value": "[1] How sensitive are the results to the choice of harm threshold 0.5? Would conclusions change under stricter thresholds?\n[2] The three sampling schedules (optimize-then-sample, uniform, block sampling) are reasonable, but their selection appears heuristic. It would strengthen the argument if the authors could explain why these three were chosen, or at least provide intuition on when each schedule is preferable.\n[3] The entropy objective only considers the first-token distribution, which may not capture downstream harmful semantics. It would be helpful to discuss why entropy is computed only at the first token and whether extending it to later tokens affects stability or optimization cost.\n[4] Because the main claim concerns stochastic sampling, omitting temperature and top-k values leaves the setup ambiguous. A table summarizing decoding hyperparameters for all models would remove this uncertainty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ekpm9lec3R", "forum": "vBmRQHW7en", "replyto": "vBmRQHW7en", "signatures": ["ICLR.cc/2026/Conference/Submission18384/Reviewer_PvN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18384/Reviewer_PvN7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682878430, "cdate": 1761682878430, "tmdate": 1762928091843, "mdate": 1762928091843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sampling-aware framework for adversarial attacks against LLMs that treats repeated sampling as a core and efficient part of the attack itself. It reframes the attack as a \"resource allocation problem\" where an attacker must decide how to spend a fixed computational budget: either on optimization or on sampling. The paper proposes three sampling schedules to explore this trade-off: optimize-then-sample, uniform sampling, and block sampling. Additionally, the paper introduces a label-free attack objective based on maximizing the entropy of the model's first token. During sampling-aware optimization, the framework generates n samples from the optimized query in each interaction. Finally, it selects the query that elicits the highest ASR (i.e., the largest number or proportion of malicious samples) as the final optimized query. The paper evaluates the proposed method on open-source LLMs, such as Gemma-3-1B, Llama-3.1 8B, Llama-3-8B-CB, and Llama-2-7B-DA. It samples 100 prompts from HarmBench for the experiments. The evaluation results show that it can improve the ASR by up to 37%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework can be integrated into the existing optimization-based attack methods, such as GCG, AutoDAN, and BEAST.\n2. The paper introduces a label-free objective, which is interesting.\n3. The paper frames the adversarial attack against LLMs as a resource allocation problem, which provides a fresh perspective for this field."}, "weaknesses": {"value": "1. **Robustness of StrongREJECT.** The ASR is calculated by StrongREJECT, which is a judge model designed for detecting malicious content. Although this judge model is designed to reduce the possibility of false positives, it still cannot achieve 100% accuracy; it may still have false negatives or false positives. Especially when comparing the results of ASR@50 (generating 50 samples) and ASR@1 (generating one sample), this effect could be amplified. The results would be more reliable if the paper can randomly sample from the generated results and conduct human evaluation to compare the inter-annotator agreement between humans and the judge model. This can assess the reliability of the judge. Secondly, HarmBench also provides a judge model; maybe the paper can also provide results using that judge to see whether the results are aligned.\n2. **Evaluation on larger models.** Based on the cost calculation, the low sampling cost appears to stem from using small models (1B–8B). However, these small LLMs may have substantially lower utility than larger models (e.g., GPT, Gemini, Claude). For attacks against larger LLMs, it is unclear whether the sampling would remain cheaper than optimization. Some optimization-based attack methods can optimize on small-size LLMs and then transfer to larger LLMs. It is suggested to report results on larger LLMs (e.g., Llama-70B) to clarify this.\n3. **Unclear result.** It is unclear which optimization strategy is used for the reported results in evaluation (e.g., optimize-then-sample).\n4. **Comparison with other sampling-based methods.** One main takeaway from this work is that sampling can also contribute to ASR, and considering the trade-off between sampling and optimization can elicit the highest ASR. However, this concept is not new, and prior work has proposed a sampling-based attack method [1]. The paper should compare the proposed method with prior work [1] to demonstrate its advantage.\n\n[1] Huang et al., Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation, ICLR’24"}, "questions": {"value": "1. According to Figure 7, the ASR appears to plateau (stop increasing) very early, around 20% of the total iterations. Why does the paper not use early stopping to reduce cost?\n2. Under a fixed budget, if the attack invests most of the budget in sampling (e.g., 50 samples per iteration for 5 iterations), how does transferability change? Can the attack still demonstrate strong transferability compared to the original setting (sampling only once at the end of optimization)?\n3. How to find the right balance? It seems that sampling more in each iteration could lead to a higher ASR. But how many samples should be drawn? Is there any guidance on this?\n4. Does the ASR reported in Figure 1 correspond to ASR@50 or ASR@1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v2E7yfyi70", "forum": "vBmRQHW7en", "replyto": "vBmRQHW7en", "signatures": ["ICLR.cc/2026/Conference/Submission18384/Reviewer_gbXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18384/Reviewer_gbXv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846206862, "cdate": 1761846206862, "tmdate": 1762928091420, "mdate": 1762928091420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the worst-case adversarial attack against LLMs. Existing methods focus on optimizing adversarial tokens to achieve a high average ASR across samples, whereas this paper focuses on the worst-case ASR across all samples. That is, although the adversarial token may not consistently trigger the harmful response, as long as it can trigger it once, the attack is considered successful. Therefore, the authors propose conducting additional sampling to identify rare yet harmful cases. Experimental results show that with more sampling, existing adversarial attacks can improve in the worst-case scenario. The authors also propose an objective to increase the randomness of the generated responses to reach more samples and thus trigger the adversarial effect."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies the sampling aspect, which was neglected in many existing methods. This is an interesting aspect.\n\n2. With more sampling budget, existing methods can also be improved.\n\n3. Even without any target response or an affirmative target template, the label-free entropy-maximization objective can trigger harmful responses."}, "weaknesses": {"value": "1. There seems to be a mismatch between the design and the evaluation. For example, Algorithm 1 involves the dynamic interaction between the optimization and sampling processes. However, during the evaluation, the prompts and samples are stored to explore the different sampling schedules post-hoc. This may not reflect the real performance and is less supportive.\n\n2. Only StrongReject is used as the harm score measurement. \n \n3. Although Figure 8 shows that the Frequency of (h(y)>0.5 | not refusal) doesn't change, this can not lead to the conclusion that GCG didn't shift the harmfulness of the compliant responses. For example, 10 out of 20 Ys of a harmful score of 0.8 is different from 10 out of 20 Ys of a harmful score of 0.6. Maybe draw the lines with different thresholds or a histogram of the scores.\n\n4. It's unclear what temperature is used for sampling. More randomness can also be achieved by increasing the temperature, besides the high entropy objective. \n\n5. Since this attack relies on sampling, it's unclear if the harmful response or behavior can be consistently reproduced.\n\n1. Why isn't there a yellow bar (250 samples) for the optimize-then-sample case in Figure 4?\n\n2. How to understand the compute-optimal frontier in Figure 3? If the total FLOPS budget is fixed, then increasing the optimization FLOPS should decrease the sampling FLOPS."}, "questions": {"value": "1. In addition to the StrongReject, will other judges, such as Llama-Guard or GPT-4-Judge, give similar and consistent results?\n\n2. If we don't use any loss objective, but use a higher temperature, will it achieve a similar adversarial worst-case effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PnUpKuBrrO", "forum": "vBmRQHW7en", "replyto": "vBmRQHW7en", "signatures": ["ICLR.cc/2026/Conference/Submission18384/Reviewer_uZGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18384/Reviewer_uZGG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855850558, "cdate": 1761855850558, "tmdate": 1762928091083, "mdate": 1762928091083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a perspective shift in the domain of LLM adversarial attacks. While most works on LLM safety only consider adversarial attacks in deterministic (T=0) settings, the authors suggest that we should focus more on the effects of random sampling (T>0) when discussing adversarial attacks and LLM safety. They propose a meta-algorithm for generating adversarial attacks in a sampling-aware manner that can be used to enhance any optimization-based attack algorithm. The authors show that their sample-aware optimization can drastically reduce the optimization costs of various SOTA attack algorithms, while also improving their efficacy. The authors also propose a new, label-free optimization objective based on first-token entropy, and show that sampling-aware optimization can be more effective when using this entropy-based loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well structured and easy to read. The paper motivation is clear, the main idea of using sampling for more exploration in the adversarial space at lower costs is simple yet effective. \n\nThe paper discusses a well-known, but not sufficiently highlighted issue in LLM safety evaluation: most works evaluate LLM safety only in the deterministic (T=0), greedy generation setting, which generally underestimates attack strength and overestimates model robustness. I think the paper does a good job at quantifying the effects of these shortcomings in evaluation protocols.\n\nThe claimed improvements in both efficiency and effectiveness are significant and worthy of community attention. \n\nThe paper presents thorough experiments on multiple settings, including a wide range of attacks, models and defences. The additional experiments, visualizations and insights are also interesting and valuable."}, "weaknesses": {"value": "**Insufficient discussion on Temperature**\n\nI think the temperature settings used in the experiments should be discussed in the main paper. While reading I was specifically looking for this information but it was only found in the appendix.\n\nMoreover, I feel that temperature should be treated as a main hyper-parameter in the sampling algorithm as it could significantly affect its performance. The paper would be strengthened by including some ablations and discussions regarding the influence of temperature. I am interested in how the temperature could increase or decrease the attack success rate, the quality of harmful responses, the need for more or less sampling steps per iteration etc.\n\n**Insufficient justification for the entropy objective**\n\nWhile the idea of maximizing the entropy of the first token makes intuitive sense for trying to avoid safe completions, there is no theoretical justification for why optimizing for entropy would be better than optimizing for an affirmative completion. Moreover, while the results in Table 2 show some improvements, it is not very clear that the improvements are consistent across attack types or statistically significant. I think more evidence is needed to affirm that entropy maximization is actually better. For example the authors should analyze the distribution of harmfulness for completions generated by the affirmative and entropy prompts respectively. Finally, the claim that the responses for entropy-based prompts are more natural is purely subjective and only backed by 3 examples in Table 5. To support this claim the authors should provide a more complete analysis, including the differences in distribution for some similarity / naturality scores between the two approaches."}, "questions": {"value": "1. The paper proposes improvements on adversarial attack generation techniques, but does not discuss any possible avenues for improving model robustness in light of their discoveries. While defence improvement can be considered outside the scope of this paper, I would like to hear the authors’ views on this issue.\n2. The entropy maximization objective might have some other interesting effects on the model responses. Have the authors observed during their experiments responses that deviate from the original question, like random gibberish text, or answers to a different question?\n3. Other questions that I have are just related to the weaknesses discussed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XzduK8XUI2", "forum": "vBmRQHW7en", "replyto": "vBmRQHW7en", "signatures": ["ICLR.cc/2026/Conference/Submission18384/Reviewer_auPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18384/Reviewer_auPK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994134481, "cdate": 1761994134481, "tmdate": 1762928088684, "mdate": 1762928088684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}