{"id": "W09KLFgDuj", "number": 20856, "cdate": 1758311018207, "mdate": 1759896955096, "content": {"title": "LDARNet: DNA Adaptive Representation Network with Learnable Tokenization for Genomic Modeling", "abstract": "Genomic foundation models increasingly adopt large language model architectures, yet almost all rely on fixed tokenization schemes such as $k$-mers or byte-pair encoding. These approaches impose arbitrary sequence boundaries and risk discarding biologically relevant signals. Recent work on H-Net introduced dynamic hierarchical tokenization in an autoregressive setup, demonstrating the feasibility of adaptive tokenization on the genome but leaving downstream evaluation unexplored. We present \\textbf{LDARNet}, a hierarchical genomic foundation model that adapts H-Net to the masked language modeling (MLM) paradigm. LDARNet combines BiMamba-2 outer layers operating at nucleotide resolution with a Transformer backbone in a compressed latent space, and uses a ratio regularizer to enforce stable learnable token boundaries. Pretrained on human and multispecies genomes, LDARNet is evaluated under a frozen embedding protocol with logistic regression probes across 26 tasks from the Genomics Benchmarks and Nucleotide Transformer suites. Despite the absence of task-specific finetuning, LDARNet achieves competitive performance with state-of-the-art Transformer baselines and sets new SOTA results on multiple histone modification tasks. These findings provide the first evidence that adaptive tokenization under MLM training yields biologically meaningful embeddings, and highlight hierarchical compression as a promising direction for scalable and interpretable genomic modeling.", "tldr": "We propose LDarNet, a hierarchical genomic foundation model that adapts H-Net’s dynamic tokenization to the masked language modeling paradigm.", "keywords": ["Genomic foundation models", "Learnable tokenization", "Efficient sequence modeling", "Hierarchical architectures"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31831cb766186c8ed093e89abbb27266e1352ffd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission presents LDARNet, a hierarchical genomic foundation model for genomics. It targets a well-known limitation: the reliance on fixed, arbitrary tokenization schemes such as k-mers or BPE, which lack biological grounding. To address this, it adapts H-Net’s learnable tokenization concept to the masked language modeling setup. \n\nBuilt upon state-space BiMamba-2 blocks (for nucleotide-level processing) and a Transformer backbone (to operate on the compressed latent representations), LDARNet performs adaptive sequence compression while preserving bidirectional context. It is pretrained on human and multi-species genomic corpora and evaluated using a probe-only, frozen embedding protocol on 26 tasks from two genomics benchmarks. The training combines an MLM objective with a ratio loss regularizer to ensure stable, non-degenerate compression.\n\nResults indicate LDARNet attains competitive performance on histone modification prediction and strong overall results compared to leading fixed-token and byte-level models. This provides the first evidence that MLM-trained adaptive tokenization can yield biologically meaningful representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**(S1)** This work is well-motivated. It explores and extends a key open modeling issue in genome-scale ML: the suitability and biological interpretability of learned, adaptive tokenization vs. the now-standard, arbitrary tokenization (k-mer or BPE). It also proposes a logical and timely solution: porting H-Net’s dynamic chunking into a non-autoregressive bidirectional MLM framework.\n\n**(S2)** The LDARNet architecture itself is an insightful contribution. The hybrid design shows a  reasonable and well-justified engineering compromise, which marries the linear-time efficiency of BiMamba-2 state-space layers at the byte-level with the expressivity of Transformers in the latent space. Bidirectional enhancements (mean fusion, parameter sharing) are mathematically justified w/ derivations and argument in Sec. 3.2.1.\n\n**(S3)** The experiment setup (Sec. 4, Tab. 1 & 2) covers a broad suite of benchmarks and provides direct comparisons to diverse baselines like GENA-LM, DNABERT-2, HyenaDNA, and others. The probe-only protocol isolates representation power from finetuning pipeline, which is a thoughtful and well-justified choice. LDARNet’s impressive performance on this setting shows evidence for the biological relevance of the learned representation.\n\n**(S4)** The writing and presentation is clear with great logical flow. Visualization of model framework in Fig. 1 is clear, and method formula are provided in detail in Sec. 3. The limitations and future directions are also acknowledged in Sec. 7."}, "weaknesses": {"value": "**(W1)** Missing References and Appendix. The most immediate and severe issue in my view is that the manuscript is incomplete. All reference symbols are missing (displayed as '?') throughout the entire manuscript. And the appendix is absent, which is explicitly referred to for hyper-parameters and reproducibility details. This should be a major flaw for a rigorous academic paper. However, given that ICLR permits revisions and iterative author-reviewer discussions, I would reserve final judgment on this shortcoming, assuming it is an oversight at this stage. I strongly encouraged the authors to provide a corrected, complete manuscript in the rebuttal phase. IMHO, this revision should include all properly formatted citations and the complete appendix. \n\n**(W2)** Incomplete literature review. The discussion of related work in Sec. 2 misses consideration of several important recent studies in tokenization for biological sequences, particularly: BiRNA-BERT [1], which targets adaptive tokenization in RNA. And [2] [3] both investigates tokenization’s direct effects in biological language models. I recommend the authors include these references in the revision to form a complete literature review.\n\n**(W3)** Empirical analysis beyond benchmark comparisons. IMHO, the qualitative analysis could be richer, especially as there are already many mature analysis methods available (such as t-SNE, UMAP, etc.). These methods align exceptionally well with the claim that the learnable tokenization in this paper is biologically grounded. Now, there is no validation of whether the learned chunking units align with regulatory, structural, or motif boundaries beyond the improved classification accuracy. This is suggestive but not conclusive. For example, as I can think of, this could be a figure visualizing the learned boundary probabilities $p_t$ from the router over genomic sequences with known, annotated motifs (e.g., TATA boxes, TF binding sites). It would be insightful and would help support the claims of biological interpretability. \n\n**(W4)** Insufficient Ablation Studies. LDARNet introduces several designs at once (the hybrid framework, BiMamba-2, bidirectional routing, ratio loss, etc.), yet the manuscript does not provide ablation studies to disentangle their individual contributions. In other words, it is unclear how critical the hybrid design is vs. a homogeneous BiMamba-2 model, or how essential the ratio loss is for stable training. My suggestions: at a minimum, two ablations are required: (i) A comparison of the hybrid LDARNet against a pure BiMamba-2 variant with a similar parameter scale, to validate the hybrid-by-design philosophy. (ii) A study of the ratio loss by training a model with its weight set to zero, to show its impact on compression and performance.\n\n**(W5)** The necessity of hybrid model. LDARNet combines BiMamba-2 with Transformers, but there is no direct comparison results showing the necessity or impact of each component, like what happens if the model uses only BiMamba-2 or only Transformer layers? In my view, this is a critical set of experiments to show the method’s validity.\n\n\n---\n### Reference\n\n[1] BiRNA-BERT: Adaptive Tokenization for Efficient RNA Language Modeling, NeurIPS 2024 FM4Science Workshop\n\n[2] Effect of Tokenization on Transformers for Biological Sequences, Bioinformatics 2024\n\n[3] The Impact of Tokenizer Selection in Genomic Language Models, Bioinformatics 2025"}, "questions": {"value": "Most of my major concerns and related recommendations have been stated in the Weaknesses section. I encourage the authors to focus their efforts on addressing those points, as they are critical for strengthening the manuscript in the rebuttal stage.\n\nThe following are more specific, minor questions to help the authors think more deeply about certain design choices and experiment setups, which might be helpful for this and future work:\n\n- Are there any limitations or failure cases in the ratio regularizer? Have the authors observed degenerate solutions in practice? More detail on how the regularizer interacts with model convergence would be helpful.\n- Are there qualitative differences in representation quality or interpretability when moving from fixed-token (k-mer) to learned-token boundaries? It is possible to illustrate (perhaps in appendix) token maps over sample sequences?\n\n\n---\n## Justifications:\n\nI first give a rating of 6, primarily due to the clear motivation, the reasonable choice of hybrid architecture, and the strong results on histone tasks. In particular, the performance suggests the potential practical value, which is critical for biology and genomics. I would be glad to raise my rating if thoughtful responses and improvements are provided. Conversely, if most of the concerns remain unaddressed, I may also lower my score.\n\nI hope these comments help my fellow reviewers and ACs understand the basis of my recommendation. I am open to follow-up discussions to reach a consensus for the final decision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LkfdNlfDDt", "forum": "W09KLFgDuj", "replyto": "W09KLFgDuj", "signatures": ["ICLR.cc/2026/Conference/Submission20856/Reviewer_ju7d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20856/Reviewer_ju7d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761191186282, "cdate": 1761191186282, "tmdate": 1762999992853, "mdate": 1762999992853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LDARNet, a hierarchical genomic foundation model that employs learnable tokenization to improve the representation of genomic sequences. Unlike traditional methods that rely on fixed tokenization schemes (such as k-mers), LDARNet adapts the H-Net architecture to the masked language modeling (MLM) paradigm. The model combines BiMamba-2 outer layers with a Transformer backbone, allowing for efficient processing of genomic data while preserving biologically meaningful features. The authors demonstrate the model's effectiveness through extensive evaluations across multiple genomic tasks, achieving competitive performance without task-specific fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Motivation is reaonsable. \n- The proposed method achieves good performance on several downstream tasks."}, "weaknesses": {"value": "- Lack of novelty. In the introduction part, the authors sumarize three key contributions (two technical contributions and one experimental contribution). Both technical points have similar work already published, and this paper lacks an in-depth discussion and performance comparison with existing work. e.g. dynamic leanable dna tokenizer [1] and mamba networks for DNA modeling [2].\n- Poor presentation. The structure of the paper is very chaotic, and the writing intentions of the paragraphs are unclear, filled with numerous writing errors. For example, the introduction section is too brief and completely lacks information about the methods, missing key details. All cross-references in this submission are incorrect.\n- Limited experiments. The experiments in the paper are insufficient to support the claims, including:\n  - lack of comparison with key models;\n  -  lack of latency analysis; \n  - and lack of ablation experiments. etc\n\nOverall, I think this manuscript is not ready for publication in ICLR'26.\n\n[1] Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA, NeurIPS'24\n\n[2] Caduceus: Bi-directional equivariant long-range dna sequence modeling, ICML'24"}, "questions": {"value": "please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ocDWPD95Qn", "forum": "W09KLFgDuj", "replyto": "W09KLFgDuj", "signatures": ["ICLR.cc/2026/Conference/Submission20856/Reviewer_fcTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20856/Reviewer_fcTb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659297495, "cdate": 1761659297495, "tmdate": 1762999992300, "mdate": 1762999992300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LDARNet, a hierarchical genomic foundation model that uses a learnable, adaptive tokenization approach instead of fixed schemes like k-mers. Featuring a hybrid BiMamba-2 and Transformer architecture, the model adapts the H-Net framework to the masked language modeling paradigm. Without the need for finetuning, LDARNet's performance is comparable to state-of-the-art models, and it has achieved new state-of-the-art results on multiple histone modification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Departing from the fixed tokenization of conventional k-mer or BPE methods, this work pioneers a dynamic, hierarchical approach that resolves their inherent limitations.\n2. Despite the absence of task-specific finetuning, LDARNet achieves competitive performance with state-of-the-art Transformer baselines and sets new SOTA results on multiple histone modification tasks."}, "weaknesses": {"value": "1. The manuscript is poorly prepared. The reference is missing. The equations contain ambiguous notations without clear definitions. For example, Eq. (1) has both s and S, while $0 \\le s < S$, i.e. S can not be reach. Eq. (5) has M2, which is undefined.\n2. The reported results are not strong enough to support the claims. In Table 2, the proposed method only achieves SOTA on two tasks. In Table 2, the proposed method only achieves SOTA on half of the tasks.\n3. The paper lacks ablation studies to demonstrate the necessity of each module.\n4. There are missing baselines in the domain, for example Evo, Evo2."}, "questions": {"value": "1. Please provide ablation results that clarify the incremental contributions of each module.\n2. Please provide comparisons against recent DNA LMs under the same experimental setup.\n3. Please include the discussion on computation budget."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qo87Z9R8K", "forum": "W09KLFgDuj", "replyto": "W09KLFgDuj", "signatures": ["ICLR.cc/2026/Conference/Submission20856/Reviewer_K7Vn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20856/Reviewer_K7Vn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823501463, "cdate": 1761823501463, "tmdate": 1762999992396, "mdate": 1762999992396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LDARNet, a hybrid BiMamba-2/Transformer model for genome sequencing that uses a novel tokenization regularizer and claims state-of-the-art results over Transformer-based foundation models. While the performance is promising, the paper lacks essential ablation studies and crucial baseline comparisons to fully validate its contributions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed LDARNet architecture has strong performance on the human and multi-species genome benchmarks, reportedly surpassing existing SOTA models.\n- The methodology is nicely presented in details."}, "weaknesses": {"value": "**Presentation**\n- Many citations throughout the paper are broken, rendering as (?) in the paper and not appearing in the reference list. Also, the citation for DNABERT renders as (dna) and is unlisted in the references list.\n- Some internal references to Tables/Figures (e.g. L503) are also broken and require correction.\n- There is a stub appendix after the references, which according to L502 is supposed to contain training details.\n\nAll together, these errors leave the paper feeling not just unpolished but also unfinished.\n\n- The methodological novelty of the shared weights mechanism in BiMamba-2 (Sec 3, L152) is unclear, as it is not sufficiently differentiated from how the weights are shared in the highly similar Caduceus model (despite this being cited heavily by the authors).\n\n**Soundness**\n- The experimental section (e.g. Table 1) fails to include a important benchmark comparison against Caduceus, which uses a highly similar bi-directional Mamba MLM setup on genomics benchmark.\n- The major contribution of the \"Learnable DNA tokenization\" (Sec 2.1, L065) is unsubstantiated, as no ablation study validates its effectiveness against simpler tokenization methods.\n- A key baseline, a vanilla Mamba-2+Transformer hybrid (cf. Sec 9.2.3 of Mamba-2), is missing, making the architectural contribution hard to assess.\n- The work is missing ablations to demonstrate the choice of hyperparameters such as the choice of compression ratio and ratio loss weighting.\n- For completeness, it would be better to report both fine-tuning and linear probe performances, rather than only linear probe. Even using a frozen encoder probe, there are other options one can consider than a linear probe on an average of the tokens - one could perform a non-linear (MLP) probe, or an attentive probe (Chen et al, 2023; Bardes et al, 2024; Greyson Brothers, 2025; Psomas et al, 2025). An attentive probe can be more indicative of the performance which will be obtained from fine-tuning than a mean pool linear probe that requires embeddings to be well aligned across the sequence length.\n- It is currently unclear whether the mean pooling (L353) includes CLS tokens of models which have them. Why was linear probing of the CLS tokens not considered instead of the mean of the embeddings?\n\n**Minor**\n- The background on hierarchical and learnable tokenization in LLMs more broadly should be more extensive. Works such as Byte Latent Transformer and Large Concept Models are not cited and I think would be appropriate in this regard.\n- L179: The citation for Mamba-2 is incorrect. It points to an empirical study, not the [original model paper](https://proceedings.mlr.press/v235/dao24a.html).\n- L188: Gap in Eq 4 for $C_t$\n\n**References**\n- [Caduceus](https://arxiv.org/abs/2403.03234): Schiff et al (2024). \"Caduceus: bi-directional equivariant long-range DNA sequence modeling\" ICML 2024.\n- [Mamba-2](https://proceedings.mlr.press/v235/dao24a.html): Dao and Gu, (2023). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\". ICML 2023.\n- [CAE](https://doi.org/10.1007/s11263-023-01852-4) Chen et al (2022). \"Context Autoencoder for Self-Supervised Representation Learning.\" Int J Comput Vis 132, 208–223 (2024). doi:[10.1007/s11263-023-01852-4](https://doi.org/10.1007/s11263-023-01852-4)\n- [V-JEPA](https://arxiv.org/abs/2404.08471): Bardes et al (2024). \"Revisiting Feature Prediction for Learning Visual Representations from Video\". TMLR 2025.\n- Greyson Brothers (2025). \"Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs\". ICML 2025. arXiv:[2506.09215](https://arxiv.org/abs/2506.09215)\n- Psomas et al (2025). \"Attention, Please! Revisiting Attentive Probing Through the Lens of Efficiency\". arXiv:[2506.10178](https://arxiv.org/abs/2506.10178)\n- [Byte Latent Transformer](https://arxiv.org/abs/2412.09871): Pagnoni et al (2024). \"Byte Latent Transformer: Patches Scale Better Than Tokens\". ACL 2025.\n- [Large Concept Models](https://arxiv.org/abs/2412.08821): Barrault et al (2024). \"Large Concept Models: Language Modeling in a Sentence Representation Space\"."}, "questions": {"value": "- L273, Eq 14: What is the notation where a subscript is a sum supposed to mean?\n- L341: The authors say \"To promote reverse–complement invariance, each sequence is sampled in forward and reverse orientations with equal probability.\", but do you actually take the complement of the sequence? I do not see this stated in the paper, either as a complemented always taken when reversing the orientation, or an augmentation where the complement is taken stochastically with p=0.5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hHsDvmgCRN", "forum": "W09KLFgDuj", "replyto": "W09KLFgDuj", "signatures": ["ICLR.cc/2026/Conference/Submission20856/Reviewer_Yccr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20856/Reviewer_Yccr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974767170, "cdate": 1761974767170, "tmdate": 1762999992353, "mdate": 1762999992353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}