{"id": "rNl8XiSHiJ", "number": 3298, "cdate": 1757396275933, "mdate": 1759898097318, "content": {"title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation", "abstract": "The ability to estimate the quality of scientific papers is central to how both humans and AI systems will advance scientific knowledge in the future. However, existing LLM-based estimation methods suffer from high inference cost, whereas the faster direct score regression approach is limited by scale inconsistencies. We present NAIPv2, a debiased and efficient framework for paper quality estimation. NAIPv2 employs pairwise learning within domain-year groups to reduce inconsistencies in reviewer ratings and introduces the Review Tendency Signal (RTS) as a probabilistic integration of reviewer scores and confidences. To support training and evaluation, we further construct NAIDv2, a large-scale dataset of 24,276 ICLR submissions enriched with metadata and detailed structured content. Trained on pairwise comparisons but enabling efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art performance (78.2\\% AUC, 0.432 Spearman), while maintaining scalable, linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it further demonstrates strong generalization, with predicted scores increasing consistently across decision categories from Rejected to Oral. These findings establish NAIPv2 as a debiased and scalable framework for automated paper quality estimation, marking a step toward future scientific intelligence systems.", "tldr": "NAIPv2 leverages pairwise learning on debiased data to predict article quality in a pointwise manner, achieving SoTA results and generalizing consistently with human decisions on unseen NeurIPS data.", "keywords": ["AI for Research", "Automated Peer Review", "Article Quality Estimation", "Literature Intelligence Systems"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c78a2123a6be7863f13232464d63ea78868cf20.pdf", "supplementary_material": "/attachment/1f0c52515cb38cfe11f117a1ed303972efa68299.zip"}, "replies": [{"content": {"summary": {"value": "LLM-assisted peer review has become a popular topic in recent years, and Paper Quality Estimation is an important part of it. This paper proposes NAIPv2, a model that takes the titles and abstracts of two papers as input and predicts their pairwise quality. The model is based on the Llama 3 backbone, trained on the ICLR dataset with human reviewer’s feedback as ground truth. NAIPv2 achieves state-of-the-art performance (AUC and rank correlation) than several baselines, including autoregressive models. In addition, the method runs in linear time, generalizes well, and works when only abstracts are available instead of full papers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a paper quality predictor NAIPv2 that achieves state-of-the-art performance using only titles and abstracts, showing a high rank correlation with human reviews.\n* NAIPv2 is efficient (linear time), generalizes well, and does not require full text, which makes it practically useful. I look forward to seeing downstream applications, such as helping authors improve their titles and abstracts.\n* The paper combines existing tools into a clear and effective workflow. Although it uses Llama 3 as a backbone, given that paper quality estimation is still a new topic, I find the overall novelty acceptable.\n* The experiments are well done. Some additional comments are listed in the “Questions” section."}, "weaknesses": {"value": "* I would like to see experiments using more proxies for paper quality (see “Questions” section).\n* Section 3.3 is a bit confusing. the training details could be explained more clearly in the main text."}, "questions": {"value": "* Many studies in this area use citation count as a proxy for paper quality because (1) their methods are unsupervised, and (2) human reviews are highly noisy, making it hard to align with them. I suggest adding experiments that use citation-based proxies and comparing them with existing methods, although they are not reliable. Such results could reveal deeper insights: if NAIPv2 performs worse on predicting citations but aligns much better with human reviews, it may suggest that human judgments rely heavily on information from titles and abstracts.\n* This is more of a curiosity than a critique: since judging paper quality solely from titles and abstracts is unrealistic, I believe that NAIPv2’s strong alignment with human reviewers may come from certain “cheap signals” in abstract / title. I wonder what those signals might be; perhaps the authors could offer some hypotheses or discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WAmKMXYGP0", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Reviewer_qFdP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Reviewer_qFdP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933208333, "cdate": 1760933208333, "tmdate": 1762916653767, "mdate": 1762916653767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As the number of submissions to peer-reviewed conferences increases exponentially each year, systems that automatically assign a review score to submissions may become increasingly important. This paper proposes to train an LLM to assign a score to a research article solely based on its title and abstract. \nIt first introduces a new dataset, NAIPD2, curated using  ICLR submissions from 2021 to 2025. \nTo account for reviewer confidence, it proposes a confidence-normalised target score, RTS, which is used as the ground-truth label for each paper. The paper claims that reviewing style (and hence review scores) varies by the paper’s domain and year. To account for this bias, instead of directly regressing on the RTS score, they propose a pairwise loss computed using a siamese-style network over the pairs selected from the same domain and year. The network is trained via BCE loss to predict which paper in a training pair has a higher RTS score.  To create domain buckets, instead of using the given keywords, the paper proposes to use hierarchical clustering of the paper’s title and abstract embeddings. To ensure that the LLMs being trained haven’t seen the papers/reviews before, it proposes to use only 2025 submissions in the test set.\nThorough ablations justify the design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, with most of the details present either in the main paper or the appendix.\n\n2. The paper has thorough ablations to justify all the design choices.\n\n3. The paper releases a new dataset that may help the research community."}, "weaknesses": {"value": "1. While I understand that regression-based methods are obviously much more efficient, the lack of any reasoning provided by the model significantly hampers their applicability. On the other hand, methods like DeepReview provided a detailed output that can assist in actually reviewing a paper and perform as good as the proposed method. \n\n\n2. Another cited contribution is release of Naidv2 dataset. However, similar datasets ( and recipes to create them) already exist, e.g. the dataset used in DeepReview.\n\n3. Another issue is a fair comparison with other baselines. All the reported experiments are ablations for the design choices made by the paper, e.g. pairwise loss vs pointwise loss, curriculum vs no curriculum etc., I don’t see an apples-to-apples comparison with other trained auto-regressive models that may be more inefficient but more accurate and useful.  \n\n4. A simple baseline would be an LLM finetuned to generate the review along with the score. This way, all the reviews from the paper can be considered as a separate training sample. \n\n5. Line 322:323: Table 1 reports numbers directly from DeepReview and CycleReview papers, which are computed on different test data (though from ICLR 2025 only). Typically, numbers reported in the same table should be computed on exactly the same dataset.\n\n6. OpenReviewer is mentioned in the related works, but not used as a baseline. \n\n7. The designed system takes only the title and abstract into account, whereas human reviewers would seldom base their decision solely on these two attributes. Does it mean that the system is capturing spurious correlations? \n\n8. While the application of a pairwise loss function to the problem at hand may be novel, the loss function used is the standard one employed to train reward models under Bradley-Terry (BT) reward model assumptions."}, "questions": {"value": "1. Line 247: 248: One of the claims in the paper is that reviewing style varies by domain and year, and clustering is used to create domain buckets. Can you show that review scores indeed follow a different distribution in each cluster-year group? Maybe a plot with cluster-year on the x-axis and some statistic quantifying the distribution on y-axis? \n\n2. Line 213-214 -- “pipeline produces domain-normalized labels”. How is RTS normalised using the domain information?  In eqn 14 and 15, what is j iterating over? Scores of the same paper, or all the scores in the dataset?\n\n3. Given that papers from the same year are used to select the pairs during training, does it mean that we can only compare scores of the papers in the same domain and year? Can we use it to meaningfully compare papers across years?\n\n4. Line 314: 316: Could you please explain the upper bound? Is the input to the MLP just a scalar or a vector of all the review scores for a paper? What if a paper has 3 reviews and another has 4 reviews? \n\n5. To compute F1 and accuracy, you need to select a threshold for the score based on validation data. Is the threshold selection dependent on domain / year?\n\n6. Table 4 / Section 4.6: When using a different statistic (e.g., mean), I hope that it is used both for training and while testing and reporting the numbers in Table 4 - i.e. please clarify that you are not training using mean but then using  RTS to compute ground-truth in pairwise accuracy and ground-truth rank in correlation. \n\n7. Section 4.7: In Figure 4, it is not clear what you are comparing.  You have pairwise difficulty on the x-axis. For a given bucket, say the “easiest” bucket, are you reporting the performance obtained when you train your model using only the “easiest” pairs?  If yes, then how come it matches exactly with the performance in  ‘w/ Curriculum’ setting in Table 5 where you are using all pairs but in a curriculum.\n\n\n\n**Comments:**\n\n1. Given that the paper claims RTS as one of its contributions, its details should be moved from the appendix to the main paper.\n\n2. Table 1: Isn’t a higher correlation better? There is no upward arrow sign beside it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lL0bSj1yTk", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Reviewer_64Pf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Reviewer_64Pf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907104587, "cdate": 1761907104587, "tmdate": 1762916653517, "mdate": 1762916653517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NAIPv2, a fast regression model for paper review score estimation. The key idea behind this model is Review Tendency Signal (RTS), which uses not only the raw scores of each paper review but also the specified confidence level to model uncertainty behind individual scores. Alongside the model, the authors also introduce NAIDv2, a collection of ICLR review data compiled from OpenReview."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear benefits of using confidence information and RTS over raw scores, as seen in Table 4\n- Computational efficiency of NAIPv2 with pointwise inference\n- NAIDv2, a collection of reviews and confidence scores from ICLR, appears to be thoroughly constructed.\n- Performance gains over more computationally expensive API and autoregressive baselines\n- Extensive evaluation of NAIPv2/NAIDv2, including interesting demonstration of model generalizability based on NeurIPS decision category hierarachy"}, "weaknesses": {"value": "- It looks like the paper does not exactly mention how regression head was implemented on top of LLaMA 3 for NAIPv2. I didn’t have time to check out NAIPv1 paper so I’m not sure if the architecture was discussed there, but still I think it would be nice to have the architecture description somewhere in the paper.\n- It would have been nice if NAIPv2 was also tested on the LLMs other than LLaMA 3 to see if the benefits of NAIPv2 still holds."}, "questions": {"value": "- Regarding Section 3.2, it’s not very intuitive to me whether NAIDv2 really handles domain bias. NAIDv2 only contains ICLR submissions, which seems to be a relatively homogenous set given that ICLR is a ML conference. Are the authors suggesting that two papers of similar content and quality could have significantly different scores just because the authors chose different submission categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9gRpFwf7oX", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Reviewer_f3tL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Reviewer_f3tL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918350795, "cdate": 1761918350795, "tmdate": 1762916653043, "mdate": 1762916653043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NAIPv2, a framework for estimating scientific paper quality by combining pairwise learning with a probabilistic Review Tendency Signal (RTS) that integrates reviewer scores and confidences. The authors introduce a new large-scale dataset (NAIDv2) derived from ICLR submissions and report strong results in both within-domain and cross-domain evaluation. The central claim is that NAIPv2 provides debiased, scalable, and efficient quality estimation compared to prior LLM-based or regression-based approaches.\nThe paper is clearly written and tackles an important and timely problem in automated peer review and scientometric prediction. However, the theoretical framing and probabilistic modeling choices raise several concerns, especially regarding the internal consistency of the modeling assumptions and the clarity of related work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is an interesting and ambitious paper tackling a key problem in automated peer review. The dataset and efficiency contributions are also substantial.\nEquation (1) is well-motivated and welcome to see; it provides a principled probabilistic link between confidence and uncertainty rather than relying on ad hoc weighting.\n\n\nThe proposal to integrate reviewer confidence directly into the score aggregation model addresses a genuine issue in automated peer review and quality estimation.\n\n\nThe focus on efficiency and domain-year stratified training is well thought out and practically motivated."}, "weaknesses": {"value": "(Main critique)\nYou introduce and motivate a Gaussian probabilistic model (RTS) but then discard it during pairwise ranking. When training with a logistic loss, you implicitly assume Gumbel noise, not Gaussian, effectively switching from a Thurstone–Mosteller to a Bradley–Terry model.\n\n\nThis inconsistency weakens the theoretical story: RTS models uncertainty via Gaussian variance, yet the pairwise comparisons ignore that structure and the confidence information is lost.\n\n\nFor example, if paper A’s reviewers are all high-confidence and paper B’s are all low-confidence, this distinction disappears once RTS is computed. A unified probabilistic model that preserves confidence across comparisons would be both cleaner and more principled.\n\nFor the avoidance of doubt, the model to which I refer is essentially a Thurstone-Mosteller model, where we take the probability of one or more reviewers scoring paper A over paper B, given the observed scores we have from reviewers for these papers, using your theoretical model of latent-factor based reviewer-paper scoring, and confidence values as additional inputs.\n\nSee e.g., \n[Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment\nCyrus Cousins, Vijay Keswani, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong] for discussion of probabilistic pairwise prediction models in the broader ML context.\n\nThe Thurstone-probit of Appendix D approaches this idea, but variance information has already been removed by this point. I’d like to see this further explored.\n\nConceptual and Modeling Issues\n(186–213)\nMany technical aspects are vague in their description, hampering understanding and readability. This is likely a writing quality issue, rather than a criticism of the work behind it.\nThe introduction of the Review Tendency Signal (RTS) as a posterior mean implicitly assumes a Bayesian framework, but the paper never defines a prior or explains how the posterior is derived. As written, the likelihood $p(s_i \\mid x, c_i)$ is defined, but this does not imply a likelihood on $x$. Please clarify whether a uniform prior is assumed, and present the closed-form expression for the posterior mean.\n\n\nGiven your confidence mapping functions, the posterior mean likely admits a clean closed form, which would help motivate the simplicity of your model.\n\n\n(Appendix F; σ(cᵢ))\nThe mapping from reviewer confidence to variance is extremely limited. Although you normalize per reviewer, a learned nonlinear function over the entire reviewer population would be both straightforward and far more flexible.\n\n\n(269)\nClarify that the additive offset correction may depend on year and domain; a global offset is only justified if all pairwise comparisons form a connected component.\n\n\n(242, 682–696)\nThe term sigmoid is too vague — you almost certainly mean the logistic function. This matters because later in the appendix you replace it with a Gaussian CDF, which is also a sigmoid; the text should explicitly reference this distinction.\n\n\nThe model should be described more clearly in the language of Bradley–Terry and Thurstone–Mosteller frameworks, which would provide a unifying probabilistic interpretation and clarify the noise assumptions.\n\n\nThe appendix (lines 682–696) is interesting but should be integrated into the main text. The theoretical implications of alternative link functions (logistic vs. Gaussian CDF) should be discussed explicitly, as should the choice of Brier loss. A connection to strictly proper scoring rules would strengthen this section.\n\n\nTerminology and Framing\nThe term scale inconsistency appears repeatedly but seems misapplied. The issue appears to be translational inconsistency (offsets), not multiplicative scale differences. The current phrasing is consistent but confusing."}, "questions": {"value": "Related Work and Fairness of Discussion\n(124)\nThe criticism of Zhang et al. (2025a) on the Re² dataset seems unfair and unsubstantiated. The statement “deviations between reported results and those presented in the original papers” needs concrete evidence or examples. If the concern is data leakage or overlap, that should be clearly documented and cited. Otherwise, this reads as an unsupported claim, can the authors clarify?\n\n\n(154)\nThe summary of Höpner et al. (2025) lacks essential details. The claim that pairwise ranking is “unreliable” for review scores is unconvincing.\n\n\nIf regression on review scores is feasible, pairwise regression should also be; at worst it merely doubles the noise variance of independent samples.\nThe critical question is how pairs are selected and what is predicted (rank order, score difference, or ordinal classification). It seems to me that pairwise comparisons are most interesting for ranking, rather than cardinal scores, as you have the additional benefit of simplifying the problem domain to a binary prediction. In cardinal (regression) settings, all that's happening is canceling out translational offsets, which can be interesting when you're controlling for specific effects, but seems unimportant for random pairings. It's not clear from the descriptions in related work which of these methods are being employed, and this seems extremely important. Pairwise methods aren't monolithic: the advantage or lack thereof in using them comes from how pears are chosen and what function of the pair one tries to predict, and I would like to see this explored or discussed.\n\nBroader Modeling Questions:\n\nIn automated paper review methods discussed, it seems like there are at least two axes of comparison here. \n\n1: The methods used to make predictions, and 2: what is being predicted. Why not treat citation counts, reviewers scores, and pairwise predictions as separate target, try to predict them all with one model, and select some combination of them for the actual quality estimate? Moreover, these various targets don't seem incompatible. Some concepts are better predictors of quality than others, but likely in complementary ways. It seems a joint predictive model would benefit from shared structure, and this multimodal learning would also make better use of available data.\n\nIt would be valuable to separate and jointly analyze multiple predictive targets (e.g., citation counts, reviewer scores, and pairwise preferences) within a unified model. These signals are not incompatible and could benefit from shared structure via multitask learning.\n\n\n\nBias:\nIt seems to me that, under a Bradley Terry or Thurstone-Mosteller model, you are controlling for bias by subtracting out factors. You subtract out year-dependent and domain dependent factors with this strategy. \n\nThe “de-biasing” strategy seems roughly equivalent to including additive terms for each year-domain combination in a regression model, since you're using pairwise comparisons to control for these effects. It would help to compare these approaches empirically or at least discuss their equivalence.\n\nIn the same vein, why not restrict pairwise comparisons to papers reviewed by the same reviewers? Although this reduces the number of pairs, it would directly control for reviewer-level bias and may yield cleaner comparisons. It also may alleviate or reduce the need to normalize for each reviewer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dLapbwz9SU", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Reviewer_g7oZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Reviewer_g7oZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945646195, "cdate": 1761945646195, "tmdate": 1762916652817, "mdate": 1762916652817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Acknowledgment of Reviews and Upcoming Clarifications"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and detailed feedback on our submission.  \n\nWe are carefully reviewing each comment and will provide a structured clarification soon, including additional analyses and theoretical explanations to address the raised questions.  \n\nWe appreciate the constructive spirit of the reviews and look forward to the discussion."}}, "id": "oCNe0ZMcKw", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762946870616, "cdate": 1762946870616, "tmdate": 1762946870616, "mdate": 1762946870616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a dataset (NAIDv2) along with a framework (NAIPv2) for automated paper quality estimation. In particular, the framework adopts pairwise learning (comparing two papers at a time) but is deployed as an efficient pointwise predictor (judging a single paper's quality). The framework also uses a gaussian model, named Review Tendency Signal, to aggregate multiple reviews into a single ground-truth quality signal. Tested on unseen NeurIPS submissions, NAIPv2 is able to achieve better performance over the previous NAIPv1 framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tackles an important problem to collect a massive dataset of paper review and propose a data-driven solution to automate the paper review process.\n2. This paper introduces several methods to make its framework efficient and practical such as pairwise learning, pointwise inference; Review Tendency Signal etc.\n3. The performance and generalizability of the framework appears to be good."}, "weaknesses": {"value": "1. The relationship between NAIDv2 and NAIPv2 is confusing due to the naming. It appears that NAIDv2 is just the dataset used for learning the NAIPv2 framework.\n2. The paper introduces models such as Review Tendency Signal, but it is unclear how good this probabilistic model is, compared to other alternatives, e.g., the review score might follow a Bradley–Terry model.  \n3. The experiment results on the neurips dataset is better than the previous method, but it is still unclear how it is useful for practical applications, e.g., can it guide the AC decision in assessing the paper quality in addition to human reviewers."}, "questions": {"value": "What are some good application scenarios for NAIPv2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i6eqlLwFhC", "forum": "rNl8XiSHiJ", "replyto": "rNl8XiSHiJ", "signatures": ["ICLR.cc/2026/Conference/Submission3298/Reviewer_hKKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3298/Reviewer_hKKi"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233676914, "cdate": 1762233676914, "tmdate": 1762916652600, "mdate": 1762916652600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}