{"id": "G1MCI1Qp9u", "number": 18760, "cdate": 1758290668075, "mdate": 1759897082720, "content": {"title": "Layer-Parallel Training for Transformers", "abstract": "We present a new training methodology for transformers using a multi-level layer-parallel approach. Through a neural ODE formulation of transformers, our application of a multilevel parallel-in-time algorithm for the forward and back propagation phases of training achieves parallel acceleration over the layer dimension. This dramatically enhances parallel scalability as the network depth increases, particularly useful in large foundational models. However, achieving this introduces errors that cause systematic bias in the gradients, which in turn reduces convergence when closer to the minima. We develop algorithms to detect this critical transition and either switch to serial training, or systematically increase the accuracy of layer-parallel training. Results, including the BERT, GPT, ViT, and machine translation architectures, demonstrate parallel-acceleration as well as accuracy commensurate with serial pre-training while fine-tuning is unaffected.", "tldr": "We apply a novel layer-parallel training paradigm to transformer models which scales with network depth.", "keywords": ["Transformers", "Training methodology", "Multi-level parallelism", "Neural ODE", "Parallel-in-time algorithm", "BERT", "Vision Transformer (ViT)", "Machine translation", "GPT2"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/372cc4e58ff06114fe30f9835c7772b532b8d659.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work develops an MGRIT-based strategy for transformer training, enabling parallelism over layers. Layer-parallel training achieves speedups and memory reduction on multiple GPUs. The paper introduces a methodology which allows the training to switch back to serial training when the gradients bias becomes too large. The authors evaluates its impact on pre-training, accuracy, and fine-tuning with language model datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conduct experiments on various tasks, including BERT pre-training, text classification, vision transformer, machine translation and GPT2 pre-training.\n\n2. The paper is well written."}, "weaknesses": {"value": "1. Although the paper includes extensive experiments across different tasks, there is a lack of a comprehensive evaluation of the model performance. For example, only some loss curves are provided (see Figure 4), but the y-axis spacing is quite large. Small differences in loss could have a significant impact on these tasks.\n\n2. I noticed in the appendix that most tasks use a very small batch size, with only 32 for BERT pre-training. This is far smaller than the batch size typically required for actual pre-training."}, "questions": {"value": "Provide comprehensive evaluation of the model performance, especially acc. on downstream tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BZRgt7B6rq", "forum": "G1MCI1Qp9u", "replyto": "G1MCI1Qp9u", "signatures": ["ICLR.cc/2026/Conference/Submission18760/Reviewer_8f2R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18760/Reviewer_8f2R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970423602, "cdate": 1760970423602, "tmdate": 1762928488573, "mdate": 1762928488573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a layer-parallel training approach for Transformer models based on the neural ODE formulation and the MGRIT method. To correct inexact gradients, the authors propose monitoring the divergence of residuals between iterations and switching to serial training when this indicator stagnates. The method is verified across various models, benchmarks, and hyperparameters, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured and easy to follow.\n- The method is evaluated across multiple model architectures and hyperparameter settings.\n- Monitoring residuals effectively identifies the transition point between parallel and serial training."}, "weaknesses": {"value": "- The main contribution of the paper is the application of an existing layer-parallel training method to encoder–decoder architectures and the introduction of a monitoring mechanism for training transition. However, applying the layer-parallel approach to encoder–decoder architectures does not appear to differ significantly from its application to encoder-only or decoder-only architectures, making the contribution rather limited.\n- In Figure 2, when training at level-1, Layers 1, 3, 5, and 7 are placed on Device 1, but they are distributed across Devices 1 and 2 at level-0. If parameter transmission is required at every training step, this would introduce significant overhead.\n- The paper lacks comparisons with existing layer-parallelism methods in multi-GPU training experiments.\n- Appendix C omits key pre-training parameters. Given the different iterations and switching strategies between training methods, details such as learning rate and scheduling are necessary.\n- Writing issues: Some key methodological details are missing, such as GPU implementation (line 259) and backward propagation (line 265). These are critical for understanding the approach and should be briefly introduced despite page limitations."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tm1Q6NTFAw", "forum": "G1MCI1Qp9u", "replyto": "G1MCI1Qp9u", "signatures": ["ICLR.cc/2026/Conference/Submission18760/Reviewer_KVCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18760/Reviewer_KVCY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595964546, "cdate": 1761595964546, "tmdate": 1762928485345, "mdate": 1762928485345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a layer-parallel training method for transformers.\nBy viewing the transformer's depth as the time axis of a neural ODE, the authors apply the MGRIT algorithm to parallelize computations over the layer dimension, i.e., forward and backward passes in different layers can run in parallel.\nBecause MGRIT produces inexact gradients, this method is switched off and reduced to none based on an indicator.\nExperiments on BERT, GPT-2, ViT, and machine-translation models show the accuracy and speedups achieved by the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is interesting. Although the methods come from numerical ODEs and are not new, applying them to training transformers is conceptually interesting and orthogonal to other parallel strategies."}, "weaknesses": {"value": "- Unfair comparison baseline: The serial baseline is single-GPU sequential training, while the proposed method uses multi-GPU resources. Instead, standard multi-GPU baselines like TP or PP should be compared.\n- Limited layer-parallel training horizon: Once switched to serial mode, the algorithm never re-enables MGRIT. Moreover, the horizon where MGRIT is enabled is very limited. In the GPT-2 pretraining experiment, MGRIT is disabled after 1,000 batches, corresponding to 0.25B tokens (batch size = 256, sequence length = 1024, as shown in Appendix B). This horizon is very short, and training on billions or trillions of tokens is common today. Overall, this method seems unable to scale in current manuscript."}, "questions": {"value": "- How does the method behave when combined with standard TP/PP on equal GPUs?\n- Could a bidirectional (serial <-> parallel) switching strategy yield further speedup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VtJcujf67B", "forum": "G1MCI1Qp9u", "replyto": "G1MCI1Qp9u", "signatures": ["ICLR.cc/2026/Conference/Submission18760/Reviewer_YbRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18760/Reviewer_YbRf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634457671, "cdate": 1761634457671, "tmdate": 1762928484301, "mdate": 1762928484301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a layer-parallel training paradigm for transformers, building on a neural ODE viewpoint to enable multilevel parallel-in-time algorithms that parallelize computation across the layer dimension. By leveraging the MGRIT algorithm, both forward and backward passes of transformers are made parallel, significantly enhancing scalability as model depth increases. The approach introduces gradient bias due to inexact solutions, for which the authors develop adaptive control mechanisms that switch to serial training or adjust the parallelization parameters when necessary. The methodology is validated on BERT, GPT, ViT, and machine translation models, demonstrating speedups with minimal loss of accuracy compared to conventional serial training, and showing negligible impact on downstream fine-tuning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a core scalability bottleneck in transformer models, layerwise serialization, by proposing a layer-parallel strategy that is compatible with large-scale distributed training.\n\n- The methodology includes an insightful and actionable bias/gradient control mechanism, allowing adaptive switching between inexact (layer-parallel) and exact (serial) computation as training approaches convergence—a pragmatic solution to bias-induced optimization pitfalls.\n\n- Experiments are substantial: Results span diverse architectures (BERT, GPT, ViT, MT) and include strong scaling, convergence fix mechanisms, and ablation studies on parameters like number of MGRIT levels and coarsening factor."}, "weaknesses": {"value": "- **Insufficient Baseline Coverage in Scaling Experiments**: In the scaling studies (Figures 6–8), while classic strong scaling is shown for different architectures, there is no direct comparison to pipeline or tensor parallelism baselines under identical hardware and problem settings. It is not clear how much of the observed speedup is unique to the proposed method, as opposed to achievable by state-of-practice alternatives.\n- **Ambiguity in Practical Integration**: The presentation lacks substantial discussion of integration overheads when combining the layer-parallel scheme with other parallelization paradigms (data, tensor, or pipeline parallelism) in realistic, large-cluster deployments. The claim that layer-parallel is \"fully compatible\" is not experimentally demonstrated.\n- **Limited Exploration of Hyperparameters**: While some space is allocated to tuning, the interplay between MGRIT hyperparameters and problem/task type (BERT, GPT, ViT, MT) is not extensively or systematically studied, and guidance remains empirical rather than principled."}, "questions": {"value": "1. Can the authors provide a more systematic quantitative comparison (wall-clock time, speedup, GPU utilization, memory footprint) between layer-parallel training and standard pipeline/tensor/data parallelism baselines, especially under identical hardware setups?\n2. What are the main challenges in integrating layer-parallel training with other forms of parallelism (data/pipeline/tensor) at scale? Are there observable overheads or compatibility pitfalls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g3HzC7ruD0", "forum": "G1MCI1Qp9u", "replyto": "G1MCI1Qp9u", "signatures": ["ICLR.cc/2026/Conference/Submission18760/Reviewer_RbvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18760/Reviewer_RbvQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967191812, "cdate": 1761967191812, "tmdate": 1762928483538, "mdate": 1762928483538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}