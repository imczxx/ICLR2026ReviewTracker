{"id": "1K3l3sjwMh", "number": 23230, "cdate": 1758341031302, "mdate": 1759896825096, "content": {"title": "Learn All You Need in One Hypernetwork", "abstract": "While attention mechanism is considered the cornerstone of Transformers, its layer-specific parameterization presents challenges for efficiency and knowledge reuse. Recent work reformulates multi‑head self-attention as a hypernetwork, suggesting it can be mathematically interpreted as an implicit hypernetwork conditioned on key–query pairs. However, prior work has been limited to small‐scale tasks or theoretical demonstrations, leaving open whether explicit hypernetworks can scale to full language-model pre-training. We first prove the existence of a shared hypernetwork that can approximate the multi-head self-attention with fewer parameters. Building on this insight, we propose HyperBERT, a BERT-style Transformer encoder in which the multi-head self-attention mechanism is replaced by a single-layer MLP dynamically generated by one explicit, shared hypernetwork. In our experiments, a 4-head, 2-layer Transformer decoder serves as the shared hypernetwork to generate a single-layer MLP to replace all query, key, value, and output (QKVO) projection matrices in each layer of a 4-head, 4-layer BERT. Pre-trained on WikiText-103, our 4-layer HyperBERT matches the average GLUE score of a BERT baseline ( $ \\Delta \\le 0.1 $ ) with 6\\% fewer parameters and outperforms other MLP-based attention alternatives. Furthermore, the transplant experiment shows that the hypernetwork's learned weights transfer more effectively to deeper models than conventional attention parameters under a fixed-parameter budget. To the best of our knowledge, this is the first pre-training study that replaces multi-head self-attention with MLPs generated by a shared hypernetwork. Our results suggest that an explicit, shared hypernetwork can serve as a modular, parameter-efficient replacement for multi-head self-attention in BERT-style Transformer encoder models while preserving language modeling capabilities.", "tldr": "", "keywords": ["Hypernetworks", "Attention", "Transformer"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb66e9457536bb4233cfd7cce95ec6adb50bb197.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes HyperBERT, which replaces the self-attention mechanism in the encoder-only transformer (BERT) with an MLP whose weights are generated by a hypernetwork parameterized as a decoder-only transformer. Experiments compare 4-layer BERT and HyperBERT models, using WikiText-103 (100M tokens) for pre-training and the GLUE benchmark for evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The reviewer found no notable strengths in this submission."}, "weaknesses": {"value": "There are three major issues: (1) the soundness and relevance of the proposed method are weak; (2) the practical significance of the results is very limited; and (3) the presentation lacks mathematical rigor.\n\nMethod: While BERT played an important role in the early stage of pre-trained language modeling (~2018–2020), it has not remained the mainstream paradigm, primarily because it is not a standalone model that can be used as-is---unlike the more versatile GPT-style decoder-only language models. Given this context, there should be a strong and specific motivation for revisiting the BERT setting; however, the reviewer did not find such a compelling justification in this work. \n\nThe main idea of using an MLP with context-dependent weights for sequence processing is also not new---as acknowledged by the authors through their citations of Schlag et al. (2021) and Schug et al. (2025).\n\nExperiment: The experimental setting is extremely limited. One of the main claims in the abstract is that *\"prior work has been limited to small-scale tasks\"*; however, pre-training on the 100M-token WikiText-103 dataset also clearly falls within the small-scale regime by today’s standards. Even in academic settings, it is now common to train models with up to 1B parameters on roughly 10B tokens to meaningfully evaluate modern language modeling methods. Additionally, as noted above, the baseline comparison is restricted to BERT. There is no clear justification for this choice, given that more general decoder-only language models can perform similar tasks.\n\nPresentation/Clarity: There are also issues with mathematical rigor, notation, and overall clarity. For example, in Sec. 3.1, two different notations—$f_l$ and $F$—are introduced in Equation (2) for the same function, and their usage is inconsistent (Sec. 3.1 uses $F$, while Sec. 3.2 uses $f_l$). In Sec. 3.2, the phrase *\"that is one-hot at position $l$\"* likely refers to the $l$-th coordinate, not the position. In Sec. 3.4, *\"Basis memory $U$ that is shared\"* appears without a clear transition; some connection to Sec 3.3. needs to be clarified more smoothly. The statement *\"Its output dimension is P, so it can replace $w_l$ inside F\"* uses non-italicized mathematical symbols ($P$, $F$), which is inconsistent with the rest. Similarly, *\"the input is a layer code $c_l$\"*---$c_l$ should be mathematically defined, including its dimensionality if it is a vector. \n\nMore broadly, given that this theoretical result is not particularly strong---hypernetworks that are properly parameterized by neural networks are naturally universal approximators themselves (and the low-rank observations motivate parameter-sharing)---the details would be more appropriate for the appendix.\n\nOverall, this submission falls well below the standards expected at general machine learning conferences."}, "questions": {"value": "The reviewer has no further questions and considers it unlikely that this work will become acceptable after any rebuttal or discussion.\n\nOne relevant missing reference:\nHyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\nAndrey Zhmoginov, Mark Sandler, Max Vladymyrov. ICML 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MAjZ4pogVv", "forum": "1K3l3sjwMh", "replyto": "1K3l3sjwMh", "signatures": ["ICLR.cc/2026/Conference/Submission23230/Reviewer_NJeA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23230/Reviewer_NJeA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587850378, "cdate": 1761587850378, "tmdate": 1762942568383, "mdate": 1762942568383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papers proposes to use a single shared hypernetwork for all attention layers. It demonstrates that HyperBERT saves parameters and improves performance in downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well motivated, interesting idea\n- Quantifiable, although minor benefits \n- explicit discussion of limitations, clearly written"}, "weaknesses": {"value": "- Considering that attention as a hypernetwork has already been implemented and the contribution is limited to having a shared hypernetwork between layers, it makes it somewhat incremental\n- The model is limited to encoder transformers\n- FLOP curves scale worse than standard BERT in terms of sequence length, furthermore, wall-clock time has not been reported which raises a question how fast the networks are in practical implementation\n- The scale of the experiments is too small to conclude about the behaviour in larger models\n- Parameters saving over BERT are very marginal especially that the comparison is against basic BERT. AlBERT[1] achieves strong results while sharing a lot of layers. HyperBERT effectively does this as well, however, it would not beat the AlBERT.\n\n[1] - **ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**"}, "questions": {"value": "Could you include the wall-clock time comparisons to other models, as well as sizes of all models compared?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp62TjPozT", "forum": "1K3l3sjwMh", "replyto": "1K3l3sjwMh", "signatures": ["ICLR.cc/2026/Conference/Submission23230/Reviewer_mtKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23230/Reviewer_mtKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665270856, "cdate": 1761665270856, "tmdate": 1762942568091, "mdate": 1762942568091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes replacing the per-layer attention weights in a transformer with weights from a learned hypernetwork that is shared across all layers. This makes the architecture more modular and reduces the number of parameters since the same hypernetwork is shared across all layers. The hypernetwork used in the experiments consists of a 4-head and 2-layer transformer decoder. Experiments and comparisons are carried out on a BERT model and the authors create their version of BERT (HyperBERT) that uses a shared hypernetwork in place of attention parameters. Experiments on GLUE show that HyperBERT can on average match BERT with 6% fewer parameters. In addition, the authors provide a theoretical analysis of the parameter efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* **Clarity**  The paper is generally well written and is easy to follow. \n* **Results** The paper has good results for the range of the experiments presented (GLUE, BERT etc). In general the paper matches BERT with fewer parameters and with the hypernetwork.\n* **Novelty** According to the authors, the paper is the first pre-training study to replace multi-head attention with MLPs generated by a shared hypernetwork. The problem of replacing MHA with a learned network is interesting in itself. [Disclaimer: I have not extensively read through the literature on hypernetworks, so I cannot verify if this is indeed the case.]\n* **Ablations** There are detailed ablations for the different tasks, e.g retraining on MNLI to investigate the source of the performance gap etc.\n* **Theoretical Analysis** The authors provide a theoretical analysis to support and ground the arguments in the paper."}, "weaknesses": {"value": "* The paper and its results are all focused on Encoder-only style Transformer models (BERT). This raises the question of whether this would transfer to decoder-only style transformers which are more common for generative models etc. In this case how could one for example ensure causality in the learned operator ?\n* What is the overall goal of the hypernetwork ? The decrease in parameters (a 6% decrease) does not look significant or helpful to me. Generally, I would expect that parameter sharing would lead to a good drop in parameter size, which could help reduce the memory requirements for training or fine-tuning e.g in LORA. Not too sure how far 6% might go! Is there a chance to get better gains in number of parameters as we scale?\n* How does introducing a hypernetwork affect the runtime/memory usage of the model during training and inference ? Is this slower or is this faster ?\n* The low rank assumption in the theory seems critical for a compact representation/hypernetwork, however the assumption is not well supported. Also why is r@90/L significantly smaller for T5-large (0.542) compared to the others (>0.8) ?\n* One of the premises of the paper is that previous work is limited to small-scale experiments/tasks. One could argue that this is the same for this paper, which uses a small dataset wikitext101 and small network. How does this method scale ? Does it work beyond smaller tasks like GLUE and datasets like Wikitext ?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "husMYRNjG9", "forum": "1K3l3sjwMh", "replyto": "1K3l3sjwMh", "signatures": ["ICLR.cc/2026/Conference/Submission23230/Reviewer_ALq8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23230/Reviewer_ALq8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954745304, "cdate": 1761954745304, "tmdate": 1762942567877, "mdate": 1762942567877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proved that a shared hypernetwork can approximate multi-head self-attention with fewer parameters. They then built HyperBERT, a BERT variant where a single 2-layer Transformer decoder dynamically generates MLPs to replace all attention mechanisms across all layers. When pre-trained on WikiText-103, HyperBERT matched standard BERT's GLUE performance with 6% fewer parameters and showed better transfer learning to deeper models. This is the first work to successfully replace multi-head self-attention with hypernetwork-generated MLPs at the scale of full language model pre-training."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured, with clear writing and clarity of thought\n2. Discussion of related work is extensive, and motivates the ideas explored in the paper well\n3. The theoretical / mathematical analysis is detailed and rigorous, and provides a strong motivation for the proposed architecture and empirical experiments\n4. Empirical methodology is well-explained and rigorous\n5. Empirical results are strong vs other approaches"}, "weaknesses": {"value": "1. The main model used for comparison, BERT, is six years old. It would be better if the authors benchmarked / based their method off more recent (and perhaps SotA) transformer models and architectures\n2. The authors only test on small-scale models, relative to those which are commonly used today for language modelling, it would be interesting to see the impact on models of at least size ~1B parameters\n3. The paper would benefit from a diagram illustrating their new architecture\n4. The reduction in parameter count, 6%, is somewhat small."}, "questions": {"value": "1. How do the authors think their method will be affected as you scale up the model size into the billions of parameters?\n2. Do the authors have any intuition for how the performance in the encoder-only setting will transfer to the decoder-only setting? (i.e. GPT-style transformers rather than BERT-style)\n3. Given the attention layer replacement becomes an MLP (with dynamically generated weights), could this also subsume whatever computations are happening in the fixed MLP (FFN) sublayers? I.e. could you use the hyper network to replace almost all the computations in the transformer, and not just attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y5C4Y5uxxa", "forum": "1K3l3sjwMh", "replyto": "1K3l3sjwMh", "signatures": ["ICLR.cc/2026/Conference/Submission23230/Reviewer_otMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23230/Reviewer_otMK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015299755, "cdate": 1762015299755, "tmdate": 1762942567672, "mdate": 1762942567672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}