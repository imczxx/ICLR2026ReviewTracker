{"id": "XYB67Nu2xD", "number": 20091, "cdate": 1758302385795, "mdate": 1759897002392, "content": {"title": "LRanker: LLM Ranker for Massive Candidates", "abstract": "Large language models (LLMs) have recently shown strong potential for ranking\nby capturing semantic relevance and adapting across diverse domains, yet existing\nmethods remain constrained by limited context length and high computational\ncosts, restricting their applicability to real-world scenarios where candidate pools\noften scale to millions. To address this challenge, we propose LRanker, a frame-\nwork tailored for large-candidate ranking. LRanker incorporates a candidate\naggregation encoder that leverages K-means clustering to explicitly model global\ncandidate information, and a graph-based test-time scaling mechanism that parti-\ntions candidates into subsets, generates multiple query embeddings, and integrates\nthem through an ensemble procedure. By aggregating diverse embeddings instead\nof relying on a single representation, this mechanism enhances robustness and\nexpressiveness, leading to more accurate ranking over massive candidate pools.\nWe evaluate LRanker on seven tasks across three scenarios in RBench with\ndifferent candidate scales. Experimental results show that LRanker achieves\nover 30% gains in the RBench-Small scenario, improves by 3–9% in MRR in the\nRBench-Large scenario, and sustains scalability with 20–30% improvements in the\nRBench-Ultra scenario with more than 6.8M candidates. Ablation studies further\nverify the effectiveness of its key components. Together, these findings demonstrate\nthe robustness, scalability, and effectiveness of LRanker for massive-candidate\nranking.", "tldr": "LRanker is a scalable LLM-based ranking framework that uses aggregated centroids and test-time ensembles to handle ultra-large candidate sets efficiently.", "keywords": ["Large candidates", "Ranking", "Scalability", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c9be3bdea9cfa9b39cf3e9f0aa096fe8e726329.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LRanker, a new LLM-based ranking framework for large candidate sets. It uses K-means clustering to summarize all candidates into a few centroids, then feeds them into the LLM with the query. At test time, it splits candidates, re-ranks subsets, generates multiple query embeddings, and averages them for the final score. They test on RBench with 7 tasks across small, large, and ultra-large scales. Results show big gains: over 30% in small sets, 3–9% MRR in large, and 20–30% in the 6.8M case. Ablations confirm both clustering and test-time scaling help.\n\nContributions include a scalable way to inject global candidate information into LLMs via clustering, a graph-based test-time ensemble that boosts ranking robustness by combining multiple query embeddings, and solid experimental evidence that LLMs can outperform specialized models even at million-scale ranking."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a practical gap - existing LLM rankers fail on million-scale candidates. Using K-means to compress candidate info into the prompt is simple but effective. The test-time ensemble mechanism is a reasonable extension of ensemble learning to ranking. Not revolutionary, but addresses an underexplored problem.\n\n2. Experimental design is solid. Testing on 7 tasks with candidates ranging from 20 to 6.8M demonstrates actual scalability. Baselines are appropriate, including both general and specialized methods. Ablations properly isolate component contributions. Results show consistent 20-37% improvements.\n\n3. Paper is well-structured with clear motivation. Figure 1 effectively illustrates the key innovation. Writing is generally readable, though sometimes repetitive when discussing existing methods' limitations. Tables and figures are informative."}, "weaknesses": {"value": "1. Table 1 labels LRanker’s latency as “Low” without actual numbers. No runtime, memory usage, or scaling curves reported. Computing 6.8M embeddings + K-means clustering + multiple test-time partitions all have costs that aren't measured. The efficiency claims lack empirical support.\n\n2. The core idea of summarizing candidates via K-means centroids is interesting, but it’s not compared against simpler alternatives like randomly sampling K candidates or using other clustering methods. It’s unclear whether the gains come from clustering *per se* or just from having *some* global summary. A few ablation variants here would strengthen the contribution.\n\n3. Section 3.1 states that the projected aggregated candidate embedding g̃ replaces a special token <|embedding|> in the prompt, but the mechanism is vague. How exactly are continuous embeddings injected into the discrete token sequence? Is this soft prompt tuning, or are embeddings added to the hidden states at a specific layer? The computation of query embeddings in Eq. 6 averages over token hidden states and a \"predicted next token\" state z_q,nt, but it's unclear what this next token represents or how it's obtained. The architecture diagram in Figure 1(b) doesn't clarify this process."}, "questions": {"value": "1. Can you provide actual runtime or memory usage for each scenario? \n\n2. The candidate aggregation relies heavily on K-means centroids, but it's unclear why this is the best choice. Have you tried random sampling K candidates or other clustering algorithms? \n\n3. How sensitive is performance to the number of clusters K? You mention selecting K on validation sets but show no ablation. \n\n4. You test up to 6.8M candidates, what's the upper limit? How does performance degrade as candidates increase beyond this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y6G3DgZwdf", "forum": "XYB67Nu2xD", "replyto": "XYB67Nu2xD", "signatures": ["ICLR.cc/2026/Conference/Submission20091/Reviewer_TeHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20091/Reviewer_TeHG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756114464, "cdate": 1761756114464, "tmdate": 1762932986868, "mdate": 1762932986868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets ranking with very large candidate sets where context limits and decoding latency make list wise prompting and token space generation impractical. LRanker augments the input with aggregated candidate information by clustering candidate embeddings with K means and projecting the resulting centroids into the prompt so the model encodes the query with an explicit view of the global candidate distribution. The output is a query embedding used to score offline candidate embeddings by inner product which decouples ranking quality from decoding latency. At inference the method partitions the candidate pool into subsets, produces multiple query embeddings under different subsets, and ensembles the scores to improve robustness. Experiments on RBench across small, large, and ultra scales report over thirty percent relative gains in the small setting, three to nine percent MRR gains in the large setting, and twenty to thirty percent gains with more than 6.8 million candidates. Ablations indicate that global aggregation, the inference ensemble, and LoRA adaptation each contribute to the improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. In this paper, authors give clear problem framing on the need to model global candidate information at input time and to perform ranking through embedding based outputs for scalability.\n\n2. The input design that injects compact centroids into the prompt provides a controllable way to expose global structure without violating context limits.\n\n3. In the paper, inference time partition and ensemble improves robustness by aggregating multiple perspectives rather than relying on a single embedding.\n\n4. The authors provide strong empirical coverage across seven tasks, and three scales with consistent gains including the ultra setting with 6.81 million candidates which supports the scalability claim.\n\n5. Ablation studies isolate the contribution of global info, test time ensemble, and LoRA which builds causal evidence for each design choice."}, "weaknesses": {"value": "1. Novelty reads as a pragmatic combination of clustering based set summarization with discriminative scoring. The paper would benefit from a deeper discussion of what is fundamentally new relative to prior LLM rankers beyond the specific combination.\n\n2. The method depends on K means quality and on the choice of the number of centroids. The paper does not provide a detailed sensitivity study for K or for the projector capacity.\n\n3. In the paper, the partition and ensemble procedure brings extra inference cost. The paper does not quantify the latency and compute trade off as width and depth vary.\n\n4. Although the formulation section is standard, but the method lacks a simple theoretical account of how the ensemble reduces variance or mitigates bias from partial candidate views.\n\n5. In the paper, it mainly focuses on accuracy metrics. There is limited analysis of stability under candidate distribution shift or frequent catalog updates which is common in recommendation and search."}, "questions": {"value": "1. How sensitive is performance to the number of clusters and to the dimensionality of the injected centroids.\n\n2. How do you think LRanker compares to a learnable set encoder that summarizes a sampled candidate pool with cross attention.\n\n3. Can you clarify whether the ensemble mainly reduces variance or corrects bias from mismatched subsets.\n\n4. How are the partition width and depth chosen at inference time, are there any details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nv2VcpY5an", "forum": "XYB67Nu2xD", "replyto": "XYB67Nu2xD", "signatures": ["ICLR.cc/2026/Conference/Submission20091/Reviewer_iWYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20091/Reviewer_iWYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894285765, "cdate": 1761894285765, "tmdate": 1762932986210, "mdate": 1762932986210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LRanker, a new framework designed to handle large-scale candidate ranking using large language models (LLMs). Traditional LLM rankers struggle with large candidate pools due to limited context lengths and high computational costs. LRanker addresses this by introducing a candidate aggregation encoder (using K-means clustering to create compact candidate centroids) and a graph-based test-time scaling mechanism that combines multiple query embeddings. The proposed framework is evaluated across multiple tasks, showing substantial improvements in ranking performance, even for ultra-large candidate pools with millions of candidates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LRanker introduces two key innovations—candidate aggregation via K-means clustering and a graph-based test-time scaling mechanism. These innovations distinguish the framework from previous methods, making it a unique contribution to large-scale ranking tasks.\n\n2. The paper is technically sound, with clear definitions of its components and detailed explanations of the model architecture. The proposed methods are novel, particularly the use of clustering for candidate aggregation and the ensemble approach at inference time.\n\n3. The structure of the paper is well-organized, and the explanations are clear. The proposed model is described in detail, making it relatively easy to understand the motivation behind the design choices. Diagrams (e.g., Figure 1 on page 3) effectively illustrate the differences between LRanker and prior models.\n\n4. The ability to scale LLM-based rankers to millions of candidates is a significant advancement. This paper’s contributions have the potential to impact practical applications in areas like recommendation systems and information retrieval where large candidate sets are common."}, "weaknesses": {"value": "1. While the paper introduces a graph-based scaling mechanism and candidate aggregation, similar ideas are explored in other works, such as ensemble learning in ranking. The clustering technique (K-means) for candidate aggregation, although novel in this context, does not offer radical innovation compared to previous techniques in unsupervised learning.\n\n2. The choice of K-means clustering for candidate aggregation could be further justified. It would be helpful to explore why clustering was selected over other potential methods (e.g., dimensionality reduction techniques like PCA or t-SNE) and whether it scales efficiently with very large datasets.\n\n3. The paper shows impressive results on specific benchmarks like RBench. However, the model’s performance on tasks with highly diverse or noisy candidate pools could be questioned. The impact of the ensemble mechanism and aggregation could vary significantly across different types of ranking tasks.\n\n4. While LRanker claims to handle large-scale candidate pools, the computational efficiency of the graph-based scaling mechanism is not fully addressed. As the candidate pool increases, the graph-based method may encounter scalability bottlenecks that could affect real-world applications, especially in production systems with real-time requirements.\n\n5. While the ablation study demonstrates the importance of each component (e.g., global info, test-time ensemble), it would have been useful to see more detailed comparisons between LRanker and other state-of-the-art methods in terms of both computational time and memory usage. This would offer a clearer understanding of the trade-offs."}, "questions": {"value": "1. Why did you choose K-means clustering over other unsupervised techniques, such as hierarchical clustering or dimensionality reduction methods?\n\n2. How does the graph-based test-time scaling mechanism perform as the number of candidates grows to the extreme (i.e., tens of millions)?\n\n3. Could the performance of LRanker degrade in scenarios with extremely noisy or irrelevant candidates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hLijWiqndN", "forum": "XYB67Nu2xD", "replyto": "XYB67Nu2xD", "signatures": ["ICLR.cc/2026/Conference/Submission20091/Reviewer_ecoE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20091/Reviewer_ecoE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895767849, "cdate": 1761895767849, "tmdate": 1762932985607, "mdate": 1762932985607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LRanker, an LLM-based ranking framework designed for large candidate pools, addressing the scalability and context-length limitations of prior LLM rankers. It introduces two key innovations — a candidate aggregation encoder using K-means clustering to incorporate global candidate information, and a graph-based test-time scaling mechanism that ensembles multiple query embeddings for robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a well-motivated framework explicitly designed for massive-candidate LLM-based ranking.\n- The proposed graph-based test-time ensemble provides a principled way to combine multiple query embeddings, improving robustness and ranking quality without retraining.\n- The paper conducts extensive experiments on seven tasks across three RBench scenarios, covering small to ultra-large candidate pools."}, "weaknesses": {"value": "- All training/evaluation is within RBench; no out-of-benchmark is tested, so generalization is unclear.\n- Lacks detailed latency analysis. The authors should provide a latency comparison to support their earlier claims.\n- The ablation study shows that the contributions of individual components are quite limited, raising concerns that the improvements of LRanker may mainly stem from the use of the Qwen3 model.\n- It is necessary to use the same base model as the baseline to ensure a fair comparison.\n- The authors need to provide more details about the process, such as how the candidate embeddings are generated, how the training and test data are specifically divided, and more details about the model training procedure.\n- The authors should improve Figure 1, as the current illustration of test-time scaling is difficult to understand."}, "questions": {"value": "Questions are expressed in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wVCfS4wMya", "forum": "XYB67Nu2xD", "replyto": "XYB67Nu2xD", "signatures": ["ICLR.cc/2026/Conference/Submission20091/Reviewer_Q9mF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20091/Reviewer_Q9mF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940577356, "cdate": 1761940577356, "tmdate": 1762932984672, "mdate": 1762932984672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}