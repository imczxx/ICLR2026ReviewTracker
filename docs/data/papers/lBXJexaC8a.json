{"id": "lBXJexaC8a", "number": 4458, "cdate": 1757683443095, "mdate": 1762351903442, "content": {"title": "IF-VidCap: Can Video Caption Models Follow Instructions?", "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. \nCurrent benchmarks, however, primarily assess descriptive comprehensiveness while largely overlook instruction-following capabilities.\nTo address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples.\nDistinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness.\nOur comprehensive evaluation of over 19 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity.\nFurthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.", "tldr": "", "keywords": ["Caption"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23b72c5f0a5b76aa259da534c673e82c1a67993a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes IF-VidCap, the first benchmark explicitly targeting controllable, instruction-following video captioning. IF-VidCap consists of 1,400 high-complexity samples—each composed of a video, a detailed, compositional set of user instructions (averaging six constraints across 27 types), and an evaluation checklist grounded in both rule-based and open-ended question assessments. The benchmark enables systematic study of MLLM video captioning models on both content accuracy and their ability to adhere to fine-grained, user-defined instructions. Detailed experimental results and analyses highlight shortcomings in current models and demonstrate the strength of instruction-specific tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**1. Novelty and Relevance:**\nThe paper identifies a clear gap in current video captioning evaluation: lack of systematic assessment for instruction-following. : Shifts video captioning evaluation from “describe everything” to controllable generation with compositional constraints, a real need for editing, generation, and content ops. The benchmark enumerates 27 constraint categories with clear coverage (format, stylistic, content; Fig. 3d), which is more granular than prior video captioning benchmarks.\n\n**2. Evaluation Metrics Design:**\nThe rule+LLM hybrid judging (LLM extracts; scripts verify) for format, complemented with retrieval-based QA for content, is sensible and scalable. Inclusion of both CSR/ISR and their rule/open-ended breakdowns is helpful for diagnosing failure modes."}, "weaknesses": {"value": "**1. Insufficient Baseline:**\nWhile Table 2 compares many models, some critical vision MLLM baselines from the missing related works (particularly methods leveraging procedural or hierarchical modeling and dynamic storyline composition) are not included or discussed, limiting the ability to guarantee current models are being fairly and comprehensively evaluated, such as LLaVA-Video, VideoChat, InternVideo, KiMi-VL, Keye-VL, MiMo-VL, GLM-4.1V and so on.\n\n**2. Unclear Difficulty Calibration:**\nBest ISR ≈ 27.8% (Gemini-2.5-Pro), with open-ended CSR ≈ 35%–36% across leaders. The paper interprets this as “difficult,” but doesn’t calibrate human or near-oracle ceilings. \n\n**3. Narrow Analysis of IF-Captioner-Qwen:**\nGains are shown only on IF-VidCap. Need other video captioning benchmark (e.g., VidCapBench, Dream-1K and so on) to demonstrate transfer and rule out overfitting to this benchmark’s constraint taxonomy.\n\n**4. Figures Need Deeper Interpretation:**\nSeveral figures are highly informative but under-discussed in the text. For example, Figure 5’s constraint-type heatmap is comprehensive, but the main text lacks deeper discussion of which constraint categories most differentiate current MLLMs, and how these signal bottlenecks for the field."}, "questions": {"value": "1. Could you add part of human-written captions under constraints to measure Human-ISR/CSR and a scripted case for format-only ceilings; otherwise, hardness is anecdotal.\n\n2. Could you quantify template overlap with IF-VidCap constraints (n-gram/Jaccard/TF-IDF) and provide cross-benchmark results to show that IF-Captioner-Qwen’s gains persist on external IF-video evaluations?\n\nOther concerns please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZvUZPBg3I6", "forum": "lBXJexaC8a", "replyto": "lBXJexaC8a", "signatures": ["ICLR.cc/2026/Conference/Submission4458/Reviewer_sjFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4458/Reviewer_sjFE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030777222, "cdate": 1761030777222, "tmdate": 1762917379323, "mdate": 1762917379323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IF-VidCap, a new benchmark designed to evaluate whether multimodal large language models can generate video captions that adhere to specific user instructions. It proposes a structured evaluation framework that measures both format correctness and content correctness, combining LLM-based and rule-based checking. The benchmark includes 1,400 samples and an associated fine-tuning dataset to enhance instruction-following abilities. Through evaluation of over 20 models, the authors provide a comprehensive analysis of current MLLMs’ capabilities and limitations in controlled video captioning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.  Valuable and timely evaluation benchmark — the proposed dataset fills a significant gap in assessing instruction-following behavior for video captioning models. \n2. Covers a wide range of different settings, including multiple constraint types, compositional tasks, and diverse video sources. \n3.  Includes a fine-tuning dataset, enabling reproducibility and extension for future research.\n4. Two-format setting (rule-based vs. open-ended checking) is well-designed and helps assess both structural and semantic capabilities. \n5. Strong methodological contribution: The combination of LLM-based and rule-based evaluation is novel and well-justified. \n6. Reliability verification — the authors confirm the stability and consistency of their evaluation metrics. \n7. Comprehensive analysis of model capabilities — Figure 5 and related results provide meaningful insights into the strengths and weaknesses of current models.\n8. Clear structure and thorough experiments: The benchmark is well-documented, with strong empirical validation across multiple models and metrics."}, "weaknesses": {"value": "1. Lack of detail on video selection and preprocessing: It’s unclear how the 350 base videos were chosen and filtered beyond general quality criteria. The authors should provide a full list or dataset summary for reproducibility. \n2. Limited discussion on annotation consistency: Although human refinement is mentioned, inter-annotator agreement or quality control statistics are not detailed. \n3. Benchmark scope limitation: The dataset focuses primarily on short or medium-length videos (2–60 seconds), leaving longer temporal reasoning largely unexplored. \n4. Complexity vs. accessibility trade-off: The multi-step evaluation protocol may limit broader adoption unless supported by easily usable code or interfaces."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Qsnml2INk", "forum": "lBXJexaC8a", "replyto": "lBXJexaC8a", "signatures": ["ICLR.cc/2026/Conference/Submission4458/Reviewer_h647"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4458/Reviewer_h647"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604930646, "cdate": 1761604930646, "tmdate": 1762917379103, "mdate": 1762917379103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IF-VidCap, a new benchmark for instruction-following video captioning that evaluates a model's ability to adhere to diverse, multi-constraint user instructions. Comprising 1,400 video-instruction pairs with complex constraints, the benchmark reveals through extensive evaluation that even top models like GPT-4o achieve only modest instruction fidelity, with specialized captioning models struggling significantly. The work also provides a 46K-pair training dataset and shows that fine-tuning can improve performance, positioning IF-VidCap as a key driver for future research in controllable video description."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work demonstrates significant strengths through its creation of IF-VidCap, the first benchmark systematically evaluating instruction-following in video captioning with complex, real-world constraints. The benchmark is built on high-quality, carefully curated data and features a comprehensive, human-validated evaluation protocol. Its extensive experiments across ~20 diverse models yield clear insights into scaling effects and model capabilities, while the accompanying training dataset proves practically useful for model improvement. The analysis is both thorough and accessible, supported by effective visualizations, making a strong case for the benchmark's value in advancing controllable video captioning research."}, "weaknesses": {"value": "The benchmark has several limitations, including its focus on short videos which excludes long-form content and constrained summarization tasks. Its evaluation, while efficient, relies on automated LLM judgments that may miss nuanced errors and depends on proprietary models, raising reproducibility concerns. Although fine-tuning demonstrates improvement, the absolute performance gains remain modest, and the analysis lacks a deeper investigation into the underlying reasons. Furthermore, the paper provides limited implementation details for key techniques like \"thinking mode\" and offers few qualitative examples to illustrate model failures, which could hinder comprehensive understanding and diagnosis of current shortcomings."}, "questions": {"value": "1. Will the authors release the IF-VidCap dataset, annotation tools, prompts, and evaluation scripts? Making these public is crucial for adoption.\n\n2. Can the authors clarify how the “thinking” mode is applied across models? Is it literally GPT-style CoT prompting, or something else? Providing the exact prompts or settings would help reproducibility.\n\n3. How do models perform on each of the 27 constraint types? Are there systematic failures on particular formats (e.g. JSON vs Markdown) or content categories (e.g. spatial vs temporal constraints)? Understanding this could guide future improvements.\n\n4. The training instructions were generated from captions using DeepSeek-V3.1. How natural and diverse are these instructions? Do they cover the same linguistic structures as the test instructions? Has any human evaluation been done on the quality of these synthetic instructions?\n\n5. For context, how do these models perform on standard video captioning metrics (BLEU, CIDEr, CLIPScore) on the same videos? It would be informative to see the drop in performance when moving from free captioning to constrained captioning.\n\n6. The paper relies on automated scores for the main results. Was any human evaluation of model outputs on IF-VidCap done (even a small sample) to check alignment with CSR/ISR? If so, what were the results?\n\n7. Have the authors considered how real-world users might specify instructions beyond the benchmark’s templates? For example, could models handle more open-form instructions (“Describe the video as a news report”) that go beyond fixed constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x3KPC3C9mq", "forum": "lBXJexaC8a", "replyto": "lBXJexaC8a", "signatures": ["ICLR.cc/2026/Conference/Submission4458/Reviewer_mY9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4458/Reviewer_mY9K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737728202, "cdate": 1761737728202, "tmdate": 1762917378156, "mdate": 1762917378156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IF-VIDCAP, the first benchmark to systematically evaluate instruction-following in video captioning models, shifting focus from traditional descriptive accuracy to fine-grained, compositional constraint adherence (e.g., format, content, style, reasoning). It comprises 1,400 high-quality video-instruction-checklist triplets (27 constraint types, averaging 6 constraints per instruction) and proposes a hybrid evaluation protocol (rule-based checks + LLM-as-Judge) for format correctness and semantic fidelity. Experiments on 20+ models show that while proprietary models lead, top open-source variants are catching up, and general-purpose MLLMs surpass specialized captioners on complex instructions. The authors further release a training dataset and their fine-tuned IF-Captioner-Qwen, which demonstrates substantial gains in instruction-following."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. IF-VidCap is the first benchmark to explicitly evaluate instruction-following in video captioning (27 constraint types), addressing a critical gap beyond traditional accuracy/fluency metrics.\n2. 1,400 video-instruction-checklist triplets via a two-stage pipeline (auto-generation + human refinement), with 83.6% modification rate and consensus-based validation.\n3. Combines rule-based checks (deterministic) + LLM-as-Judge QA (semantic), achieving 96.33% human-agreement for reliable assessment."}, "weaknesses": {"value": "1. 1,400 samples is relatively small compared to text-only instruction-following benchmarks (e.g., IFEval, CFBench). And videos average 20.5s and max out at 60s — does not test long-form temporal reasoning or multi-scene narratives.\n2. Evaluation focuses on compliance, not quality. Does not assess fluency, coherence, or creativity of generated captions.\n3. Training data distribution gap: Uses a \"caption-to-instruction\" generation method, which may not reflect real user instruction distributions."}, "questions": {"value": "1. Do you plan to extend IF-VIDCAP to longer videos (e.g., >1 min) or multi-scene narratives that require temporal summarization or causal reasoning?\n2. Are there plans to support multi-turn instruction-following, where users refine their requests iteratively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XppOiOve8l", "forum": "lBXJexaC8a", "replyto": "lBXJexaC8a", "signatures": ["ICLR.cc/2026/Conference/Submission4458/Reviewer_z26H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4458/Reviewer_z26H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894567625, "cdate": 1761894567625, "tmdate": 1762917377487, "mdate": 1762917377487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}