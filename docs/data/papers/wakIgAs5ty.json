{"id": "wakIgAs5ty", "number": 15015, "cdate": 1758246874114, "mdate": 1759897335454, "content": {"title": "CASh: Causality Alignment Shifting to Unveil Vulnerabilities in Visual-Language Model", "abstract": "Existing adversarial attacks on vision-language models (VLMs) primarily use joint occurrence likelihoods to capture interdependency, often missing the true relationship between the text and the image.This paper presents a novel attack, CASh, on VLMs by manipulating latent causal representations between images and text in pre-trained models. We leverage the cross-attention matrix to capture causality alignment and exploit its singular properties to develop an efficient perturbation algorithm that modifies VLM tasks. Our attack targets the core causal relationships that exist independently of specific VLMs, ensuring transferability across models. Unlike existing attacks that primarily perturb inputs using correlation-based patterns, our approach accounts for causality, offering interpretability by showing how causal shifts lead to changes in VLM behavior. We evaluate CASh across various VLMs and compare it to existing attack methods. Our results demonstrate a significant performance boost, with an average improvement of 20.88\\% in transferable attack capability.", "tldr": "", "keywords": ["Causality", "VLMs", "cross-attention matrix"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a35a9a3fef9361e5d5c910f7a4acc72478172918.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CASh (Causality Alignment Shifting), a causality-based adversarial attack framework for VLMs. CASh perturbs the latent causal relationships between image and text modalities by modeling them through SCMs and analyzing their cross-attention alignments. Using SVD they identify high-impact causal directions. Experiments across various VLM tasks and several models show that CASh consistently achieves higher attack success and improves transferability by over 20% compared to prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The causality-driven adversarial attack framework represents a clear conceptual novelty beyond pixel-level attacks on VLMs. I like it because it's more explainable and intuitive compared to previous adversarial attacks. \nThe use of SVD to identify high-impact causal directions is also an interesting methodological choice and seems to be pretty effective. Overall, I think the motivation behind this paper is interesting; however, it needs significant rewriting and modifications as mentioned in the weaknesses section."}, "weaknesses": {"value": "- Respectfully, the writing quality of the paper is very poor and appears to have been prepared hastily without sufficient proofreading. There are numerous grammatical and logical errors that make the paper difficult to read, unenjoyable, and at times confusing. Below are only a few examples, though many more exist:\nvisual Reasoning(VR)Chen et al. (2023b) → missing space.\nas SGA Lu et al. (2023), Co-Attack Zhang et al. (2022a), VLATTACK Yin et al. (2024a), and TMM Wang et al. (2024a), → inconsistent and poor citation style.\na robust means to test VLM resilience Lu et al. (2023) → same citation issue.\nFor example, Figure 1 shows that the “kitchen” and “microwave” form a causal relationship... → multiple grammatical errors and incorrect verb agreement.\n\n- Beyond grammar and typography, the overall presentation and writing logic are of low quality. The Introduction contains many long, convoluted sentences and fails to clearly present the motivation, problem, importance, and proposed solution. The section reads as disorganized and hard to follow, requiring significant rewriting. The excessive use of em dashes (—) also suggests possible overreliance on LLM-generated text.\n\n- Section 2.3 on causal adversarial attacks is poorly structured. Each related work is briefly summarized without explaining how they connect to one another or to this paper’s contributions. As a result, the section reads like a collection of disconnected summaries rather than a coherent narrative situating prior work in context.\n\n- Shallow treatment of SCMs, which are central to the proposed method, is also a weakness that makes the methodology less clear. The paper does not clearly explain how these causal graphs are constructed, validated, or whether they accurately capture the model’s underlying causal dependencies. \nMoreover, several methodological details such as the implementation of causal adjacency matrices, SVD component selection, and reconstruction losses, are vague or missing, making the approach difficult to reproduce. A deeper and more transparent investigation into SCM background, validation, and sensitivity would have substantially strengthened the paper’s technical rigor and credibility.\n\n- It would have been much better if the paper had included Reproducibility and Ethics Statement sections, as strongly encouraged by ICLR, given that this work focuses on adversarial attacks and involves extensive experimental setups."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CzCnr9sSYi", "forum": "wakIgAs5ty", "replyto": "wakIgAs5ty", "signatures": ["ICLR.cc/2026/Conference/Submission15015/Reviewer_4Ybc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15015/Reviewer_4Ybc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761347230951, "cdate": 1761347230951, "tmdate": 1762925345886, "mdate": 1762925345886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CASh (Causality Alignment Shifting Attack), an adversarial attack framework targeting vision-language models (VLMs) by explicitly perturbing their latent causal alignment between image and text modalities. Unlike correlation- or co-occurrence-based attacks, CASh leverages cross-attention matrices and structural causal models (SCMs) to identify and manipulate high-impact causal relationships through singular value decomposition (SVD). The approach is demonstrated across several downstream VLM tasks (retrieval, grounding, reasoning, VQA, entailment), showing substantially improved attack success rates and notable transferability to black-box commercial models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The central idea of attacking VLMs at the level of causal alignment, rather than simply perturbing superficial co-occurrence statistics, is intellectually interesting.\nThe paper is supported by a comprehensive experimental evaluation across a wide range of tasks, models (including commercial APIs), and baselines."}, "weaknesses": {"value": "1. The entire paper is premised on these graphs capturing causal relationships, but there is no information on how the nodes (entities) are defined and how the edges (causal links) are established. Are scene graph parsers used for images and dependency parsers for text? This omission severely impacts the paper's reproducibility and makes it difficult to assess the validity of the \"causality\" claims. The authors should provide a detailed description of this process in the main paper.\n2. The overall loss function in Eq. (10) combines multiple components. However, the paper lacks an ablation study to disentangle the contribution of each loss term. It is unclear how much each component contributes to the final attack performance.\n3. The stealthiness of the attack is questionable. The adversarial example shown in Figure 2 exhibits visually obvious artifacts (e.g., the modified shirt color and background). This may limit the practical applicability of the attack in scenarios requiring imperceptible perturbations.\n4. Unclear Presentation of Results: The presentation of results is sometimes unclear. For example, in Table 1, several entries in the 'IR' columns for DS-VL2 and ViLT are bolded despite not being the best-performing results in those columns. This is misleading and should be corrected for clarity."}, "questions": {"value": "1. Could the authors provide a detailed, reproducible description of how the SCM graphs are constructed? Specifically: (a) How are nodes (e.g., entities, relations) extracted from images and text? (b) How are the directed edges representing causal dependencies determined? Is this process automated, and if so, what tools are used?\n2. Could you provide an ablation study on the different components of the loss function (Eq. 10)? What is the individual impact on the attack's success and transferability?\n3. The example in Figure 2 shows a significant visual change. Could you comment on the stealthiness of the generated perturbations?\n4. Please clarify the meaning of bolded numbers in the tables. If they are meant to indicate the best results, please ensure they are used consistently and correctly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zi2DBWQ8Os", "forum": "wakIgAs5ty", "replyto": "wakIgAs5ty", "signatures": ["ICLR.cc/2026/Conference/Submission15015/Reviewer_wv4C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15015/Reviewer_wv4C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902031127, "cdate": 1761902031127, "tmdate": 1762925345420, "mdate": 1762925345420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose CASh, a new adversarial attack on vision–language models (VLMs) that targets causal rather than correlational dependencies between images and text. By modeling each modality as a Structural Causal Model (SCM) and integrating causal relations into the cross-attention mechanism, CASh identifies and perturbs the most influential causal alignments using singular value decomposition (SVD). The approach effectively disrupts multimodal reasoning while preserving visual and textual realism. Across several VLM tasks (VQA, VE, VR, and etc), CASh consistently outperforms correlation-based baselines, showing up to 20% higher transferable attack success."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper tackles a relevant and timely topic—improving the robustness of vision–language models through developing causality-aware adversarial attacks in important and interesting.\n\n* The proposed CASh framework is conceptually novel, moving beyond correlation-based perturbations toward causal alignment modeling.\n\n* The main ideas are intuitive and well motivated, though some technical sections (especially those describing the causal regularization and SVD-based perturbation) could be explained more clearly.\n\n* The experimental evaluation is extensive and demonstrates consistent improvements in attack transferability across diverse VLMs and tasks."}, "weaknesses": {"value": "* The methodology is not always clearly explained—particularly how the causal graphs are constructed in Section 3.1. This lack of clarity makes the approach difficult to reproduce and obscures the precise contribution of each component. Moreover, since it is not entirely clear how these causality graphs are obtained or parameterized, it is unclear whether the method would still perform effectively if an alternative strategy were used to define the causal graphs.\n\n* The experimental evaluation focuses mainly on Attack Success Rate without deeper qualitative or causal analysis. Including visualizations or ablations (e.g., showing how causal alignments shift) would make the results more interpretable and convincing. It would also be important to assess the computational overhead introduced by the causal modeling and SVD-based perturbation—specifically, how much slower CASh is compared to correlation-based attacks and whether the added causal reasoning meaningfully justifies the increased latency or complexity."}, "questions": {"value": "* Could the authors clarify how the causal graphs G^I and G^T are constructed or learned in practice?\n\n* If a different approach were used to build the causal graphs, do the authors expect any change in performance? How to pick up the best approach for this? Additional ablations might be needed.\n\n* What is the computational overhead introduced by the SCM modeling and SVD-based perturbation? How does CASh’s runtime compare to correlation-based attacks in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5bXJnhjsnm", "forum": "wakIgAs5ty", "replyto": "wakIgAs5ty", "signatures": ["ICLR.cc/2026/Conference/Submission15015/Reviewer_pwfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15015/Reviewer_pwfj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168645741, "cdate": 1762168645741, "tmdate": 1762925344910, "mdate": 1762925344910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}