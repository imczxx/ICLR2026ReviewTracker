{"id": "uN5cHI3kpt", "number": 22769, "cdate": 1758335233256, "mdate": 1759896847604, "content": {"title": "Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill", "abstract": "Efficient LLM inference is critical for real-world applications, especially within heterogeneous GPU clusters commonly found in organizations and on-premise datacenters as GPU architecture rapidly evolves. Current disaggregated prefill strategies, which separate the prefill and decode stages of LLM inference across different GPUs, often suffer from suboptimal performance due to imbalances between GPU capabilities and workload demands. On the other hand, extending conventional data parallelism and pipeline parallelism to heterogeneous setups incurs high inference latencies. To address these challenges, we introduce Cronus, a novel LLM inference system designed to dynamically balance workloads across heterogeneous GPUs using partially disaggregated prefill. Cronus partitions each prefill stage and executes its initial portion on the low-end GPU, while overlapping the remaining prefill and decode stages of earlier requests on the high-end GPU. Extensive evaluations across various high-end and low-end GPU combinations demonstrate that Cronus significantly improves the throughput over disaggregated prefill. It also reduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining similar or better throughput.", "tldr": "", "keywords": ["LLM inference", "Heterogeneous Architecture"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/709dd182941bf2ca45d68790683b880d32f85597.pdf", "supplementary_material": "/attachment/6f840e5fd42c81d82534ef0ea426883bcda4114d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Cronus, a system for efficient LLM inference on heterogeneous GPU clusters, with a particular focus on configurations involving one high-end and one low-end GPU. The key proposal is \"partially disaggregated prefill\", in which the prefill stage of inference is split between the two GPUs: part of the prefill is executed on the low-end GPU, while the remainder (along with the decode phase) is handled by the high-end GPU. In addition, the authors provide a simple performance model for estimating execution time on both GPU types. Experiments compare Cronus against data parallelism, pipeline parallelism, and existing disaggregated prefill strategies using two LLMs (LLaMA3-8B and Qwen2-7B) and two GPU pairings. The results show that Cronus improves throughput as well as the 99th-percentile Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) compared to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a common problem in LLM deployment, where heterogeneous GPU clusters are utilized. The inefficiencies of conventional prefill/decode separation and parallelism methods in such settings are convincingly motivated.\n\n- The proposed partially disaggregated prefill approach is well-justified. Unlike prior works that fully assign prefill or decode to specific devices, Cronus adaptively splits and schedules prefill operations to maximize GPU utilization while balancing compute and memory constraints.\n\n- The system mechanics are described with adequate precision. The paper provides a load-balancing heuristic (Section 4.3), explicit timing models (Equations 2 and 3), and an ablation study explaining how prefill and chunked prefill times are profiled and predicted.\n\n- The evaluation is thorough, including two real LLMs of different sizes, two heterogeneous cluster setups (A100+A10 and A100+A30), and real workload traces from Azure LLM inference logs. The results show that Cronus achieves the best of both worlds, consistently demonstrating superior throughput, TTFT P99, and TBT P99 performance curves."}, "weaknesses": {"value": "- While the paper introduces linear models for prefill and chunked prefill prediction, these models are derived from limited profiling and lack deeper statistical motivation or robustness analysis. Equation (3) accounts for context length, but the effects of real-world queueing, arrival rate variance, or workload burstiness are not theoretically addressed.\n\n- The experiments rely on a single real-world trace with average input and output lengths. There is insufficient exploration of other workloads, particularly those with varying input/output ratios, highly bursty traffic, or adversarial scenarios (e.g., predominantly long outputs or very short prompts).\n\n- Cronus is validated only on a pair of high-end and low-end GPUs. The paper does not discuss scalability to larger clusters, arbitrary multi-GPU topologies, or configurations involving more than two device classes. Although the system is described as “dynamically balancing workloads,” it remains unclear how this approach generalizes to clusters with more than two heterogeneous GPUs.\n\n- The front-end (with the Balancer) must orchestrate queueing, dispatching, prediction, and transfer notifications for each request, as illustrated in Figure 1. The computational and synchronization overheads of the Balancer are not quantified. In real-world high-throughput settings, front-end coordination, especially notification handling, batch assembly, and queue state refreshing, could introduce non-negligible latency.\n\n- Minor errors: (1) Line 107 mentions \"QoE\" without explanation; (2) Line 158 contains a typo (\"reqeusts\"); (3) Line 320 lacks a definition for $R_l^D$.\n\n- Although the reviewer appreciates the useful strategy proposed for handling heterogeneous GPU settings, the paper’s format is incorrect (it lacks the statement “Under review as a conference paper at ICLR 2026” in the header). As a result, the reviewer must assign a score of 0 for formatting compliance."}, "questions": {"value": "- How would Cronus generalize to clusters with more than two GPUs or more than two types/classes of GPUs? Is the current design readily extensible to K heterogeneous devices, or are there architectural bottlenecks that limit scalability?\n\n- How robust is the Balancer to errors in its predictive models? What is the performance impact if queue statistics, chunk timing, or device throughput are inaccurately estimated?\n\n- Can the authors provide empirical or analytical evidence on the latency and computational overhead incurred by the front-end Balancer? Is this overhead negligible under high-load conditions, or could it become a bottleneck in practice?\n\n- Under high batch concurrency or network contention, have any bottlenecks been observed in partial KV-cache transfers? How does Cronus handle degraded network performance in such cases?\n\n- Have the authors evaluated Cronus under more diverse or adversarial workloads, for example, scenarios with extreme input/output length ratios, non-stationary request arrivals, or multi-tenant LLM inference (serving multiple models or user groups in parallel)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sLQAq55e3V", "forum": "uN5cHI3kpt", "replyto": "uN5cHI3kpt", "signatures": ["ICLR.cc/2026/Conference/Submission22769/Reviewer_URKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22769/Reviewer_URKn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922783207, "cdate": 1761922783207, "tmdate": 1762942379510, "mdate": 1762942379510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cronus, a serving system for heterogeneous GPU clusters that partially disaggregates prefill: a short prefix of prefill runs on a low-end GPU, then the remainder of prefill plus all decoding run on a high-end GPU, overlapping compute and KV transfers. A predictor chooses the split per request. Experiments on A100+A10 and A100+A30 with LLaMA3-8B and Qwen2-7B report higher throughput than disaggregated prefill and lower TTFT/TBT P99 than DP and PP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tries to tackle an important problem of how to improve LLM serving efficiency on heterogeneous clusters.\n\n2. There is a good articulation of the key design of the system that is based on partial disaggregation of prefill and decoding.\n\n3. The paper reports max throughput close to existing DP baselines and gains on P99 TTFT and P99 TBT compared with baselines."}, "weaknesses": {"value": "1. The paper misses a thorough technical discussion and evaluation with the baselines. Moreover, the motivation part is weak as only conceptual comparisons are given.\n\n2. It is unclear why the system chooses to support DP and PP but not TP, given that TP is so widely used in LLM serving workloads these days.\n\n3. There is no evaluation showing that the approach could scale up with a larger number of nodes in the cluster or a pool of GPUs.\n\n4. Evaluation is mainly based on a single dataset."}, "questions": {"value": "It is not clear to me both conceptually and technically why Cronus is a better approach compared to existing state-of-the-art LLM serving disaggregation methods such as Splitwise and DistServe.\nThe paper mentions that existing systems often struggle to achieve optimal performance due to a mismatch with GPU capabilities.\nHowever, existing systems are neither evaluated in the previous approach section nor in the evaluation section.\nThe previous approach section is mainly explaining conceptual weaknesses of simpler baselines such as disaggregating prefill to high-end GPUs and decode to low-end GPUs and vise versa.\nI think the baselines are an oversimplification of existing methods. \nFor example, depending on the workload, Splitwise has a mixed pool of GPU instances that runs mixed batches of prefill and decode.\nAlso, DistServe has optimizations on the batching strategy such that it controls the batch size of prefill instances to prevent making prefill more compute-bound.\nWithout actual comparisons with such methods, it is hard to evaluate the benefits of the paper.\n\nAlso, the system seems to not support TP, which is widely used in LLM serving.\nTP is shown to have better throughput performance than using PP under the same setup, since PP may introduce extra latency due to communications between stages.\nTherefore, it is a bit confusing why the system only supports and targets the optimizations on DP and PP alone but neglect TP.\n\nThe evaluation setup is limited. For the hardware setup, the design analysis and evaluation are limited to only two GPUs.\nIt is not clear how the system could scale as the number of GPUs increases. \nAlso, only one dataset Splitwise is evaluated.\nIt would be beneficial to evaluate the system performance on other datasets that have different request characteristics.\n\n\n1. Can we show evaluation results comparing with existing state-of-the-art methods such as Splitwise and DistServe?\n\n2. How can the current system handle burstiness in the workload?\n\n3. How would the system support TP and what will be the throughput and latency if TP is used?\n\n4. In Section 4.2, \"By limiting the total number of requests in the PPI to at most two at a time, ...\", what does that mean? Why is the batch size of PPI limited to 2?\n\n5. How does the system scale with a different number of GPUs or nodes? How would the system perform under other datasets with different input and output distributions that lead to different compute and memory characteristics? Can the system handle long-context workloads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xt4XjCFy69", "forum": "uN5cHI3kpt", "replyto": "uN5cHI3kpt", "signatures": ["ICLR.cc/2026/Conference/Submission22769/Reviewer_rtB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22769/Reviewer_rtB8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960477224, "cdate": 1761960477224, "tmdate": 1762942379097, "mdate": 1762942379097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Cronus, an LLM inference system for heterogeneous GPUs that partially disaggregates prefill to dynamically balance workloads. Cronus executes part of the prefill stage on low-end GPUs and the remaining prefill and decode stage on high-end GPUs, working asynchronously. Evaluation results show Cronus improves throughput and P99 TTFT and TBT over other parallelization techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting disaggregation approach for heterogeneous GPUs, backed by careful analysis"}, "weaknesses": {"value": "- No ablation study is presented. It would be interesting to show how the design choices (especially the partial prefill length) would affect the overall performance. With the current evaluation, it is unclear whether the proposed design optimally balances the workloads between weak and strong GPU instances.\n- No or little improvement in throughput compared to DP baseline (Table 2). Although it has a better P99 of TTFT/TBT, DP might be a better solution depending on the SLO requirements. For example, TTFT/TBT SLO requirements can be tens of seconds/hundreds of milliseconds [1, 2], and DP is better in those cases.\n\n### References\n\n- [1] https://arxiv.org/abs/2407.00079\n- [2] https://arxiv.org/abs/2408.12757"}, "questions": {"value": "- Is it possible to quantify the labels in Table 1 in some way? Instead of qualitative labels like small/medium/large or high/low, it would be helpful to provide some approximation.\n- Why do you model prefill times as a linear function of prefill context length (e.g., in Equation 2)? Does the computation not scale to the square of context length due to self-attention?\n- Why do you use vLLM version 0.6.1.post2, which was released more than one year ago? Will there be any difference with the newer version of vLLM, especially the V1 architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f6wWSj5BOm", "forum": "uN5cHI3kpt", "replyto": "uN5cHI3kpt", "signatures": ["ICLR.cc/2026/Conference/Submission22769/Reviewer_xTCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22769/Reviewer_xTCU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968744656, "cdate": 1761968744656, "tmdate": 1762942378821, "mdate": 1762942378821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to enhance the PD-disaggregation by executing partial of the prefill on the prefill instance and executing the remaining prefill and the decoding on the decoding instance. It aims to solve the load balance problem of the vanilla PD approach. It also studies how to estimate the execution time of prefill to make the workload more balanced between instances. Evaluation shows that this method can achieve better TTFT and TBT performance than different baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It focuses on an important problem of PD-disaggregation.\n2. It presents a different method than the vanilla PD method."}, "weaknesses": {"value": "1. The motivation is weak as for using the split prefill to solve the imbalance problem of PD.\n2. The method is relatively simple, and not practical enough for the industrial workloads and environment.\n3. The analysis of different parallel methods is not solid. And the discussion of the related work is weak.\n4. The balancer does not consider the length of the decoding."}, "questions": {"value": "1. Given that PD is usually used for the industrial scenarios for better TTFT and TBT, why not using different numbers of GPUs for prefill and decoding directly to solve the imbalance problem? Note the industrial scenario will usually serve the models with much more than 2 GPUs. This method can be easier for the deployment and maintenance.\n2. Given that the decoding length is not known ahead, even splitting the prefill on different instance, it still cannot achieve a perfect balance. How to address this problem?\n3. In figure 2, why the remaining prefill is chunked, but the first partial prefill is not?\n4. There lacks a discussion of the effort to address the imbalance problem in the related work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oJ5McBgAFa", "forum": "uN5cHI3kpt", "replyto": "uN5cHI3kpt", "signatures": ["ICLR.cc/2026/Conference/Submission22769/Reviewer_fGMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22769/Reviewer_fGMx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251000264, "cdate": 1762251000264, "tmdate": 1762942378445, "mdate": 1762942378445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}