{"id": "qHmfByRRGn", "number": 10518, "cdate": 1758174483799, "mdate": 1759897646033, "content": {"title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "abstract": "Theory of Mind (ToM)—an understanding of the mental states of others—is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.", "tldr": "", "keywords": ["Thoery of Mind", "social reasoning", "LLM agents"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87066fb659c48e027b59019bbe6689e34f642c7c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ToMAgent (TOMA), a framework designed to enhance the social intelligence of Large Language Models (LLMs) by explicitly integrating Theory of Mind (ToM). The core of ToMA is a look-ahead training method. The target agent generates multiple mental states-based  conversation candidates and simulates the outcome. The utterance with high score are used as data for finetuning. The method is evaluated on the Sotopia benchmark, analyses indicate that TOMA achieves better outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- TOMA outperforms base models and simple prompting baselines across metrics (Goal achievement, Relationship, and Knowledge).\n- The comprehensive analyses provide insights in model behaviours."}, "weaknesses": {"value": "- There is no evaluation of whether the generated mental states are correct or accurate, the performance improvement could come from better goal-oriented planning, not necessarily from correct prediction of mental states.\n- The intuition behind selecting the five specific mental state dimensions is not justified. The paper does not explain why this particular set was chosen or if alternatives were considered, leaving the design choices unclear.\n- The selection of LLM evaluator could have limitations in alignment with human evaluation, reproducibility and consistency.\n- Baselines are limited to the Qwen family. The evaluation does not robustly demonstrate TOMA's effectiveness across different model family.\n- How each specific type of mental state contributes to performance is unclear."}, "questions": {"value": "- For different types of scenarios, different mental states may contribute to the goal differently. An ablation study on the mental state dimensions would be highly valuable to understand the how mental states contributes to TOMA's success.\n- Why not use the same LLM evaluator in Sotopia-Eval, does GPT-5-mini also align with human evaluation?\n- If multiple LLMs are used as the evaluator, will the evaluation results be consistent? \n- For a single LLM evaluator, can the results be reproduced? (e.g. prompt evaluation with the same data multiple times?) How sensitive is the results to the prompt?\n- Using GPT-5-nano both as baseline and evaluator could have bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WcgCuCBNZH", "forum": "qHmfByRRGn", "replyto": "qHmfByRRGn", "signatures": ["ICLR.cc/2026/Conference/Submission10518/Reviewer_yVB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10518/Reviewer_yVB1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521535702, "cdate": 1761521535702, "tmdate": 1762921801411, "mdate": 1762921801411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToMAgent (ToMA), a training framework that explicitly models Theory of Mind (ToM) as a latent textual variable during dialogue. From Sotopia contexts, the method samples multiple mental-state hypotheses and paired utterances, runs short rollouts with a partner model, scores outcomes with an LLM judge, retains high-utility (mental state, utterance) pairs, and then SFTs the target model to jointly predict mental states and utterances. On Sotopia (all/hard) with Qwen2.5 3B/7B, ToMA improves Goal/Relationship/Knowledge over baselines (vanilla, mental-states-only, utterances-only, and a Base+MS prompting variant). Analyses report longer-horizon adaptation, increased first-order mental-state generation, and more strategic behaviors (compromise/solution offering). The approach is simple, practical, and relatively compute-efficient (LoRA)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Practical idea with clear recipe:** Treating ToM as a latent text variable and SFT-ing on selected (mental state, utterance) pairs is simple, reproducible with open models, and lowers inference-time cost vs. hypothesis exploration at test time.\n* **Consistent empirical gains:** Improvements hold across sizes (3B/7B) and evaluation metrics (Goal/Rel/Know), with notable boosts on the Relationship dimension which is important for social agents.\n* **Thoughtful qualitative/diagnostic analysis:** Long-horizon behavior, partner-size sensitivity, strategy labeling (success/failure), and mental-state distributions provide insight into *how* the method changes behavior."}, "weaknesses": {"value": "* **Judge and self-play dependence:** Results and data selection rely on proprietary **LLM judges** and mainly **self-play**, risking judge overfitting and intra-policy conventions. No inter-judge agreement, open-judge replication, or human evaluation is provided.\n* **Insufficient ablations/sensitivity:** Key design choices (K/J candidates, rollout length, threshold 9, selection objective using averaged scores) are not systematically ablated; uncertainty estimates (CIs, significance tests) are missing.\n* **Safety/external validity claims:** Relationship scores from LAaJ are not a proxy for safety or fairness; no human ratings of coercion/manipulation, nor tests with human partners or out-of-domain dialogues."}, "questions": {"value": "1. **Overlap control:** Are any Sotopia-π scenarios (or near paraphrases) present in the evaluation splits? Please provide an overlap audit and, ideally, a clean re-split with zero overlap.\n2. **Judge robustness:** How do results change with (a) a different proprietary judge, (b) an open-weights judge, and (c) 2-judge majority voting? \n3. **Ablations:** Vary (K), (J), rollout horizon, the **score threshold**, and the **selection objective** (target-only, average, Pareto). Do gains persist?\n4. **Human and safety eval:** Include a small human study rating rapport/pressure/manipulation and standardized social-harm probes (e.g., refusal to escalate violence); report failure rates."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DHfwdbmY34", "forum": "qHmfByRRGn", "replyto": "qHmfByRRGn", "signatures": ["ICLR.cc/2026/Conference/Submission10518/Reviewer_iscP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10518/Reviewer_iscP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941128433, "cdate": 1761941128433, "tmdate": 1762921800984, "mdate": 1762921800984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToMAgent (TOMA), a training pipeline designed to infuse Theory of Mind (ToM) reasoning into large language model (LLM) agents for socially intelligent dialogue. At each dialogue turn, the model first generates multiple hypotheses about the partner’s potential mental states (e.g., beliefs, desires, intentions), then produces corresponding candidate utterances. The system simulates rollouts of these candidate interactions and retains only those that successfully achieve the social goal. These high-utility pairs of mental states and utterances are then used to fine-tune the model jointly on both tasks — predicting the partner’s mental states and generating appropriate responses.\n\nExperiments on the Sotopia-Pi social reasoning benchmark show that ToMAgent significantly improves goal achievement, relationship maintenance, and long-horizon reasoning compared to baseline and ablated models. The authors also provide extensive analyses of conversational strategies, scenario types (cooperation, negotiation, persuasion, conflict), and mental-state distributions to explain the model’s behavior."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts a detailed examination of model behavior across different conversation types and success/failure strategies, as well as distributions of generated mental states. This provides valuable interpretability and insight into how ToM reasoning shapes dialogue outcomes.\n\nThe look-ahead training pipeline combining mental-state reasoning and goal-based simulation offers an elegant alternative to purely reinforcement-learning or prompt-based approaches."}, "weaknesses": {"value": "1. Clarification of Training and Simulation Setup:\n\n- Which backbone LLM is used to generate ToM hypotheses and simulate dialogues (LM_target vs. LM_partner)? Does the choice of this model affect the quality of the collected data and fine-tuning outcomes?\n\n- During rollouts, are new mental states generated at every turn, or only at the initial turn? Is this consistent with inference behavior after fine-tuning?\n\n- How sensitive is the model to the number of mental-state and utterance samples (K and J)? An ablation varying these hyperparameters would clarify the trade-off between diversity and efficiency.\n\n- Since the rollout mechanism directly influences training data quality, it would be helpful to compare performance with and without simulation-based filtering (i.e., random sampling of mental-state–utterance pairs).\n\n- A comparison with a prompt-only baseline — e.g., prompting the base model to first reason about mental states (CoT) and then generate utterances, without fine-tuning — would isolate the effect of supervised ToM integration.\n\n2. Although Sotopia and GPT-5-mini (LLM-as-a-Judge) are widely used, the evaluation remains automated and subjective. Human or inter-rater validation would make the social intelligence claims more convincing.\n\n3. The paper should cite MindDial: Enhancing conversational agents with theory-of-mind for common ground alignment and negotiation (SIGDIAL 2024), which also introduces ToM-guided dialogue reasoning and negotiation."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ru0wsFAa6O", "forum": "qHmfByRRGn", "replyto": "qHmfByRRGn", "signatures": ["ICLR.cc/2026/Conference/Submission10518/Reviewer_WHSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10518/Reviewer_WHSg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957107688, "cdate": 1761957107688, "tmdate": 1762921799864, "mdate": 1762921799864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TOMAgent (ToMA), a Theory of Mind-focused prompting and training framework for dialogue agents. Experiments are conducted in Sotopia, an LLM-based environment simulating realistic social interactions. The core idea is to explicitly prompt LLMs to generate a mental state representation before producing each utterance or action.\n\nFor training, the authors perform supervised fine-tuning (SFT) using short-horizon rollouts sampled under diverse mental state generations, filtering trajectories with high judge scores as training data. The framework models both mental state generation P(m \\mid H) and utterance generation P(u \\mid m, H) as learning objectives, and trains LLMs jointly on the combined and mental-state-only behaviors.\n\nEmpirically, ToMA outperforms baselines on relationship, knowledge, and goal achievement metrics evaluated by an LLM judge. Moreover, ToM-augmented training shows favorable properties such as preserving goal completion performance over longer horizons and improving not only the focal agent’s outcomes but also those of its partner agents. The paper further provides an extensive analysis of ToMA’s behavior across scenario types and success strategies, highlighting that ToM guidance encourages more strategic reasoning and activeness during interactions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper investigates the effectiveness of incorporating explicit Theory of Mind (ToM) mechanisms into dialogue agents for social interaction tasks. It demonstrates that training LLM agents to generate high-quality ToM tokens as auxiliary outputs improves their goal achievement, knowledge acquiring, and relationship building performance during social interactions. This work represents the first systematic exploration of explicit ToM strategies in realistic multi-agent social settings, effectively bridging important topics in the community.\n\nI appreciate the paper’s contribution in both methods for training socially intelligent agents and emphasizing the evaluation of collaborative behaviors, which is highly relevant to present challenges in multi-agent collaboration, AI safety, and human AI collaboration. The paper is properly written with clear motivation, organization, and visualizations that effectively support its claims.\n\nMoreover, compared to prior work on socially intelligent agents, this paper provides more comprehensive and detailed analysis of interaction dynamics. I found the categorization of scenario types and strategies to be potentially useful taxonomy for granular analysis of social behaviors. Similarly, the in depth analysis of success and failure modes in relation to agent's strategic reasoning, activeness, and generated mental states provides valuable insight into how explicit ToMA changes social behaviors."}, "weaknesses": {"value": "Despite the originality of exploring explicit Theory of Mind (ToM) strategy injection for social interactions, the paper’s methodological novelty is somewhat limited. Prior works on the same benchmark have already explored similar approaches, such as explicit strategy injection and the elicitation of social reasoning. As a result, the proposed method can be viewed as an extension or specific instantiation of these earlier techniques rather than a fundamentally new training strategy.\n\nThe paper’s completeness is also constrained by several experimental design choices:\n\nTraining data: Positive mental states are curated using short-horizon rollouts (four turns) instead of full dialogue trajectories. While this filtering is reasonable, it would be more robust to include results trained on full rollouts for a comprehensive comparison.\n\nTraining objectives: The paper models mental state generation P(m \\mid H) and utterance generation P(u \\mid m, H) as two objectives, training LLMs jointly on both the combined and the mental-state-only objectives. However, results show that training solely on P(m \\mid H) does not improve baseline performance, which is counterintuitive to me given they claim the main contribution as equipping Theory of Mind to improve social reasoning. Furthermore, this discrepancy might suggest that the model is not faithful to its own mental state estimates, which make deeper analysis even more imperative. It would also be informative to include experiments that isolate training on P(u \\mid m, H) while maintaining the original distribution of P(m \\mid H).\n\nEvaluation: The paper only reports results on three social dimensions from Sotopia, even though the benchmark defines seven, including important aspects such as secret-keeping and adherence to social rules. Moreover, the benchmark recommends using the overall averaged score as the core evaluation metric. While I agree that focusing on goal, knowledge, and relationship dimensions provides valuable insights, reporting the full set of metrics would ensure completeness and allow for fairer comparisons with prior work.\n\nLLM Judge Setup: Lastly, the evaluation relies solely on an LLM-based judge (GPT-5-mini). Although the training and evaluation judges are different, depending on a single automated judge limits the reliability of the results. Incorporating multiple LLM judges or including human evaluation would significantly strengthen the validity of the findings."}, "questions": {"value": "What is the intuition behind training P(m \\mid H) and P(u \\mid m, H) separately rather than directly modeling P(u, m \\mid H)? Most existing reasoning and social interaction methods train the joint distribution. Was P(u, m \\mid H) tested as a baseline? Given that training on P(m \\mid H) alone did not improve results, this may suggest that performance gains come mainly from the conditional generation rather than explicit mental state modeling.\n\nWhat motivated the choice of a four-turn rollout horizon for sampling positive trajectories? How sensitive are the results to the rollout length? Since goal achievement in social interactions is often non-linear and requires back-and-forth exchanges, treating the horizon as a tunable rather than fixed parameter might reveal different dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XNm33pJbqV", "forum": "qHmfByRRGn", "replyto": "qHmfByRRGn", "signatures": ["ICLR.cc/2026/Conference/Submission10518/Reviewer_QMVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10518/Reviewer_QMVM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973116531, "cdate": 1761973116531, "tmdate": 1762921799326, "mdate": 1762921799326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}