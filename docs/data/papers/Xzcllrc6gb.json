{"id": "Xzcllrc6gb", "number": 2786, "cdate": 1757249397732, "mdate": 1759898127331, "content": {"title": "Quantized Visual Geometry Grounded Transformer", "abstract": "Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have achieved remarkable progress with large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has emerged as a common practice to compress and accelerate models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. \nThis paper proposes the first **Quant**ization framework for **VGGT**s, namely **QuantVGGT**. This mainly relies on two technical contributions: First, we introduce *Dual-Smoothed Fine-Grained Quantization*, which integrates pre-global Hadamard rotation and post-local channel smoothing to robustly mitigate heavy-tailed distributions and inter-channel variance. Second, we design *Noise-Filtered Diverse Sampling*, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges.\nComprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with a great margin.\nWe highlight that our 4-bit QuantVGGT can deliver a **3.7$\\times$** memory reduction and **2.5$\\times$** acceleration in real-hardware inference, while preserving over **98\\%** reconstruction accuracy of the full-precision counterparts. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios.", "tldr": "", "keywords": ["Geometry Grounded", "Model Quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf86e4d1b5e2f2dab165a214cefcb515281a7f3b.pdf", "supplementary_material": "/attachment/90896b8330987a7347292a9a5489b1428b67592e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed QuantVGGT, the first comprehensive post-training quantization framework designed for visual geometry grounded transformer. QuantVGGT consists of two components: Dual-Smoothed Fine-Grained Quantization architecture for smoothing the distribution for better quantization performance from both global and local perspective and Noise-Filtered Diverse Sampling strategy for constructing information-maximized calibration dataset. Extensive experiments on multiple tasks compared with different strong baseline methods shown that QuantVGGT greatly outperforms the compared methods and achieves SOTA performance across different bit-width and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of QuantVGGT is clear and quite meaningful, as a systematic quantization research on large-scale 3D foundational models in is heavily unexplored and crucial.\n\n2. The proposed method is supported by extensive empirical analysis and theoretical theorems, and appears to effectively solve the corresponding problems.\n\n3. The paper writing is very clear and accompanied by key illustration figures and method flowcharts, which are easy to understand and implement.\n\n4. The paper conducted high bit (8-bit) and low bit (4-bit) experiments on multiple task datasets, significantly surpassing strong baseline methods from different fields, demonstrating the effectiveness and generalization of QuantVGGT.\n\n5. QuantVGGT reported the true efficiency in hardware, calibration consumption, various detailed ablation experiments, and extensive experimental analysis. This fully demonstrates the effectiveness of its method and the effectiveness of acceleration in real-world scenarios."}, "weaknesses": {"value": "1. The symbol writing on line 192 seems to be incorrect. And for the compared method in line 377, I believe it should be spelled QuaRot.\n\n2. Providing acceleration effects at different sequence lengths will further enhance its practicality in different scenarios.\n\n3. Adding different comparison methods reconstruction visualization will better demonstrate the visual effectiveness of the proposed methods."}, "questions": {"value": "1. The symbol writing on line 192 seems to be incorrect. And for the compared method in line 377, I believe it should be spelled QuaRot.\n\n2. Providing acceleration effects at different sequence lengths will further enhance its practicality in different scenarios.\n\n3. Adding different comparison methods reconstruction visualization will better demonstrate the visual effectiveness of the proposed methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PYR35MZkPC", "forum": "Xzcllrc6gb", "replyto": "Xzcllrc6gb", "signatures": ["ICLR.cc/2026/Conference/Submission2786/Reviewer_9UAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2786/Reviewer_9UAK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559158132, "cdate": 1761559158132, "tmdate": 1762916378423, "mdate": 1762916378423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission presents QuantVGGT, the first Post-Training Quantization (PTQ) framework for Visual Geometry Grounded Transformer (VGGT), a state-of-the-art model for learning-based 3D reconstruction. The work addresses two critical challenges in VGGT quantization: (1) heavy-tailed activation distributions induced by data-independent camera and register tokens, and (2) unrepresentative calibration dataset building due to the multi-view complexity of 3D data.\nFor challenge 1, the authors propose Dual-Smoothed Fine-Grained Quantization (DSFQ), which consists of a pre-global Hadamard rotation for dispersing outliers and smoothing heavy tails, a post-local channel scaling for normalizing inter-channel variance. Furthermore, the authors adopt token-wise quantization for activation and outer-dimension-wise quantization for weight, forming fine-grained quantization, which leads to minor quantization error.\nFor challenge 2, the authors propose Noise-Filtered Diverse Sampling (NFDS), which filters noisy outliers using deep-layer activation statistics and constructs frame-aware calibration clusters concerning VGGT’s inductive bias of relationships between first-frame and subsequent-frames.\nComprehensive experiments on Co3Dv2 for camera pose estimation and DTU for point map estimation demonstrate that, QuantVGGT outperforms existing quantization methods across various bit-widths both in theoretical validation and real-hardware deployment."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Target model quantization on VGGT, a new SOTA 3D reconstruction model, analyzing concrete experimental phenomena, heavy-tailed activation distribution and unrepresentative calibration dataset, and proposing corresponding observation-driven solutions, DSFQ and NFDS.\n\n2. Validation: Detailed mathematical theoretical derivations and justifications for technical innovations. Comprehensive experiments with additional ablation studies across tasks, quantization bit-widths, comparison methods, and proposed components.\n\n3. Clarity: Hierarchical narrative logic, clear writing, well-organized sections, and informative visualizations make complex concepts (e.g., Hadamard rotation and frame-aware sampling) accessible to readers from both 3D vision and quantization backgrounds.\n\n4. Significance: Bridge the gap between large-scale 3D reconstruction model performance and deployment efficiency, with results that are both scientifically influential and practically useful, which advances model quantization in 3D field and enables edge 3D reconstruction."}, "weaknesses": {"value": "1. Insufficient Related Works: Not involve other works on VGGT acceleration and optimization, such as FastVGGT using token compression and FasterVGGT using sparse attention, let alone comparison with these methods in Sec 4 to highlight the efficiency of QuantVGGT.\n\n2. Rough Analysis of Special Tokens: The paper mentions that data-independent special tokens cause heavy tails, but does not explicitly and concretely state the respective influences of camera tokens and register tokens.\n\n3. Experiment Generalization:\n(1) Lack of experiments on the widely adopted W4A8 quantization configuration to enable more comprehensive comparisons with existing methods.\n(2) Lack of experiments on more tasks (e.g., Multi-view Depth Estimation on DTU, Image Matching on ScanNet-1500) and datasets (e.g., Camera Pose Estimation on RealEstate10K, Point Map Estimation on ETH3D) compared with the original VGGT paper.\n\n4. Experiment Credibility:\n(1) In Sec 3, the authors claim that VGGT has an inductive bias for modeling relationships between the first frame and subsequent frames. From the theoretical perspective, this phenomenon perhaps originates from the unique design of VGGT’s two distinct sets of special tokens. However, the paper barely supply sufficient experimental evidence to reflect it.\n(2) Activation Distribution only in two adjacent blocks (frame/global block 7/8) in Appendix D fail to confirm the claimed ubiquity of the heavy-tailed phenomenon across different layers.\n(3) Lack of more subjective visualization results in Appendix H in across more scenarios, including comparisons between the FP16 baseline, QuantVGGT, and other quantization methods under various quantization settings."}, "questions": {"value": "1. For frame-aware clustering in NFDS, the correlation vector c^i measures similarity between the first frame and subsequent frames. How does this strategy perform on real-world 3D sequences where the first frame is an outlier (e.g., occluded or low-light)?\n\n2. Why does the QuantVGGT in W8A8 outperform the FP16 baseline in Camera Pose Estimation on Co3Dv2? Please analyze the phenomenon and clarify the potential causes. Moreover, provide a reasonable explanation for the marginal performance difference between QuantVGGT and QuaRot under W8A8 setting.\n\n3. Is there a possibility of deploying QAT or extremely low-bit PTQ in VGGT? How should the quantization strategies be adapted or specially designed for these scenarios respectively?\n\n4. Have you ever studied whether other 3D reconstruction models (e.g., DUSt3R, MASt3R) also have similar phenomenon of heavy-tailed activation distribution and unstable calibration? Would DSFQ and NFDS perform well on them?\n\n5. Presentation and Visualization Suggestions: \n(1) Supplement brief explanations of variables R_j^*, V_j^*, and V^* directly in Theorem 3.2, and add a hyperlink to Appendix A immediately for quick reference to related details.\n(2) Consider enlarging the size of Fig. 4 (a) and supplementing the specific attributes (labels or features) corresponding to each cluster in Fig. 4 (b)(c)(d) in Appendix E to improve readability.\n(3) Include a concise introduction to \"global robust moments\" before Eq. 9.\n(4) Give an explicit definition of \"static quantization\" and \"dynamic quantization\" and clarify whether \"tensor-wise\" and \"token-wise\" refer to activation quantization specifically.\n(5) Merge Table 6 in Appendix D into Table 3 of Sec 4.3 to avoid redundant presentation.\n(6) Provide detailed hardware platform information together with absolute latency and optimize the presentation format of Fig. 6.\n(7) Consider consolidating foundational knowledge of rotation-based quantization (e.g., Hadamard transform) and migration-factor-based quantization (e.g., SmoothQuant) into Sec 3.1 (Preliminary) for better logical coherence.\n(8) Add optional ablation studies for hyperparameters, including the filtering threshold T (or p), the cluster number K and the calibration samples scale N."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cbhp0SE8Ck", "forum": "Xzcllrc6gb", "replyto": "Xzcllrc6gb", "signatures": ["ICLR.cc/2026/Conference/Submission2786/Reviewer_7NMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2786/Reviewer_7NMK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816688635, "cdate": 1761816688635, "tmdate": 1762916378138, "mdate": 1762916378138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QuantVGGT, the first post-training quantization framework specifically designed for billion-parameter Visual Geometry Grounded Transformers (VGGTs), tackling two unique challenges in 3D reconstruction: (1) Dual-Smoothed Fine-Grained Quantization (DSFQ) employs a pre-global Hadamard rotation to disperse heavy-tailed activations from data-independent tokens, followed by post-local channel smoothing to stabilize variance without runtime overhead; and (2) Noise-Filtered Diverse Sampling (NFDS) filters outliers using deep-layer statistics and constructs calibration clusters based on VGGT’s intrinsic frame-relative geometric bias, ensuring representative sampling. The approach achieves state-of-the-art results under 4-bit quantization, preserving over 98% of full-precision accuracy while delivering 2.5× speedup and 3.7× memory compression—enabling practical deployment of large 3D vision models in resource-constrained environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: Novel and Insightful Problem Formulation. The paper's core originality lies not in inventing a new algorithm, but in being the first to identify and formalize the unique quantization challenges specific to 3D vision transformers—namely, the instability caused by data-independent special tokens and multi-view calibration. This re-framing of the problem is a significant contribution in itself.\n\nS2: High-Quality and Tailored Technical Solutions. The proposed methods (DSFQ and NFDS) are technically robust and specifically tailored to the identified problems. In particular, NFDS cleverly leverages the model's own geometric inductive bias for sampling, demonstrating a deep understanding of the architecture that goes beyond generic quantization techniques.\n\nS3: High Practical Significance and Impact. The work's significance is substantial, as it bridges the gap between powerful, large-scale 3D models and real-world deployment. Achieving 98% accuracy at 4-bit with significant memory and latency reduction makes these models viable for edge devices, unlocking new possibilities in robotics, AR/VR, and other resource-constrained applications.\n\nS4: Excellent Clarity and Presentation. The paper is exceptionally clear and well-written. The use of informative figures to visualize abstract concepts like activation distributions, combined with intuitive and rigorous explanations, makes the complex contributions accessible and highly convincing."}, "weaknesses": {"value": "W1: Lack of Real-World Hardware Benchmarks. The reported latency and memory gains are theoretical. The paper lacks empirical validation on actual hardware using standard inference engines (e.g., TensorRT, ONNX Runtime), making it difficult to confirm if the claimed speedups translate to real-world deployment.\n\nW2: Unclear Sensitivity to Calibration Data Size. The robustness of the method to the number of calibration samples is not explored. An ablation study on performance with significantly fewer samples (e.g., 10 or 20) is missing, which is critical for understanding its practicality in data-scarce scenarios.\n\nW3: Limited Generalization to Other Architectures. The evaluation is limited exclusively to the VGGT architecture. It remains unclear if the proposed solutions are generalizable to other 3D vision transformers (e.g., DUSt3R, MASt3R), which limits the broader impact of the work.\n\nW4: Missing Comparison to Quantization-Aware Training (QAT). The paper does not include a comparison to QAT. A QAT baseline, even with minimal fine-tuning, would provide crucial context on the absolute performance ceiling and help better evaluate the effectiveness of this PTQ-only approach."}, "questions": {"value": "Q1: How does performance degrade as the calibration set size is significantly reduced (e.g., to 20, 10, or 5 samples)?\n\nQ2: In the extreme low-data regime (e.g., <10 samples), does your NFDS method maintain superior stability (lower variance) compared to random sampling?\n\nQ3: How specific are the identified quantization challenges to VGGT? Could the core principles of DSFQ and NFDS be applied to other 3D vision transformers like DUSt3R or MASt3R?\n\nQ4: Could you clarify what \"random Hadamard matrix\" means? How was it generated, and is the model's performance sensitive to the specific matrix chosen?\n\nQ5: Is the remaining performance gap at W4A4 a fundamental limitation of PTQ for this model? How would it compare against a minimally fine-tuned QAT baseline (e.g., after 1 epoch)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hAcXFy97oV", "forum": "Xzcllrc6gb", "replyto": "Xzcllrc6gb", "signatures": ["ICLR.cc/2026/Conference/Submission2786/Reviewer_o5ar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2786/Reviewer_o5ar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909432294, "cdate": 1761909432294, "tmdate": 1762916376099, "mdate": 1762916376099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents QuantVGGT, the first post-training quantization (PTQ) framework specifically designed for Visual Geometry Grounded Transformers (VGGTs) they propose:\nDual-Smoothed Fine-Grained Quantization (DSFQ) — applying a Hadamard pre-rotation and channel-wise post-smoothing to reduce outlier sensitivity and inter-channel variance.\nNoise-Filtered Diverse Sampling (NFDS) — filtering outliers using deep-layer activation statistics and building frame-aware diverse calibration clusters for robust calibration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is the first quantization framework tailored for billion-scale 3D transformers, a meaningful step for deploying 3D vision foundation models efficiently.\n\nAblation studies clearly isolate contributions from DSFQ and NFDS.\n\nQuantizing VGGT can significantly lower compute/memory costs for 3D perception and reconstruction in real-world systems"}, "weaknesses": {"value": "Only two datasets (Co3Dv2, DTU) are used. Broader testing on outdoor scenes could strengthen claims of generality.\n\nThe DSFQ pipeline introduces additional preprocessing (Hadamard transform, smoothing, and clustering). The cost and integration details for deployment (e.g., on mobile devices) are not deeply analyzed.\n\nThe paper only focuses on PTQ, so it’s unclear how much performance could be recovered with small-scale fine-tuning.\n\nTheorem 3.2 is theoretically elegant but its real contribution to calibration robustness could be better demonstrated through comparative sampling visualizations or empirical sensitivity analysis."}, "questions": {"value": "Do the authors plan to further validate the robustness of their method on more diverse 3D datasets (e.g., ScanNet, DL3DV, or Tanks and Temples)?\n\n\nHow does QuantVGGT perform under mixed precision or asymmetric quantization setups?\n\nHow sensitive is QuantVGGT to the choice of Hadamard matrix and smoothing coefficient α?\nWould small perturbations in these hyperparameters impact stability?\n\nDoes the method generalize to other architectures such as DUSt3R or diffusion-based 3D transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4dJGKMcxRk", "forum": "Xzcllrc6gb", "replyto": "Xzcllrc6gb", "signatures": ["ICLR.cc/2026/Conference/Submission2786/Reviewer_48HL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2786/Reviewer_48HL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962590137, "cdate": 1761962590137, "tmdate": 1762916375666, "mdate": 1762916375666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles PTQ of billion‑parameter VGGT, a recent 3D reconstruction backbone. The authors diagnose two obstacles for VGGT PTQ: (i) heavy‑tailed activations induced by data‑independent special tokens (camera & register tokens), and (ii) unstable calibration due to the multi‑view nature of 3D inputs. They propose Dual‑Smoothed Fine‑Grained Quantization (DSFQ) and Noise‑Filtered Diverse Sampling (NFDS) to address the issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides convincing evidence that VGGT’s data‑independent registration tokens produce heavy‑tailed, high‑variance channels that break naïve PTQ; the visualizations in Fig. 3 and Fig. 7 make this concrete. \n2. DSFQ is well‑motivated: Hadamard rotation preserves matmul but spreads outliers, and the subsequent channel scale is computed after rotation, avoiding pre‑scale instability. The choice of token‑wise activation is sensible for transformer matmuls and validated in Table 5.\n3. Strong empirical results at low bit‑widths. W4A4 QuantVGGT clearly surpassing generic baselines."}, "weaknesses": {"value": "1. The baseline set is strong (GPTQ, SmoothQuant, QuaRot, DopQ‑ViT), but some competitive activation‑aware baselines (e.g., AWQ‑style or ViT‑specific PTQ variants) are not included; also unclear if all baselines received equal calibration size/selection tailored to 3D sequences.\n2. External validity beyond VGGT. Claims focus on VGGT‑1B and VGGT is a great model. However, it is unclear whether the observations and designs can translate to other 3D backbones."}, "questions": {"value": "Have you tried QuantVGGT on models without VGGT’s special tokens, eg. Fast3R? Does NFDS remain effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YK3xGDHxtA", "forum": "Xzcllrc6gb", "replyto": "Xzcllrc6gb", "signatures": ["ICLR.cc/2026/Conference/Submission2786/Reviewer_KMES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2786/Reviewer_KMES"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977061235, "cdate": 1761977061235, "tmdate": 1762916375350, "mdate": 1762916375350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}