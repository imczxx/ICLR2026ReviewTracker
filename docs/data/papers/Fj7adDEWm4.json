{"id": "Fj7adDEWm4", "number": 21016, "cdate": 1758312855481, "mdate": 1759896946932, "content": {"title": "TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law", "abstract": "As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps: strong generalist models can meet basic expectations, whereas domain-specialized models often fail. This highlights an urgent need for more robust safeguards in high-stakes domains. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released.", "tldr": "Benchmarking LLM safety in Finance, Medicine, and Law", "keywords": ["Safety", "Benchmark", "Large language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bde8cbeae70ac438c261af346a6f25640a30b12.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new benchmark suite for evaluating LLM safety in high-stakes domains: finance, medicine, and law.\n\nThe contribution is mainly the dataset itself, which is made by systematically constructing harmful prompts and assessing LLM outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important setting in which to build new benchmark datasets.\n\nDataset certainly seems different from the many other benchmarks that already exist in related areas."}, "weaknesses": {"value": "-- (1) Even though the domains are high-stakes, I don't fully understand what makes *evaluation* in these domains different from other benchmark datasets. Yes, they're higher stakes, but are the properties of the dataset (need for memory, reasoning, etc.) really all that different from other datasets? Perhaps more argument could be provided here.\n\n-- (2) The construction of the dataset is perhaps fairly limited. \"Dangerous prompts\" does not necessarily measure an LLMs ability to act dangerously. Indeed, I don't think a user explicitly asking for the law to be broken is the most realistic test for an LLM. What about models that give dangerous advice to more innocuous-seeming queries? The benchmark seems unable to test more realistic or nuanced use-cases and instead seems limited to more \"obviously dangerous\" scenarios (even though existing models admittedly do some to struggle on these tasks to some extent)\n\n-- Ultimately the dataset is a dataset of unsafe prompts. Perhaps still a reasonable contribution but less of a comprehensive evaluation of LLMs in critical domains."}, "questions": {"value": "Can you say more about the expert selection process and how expert oversight was used? Perhaps this was buried in the appendix somewhere but I missed it in the main text.\n\nAlso please respond to specific questions (1) and (2) listed among weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Vjq5A72vAM", "forum": "Fj7adDEWm4", "replyto": "Fj7adDEWm4", "signatures": ["ICLR.cc/2026/Conference/Submission21016/Reviewer_nTtW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21016/Reviewer_nTtW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760569224037, "cdate": 1760569224037, "tmdate": 1763000001628, "mdate": 1763000001628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Trident-Bench, a benchmark for evaluating LLM safety in high-stakes expert domains: finance, medicine, and law. The authors construct the benchmark based on alignment with professional codes in these domains. The authors conduct experiments across general-purpose,domain-specific, and safety-aligned LLMs and reveal useful insights for future developers on building robust and safe LLMs in these high-stakes domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic being studied is of high importance: the three domains are high-stakes yet less studied in the current community. Building effective, robust, and safe models in these domains is a critical challenge. \n\n2. The benchmark is a great resource contribution for the community for evaluating and developing future LLMs in these high-importance domains. \n\n3. The benchmark construction process involves extensive domain expert collaborations, which ensures the professionalism and high quality of the benchmark.\n\n4. The analysis and the experiments for current LLMs can provide valuable insights for developers, practitioners, and stakeholders."}, "weaknesses": {"value": "1. The choice of professional codes is the foundation for this framework. Does the choice of the three sets of principles come from collaboration with domain experts? How can we know if these principles ensure a good coverage of all the use cases? \n\n2. For the harmful prompt generation, how do you ensure the generated prompts cover all the principles? Based on the current description of the generation process, it's more like the prompts are first generated and then filtered using alignments with the principles. So I'm wondering if this procedure can ensure good coverage. \n\n3. The evaluation heavily relies on LLMs, and the human evaluation only includes 25 examples in the finance domain. The sample size is too small, and there are no results for the other two domains. This raises concerns for the reliability of the evaluation. \n\n4. More details about the domain experts should be presented, i.e., what are the backgrounds of the experts for each domain? How do you pay the experts? \n\n5. The writing is a bit redundant and fragmented, like for the description of the principles and dataset construction. These parts could be restructured to be clearer."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nu96NuM6ip", "forum": "Fj7adDEWm4", "replyto": "Fj7adDEWm4", "signatures": ["ICLR.cc/2026/Conference/Submission21016/Reviewer_8Awp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21016/Reviewer_8Awp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864230936, "cdate": 1761864230936, "tmdate": 1762940498340, "mdate": 1762940498340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the domain-specific safety behavior of LLMs in high-risk professional domains such as law, finance, and medicine. It identifies a key limitation of existing research, most benchmarks focus on accuracy and factual knowledge but overlook compliance with professional ethical principles. To address this, the authors propose Trident-Bench, a new benchmark grounded in real-world ethical codes from the American Medical Association, the American Bar Association, and the CFA Institute. The benchmark includes over 2,600 harmful prompts verified by domain experts to test whether LLMs recognize and refuse unsafe requests. Experiments across 19 models reveal that even domain-specialized LLMs often fail to meet ethical safety standards, while safety-aligned models like LLaMA Guard exhibit strong refusal behavior. The work offers a systematic resource for assessing LLM safety in regulated domains and highlights the importance of ethical alignment beyond factual performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper addresses a timely and underexplored problem of evaluating the LLM ethical safety in high-stakes fields.\n2.\tThe benchmark is built on authoritative ethical codes (AMA, ABA, CFA), ensuring that its grounding in real-world principles is robust and credible.\n3.\tThe multi-stage, expert-verified annotation pipeline adds strong methodological rigor, with unanimous expert agreement enhancing the benchmarkâ€™s precision and trustworthiness.\n4.\tThe evaluation covers a diverse range of models, yielding comprehensive insights into current strengths and vulnerabilities of LLMs across settings."}, "weaknesses": {"value": "1.\tUnsafe behavior often arises in evolving conversations, missing consideration of these cases can limit the utility.\n2.\tThe total-refusal evaluation design might overemphasize binary refusal behavior rather than nuanced ethical reasoning or contextual understanding of safe alternatives.\n3.\tAlthough expert validation is emphasized, the annotation cost and scalability (>$18k) may make it difficult to reproduce or expand the dataset, restricting long-term accessibility."}, "questions": {"value": "1. Except for those harmful prompts, does the Trident-Bench consider mixed or ambiguous cases?\n2. How were the professional principles translated into concrete prompts, were there standardized templates or heuristic mappings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6r1angriMZ", "forum": "Fj7adDEWm4", "replyto": "Fj7adDEWm4", "signatures": ["ICLR.cc/2026/Conference/Submission21016/Reviewer_1mpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21016/Reviewer_1mpk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896452812, "cdate": 1761896452812, "tmdate": 1762940434490, "mdate": 1762940434490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides Trident-Bench,  a benchmark dataset consisting of n 2,600 harmful prompts spanning finance, law, and medicine domains. Those prompts are designed to test whether the models refuse all of them since those prompts are verified by human experts to ensure they should not be answered by any LLMs. Those principles are built from the CFA Institute Code of Ethics and Standards of Professional Conduct (finance), the Principles of Medical Ethics from the American Medical Association (medicine), and the Model\nRules of Professional Conduct from the American Bar Association (law). Those prompts are generated by jailbreaking other LLMs and further used as a testbed for the safety of LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The contributed dataset might be useful for the safety test of LLMs on those critical domains.\n\n2. The results show that current LLMs still generate high harmfulness score responses to those prompts, revealing a potential safety concern.\n\n3. The paper claimed to contribute the first prompts set for the law and finance domain, though this claim is not grounded in the literature."}, "weaknesses": {"value": "1. The biggest concern is the generation of those harmful prompts. They are generated by jailbreaking current LLMs and further reviewed by human experts. However, since they are generated through jailbreaking, they do not represent the natural distribution of real-life harmful prompts where the models should refuse without jailbreaking. Since there are always ways to jailbreak current models, the model's refusal without jailbreaking is more meaningful.\n\n2. The authors claim the dataset to be the \"the first of its kind in law and finance\", however, this is not true. Many previous benchmarks like harmbench already include those domains.\n\n3. The result is confusing. Figure 4 reports the harmfulness score instead of the refusal rate. It would be more meaningful to learn the refusal rate of those prompts rather than the harmfulness score.\n\n4. There is no statistics about how diverse those prompts are, limiting their value."}, "questions": {"value": "1. If you want to generate some harmful prompts, why not just fine-tune a safety-aligned LLMs to be a bad model so that it can be used for generating those prompts? because jailbreaking LLMs for generation is more expensive.\n\n2. Though those prompts are designed for testing the safety of llms, do you have any measure of how practical those answers are? Are they really useful or misleading? to what extent?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "Those harmful prompts might be misused."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "133bFHyyIW", "forum": "Fj7adDEWm4", "replyto": "Fj7adDEWm4", "signatures": ["ICLR.cc/2026/Conference/Submission21016/Reviewer_GHdF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21016/Reviewer_GHdF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030185738, "cdate": 1762030185738, "tmdate": 1762940393342, "mdate": 1762940393342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}