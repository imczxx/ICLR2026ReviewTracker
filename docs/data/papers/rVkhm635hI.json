{"id": "rVkhm635hI", "number": 7448, "cdate": 1758022551485, "mdate": 1759897852303, "content": {"title": "RobGrad: Robustness-driven gradient descent for stochastic multi-objective optimization", "abstract": "Stochastic multi-objective optimization (SMOO) has become an influential framework for many machine learning problems with multiple objectives, where the gradient conflict problem is a fundamental bottleneck for effective training of models. Most existing methods address this problem with gradient-based approaches, which find an optimization direction that improves each objective through gradient manipulation techniques. However, these methods are based on instantaneous gradients and lack a global optimization perspective, which may lead to suboptimal solutions. In this paper, we consider minimizing the worst-case objective value from a global optimization perspective and transform the SMOO problem into a min-max optimization problem. Further, theoretical correspondences between this min-max problem and the SMOO problem are established. Based on this, we propose a robustness-driven gradient descent (RobGrad) algorithm. RobGrad guarantees that each objective performs not badly from a global perspective without introducing additional a priori parameters. Furthermore, we establish non-asymptotic convergence upper bounds for RobGrad in both convex and non-convex settings, which portray the expected performance gap under the worst weight assignment and the rate of RobGrad's decision approaching a Pareto stationary solution. Extensive experiments show that RobGrad has competitive or improved performance compared to state-of-the-art SMOO methods in a series of tasks on multi-task learning.", "tldr": "", "keywords": ["Stochastic multi-objective optimization", "Multi-task learning", "Pareto stationarity"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a4ddfe07a466980b0b42db814b60c69b6d254a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a robust stochastic multi-objective optimization (MOO) method for multi-task learning. The proposed method is built from Tchebysheff scalarization, a standard method in MOO, with an additional optimization term related to Pareto stationarity measure. Theoretical convergence rate of the proposed method is analyzed. Empirical studies on some benchmarks are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a robust stochastic multi-objective optimization method for multi-task learning, which is an important problem in optimization and machine learning.\n\n2. The paper provides convergence analysis and empirical studies on some benchmarks."}, "weaknesses": {"value": "1. A major weakness is that the ```convergence guarantees are too weak```. \nIt requires bounded objective assumption, and the convergence rate in the nonconvex case (Thm 2, O(K^{-1/6})) is too slow. It is much worse than the existing results. Check [1,2,3] for example, and the survey paper [4]. Some existing methods do not require the bounded objective assumption, and have better convergence rates. Moreover, it is not even comparable to the convergence rate of linear scalarization. \n\n2. ```There is no theoretical benefit of using the proposed method compared to existing stochastic MOO methods or even linear scalarization methods```. As mentioned by 1, only the convergence rate is slower with stronger assumptions. No other theoretical benefits are provided compared to existing stochastic MOO or scalarization methods. This leaves the audience wonder under what scenarios, the proposed method could be useful theoretically. More discussion is needed.\n\n\n\n5. The notations and theoretical results are not presented clearly. For example, there is no $\\xi$ in Theorems 1 or 2. See more in **Questions**.\n\n6. Missing references. There are some other important works in stochastic multi-objective optimization, which are not discussed and compared in this paper. See a list below.\n\n>[1] \"Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance,\" L. Chen et al. NIPS 2023\n\n>[2] \"Variance reduction can improve trade-off in multi-objective learning,\" H. Fernando et al. ICASSP 2024\n\n>[3] \"Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization,\" H. Yang et al. ICLR 2025\n\n>[4] \"Gradient-based multi-objective deep learning: Algorithms, theories, applications, and beyond,\" W. Chen et al. arXiv 2025"}, "questions": {"value": "1. What is $\\lambda^*$ in Theorem 1?\n\n2. $x^*$ in Theorem 1 is one solution of (2), which is only a subset of $Y_{WN}$. The motivation for achieving this solution needs more explanation.\n\n3. The motivation of this paper is not clear. The key idea is in Eq. (2). But when $\\alpha = 0$, the method reduces to Tchebyshef scalarization, which yields weakly Pareto optimal or Pareto stationary solutions, with $\\min_\\lambda ||\\nabla f(x)^\\top \\lambda ||=0$. What is the point of adding the additional minimum norm gradient term with parameter $-\\alpha$? It seems to deviate from achieving Pareto stationarity.\n\n4. Why do you provide convergence to the optimal solution of (2) in the convex case, but the convergence to Pareto stationarity in the nonconvex case? Can you discuss the stationary condition of (2) and convergence to it in the nonconvex case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4gFF5jpYG3", "forum": "rVkhm635hI", "replyto": "rVkhm635hI", "signatures": ["ICLR.cc/2026/Conference/Submission7448/Reviewer_NuD6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7448/Reviewer_NuD6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512639938, "cdate": 1761512639938, "tmdate": 1762919562268, "mdate": 1762919562268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the stochastic multi-objective optimization problem and claims that recent methods are based on instantaneous gradients and lack a global optimization perspective, which may lead to suboptimal solutions. Thus, they propose a robustness-driven gradient descent (RobGrad) algorithm. Meanwhile, the theoretical analysis is provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation of this paper is good and easy to follow.\n2. The proof seems to be rigorous to me.\n3. Sufficient illustration, Tables, and Figures help with understanding and comparison."}, "weaknesses": {"value": "1. The authors point out that gradient-based MOO approaches lack a global optimization perspective and are sensitive to the initial decision in lines 83-85. I must admit that this is not straightforward to me, and could authors provide more explanation (either experimental results or analysis work)? \n2. Besides, the method is claimed to improve the robustness of the worst-case of weight assignment and stability. The addition of the term $R(x)$ can indeed improve stability. But how does it improve the robustness?\n3. I appreciate the idea of using \"A PEDAGOGICAL EXAMPLE\" before presenting the method; however, I do not believe this example motivates the RobGrad. If a balanced result is preferred, we can also add a constraint on the empirical loss and a regularization term so that $r(x)=l(x)$, following the paper https://arxiv.org/pdf/2502.08585.\n3. In the algorithm, the weight is updated by optimizing problem (3). However, there is no description of this part. Though it is a quadratic problem, what is the cost of it?\n4. In Theorems 1 and 2, the function values are bounded. Is this assumption necessary? SDMGrad does not have this assumption.\n5. Meanwhile, Theorem 1 is in a \"high probability\" version. Why is there no \"certain expectation upper bound\" even with the function values' boundness? \n6. [General question] The gradient-based method is not computationally efficient, with $p$ times the gradient computation, and recent scalarization methods are showing stronger performance (GO4Align, LDC-MTL). In this case, what are the benefits of RobGrad over them."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z2snJ1F0lS", "forum": "rVkhm635hI", "replyto": "rVkhm635hI", "signatures": ["ICLR.cc/2026/Conference/Submission7448/Reviewer_Xocf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7448/Reviewer_Xocf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538053544, "cdate": 1761538053544, "tmdate": 1762919561782, "mdate": 1762919561782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RobGrad, a robustness-driven gradient descent method to address the stochastic multi-objective optimization problem. RobGrad leverages information from three aspects: $\\mathcal{C}_1$ loss value balancing, $\\mathcal{C}_2$ gradient conflict, and $\\mathcal{C}_3$ weight smoothing. Theoretical analysis under both convex and non-convex settings is provided. Experiments demonstrate that the proposed method can achieve competitive performance compared to existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The optimization formulation is clear. The objective leverages the loss, gradient conflict information, and the theoretical analysis justifies the proposed algorithm.\n2. Extensive experiments are conducted to show the effectiveness of RobGrad."}, "weaknesses": {"value": "1. Some parts of the paper need further clarification:\n   - Lines 351-352: The citation of FairGrad is incorrect. The reported results are from [1], but were not cited correctly.\n   - There may be typos in the proof of Lemma 1. The term $\\alpha (\\sum_{i=1}^p \\lambda^i \\nabla^2 f(x^\\star)^\\top \\lambda^\\star)$ seems should be $\\alpha (\\sum_{i=1}^p \\lambda^i \\nabla^2 f^i(x^\\star)^\\top)$. Please double-check.\n   - Theorem 1 uses the surrogate $\\mathcal{L}(x,\\lambda)=\\lambda^\\top f(x)$, but the algorithm updates $\\lambda$ with Eq. (3). Though Lemma 2-4 provide insights, it still reads inconsistently. Additional clarification or explanation would help improve coherence.\n   - How to solve Eq. (3) should be made clear. It is a QP problem, and an exact solution can be obtained. Otherwise, if an approximated solution is used, then the theoretical analysis should account for the approximation error. Well, the attached code shows that an exact optimal solution is computed, but it would still be helpful to explicitly clarify this in the paper.\n\n2. The proposed method lacks novelty. The loss value balancing term $\\mathcal{C}_1$ is just a standard scalarization idea. The proximal term $\\mathcal{C}_3$ is a common choice to stabilize the updates and make the inner problem strongly convex.\n\n3. The analysis of non-convex setting adopts the bounded function value assumption, which is a stronger condition, but the convergence rate is the same, compared to existing baselines.\n\n4. I am concerned that Eq. (3) is sensitive to loss scales. Consider different types of loss functions, such as cross-entropy, mIoU, mean absolute error (MAE), the resulting losses vary significantly in scale. Scaling task $i$ by​ $c_i$ (loss $l_i$ to $c_i l_i$) in Eq. (3), the value balancing term scales linearly in ​$c_i$, but the conflict term scales quadratically. \n\n\n[1] Fair Resource Allocation in Multi-Task Learning. [ICML 2024]"}, "questions": {"value": "See the weaknesses discussed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ffnWV24vh6", "forum": "rVkhm635hI", "replyto": "rVkhm635hI", "signatures": ["ICLR.cc/2026/Conference/Submission7448/Reviewer_gcfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7448/Reviewer_gcfw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880581539, "cdate": 1761880581539, "tmdate": 1762919561104, "mdate": 1762919561104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SMOO often struggles with gradient conflicts, and current methods focusing on instantaneous gradients can lead to suboptimal results. This paper reformulates the SMOO problem as a min-max optimization, aiming to minimize the worst-case objective value from a global perspective. The authors propose a new algorithm, RobGrad, which guarantees that all objectives perform reasonably well and demonstrates competitive or superior performance in multi-task learning experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper organization is clear and easy to follow.\n2. Provide a theoretical analysis for the proposed algorithm.\n3. Provide comprehensive experimental results."}, "weaknesses": {"value": "1. The motivation of the proposed method is unclear. The author claims that \"these methods are based on instantaneous gradients and lack a global optimization perspective\". What is \"global optimization perspective\"? I do not find a clear definition of this concept in this paper. Does the momentum update or the preference-based MOO method consider \"global optimization\"? Moreover, the logic from 3.2 to 4.1 is also confused. It seems section 3.2 does not provide any meaningful information. For the conducted experiments, such as NYUv2. The loss function of each task is completely different, so \"all objectives are equally important\" can not happen in most MTL settings. Then the authors directly proposed an optimization problem (3) and say that this optimization problem can solve the problem they focus on? Why? So compared with so many task balancing methods (CAGRAD, PCGRAD, MOCO, NASH-MTL, etc. ). Why does this method suddenly achieve SOTA in all benchmarks? Figure 3 tries to decompose each part in the optimization problem (3), each part is not a novel idea in the gradient-based MOO field.\n\n2. No discussion on the theoretical result. No comparison with any previous work, such as assumptions and convergence rate. My intuitive understanding is that the proof in this paper is no more complex than that of SDMGrad, since both use iterative methods to solve for the weights, and both consider stochastic objectives. \n\n3. Lacking a study on computational cost. About two or three years ago, I think similar approaches were advanced in solving MTL tasks. However, the current study shows that $\\mathcal{O}(1)$ cost method is enough to achieve SOTA performance in the MTL problem, see FAMO and \"Smooth Tchebycheff Scalarization for Multi-Objective Optimization\". These approaches are fast and efficient and available for industrial tasks (such as training large-scale recommender systems). The proposed method needs to update $\\lambda$ and save all gradients and update parameters, it is too heavy on both computational cost and memory cost."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eZE7e6RW8F", "forum": "rVkhm635hI", "replyto": "rVkhm635hI", "signatures": ["ICLR.cc/2026/Conference/Submission7448/Reviewer_vano"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7448/Reviewer_vano"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995797941, "cdate": 1761995797941, "tmdate": 1762919560075, "mdate": 1762919560075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}