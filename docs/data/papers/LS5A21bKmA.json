{"id": "LS5A21bKmA", "number": 9773, "cdate": 1758139374513, "mdate": 1759897698851, "content": {"title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "abstract": "Recent advances in browser-based large language model (LLM) agents have demonstrated promise for automating tasks ranging from simple form filling to complex activities such as hotel booking and online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. In real-world conditions, users encounter instability arising from multiple sources, including client-side and server-side failures, insecure HTTPS connections, or broader system failures. Moreover, real-world websites are vulnerable to threats such as cross-site scripting (XSS), malicious pop-ups, and general site modifications, all of which can compromise both safety and task completion. To address this gap, we introduce WAREX, a plug-and-play tool that integrates with existing web agent benchmarks to simulate common website failures. WAREX uses a proxy-based approach to enable the evaluation of the robustness of web agents. We apply WAREX across three widely used benchmarks — WebArena, WebVoyager, and REAL — and show that introducing simulated web failures leads to substantial drops in task success rates for state-of-the-art agents. Our results reveal fundamental weaknesses in current web agent benchmarks and establish WAREX as a practical evaluation infrastructure for testing web agents under realistic failure conditions.", "tldr": "WAREX is a tool that injects common web failures to evaluate the reliability of LLM web agents, without making any modifications to agent code or benchmarks.", "keywords": ["Web Agents; Web Failure Injections; Network Proxy; Robustness Evaluation; Black box Approach; Security"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce53f452589fa78840ab01e9e203761874cef91b.pdf", "supplementary_material": "/attachment/6a66665918d17de663f5bb302e3957e49553af25.zip"}, "replies": [{"content": {"summary": {"value": "WAREX proposes a plug-and-play, proxy-based framework for evaluating reliability and adversarial robustness of browser-based LLM web agents by intercepting and modifying HTTP(S) traffic between the agent and web servers. WAREX uses a Mitmproxy addon + config.json that specifies failure types (network delays, server-side 4xx/5xx, JS loading failures, popups/overlays) and frequency rules (regex/exact URL match, k-th occurrence, random n times). It logs efficiency metrics (latency, API call counts, token usage) by intercepting outbound calls to LLM backends. The authors validate WAREX on three popular benchmarks (WebArena, REAL, WebVoyager) and three agents (SteP, REAL demo, WebVoyager agent) with multiple LLM backbones (GPT-4o, Qwen2.5-VL, GPT-OSS), showing substantial drops in task success under injected failures and limited mitigation from simple prompting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Practical, reproducible engineering: Uses Mitmproxy and provides clear setup options (env vars, Playwright proxy, iptables) so replication is straightforward. \n- Broad empirical evaluation: Tests across three benchmarks, three agents, and multiple LLMs; measures both success and efficiency metrics (tokens, latency). This breadth strengthens generality claims. \n- Novel framing: Shifts attention from purely adversarial overlay attacks to common web failures (network/server/JS), which are practically important and understudied. \n- Non-intrusive instrumentation + logging: Ability to capture LLM call metadata and token counts even for closed-source benchmarks is valuable for auditing."}, "weaknesses": {"value": "Covered in questions"}, "questions": {"value": "- What exactly is logged for “LLM interactions”? Do you log full prompts and responses, or only token counts and timestamps? \n- How did you choose the default injection magnitudes (10s delay, error rates, frequency rules)? Can you provide evidence these match real-world distributions (or include a sensitivity sweep)?\n- Mitigation attempts beyond prompting: Did you try non-prompting mitigations (retry heuristics in harness, DOM-based checks, or simple rule-based filters) and, if so, how did they compare? If not, can you add a small evaluation?\n- Please provide example per-step traces (timestamped) for at least 3 tasks: a successful baseline, a network-failure run where the agent fails, and the malicious-popup scenario where it clicks. If full prompts cannot be released, provide redacted prompt snippets and token counts to understand what’s going under the hood."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CLvHXVOOG7", "forum": "LS5A21bKmA", "replyto": "LS5A21bKmA", "signatures": ["ICLR.cc/2026/Conference/Submission9773/Reviewer_cv5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9773/Reviewer_cv5z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421324567, "cdate": 1761421324567, "tmdate": 1762921261550, "mdate": 1762921261550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WAREX, a framework designed to evaluate the reliability of LLM-based web agents under realistic, failure-prone web conditions. Unlike existing benchmarks that assume stable and deterministic websites, WAREX functions as a network-layer proxy that injects real-world issues such as network delays, server-side errors, JavaScript failures, and malicious popups into existing benchmarks without requiring modifications to the agents or their environments. The framework’s main contribution is to provide a plug-and-play mechanism for simulating diverse web failures and adversarial scenarios across benchmarks, while also logging efficiency metrics such as latency, API call counts, and token usage to jointly assess robustness and computational cost.\n\nThrough experiments on WebArena, REAL, and WebVoyager, the authors show that current state-of-the-art web agents using models like GPT-4o, Qwen2.5-VL, and GPT-OSS experience substantial performance degradation when subjected to such injected faults, with success rates decreasing by up to 70–95%. The study also finds that most agents remain vulnerable to deceptive popups, and that prompt-based mitigation strategies only partially alleviate these issues. Overall, WAREX provides a benchmark-agnostic and extensible methodology for evaluating the robustness, safety, and efficiency of web agents, aiming to bridge the gap between controlled benchmarks and real-world deployment conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely problem by introducing a benchmark framework that tests web agents under realistic and failure-prone conditions. This type of reliability evaluation is currently missing in the field, and WAREX provides a practical and well-motivated step toward bridging the gap between controlled benchmarks and real-world deployment."}, "weaknesses": {"value": "The experimental sections are difficult to follow, which raises uncertainty about the reliability of the results. In particular, there are no uncertainty or variance metrics, making it impossible to judge whether the reported differences are statistically significant. Section 4.2 is also awkwardly structured: it introduces experiments but defers their explanation to Section 5, which breaks the logical flow and should be merged. Overall, the presentation of the experiments is not very clear or polished, which makes it harder to assess the validity of the findings.\n\nThe figures and tables could be improved. It is not obvious which agent is represented in Figure 4, despite the manuscript stating that three agents were tested across three benchmarks. Contrasting Figure 4 and Table 5 is confusing and should be avoided; the results should instead be presented consistently in one place, with deltas or comparisons made directly in the text or tables. In Figure 6, two of the three models appear to be missing data columns, which leaves the comparison incomplete.\n\nThe efficiency metric is also problematic. Metrics such as “average cost per task,” “latency,” and “cost” all decrease under failure conditions, which the paper interprets as negative outcomes. However, lower values for those metrics are nominally positive. The evaluation should be redesigned to emphasize high performance together with low cost, latency, and number of steps, rather than treating lower values as inherently bad.\n\nOther presentation issues include the absence of references to Table 1, weak figure captions that do not make figures self-contained, and numerous missing citations, especially in the related-work and methods sections. Overall, the paper would benefit from a more coherent and polished presentation of results and supporting materials"}, "questions": {"value": "Missing citation to BrowserGym (De Chezelles et al., 2024) when mentioning prior web-agent environments, as it provides a unified gym-style framework. \n\nThe claim about the lack of real-world web benchmarks should acknowledge WorkArena (Drouin et al. 2024)  and WorkArena++ (Boisvert et al., 2024), which specifically target realistic enterprise web tasks. F\n\ninally, AgentRewardBench (Lù et al., 2025) should be cited for its analysis of side effects and looping behaviour in agent evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L79lI6eWxl", "forum": "LS5A21bKmA", "replyto": "LS5A21bKmA", "signatures": ["ICLR.cc/2026/Conference/Submission9773/Reviewer_qrMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9773/Reviewer_qrMr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787700986, "cdate": 1761787700986, "tmdate": 1762921260812, "mdate": 1762921260812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WAREX, a proxy sitting between a web-agent and a benchmark environment. The proxy rewrites HTTP responses to simulate web errors and adversarial conditions. The authors argue that injection at the network layer makes WAREX benchmark and agent agnostic. They run a GPT-4o-based agent on three existing web benchmarks with WAREX and show that performance decreases when errors are injected, and that some of it can be recovered by prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Demonstrates the brittleness of (some) agents under common web errors.\n- Gives a benchmark-agnostic way to inject failures via network interception."}, "weaknesses": {"value": "- Lacks novelty: differences from prior work (e.g. DoomArena, 2025) are primarily engineering rather than scientific, changing the attachment point to be the network rather than wrapping the agent or env.\n- Weak backbone models: GPT-4o, Qwen2.5, and GPT-OSS are relatively poor models for agentic tasks compared to current SoTA. WebArena and REAL success rates reported (without errors) are far below current reported performance for strong agents.\n- Shallow analysis: the paper only shows drops in performance and reports behaviour, without a real exploration of the cause of failures.\n- Results unsurprising: if there is an error, and the agent doesn't know to reload, it will tautologically fail.\n- REAL already allows configuring errors and latency, and so the benefit of WAREX in this case is unclear.\n- Difficulty of comparison: to allow future work to compare fairly, a record of the faults injected and when would be necessary."}, "questions": {"value": "- It would strengthen the paper a lot to show that training on successful WAREX traces, or some sort of WAREX-agent improves robustness across failure modes (including OOD ones). Could this be done?\n- The picture referred to as Omnizon homepage looks like a calendar website, whereas you state that it is an Amazon clone. Which is it?\n- Can you think of a better metric to capture an agent's recovery ability than simply 0-1 overall task success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPGYAdD36Z", "forum": "LS5A21bKmA", "replyto": "LS5A21bKmA", "signatures": ["ICLR.cc/2026/Conference/Submission9773/Reviewer_D6ms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9773/Reviewer_D6ms"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821390039, "cdate": 1761821390039, "tmdate": 1762921260241, "mdate": 1762921260241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WAREX is a plug-and-play framework designed to augment existing web agent benchmarks (such as WebArena, REAL, and WebVoyager) with realistic stress conditions. Existing evaluations rely on controlled environments, giving a false sense of reliability. WAREX acts as a transparent proxy layer by intercepting and modifying HTTP(S) traffic to inject common web failures (e.g., network delays, server errors) and adversarial attacks (malicious popups). Experiments show that introducing WAREX leads to significant drops in task success rates (e.g., over 70% decrease due to network errors on WebArena), exposing fundamental robustness gaps in state-of-the-art agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of WAREX is its maximum interoperability, operating at the network layer via a transparent proxy. This modular design is benchmark-agnostic and requires no changes to agent or benchmark source code, facilitating its use with existing, even closed-source, resources. WAREX systematically evaluates robustness against pervasive, everyday web failures (network, server, JavaScript errors), beyond the narrow focus of previous work on overlays. It also offers a dual capability by logging efficiency metrics, such as LLM token counts and latency, providing a holistic reliability and cost analysis."}, "weaknesses": {"value": "The design philosophy and measurement results of the \"stress test\" proposed in this work for GUI Agents constitute an excellent engineering achievement. However, it needs to be demonstrated whether this can inform the Agent's capability training itself in a sufficiently profound and intuitive way, rather than merely being a superb engineering tool."}, "questions": {"value": "1. Please explicitly state how the WAREX framework provides unique insights or data structures for training a new generation of failure-aware web agents, distinct from simple data augmentation or reinforcement learning environment setups.\n2. What is the scalability of this framework? What are the next directions for expansion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yXRyPiGuRg", "forum": "LS5A21bKmA", "replyto": "LS5A21bKmA", "signatures": ["ICLR.cc/2026/Conference/Submission9773/Reviewer_zE6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9773/Reviewer_zE6V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998024556, "cdate": 1761998024556, "tmdate": 1762921259815, "mdate": 1762921259815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}