{"id": "cS2zLrJ3qN", "number": 20927, "cdate": 1758311740794, "mdate": 1759896951773, "content": {"title": "Noise-Resilient Quantum Neural Networks via Zero-Noise Knowledge Distillation", "abstract": "Quantum neural networks (QNNs) show promise for learning on noisy intermediate-scale quantum (NISQ) devices, but two-qubit gate noise remains a significant barrier to practical implementation. Zero-noise extrapolation (ZNE) reduces errors by running circuits with scaled noise levels and extrapolating to the zero-noise limit, although it needs many evaluations per input and is susceptible to time-varying noise. We propose zero-noise knowledge distillation (ZNKD), a training-time technique that involves a ZNE-augmented teacher QNN supervising a compact student QNN. Variational learning is used to optimize the student's ability to duplicate the teacher's extrapolated outputs, resulting in robustness without the need for inference extrapolation. We additionally present a formal analysis that demonstrates how robustness flows from the ZNE instructor to the distilled student, with proofs regarding noise scaling, extrapolation error, and student generalization. In dynamic-noise simulations (IBM-style $T_1/T_2$, depolarizing, readout), ZNE-guided distillation lowers student MSE by $0.06$-$0.12$ ($\\approx$10-20\\%) across Fashion-MNIST, AG News, UCI Wine, and UrbanSound8K, keeping students within $0.02$-$0.04$ of the teacher and achieving $6{:}2$-$8{:}3$ ratio of teacher to student.  ZNKD, which amortizes ZNE to training, provides an efficient way to drift-resilient QNNs on NISQ hardware without per-input folding or extrapolation.", "tldr": "", "keywords": ["Quantum neural networks", "quantum noise", "zero noise extrapolation", "knowledge distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f14883415ff961cbfac553875230f242ec7696f3.pdf", "supplementary_material": "/attachment/1b930422d2af3af9c0f8be470ad52d3b1f5d4eb3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Zero-Noise Knowledge Distillation (ZNKD), a hybrid framework that combines zero-noise extrapolation (ZNE) with knowledge distillation (KD) for training quantum neural networks (QNNs) robust to hardware noise. The method trains a noise-mitigated “teacher” QNN using Richardson extrapolation and transfers robustness to a smaller “student” QNN, which can then operate without costly noise extrapolation at inference. The paper provides formal theoretical results establishing bounds on robustness transfer and empirical evaluations on several small datasets (Fashion-MNIST, AG News, Wine Quality, and UrbanSound8K) under IBM-style noise models and limited hardware experiments. Results suggest a 10–20% MSE reduction versus non-distilled baselines and up to 8:3 model compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe idea of amortizing noise mitigation through distillation is novel within quantum machine learning.\n2.\tTheoretical sections provide a formal foundation linking extrapolation error, approximation gap, and generalization.\n3.\tExperiments include multiple modalities (image, text, audio, tabular), illustrating the generality of the pipeline.\n4.\tWriting quality is generally clear, with correct referencing and structured proofs."}, "weaknesses": {"value": "1.\tThe related work (Section 1) insufficiently contextualizes prior studies that combine error mitigation with compression or distillation. For instance, [Cerezo et al. 2021] and [Gou et al. 2024] are mentioned but not contrasted analytically. The paper should specify how ZNKD differs in mechanism or achievable robustness beyond replacing extrapolation by distillation.\n2.\tThe compression ratios (6:2–8:3) are arbitrary and unexplained. It is unclear how teacher-to-student dimensional reduction is chosen or whether the student topology is optimal. Without ablation, one cannot assess trade-offs between expressivity and noise resilience.\n3.\tThe link between ZNE theory (Section 2.3.1) and the distillation mechanism (Section 2.2.1) is unclear. There is no empirical demonstration that Richardson-extrapolated teacher labels are smoother or more stable targets for the student than raw noisy outputs.\n4.\tAlthough the paper argues amortization of ZNE cost, it does not quantify teacher training overhead (number of fold levels, total circuits executed). Practical resource savings remain unclear."}, "questions": {"value": "1.\tCan you provide quantitative runtime comparisons (in circuit executions) between ZNKD training and classical ZNE at inference to substantiate the claimed efficiency gain?\n2.\tHow sensitive is ZNKD performance to mismatch between the simulated noise model and real hardware noise?\n3.\tCould you report results on larger circuits (≥16 qubits) or different ansätze to assess scalability?\n4.\tHow were the Richardson extrapolation orders (λ ∈ {1,3,5}) chosen, empirically or theoretically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sJbvv3XBJZ", "forum": "cS2zLrJ3qN", "replyto": "cS2zLrJ3qN", "signatures": ["ICLR.cc/2026/Conference/Submission20927/Reviewer_UuuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20927/Reviewer_UuuJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291976099, "cdate": 1761291976099, "tmdate": 1762939028804, "mdate": 1762939028804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a zero-noise knowledge distillation on QNN, which create noise-resilient QNNs for NISQ. The approach trains a teacher QNN with zero-noise extrapolation to generate noise-mitigated outputs, then distills this knowledge to a compact student QNN. The student inherits noise robustness without requiring runtime extrapolation, achieving 10-20% lower loss than non-distilled models while maintaining 6:2-8:3 compression ratios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. ZNE is responsible for teacher denoising, while the student takes on robustness, decoupling the costs of training and deployment; the algorithm and loss definition are intuitive and clear.\n2. The paper attempts to tackle a highly relevant and practical problem in the NISQ era."}, "weaknesses": {"value": "1. There are several data inconsistencies throughout the paper, such as 3090+256gb, which is later changed to 3090+128gb. The main text uses 64 shots, while the distillation early stopping method uses 1000 shots, and the appendix changes to 1024 shots.\n2. The choice of metric is questionable: the classification task mainly reports MSE, which has limited explanatory power for the \"improvement of 0.06–0.15\", and the main text only mentions accuracy \"incidentally\", while the main table still focuses on MSE.\n3. Missing ablation studies in main text."}, "questions": {"value": "1. Could you please clarify and explain the differences in the settings for shots/noise parameters/λ? Which are used in the main results and which are only used in the appendix discussion? Do the corresponding figures need corrections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "utMgzu93kw", "forum": "cS2zLrJ3qN", "replyto": "cS2zLrJ3qN", "signatures": ["ICLR.cc/2026/Conference/Submission20927/Reviewer_EGCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20927/Reviewer_EGCa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749332644, "cdate": 1761749332644, "tmdate": 1762939028163, "mdate": 1762939028163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Zero-Noise Knowledge Distillation (ZNKD), a training-time technique that enhances noise robustness in QNNs for NISQ hardware. The method combines zero-noise extrapolation (ZNE) with teacher-student distillation, where a large, ZNE-augmented teacher QNN supervises a smaller student QNN. During training, the student learns to reproduce the teacher’s extrapolated (near-noiseless) outputs, thus inheriting noise robustness without requiring extrapolation or circuit folding during inference.\n\nThe authors provide a formal analysis showing how robustness properties transfer from teacher to student, including proofs for extrapolation error bounds and generalization. Experiments on several datasets demonstrate consistent improvements in MSE and accuracy over baselines, achieving 10–2\\% reductions in error and maintaining close alignment between teacher and student performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-motivated and timely contribution: The work directly addresses one of the central challenges in QML—the mitigation of noise on NISQ devices—by amortizing the cost of ZNE into the training phase.\n\nTheoretical rigor: The formal treatment of robustness transfer, including proofs of noise scaling and extrapolation error, strengthens the methodological foundation.\n\nComprehensive empirical validation: The authors benchmark ZNKD across multiple datasets and compare it against relevant baselines, demonstrating consistent performance improvements.\n\nPractical impact: Moving ZNE to the training stage significantly reduces inference overhead, making the approach more suitable for deployment on near-term quantum devices.\n\nClear writing and presentation: The paper is well organized, with intuitive figures and strong technical exposition.\n\nThis is a well-executed paper that offers a promising and practical path toward noise-resilient QNNs on NISQ devices. The theoretical grounding and breadth of experiments justify its acceptance. However, a more transparent analysis of computational cost and explicit discussion of teacher–student trade-offs would strengthen its impact and reproducibility."}, "weaknesses": {"value": "Resource cost not fully analyzed: Although ZNKD avoids per-inference extrapolation, it still depends on an expensive teacher model trained with ZNE. The paper does not provide a quantitative analysis of total training cost (e.g., total number of circuit executions or measurement calls) relative to baseline methods. Including this in Table 3 or as a separate resource table would clarify the true computational trade-off.\n\nTeacher dependence: The performance advantage largely stems from the strong teacher QNN, which is already near state-of-the-art compared with existing approaches. It is therefore unclear how much of the observed gain arises from distillation versus the teacher’s own performance.\n\nMissing citations: While the authors reference general distillation literature, they omit several relevant works in quantum knowledge distillation, including:\n\nKnowledge Distillation in Quantum Neural Networks using Approximate Synthesis\nBridging Classical and Quantum Machine Learning: Knowledge Transfer from Classical to Quantum Neural Networks using Knowledge Distillation\nHybrid Quantum–Classical Machine Learning with Knowledge Distillation"}, "questions": {"value": "How does the distillation benefit scale with the teacher’s quality?\n\nCan the authors provide resource scaling estimates (e.g., number of circuit evaluations or measurement calls) for teacher during the separate training?\n\nAdd a resource cost column to Table 3 or a figure comparing total training-time circuit evaluations among ZNKD, ZNE, and baseline models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mvAaRP12D3", "forum": "cS2zLrJ3qN", "replyto": "cS2zLrJ3qN", "signatures": ["ICLR.cc/2026/Conference/Submission20927/Reviewer_JZLH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20927/Reviewer_JZLH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828189098, "cdate": 1761828189098, "tmdate": 1762939025633, "mdate": 1762939025633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is concerned with Zero-noise extrapolation (ZNE) for addressing the challenge of gate noise in quantum circuits. ZNE increases quantum circuit noise and extrapolates the measurement outcomes to the zero-noise limit, which may be very costly. The paper introduces a new student-teacher knowledge distillation approach for error prevention and compression. This conceptually new method is analyse mathematically and in experiments using both simulated noise and real quantum hardware."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a timely and relevant topic: noise mitigation is a crucial issue for enabling quantum machine learning on NISQ devices\n- The experiments appear to be comprehensive, covering different noise levels and datasets. \n- The mathematical analysis appears to be sound and provides theoretical insights on the relation between the ZNE scaling factors and the sample size."}, "weaknesses": {"value": "Mismatch between provided source code and details in the paper:\n- The source code is using qiskit version 0.45.3 (according to its \"requirements.txt\" file), which is depreciated since February 2024. \n- The code is not consistent at all with the procedure described in Section 2.2.1. For example, consider the loss described in Step (iii) \"Distillation Objectives\" using tanh and ZNE corrected expectations. In contrast, in the code (take wine/kd.py) in sections 1.6.-1.8 a bigger QNN is trained, then used to generate predictions, and then a smaller QNN is trained on those predictions. \n- The code also does not include any of the benchmarks reported in Table 4. On the other hand, some of these benchmarks (e.g. Wang et al. 2025) do not include experiments with the datasets considered here. This heavily undermines the paper's claims. \n\nStyle and presentation:\n- Writing and presentation: The level of presentation is currently below ICLR standards. Section 2 requires the reader to jump back and forth between Supplement and Main text several times within the first lines of reading. The main paper should be self-contained, with supplement  providing additional background and further supporting material. However, the supplement should not be needed for a basic understanding of what the paper aims to do. For example, the presentation of the method in Section 2.1 starts with \"After defining gate-level decoherence using the Lindblad-informed noise model in A.1 ...\" Then in \"Motivation\" it goes on \"Using the single-gate fidelity euqation 32, demonstrates...\". The same happens in several places below. \n- Typos: The paper still contains many typos, such as typsetting of \"U\" in l102 and expressions like \"These denoised outputs are used as soft labels for training students (clients).\" (what does \"clients\" refer to here?). Section 3.2 (lines 403-405) contain several misplaced \"!\"-symbols. \n- References out of place: In line 90-91 ZNE is attributed to a paper from 2024. However, this does not appear to be the correct reference.  Another example is Wang et al. (2025), which (differently than stated in the text) is not concerned with knowledge distillation or quantum transfer learning. Nevertheless, it is listed as a benchmark method in Table 4. \n- The paper seems to still contain an LLM prompt: line 284-285 states: \"Give an explanation of the extrapolated estimator’s mean squared error (MSE) as ...\"."}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IKfFJtBNFu", "forum": "cS2zLrJ3qN", "replyto": "cS2zLrJ3qN", "signatures": ["ICLR.cc/2026/Conference/Submission20927/Reviewer_jmcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20927/Reviewer_jmcu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836763115, "cdate": 1761836763115, "tmdate": 1762939022087, "mdate": 1762939022087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}