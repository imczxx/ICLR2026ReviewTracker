{"id": "i0zjotaTnv", "number": 12733, "cdate": 1758209861551, "mdate": 1763460343313, "content": {"title": "Not How You Think, It's What You See: Decoupling Perception from Reasoning", "abstract": "The ability of Vision-Language Models (VLMs) to reason depends on a complex interplay between visual perception and abstract cognition. While it is widely recognized that perception is a significant bottleneck, systematically diagnosing how it fails and developing methods to unlock latent reasoning capabilities remains a key challenge. To address this, we introduce a cognitively-inspired framework that decomposes VLM behavior through four distinct paradigms: 1) Direct Visual Rule Learning (holistic processing), 2) Deductive Rule Learning (explicit rule extraction), 3) Componential Analysis (CA), which decouples perception by reasoning over task-agnostic textual descriptions, and 4) Interactive Componential Analysis (ICA), which introduces a feedback loop for targeted visual probing. Our framework's emphasis on task-agnostic decomposition and cognitive parallels provides a unique lens for analysis compared to prior decoupling efforts. Applying this framework across an expanded suite of benchmarks, we conduct a comprehensive evaluation on both proprietary and open-source multi-image VLMs. Our results confirm that perception is a primary bottleneck and show that our CA and ICA paradigms yield substantial performance gains, unlocking the latent reasoning abilities of powerful LLMs. Crucially, ICA demonstrates that an interactive loop can resolve fine-grained visual ambiguities that static descriptions cannot, outperforming the non-interactive CA approach. Our work provides a robust diagnostic toolkit for the community and offers concrete architectural insights, demonstrating that interactive, decoupled systems are a promising path toward more general and capable visual intelligence.", "tldr": "We show VLMs' reasoning is limited by perception, not logic. Our framework decouples these processes by reasoning over text descriptions, and an interactive loop lets the model \"look again\" at images, unlocking latent reasoning and better performance", "keywords": ["Visual Reasoning", "Vision-Language Models (VLMs)", "Evaluation Framework", "Evaluation Methodology", "Cognitive Paradigms", "Perception-Reasoning Interface", "Perception Bottleneck", "Bongard Problems", "Multi-Image Reasoning", "Human-centered evaluation", "Interpretability"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4c25f84b1e9b1d2ac2269e4ffbca9e751c35297.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework with four different approaches to prompt VLM for visual rule learning tasks, demonstrating that models are primarily bottlenecked by a visual description task rather than a deductive reasoning task. When evaluating specific subsets of the Bongard-OW, Bongard-HOI, and Winoground datasets, it is found that performance can be improved by the approaches that first try to decompose visual rule learning into visual description and deductive reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper investigates some significant limitations of current VLM capabilities."}, "weaknesses": {"value": "Unfortunately, the paper has several severe weaknesses:\n\n1.  The methodology is not presented well. The motivation for the \"cognitively-inspired\" prompting strategies is located in the middle of the related work section rather than at the beginning of the introduced method. Additionally, there is no discussion of the necessary assumptions or potential limitations of prompting the VLM to solve presumed subtasks instead of the original task. The decompositionality of a task into such subtasks is a strong assumption and limitation that must be properly discussed and justified. Specifically, A.5.3.1 shows that, for the visual description subtask, the VLM is given many additional context examples of specific visual concepts from the Bongard-OW and Bongard-HOI datasets. This completely undermines the claim of task agnosticism or generalizability of the contribution. Interestingly, it is also an intended feature of the original Bongard problems that are hardly decomposable into perception and reasoning, as a useful description of a single image heavily depends on the patterns found across all of them. (E.g. discussed in Depeweg et al., 2024)\n2.  The primary area and use case of the overall contribution are difficult to understand. The \"evaluation framework\" seems most similar to a benchmark, yet the authors emphasize improving visual intelligence with their prompting strategy. It is clearly not about causal reasoning, which was selected during the submission process.\n3.  The curation process of the dataset, which is used for evaluation, appears to be arbitrary. No justification is provided for the selection of the subset of Bongard-OW, and there is no discussion of potential difficulties associated with evaluating a subset of public datasets, which might also conflict with the original design of the dataset.\n4.  The paper also lacks novelty, as there are several works explicitly assessing the VLM struggle with visual perception tasks (Geigle et al., 2024; Gou et al., 2024; Kamath et al., 2023; Rahmanzadehgervi et al., 2024; Zhang et al., 2024; Zhou et al., 2023; Wang et al., 2024) as well as evaluating VLM on Bongard problems (Małkiński et al., 2025; Wüst et al., 2025).\n\nIf I have completely misunderstood the scope of the paper, I am willing to increase my score; however, at this stage, the paper definitely lacks scientific clarity."}, "questions": {"value": "1.  Why is only 1/4 of Bongard-OW evaluated?\n2.  How are VLM responses to tasks other than classification compared to the ground truth? Is there any form of Human Verification or LLM judging involved?\n3.  Where do the captions for the Winoground score (A.4.4 and A.6) come from? If they are ground-truth, what can we conclude from CA outperforming DRL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MgQ9Dwz8Cb", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Reviewer_nJ3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Reviewer_nJ3X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774382616, "cdate": 1761774382616, "tmdate": 1762923555354, "mdate": 1762923555354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework with four different approaches to prompt VLM for visual rule learning tasks, demonstrating that models are primarily bottlenecked by a visual description task rather than a deductive reasoning task. When evaluating specific subsets of the Bongard-OW, Bongard-HOI, and Winoground datasets, it is found that performance can be improved by the approaches that first try to decompose visual rule learning into visual description and deductive reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper investigates some significant limitations of current VLM capabilities."}, "weaknesses": {"value": "Unfortunately, the paper has several severe weaknesses:\n\n1.  The methodology is not presented well. The motivation for the \"cognitively-inspired\" prompting strategies is located in the middle of the related work section rather than at the beginning of the introduced method. Additionally, there is no discussion of the necessary assumptions or potential limitations of prompting the VLM to solve presumed subtasks instead of the original task. The decompositionality of a task into such subtasks is a strong assumption and limitation that must be properly discussed and justified. Specifically, A.5.3.1 shows that, for the visual description subtask, the VLM is given many additional context examples of specific visual concepts from the Bongard-OW and Bongard-HOI datasets. This completely undermines the claim of task agnosticism or generalizability of the contribution. Interestingly, it is also an intended feature of the original Bongard problems that are hardly decomposable into perception and reasoning, as a useful description of a single image heavily depends on the patterns found across all of them. (E.g. discussed in Depeweg et al., 2024)\n2.  The primary area and use case of the overall contribution are difficult to understand. The \"evaluation framework\" seems most similar to a benchmark, yet the authors emphasize improving visual intelligence with their prompting strategy. It is clearly not about causal reasoning, which was selected during the submission process.\n3.  The curation process of the dataset, which is used for evaluation, appears to be arbitrary. No justification is provided for the selection of the subset of Bongard-OW, and there is no discussion of potential difficulties associated with evaluating a subset of public datasets, which might also conflict with the original design of the dataset.\n4.  The paper also lacks novelty, as there are several works explicitly assessing the VLM struggle with visual perception tasks (Geigle et al., 2024; Gou et al., 2024; Kamath et al., 2023; Rahmanzadehgervi et al., 2024; Zhang et al., 2024; Zhou et al., 2023; Wang et al., 2024) as well as evaluating VLM on Bongard problems (Małkiński et al., 2025; Wüst et al., 2025).\n\nIf I have completely misunderstood the scope of the paper, I am willing to increase my score; however, at this stage, the paper definitely lacks scientific clarity."}, "questions": {"value": "1.  Why is only 1/4 of Bongard-OW evaluated?\n2.  How are VLM responses to tasks other than classification compared to the ground truth? Is there any form of Human Verification or LLM judging involved?\n3.  Where do the captions for the Winoground score (A.4.4 and A.6) come from? If they are ground-truth, what can we conclude from CA outperforming DRL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MgQ9Dwz8Cb", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Reviewer_nJ3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Reviewer_nJ3X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774382616, "cdate": 1761774382616, "tmdate": 1763480274888, "mdate": 1763480274888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cognitively-inspired framework to decouple perception and reasoning in Vision-Language Models (VLMs), featuring four paradigms: Direct Visual Rule Learning (DVRL), Deductive Rule Learning (DRL), Componential Analysis (CA), and Interactive Componential Analysis (ICA). It evaluates proprietary and open-source VLMs on benchmarks including Bongard-OW, Bongard-HOI, and Winoground, confirming that perception is a primary bottleneck for visual reasoning. The CA and ICA paradigms, which leverage task-agnostic textual descriptions and interactive feedback loops respectively, achieve state-of-the-art performance, unlocking latent reasoning capabilities in LLMs. Key contributions include a diagnostic toolkit for VLMs, a modular decoupling method applicable to diverse architectures, comprehensive empirical validation of the perception bottleneck, and demonstration of interactive systems as a promising direction for visual intelligence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Proposes an innovative cognitively-inspired framework that models human problem-solving strategies, filling the gap of systematic perception-reasoning decoupling in VLMs.\n\n(2) Conducts comprehensive evaluations across diverse benchmarks (abstract rule discovery, HOI reasoning, compositional grounding) and model types, ensuring robust results.\n\n(3) The CA/ICA paradigms enable multi-image reasoning for single-image architectures and visual reasoning evaluations for text-only LLMs, expanding application scenarios.\n\n(4) In-depth ablation studies isolate perception and reasoning, providing clear insights into model bottlenecks.\n\n(5) Delivers state-of-the-art performance on key benchmarks, demonstrating practical value of the proposed methods."}, "weaknesses": {"value": "(1) The componential paradigms (CA/ICA) rely heavily on language as an intermediate representation, but the paper lacks sufficient discussion on limitations in tasks requiring non-verbalizable reasoning (e.g., geometric reasoning), making it unclear how the framework performs in such scenarios.\n\n(2) Evaluation of open-source models is incomplete: most open-source VLMs are only tested under CA, with no attempts to adapt DVRL/DRL to their input constraints (e.g., batch processing images), limiting the comparability of paradigm effectiveness across model types.\n\n(3) The selection basis for the 500-case Bongard-OW subset is not clearly explained. The paper mentions it is derived from the first 250 samples with two test cases each, but fails to justify why this subset is representative or whether it introduces selection bias.\n\n(4) Implementation details for prompt engineering are insufficient. While prompts are provided in the appendix, there is no analysis of how minor prompt variations affect performance, which is critical for reproducibility given the sensitivity of VLMs to prompts.\n\n(5) Hardware configuration details are vague. The paper mentions using NVIDIA GPUs (2080Ti, 3090, 6000 Ada) but does not specify which GPU is used for which model, leading to uncertainty about potential performance impacts from hardware differences.\n\n(6) Comparisons with state-of-the-art baselines are incomplete. For example, on Winoground, the paper only compares with a few CoT-based methods but omits recent strong performers (e.g., Claude 3 Opus, Gemini Ultra) or specialized compositional reasoning models.\n\n(7) The number of interaction steps in ICA is not optimized. The paper uses a single feedback loop but does not test whether multiple rounds of probing further improve performance or lead to diminishing returns.\n\n(8) There is no quantitative metric for description quality. The paper claims GPT-4o's descriptions are high-fidelity but does not use objective metrics (e.g., BLEU, ROUGE, or human evaluation) to validate this, making the link between description quality and reasoning performance less rigorous.\n\n(9) Cross-domain generalization is under-tested. The framework is evaluated on natural image benchmarks but not on complex domains like scientific chart interpretation, mathematical reasoning, or medical imaging, leaving its applicability to specialized fields unclear.\n\n(10) Computational cost and latency analysis is missing. The multi-stage CA/ICA paradigms are likely more computationally expensive than end-to-end methods, but the paper provides no data on inference time, memory usage, or scalability for large-scale applications.\n\n(11) Prompt sensitivity is not investigated. The paper uses fixed prompts for all experiments but does not analyze how changes in prompt structure, detail, or tone affect paradigm performance, which is essential for understanding the framework's robustness.\n\n(12) Error analysis is limited. The paper only provides 15 misclassified cases, without systematic categorization of errors (e.g., perceptual vs. reasoning errors) or analysis of whether errors are consistent across models or benchmarks.\n\n(13) Robustness to adversarial examples is untested. The paper does not evaluate how the framework performs when images contain noise, distortions, or adversarial perturbations, which is critical for real-world deployment.\n\n(14) The optimality of CA's JSON description structure is unvalidated. The paper uses a specific hierarchical JSON format but does not compare it with other representation formats (e.g., free-text descriptions, structured lists) to show that JSON is the most effective for reasoning.\n\n(15) ICA's question generation strategy is not detailed. The paper states the reasoning LLM formulates targeted questions but does not explain how questions are prioritized or whether the question generation process itself introduces biases.\n\n(16) Rule complexity analysis is missing. The paper does not quantify the complexity of rules in different benchmarks, making it unclear whether the framework's performance gain varies with rule difficulty.\n\n(17) The scope of text-only LLMs' applicability is undefined. The paper shows text-only LLMs perform well with high-quality descriptions but does not specify the types of visual tasks where this modality transfer fails.\n\n(18) Comparisons with other decoupling frameworks are superficial. The paper distinguishes itself from Prism and CoT methods but does not provide head-to-head performance comparisons or detailed analysis of architectural differences.\n\n(19) Perception module evaluation is absent. The paper focuses on reasoning performance but does not evaluate the perception module in isolation (e.g., accuracy of object detection, attribute recognition), making it hard to quantify the exact impact of perceptual errors.\n\n(20) Reasoning module selection is not justified. The paper uses powerful LLMs (e.g., GPT-4o) as reasoning engines but does not explain why these models are chosen or whether weaker LLMs would still yield effective results.\n\n(21) Multilingual applicability is untested. The framework uses English descriptions and prompts but does not evaluate performance in other languages, limiting its relevance for non-English use cases.\n\n(22) Few-shot learning limits are not explored. The paper uses few-shot prompting but does not test how the framework performs with fewer or zero shots, which is important for low-resource scenarios.\n\n(23) Training data influence is not discussed. The paper does not analyze whether VLMs trained on specific datasets (e.g., more HOI data) perform better under the framework, leaving the impact of training data distribution unclear.\n\n(24) Evaluation metrics are overly simplistic. The paper relies primarily on classification accuracy, without considering other metrics (e.g., precision, recall, F1-score) that are more informative for imbalanced datasets.\n\n(25) Dataset bias analysis is insufficient. Bongard-OW's subset has a dominant category (ID 0, 73%), but the paper does not discuss how this bias affects model performance or generalization.\n\n(26) ICA's feedback loop efficiency is unanalyzed. The paper does not measure how much time the interactive probing adds or whether the added complexity is justified by performance gains.\n\n(27) Image resolution impact is untested. The paper uses default or 1024px images but does not evaluate how lower or higher resolutions affect description quality and subsequent reasoning.\n\n(28) Description consistency is not evaluated. The paper does not check whether the same image generates consistent descriptions across multiple runs, which is important for the framework's reliability.\n\n(29) Rule extraction accuracy is not quantified. The paper evaluates classification accuracy but not the accuracy of the extracted rules themselves (e.g., how well extracted rules match ground-truth rules).\n\n(30) Human reasoning comparison is superficial. The paper mentions human average accuracy but does not compare the framework's reasoning process with human reasoning steps (e.g., sequence of analysis, focus on details).\n\n(31) Open-source model performance gap analysis is inadequate. The paper notes lower CA accuracy for some open-source models but does not deeply investigate whether the gap stems from poor description generation, weak reasoning, or both.\n\n(32) Framework scalability to video is unaddressed. The paper focuses on static images but does not discuss how to extend the paradigms to video reasoning, where temporal dynamics add complexity.\n\n(33) Ethical considerations are missing. The paper does not discuss potential biases in description generation (e.g., racial, gender biases) or how these biases could propagate to reasoning outcomes.\n\n(34) Hyperparameter tuning is not reported. The paper uses zero temperature for all models but does not explain why this choice is optimal or whether tuning temperature affects performance.\n\n(35) Cross-model generalization of paradigms is untested. The paper evaluates a fixed set of models but does not test whether the paradigms perform consistently across newly developed VLMs.\n\n(36) The impact of description length is unanalyzed. The paper does not test whether shorter or longer descriptions affect reasoning performance, leaving the optimal description length unclear.\n\n(37) Benchmark-specific optimizations are not disclosed. It is unclear whether the paradigms are tailored to the tested benchmarks or if they generalize to unseen visual reasoning tasks.\n\n(38) Collaboration between perception and reasoning modules is not modeled. The paper treats the modules as separate but does not explore how bidirectional communication beyond ICA's feedback loop could further improve performance.\n\n(39) Low-resource language model adaptation is untested. The paper uses high-resource models but does not evaluate how the framework performs with low-resource VLMs or LLMs.\n\n(40) Real-world application case studies are absent. The paper demonstrates performance on benchmarks but provides no case studies of applying the framework to practical tasks (e.g., image retrieval, visual question answering)."}, "questions": {"value": "**To facilitate discussions during the Rebuttal phase, authors are advised to respond point-by-point (indicating the question number).**\n\n(1) Could you provide specific examples of the JSON descriptions generated in CA (e.g., 3-5 full descriptions for Bongard-OW samples) to demonstrate the structure and detail of the task-agnostic representations?\n\n(2) How was the number of interaction steps in ICA determined? Have you tested whether 2-3 rounds of probing further improve performance, or does a single round already reach diminishing returns?\n\n(3) For open-source models that do not support large multi-image inputs, have you attempted to adapt DVRL/DRL by batch-processing images or using image embeddings? If not, what technical barriers prevented this adaptation?\n\n(4) What specific criteria were used to select the 500-case Bongard-OW subset? Could you provide statistical evidence (e.g., rule category distribution, difficulty distribution) that this subset is representative of the full dataset?\n\n(5) Could you add comparisons with recent state-of-the-art VLMs (e.g., Claude 3 Opus, Gemini Ultra, Qwen-VL Max) on all benchmarks to better contextualize the performance of your framework?\n\n(6) Have you tested the impact of prompt variations (e.g., changing the level of detail, rephrasing instructions) on paradigm performance? If so, please provide quantitative results; if not, could you explain why prompt sensitivity is not a concern?\n\n(7) Could you provide detailed computational cost data (e.g., inference time per sample, memory usage) for CA, ICA, DVRL, and DRL, and compare them with end-to-end VLM approaches?\n\n(8) Have you used objective metrics (e.g., BLEU, ROUGE, human evaluation scores) to quantify the quality of image descriptions generated by different VLMs? If yes, please share the results; if not, could you conduct such an evaluation to validate the claim of \"high-fidelity\" descriptions?\n\n(9) Why do performance variations exist across commonsense categories (Table A.7)? For example, why does GPT-4o achieve 100% accuracy on \"Taste/Nutrition/Food\" while Gemini 2.0 only achieves 85.71%?\n\n(10) Could you extend the error analysis to include systematic categorization of errors (e.g., perceptual errors, rule extraction errors, rule application errors) and report the distribution of error types across models and benchmarks?\n\n(11) Have you evaluated the framework's performance on adversarial examples (e.g., noisy images, distorted objects) or out-of-distribution samples? If yes, please share the results; if not, could you explain the framework's robustness in real-world scenarios?\n\n(12) How does the JSON description format in CA compare to other representation formats (e.g., free-text descriptions, structured bullet points) in terms of reasoning performance? Could you provide a head-to-head comparison?\n\n(13) Could you detail the question generation strategy in ICA? How does the reasoning LLM prioritize which visual details to probe, and how do you ensure the questions are not redundant or irrelevant?\n\n(14) Have you quantified the complexity of rules in the benchmarks (e.g., number of attributes, logical relationships)? If so, how does the framework's performance correlate with rule complexity?\n\n(15) For text-only LLMs, what types of visual tasks do you find the modality transfer fails? Could you provide specific examples and explanations?\n\n(16) Could you conduct a detailed head-to-head comparison with Prism and state-of-the-art CoT methods, including architectural differences, computational costs, and performance trade-offs?\n\n(17) Have you evaluated the perception module in isolation (e.g., accuracy of object detection, attribute recognition, spatial relationship identification)? If yes, please share the results to quantify the impact of perceptual errors on overall performance.\n\n(18) Why did you choose specific LLMs (e.g., GPT-4o, Gemini 2.0) as reasoning engines in CA/ICA? Could you test weaker LLMs (e.g., Llama3-8B, Mistral-7B) to see if the framework's effectiveness is dependent on reasoning module strength?\n\n(19) Have you tested the framework's performance in non-English languages (e.g., Chinese, Spanish)? If yes, please share the results; if not, could you discuss potential challenges for multilingual adaptation?\n\n(20) How does the framework perform with fewer shots (e.g., 1-shot, 2-shot) compared to the reported few-shot setting? Could you provide results to demonstrate its performance in low-resource scenarios?\n\n(21) Have you analyzed how the training data distribution of VLMs affects framework performance? For example, do VLMs trained on more HOI data perform better on Bongard-HOI under your paradigms?\n\n(22) Could you supplement the evaluation with additional metrics (e.g., precision, recall, F1-score, confusion matrices) to provide a more comprehensive view of performance, especially for imbalanced subsets?\n\n(23) How does the dominant category (ID 0) in the Bongard-OW subset affect model training and generalization? Could you test the framework on a more balanced subset to validate robustness?\n\n(24) Could you provide a cost-benefit analysis of ICA's feedback loop, including the additional inference time and memory usage compared to CA, and whether the performance gain justifies the added complexity?\n\n(25) Have you tested the impact of image resolution (e.g., 512px, 2048px) on description quality and reasoning performance? If yes, please share the results; if not, could you discuss how resolution affects the framework?\n\n(26) Have you evaluated the consistency of description generation (e.g., same image, multiple runs)? If yes, please provide quantitative results (e.g., consistency rate); if not, could you explain how you ensure description reliability?\n\n(27) Could you quantify the accuracy of rule extraction (e.g., similarity between extracted rules and ground-truth rules using semantic similarity metrics)? This would strengthen the link between rule quality and classification performance.\n\n(28) Could you compare the framework's reasoning process with human reasoning steps (e.g., sequence of analysis, focus on key details) using qualitative case studies?\n\n(29) For open-source models with low CA accuracy (e.g., Llama-Vision-11B), have you conducted a detailed analysis to determine whether the gap stems from poor description generation, weak reasoning, or both? Could you provide evidence (e.g., replacing descriptions with GPT-4o's for these models)?\n\n(30) How would you extend the paradigms to video reasoning? Could you outline a preliminary design and discuss potential challenges (e.g., handling temporal dynamics, reducing computational cost)?\n\n(31) Have you analyzed potential biases in description generation (e.g., racial, gender, cultural biases) and their impact on reasoning outcomes? If yes, please share the results; if not, could you discuss how to mitigate such biases?\n\n(32) Why did you use zero temperature for all models? Have you tested other temperature values (e.g., 0.3, 0.7) and their impact on paradigm performance?\n\n(33) Could you test the framework on newly developed VLMs (e.g., Llama-4, Mistral Large) to demonstrate its cross-model generalization ability?\n\n(34) Have you analyzed the impact of description length on reasoning performance? For example, do shorter, more concise descriptions perform as well as longer, detailed ones?\n\n(35) Are the paradigms tailored to the tested benchmarks, or do they generalize to unseen visual reasoning tasks? Could you test on a new benchmark (e.g., a custom dataset) to validate generalization?\n\n(36) Could you explore bidirectional communication between perception and reasoning modules beyond ICA's feedback loop? For example, could the perception module proactively flag ambiguous details to the reasoning module?\n\n(37) Have you tested the framework with low-resource VLMs/LLMs (e.g., models with <7B parameters)? If yes, please share the results; if not, could you discuss the framework's accessibility for resource-constrained users?\n\n(38) Could you provide case studies of applying the framework to practical real-world tasks (e.g., image retrieval, visual question answering, medical image analysis) to demonstrate its practical value?\n\n(39) For Winoground, why do the Image Score and Group Score remain lower than the Text Score even with ICA? Could you analyze the specific challenges that prevent further improvements in these metrics?\n\n(40) Could you provide detailed reproducibility instructions, including exact prompt templates, model versions, hardware specifications, and step-by-step evaluation pipelines, to ensure other researchers can replicate your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W7lUeI5qGu", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Reviewer_TEuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Reviewer_TEuy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919716649, "cdate": 1761919716649, "tmdate": 1762923554728, "mdate": 1762923554728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cognitively inspired framework that decouples perception from reasoning for multi-image visual reasoning. It introduces four paradigms: DVRL (holistic, all images at once), DRL (rule extraction → rule application), CA (reasoning over task-agnostic textual descriptions), and ICA (CA with an interactive feedback loop where the reasoner can look again). The framework is evaluated on Bongard-OW, Bongard-HOI, and Winoground, showing a consistent trend DVRL < DRL < CA, and additional gains from ICA. The central claim is that many VLMs are limited primarily by perception bottlenecks; when perception is bypassed via high-fidelity descriptions, downstream reasoning can be very strong, even for text-only LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem decomposition. CA creates a clean separation between perceptual description and downstream reasoning; ICA adds a bidirectional loop that lets reasoning guide perception. The design is well motivated and consistently effective across tasks."}, "weaknesses": {"value": "- The evaluation centers on GPT-4o, Gemini 2.0, and a set of 2024-era open models (Qwen2.5, Llama-Vision, Pixtral, etc.). There is no direct evaluation of 2025 releases such as GPT-5, Gemini-2.5, R1-series variants, or Qwen3-VL, despite the paper positioning itself as a diagnostic/prognostic framework. Given that limitations like perception bottlenecks were already raised by BLINK and PRISM, the absence of recent frontier models makes it hard to judge whether the proposed remedies remain necessary at the current frontier."}, "questions": {"value": "- Can you include a 2025 refresh (e.g., GPT-5, Gemini-2.5, R1-series 2025, Qwen3-VL) to validate whether the perception bottleneck and CA/ICA gains persist at the current frontier? A small but representative subset on Bongard-OW/HOI and Winoground would suffice.\n\n- Beyond text intermediates. Do you foresee non-linguistic symbolic or visual-token intermediates that retain CA’s separability but cover BLINK-type cases (depth/correspondence) better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G14ePImXLn", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Reviewer_PgfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Reviewer_PgfV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086609764, "cdate": 1762086609764, "tmdate": 1762923554041, "mdate": 1762923554041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces four \"cognitively inspired\" evaluation paradigms, DVRL, DRL, CA, and ICA, to explicitly decouple visual perception from symbolic reasoning in multi-image reasoning tasks, like Bongard-style benchmarks (OW/HOI) and Winoground."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The reported results show clear, monotonic improvements from DVRL to DRL and to CA, and additional boosts with ICA, showing that the framework is practically useful."}, "weaknesses": {"value": "1. The contribution is primarily a workflow. The paper does not convincingly justify why this workflow design works, like experiments for motivation. Ablations on prompt choices, alternative perception backbones, and end-to-end VLMs under matched constraints are also limited, making the empirical case insufficient for a main-track ML venue focused on algorithmic advances.\n2. The evaluation subset of Bongard-OW is built from the first 250 items to produce 500 cases, and the class distribution is highly skewed (73% are class 0). This raises concerns about hidden biases. \n3. The systematic study of description granularity/length/structure are underexplored."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MKOwcfNlN3", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Reviewer_eYbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Reviewer_eYbX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762632373468, "cdate": 1762632373468, "tmdate": 1762923553574, "mdate": 1762923553574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reviewer Misunderstandings vs. Paper Evidence"}, "comment": {"value": "### Table 1: Dossier of Reviewer Misunderstandings vs. Paper Evidence\n\n| Reviewer | Reviewer's Claim | Paper Evidence (Location) | The Fact |\n| :--- | :--- | :--- | :--- |\n| **eYbX** | \"Ablations on... end-to-end VLMs... are also limited.\" | **Sec 7.2, Table 5** | An extensive ablation *is* provided, showing (e.g.) Llama-Vision-90B performance jumping from **55.1% to 90.98%** when the perception bottleneck is bypassed. |\n| **PgfV** | \"Limitations... [like] BLINK-type cases\" are not discussed. | **Sec 9, \"Limitations\"** | The paper *explicitly* cites and discusses **BLINK** in the limitations section as a task for which text-mediation is unsuited. |\n| **nJ3X** | **(CRITICAL ERROR)** \"A.5.3.1... given many... examples of specific visual concepts from the Bongard-OW... datasets.\" | **Appendix A.5.3.1** | **False.** The prompt *only* gives generic schema placeholders. This claim is a complete misreading. |\n| **nJ3X** | \"The curation process... appears to be arbitrary.\" | **Appendix A.3.1** | **False.** The process was systematic: \"taking **250 samples**... resulting in 500 balanced test cases\". This is the opposite of arbitrary. We can make it more explicit. |\n| **nJ3X** | \"The paper also lacks novelty.\" | **Sec 5.4 (ICA)** | **False.** The paper proposes a novel **Interactive Componential Analysis (ICA)** paradigm with a *dynamic feedback loop*, distinct from all cited static-decoupling works (e.g., PRISM). |\n| **TEuy** | (W1) \"lacks sufficient discussion on limitations in tasks requiring non-verbalizable reasoning\" | **Sec 9, \"Limitations\"** | **False.** This is identified as the \"**primary limitation**\". The text explicitly cites **BLINK** as an example of a \"non-verbalizable\" task where the \"utility of a purely text-mediated approach may be reduced.\" |\n| **TEuy** | (W3) \"The selection basis for the 500-case Bongard-OW subset is not clearly explained.\" | **Appendix A.3.1** | **False.** The methodology is explicitly stated: \"taking **250 samples**\" to create 500 cases. This is a systematic, non-random, and reproducible method. |\n| **TEuy** | (W12) \"Error analysis is limited... without systematic categorization of errors\" | **Sec 7.3 & Table A.9** | **False.** The paper provides this exact systematic categorization. **Table A.9**'s \"Reason for Error\" column explicitly categorizes failures (e.g., \"**Rule extraction error**,\" \"**Perceptual description error**,\" \"**Weak reasoning**\"), which are then summarized in **Section 7.3**. |\n| **TEuy** | (W20) \"Reasoning module selection is not justified... [does not test] whether weaker LLMs would still yield effective results.\" | **Sec 7.2, Table 5** | **Critical Misunderstanding.** This is a *central finding* of the paper. **Table 5** is *entirely* dedicated to this test, showing that weaker, text-only LLMs like **Qwen2.5:7b (90.38%)** and **Phi4:14b (91.98%)** achieve SOTA-level results, proving the method's effectiveness with weaker LLMs. |\n| **TEuy** | (W31) \"Open-source model performance gap analysis is inadequate... does not deeply investigate whether the gap stems from... poor description generation, weak reasoning, or both.\" | **Sec 7.2, Table 5** | **Critical Misunderstanding.** This is the *primary goal and finding* of Section 7.2. The paper *proves* the gap stems from description generation by showing Llama-Vision-90B's score leaps from **55.1% (Table 1) to 90.98% (Table 5)** when its own poor descriptions are replaced with high-quality ones. |\n| **TEuy** | comparisons with recent state-of-the-art VLMs (e.g., Claude 3 Opus, Gemini Ultra, Qwen-VL Max) |  | All the models cited are older than what we have used in the paper |"}}, "id": "hnCmcleFSN", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763073665894, "cdate": 1763073665894, "tmdate": 1763074635428, "mdate": 1763074635428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Comment to PC and AC and Reviewers"}, "comment": {"value": "Dear Area Chairs, Program Chairs, and Reviewers,\n\nWe are grateful for the time and effort all reviewers dedicated to our submission. We have provided detailed, point-by-point responses to each reviewer.\n\nHowever, in preparing our responses, we identified a significant pattern of *critical, factual misunderstandings* that form the primary basis for the lower-scoring reviews (e.g., the 'Strong Reject' 0 and the 4-score review).\n\nThese are not subjective disagreements about our paper's impact but *objective, verifiable discrepancies* between the reviewers' claims and the explicit content of our submitted paper and its appendix. Because these claims underpin the \"poor\" and \"reject\" ratings, we feel it is crucial to highlight them for the Area Chair's consideration.\n\nWe recognize the immense workload of the ACs and PCs. To facilitate your evaluation of the *reviews'* soundness, we compiled the attached \"`Dossier of Reviewer Misunderstandings`\" (Table 1). This table provides a clear, side-by-side ledger of the most damaging reviewer claims versus the *direct, citable evidence* from our paper that refutes them.\n\nWe respectfully ask that our paper's evaluation be based on the *content of our submission itself*—which achieves SOTA results, surpasses human performance on Bongard-OW, and provides the very ablations and limitations the reviewers claimed were missing. We are confident that an evaluation based on this evidence will demonstrate that the critiques underpinning the low scores are, with all due respect, factually unfounded.\n\nThank you for your time and careful reconsideration."}}, "id": "L67R0HDWh9", "forum": "i0zjotaTnv", "replyto": "i0zjotaTnv", "signatures": ["ICLR.cc/2026/Conference/Submission12733/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12733/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12733/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763073989089, "cdate": 1763073989089, "tmdate": 1763076275058, "mdate": 1763076275058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}