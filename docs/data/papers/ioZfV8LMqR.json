{"id": "ioZfV8LMqR", "number": 2431, "cdate": 1757085232130, "mdate": 1759898148425, "content": {"title": "Improving Few-Shot Design Optimization By Exploiting Auxiliary Information", "abstract": "Many real-world design problems involve optimizing an expensive black-box function $f(x)$, such as hardware design or drug discovery. Bayesian Optimization has emerged as a sample-efficient framework for this problem. However, the basic setting considered by these methods is simplified compared to real-world experimental setups, where experiments often generate a wealth of useful information. We introduce a new setting where an experiment generates high-dimensional auxiliary information $h(x)$ along with the performance measure $f(x)$; moreover, a history of previously solved tasks from the same task family is available for accelerating optimization. A key challenge of our setting is learning how to represent and utilize $h(x)$ for efficiently solving new optimization tasks beyond the task history. We develop a novel approach for this setting based on a neural model which predicts $f(x)$ for unseen designs given a few-shot context containing observations of $h(x)$. To evaluate our method, we develop a new benchmark task involving designing customized robotic grippers for stably grasping objects. On this task, our approach which incorporates $h(x)$ significantly outperforms a baseline which only uses reward information, demonstrating improved few-shot prediction capability and more efficient optimization.", "tldr": "We introduce a new black-box optimization setting closer to real-world design problems, where a trial provides auxiliary information beyond reward and a task history is available; we propose a novel few-shot prediction approach and benchmark task.", "keywords": ["Black-box optimization", "Bayesian optimization", "Few-shot Learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad62ea2cba18ed0158f36fc588c6f8b511761d39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new few-shot design optimization problem setting in which, rather than observing only scalar objective values f(x), the designer also has access to auxiliary information h(x) from each evaluation. The authors argue that such side information (often high-dimensional or structured) is commonly available in real-world design tasks and can provide valuable signal for generalization across related problems. To address this setting, the paper proposes a new method which can leverage the auxiliary data to accelerate optimization for new tasks. The authors also introduce a new gripper design benchmark designed to capture this “auxiliary information” scenario and demonstrate that their method improves both prediction accuracy and optimization performance relative to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\n\nThe paper introduces a novel and practically motivated few-shot design optimization setting in which evaluations yield not only ypical scalar objective values f(x) but also auxiliary information h(x). This framing is original and formalizes a situation that commonly arises in real-world scientific and engineering design tasks. Although this problem setting relevant for many real-world tasks, as far as I am aware, this problem setting is largely absent from current optimization literature. The author's proposed method is also novel and presents a principled approach to this problem setting. \n\nQuality:\n\nThe method itself is technically sound and well-motivated. The paper clearly describes how the auxiliary information is incorporated into the predictive model and presents results showing improved performance compared to a “no-h(x)” variant of their approach. The experimental design appears careful and internally consistent. The results demonstrate that modeling auxiliary information provides benefits within the proposed setup. \n\n\nClarity:\n\nThe paper is generally well-written and easy to follow. The motivation for the problem is clearly articulated. \n\n\nSignificance:\n\nThe problem setting has high potential impact. Many real-world optimization problems (drug discovery, materials design, robotics, etc.) generate auxiliary structured measurements or sensor readouts alongside objective values. The idea of using these for transfer is compelling. This work could open a new line of research into auxiliary-data-aware optimization frameworks."}, "weaknesses": {"value": "Weakness 1: Limited Evaluation Scope\n\n\n\nThe method is evaluated only on a single benchmark: a custom gripper design task introduced by the authors. While this task is interesting, a single domain makes it difficult to assess general applicability. There are many relevant settings (e.g., molecular design, materials design) where auxiliary signals naturally arise, and it would strengthen the paper considerably to include at least 2–3 additional benchmarks, even if lightly adapted from existing benchmarks to produce auxiliary outputs.\n\n\n\nWeakness 2: Lack of Comparison to Existing Methods\n\n\nAs far as I could tell, the paper does not directly compare against any state-of-the-art black-box optimization methods from the literature (e.g., existing Bayesian optimization methods). Even though existing methods don’t provide a mechanism to leverage auxiliary h(x) information, they could still be included as baselines by applying them to optimize f(x) directly. Without these direct comparisons, it is difficult to quantify the empirical advantage of the author’s method."}, "questions": {"value": "1: Generality: Do you expect the proposed framework to transfer easily to domains such as molecular or materials design? \n\n\n2: Baselines: Did you attempt to run any existing BO methods as baselines? Even if they ignore h(x), such results would be valuable to show whether auxiliary modeling yields measurable gains. Is there any reason why you opted not to include such baseline comparisons? Maybe I'm missing something here? \n\n\n3: Reproducibility: Will the code and benchmark be released publicly upon acceptance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SRqh2FJj86", "forum": "ioZfV8LMqR", "replyto": "ioZfV8LMqR", "signatures": ["ICLR.cc/2026/Conference/Submission2431/Reviewer_kZ6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2431/Reviewer_kZ6t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760997168994, "cdate": 1760997168994, "tmdate": 1762916235658, "mdate": 1762916235658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a design optimization setting where, in addition to expensive black‑box evaluations f(x), each experiment yields high‑dimensional auxiliary information h(x). They further assume access to a history of related tasks that share the structure of h, and train a neural surrogate to perform a few‑shot probabilistic prediction of f(x) for unseen x. The model is transformer‑based and is used within a BayesOpt loop with Probability of Improvement. Empirically, the paper provides a new robotic gripper benchmark with tactile feedback and reports improvements over an \"f-only\" surrogate baseline in both prediction and optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates why auxiliary information h(x) is abundant in real experiments, and how it can guide the optimization process.\n2. The idea to condition a transformer surrogate on a context that includes (x,f,h) is reasonable; the architecture enforces target‑to‑context attention and separates encoders for context/targets, a sensible design for few‑shot prediction.\n3. Benchmarking effort: A gripper task with tactile sequences is constructed, and could benefit future research.\n4. On the proposed task, the method consistently outperforms an f-only surrogate."}, "weaknesses": {"value": "1. Narrow empirical scope. Despite repeatedly motivating drug discovery and hyperparameter tuning as application areas (in both sections 1 and 3), all validations are on a single, author‑created robot gripper benchmark. To substantiate the “new setting across domains” claim, please consider adding at least one additional domain:\n- HPO (e.g., Treat partial learning curves, gradients, or training diagnostics as h; test on modern benchmarks)\n- Drug Discovery\n2. The paper mainly compares with an f-only surrogate (and a size‑matched f-only(+p) variant and nearest‑neighbor). However, I'm wondering whether some methods can be applied to this setting as well.\n- Based on the composite BO formulation, can we learn a mapping from h to z and a surrogate model $g_{\\theta}(z)$ $\\approx$ f(x)? so that methods used for the composite BO problem can be used in the proposed setting as well?\n- In addition to the f-only surrogate, I'm curious how Multi-task Gaussian Process Prediction[1] perform in the proposed setting, to test whether gains come from h vs. generic meta‑learning.\n- Some recent works proposed to use pretrained LLMs as optimizers [2], in which we can also easily provide additional information and context through prompting. I am also curious about how SOTA LLMs, such as GPT-4o or GPT-5, perform in the proposed setting?\n\n[1] Bonilla, Edwin V., Kian Chai, and Christopher Williams. \"Multi-task Gaussian process prediction.\" Advances in neural information processing systems 20 (2007).\n\n[2] Yang, Chengrun, et al. \"Large language models as optimizers.\" The Twelfth International Conference on Learning Representations. 2023."}, "questions": {"value": "Please see my main concerns above.\n\nAdditionally, there are some typos and inconsistencies regarding the paper writing:\n- “inrtoduced” at the start of section 4 (line 203); \n- “Fig. 2b” is referenced (line 269), though either the Figure 2 caption or Figure 2 itself does not label subfigures (left/right only); both should be fixed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPqPPoeWCJ", "forum": "ioZfV8LMqR", "replyto": "ioZfV8LMqR", "signatures": ["ICLR.cc/2026/Conference/Submission2431/Reviewer_YWaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2431/Reviewer_YWaS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573449820, "cdate": 1761573449820, "tmdate": 1762916235491, "mdate": 1762916235491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new few-shot design optimization framework that aims to integrate auxiliary high-dimensional information (e.g., sensor time series or tactile feedback) with traditional scalar performance signals in black-box optimization. The authors extend the classical Bayesian Optimization paradigm by assuming that each evaluation yields both a scalar reward f(x) and auxiliary data \nh(x), and that historical tasks with shared auxiliary structures are available.\n\nThey propose a transformer-based neural process model that predicts f(x) for unseen designs conditioned on a few-shot context of prior evaluations including h(x). The model serves as a surrogate for optimization via a probabilistic acquisition function. To benchmark this setting, the paper introduces a new large-scale tactile robotic gripper design task built in MuJoCo, containing ~4.3M simulations across nearly 1K ShapeNet objects. Experiments demonstrate that incorporating h(x) yields significant gains in both few-shot prediction accuracy and Bayesian optimization efficiency compared to baselines that only use f(x).\n\nOverall speaking, this paper sounds interesting, and the reasoning process is generally reasonable and easy to understand. However, the core design, the blurry algorithmic illustration, and novelty of this paper caused some skepticism, spanning from the novelty of the idea, and the benchmark choice. I would like to see how the authors respond to my comments and make decision after engaging with the discussion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally reasonable and self-contained, idea level speaking. The experimental design, though looks like a bit limited to the only one benchmark, is overall sound.  The results and the proposed improvement sounds reasonable.  The problem setting considered in the paper is also an interesting problem."}, "weaknesses": {"value": "I will list my skepticism below:\nFirst of all, one of the core idea - leveraging context information, is not always a very novel idea as of now. The recent advances in multi-modal foundation model, such as vision language model or the more recent VLA style model have already proven that integrating multi-modal information is beneficial for model generalizability throughout large-scale training. In other words, in the proposed setting, auxiliary information can be seen as 'extra modality' which encodes useful information. If going from this understanding perspective, I would like to ask the author to again justify the novelty of this idea, and re-consider the related work /baselines that may be worth to compare, such  as a RL algorithm (such as DQN with memory replay) that can take both tactile time series and f(x).\n\nSecond, the overall optimization process and the actual few-shot training / evaluation algorithm remains a bit of unclear. Can the authors provide a detailed pseudo code / algorithmic illustration on this part? Without this, it will be a bit hard to understand and justify the few-shot design and the actual utility of the model.\n\nThird, my worry comes from only having one benchmark to serve as the testbed. Also, since it is a new benchmark, some of the measurement utility worth a further clarification. For example, how do we interpret the scale of MSE and the value utility  reported in Figure 4 and 5? Also, it seems like the number of total design is ~4M - pretty large spanning from ~1K object (not that large). I thus wonder would the author call it large-scale or not? In some sense, large-scale dataset usually means a lot of computing resources to be put on the model training, however, the majority of the implementation details is about the implementation of the model architecture. In a new benchmark, and a new task, such information should be clearly addressed in order to fully address the unclarity."}, "questions": {"value": "1. How sensitive is performance to the dimensionality or noise in h(x)? Do the authors have any ways to justify the information density in h(x)? Also, how would the authors justify the more general use case of their algorithm, when h(x) is more complex? E.g., can the model handle tasks where h(x) has different modalities or missing components?\n\n2. Would a pre-trained encoder for h(x) (e.g., self-supervised tactile embedding) further help few-shot adaptation?\n\n3. Have the authors tested continuous search spaces with gradient-based acquisition (as mentioned in Sec. 4.3)?\n\n4. Dataset & Code publicity: Any plans here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z3Miw0lXBd", "forum": "ioZfV8LMqR", "replyto": "ioZfV8LMqR", "signatures": ["ICLR.cc/2026/Conference/Submission2431/Reviewer_Qty9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2431/Reviewer_Qty9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935839726, "cdate": 1761935839726, "tmdate": 1762916235340, "mdate": 1762916235340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of evaluating human design performance in few-shot, open-ended tasks, where contextual uncertaint can mask true human capability. The authors propose a framework for controlling contextual difficulty by: (1) Developing a context calibration model that predicts how challenging a given design prompt is likely to be. (2) Using this model to select more controlled prompts or to adjust evaluation metrics accordingly. They validate the approach across three human-in-the-loop, few-shot design domains: instruction writing, question generation, and visual layout design, demonstrating improved correlation between observed performance and true ability when controlling for prompt difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a previously underexplored confound in few-shot human evaluation: variation in prompt difficulty leads to noisy or misleading performance measurements, particularly in low-sample regimes.\n\n2. The authors propose a label-efficient, interpretable framework that predicts prompt difficulty using features derived from early model outputs and prompt surface characteristics. It works across domains: text generation and visual design."}, "weaknesses": {"value": "1. Lack of evaluation in non-LLM or human-annotated settings: The difficulty model’s output quality (and evaluation scores) depend heavily on LLM-generated outputs and ratings. The paper does not include human-annotated baselines to validate the model-based evaluation pipeline. E.g., in appendix B.2., evaluations in instruction writing and question generation use GPT-4 or LLaMA-2 reward models, without human verification.\n\n2. Lack theoretical justification for difficulty modeling assumptions: The approach assumes that prompt difficulty can be accurately and stably inferred from features like entropy, diversity, and error rate, but provides no theoretical justification or bounds on how reliably these features reflect true difficulty. E.g., no discussion of how error in the model’s predictions affects downstream decisions (e.g., participant ranking or prompt selection)."}, "questions": {"value": "1. In this work, difficulty is treated as a single scalar, despite possibly being multi-dimensional in some real-world cases (e.g., lexical complexity vs. semantic ambiguity). How to deal with such situation?\n\n2. In Section 4, each task (instruction writing, QG, layout design) is treated independently. Does the method also work well for cross-domain? (e.g., train on QG, test on instruction writing).\n\n3. In Appendix B.1, population statistics are from specific curated datasets. I am curious about the analysis on cross-demographic robustness (e.g., whether the difficulty model would hold for people of different groups and backgrounds) or prompt representation bias (e.g., are prompts equally representative of different content domains)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3z1mEUwOfc", "forum": "ioZfV8LMqR", "replyto": "ioZfV8LMqR", "signatures": ["ICLR.cc/2026/Conference/Submission2431/Reviewer_Uuep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2431/Reviewer_Uuep"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937240315, "cdate": 1761937240315, "tmdate": 1762916235129, "mdate": 1762916235129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}