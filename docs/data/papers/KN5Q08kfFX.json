{"id": "KN5Q08kfFX", "number": 24050, "cdate": 1758352181501, "mdate": 1759896784384, "content": {"title": "MetaModelSelect: A Lightweight Post-hoc Metamodel for Selective Classification", "abstract": "Selective classification equips neural networks with a reject option, abstaining on low‑confidence inputs. Most post‑hoc selectors compress the logit vector into a single scalar (e.g., maximum softmax probability or energy), discarding structure in the remaining logits. We introduce MetaModelSelect, a lightweight two‑layer metamodel ($\\approx 49$k parameters, <1ms overhead) trained on a frozen backbone to predict per‑example correctness. The metamodel leverages (i) a learnable embedding of the predicted class, (ii) the top‑3 entries of the normalized logit vector $\\tilde z = z/\\\\|z\\\\|_{p^\\*}$, and (iii) a logit‑concentration statistic $h(z) = \\\\tfrac{1}{C}\\\\sum_i \\\\tilde z_i^{\\\\,2}$. On ImageNet‑1k, Stanford Cars, and the long‑tailed iNaturalist‑2019, MetaModelSelect achieves state‑of‑the‑art risk–coverage with relative AURC reductions of 2.0-4.2\\% over tuned MSP, Energy, and MaxLogit-$p$-Norm baselines, without additional data or backbone retraining.", "tldr": "MetaModelSelect is a low‑parameter post‑hoc metamodel that combines class embeddings, top‑3 p‑normalized logits, and a logit‑concentration feature to achieve state‑of‑the‑art risk–coverage (AURC) on ImageNet‑1k, Stanford Cars, and iNaturalist‑2019.", "keywords": ["Selective classification", "Risk–coverage / AURC", "Uncertainty estimation", "Post‑hoc confidence estimation", "Calibration", "Metamodel / post‑hoc selector"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79e6427b05e7bb3a8130e020c890df54a4853e80.pdf", "supplementary_material": "/attachment/5f16c47c26c827ecc01e726c614699ad7bb5878a.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduced the MetaModelSelect a post-hoc selective classification approach. The main contributions lie in making use of local logits geometry (top-3 p-normalised) and global logit concentration, together with class-specific feature into a single model. MetaModelSelect improves RC curve and achieves AURC between 2.2-3.7% over similar baselines showing empirical gains. The model do not require additional data or backbone retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well defined feature design, combining local logit geometry and global concentration with class priors via embedding shows a novel formulation for selective classification.\n\n- The lightweight approach 49k-parameters with < 1ms overhead show the authors concern to efficiency and it is a welcome contribution to the area."}, "weaknesses": {"value": "- MetaModelSelect result gains are hard to define if they are substantial given the narrow scope. The paper deliberately did not compared against stronger baselines (multiple-pass post-hoc scores) makes it hard to assess absolute progress.  I suggest the authors to include stronger baselines and discuss the performance deltas, in terms of predictions and computational requirements, to clarify absolute progress."}, "questions": {"value": "- Calibration is a known problem in selective approaches, can the authors elaborate on the calibration procedure.\n\n- How stable is the chosen p-norm across seeds/datasets? Any benefit from learning p end-to-end within the metamodel?\n\n- An important ablation study is top-k sensitivity of the method. I asked myself with k=3 and not 2 or 5? Can the authors provide some small experiment and elaborate on why k=3. \n\n- Could you make the class embedding input-aware (conditioned on metadata) while staying post-hoc?\n\n- How would MetaModelSelect compare against learning linear combination of the tested baselines(Energy, Max Logit and p_Norm)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gA3FHqfVNM", "forum": "KN5Q08kfFX", "replyto": "KN5Q08kfFX", "signatures": ["ICLR.cc/2026/Conference/Submission24050/Reviewer_a6Wp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24050/Reviewer_a6Wp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761213806358, "cdate": 1761213806358, "tmdate": 1762942913362, "mdate": 1762942913362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a new post-hoc approach for selective classification called MetaModelSelect.\nMetaModelSelect is a simple two-layer neural network trained on top of pre-existing classifiers that aims to predict per-sample correctness.\nExperimental evaluation over three datasets shows (small) improvements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The main strengths are:\n\n1. The approach is intuitive and straightforward, making it easy to implement in real-life cases\n2. The authors perform experiments over two large datasets, i.e., iNaturalist and ImageNet, supporting the usage of their approach on large-scale benchmarks;"}, "weaknesses": {"value": "In my view, these are the main shortcomings of the paper:\n\n*Novelty:* I am not fully convinced by the novelty of the approach. \n1. First, it seems to me that the only modification to ConfidNet [1] is the explicit usage of  predicted class embeddings, top-k local logits and concentration measures,  while in ConfidNet, the authors do not explicitly consider this possibility (but still learn an entire network to predict possible mistakes). I would argue $(i)$ this change is rather incremental compared to ConfidNet; $(ii)$ I would like to see how the proposed approach compares w.r.t. ConfidNet. \n2. Second, the proposed approach seems once again a slight modification to the regression approach proposed in [2] (Theorem 9), where the authors advocate for the usage of a post-hoc trained model. I think the authors should discuss this point in detail.\n\n*Clarity* While the overall idea is clear enough, I think the paper could benefit from a better writing.\n1. some choices seem quite arbitrary (e.g., why top 3 logits and not top 4 logits?). The authors should motivate this better.\n2. The experimental part is quite confusing. E.g., the authors state *we report accuracy (or error) at coverages C*. I think the authors should be clear on how they evaluate the methods.\n\n*Soundness* I do not understand what the advantage is of considering a pre-defined set of transformations over the logits compared to considering a deeper neural network, which could extract the same information starting from the original logits.\n\n*Relation with previous works* A few benchmarks have been proposed to evaluate existing selective classification methods, i.e., [3] and [4]. In [3], the authors show that there is no clear winner across methods in terms of performance; hence, the results are not particularly surprising when evaluated on only 3 datasets. Moreover, I wonder how the proposed approach works w.r.t. $(1)$ coverage failures (as shown in [3]) and $(2)$ w.r.t. the AUGRC metric proposed in [4].\n\n\n**References**\n\n[1] - Corbière, C., Thome, N., Bar-Hen, A., Cord, M., & Pérez, P. (2019). Addressing failure prediction by learning model confidence. Advances in neural information processing systems, 32.\n\n[2] - Franc, V., Prusa, D., & Voracek, V. (2023). Optimal strategies for reject option classifiers. Journal of Machine Learning Research, 24(11), 1-49.\n\n[3] - Pugnana, A., Perini, L., Davis, J., & Ruggieri, S. (2024). Deep Neural Network Benchmarks for Selective Classification. Journal of Data-centric Machine Learning Research.\n\n[4] - Traub, Jeremias, Till J. Bungert, Carsten T. Lüth, Michael Baumgartner, Klaus H. Maier-Hein, Lena Maier-Hein, and Paul F. Jäger. \"Overcoming common flaws in the evaluation of selective classification systems.\" Advances in Neural Information Processing Systems 37 (2024): 2323-2347."}, "questions": {"value": "I have the following questions:\n\n1. please discuss the weaknesses I highlighted\n2. I did not understand how the frozen classifier is trained. Are you using the same training set to train both the classifier and the MetaModelSelect? I think this might be prone to overfitting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLbYBkOFYT", "forum": "KN5Q08kfFX", "replyto": "KN5Q08kfFX", "signatures": ["ICLR.cc/2026/Conference/Submission24050/Reviewer_r2hJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24050/Reviewer_r2hJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814077498, "cdate": 1761814077498, "tmdate": 1762942912975, "mdate": 1762942912975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new post-hoc method for selective classification via a lightweight metamodel. Specifically, the metamodel is a MLP with two hidden layers. The predicted class, top-k of normalized logits, mean of squares of the normalized logits are integrated as the input for the metamodel, which direct predicts the confidence score of whether the classifier is correct. Experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow. \n2. The proposed post-hoc method can be adapted on different base classifiers. \n3. The metamodel is lightweight, which brings little extra cost."}, "weaknesses": {"value": "1. The improvements are not significant. Statistical tests are expected. Meanwhile, the statement “2~4% AURC reductions” is not proper. Compared to the simple but effective baseline Softmax, the proposed methods seem to achieve limited improvements.\n2. The selection processes and results of (p,d,B) are not provided, which makes the results less convincing.\n3. There is no theoretical or in-depth experimental analysis on why such a simple metamodel can work well."}, "questions": {"value": "1. As the backbone model is CCL-SC, why not use the datasets in its paper?\n2. What are the final settings of hyperparameter p and d? Are the results sensitive to the choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5gA1aPX9LY", "forum": "KN5Q08kfFX", "replyto": "KN5Q08kfFX", "signatures": ["ICLR.cc/2026/Conference/Submission24050/Reviewer_9hLq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24050/Reviewer_9hLq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912219068, "cdate": 1761912219068, "tmdate": 1762942912725, "mdate": 1762942912725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to train a small auxiliary model on validation data in order to perform confidence estimation for selective classification. They demonstrate their approach leads to performance improvements on a number of image classification datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach is simple and direct. The inference cost is minimal.\n- The proposed approach leads to improved performance on the discussed benchmarks."}, "weaknesses": {"value": "The paper load for ICLR this year has been large, and so I have not been able to spend as much time as I would like on reviewing. I encourage the authors to correct any errors/misunderstandings I may have with regards to the paper.  \n\n1. **Poor presentation**\n    1. Table 1 overflows into the margin.\n    1. It is unclear what Figure 2 is trying to illustrate.\n    1. Large tables are hard to parse, when the same information could be easily conveyed using RC curves (see [1]).\n    1. Before the conclusion the authors make claims about calibration without at any point evaluating calibration error.\n    1. Surely the mean of squares measures spread, not concentration.\n2. **Lack of knowledge advancement in contribution**\n    1. Model design and feature selection is not well motivated -- in fact, in the appendix, it states that 50 different features were tried for effectiveness, a gridsearch, shotgun approach.\n    1. There is little explanation for *why* the proposed approach outperforms the baseline. From the reader's perspective they've just trained an auxiliary model on some features and shown that it performs. A better contribution would have an analysis with the conclusion e.g. \"we demonstrate the mean of squares is a useful feature *because* it captures epistemic uncertainty in logit vector\".\n3. **Experimental weaknesses**\n    1. Experiments are all based on top of the CCL-SC baseline -- if the approach is post hoc, it should generalise across various pre-training recipes. Besides, many practitioners are likely to not have used CCL-SC for their model. [1] demonstrate that certain training recipes degrade softmax SC -- the meta model approach would be more appealing if it were demonstrated to work generally, regardless of whether the pretraining recipe has degraded the softmax.\n    1. The demonstrated absolute performance improvements over softmax are modest.\n    1. No experiments on data efficiency (how many samples does the meta model need to perform well?).\n 4. **Lack of awareness of the literature**\n    1. [2] Propose a similar approach for calibration, but it is not referenced. \n    1. [1] Establish and explain that p-Norm is only effective under certain circumstances, e.g. models trained with label smoothing, and not effective for models trained with vanilla CE and data augmentations. This needs to be considered when including it in an empirical comparison.\n    1. [3] encode class-specific (and inter-class) uncertainty into a post-hoc optimisable confidence score, similar to this work, but are not referenced or compared against. \n\n\n[1] Xia et al, Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It, ICLR 2025\n\n[2] Tomani et al, Parameterized Temperature Scaling for Boosting the Expressive Power in Post-Hoc Uncertainty Calibration, ECCV 2022\n\n[3] Gomes et al, A Data-Driven Measure of Relative Uncertainty for Misclassification Detection, ICLR 2024"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5JmHnWgnNx", "forum": "KN5Q08kfFX", "replyto": "KN5Q08kfFX", "signatures": ["ICLR.cc/2026/Conference/Submission24050/Reviewer_fgif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24050/Reviewer_fgif"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930278190, "cdate": 1761930278190, "tmdate": 1762942912548, "mdate": 1762942912548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}