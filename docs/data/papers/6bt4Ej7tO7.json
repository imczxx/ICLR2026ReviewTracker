{"id": "6bt4Ej7tO7", "number": 3414, "cdate": 1757420812846, "mdate": 1759898090910, "content": {"title": "HyperKAN: A Plug-and-Play Tool for Personalized Weights Generation", "abstract": "Personalized weight generation is vital for adapting models to tailored patterns in real-world applications. However, existing methods struggle to capture nuanced feature variations across heterogeneous data distributions, resulting in suboptimal personalization performance. In this paper, we introduce HyperKAN, a plug-and-play personalized weights generator that integrates Kolmogorov-\nArnold Networks (KANs) for capturing feature variations among clients and enhancing personalization capacity. We design a novel\nPersonalized Federated Learning (pFL) framework that embed HyperKAN, enabling tailored model aggregation for each client with faster convergence. Our evaluations on four datasets present HyperKAN’s versatility and effectiveness, achieving up to 48% higher ac-\ncuracy than state-of-the-art methods. In a nutshell, HyperKAN offers a highly adaptable solution for enhancing model personalization, par-\nticularly in non-IID scenarios that challenge traditional weight generation approaches.", "tldr": "", "keywords": ["Personalized weight generation", "Kolmogorov- Arnold Networks", "Personalized Federated Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aec0b8ac77c478efe397d8f48528fc654273f040.pdf", "supplementary_material": "/attachment/cd5c3466df314bffb9af171c82862dcc30becfae.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces HyperKAN, a personalized weight generation module that employs Kolmogorov–Arnold Networks (KANs) to capture feature variations across clients and enhance personalization. Building on HyperKAN, the authors propose a new Personalized Federated Learning (pFL) framework called KpFL, which integrates HyperKAN with a critical block aggregation mechanism operating on the server side to generate personalized aggregation weights for each client.\n\nThe core problem addressed is the limitation of existing pFL methods—especially those based on hypernetworks—in modeling the complex, nonlinear relationships that arise from heterogeneous (non-IID) data distributions across clients. The authors conduct experiments on four vision datasets, demonstrating significantly higher accuracy compared to state-of-the-art methods. They also provide analyses on sensitivity, computational cost, and convergence behavior of the proposed KpFL framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Strong Empirical Performance:** The empirical results demonstrate significant improvements over state-of-the-art pFL methods: up to 48% higher accuracy on CIFAR-100, 9.6% on CIFAR-10, and 2–5% on EMNIST and FMNIST. \n\n**2. Comprehensive Experimental Evaluation:** The paper conducts a wide range of experiments, including sensitivity analysis, computational cost, convergence behavior, scalability, and interpretability of the proposed KpFL framework. This comprehensive analysis strengthens the empirical credibility of the approach.\n\n**3. Clear Problem Framing:** The motivation part is well-articulated: the paper clearly identifies the limitations of existing hypernetwork-based pFL methods in handling non-IID client data and complex nonlinear relationships. \n\n**4. Server-Side Efficiency:** A key practical advantage is that HyperKAN operates entirely on the server, introducing no additional computational or memory burden on clients. This becomes important, especially in resource-constrained federated environments."}, "weaknesses": {"value": "**1. Missing Critical Ablations:**\nThe experiments compare the full KpFL framework against other baselines (e.g., pFedLA, FedAvg). However, it remains unclear whether the large performance gains stem primarily from the critical-block aggregation strategy (updating only top-k blocks) rather than the use of KANs themselves.\nA key ablation is missing: how would performance change if a standard MLP with the same parameter count replaced the KAN? Without this, it is difficult to disentangle whether the improvements come from KAN-specific expressivity or simply higher capacity.\n\n**2. Experimental and Reproducibility Details:**\nThe paper claims that each baseline was “individually tuned for optimal performance,” but no details are provided. For fair comparison, it would help to include:\n\n- A table of final hyperparameters per method and dataset.\n\n- Compute budget parity (epochs, rounds) across methods.\n\n- Multiple seeds with error bars, instead of a single seed.\n\n\n**3. Limited Generalization Beyond Vision Tasks:**\nAll experiments are conducted on vision datasets with a relatively small CNN–BN backbone. Can the proposed method generalize to non-vision domains such as tabular or text data?\nTo truly support the claim that HyperKAN is a plug-and-play generator, it would be valuable to include results with a different backbone or a non-visual modality.\n\n**4. Critical-Block Mechanism Justification:**\nThe empirical choice of K = 2 (Figure 8) is also dataset-specific. How would this hyperparameter behave under deeper architectures or more complex tasks? This component adds an extra layer of tuning, which may undermine the framework’s “plug-and-play” characterization.\n\n**5. Presentation and Clarity of Mathematical Descriptions:**\nWhile the figures are generally informative, the textual presentation could be improved. Some symbols (e.g., $v_i$ or block-wise variation) appear before being defined or are only clarified within figures, making it difficult to follow the mathematical reasoning linearly.\nAdditionally, the figure order does not always match their first appearance in the text. Was this intentional for narrative reasons? If so, a brief explanation would help; otherwise, reordering could improve readability.\n\n**6. Incomplete Literature Coverage in pFL Section:**\nThe related-work section overlooks some well-known and highly cited pFL papers. Including them would strengthen the positioning of this work, especially since HyperKAN builds upon this lineage. For example:\n\n- Personalized Federated Learning with Moreau Envelopes — Dinh et al., NeurIPS 2020.\n\n- Exploiting Shared Representations for Personalized Federated Learning — Collins et al., ICML 2021.\n\n\n### **Minor Points**\n\n**1. Formatting and Spacing:** Vertical spacing between figures and text (e.g., Figures 6 and 13) should be increased for readability.\n\n**2. Consistency:** Line 44 — correct “pFl” → “pFL”."}, "questions": {"value": "See the Weaknesses section, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AaUxXWTDHm", "forum": "6bt4Ej7tO7", "replyto": "6bt4Ej7tO7", "signatures": ["ICLR.cc/2026/Conference/Submission3414/Reviewer_gt57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3414/Reviewer_gt57"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710931226, "cdate": 1760710931226, "tmdate": 1762916713615, "mdate": 1762916713615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HyperKAN, a plug-and-play personalized weight generator integrated with Kolmogorov-Arnold Networks (KANs), and embeds it into a novel Personalized Federated Learning (pFL) framework called KpFL. KpFL also incorporates a critical block aggregation mechanism to select top-k parameter-variation blocks for aggregation, accelerating convergence. Evaluations on four datasets (FMNIST, CIFAR-10, CIFAR-100, EMNIST) show HyperKAN achieves up to 48% higher accuracy than state-of-the-art methods (e.g., pFedLA, Floco), especially in non-IID scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ To some extent, replacing traditional MLPs with KANs in weight generation is a novel idea. Moreover, HyperKAN effectively captures complex non-linear relationships in heterogeneous data, addressing the representational bottleneck of existing hypernetwork-based pFL methods.\n+ HyperKAN operates on the server-side and requires minimal modifications to existing pFL pipelines, enhancing its applicability in real-world scenarios (e.g., AIoT, recommendation systems).\n+ Comprehensive evaluations across datasets, client counts (10, 20, 80), and non-IID levels (controlled by Dirichlet β) demonstrate HyperKAN’s scalability and adaptability."}, "weaknesses": {"value": "- Despite lower FLOPs, HyperKAN has significantly more trainable parameters (382,440) than MLP-based hypernetworks (39,346), potentially increasing server-side memory overhead, which is not fully discussed for resource-constrained servers.\n- The paper mentions spline order k affects computational complexity (O(2ᵏBN²GL)) but lacks detailed experiments on how k influences accuracy or efficiency, leaving gaps in hyperparameter tuning guidance.\n- Evaluations focus solely on image classification datasets; no tests on non-vision tasks (e.g., NLP, tabular data) limit conclusions about HyperKAN’s generalizability."}, "questions": {"value": "1. How does HyperKAN’s server-side memory overhead scale with increasing client numbers (e.g., 100+ clients), and are there optimization strategies to reduce parameter-related costs?\n2. What is the optimal spline order k for different dataset types (e.g., simple FMNIST vs. complex CIFAR-100), and how does k interact with other hyperparameters (e.g., learning rate) to affect performance?\n3. Would the critical block aggregation mechanism require re-tuning of k (number of critical blocks) for non-vision tasks, or can a universal k (e.g., k=2) be applied across task types?\n4. In scenarios with extreme non-IID data (e.g., Dirichlet β=0.01), does HyperKAN still outperform baselines, and what causes performance degradation (if any) under such conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4p559HJ4rF", "forum": "6bt4Ej7tO7", "replyto": "6bt4Ej7tO7", "signatures": ["ICLR.cc/2026/Conference/Submission3414/Reviewer_rnVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3414/Reviewer_rnVx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665651275, "cdate": 1761665651275, "tmdate": 1762916713246, "mdate": 1762916713246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed KAN-based hypernetwork and its application to personalized federated learning. The novelty seems to be limited since there are already hypernet-based pFL methods proposed before, and the only new thing appears to be the replacement of the MLP by KAN. Some empirical study was demonstrated, but not sufficient in various aspects including: lack of architecture variations, few and old baseline comparisons, lack of comparison on diverse FL settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "An interesting idea of adopting KAN to the personalized federated learning problem."}, "weaknesses": {"value": "In their experiments, they showed large improvements in accuracy compared to the previous MLP-based hypernet pFL method. In the main text, they said \"the previous MLP-based methods face the challenge of adequately capturing the intricate non-linearities present in heterogeneous data distributions\", and \"they struggle to capture complex relations of training parameters with heterogeneous data\". But according to the universal function approximation theorem for MLPs means that anything that KAN can do can also be done by MLP. So it's a bit suspicious that KAN-based achieved such a higher performance improvement over the MLP-based one. I guess more extensive experiments with diverse FL and hyperparameters settings would clarify this issue, which is not done in the current paper.\n\nThey proposed hyperkan and its pFL model kpfl. If they want to emphasize the proposal of the hyperkan as one of the main contributions, they need to show some results on general tasks beyond pFL. But they only demonstrated kpfl on FL benchmarks in their empirical study. So it may not be appropriate to highlight the hyperkan itself as a main contribution in the intro section. To do so, they should have provided empirical evidence for hyperkan's performance on general problems beyond FL.\n\nSec.4 (kpfl, the main part) is mostly the same as previous hypernet-based pFL algorithms, esp., that of (Ma et al. 2022). The only difference appears to be changing the mlp hypernet by kan-based hypernet. This is a lack of novelty.\n\nTable 1 only shows one data heterogeneity setting. It may be hard to draw a conclusion with only this one setting. The authors may need to try different levels of heterogeneity, also vary client participation rates, and the number of local updates (client epochs) as usually done in most FL papers. Although Fig.5 did some ablation, this only shows their approach, not for baselines.\n\nBaseline methods are a bit old. Only one hypernet-pFL method is compared. \n\nThe paper seems to see MLP as a default net for FL, but as the experiments are done mostly on vision datasets, it would be nice to test different architectures like ResNet. \n\nOverall, due to the lack of novelty, I think the paper may not be published in the current form.\n\nMinor:\n- Eq.(3) is not necessary, obvious from (2)?\n- The paper presentation and layout are poor. Eg, several typos, texts in some figures are too small to read, the overlapping texts/captions in p.13, etc."}, "questions": {"value": "See questions in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o84tdbfSr6", "forum": "6bt4Ej7tO7", "replyto": "6bt4Ej7tO7", "signatures": ["ICLR.cc/2026/Conference/Submission3414/Reviewer_7bZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3414/Reviewer_7bZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923038775, "cdate": 1761923038775, "tmdate": 1762916712972, "mdate": 1762916712972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HyperKAN, a KAN-based personalized weight generator for federated learning that captures client-specific feature variations, achieving higher accuracy than prior methods on four datasets under non-IID settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method outperforms existing baselines by a notable margin, though the number of compared baselines is limited.-"}, "weaknesses": {"value": "- The overall quality and completeness of the paper are below expectations. The writing could be improved for clarity and readability. \n    - All in-text citations currently use \\citet, which often disrupts sentence flow. Replacing them with \\citep where appropriate would make the text easier to follow.\n    - The paragraph about KAN in the Preliminary section is missing a period after the section name.\n    - Table 1 should appear on page 7, not page 6.\n    - The caption of Figure 13 overlaps with the main text — vertical spacing (\\vspace) adjustment is needed.\n\n- Missing Recent Baselines. The comparison with existing baselines is limited. The paper lacks evaluations against recent and strong methods such as FedDPA [1] and PerAda [2]. \n\n- Computation Comparison. The computational cost comparison is made only against HyperNetwork, which is insufficient. It would be more convincing to include comparisons with other federated learning baselines as well, to show the broader computational efficiency and scalability advantages.\n\n- Scalability Concern. The largest experiment in the paper is on CIFAR, which is relatively small-scale. To demonstrate the scalability of the proposed method, experiments on larger benchmarks (e.g., ImageNet) are needed. As shown in Table 2, HyperKAN has more than 10× the parameters of HyperNetwork, raising concerns about its scalability on large datasets. \n\n[1] Yang et al., Dual-Personalizing Adapter for Federated Foundation Models, NeurIPS 2024.\n\n[2] Xie et al., PerAda: Parameter-Efficient Federated Learning Personalization with Generalization Guarantees, CVPR 2024."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3d2da9KrOn", "forum": "6bt4Ej7tO7", "replyto": "6bt4Ej7tO7", "signatures": ["ICLR.cc/2026/Conference/Submission3414/Reviewer_cTcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3414/Reviewer_cTcN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969410809, "cdate": 1761969410809, "tmdate": 1762916712550, "mdate": 1762916712550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}