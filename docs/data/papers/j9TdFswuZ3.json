{"id": "j9TdFswuZ3", "number": 11109, "cdate": 1758189456331, "mdate": 1759897608075, "content": {"title": "MMClima: A Framework for Multimodal Climate Science Data and Evaluation", "abstract": "Climate change research increasingly requires AI systems that can operate across multiple modalities, including natural language, dynamic visual content, and scientific figures. Yet existing climate QA benchmarks remain limited: they include relatively small sets of questions, rely almost exclusively on text, and evaluate only a narrow range of models. As a result, they fail to reflect the multimodal and large-scale nature of climate knowledge. In this work, we introduce MMClima, a multimodal framework for climate question answering. MMClima contains over 104k expert-validated question–answer pairs spanning text, video transcriptions, and figures, alongside covering a diverse range of five core climate science domains. The dataset is constructed through automated claim extraction combined with human-in-the-loop validation to ensure both scale and reliability. Beyond serving as a dataset, MMClima provides a reusable framework for extending QA resources across modalities. Using MMClima, we evaluate state-of-the-art multimodal language models on tasks spanning factual recall, visual interpretation, and cross-modal synthesis. We further fine-tune on the textual split, yielding mmclima-70b-txt, a domain-adapted baseline that surpasses both open- and closed-source models. Finally, we release the dataset, evaluation pipeline, fine-tuned model weights, and data creation framework as open resources, establishing the first step toward standardized multimodal evaluation in climate science.", "tldr": "", "keywords": ["Multimodal Climate Benchmark", "Scientific Foundation Models", "Scientific Question Answering", "Large Language Models", "Automated QA generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e415867d52bfa3502537d06c9beafcc2652409a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discusses the field of climate change and proposes a new multimodal dataset about the topic of climate change. The collected dataset contains over 104k expert-validated question–answer pairs spanning text, video transcriptions, and figures, alongside covering a diverse range of five core climate science domains. The authors claim that this will boost the research in related fields. The authors fine-tune on the textual split, yielding mmclima-70b-txt, a domain-adapted baseline that surpasses both open- and closed-source models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have collected a very large dataset, containing 104k expert-validated QA pairs across multiple sub-domains in climate studies and the questions take various forms.\n2. The proposed dataset contains multiple domains, including text, video transcriptions, and figures.\n3. The authors propose a pipeline that generates textual QA for new topics."}, "weaknesses": {"value": "1. The authors claim that the dataset is about climate change and that the dataset contains multiple data. As climate studies are highly related to the analysis of numbers, I wonder whether time series data or spatial-temporal data are included in the dataset as a separate modality.\n2. There are a lot of recent works on multimodal evaluation of multimodal large language models[1-3]. The authors should include more of them in the paper.\n3. As this paper is focused on the specific field of climate studies, the authors should discuss more about why previous evaluation efforts are not enough for this specific field.\n4. The authors claim that they have collected a dataset, but there is no link to the data.\n\n\n[1] MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation\n\n[2] Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness\n\n[3] Evaluating mllms with multimodal multi-image reasoning benchmark"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b6BtkuWJMn", "forum": "j9TdFswuZ3", "replyto": "j9TdFswuZ3", "signatures": ["ICLR.cc/2026/Conference/Submission11109/Reviewer_HDct"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11109/Reviewer_HDct"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392337004, "cdate": 1761392337004, "tmdate": 1762922283879, "mdate": 1762922283879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MMClima is a large-scale multimodal dataset and benchmarking framework for climate science. For creating dataset, it aggregates Wikipedia and YouTube transcripts with IPCC/OWID figures to produce 104.9K expert-validated QA items across five domains, spanning textual tasks (MCQ, cloze, free-form) and image-grounded VQA (MCQ, yes/no, open-ended). A scalable pipeline, (retrieve -> claim extraction -> QA synthesis -> human validation), supports high fidelity and extensibility. Under unified zero-shot prompting, 28 LLMs and 8 VLMs are evaluated: cloze is the most discriminative task, free-form is easier. Besides, a domain-specific QA model is fine-tuned on the training set, called MMCLIMA-70B-TXT, outperforms strong open- and closed-source models on text QA. MMCLIMA fills gaps in scale, modality coverage, and auditability, providing infrastructure for standardized, extensible evaluation in climate AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The multimodal dataset is large and broadly scoped, covering five subdomains. Its noise level appears low thanks to expert annotation and verification.\n\n(2) The evaluation is thorough, spanning multiple open- and closed-source models, with a reasonably comprehensive comparison for the fine-tuned model MMCLIMA-70B-TXT.\n\n(3) The work also introduces the first multimodal comprehensive knowledge-oriented QA benchmark for the climate domain."}, "weaknesses": {"value": "(1) It would be even more useful to fine-tune a multimodal QA model directly on the visual QA portion of the dataset."}, "questions": {"value": "(1) Compared to fine-tuning on a database, a retrieval-augmented generation (RAG) approach might yield higher scores?\n\n(2) For the text QA benchmark, might try higher-tier closed models? since they often have wider “knowledge” and stronger reasoning ability (e.g., gpt-5, grok-4, claude-4-sonnet, gemini-2.5-pro). Because the test set is large and full runs are costly, may consider sampling a proportion of examples from the models you’ve already evaluated and adjust that proportion until the estimated precision closely matches the full-set precision (e.g., around 20% as a starting point). Use this as an “effective subset” for evaluation. This calibration should be done across several already-run models to ensure that, at this sampling rate, each model’s precision approximates its full-set precision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PJjyh6F8vb", "forum": "j9TdFswuZ3", "replyto": "j9TdFswuZ3", "signatures": ["ICLR.cc/2026/Conference/Submission11109/Reviewer_yrtc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11109/Reviewer_yrtc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802140119, "cdate": 1761802140119, "tmdate": 1762922283044, "mdate": 1762922283044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a multimodal dataset of over 104k expert-validated climate science QA pairs, spanning five domains: (1) Atmospheric Composition & Air Quality, (2) Oceanic and Coastal Dynamics, (3) Cryosphere and Glacial Systems, (4) Climate-Driven Extreme Events, and (5) Climate Policy, Governance, and Mitigation Pathways. They additionally present the full data-generation pipeline and argue that it can be extended to other domains with minimal modification. The paper further benchmarks 28 LLMs and VLMs on this dataset. Finally, the authors fine-tune Llama-3.3-70B on the training portion of the corpus, demonstrating that the resulting model surpasses the other evaluated systems on the textual QA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dataset has over 100k QA pairs, which supports robust training and evaluation. Moreover, the inclusion of human-in-the-loop expert validation improves factual quality and clarity.\n\n\n2. The dataset covers text, video transcripts, and figure-based VQA, across five different climate domains.\n\n\n3. The authors run a broad evaluation over various LLMs and VLMs, giving a solid view of current performance.\n\n\n4. The data-generation pipeline is modular and reusable, making it possible to extend to new domains and sources."}, "weaknesses": {"value": "1. The dataset is explicitly single-evidence grounded, which constrains items to single-hop questions; this improves attribution but underrepresents real-world multi-hop reasoning that requires reasoning over multiple sources or pieces of information.\n\n\n2. The textual split relies on Wikipedia articles and YouTube transcripts, which are broad but potentially noisy compared to higher-reliability corpora (e.g., peer-reviewed journals, textbooks). A higher-precision textual subset or stricter filtering would strengthen reliability.\n\n\n3. Open-ended VQA uses LLM-as-a-judge with Llama-3.3-70B-Instruct-Turbo, while the paper’s textual baseline fine-tunes Llama-3.3-70B (MMCLIMA-70B-TXT). Using the same model family for evaluation might introduce bias.\n\n\n4. All systems are evaluated with temperature = 0 and default decoding settings. While comparable, the results may not reflect each model’s best achievable performance. Some models are sensitive to decoding hyperparameters, so small tunings would improve robustness.\n\n\n5. The appendix examples mostly look like simple masked-number or surface-form / general questions and don’t convincingly reflect practical, decision-support QA a climate scientist would use."}, "questions": {"value": "1. Why is LLM-as-a-judge used only for open-ended VQA and not for textual free-form answers? BERTScore might not be a reliable measure of factual correctness or faithfulness.\n\n\n2. The authors note that each visual is paired with its caption, legend, and surrounding text to preserve grounding. Would the authors consider adding an ablation to assess the incremental contribution of the image itself? For example, caption/legend-only (no image), image-only, and image + caption settings? If caption-only performance approaches image+caption, that would suggest the visual modality contributes limited additional value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fIM1hZST6E", "forum": "j9TdFswuZ3", "replyto": "j9TdFswuZ3", "signatures": ["ICLR.cc/2026/Conference/Submission11109/Reviewer_UMg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11109/Reviewer_UMg5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874228369, "cdate": 1761874228369, "tmdate": 1762922282200, "mdate": 1762922282200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMCLIMA, a large-scale multimodal framework for climate QA. Addressing the limitations of existing text-only and small-scale benchmarks, MMCLIMA includes over 104k expert-validated QA pairs spanning text, video transcripts, and scientific figures across five core climate domains. Beyond the dataset, MMCLIMA serves as a reusable framework for multimodal QA evaluation. The authors also present MMCLIMA-70B-TXT, a domain-adapted model that surpasses both open- and closed-source baselines, demonstrating the value of specialized multimodal resources for climate science."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work establishes a systematic and extensible framework for multimodal climate QA, integrating rigorous pipeline design with comprehensive benchmarking. Its multi-stage approach—combining automated claim extraction, factual verification, and expert validation—ensures unprecedented scale and reliability, surpassing previous text-centric benchmarks. The evaluation of 28 LLMs and 8 VLMs reveals critical inter-modal performance gaps, while domain-adapted fine-tuning demonstrates significant gains, highlighting the value of specialized data. This foundational resource advances multimodal scientific reasoning through its scalable methodology and actionable insights."}, "weaknesses": {"value": "Although the benchmark includes visual question answering (VQA), the dataset is overwhelmingly dominated by textual QA pairs (100k), while video transcripts (8k) and visual QA pairs (around 4k by estimation) constitute only a small fraction. This imbalance may cause the benchmark to emphasize textual understanding over genuine multimodal reasoning, offering limited stress testing of models’ visual interpretation capabilities."}, "questions": {"value": "1.The paper lacks an ablation study isolating the contribution of each modality. Specifically, it would be valuable to evaluate models using only text, only transcripts, and only images, then compare these results with the full multimodal setup. Such an analysis would clarify whether multimodal fusion truly introduces new knowledge and reasoning capabilities beyond text alone, or if the benchmark essentially remains text-dominated with limited added value from visual and video modalities.\n\n2.Although the paper highlights the strong performance of the fine-tuned MMCLIMA-70B-TXT, it lacks ablation studies to explain the source of its improvement. Experiments varying the amount of training data and applying the same fine-tuning process to different base models would clarify whether the gains arise from data quality or sheer quantity, and whether the dataset is truly generalizable or mainly effective for the chosen LLaMA-3.3-70B architecture."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mig0RVQuj6", "forum": "j9TdFswuZ3", "replyto": "j9TdFswuZ3", "signatures": ["ICLR.cc/2026/Conference/Submission11109/Reviewer_epXe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11109/Reviewer_epXe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11109/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989435904, "cdate": 1761989435904, "tmdate": 1762922281172, "mdate": 1762922281172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}