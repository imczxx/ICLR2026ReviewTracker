{"id": "ePRnK88z5p", "number": 5034, "cdate": 1757837208945, "mdate": 1763402907106, "content": {"title": "Post-training quantization of vision encoders needs prefixing registers", "abstract": "Transformer-based vision encoders---such as CLIP---are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose \\textit{RegCache}, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.", "tldr": "We show that outliers in vision encoders share similar components across images. By caching them as prefix tokens, attention sinks are mitigated and outlier tokens can be removed, resulting in improved quantization performance.", "keywords": ["quantization", "vision encoder", "vision transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/88b4a9922413ca37e72c2e1ca84dae453e1f298d.pdf", "supplementary_material": "/attachment/2c0234ce9d3a7cc3c893e95cc92acabef3962c94.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RegCache, a method to improve post-training quantization of vision encoders by using prefixing registers to handle activation outliers. It claims to be training-free and shows improved results across various models and quantization techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting observation about middle-layer outlier emergence in vision encoders\n\n- Clear empirical demonstration of performance improvements\n\n- Comprehensive evaluation across multiple vision encoders (CLIP, DINOv2, SigLIP, etc.) and quantization methods"}, "weaknesses": {"value": "- The method tunes the number of tokens to delete (k̃) based on downstream performance.\nThis process constitutes a form of task-specific, validation-based tuning that is computationally expensive and requires labeled data from the target domain. This violates the standard premise of PTQ, which is designed to be a lightweight process that does not require access to the final task's labels or metric. \n\n- The approach inserts external tokens as pre-computed KV at middle layers and removes tokens deemed sinks. This goes beyond calibration as it alters the attention context and risks semantic/architectural drift."}, "questions": {"value": "- What is the computational cost of the entire RegCache pipeline, including the candidate curation and hyperparameter search? How does this cost compare to simply performing a brief Quantization-Aware Training (QAT) round, which would likely be more effective?\n\n- The paper compares against other PTQ methods. Were these baselines also allowed to perform a similar level of task-specific hyperparameter tuning on the ImageNet validation set? If not, the comparison is not fair."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H06FFaDKlQ", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Reviewer_TDJs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5034/Reviewer_TDJs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760619176491, "cdate": 1760619176491, "tmdate": 1762917832714, "mdate": 1762917832714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QpsYTgniAF", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763402906000, "cdate": 1763402906000, "tmdate": 1763402906000, "mdate": 1763402906000, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RegCache is a training-free method to reduce inference cost in transformer-based vision encoders (e.g., CLIP) by mitigating activation outliers that hinder 8-bit post-training quantization. It injects semantically meaningless, outlier-prone prefix tokens to prevent other tokens from producing outliers and, based on the distinct behavior of vision encoders vs. language models, introduces two key techniques: middle-layer prefixing and token deletion. This enables quantization with significantly smaller accuracy drops and consistently improves performance across both text-supervised and self-supervised encoders."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well organized and easy to follow.\n2. The method is plug-and-play.\n3. The authors provide code for reproducing."}, "weaknesses": {"value": "2. There are lots of works that propose to address the issue of outliers for LLM (which can also apply to ViT) and ViT. However, this paper does not compare the proposed method with them. For example, QuaRot (NeurIPS 2024), https://openreview.net/pdf?id=Uh5XN9d2J4 (ICML 2024), etc.\n3. This paper does not implement their method on top of the SOTA quantization methods to show the effectiveness of their approaches. For example, FIMA-Q (CVPR 2025) and APHQ-ViT (CVPR 2025).\n4. The introduced overhead of this method for real-time inference is not being discussed.\n5. The idea that introducing additional tokens to mitigate outliers and sinks is not novel (see https://arxiv.org/abs/2402.17762 and https://arxiv.org/abs/2410.05265)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "foP6vK14MC", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Reviewer_u2Yn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5034/Reviewer_u2Yn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748772778, "cdate": 1761748772778, "tmdate": 1762917832432, "mdate": 1762917832432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the RegCache algorithm to address the outlier problem in post-training quantization (PTQ) of vision encoders. Specifically, RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. This design ultimately reduces the quantization error of PTQ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The analysis of outliers in PTQ quantization is interesting, and its approach to reducing model outliers is innovative, which merits further research."}, "weaknesses": {"value": "The paper proposes several key hypotheses and observations that underpin its RegCache method, yet these claims undermine the work’s generality and reliability. They are not sufficiently supported by either extensive experimental validation (e.g., across more diverse image domains or model architectures) or in-depth theoretical analysis—leaving their applicability to vision encoder post-training quantization (PTQ) insufficiently verified."}, "questions": {"value": "1. The paper lacks theoretical analysis for key observations—specifically, it fails to provide a theoretical explanation for the intrinsic reason behind the \"cross-image similarity of middle-layer outliers\". For instance, an analysis from perspectives such as the attention mechanism or LayerNorm is absent.  \n2. The paper has issues with its figures. The left subfigure of Figure 1 does not provide sufficiently detailed information. The right subfigure of Figure 1 lacks adequate descriptions of experimental setup details. Figure 2 is deficient in introductions to key terms.  \n3. Although the authors cite numerous references, the paper still needs to explain in the text why sink tokens can mitigate outliers.  \n4. Provide a detailed comparison between RegCache and recent outlier quantization methods for vision encoders.  \n5. Supplement quantitative experiments to analyze the proportion of quantization error contributed by outliers, so as to further support the urgency of the research motivation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zz2HEG5rc9", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Reviewer_JjR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5034/Reviewer_JjR7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931781195, "cdate": 1761931781195, "tmdate": 1762917832184, "mdate": 1762917832184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a novel training-free algorithm to mitigate outliers in post-training quantization of vision encoders through pruning semantically meaningless prefix tokens. Experiments reveal the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The discovery of semantically meaningless prefix tokens is enlightening. \n2. The motivations and corresponding methods are clearly revealed and discussed."}, "weaknesses": {"value": "Major: \n1. In Line 96-98, the prefixed tokens are only applied for middle-to-final layers. Is there any experimental comparison and corresponding discussion about how the layer settings effect the quantization process and model performance?\n2. In section 3.1, why only layer-wise sensitivities are discussed, since the sensitivities and outliers are in transformers are about channel level or even token level as discussed in previous arts. \n3. How to deal with vision encoders like NaViT, which process images as any arbitrary resolution. Thus in these vision encoder pipeline, the prefix can be varied and the quantization strategy proposed in this paper may not perform well. \n\n\nMinor: \n1. It would be better to add legends in Figure 2 for more clarity. \n2. Maybe section 3.1 and 3.3 can be combined as one section about how layer index affect the outlier and quantization performance, for better writing logic."}, "questions": {"value": "See weaknesses. My major concern is the third point in the \"Major\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MG3SSPIaeb", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Reviewer_d41b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5034/Reviewer_d41b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762331694584, "cdate": 1762331694584, "tmdate": 1762917831891, "mdate": 1762917831891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "Dear reviewers and the AC,\n\nThank you for taking your time to review our manuscript.\n\nAlthough we have decided to ***withdraw*** our manuscript, we would like to take this chance to briefly address some of your concerns. This is not only to clarify some misunderstandings, but also our sincere acknowledgment of your efforts to improve our manuscript—which we deeply appreciate.\n\nWe have responded to your comments one-by-one.\n\nBest regards,  \nAuthors."}}, "id": "hOZFhDE1g9", "forum": "ePRnK88z5p", "replyto": "ePRnK88z5p", "signatures": ["ICLR.cc/2026/Conference/Submission5034/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5034/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission5034/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763402698468, "cdate": 1763402698468, "tmdate": 1763402698468, "mdate": 1763402698468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}