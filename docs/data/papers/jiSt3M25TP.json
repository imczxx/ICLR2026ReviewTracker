{"id": "jiSt3M25TP", "number": 18965, "cdate": 1758292406627, "mdate": 1759897070359, "content": {"title": "MoMa: A Simple Modular Learning Framework for Material Property Prediction", "abstract": "Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a simple Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMa's potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration.", "tldr": "", "keywords": ["material property prediction", "modular deep learning", "AI4Materials"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/353907c2f669e7d717e0f4ed5eddec1f4e05c68e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an approach for material property prediction that trains specialised modules for different tasks and then adaptively combines them for new prediction scenarios. An algorithm is proposed to efficiently estimate the performance of each module on a target task and then optimise a weighted combination of these modules to create a tailored model for the specific task.  Experiments across 17 datasets show the effectiveness of the approach, especially in few-shot settings, enabling continual learning and scalable integration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluation spans 17 diverse downstream tasks. The authors compare  against strong and relevant baselines, including not just older methods (CGCNN) but state-of-the-art pre-trained models fine-tuned in different ways (JMP-FT, JMP-MT).\n2. The authors have diligently validated their design choices. The ablation study presented in Figure 4 is particularly effective, clearly demonstrating that both the selection of modules (vs. averaging all) and the learned weighting (vs. uniform averaging) are critical to AMC's success."}, "weaknesses": {"value": "1. The method of weighted-averaging fine-tuned model parameters seems to be nearly identical to the 'Model Soups' approach [1]\n2. The algorithm's effectiveness hinges on the assumption that the kNN-based proxy error (Eq. 2) is a reliable indicator of final, post-fine-tuning performance. This is a strong heuristic, but its validity is not fully demonstrated. The authors should conduct an experiment correlating the proxy error of individual modules with their actual performance after being fine-tuned alone on the downstream task.\n3. Section 3.5 presents a 'Continual Learning' experiment by adding 12 QM9 molecular modules to the Hub. While this demonstrates the framework's ability to incorporate new knowledge, it doesn't align with the standard definition of continual learning, which typically involves sequential training and mitigating 'catastrophic forgetting.'\n\n\n[1] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, In ICML'22."}, "questions": {"value": "1. Would inclusion of a “Naive Soup” baseline, averaging all eighteen fine-tuned modules with uniform weights, help isolate AMC’s benefits by demonstrating whether ensembling everything is inferior to selection and weighting?\n2. Could validation be provided showing correlation between the kNN-based proxy error and post-fine-tuning performance, e.g., scatter plots comparing modules’ proxy errors versus their results after standalone fine-tuning across representative tasks?\n3. Could sensitivity analyses for K in kNN, distance metric choices, normalization schemes, and optimizer tolerance be included, reporting how MAE and selected weights vary across these settings for robustness measures?\n4. Could a second backbone be added to main results, even on a subset, to validate backbone-agnostic gains, and discuss layer alignment issues impacting weight-space merging beyond primarily JMP-based experiments reported?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LWuHYYSt77", "forum": "jiSt3M25TP", "replyto": "jiSt3M25TP", "signatures": ["ICLR.cc/2026/Conference/Submission18965/Reviewer_1mck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18965/Reviewer_1mck"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110605253, "cdate": 1761110605253, "tmdate": 1762931015935, "mdate": 1762931015935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoMa, a simple modular learning framework to overcome the diversity and disparity challenges in material property prediction. It first trains specialized modules on a wide range of tasks and stores them in a MoMa Hub. Then, Adaptive Module Composition (AMC) composes the most synergistic modules from this Hub into a single, tailored model for new tasks, achieving substantial performance gains over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper effectively tackles the challenge of material learning by applying a modular deep learning framework.\n\n2. The paper is clearly written, presenting an intuitive and well-motivated method.\n\n3. MoMa achieves superior performance, showing substantial average improvements over existing baselines."}, "weaknesses": {"value": "1. AMC's design relies heavily on heuristics, such as using a kNN performance proxy, and the paper would be stronger if it provided sufficient justification for this design choice.\n\n2. As the authors note for future work, it is questionable whether the kNN-based proxy will remain sufficiently efficient and effective as MoMa scales to hundreds or thousands of modules."}, "questions": {"value": "Please refer to the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7OHKJ6vRxN", "forum": "jiSt3M25TP", "replyto": "jiSt3M25TP", "signatures": ["ICLR.cc/2026/Conference/Submission18965/Reviewer_3v4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18965/Reviewer_3v4v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970881533, "cdate": 1761970881533, "tmdate": 1762931015502, "mdate": 1762931015502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a straightforward machine learning framework for materials science. The framework is composed of multiple independently trained modules, each tailored for a specific task. For a given downstream application, the framework dynamically selects and integrates a combination of modules based on their estimated predictive performance, followed by joint fine-tuning. Extensive experiments across 17 material property prediction benchmarks demonstrate the framework’s superior accuracy and adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The MoMa framework have individual modules trained on a wide range of properties and tasks. This architecture aligns with the diversity and disparity of material properties. It also enables the framework to extend to other modules trained on specialized material properties easily. By having a full module and an adapter module, the framework is parameter-efficient. It also allows easy fine-tuning on downstream tasks. The author also shows that the module selected by the algorithm correlates with the physical relationships between material properties, showing the effectiveness and trustworthiness of the algorithm."}, "weaknesses": {"value": "The MoMa is a flexible framework for material property prediction. However, there might be other material properties that could not be captured by the trained modules, resulting in inaccurate property prediction."}, "questions": {"value": "1. Does using the open source cvxpy package guarantee to find the optimal weight? \n2. How much does the accuracy of individual modules affect the weight calculation and model performance? \n3. Is there any correlation between individual modules? Does it affect molecule weight calculation or model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JhK1ZZMLlA", "forum": "jiSt3M25TP", "replyto": "jiSt3M25TP", "signatures": ["ICLR.cc/2026/Conference/Submission18965/Reviewer_nxqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18965/Reviewer_nxqB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982617677, "cdate": 1761982617677, "tmdate": 1762931014923, "mdate": 1762931014923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoMa, a modular learning framework for material property prediction designed to tackle the diversity (heterogeneity of tasks and systems) and disparity (incompatible physical laws) in materials datasets.\nIt first trains a collection of task-specific modules (either full fine-tuned models or lightweight adapters) on high-resource material datasets, and later composes these modules adaptively for new downstream tasks via the proposed AMC algorithm, which estimates module relevance using kNN-based proxy prediction, optimizes non-negative weights through convex programming, merges module parameters via weighted averaging, and fine-tunes the composed module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introducing a hub-and-composition paradigm is new to this domain and nicely parallels modular LLM work (AdapterHub, LoRAHub).\n- AMC is simple, requiring only kNN embedding estimation and a small convex optimization step.\n- Evaluation is pretty comprehensive.\n- The visualization of module weights (Fig. 6) provides interesting physical correlations and hints at cross-property transferability.\n- Open-sourcing MoMa Hub could foster standardized module reuse and privacy-aware data sharing in materials science."}, "weaknesses": {"value": "- While AMC is well-engineered, its core ideas (kNN prediction + convex weighting + parameter averaging) largely repackage known components from model-merging and ensemble literature. The conceptual advance over prior modular or LoRAHub methods is modest.\n- The paper lacks formal justification or analysis of why the convex proxy optimization correlates with post-fine-tuning accuracy, beyond empirical correlation (r = 0.69). A clearer theoretical connection between proxy error and final generalization would strengthen the contribution.\n- Baselines such as JMP-MT and MoE-(18) are implemented in-house; comparisons to other modern material foundation models (e.g., MatterSim [1], Orb-v3 [2], UMA [3]) are missing, limiting claims of SOTA performance.\n- Also, ablations explore only AMC variants, not architectural or hub-scale sensitivities.\n- Although the framework is claimed backbone-agnostic, all results use JMP. It remains uncertain whether MoMa generalizes to other encoders or representations (e.g., E3NN [4], GemNet-OC [5]).\n- While clear overall, the paper is somewhat long and repetitive; core algorithmic details could be condensed, and figures (e.g., Fig. 3) are descriptive but not deeply analytical. The continual-learning section feels exploratory and under-developed.\n- The work focuses on crystalline and small-molecule systems; polymeric, amorphous, and multi-phase materials are not addressed, limiting generality.\n\n[1] Yang, H., et al. \"A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures.\" arXiv preprint arXiv:2405.04967 (2024).\n\n[2] Rhodes, Benjamin, et al. \"Orb-v3: atomistic simulation at scale.\" arXiv preprint arXiv:2504.06231 (2025).\n\n[3] Wood, Brandon M., et al. \"UMA: A Family of Universal Models for Atoms.\" arXiv preprint arXiv:2506.23971 (2025).\n\n[4] Geiger, Mario, and Tess Smidt. \"e3nn: Euclidean neural networks.\" arXiv preprint arXiv:2207.09453 (2022).\n\n[5] Gasteiger, Johannes, et al. \"GemNet-OC: developing graph neural networks for large and diverse molecular simulation datasets.\" arXiv preprint arXiv:2204.02782 (2022)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7v2R8k5vV", "forum": "jiSt3M25TP", "replyto": "jiSt3M25TP", "signatures": ["ICLR.cc/2026/Conference/Submission18965/Reviewer_tuSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18965/Reviewer_tuSk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115049072, "cdate": 1762115049072, "tmdate": 1762931014467, "mdate": 1762931014467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}