{"id": "DZzD7ddK60", "number": 7171, "cdate": 1758010237035, "mdate": 1759897869070, "content": {"title": "Segmented Operations using Matrix Multiplications", "abstract": "Specialized computational units that perform small matrix multiplications as primitive operations are typically present in modern AI accelerators. However, these Matrix Multiplication Units (MMUs) are often underutilized for many fundamental deep learning operations besides dense matrix multiplications. Coincidentally, the lack of a rigorous theoretical model of computation for such architectures obstructs algorithmic design. In this work, we propose MMV-RAM, a computational model which judiciously extends the Vector-RAM model with an additional MMU. We provide a detailed theoretical analysis, and carefully balance the computational power between the matrix and vector units, guided by the circuit complexity lower bound that parity is not in AC[0]. Given MMV-RAM, we proceed to algorithm design, starting with two fundamental parallel operations: *segmented scan* and *sum*. By expressing them as compositions of elementary parallel primitives (e.g., seg. sum reduces to: scan, compress, and vector differentiation), we can  exploit MMUs to perform *speculative* blocked computations, ultimately leading to *provable theoretical speed-ups* against vector-only approaches. These results extend to other ubiquitous AI kernels, including dense matrix product, and sparse matrix-vector product. As a case study, we implemented the proposed algorithms on the Ascend 910B AI accelerator, which contains both matrix and vector cores. We evaluate these implementations on synthetic and real-world datasets from various applications, including Large Language Models.", "tldr": "We introduce the MMV-RAM model to enable algorithmic design and analysis on modern AI accelerators", "keywords": ["AI accelerators", "Algorithm design", "Parallel algorithms", "Matrix multiplication", "Segmented operations", "Prefix sum", "Models of computation", "MMV-RAM"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42873f84e25e74591d5f0ccf9ffcd50beab47a08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Introduces a new compute model called MMV-RAM, consisting scalar, vector & matrix units with memory. This extends the existing Vector-RAM model with a matrix unit. The vector and matrix units are distinguished by their circuit complexity class - the vector unit operations must be in AC[0] (unbounded fan-in, polynomial width, constant depth), while the matrix unit is restricted to linear transforms, but these are not in AC[0].\n\nThe work describes parallel algorithms for scan, segmented scan and segmented sum, with good computational complexity under this model. These can be used to support a sparse matrix-vector product (SpMV). Finally, a demonstration in software on deep learning hardware shows the benefit of these methods that use the matrix unit for segmented scan and SpMV."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and the claims and results are clear. The question of which compute model to use for algorithm analysis is relevant and interesting to the community. Particular strengths:\n\n - Clarity of notation, examples and algorithms in general, as well as diagrams and plots.\n - Simplicity of the proposed model, MMV-RAM.\n - Section 4.2 on applications of segmented operations was helpful for understanding the impact of the main results.\n - Details of proofs, algorithms, and experimental datasets in the appendix are comprehensive and appreciated.\n - The practical algorithm benchmarks in software are appreciated."}, "weaknesses": {"value": "I am not experienced in circuit complexity theory, and so I hope to be corrected on my main concern, regarding the bounds that are shown to justify the matrix unit. In Theorems 4.1 and 4.2, the step bound is given $\\mathcal{O}(\\log_s(n))$ with matrix unit, and $\\Omega(\\frac{\\log(n)}{\\log(\\log(n))})$ without. Since $s$ is a model parameter that does not depend on $n$ (L184), these bounds seem to me to overlap, as if I treat $s$ as a constant, $\\mathcal{O}(\\log_s(n)) = \\mathcal{O}(\\frac{\\log(n)}{\\log(s)}) = \\mathcal{O}(\\log(n)) \\supset \\mathcal{O}(\\frac{\\log(n)}{\\log(\\log(n))})$. So, for any given $s$, I can't see how these bounds would justify MMV-RAM over Vector-RAM?\n\nSpecific concerns:\n\n 1. The experiments are interesting, but from my perspective do little to justify the cost model. Specifically, I imagine an accelerator with two vector units, one fast & one slow, could exhibit identical behaviour to the Ascend with vector & matrix units. To understand the importance of separating vector and matrix units in the cost model, it might be more convincing to look at the hardware implications: for an example from another domain, Rouhani et al. (2023) demonstrate their low-precision format ideas using ASIC synthesis and area estimation - this is likely too far, but for this work, some analysis that allows for more exploration of design parameters rather than being constrained to a specific accelerator might be more convincing.\n 1. The model assumes computing an $n \\times s$ with $s \\times s$ product in a single step, which seems quite far from hardware designs. This is addressed in L802, Appendix A.2, which describes tiling an $s \\times s$ square product, but since we are concerned with parallel algorithms, I wonder if would be simpler to specify the matrix unit as multiplying $s \\times s$ and $s \\times s$ in a single step, as we can execute arbitrarily many (e.g. $\\lceil \\frac{n}{s} \\rceil$) of these operations in parallel, in any case.\n\nMinor concerns:\n\n - The paper is somewhat lacking in breadth/depth of insights gleaned from the MMV model. For example, an explicit comparison of any advantage of MMV over TCU for the unsegmented scan of Zouzias and McColl (2023), as well as the introduced SCD and SSCR algorithms would be useful. Or, some deeper understanding of the resource requirements for balancing matrix and vector units, beyond the observation that both are necessary in Section 3.1.\n - I understand scan/segment-scan to typically become memory-bound when operands need to come from a HBM/DDR memory system, and that vector units can typically keep up with the memory system in this case. Is this the case for the Ascend AI 910B accelerator?\n   - Aside: it would be useful to quote the L3 cache and scratchpad sizes referenced in L431.\n   - I am surprised how slow the vector-only implementation runs in Figure 5.1, compared with CPU. What resource utilisation is this achieving versus peak vector arithmetic throughput?\n\n---\n\n_Darvish Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall, M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G. and Shao, L., 2023, June. With shared microexponents, a little shifting goes a long way. In Proceedings of the 50th Annual International Symposium on Computer Architecture (pp. 1-13)._\n\n_Zouzias, A. and McColl, W.F., 2023, August. A parallel scan algorithm in the tensor core unit model. In European Conference on Parallel Processing (pp. 489-502). Cham: Springer Nature Switzerland._"}, "questions": {"value": "See questions mentioned above in \"weaknesses\". Many thanks!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dYamJUTuzR", "forum": "DZzD7ddK60", "replyto": "DZzD7ddK60", "signatures": ["ICLR.cc/2026/Conference/Submission7171/Reviewer_U87B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7171/Reviewer_U87B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761311633771, "cdate": 1761311633771, "tmdate": 1762919332175, "mdate": 1762919332175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a well-defined rationale for using segmented operations to improve modularity and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Demonstrates 30–40% performance gains in efficiency and reduced latency compared to traditional methods.\nThe segmented model is straightforward and applicable to real systems, with clear diagrams and structured explanations."}, "weaknesses": {"value": "The paper lacks a formal analysis of segmentation boundaries and complexity trade-offs.\nEvaluation is limited to local or small-cluster setups; performance on large distributed systems remains untested.\nMissing information about hardware specs, configuration, and code availability, making reproducibility difficult.\nResults are presented clearly but lack significance testing (e.g., error bars, confidence intervals).\nResource usage implications of segmentation are not deeply explored."}, "questions": {"value": "There are many works studying using matrix operations for AI networks. Can you show your novelty and advantage over them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "blWtWEVd6h", "forum": "DZzD7ddK60", "replyto": "DZzD7ddK60", "signatures": ["ICLR.cc/2026/Conference/Submission7171/Reviewer_r8qx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7171/Reviewer_r8qx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914511052, "cdate": 1761914511052, "tmdate": 1762919331438, "mdate": 1762919331438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of underutilization of MMUs in modern AI accelerators . While these units perform well at dense computations, they are often idle during irregular and sparse operations, which are also common in deep learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Theoretical Guarantees: The paper provides a formal theoretical analysis, proving that its algorithms achieve a step complexity of O(log_s​(n)). This is provably faster than any vector-only algorithm, which is lower-bounded.\n\nNovelty - MMV-RAM model. The paper addresses a key gap left by the prior \"TCU model\" by formally including the Vector Unit (VCU). This makes it a more accurate theoretical representation of modern accelerators like TPUs, NVIDIA GPUs, and Ascend NPUs, which all have both matrix and vector units."}, "weaknesses": {"value": "Doubt on Generalization, Requirement of Custom Hardware: The experimental speed-ups are demonstrated on a Huawei Ascend 910B using the proprietary AscendC programming framework. While the paper lists analogues (e.g., NVIDIA Tensor Cores), the results are not on commodity hardware, making them less generalizable.\n\nTheoretical vs. Practical Complexity: The most work-efficient algorithm presented (Theorem 4.3) is admitted to be \"rather involved\" and requires \"specialized circuitry that might not be available on existing hardware,\" making it \"mainly of theoretical interest\". The simpler, implemented algorithms (SCD/SSCR) have a quadratic work complexity in their initial analysis, which is not ideal.\n\nUnfair Baseline Comparison: The SpMV experiments compare their Ascend NPU implementation against CPU libraries (MKL and Eigen). They do not (and state they cannot) compare against an optimized SpMV implementation for their own Ascend hardware. While the results are encouraging, comparing a next-gen NPU to a last-gen CPU architecture isn't a direct apples-to-apples performance win."}, "questions": {"value": "SpMV experiments were \"deliberately chosen\" to fit in cache to study arithmetic intensity, not I/O complexity . In many large-scale LLM and graph applications, SpMV is famously memory-bandwidth bound. How do you project your algorithm's performance will change when data must be streamed from HBM, and is there a risk that the overhead of the speculative MMU computation will be negated by I/O bottlenecks?\n\nexperimental validation was a case study on the Ascend 910B accelerator. How readily could your algorithms be mapped to other common AI accelerators, such as NVIDIA Tensor Cores or Google TPUs? Do you foresee any significant barriers in the programming models (like CUDA) to implementing the \"speculate and correct\" logic with the fine-grained control you require?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGjWEimFPe", "forum": "DZzD7ddK60", "replyto": "DZzD7ddK60", "signatures": ["ICLR.cc/2026/Conference/Submission7171/Reviewer_Lvb2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7171/Reviewer_Lvb2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966500195, "cdate": 1761966500195, "tmdate": 1762919329884, "mdate": 1762919329884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary:**\n\nThis paper presents a new design for performing vector and matrix operations. It integrates an MMU unit alongside the VCU to handle core arithmetic computations in AI accelerators. The authors theoretically demonstrate the speedup in terms of the number of steps and total work required. They also propose an algorithm for scan and segmented-sum operations and evaluate its performance empirically.\n\n**Strengths:**\n\n1. The paper provides theoretical guarantees on the number of operations (steps) required for scan and segmented operations, which extend to more general computations.\n\n2. The proposed MMV-RAM model is general and treats existing VCU and MMU architectures as black boxes, allowing future improvements in these components to directly benefit the model.\n\n3. The authors develop efficient algorithms for segmented scan and related primitive operations.\n\n4. Experimental results demonstrate a clear speedup, supporting the theoretical analysis.\n\n**Weaknesses and Questions:**\n\n*Trade-offs in Hardware:*\n\n1. The design integrates an MMU alongside the VCU; what are the trade-offs involved? In particular, since *energy efficiency* is critical for AI accelerators, does this addition significantly increase power consumption? A discussion or experimental analysis on this aspect would strengthen the paper.\n\n2. From a *hardware implementation* perspective, does incorporating an MMU introduce challenges in *physical circuit design* or substantially increase the silicon area? (Acknowledging that I am not an expert in this area.)\n\n*Theoretical Analysis:*\n\n3. The paper states that $T^\\prime = \\frac{\\log n}{\\log s}$ is an improvement over $T = \\frac{\\log n}{\\log \\log n}$. However, theoretically, when $s = O(1)$, we still have $T^\\prime = \\Omega(T)$. Can you explain more about this? I know you said for **an appropriate value of $s$** it is true. More clarification on this value is needed. In addition, a discussion of the *appropriate* values of $s$ in the experiments (for what values this yields the speedup and for which doesn't) is important.\n\n*Experiments:*\n\n4. The experiments should include a runtime analysis of MMV-RAM across varying matrix sizes $n$. It would also be valuable to examine how different choices of $s$ impact the overall performance.\n\n5. The paper does not appear to include experiments on general matrix–matrix operations beyond SCAN and SpMV. It would be helpful to include results for matrix–matrix multiplication, unless I have overlooked them.\n\n6. It would be valuable to include evaluations on more **end-to-end** tasks, such as Transformers or other deep neural networks, to assess whether the proposed design leads to practical end-to-end speedups. In particular, I recommend adding experiments related to the *attention head*, which is especially relevant due to its use of keys and queries and the potential for sparsity. It would be interesting to see whether the proposed algorithms achieve measurable acceleration in this setting. I know adding additional experiments might be painful, but the current experiments do not seem to cover more general tasks, unless you can convince me on that.\n\n*Related work:*\n\n7. Several prior works have achieved speedups using segmented-sum and scan-like operations, such as [1]. It would be useful to discuss how the proposed approach relates to or could be integrated with such methods. Including this in the related work section would help clarify the broader impact and applicability of the proposed design.\n\n*Minor Issues:*\n\n8. On line 283, there is an extra “is” in the sentence — it should read “parallel COMPRESS can be” instead of “parallel COMPRESS is can be.”\n\n**References:**\n\n[1] “An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks.”"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Please see the 'Summary'."}, "weaknesses": {"value": "Please see the 'Summary'."}, "questions": {"value": "Please see the 'Summary'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "84b1GKQYjL", "forum": "DZzD7ddK60", "replyto": "DZzD7ddK60", "signatures": ["ICLR.cc/2026/Conference/Submission7171/Reviewer_UDDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7171/Reviewer_UDDB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012709611, "cdate": 1762012709611, "tmdate": 1762919329507, "mdate": 1762919329507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We would like to thank all the reviewers for their feedback, who carefully read the manuscript and posed interesting questions. As a general remark, we want to mention that the raised concerns were more focused on the experimental part of this work, while fewer were related to the theory part. This is not a \"bad thing\", but we want to remind and to highlight that this work is in its majority **theoretical**, and it should be evaluated as such. The main contributions are all theoretical, with rigorous mathematical proofs, focusing on understanding the fundamental properties of matrix-multiplication/AI accelerators, and whether the notorious matmul-units are any useful *at all* for algorithm design in AI workloads. The experimental part barely covers  about four out of twenty-eight pages of content ($\\approx 14$%). The experiments were provided as a reference to give some intuition on how the main (theoretically-inspired) algorithms would perform in practice, with an \"out-of-the-box\", barely-optimized implementation. We will appreciate if the reviewers and the AC keep these facts in mind, for the final evaluations. Below we provide detailed responses to all specific questions / concerns raised by the reviewers."}}, "id": "v4CI0W62kc", "forum": "DZzD7ddK60", "replyto": "DZzD7ddK60", "signatures": ["ICLR.cc/2026/Conference/Submission7171/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7171/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission7171/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763554257933, "cdate": 1763554257933, "tmdate": 1763556386495, "mdate": 1763556386495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}