{"id": "EfxT5mUQgS", "number": 9201, "cdate": 1758114960944, "mdate": 1759897738072, "content": {"title": "Test-Time Exploration in Unknown Environments", "abstract": "With the continuous advancement of Large Language Models (LLMs), intelligent agents are becoming increasingly vital. However, these agents often fail in environments governed by implicit rules—hidden constraints that cannot be observed directly and must be inferred through interaction. \nThis causes agents to fall into repetitive trial-and-error loops, ultimately leading to task failure.\nTo address this challenge, we propose **Test-Time Exploration (TTExplore)**,  a framework where a thinker component analyzes interaction history to infer these implicit rules and guide an actor. As training a thinker is challenged due to sparse task rewards, we introduce a novel training pipeline for stable reinforcement learning by incorporating techniques such as task decomposition and difficulty filtering. Using this pipeline, we train a specialized 7B model, **Exp-Thinker**. Evaluated on five text-based embodied tasks, TTExplore with our trained Exp-Thinker significantly improves baseline agent scores by an average of $14$-$19$ points, demonstrating the effectiveness of explicitly reasoning about implicit rules.", "tldr": "We propose a framework which enables agents to explore environmental implicit rules at test time, and  train a specialized thinker to improve its performance.", "keywords": ["LLM-based agent", "learn to explore", "reinforcement learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/354e94f06b150c770c413ea43d895e83279a0064.pdf", "supplementary_material": "/attachment/d1f6b46259ff4b35efac6b9472e3441f7b9fa427.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Test-Time Exploration (TTExplore), a framework designed to enhance large language model (LLM)-based agents operating in environments with implicit rules—that is, hidden constraints that cannot be directly observed and must be inferred through interaction. Existing LLM agents tend to fail in such environments, falling into repetitive trial-and-error loops. TTExplore addresses this by introducing a two-role architecture: an actor that performs ReAct-style actions, and a thinker that periodically conducts “deep thinking” to infer unobservable environmental rules from the interaction history and guide the actor’s future decisions.\n\nTo train the thinker, the authors develop a stable reinforcement learning pipeline that mitigates the challenges of sparse rewards. Evaluations on five text-based embodied benchmarks (ALFWorld, SciWorld, BabyAI, Jericho, and PDDL) show that TTExplore improves agent performance. The approach also enhances well-trained agents on out-of-domain tasks, demonstrating strong generalization and complementary benefits to traditional SFT/RL methods.\n\nOverall, this work contributes a new perspective on test-time reasoning and exploration in LLM agents, providing both a conceptual framework and a training methodology for reasoning about implicit environmental rules."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an original and timely contribution to LLM-based agent research by focusing on test-time exploration—a crucial problem of reasoning under implicit environmental rules. The proposed thinker–actor framework introduces a conceptually clear and modular design that separates high-level reasoning from low-level execution, which is broadly applicable.\n\n- From a technical quality perspective, the paper demonstrates solid methodological rigor through a carefully constructed reinforcement learning pipeline that tackles the instability of sparse rewards via task decomposition and difficulty filtering. The experiments are comprehensive, covering multiple embodied benchmarks with consistent and significant improvements over strong baselines.\n\n- The paper is well written and structured, with clear motivation, diagrams, and examples that effectively illustrate the challenges of implicit rule reasoning.\n\n- The work has good amount of contributions: it advances agent autonomy by enabling reasoning at test time, offering a complementary direction to existing fine-tuning or prompt-based methods. This framework could inspire further research on adaptive, self-reflective LLM agents operating in complex and uncertain environments."}, "weaknesses": {"value": "- **Limited theoretical grounding:** The framework lacks a formal analysis of why the thinker–actor interaction improves exploration or generalization. A more principled explanation—e.g., through information-theoretic or reinforcement-learning formulations—would strengthen the conceptual foundation beyond empirical observation.\n\n- **Evaluation scope and metrics:** The experiments focus primarily on text-based embodied environments (e.g., ALFWorld, SciWorld), which may not generalize to other modalities such as visual or tool-use agents. Furthermore, the evaluation metric (process score) captures task progress but not reasoning quality or exploration efficiency. Including qualitative analysis or human evaluation of “deep thinking” outputs would provide stronger evidence of reasoning improvement.\n\n- **Ablation and comparison limitations:** Although the paper compares against ReAct and reflection-style methods, it omits other recent test-time reasoning approaches such as Reflexion (Shinn et al., 2023) or SwiftSage (Lin et al., 2023) in a direct, controlled setting. A more explicit comparison with those would clarify novelty and performance differences. As well, missing the comparison of more recent methods. \n\n- **Scalability and efficiency concerns:** The framework introduces additional inference cost by invoking the thinker periodically. While the paper briefly discusses this, there is no quantitative analysis of compute trade-offs or latency, which is crucial for assessing practicality."}, "questions": {"value": "- Clarification on the thinker–actor interaction mechanism: Could the authors elaborate on how the thinker’s “deep thoughts” are integrated into the actor’s context at inference time? Is there a gating or weighting mechanism for when to trust the thinker’s guidance, or are all thoughts appended uniformly? \n\n- Reward assignment in thinker training: The paper mentions using a single deep-thinking node per trajectory to stabilize RL training. How sensitive is performance to this design choice? Would multiple thought nodes with appropriately discounted rewards destabilize training, or could a credit assignment scheme (e.g., temporal difference learning) be applied?\n\n- Comparison with existing self-reflective frameworks: The paper cites ReAct and Reflexion but does not present a direct quantitative comparison under matched setups. How does TTExplore differ empirically or conceptually from Reflexion’s “verbal reinforcement” and from SwiftSage’s fast–slow thinking architecture?\n\n- Generalization across modalities and domains: Since all experiments are on text-based embodied tasks, can the framework extend to multimodal or real-world robotic environments? Would the thinker’s reasoning process remain effective when observations are visual or high-dimensional?\n\n- Ablation on the training pipeline: The “task decomposition” and “difficulty filtering” components are central to stability. Can the authors provide ablations showing how each contributes quantitatively to final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JMg800R1BD", "forum": "EfxT5mUQgS", "replyto": "EfxT5mUQgS", "signatures": ["ICLR.cc/2026/Conference/Submission9201/Reviewer_UZ3W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9201/Reviewer_UZ3W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760571323230, "cdate": 1760571323230, "tmdate": 1762920869821, "mdate": 1762920869821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to improve agent performance on partial-observable tasks with implict rules. The authors propose the framework **Test-Time Exploration**, which introduces another agent as *thinker* in additional to the actor, and trains a corresponding *thinker* model **Exp-Thinker**. Training this model requires trajectory sampled from a stronger model. The framework utilizes these trajectories through the following steps: it first decomposes a trajectory into sub-tasks (**Sub-Task Division**). Then, the system filtered out those sub-tasks that the actor agent has already been able to solve (**Sub-tasks Filtering**);  Then, the trainable thinker model generates thoughts by comparing strong trajectory that is generated by a stronger model with trajectory of current agent. Eventually, the **Reward Computation** step gives binary rewards: when the thoughts generated help improve the further actions, it will be rewarded with 1, otherwise 0. The paper compares their method with large models e.g. GPT-4o and fine-tuned models for agent tasks, and observe improvement in domains including ALFWorld, SciWorld, BabyAI, Jericho and PDDL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The paper discusses a way of fine-tuning a model using SFT&RL for usage as an agent, which could be of interest of a part of the research community."}, "weaknesses": {"value": "Overall I have three major concerns regarding the idea and implementation of the paper. Specifically\n1. Lack of novelty.  The paper can be regarded as distilling the knowledge gotten from the environment to teach an action model. This idea has long been researched by a sequence of papers: including\n - [O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models](https://arxiv.org/abs/2310.14403): Offline framework that discovers reusable skills from trajectories and distills them into an LLM agent. it tests on **ALFWorld** and WebShop(which is more complex then ALFWorld).\n- [Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.pdf): Trains a VLM in the visual ALFWorld by distilling an LLM expert’s reflections from the parallel text world;\n- [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820): Distills a ReAct-style teacher by segmenting trajectories into [REASON] and [ACT] spans with segment-specific losses. Benchmarks include **ALFWorld**, HotPotQA-ReAct, WebShop.\n\n**These methods share similar philosophy and tested on similar benchmarks. But none of them is included in related works or tested as baseline.**\n\n2. I doubt if \"test time exploration\"  is a suitable name for what is actually implemented in the paper. While the research community has observed advances in test time scaling methods in domains such as large reasoning model, this improvement/novelty may not naturally transfer to agents. This is because, under partial observable environments, it is natural, and actually a **must**, for agents to do \"test time exploration\".  In fact, planning and exploration has long been a core research topic for agents.  This actually has been reflected in the related work section of this paper: \"KnowAgent (Zhu et al.,2024), and KWM (Qiao et al., 2024) utilize pre-acquired knowledge bases to support better planning. KnowSelf (Qiao et al., 2025) advances this idea by enabling agents to actively seek environment-specific knowledge in critical steps. \" Moreover, I don't think the method really fits the name \"test time exploration\". The core idea of this paper is to train another model to guide the action agent, and I don't think it has much to do with \"test time exploration\".\n\n3. The critical weakness of the method I notice, is that it requires **presence of ground truth trajectory**. To enable any of the proposed steps, including sub-task division and filtering, as well as generating the thoughts, it requires the presence of ground truth. In current pipeline, such ground truth trajectories are sampled from strong models. This is doable for simple environments, such as ALFWorld, but this may not hold true for more complex environments, such as WebShop(which is tested in previous ) and WebArena. The authors need to show that their method also work for more complex environments. There are at least two ways to do it: one is to show that their thinker model can directly generalize to more difficult domains, and the second one is to show that such trajectory can still be collected in more complex domains AND the framework can still utilize them well."}, "questions": {"value": "Following the weaknesses described above, the major questions are as follows:\n- How do you think the method is different from the previous works? Could you demonstrate that your methods are better than the previous methods through experiments and discussions(insights)? We may need to see results including \n     - baseline methods on ALFWorld and WebShop \n     - and your results on WebShop(and maybe other more complex domains)\n- Why \"test time exploration\" is a suitable name for what is actually implemented in the paper?\n- Can the thinker model work on more complex domain? This links to question 1 and weakness point 3. \n\nMinor points include:\n- Why adding thoughts at fixed steps is a good alternative for adding thoughts when it is needed? The later one seems much more intuitive. Authors are encouraged to present further experiment results and justify their choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rD2GvDo1Nf", "forum": "EfxT5mUQgS", "replyto": "EfxT5mUQgS", "signatures": ["ICLR.cc/2026/Conference/Submission9201/Reviewer_aoxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9201/Reviewer_aoxx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761089976493, "cdate": 1761089976493, "tmdate": 1762920869373, "mdate": 1762920869373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TTExplore, which augments an acting agent with a specialized “thinker” model trained (via a mix of distillation and RL) to infer hidden environment rules and replan mid-trajectory, and shows large process-score gains across multiple interactive benchmarks using relatively small (~7B/8B) actors. The approach is interesting and practically relevant, but several core pieces currently feel partially hand-engineered (for ex. heuristics in the thinker RL pipeline, fixed intervention frequency), and some claims would benefit from stronger baselines (prompted large-model thinker, adaptive triggering of thinker mode.)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Separates an “actor” from a dedicated “thinker” that explicitly infers hidden environment rules and replans at test time, which is a meaningful and novel framing for agent improvement (not just bigger models or more fine-tuning).\n* Strong empirical gains: Consistently boosts performance across five interactive benchmarks, including out-of-domain settings, and lifts relatively small open models (7B/8B) to or near the level of much larger / heavily tuned baselines.\n* The ablations are tested clearly along the distill, RL axes and has sufficient experiments on different model famlies and envs."}, "weaknesses": {"value": "* The thinker training pipeline relies on several hand-crafted heuristics (trajectory segmentation, difficulty filtering, binary short-horizon rewards with a frozen actor), so it’s unclear how much of the benefit is general vs. engineered for these benchmarks and how transferable it is to new domains.\n* The method is positioned as “test-time exploration,” but parts of the pipeline resemble best-of-N sampling and selection; the paper does not compare against a strong baseline where a large model is simply prompted to generate multiple reflective plans/rule hypotheses and the best one is chosen. A simple baselin of Best-of-N without replanning would strengthen the claims.\n* Claims that a distilled+RL thinker beats scale are somewhat overstated: in some settings a huge off-the-shelf model still does better, especially out-of-domain (see Qwen 7B), and the paper doesn’t fully explain when targeted training wins vs. when raw scale wins.\n* The thinker is triggered on a fixed schedule (every n steps), not adaptively when the agent is actually stuck, and there’s limited discussion of compute overhead or cost/latency tradeoffs versus plain actor baselines. The results on 'thinking' on every 3 steps for larger models and 6 for smaller models is counterintuitive, any hypothesis from the authors would be helpful to explain this behavior.\n\n* Results are reported with Process Score, which rewards incremental progress; the paper doesn’t clearly report actual task success/completion rates or failure cases where the thinker’s inferred “rules” are confidently wrong and actively harms behavior."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gS0xCECk92", "forum": "EfxT5mUQgS", "replyto": "EfxT5mUQgS", "signatures": ["ICLR.cc/2026/Conference/Submission9201/Reviewer_7Rsq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9201/Reviewer_7Rsq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981223510, "cdate": 1761981223510, "tmdate": 1762920869000, "mdate": 1762920869000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TTExplore, a framework for improving LLM-based agents in environments with implicit, unobservable rules. The method separates reasoning and acting into a Thinker–Actor setup, where the Thinker infers hidden constraints at test time to guide the Actor. The agent formalize reward functions to train the LLM thinkers and experiments on five text-based embodied benchmarks show consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focuses on an important and interesting problem: enabling LLM-based agents to handle implicit environmental rules at test time.\n- The proposed Thinker–Actor separation is clearly motivated, where the Thinker discovers hidden constraints and replans to guide a lower-level Actor, and the paper is well-organized, easy to follow, \n- The paper includes comprehensive experiments across multiple benchmarks, with ablation studies and analysis that convincingly support the claims."}, "weaknesses": {"value": "- The approach is largely a combination of existing ideas and algorithms. The overall pipeline of letting LLMs proposing candidates, letting a low-level actor act with the outcomes, and using selected one to refine the model resembles many previous frameworks [1].\n---\n[1] Lee, Dongjun, et al. \"Learning to contextualize web pages for enhanced decision making by LLM agents.\" arXiv preprint arXiv:2503.10689 (2025)."}, "questions": {"value": "- Is the reward for the Thinker binary (0/1)? Have the authors tried using a progression-based reward to provide denser signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hi301JcOxJ", "forum": "EfxT5mUQgS", "replyto": "EfxT5mUQgS", "signatures": ["ICLR.cc/2026/Conference/Submission9201/Reviewer_C4Hx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9201/Reviewer_C4Hx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995846700, "cdate": 1761995846700, "tmdate": 1762920868363, "mdate": 1762920868363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}