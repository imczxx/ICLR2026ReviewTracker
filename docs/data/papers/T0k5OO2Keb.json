{"id": "T0k5OO2Keb", "number": 3061, "cdate": 1757324437774, "mdate": 1759898110960, "content": {"title": "NeuMatC: A General Neural Framework For Fast \tParametric Matrix Operation", "abstract": "Matrix operations (e.g., inversion and singular value decomposition (SVD)) are fundamental in science and engineering. In many emerging real-world applications (such as wireless communication and signal processing), these operations must be performed repeatedly over matrices with parameters varying continuously.  However, conventional methods tackle each matrix operation independently, underexploring the inherent low-rankness  and continuity along the parameter dimension, resulting in significantly redundant computation.  To address this challenge, we propose \\textbf{\\textit{Neural Matrix Computation Framework} (NeuMatC)}, which elegantly tackles general parametric matrix operation tasks by leveraging  the underlying low-rankness and continuity along the parameter dimension. Specifically, NeuMatC unsupervisedly learns a low-rank and continuous mapping from parameters to their corresponding matrix operation results. Once trained, NeuMatC enables efficient computations at arbitrary parameters using only a few basic operations (e.g., matrix multiplications and nonlinear activations), significantly reducing redundant computations. Experimental results on both synthetic and real-world datasets demonstrate the promising performance of NeuMatC, exemplified by over $3\\times$ speedup in parametric inversion and $10\\times$ speedup in parametric SVD compared to the widely used NumPy baseline in wireless communication, while maintaining acceptable accuracy.", "tldr": "", "keywords": ["Low-rank representation;continuous representation;parameterized matrix computation;tensor factorization;machine learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79ad1327df2921421b78f0d9b9d0cacf8336c0ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a deep-learning-based method to efficiently perform operations on a one-parameter family of matrices.\nThe method is based on the assumption that the underlying structure of the transformed matrices is low-rank, i.e., that the matrix curve $p \\mapsto G_i(p)$ spans a low-dimensional subspace of the space of matrices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem the authors try to tackle is a very important one in the community of scientific computing. \n2. The presentation is overall clear, the proposed method is easy, and it seems effective in terms of CPU timing."}, "weaknesses": {"value": "1. For what concerns the problem of computing the SVD, there is a very large literature that studies the problem of doing it in an effective way, see e.g. [1]. The idea here is that if one has the first $A(0) = U(0)S(0)V(0)^\\top$ factorized, then one can evolve the factors using $U(p),S(p),V(p)$ in an efficient way by integrating the equations\n$$\n\\dot U(p) = (I-U(p)U(p)^\\top) \\dot A(p) V(p) S^{-1}(p), \\quad \\dot V(p) = (I-V(p)V(p)^\\top) \\dot A(p)^\\top U(p) S^{-\\top}(p), \\quad \\dot S(p) = U^\\top(p) \\dot A(p) V(p)\n$$\nWhy didn't the authors compare with this family of methods, as these are known as state-of-the-art for parametric SVD computation, no one would ever compute the SVD pointwise.\n\n2. I am a bit skeptical about the low-rankness assumption, especially in the case of parametric inversion. In particular, for inversion, the matrix $A(p)$ needs to be too, and in general, I fail to see why it is reasonable to assume that there is a low-rank structure hidden in the path $p \\mapsto A(p)^{-1}$. Even more, it is known that the matrix inversion is a rational function of the entries (therefore not polynomial), therefore I would expect the path $p \\mapsto A(p)^{-1}$ to not have low-rank \"generically\" in the paths [2].\n\n3. As far as I understand, the rationale behind and the training itself resemble the idea of PINNS, as depicted in Figure 1, the NN takes as input the time parameter stamps and it outputs the matrix $G(p)$. Unfortunately, contrary to operator learning like methods, the proposed approach requires training a new model for each curve $p \\mapsto A(p)$. \n\n[1] O.Koch, C. Lubich, Dynamical low-rank approximation, SIMAX 2007.\n[2] A. Pinkus,  Approximation theory of the MLP model in neural networks, Acta Numerica (1999), pp. 143-195"}, "questions": {"value": "1. Why in Tables 1,2,3 are the timings only in CPU? Could the authors also show the times on the GPU? \n2. I understand the advantage of the proposed approach is at inference time,  but could the authors discuss a bit more in detail how expensive the model fitting?\n\nI would also appreciate it if the authors could discuss the \"weaknesses\" section, as I believe fixing those points could significantly improve the quality of the work"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjgmFqz5At", "forum": "T0k5OO2Keb", "replyto": "T0k5OO2Keb", "signatures": ["ICLR.cc/2026/Conference/Submission3061/Reviewer_4t5r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3061/Reviewer_4t5r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761071210430, "cdate": 1761071210430, "tmdate": 1762916534719, "mdate": 1762916534719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of efficiently computing matrix operations such as matrix inversion or SVD for a family of matrices $A(p)$ parameterized by a variable $p \\in \\mathbb{R}$. To this end, the authors construct a neural networkâ€“based mapping with trainable parameters that directly maps the parameter $p$ to the result $G(p)$ of the matrix operation applied to $A(p)$. The design of this mapping relies on continuity and low-rankness assumptions on the function $p \\mapsto G(p)$. The experimental results compare the proposed approach with traditional solvers, which cannot exploit such assumptions when computing a batch of matrix operations over a family of matrices $A(p)$ for multiple values of $p$. The authors show that these traditional solvers are slower than the proposed method, which can leverage these assumptions effectively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The experimental results on SVD seem to be sound. The method effectively shows a speedup compared to baselines without sacrificing accuracy.\n* Theorem 1 motivates the design of a compact mapping, which makes the proposed method well suited for the SVD operation. Indeed, when the matrices \\( A(p) \\) are low-rank, the corresponding SVD components \\( U(p) \\), \\( V(p) \\), and \\( S(p) \\) are also low-rank, thereby satisfying the assumptions of Theorem 1.\n* The authors proposed an ablation study to understand some key designs of the method: adaptive sampling, hyperparameters, etc.\n* The proposed approach scales with the matrix sizes, and benefits from the parallelism in GPU because it relies only on GEMM or matrix-tensor product."}, "weaknesses": {"value": "Theorem 1 might be misleading in some aspects when considering matrix inversion. The assumption of Theorem 1 is that the matrix operation results $G(p)$ are low-rank. However by definition an invertible matrix cannot be low-rank. From this point of view the Figure 2 might be misleading as well: it shows that both A(p) and A(p)^{-1} are close to low-rank, but this is not possible in general unless there are some assumption on A(p). Take for instance the matrices $H(p) = A(p) B(p)^\\top + \\epsilon I_n$ as considered in the experimental section 3.1 of the paper. By the Woodberry identity, we have that: $H(p)^{-1} = \\frac{1}{\\epsilon} I_n - \\frac{1}{\\epsilon^2} A(p) \\Big(I_r + \\tfrac{1}{\\epsilon} B(p)^\\top A(p)\\Big)^{-1} B(p)^\\top$. As $\\epsilon \\to 0$, $H(p)$ tends to a low-rank matrix, while its inverse behaves very differently: $H(p)^{-1} = \\frac{1}{\\epsilon}\\Big(I - A(p)(B(p)^\\top A(p))^{-1}B(p)^\\top\\Big) + o \\left(\\frac{1}{\\epsilon}\\right)$,\nso $H(p)^{-1}$ becomes full rank and its norm grows like $1/\\epsilon$. Therefore the application of the proposed method to matrix inversion needs a clarification from the authors."}, "questions": {"value": "* In Tables 1 and 2, it is unclear whether the experimental protocol evaluates the proposed method on a **batch** of matrices or on a **single** matrix at a time. Since the primary application of the method concerns batched matrix operations, it is essential to compare its performance against batched versions of baseline algorithms (e.g., batched SVD in NumPy, batched randomized SVD, etc.). The authors should clarify this point to ensure a fair and meaningful validation of their results.\n* To what extent can the proposed method be extended to other matrix operations, such as QR decomposition, LU decomposition, Cholesky decomposition, or interpolative decomposition?  \n* During training, how did the authors handle the non-uniqueness of the solutions to a given matrix operation? For example, the SVD decomposition is not unique when an eigenvalue has a multiplicity greater than one."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n6nJkWzidn", "forum": "T0k5OO2Keb", "replyto": "T0k5OO2Keb", "signatures": ["ICLR.cc/2026/Conference/Submission3061/Reviewer_SGgi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3061/Reviewer_SGgi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767783389, "cdate": 1761767783389, "tmdate": 1762916534349, "mdate": 1762916534349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NeuMatC, a neural framework designed to accelerate parametric matrix operations, such as inversion and singular value decomposition (SVD). The core idea is to leverage the assumed low-rank structure and continuity of the matrix operation results with respect to a varying parameter $p$. Instead of solving each matrix problem independently, NeuMatC learns a continuous mapping from the parameter $p$ to a low-dimensional latent representation, which is then used to reconstruct the final operation result (e.g., the inverse matrix or SVD components) via a multilayer perceptron (MLP) and a tensor product. The authors claim that, once trained, NeuMatC can achieve significant speedups (3-10x) over conventional methods like NumPy on both synthetic and real-world wireless communication datasets while maintaining acceptable accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**Significance:** The paper addresses the problem of parametric matrix computations, which is indeed significant and computationally demanding in many emerging applications like wireless communications, signal processing, and control systems. Developing methods to accelerate these repeated operations is a valuable research direction.\n\n**Originality:** The core concept of learning a continuous, low-rank mapping from parameters directly to matrix operation results is an interesting and novel approach. It moves beyond treating each matrix instance independently and attempts to learn the underlying structure of the problem family, which is a promising idea.\n\n **Clarity:** The paper is generally well-written and clearly presents its proposed framework, NeuMatC. The motivation and the high-level mechanism of the algorithm are easy to follow."}, "weaknesses": {"value": "##### \n\nDespite the interesting premise, the paper suffers from significant weaknesses in its experimental evaluation, scope, and positioning within the existing literature. These issues are severe enough to undermine the paper's central claims.\n\n1.  **Insufficient and Unfair Experimental Comparisons:** The experimental setup is not rigorous enough to support the claims of superiority.\n    *   **CPU vs. GPU:** A major weakness is the comparison against inappropriate baselines. The proposed GPU-based method is compared primarily against CPU-based Python libraries (NumPy). This is not an apples-to-apples comparison. For a convincing evaluation, comparisons against GPU-accelerated libraries like **cuSOLVER** are essential.\n    *   **Choice of Libraries:** High-performance scientific computing typically relies on highly optimized libraries in languages like C++, Fortran, or Julia (e.g., **SLEPc, Eigen, Arpack**), which are often significantly faster than their Python counterparts. The reported speedups against NumPy may be inflated due to the choice of a suboptimal baseline implementation.\n    *   **Lack of Preconditioning:** The paper's comparisons with traditional iterative solvers appear to omit preconditioning. In practice, techniques like spectral transformation preconditioning are standard for solvers like LOBPCG and can dramatically accelerate convergence. Omitting them from the comparison makes the traditional methods appear much slower than they would be in a realistic setting and undermines the validity of the claimed speedup.\n    *   **Neglect of Relevant Literature:** The paper fails to compare against a significant body of relevant work.\n        *   There is no direct comparison with other neural network-based eigenvalue/SVD solvers that share a similar PINN-like architecture (e.g., **neuralSVD, PMNN, STNet** [1-3]). A comparison of accuracy and speed against these methods is necessary to position the contribution.\n        *   More importantly, the paper ignores a rich field of traditional numerical methods designed specifically for series of related matrix problems. Techniques like **Krylov subspace recycling** [4], the **CHASE** algorithm [5], or using the invariant subspace from a previous solution as the initial guess for **LOBPCG** are designed to exploit the very same problem structure. A thorough comparison with these methods is essential to demonstrate the novelty and practical advantage of the proposed framework.\n\n2.  **Limited Scope and Scalability Analysis:** The experiments are conducted in a narrow and relatively easy regime, which limits the generalizability of the findings.\n    *   **Small Matrix Sizes:** The experiments are confined to relatively small matrices (dimension < 10,000). These problems can often be solved in seconds or less by traditional methods. The true challenge in industrial and scientific applications lies in large-scale matrices (e.g., dimensions from $10^4$ to $10^6$). The paper fails to demonstrate the method's performance in this critical regime. Comparisons should be made against state-of-the-art iterative solvers like **Krylov-Schur, LOBPCG, and JD** (e.g., using implementations from SLEPc [6]) for such large-scale problems.\n    *   **Limited Matrix Types:** The evaluation should consider both **sparse matrices** (common in PDE discretizations) and **dense matrices** (e.g., from quantum chemistry). The paper's focus on low-rank matrices from communication is a significant limitation. The performance on non-low-rank matrices, such as those arising from PDE stability analysis (e.g., modal analysis of Maxwell's equations), is not explored. If the method is only applicable to low-rank problems, this limitation must be clearly stated in the abstract and introduction.\n\n3.  **Inadequate Justification of Low-Rank Claims:** While the paper is motivated by the low-rank property of parametric matrices, the experimental validation is superficial.\n    *   The authors do not provide details on the rank structure of their test matrices (e.g., a plot of singular value decay or loss versus rank). This makes it difficult to assess how \"low-rank\" the problems truly are and how well the method exploits this property.\n    *   To properly validate the low-rank performance, the authors should construct synthetic datasets with explicitly controlled ranks and compare against specialized low-rank SVD algorithms. Simply testing on real-world datasets is insufficient.\n    *   When comparing to randomized SVD, crucial details like the choice of the compression dimension (rank) are omitted. The performance of randomized SVD is highly dependent on this parameter, which should be chosen based on the target rank or error tolerance [7].\n\n[1] Operator SVD with neural networks via nested low-rank approximation, ICML 2024\n\n[2] Neural networks based on power method and inverse power method for solving linear eigenvalue problems, Computers & Mathematics with Applications 2023\n\n[3] STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem,  NeurIPS 2025\n\n[4] Recycling Krylov subspaces for sequences of linear systems, SIAM 2006\n\n[5] ChASE: Chebyshev accelerated subspace iteration eigensolver for sequences of Hermitian eigenvalue problems, ACM 2019\n\n[6] SLEPc: A scalable and flexible toolkit for the solution of eigenvalue problems, ACM 2015\n\n[7] Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions, SIAM 2011"}, "questions": {"value": "My decision could potentially be changed if the authors can provide satisfactory answers to the following fundamental questions:\n\n1.  **Handling of Singular Value/Vector Crossings:** A critical concern is the handling of singular value/vector crossings. As a parameter $p$ varies continuously, the ordering of singular values can change (e.g., the first and second singular values swap). A neural network trained to output the \"k-th\" singular vector would likely fail at such crossing points, as it would need to abruptly switch its output from one vector function to another, which is a discontinuous behavior. How does the proposed framework handle this well-known phenomenon in parametric eigenvalue problems? Were the datasets specifically chosen to avoid such crossings? A discussion of this issue can be found in literature such as [1] P376.\n\n2.  **Scalability and Fair Baselines:** Could the authors provide performance benchmarks on truly large-scale matrices (e.g., dimensions from $10^4$ to $10^6$), for both sparse and dense cases? In this context, could you compare against established large-scale iterative solvers like **Krylov-Schur, LOBPCG, and JD** (e.g., from SLEPc), including fair comparisons on GPU (e.g., vs. **cuSOLVER**) and with appropriate **preconditioning**?\n\n3.  **Practical Applicability and Overhead:** The proposed method requires a pre-generated training set of related matrices.\n    *   In a real-world scenario where matrix problems may arise independently and without a unified parametric representation, what is the applicable scope of this method?\n    *   When comparing against traditional solvers that work on a single instance, should the overhead of data generation and network training be included in the timing? What are the specific application scenarios where generating such a large, correlated dataset is feasible and the amortization of training cost makes this approach superior to traditional methods that can exploit problem-to-problem correlation without pre-training (e.g., **Krylov subspace recycling** [1], the **CHASE** algorithm [2], or using the invariant subspace from a previous solution as the initial guess for **LOBPCG**)?\n\n4.  **Low-Rank Analysis:** To substantiate the claims about leveraging low-rank structure, could the authors provide a more detailed analysis? This should include: (a) visualizations of the singular value decay for the matrices in your datasets, (b) an ablation study showing performance on synthetic datasets with explicitly controlled ranks, and (c) the specific configuration details for the randomized SVD baseline, particularly the rank of the projection.\n\n[1] Spectra and Pseudospectra by Trefethen and Embree\n\n[2] Recycling Krylov subspaces for sequences of linear systems, SIAM 2006\n\n[3] ChASE: Chebyshev accelerated subspace iteration eigensolver for sequences of Hermitian eigenvalue problems, ACM 2019"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "glpaOyIFjf", "forum": "T0k5OO2Keb", "replyto": "T0k5OO2Keb", "signatures": ["ICLR.cc/2026/Conference/Submission3061/Reviewer_WYpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3061/Reviewer_WYpt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815703234, "cdate": 1761815703234, "tmdate": 1762916533827, "mdate": 1762916533827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors train a MLP with a tensor head to solve matrix problems that are related in a 1 dimensional parameter space\nby minimizing a data fidelity and structure consistency loss."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The authors tackle a high impact problem as matrix operations are ubiquitous in science applications. Also the\n  approach seems quite novel (when compared to the related work).\n* The authors show strong results against classical numerical algorithms which are hard to beat. The results contain\n  comprehensive metrics on test relative error, GFLOPs and runtime.\n* The authors provide several theoretical claims that support their choice of mapping function."}, "weaknesses": {"value": "* Having a method that only applies to 1 dimensional parameters seems limiting. Would the objective in Eq (3) be able to\n  generalize to larger dimensions?\n* No code is shared through a anonymized link. How are the authors planning to promote adoption and reproducibility?\nIndeed there are some details in Appendix F, though the details are about ranges of hyperparameters.\n* The speed-up improvements do not account for the NN training. Imagine that you have 100 parameters points where you\n  want to run a matrix operations. How much more competitive is your method if you were to account for the MLP training?\n  If your loss already requires $N_s = 100$ then there wouldn't be any improvement."}, "questions": {"value": "* Lines 231, 245. Could you expand on the definition of \"dense samples\"?\n* How do you choose between $N_s$ and $N_c$? It seems that $N_c$ should always be much larger than $N_s$ as we avoid\n  having to create data with a numerical solver?\n* What would happen if I pass all the unsupervised $N_c$ points to the Data Fidelity Loss? Would the trained model\n  always be better? Or I guess you want the backprop to consider the structure so that the models is aware of that?\n  Do the matrices have to be exactly the same size or can they vary? Imagine that I have the classical least squares solution $(X^T X + \\alpha I)^{-1} X^Ty$ where $\\alpha$ is a single\n  dimensional parameter. Would your method work for d=3 coefficients and for d=5?\n* Besides the Lipschitz constant, is there any intuition on which activations would work better for these type of matrix\n  problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ffKb5uAiU0", "forum": "T0k5OO2Keb", "replyto": "T0k5OO2Keb", "signatures": ["ICLR.cc/2026/Conference/Submission3061/Reviewer_8kXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3061/Reviewer_8kXx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863959926, "cdate": 1761863959926, "tmdate": 1762916533535, "mdate": 1762916533535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}