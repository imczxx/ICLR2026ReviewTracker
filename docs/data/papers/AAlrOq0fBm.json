{"id": "AAlrOq0fBm", "number": 6296, "cdate": 1757965233462, "mdate": 1759897924231, "content": {"title": "Learning to Reason over Neighborhoods: A Differentiable Guarded Logic Approach", "abstract": "Systematic generalization remains a well-recognized fundamental barrier for deep learning, especially in tasks requiring multi-hop relational reasoning. We posit this failure stems from a missing \\emph{inductive bias} for local, compositional inference---a structure that is inherent to symbolic logic but absent in monolithic neural architectures. Our core insight is that the Guarded Fragment (GF)---a classic, decidable fragment of first-order logic---provides the ideal computational primitive for this paradigm. We reveal that its syntactic `guard' is not merely a constraint, but is formally equivalent to a mechanism for reasoning over local, relational neighborhoods. We operationalize this insight in \\textsc{GuardNet}, the first framework to leverage GF as a principled inductive bias for neighborhood reasoning, featuring a novel dynamic domain strategy to prevent representational collapse. \\textsc{GuardNet} employs a principled fuzzy semantics derived from Product t-norms, grounding it in theoretical soundness while enabling stable, end-to-end integration with neural architectures. On challenging benchmarks for knowledge base completion, \\textsc{GuardNet} unlocks superior systematic generalization, succeeding on complex inferences where purely neural and prior neuro-symbolic systems falter. Our work demonstrates that classical logics can be reframed as a powerful inductive bias for modern representation learning, offering a principled pathway toward neural networks that can robustly reason.", "tldr": "", "keywords": ["systematic generalization", "inductive bias", "guarded fragment", "fuzzy sem"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86ad106c52504ec719e56aa7c348f54d9802a920.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the use of guarded fragment logic for multi-hop reasoning in knowledge bases."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Interesting idea of using guarded fragment of first-order logic\n- Outperforms all baselines"}, "weaknesses": {"value": "- A dedicated related work section is missing.\n- Highly relevant related work is missing, for example [1] and [2].\n- Important baselines are missing, e.g., CQD [1].\n- In the paper, for a long time, it does not become clear what exactly the task(s) to be solved are. Only at the beginning of the evaluation section, it becomes clear that the paper targets concept subsumption prediction and (multi-hop) link prediction. This should become clear already in the introduction.\n- The novelty does not become sufficiently clear. Large parts of Sections 2, 3, and 4 do not seem to be novel (e.g. guarded fragment, fuzzy logic, ...). It should be made more clear what parts are novel.\n- The loss function might be new. However, a precise comparison and possibly even an ablation study to previous loss functions would be interesting (e.g., of Box2EL and similar approaches)\n\n[1] Arakelyan, Erik, Daniel Daza, Pasquale Minervini, and Michael Cochez. \"Complex query answering with neural link predictors.\" arXiv preprint arXiv:2011.03459 (2020).\n\n[2] Ren, Hongyu, Mikhail Galkin, Michael Cochez, Zhaocheng Zhu, and Jure Leskovec. \"Neural graph reasoning: Complex logical query answering meets graph databases.\" arXiv preprint arXiv:2303.14617 (2023)."}, "questions": {"value": "- How is your approach related to knowledge graph embeddings?\n- How is your approach related to neural graph databases [2]?\n- How is your approach related to CQD [1]?\n- Can you provide evaluation results comparing your approach to CQD?\n- What exactly is novel in Section 2 and 3 of the paper?\n- What exactly is novel in Section 4 of the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qFecphv3Fk", "forum": "AAlrOq0fBm", "replyto": "AAlrOq0fBm", "signatures": ["ICLR.cc/2026/Conference/Submission6296/Reviewer_6ReF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6296/Reviewer_6ReF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761066660362, "cdate": 1761066660362, "tmdate": 1762918598457, "mdate": 1762918598457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel neural reasoning method, ‘reason over neighbourhoods’, for a decidable fragment of first-order logic – Guarded Fragment. An end-to-end differentiable framework – GuardNet -- is implemented for a fuzzy semantics of Guarded Fragment. GuardNet is experimented on several knowledge graphs in two tasks: concept subsumption prediction and link prediction."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Authors target to move beyond the statistical paradigm of machine learning, and propose a paradigm shift to view logic as a computational primitive and seamlessly integrated into the core of deep learning.\n\nUsing fuzzy logic to reason with knowledge graphs that contain inconsistent information."}, "weaknesses": {"value": "Authors started with a brave claim for a paradigm shift through introducing Guarded Fragment – a fragment of first-order logic – into deep learning, then moved backwards to fuzzy Guarded Fragment and implemented a GuardNet within the statistical paradigm. \n\nThe theoretical part, which glowed with crisp logic, became an unreachable utopia in GuardNet with fuzzy logic.\n\nGuarded Fragment can be easily implemented by extending Sphere Neural Networks and keep the rigour of classic logic in the neural world. \n\nThe second part (fuzzy logic + GuardNet) can be another paper.\n\nGuardNet will not achieve the total loss of zero, because knowledge base are incomplete and inconsistent. Thus, Theorem 1 is dispensable."}, "questions": {"value": "In the experiment, why didn't you list H@1 results for Yeast PPI+GO, Human PPI + GO, FB15k-237, and WN18RR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xHSBl7i7z8", "forum": "AAlrOq0fBm", "replyto": "AAlrOq0fBm", "signatures": ["ICLR.cc/2026/Conference/Submission6296/Reviewer_ZtWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6296/Reviewer_ZtWL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635588660, "cdate": 1761635588660, "tmdate": 1762918598116, "mdate": 1762918598116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors limit the expressivity of neurosymbolic methods based on fuzzy logic (eg LTN) to the guarded fragment of First Order Logic. They also design a special aggregation function for universal and existential aggregation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Provides a formal assessment of the complexity benefits of the guarded fragment compared to unrestricted FOL, which is not yet discussed in the neurosymbolic literature. \n- Experiments suggest the method gets good performance\n- The paper was relatively easy to follow"}, "weaknesses": {"value": "- The ideas do not seem novel to me. The LTN framework [1] already has (optional) guarded quantifiers. Except for the different aggregation function, it is not obvious how this paper extends or improves on LTN. \n- The paper is quite unclear about its goals. The abstract and introduction mostly talk about multi-hop reasoning and inductive biases, but in the end the paper is about adding a loss function based on background knowledge. This seemed incongruent. \n- The LSE aggregator is not properly defined for fuzzy logics\n- The experimental description is very unclear"}, "questions": {"value": "- How is this paper supporting the claim (abstract) that GuardNet 'reframes logic as a powerful inductive bias for modern representation learning, offering a principled pathway toward neural networks that can robustly reason'? And how is this different compared to existing approaches (eg LTN). \n- Since LTN already implements guarded quantification, I am surprised it times out on one dataset where GuardNet doesn't (Table 1). Their complexities should be the same. Why is this?\n- On other datasets, how do you explain the large difference in performance with LTN? \n- The LSE aggregation functions do not return a value in $[0, 1]$. Eg, say $\\mathbf{z}=[0.1, ..., 0.1]\\in [0, 1]^{10}$, then the function $f(\\tau) = \\tau \\log (\\sum_{i} \\exp(z_i/\\tau))$ is just a linear function and could return values above 1 depending on $\\tau$. Similarly the $\\inf$ generalisation would be negative. It is unclear how to interpret these from a fuzzy perspecitve. \n    - Note that the paper does define the semantics $[[\\phi]]$ as being in $[0, 1]$ so this is definitely not intentional. The soundness also relies on this. \n- Experiments:\n    - It is not clear where the knowledge comes from that is used to create the theories.\n    - It is not clear how the multi-hop reasoning experiments are done\n- Related work:\n    - This should be in the main text\n    - C.1.1 conflates expressivity and intractability with undecidability. On finite domains (commonly assumed in all mentioned neurosymbolic frameworks) all methods are decidable. \n    - C.1.2: The paper argues superiority over previous EL description fuzzy logics as it is more expressive. How is this increased expressivity used and how does this reflect in the experiments?\n    \n\nMinor:\n- \"Since $\\sup$ and $\\inf$ are non-differentiable: They are differentiable except when inputs are the same. They just return 0."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w3XeKI7FbZ", "forum": "AAlrOq0fBm", "replyto": "AAlrOq0fBm", "signatures": ["ICLR.cc/2026/Conference/Submission6296/Reviewer_VECM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6296/Reviewer_VECM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668126203, "cdate": 1761668126203, "tmdate": 1762918597738, "mdate": 1762918597738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GUARDNET, a neuro-symbolic framework that leverages the Guarded Fragment (GF) of first-order logic as a differentiable inductive bias for neighborhood reasoning. The authors argue that guarded quantification provides a principled locality bias, improving systematic generalization in reasoning tasks. The model extends GF with fuzzy semantics based on the Product t-norm and a Reichenbach-style implication, and integrates it with neural architectures through MLP-based predicate grounding and a hybrid domain decomposition (core and latent). Empirical results on several knowledge base completion benchmarks report large performance improvements over prior work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and presents a clear exposition of the Guarded Fragment and its differentiable extension.\n\nThe idea of interpreting guarded quantification as a locality bias for reasoning over relational neighborhoods is conceptually appealing.\n\nThe hybrid domain strategy (core plus latent) is interesting and could provide a mechanism to balance logical fidelity and generalization."}, "weaknesses": {"value": "Limited novelty of the guarded fragment focus. \n\nLimited novelty of  fuzzy semantics and grounding. \n\nEmpirical gains lack explanatory analysis. \n\nMissing statistical interpretation of the hybrid domain. \n\n\nI will detail these points in the section below."}, "questions": {"value": "**Limited novelty of the guarded fragment focus.**\nThe emphasis on the Guarded Fragment seems overstated. While the paper presents it as a novel inductive bias, similar mechanisms already appear in existing neuro-symbolic frameworks such as Logic Tensor Networks (LTN)[1] and Semantic-Based Regularization (SBR)[2]. In these systems, quantification is effectively guarded. For example, \n- LTN employs diagonal quantification that limits grounding to dataset tuples, and \n- SBR uses multi-sorted logic that restricts quantification domains. \n- Even standard manifold regularization in extensions of SBR[3], ala \"MLN smokers”, restricts reasoning to subsets defined by the manifold relation R [forall x,y: R(x,y) -> A(x) <-> A(y). \n\nThese mechanisms seem all very conceptually related to guarded quantification. The paper would have benefited from a clearer discussion of what new expressive or computational capabilities the GF formalization provides beyond these earlier works.\n\n**Limited novelty of  fuzzy semantics and grounding.**\nThe proposed fuzzy semantics and neural grounding choices are conventional. The combination of Product t-norm, S-implication, and LogSumExp approximations is well known from previous differentiable fuzzy logic frameworks, e.g. LTN, logLTN. Similarly, using MLPs with concatenated embeddings to ground predicates has been standard practice even before early embedding-based approaches such as TransE and ComplEx (see corresponding related works). As presented, these sections largely restate established design patterns rather than offering conceptual innovation.  Going to simpler neural parameterization of the predicates reflects a broader and well-known trade-off in knowledge graph embeddings models. Recent approaches (for instance, those[4] using Clifford algebras for structured embeddings) explore explicit mechanisms for balancing expressivity and generalization. The paper would have benefited from positioning GUARDNET within this wider design space.\n\n**Empirical gains lack explanatory analysis.**\nThe reported performance improvements are unexpectedly large given the strong methodological overlap with prior models. The paper does not isolate which components actually drive these gains. For instance, it is unclear whether the guarded restriction, the latent domain sampling, or other training details are responsible. A targeted ablation study would help validate the core claim that guardedness itself leads to better generalization.\n\n**Statistical interpretation of the hybrid domain.**\nThe two-domain decomposition (core and latent) is interesting but under-specified statistically. The latent domain is sampled from a Gaussian prior, similar to variational autoencoders, yet it is not clear whether constants and latent samples share the same embedding space or semantics. There are no guarantees about model behavior outside the support of the prior (if any within). The paper should clarify how this stochastic component interacts with logical constants and what assumptions underlie their joint interpretation.\n\n\n[1] Badreddine, Samy, et al. \"Logic tensor networks.\" Artificial Intelligence 303 (2022): 103649.\n\n[2] Diligenti, Michelangelo, Marco Gori, and Claudio Sacca. \"Semantic-based regularization for learning and inference.\" Artificial Intelligence 244 (2017): 143-165.\n\n[3] Marra, Giuseppe, et al. \"Relational Neural Machines.\" ECAI 2020. IOS Press, 2020. 1340-1347.\n\n[4] Kamdem Teyou, Louis Mozart, Caglar Demir, and Axel-Cyrille Ngonga Ngomo. \"Embedding Knowledge Graphs in Degenerate Clifford Algebras.\" ECAI 2024. IOS Press, 2024. 1293-1300"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GeG3QRXoKr", "forum": "AAlrOq0fBm", "replyto": "AAlrOq0fBm", "signatures": ["ICLR.cc/2026/Conference/Submission6296/Reviewer_4PgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6296/Reviewer_4PgB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816156318, "cdate": 1761816156318, "tmdate": 1762918597280, "mdate": 1762918597280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}