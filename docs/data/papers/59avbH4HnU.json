{"id": "59avbH4HnU", "number": 14654, "cdate": 1758240870078, "mdate": 1759897356980, "content": {"title": "Relatron: Automating Relational Machine Learning over Relational Databases", "abstract": "Predictive modeling over relational databases (RDBs) powers applications in various domains, yet remains challenging due to the need to capture both cross-table dependencies and complex feature interactions. Recent Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite promising performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood.\nWe present a comprehensive study that unifies RDL and DFS in a shared design space and conducts large-scale architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a curated model performance bank that links model architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL–DFS performance gap and introduce two task signals—RDB task homophily and an affinity embedding that captures path, feature, and temporal structure—whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that first chooses between RDL and DFS and then prunes the within-family search to deliver strong performance. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the “**more tuning, worse performance**” effect and, in joint hyperparameter–architecture optimization, achieves up to 18.5\\% improvement over strong baselines with $10\\times$ lower computational cost than Fisher information–based alternatives.", "tldr": "", "keywords": ["AutoML", "Relational database", "Relational deep learning", "Graph machine learning", "Tabular machine learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcaab3502e1798cc27a87b622325513bc1bd7aab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an empirical study comparing Relational Deep Learning (RDL) and Deep Feature Synthesis (DFS) for predictive modeling on relational databases. The authors construct a large-scale design space for both model families and build a \"model performance bank\" by running an architecture-centric search across numerous RDB tasks. Relatron uses diffferent signals in a meta-predictor to first choose between RDL and DFS (macro-selection) and then prune the search within that family."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets an important problem, relational deep learning (RDL), in the machine learning.\n- The experimental analysis across models and datasets is interesting."}, "weaknesses": {"value": "- The recommendation task is not involved. Only classification and regression task types from Relbench.\n- The results of a foundation model, KumoRFM, outperforms Relatron that selects between models. This questions the practical usage of Relatron. Since there is already a stronger foundation model, is it still necessary to design and train a model selector?"}, "questions": {"value": "- The paper provides a theoretical argument in Appendix C.2 for RDL's strength in low-homophily settings. It would be better to explain the other, more surprising half: why DFS is so strong in high-homophily regimes. \n- Could Relatron also include KumoRFM, the current state-of-the-art foundation model, into its selection pool?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IjSDf6Hfsx", "forum": "59avbH4HnU", "replyto": "59avbH4HnU", "signatures": ["ICLR.cc/2026/Conference/Submission14654/Reviewer_Yptq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14654/Reviewer_Yptq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947869495, "cdate": 1761947869495, "tmdate": 1762925026669, "mdate": 1762925026669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper lays out a design space of modeling approaches for machine learning (ML) on relational database (RDB) tasks, and does a comprehensive benchmarking study on it. The findings suggest that different tasks require different design choices (including model architecture). To automate this choice the paper proposes a few metrics which have high correlation with the performance of various approaches on different tasks and can be used to predict the best method. This AutoML system (Relatron) achieves performance improvements over hyperparameter-tuning and auto-transfer baselines, while being more efficient computationally."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is thorough in its parameterization of the design space.\n* The findings are interesting. I like the experiment and finding that validation metrics are unreliable as this is very important for temporal splits as in leading relational benchmarks.\n* The metrics proposed for \"task embeddings\" are simple, clear, clever, and have high predictive power.\n* The baseline comparisons are comprehensive.\n* The paper is well-written overall."}, "weaknesses": {"value": "I think it is important to include intuitions and analysis (theoretical as well as empirical) for why RDL is better at low-homophily tasks and DFS is better at high-homophily tasks in the main paper. Currently it is in Appendix C.2, but it is too long (4 pages!) and it is not clear what the intuitive takeaways are. It would be nice to have a discussion of the main intuitions and takeaways in the main paper. It would be ideal if the theory can be substantiated with some experiments."}, "questions": {"value": "1. Why is RDL better at low-homophily tasks and DFS better at high-homophily tasks?\n2. How can RDL be improved based on insights from 1?\n3. Are there some kind of error bars for Figure 2?\n4. What is \"labeling tricks for RDL\"?\n5. L412: What is GraphGym similarity? How is it \"ground truth\"?\n\nMinor:\n* L040: repeated citation\n* L059: remove space before footnote\n* L362-363: You might be interested in [1] as they have similar findings/methodology and propose similar terminology \"post-hoc selection\".\n\n[1] Post-Hoc Reversal: Are We Selecting Models Prematurely? NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mOxuiEKIlP", "forum": "59avbH4HnU", "replyto": "59avbH4HnU", "signatures": ["ICLR.cc/2026/Conference/Submission14654/Reviewer_Eebt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14654/Reviewer_Eebt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968067811, "cdate": 1761968067811, "tmdate": 1762925025993, "mdate": 1762925025993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work conduct a detailed analysis between Relational Deep Learning (RDL) and DFS method with heuristic feature aggregator. It reveals that 1) DFS can outperforms  RDL on some tasks. 2)Different architecture are needed for different RDB tasks. 3) valid performance is not reliable. So this work propose an automl framework"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extensive experiments over various baselines and datasets. Figure 2, Table 2-4, shows various experiments.\n2. Insightful observation for model selection. To me, “Correlation between homophily and RDL-DFS performance gap\" is the most interesting finding. Though homophily is a common tool in graph learning, it is first used in RDB datasets to our best knowledge."}, "weaknesses": {"value": "1. Griffin cited in this work should also be included as one important baseline on RDB tasks.\n2. The comparison between these auto ml framework and vanilla baseline is missing. If this framework leads to large computation overhead, then vanilla baseline may be prefered in real-world application."}, "questions": {"value": "1. While automl framework is useful, I still think a unified RDB foundation model is the future. Can observations in this work helps development of RDB foundation model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WCadbFKxDm", "forum": "59avbH4HnU", "replyto": "59avbH4HnU", "signatures": ["ICLR.cc/2026/Conference/Submission14654/Reviewer_FPiy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14654/Reviewer_FPiy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994921611, "cdate": 1761994921611, "tmdate": 1762925025467, "mdate": 1762925025467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on automatic architectural selection for predictive task on relational databases. The model proposes Relatron, a task embedding-based selector using novel task signals. The performance bank is a reusable resource, and findings like task-dependent RDL vs. DFS trade-offs challenge prevailing assumptions in the field. Experiments demonstrate meaningful gains (up to 18.5% improvement with 10× compute savings), making it relevant for real-world deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The architecture search (180 configs/task for entity-level, 20 for DFS) is comprehensive. The findings are interesting as it show DFS wins on more tasks , attributing gaps to homophily."}, "weaknesses": {"value": "1. The scope of the paper is limited, as it focuses on from-scratch models and defers foundation models(e.g., Griffin, KumoRFM) despite comparisons (Fig. 2). It would be interesting to see how Relatron can handle pretrained relational foundation models. \n2. The experiment dataset is limited, only covering most of RelBench and two additional tasks. It would be interesting to see the performance across multiple datasets. \n3. The correlation between homophily and RDL-DFS performance gains are strong, but non-parametric. It would be interesting to see if the authors can.generate synthetic data(tasks with varying homophily) to strengthen the claim."}, "questions": {"value": "1. How sensitive is Relatron to bank size? With fewer tasks, does transfer degrade?\n2. Could homophily be extended to link-level tasks (e.g., recs), and does it correlate with higher-order passing (RelGNN)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zHH0e2qjDh", "forum": "59avbH4HnU", "replyto": "59avbH4HnU", "signatures": ["ICLR.cc/2026/Conference/Submission14654/Reviewer_7kcR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14654/Reviewer_7kcR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762469768036, "cdate": 1762469768036, "tmdate": 1762925024980, "mdate": 1762925024980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}