{"id": "Faz58I42h6", "number": 3219, "cdate": 1757378338180, "mdate": 1763725888124, "content": {"title": "EEG-RAGNet: Retrieval-Augmented Graph Structure Refinement for Clinical Seizure Diagnosis", "abstract": "Seizure diagnosis from EEG signals is a critical yet persistently challenging task, due to the complicated neural dynamics and the spurious connections in inter-channel modeling. While spatial-temporal graph neural networks (STGNNs) have advanced EEG brain network representation learning, the resulting graph structures suffer from low clinical plausibility and limited interpretability due to their purely data-driven nature. To this end, we introduce EEG-RAGNet, a retrieval-augmented graph refinement framework that incorporates external medical knowledge to calibrate noisy EEG graphs. We first construct a large-scale, domain-specific knowledge base derived from authoritative clinical guidelines. Leveraging large language models (LLMs), we extract structured biomedical entities and relations to form a textual knowledge graph (KG), which serves as external knowledge source of clinical priors. Our framework performs alignment-aware query construction by projecting STGNN-generated EEG node embeddings into the semantic space of KG. Semantic queries are then executed via FAISS-based similarity search over knowledge triplets to retrieve relation evidence. Each predicted edge is assigned a confidence score based on retrieved similarity, relation type, and source reliability, enabling us to prune medically implausible edges from the originally predicted graph. Extensive experiments on TUSZ benchmark demonstrate that EEG-RAGNet not only improves seizure detection accuracy but also enhances interpretability by grounding each prediction in clinically validated knowledge. This work provides the first unified framework that tightly couples brain dynamics with external medical expertise via retrieval-augmented reasoning, paving the way for knowledge-enhanced, explainable clinical diagnosis. The code is available at: https://anonymous.4open.science/r/EEG-RAGNet-63EE/.", "tldr": "", "keywords": ["Retrieval-Augmented Generation (RAG)", "Graph Learning", "Clinical Decision Making", "Large Language Models", "Graph Neural Networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4add48f307e77cf69e341cd6a49a8e9866bda197.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes EEG-RAGNet, a retrieval-augmented graph refinement framework for EEG-based seizure diagnosis. It integrates medical knowledge extracted via large language models (LLMs) into spatial-temporal graph neural networks (STGNNs) to prune clinically implausible connections in EEG brain graphs. Specifically, the method constructs a domain-specific knowledge base from epilepsy guidelines, builds a knowledge graph through entity and relation extraction, and uses semantic alignment and FAISS-based retrieval to score and refine STGNN-generated edges. Experiments on the TUSZ dataset show modest improvements in seizure detection metrics (F1 and AUROC) and claim better interpretability by grounding graphs in medical knowledge. However, while the framework sounds innovative, the technical novelty appears limited, it mainly combines existing concepts such as RAG, STGNNs, and clinical knowledge graphs without introducing a fundamentally new algorithmic contribution. The improvements are relatively small and demonstrated only on one dataset. Moreover, the reliance on LLM-generated medical triplets raises concerns about reliability, reproducibility, and scalability of the knowledge extraction process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The idea of combining retrieval-augmented generation with EEG graph refinement is somewhat novel but primarily extends existing RAG and STGNN concepts rather than defining a fundamentally new paradigm. The contribution lies more in integration than in theoretical innovation.\n\n2.The ablation study is helpful for tracing component contributions.\n\n3.The experiments are clearly reported."}, "weaknesses": {"value": "1.The paper claims the code is released, but the provided link is empty. Without access to implementation details, it is impossible to verify results or reproduce experiments, which weakens credibility and transparency.\n\n2.The framework mainly combines existing ideas, STGNNs, RAG, FAISS retrieval, without introducing a new algorithmic contribution. The innovation is largely compositional rather than methodological.\n\n3.The reported improvements in F1 and AUROC are small, raising doubts about practical impact. Statistical significance or efficiency trade-offs are not analyzed.\n\n4.Figure 1 lacks visual clarity and compactness. The layout appears crowded, with inconsistent labeling and spacing. A more concise and professional design would improve readability.\n\n5.Experiments are limited to a single dataset (TUSZ). Testing on other EEG corpora would strengthen claims of generalization.\n\n6.The interpretability claims rely on qualitative visualizations without quantitative evidence or expert feedback. \n\n7.The framework depends heavily on LLM-generated triplets, yet no analysis of extraction accuracy or noise impact is provided. This raises concerns about reliability and consistency.\n\n8.The addition of retrieval and alignment modules increases computational cost, but runtime and memory usage are not reported or compared with simpler baselines.\n\n9.Some methodological sections are verbose and repetitive, making the technical narrative less concise. Simplifying these parts would improve clarity.\n\n10.The paper omits a clear discussion of weaknesses, such as scalability, dependency on knowledge quality, or applicability in real clinical settings."}, "questions": {"value": "1.Could the authors provide a working and verified link to the full implementation, including data preprocessing and knowledge extraction scripts? Public code would greatly enhance reproducibility and confidence in the reported results.\n\n2.How does EEG-RAGNet differ conceptually from previous RAG-based graph reasoning methods (e.g., GraphRAG or knowledge-guided GNNs)? Clarifying the theoretical novelty beyond integration would help assess its contribution.\n\n3.Are the reported improvements statistically significant? Providing variance analysis, confidence intervals, or efficiency comparisons could justify the added complexity.\n\n4.Can the authors revise Figure 1 to make it more compact and visually consistent? A clearer diagram highlighting the data flow and key modules (e.g., SemAlignQuery, FAISS retrieval) would improve understanding.\n\n5.Do the authors plan to evaluate the model on other EEG datasets such as CHB-MIT or EPILEPSIAE? Multi-dataset results would better demonstrate generalizability and robustness.\n\n6.Could the authors include quantitative interpretability metrics (e.g., edge importance consistency, clinician agreement) or expert assessments to support qualitative findings?\n\n7.How reliable is the LLM-generated knowledge graph? Have the authors measured extraction precision or analyzed how noisy triplets affect graph refinement?\n\n8.What is the additional runtime and memory cost introduced by retrieval and alignment modules? Reporting efficiency comparisons would clarify practical feasibility.\n\n9.Would the authors consider condensing Sections 3.3–3.5 and providing more algorithmic pseudocode or key equations instead of extensive textual description?\n\n10.It would be valuable if the authors could explicitly discuss known limitations, such as scalability to higher-density EEG systems or dependency on knowledge quality, and outline possible mitigation strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nmuK1Ilpb2", "forum": "Faz58I42h6", "replyto": "Faz58I42h6", "signatures": ["ICLR.cc/2026/Conference/Submission3219/Reviewer_dvQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3219/Reviewer_dvQF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510985699, "cdate": 1761510985699, "tmdate": 1762916606420, "mdate": 1762916606420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EEG-RAGNet, a framework for refining EEG-derived brain connectivity graphs used for clinical seizure diagnosis by integrating external medical knowledge. The core idea is to enhance spatial-temporal GNN models (STGNNs) by retrieving evidence from a structured knowledge base derived from authoritative epilepsy clinical guidelines via LLM-driven entity and relation extraction. Retrieved knowledge is used to ground, grade, and prune edge predictions in learned brain graphs, resulting in improved accuracy, robustness, and interpretability as demonstrated on the TUSZ dataset. The paper includes extensive experiments, ablation studies, and qualitative visualization to support its claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of retrieval-augmented generation (RAG) for knowledge-guided graph refinement represents a creative advance in bridging data-driven graph learning and domain-specific medical expertise (Section 3). EEG-RAGNet's model-agnostic design allows it to serve as a plug-in to various GNN backbones.\n\n2. Table 1 and Figure 2 present convincing improvements in F1, AUROC, and recall for seizure detection across multiple strong baselines and EEG window sizes. These gains are consistent and statistically significant (as reflected in reported standard deviations).\n\n3. Table 2's ablation studies carefully disentangle the contributions of query construction, retrieval scoring, source reliability, and LLM selection, offering valuable insights for both practitioners and researchers."}, "weaknesses": {"value": "1. All experiments, visualizations, and ablation studies are conducted solely on the TUSZ benchmark. While TUSZ is a large and authoritative dataset, relying exclusively on it raises concerns about external validity, generalizability, and robustness. Would the same improvements translate to other public (or even private) clinical EEG datasets? The lack of cross-dataset validation diminishes the strength of the empirical claim.\n\n2. The main text lacks adequate discussion of how sensitive calibration outcomes are to key thresholds (e.g., knowledge-confidence cutoff $\\rho$, size/top-K of retrieved evidence), as well as the impact of varying LLM quality on knowledge base extraction.\n\n3. The framing hints at generality, but no evidence is provided that the RAGNet approach would benefit other clinical EEG graph learning or neurological diagnosis tasks.\n\n4. The knowledge base is meticulously constructed from five authoritative clinical guidelines. The current ablation study tests the LLMs used for extraction but not the sources themselves. An experiment comparing the full KB (all 5 sources) against a minimal KB (e.g., using only the ILAE guidelines ) would effectively quantify the benefit of integrating multiple, diverse knowledge sources. \n\n5. Section 3.3 introduces a crucial **shared projection head** ( $\\phi_{\\text {proj }}$ ) that maps STGNN-generated EEG channel embeddings into the KG semantic space. However, the paper completely omits details on how this projection head is trained. It is unclear whether it is trained end-to-end with the downstream seizure detection task, or if it requires a separate pre-training stage (e.g., using contrastive learning) to achieve meaningful semantic alignment between the disparate EEG and text modalities."}, "questions": {"value": "1. Do the authors have results or ongoing experiments on other EEG benchmarks or clinical datasets to validate generalizability? Could they discuss expected performance or known limitations?\n\n2. How sensitive is overall diagnostic performance to the pruning threshold $\\rho$? Has any systematic grid or sensitivity analysis been performed, and could the authors share concrete findings/plots?\n\n3. Could the authors share measurements on runtime, memory, and cost overhead introduced by the retrieval, LLM, and FAISS pipeline, especially in a prospective clinical setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6veX2fiJzh", "forum": "Faz58I42h6", "replyto": "Faz58I42h6", "signatures": ["ICLR.cc/2026/Conference/Submission3219/Reviewer_ijVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3219/Reviewer_ijVV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519561520, "cdate": 1761519561520, "tmdate": 1762916606047, "mdate": 1762916606047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, EEG-RAGNet is presented. This is a unique approach for diagnosing seizures from EEG signals. The authors clearly explain that spatiotemporal graph neural networks  are the most recent gold standard for handling such predictions, but the graphs are noisy, redundant, and lack alignment with domain knowledge. This approach generates spatial-temporal EEG brain networks but refines them using a RAG-based approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The authors do a nice job presenting background information.\n- The need for the incorporation of domain knowledge is clear and well-motivated. \n- Novel approach, have not seen anything similar.\n- The design of the approach is customized to fill a domain-specific niche, so the need for such a method is clear.\n- This paper demonstrates a great use case for LLMs in a biomedical domain. As the authors acknowledge, LLMs can hallucinate within scientific context. Using an LLM in a guided manner to refine a knowledge base, however, is a feasible and lower-risk task for such a high-risk domain (a medical one)."}, "weaknesses": {"value": "- The performance gains in Table 1 are consistent but often small. When taking into account the margin of error, the performance gains don’t seem particularly significant. The authors should maybe acknowledge this, at least briefly.\n- Figure 1 made me feel somewhat confused. The authors might consider putting more of an explanation in the caption or better labels each step on the figure. For example, what are the things on the bottom left?\n- “FAISS similarity search” is referred to throughout the paper, but there’s no explanation as to what this acronym means or what it is."}, "questions": {"value": "- What is the significance of using 12s and 60s for the clips? Is there medical relevance to those particular numbers?  \n- What is FAISS? Please provide an explanation in the paper.\n- Could the authors please provide more labelling in Figure 1 and reference it throughout the text where relevant parts of the figure are mentioned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "neUXD3FZtX", "forum": "Faz58I42h6", "replyto": "Faz58I42h6", "signatures": ["ICLR.cc/2026/Conference/Submission3219/Reviewer_DPwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3219/Reviewer_DPwk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923322012, "cdate": 1761923322012, "tmdate": 1762916605703, "mdate": 1762916605703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose EEG-RAGNet, a novel mechanism for constructing the adjacency matrix that models connections between EEG channels in a spatio-temporal graph.\nAn initial adjacency matrix is first learned by a spatio-temporal graph neural network (STGNN).\nThen, this graph is refined using a Retrieval-Augmented Generation (RAG) framework that incorporates external clinical knowledge to ensure physiologically plausible connections.\nFinally, a downstream model performs seizure classification using the refined graph structure.\nExperiments on the Temple University Hospital EEG Seizure Corpus (TUSZ) demonstrate that EEG-RAGNet significantly improves the performance of the downstream EEG classifier compared to baseline STGNN models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The two-stage process for finding the adjacency matrix is new. While STGNN is generally used to learn the graph structure (adjacency matrix), providing domain knowledge can help improve its prediction.\n\n2. Experimental results on one dataset demonstrate that the proposed EEG-RAGNet is a good wrapper around the downstream models."}, "weaknesses": {"value": "(1) The paper lacks a clearer, step-by-step explanation of the proposed methodology. Several important details remain ambiguous:\n\n(i) $c_t^i$ is aleardy an embedding of $x_t^i$. It is unclear why we need to project it again using $\\phi_\\text{proj}$?\n\n(ii) The text mentions retrieving the top-$M$ most semantically similar knowledge triplets $\\{\\tau_m\\}_{m=1}^M$. How are these triplets associated with the subgraph $G_t^{(i)}$? Is each subgraph linked to all $M$ triplets?\n\n(iii) In eq. 3, the computation of $\\mathbf{1}_{\\text{match}}$ is not clearly described.\n\n(iv) In the same eq., when multiple triplets are retrieved ($M > 1$), which specific triplet contributes to the computation for a given edge $e_{ij}^t$?\n\n\n(2) In the paper, only the embedding of the channels is used, but their semantic information about the channels is unknown to the model. Hence, it is unclear how the external knowledge corpus can effectively refine the adjacency structure learned by the STGNN\n\n(3) The entire model is dependent on the performance of GPT-4o in extracting useful information from the knowledge corpus. However, to the best of my knowledge, GPT-4o is a general-purpose LLM, and it may provide bad results for the domain-specific tasks. Did the authors evaluate the triples provided by the GPT-4o after the relation extraction?\n\n(4) Experimental setup needs to be clarified. What are the hyperparameters, and how are they tuned for all the models?\n\n(5) In the ablation studies, authors verified the importance of the components by removing it and comparing with the main result. However, it is not clear how the model looks without the components. For example, without SemAlignQuery, how is the similarity score (eq. 2) computed?\n\nMinor:\n\n(i) It was not clear from the methodology that there exists a downstream model for the classification task. From Problem 1, it is assumed that the proposed model does the job. It is okay to have a downstream model, but please clarify upfront to avoid confusion.\n\n(ii) $\\mathbf{X}_t$ (l. 110) denotes the features of all the nodes at time step $t$. Why the feature size of a node is $T$? Isn't $T$ the length of the sequence (Problem 1)? Further confusion arises in l.150-152."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hzr4y5pHl6", "forum": "Faz58I42h6", "replyto": "Faz58I42h6", "signatures": ["ICLR.cc/2026/Conference/Submission3219/Reviewer_KCx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3219/Reviewer_KCx2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762354410107, "cdate": 1762354410107, "tmdate": 1762916605440, "mdate": 1762916605440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}