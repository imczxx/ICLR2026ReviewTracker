{"id": "xz3hPommuG", "number": 21576, "cdate": 1758319234645, "mdate": 1759896914431, "content": {"title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport", "abstract": "Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences.\nWe propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution.\nWe evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.", "tldr": "", "keywords": ["Representation Similarity", "Representational Alignment"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b84c8cba287cbaec4fed9385d971c837a0540e9e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Traditional representational similarity methods align each layer of a network to its best match in another network, resulting in asymmetric results, lack of a global alignment score, and difficulty handling networks of varying depths. These limitations stem from ignoring the global activation structure and restricting the mapping to a rigid layer-to-layer correspondence. We propose the HOT framework, which jointly infers soft, globally consistent inter-layer couplings with neuron-level transmission schemes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "HOT uses a hierarchical framework to optimize the global coupling relationship between all layers while optimizing the matching at the neuron level, ensuring that each layer contributes to the final alignment, which is quite good."}, "weaknesses": {"value": "This method faces a huge computational cost problem, which limits its practicality on larger models. The authors clearly point out in the limitations section of the paper that the complexity of the algorithm increases sharply with the number of network layers and the number of neurons in each layer, and the overall computational complexity is about O(L^2n ^3logn)\n\nThis framework is essentially a descriptive tool, not an explanatory model. It is very effective in revealing how the representations of two systems (whether between models or between a model and the brain) align structurally, for example, which layer corresponds to which layer. However, it cannot explain why this alignment occurs. It cannot reveal the computational principles behind these similar representations, nor can it explain which training objectives, architectural constraints, or data characteristics lead to this cross-system convergence."}, "questions": {"value": "The paper demonstrates that rotation-invariant extension (HOT+R) is crucial for aligning visual Transformers, but computationally it has been limited in its application to large language models. Given that LLM representation spaces can also be subject to rotational ambiguity, can we develop a more computationally efficient approximation to achieve rotational alignment? For example, could we first perform a global Procrustes alignment of the entire network to correct the primary coordinate system, and then apply the less computationally expensive original HOT as a compromise between performance and efficiency?\n\nThis method uses a distance function (1−ρ) based on the Pearson correlation to calculate the internal transfer cost between neurons. While this choice is intuitive and computationally simple, it primarily captures linear relationships. Since the tuning relationships between neurons can be highly nonlinear, does using this linear metric limit the model's ability to discover more complex correspondences? Exploring other cost functions, such as distance defined by mutual information or training a small nonlinear predictor, might provide richer signals for optimal transfer and reveal more subtle neuron-level alignment patterns?\n\nThe current framework provides a static snapshot of the alignment of two trained models, but representation formation is a dynamic process that evolves throughout training. Can we extend the HOT framework to the temporal dimension to analyze the \"dynamics\" of representation alignment? For example, by treating checkpoints at different training stages as a third layer, can we use this approach to track how the layer-by-layer correspondences between two models evolve during training and identify key training points where their representations begin to converge or diverge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hPBExfa7qT", "forum": "xz3hPommuG", "replyto": "xz3hPommuG", "signatures": ["ICLR.cc/2026/Conference/Submission21576/Reviewer_K8P6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21576/Reviewer_K8P6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760454460276, "cdate": 1760454460276, "tmdate": 1762941841193, "mdate": 1762941841193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a way to compare what different neural networks (and even brains) represent across their layers, all at once, rather than matching layers and/or neurons one by one. Using a hierarchical optimal transport approach, it softly links each layer in one system to possibly several layers in another, producing a single overall alignment score and naturally handling models with different depths. Across vision models, language models, and brain imaging, the method matches or beats standard layer-by-layer matching and reveals clean, intuitive hierarchies, where early layers align with early layers and deeper with deeper, without hand crafting those rules."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Original extension of the optimal transport framework to entire hierarchies, integrating many-to-many soft matching across layers and neurons in a unified manner; the method can handle models of different depths/sizes without hand-crafted layer pairing.\n- Clearly written with conceptual claraity, and well explained.\n- Empirical evaluation spans three relevant, diverse application domains.\n- Candid discussion of limitations.\n- The problem is important yet not well understood, so the potential impact is high."}, "weaknesses": {"value": "As the limitations section notes, hierarchical OT is computationally costly, which is likely to limit the method's scalability. The somewhat narrow experimental scope leaves it unclear whether the method can effectively generalize to long-context settings, multimodal models, or tasks beyond vision and language.\n\nSome typos remain  (e.g., 'and and' in line 086)."}, "questions": {"value": "Points that require further attention: (i) Sensitivity to the choice of hyperparameters. (ii) The correspondence between the observed alignment and behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "l8qfGChjOX", "forum": "xz3hPommuG", "replyto": "xz3hPommuG", "signatures": ["ICLR.cc/2026/Conference/Submission21576/Reviewer_8PNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21576/Reviewer_8PNK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918032117, "cdate": 1761918032117, "tmdate": 1762941840871, "mdate": 1762941840871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hierarchical Optimal Transport for representational alignment. Instead of matching each source layer to one best target layer in isolation, HOT matches neurons inside each candidate layer pair and, on top of that, a soft layer-to-layer coupling that must be globally consistent. This gives a single network-level alignment score, handles depth mismatches, and, the authors argue, reveals smooth hierarchical correspondences that greedy pairwise methods miss. The method also has a rotation-invariant variant (HOT+R) via alternating OT and Procrustes. Experiments span LLM to LLM, vision to vision, and NSD brain to brain. Vanilla HOT typically matches or beats pairwise in LLMs and brain, while HOT+R strongly improves on vision transformers, where rotation invariance is known to matter."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This work addresses an important challenge that a lot of representational similarity comparison studies currently face: how do we match the layers in a principled manner? This problem has been overlooked, but requires attention. Therefore, the significance of this work is high. \n\n1. It is great that the authors introduce the algorithms for both rotation-sensitive and rotation-invariant similarity measures.\n\n2. It is simple to extend the proposed method to incorporate more hierarchies. As noted in the discussion section, this work opens up a possibility to perform interesting matching experiments that were not possible before. One additional application with the third hierarchical level would be simply matching models (which would, at the lower level, match the layers, and then, at an even lower level, match neurons).\n\n3. The authors provide reasonable baselines for proper comparisons\n\n4. The results are remarkably interpretable and biologically useful. The high clarity in the presentation helps.\n\n5. As far as I know, this work is original."}, "weaknesses": {"value": "As already noted in the limitations section of the paper, HOT/HOT+R is computationally demanding. This can be alleviated by subsampling the neurons, particularly for HOT+R. I am guessing that subsampling the neurons for HOT would lead to a dramatically bad matching score, however. It would be nice if the authors could experimentally test how sensitive the matching (HOT/HOT+R) is to subsampling of neurons."}, "questions": {"value": "Could the authors perform subsampling sensitivity analysis, as noted above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ybZotlimsa", "forum": "xz3hPommuG", "replyto": "xz3hPommuG", "signatures": ["ICLR.cc/2026/Conference/Submission21576/Reviewer_xsQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21576/Reviewer_xsQG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960355830, "cdate": 1761960355830, "tmdate": 1762941840550, "mdate": 1762941840550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Hierarchical Optimal Transport (HOT), a unified framework for aligning representations across neural networks or between brains and models. Unlike standard pairwise layer-matching methods that treat each layer independently, HOT jointly infers globally consistent layer-to-layer and neuron-to-neuron transport plans under marginal constraints. This allows the transport “mass” to be distributed across multiple target layers while preserving global balance, yielding a single, symmetric alignment score for the entire hierarchy.\nThe authors apply HOT to compare representations across large language models, vision transformers, and human visual cortex recordings. They evaluate alignment using two criteria: (1) the average correlation between reconstructed and ground-truth responses, and (2) whether the inferred transport plans preserve hierarchical correspondences, such that layers at similar depths in the source and target systems align with each other"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and timely problem in representational alignment—how to compare internal representations across networks and between brains and models when architectures differ in depth or structure. The proposed Hierarchical Optimal Transport (HOT) framework is an original and elegant reformulation of this problem, combining optimal transport with a hierarchical layer coupling that enforces global consistency. This formulation provides a principled way to compute both neuron-level and layer-level correspondences simultaneously, which is conceptually innovative compared to standard pairwise matching approaches.\nThe paper is generally well-written and situates the work in a broad and relevant context, bridging literature from both neuroscience and machine learning. The authors’ application of HOT across diverse domains—large language models, vision transformers, and human fMRI data—demonstrates methodological versatility and cross-domain relevance, which is a significant strength. The idea of producing a single, symmetric alignment score and a transport plan that naturally handles networks of different depths is both creative and potentially useful for future comparative studies of representation learning."}, "weaknesses": {"value": "On Soundness: \nThe central claim of the paper is that Hierarchical Optimal Transport (HOT) provides a superior measure of representational alignment, with alignment “quality” quantified by the correlation between reconstructed and ground-truth neural responses. However, the empirical evidence does not convincingly support this claim. Across the three experimental domains, improvements are limited to the large language model comparisons, while results on human visual cortex and vision model alignments show marginal or no advantage. In fact, for vision models (Table 3), HOT underperforms the Pairwise Best OT baseline, and only the rotation-augmented version (HOT + R) shows improvement—suggesting that the observed gains stem primarily from the rotation component rather than the hierarchical formulation itself. Furthermore, critical baselines such as RSA, CKA, and linear predictivity are missing, and no measures of variance or statistical error are reported in Tables 1–3. As a result, the empirical support for the claimed superiority of HOT remains weak and incomplete.\n\nOn Presentation: \nPresentation and clarity could be improved. A short introductory paragraph or visual summary explaining optimal transport theory in the context of representational alignment would also help orient readers unfamiliar with OT.\nFigures and tables could be clearer and more polished. Axis ticks and labels are sometimes difficult to read. In Figure 2, the axis dimensions should be unified and limits matched across panels to make visual comparison meaningful; the overall aesthetics of the figure could also be improved. The tables listing model or subject-pair reconstruction scores might be better visualized as heatmaps, allowing all pairwise results to be compared at a glance. Additionally, in some heatmaps, it is unclear why certain axis labels are highlighted.\nIn the Results section for Experiment 3, the statement that the lack of a diagonal structure in transport plans is “consistent with” lower reconstruction scores is not well justified—this seems inconsistent with Experiment 2, where low reconstruction scores still coincided with clear hierarchical (diagonal) alignment patterns. Finally, the claims that HOT provides more generalizable and interpretable results are not clearly substantiated in the text: the paper does not explain what “generalizable” means in this context, and interpretability is equated only with hierarchical structure. However, whether layers at similar depths should align is itself an open empirical question, not an inherent criterion for interpretability.\n\nOn Contribution: \nWhile the idea of combining hierarchical structure with optimal transport is interesting, the paper’s contribution is primarily incremental in practice. The empirical analyses do not convincingly demonstrate substantial improvements over existing OT-based methods, and the hierarchical formulation seems more interpretive than functionally necessary. Several important baselines (e.g., RSA, CKA, linear predictivity) and control analyses are missing, making it difficult to assess the true scope of the contribution. Moreover, claims about improved interpretability and generalizability remain qualitative and are not rigorously supported.\n\nExpanded comment: \nThe empirical results do not convincingly support the central claims. The paper argues that layer-wise alignment is too rigid and proposes a hierarchical coupling (HOT) that distributes representational mass across layers. However, if meaningful correspondences truly span multiple layers, it is unclear why one should impose any hierarchical structure at all. A global all-to-all mapping—such as linear predictivity or optimal transport across all neurons, regardless of layer boundaries—could directly capture these distributed relationships without the additional constraint.\n\nThe authors do not justify why the hierarchical formulation is preferable or demonstrate that it yields empirical benefits beyond interpretability. In its current form, the hierarchical constraint seems imposed rather than validated, and the layer-level mass-balancing constraint may even bias the method toward producing smooth, diagonal correspondences that are visually appealing but potentially artifactual. A comparison with a fully unconstrained baseline (e.g., all-to-all linear mapping or global OT) would be critical to determine whether HOT provides genuine improvements in alignment accuracy or interpretability.\nAdditionally, incorporating statistical measures of uncertainty and comparing against established baselines such as RSA, CKA, and layer-wise linear predictivity would substantially strengthen the empirical case. Other recent OT-based methods such as Soft Matching Distance (Khosla & Williams, 2024) or model stitching (Bansal et al., 2021) are directly relevant and cited; including these would clarify whether the hierarchical layer coupling truly adds value over existing OT approaches.\n\nFinally, there is ambiguity in the interpretation of results: in Experiment 3 (vision models), the authors interpret the lack of diagonal structure in transport maps as consistent with low reconstruction scores, but in Experiment 2 (visual cortex), low reconstruction scores still yielded diagonal correspondences. This inconsistency weakens the interpretation of what “hierarchical structure” signifies. A clearer, quantitative definition of hierarchy (e.g., correlation between layer indices or mutual information across depth) would improve interpretability. Moreover, the scalability of HOT remains untested in practice: although the paper notes a computational complexity of O(L2n3log⁡n), it would benefit from empirical runtime or memory comparisons with pairwise OT and linear predictivity to demonstrate its practical feasibility."}, "questions": {"value": "1.\tOn methodological necessity:\n The paper argues that layer-wise alignment is too rigid and introduces a hierarchical coupling (HOT) that distributes representational mass across layers. However, if meaningful correspondences genuinely span multiple layers, why impose any hierarchical structure at all? Wouldn’t a global all-to-all alignment—such as linear predictivity or optimal transport across all neurons, irrespective of layer boundaries—capture these distributed relationships more directly? Could the authors clarify why the hierarchical formulation is necessary or advantageous compared to such an unconstrained alternative? At present, the hierarchical constraint seems imposed for interpretability rather than empirically justified. Additionally, might the layer-level mass-balancing constraint bias the model toward producing smooth, diagonal correspondences that appear hierarchical but could be artifactual? A comparison with a fully unconstrained baseline would help clarify whether HOT provides genuine advantages beyond aesthetic interpretability.\n\n\n2.\tOn missing baselines and metrics:\n Why were standard representational alignment baselines such as RSA, CKA, Procrustes distance, and layer-wise linear predictivity not included in the evaluation? These are commonly used and directly comparable to the proposed method. Similarly, why were fully global baselines (e.g., all-to-all OT or global linear mapping) not tested, given that they would directly address whether HOT’s hierarchical constraint provides empirical benefits?\n\n\n3.\tOn model and domain coverage:\n Why not also test HOT on convolutional neural networks (CNNs)? CNNs have known architectural inductive biases, and prior work (e.g., Yamins et al., 2014, PNAS) has shown that their layer activations align well with the ventral visual stream hierarchy. Including CNNs would clarify whether HOT can recover known brain-model hierarchical correspondences, not just those in transformers.\n4.\tOn the biological data scope:\nWhy was the inferotemporal (IT) cortex excluded from the visual cortex analyses? IT is an established component of the ventral visual stream, and CNN layer–IT alignment using linear predictivity is well documented. Including IT data could reveal whether HOT captures higher-level cortical correspondences and provide a more complete test of its claims about hierarchical alignment.\n\n5.\tOn representational choices:\nFor the transformer models, token activations were averaged to form layer-wise representations. Could the authors clarify what representational information this averaging preserves, and whether concatenating token embeddings or using other aggregation strategies changes the inferred alignment patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "awwUvzpVM8", "forum": "xz3hPommuG", "replyto": "xz3hPommuG", "signatures": ["ICLR.cc/2026/Conference/Submission21576/Reviewer_1X33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21576/Reviewer_1X33"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061253888, "cdate": 1762061253888, "tmdate": 1762941840195, "mdate": 1762941840195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}