{"id": "M5R7wW9CBm", "number": 22255, "cdate": 1758328477310, "mdate": 1759896877425, "content": {"title": "Next-Token Prediction and Regret Minimization", "abstract": "We consider the question of how to employ next-token prediction algorithms in adversarial online decision making environments. Specifically, if we train a next-token prediction model on a distribution $\\mathcal{D}$ over sequences of opponent actions, when is it the case that the induced online decision making algorithm (by approximately best responding to the model's predictions) has low adversarial regret (i.e., when is $\\mathcal{D}$ a low-regret distribution)?\n\nFor unbounded context windows (where the prediction made by the model can depend on all the actions taken by the adversary thus far), we show that although not every distribution $\\mathcal{D}$ is a low-regret distribution, every distribution $\\mathcal{D}$ is exponentially close (in TV distance) to one low-regret distribution, and hence sublinear regret can always be achieved at negligible cost to the accuracy of the original next-token prediction model. In contrast to this, for bounded context windows (where the prediction made by the model can depend only on the past $w$ actions taken by the adversary), we show that there are some distributions $\\mathcal{D}$ of opponent play that are $\\Theta(1)$-far from any low-regret distribution $\\mathcal{D'}$ (even when $w = \\Omega(T)$ and such distributions exist). Finally, we complement these results by showing that the unbounded context robustification procedure can be implemented by layers of a standard transformer architecture, and provide empirical evidence that transformer models can be efficiently trained to represent these new low-regret distributions.", "tldr": "When do next-token prediction models have adversarial regret guarantees?", "keywords": ["online decision making", "regret", "next-token prediction", "transformer"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aeb1f77373409c1ef221aaf2381381df4c1d9da5.pdf", "supplementary_material": "/attachment/e92268e86ff6c135a7f63c332e0e5db64c19aa82.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies when and how a model trained for next‑token prediction can be used to make adversarially robust online decisions. The core idea is to define low‑regret distributions, in the sense that the next‑token models whose quantal best‑response induces sublinear adversarial regret when used as a policy. Authors show that (1) the Polya‑urn next‑token model becomes an exponential‑weights learner via quantal best response and thus achieves $1/\\sqrt{T}$ average regret bound; (2) a robustification procedure that, given any unbounded‑context model $M_0$ yields a low‑regret model $M$, both of which are close w.r.t. TV distance; (3) For bounded context $L$, robustification can be impossible but becomes feasible if the model can use a slightly longer window. (4) a transformer that solves next‑token prediction can be extended by a few layers to implement the robustified policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the studied setting looks interesting to me, and this paper asks a natural and practical question: when does next‑token prediction suffice for adversarial online decision making.\n\n- The robustification used in this paper is standard and easy-to-understand. I appriciate authors for explaning intuitions and motivations behind analysis and algorithms, which make this paper easy-to-follow.\n\n- Authors provide experiments to demonstrate that a tiny decoder can learn the robustified behavior and achieve vanishing regret on the stationary and drifting processes."}, "weaknesses": {"value": "- The technical contributions of this paper seem limited to me. For example, making argmax to softmax allows the learner to achieve a low-regret in adversarial setting is quite standard and well-known, while it is stated in the next-token prediction context. Most analysis directly follow previous work or standard ones.\n\n- The learner is too powerful in the studied model, which significantly simplies the problem. In this paper, the learner is assumed to know the utility function and the state chosen by the adversary is also revealed at the end of round. In this case, the learner can even calculate the external regret by herself, which thus enables Algorithm 1 to compare regret bounds. However, assuming the knowledge of these is strong in real-world scenarios since utility function should be learned during interaction."}, "questions": {"value": "- I am curious what if the learner does not know the utility function in advance, but only has a bandit feedback. Do your algorithms still work? If not, what's the main difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8J5uv4bOKN", "forum": "M5R7wW9CBm", "replyto": "M5R7wW9CBm", "signatures": ["ICLR.cc/2026/Conference/Submission22255/Reviewer_gunY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22255/Reviewer_gunY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538004549, "cdate": 1761538004549, "tmdate": 1762942138128, "mdate": 1762942138128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the full-information online learning problem, where the action set is finite and the losses are determined by an oblivious adversary. These losses are parameterized by the sequence $(\\theta_t)\\_t$. The paper's main idea is simulate the adversary by a next-token prediction algorithm that predicts $\\theta_t$ given $\\theta_1, \\theta_2, \\dots, \\theta_{t-1}$. A key contribution is the new regret upper bound $O(\\sqrt{T} \\log(TA))$ in Lemma 2.3. Other key contributions concern the effectiveness of learning to predict $\\theta_t$ with bounded/unbounded context length and transformers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The concept of modelling the adversary as a next-token prediction process is interesting.\n- While being sloppy in a few places, the paper is generally well-written.\n- The proof of Lemma 2.1 is correct. I did not verify other proofs."}, "weaknesses": {"value": "Major:\n- *Weak results:* The best upper bound of the proposed method is $O(log(AT)\\sqrt{T})$, which is significantly worse than the $O(\\sqrt{T\\log(A)})$ of simply running Hedge . This is despite the facts that 1) the proposed method has significantly higher computational cost to run to the algorithm for predicting $\\theta_t$ and 2) the proposed method only works for an adversary that chooses losses from a finite, discrete set. As such, both the key theoretical result and the practicality are limited.\n- *Unclear motivation:* the online learning setting in the paper considers a non-stochastic adversary, and yet the paper attempts to model how that adversary chooses the losses. The key assumption in adversarial online learning is that the learner makes no statistical assumption about the adversary. As such, there should be no ``pattern\" to be learned by a transformer (or any next-token prediction algorithm, for that matter)\n\nMinor: The writing, especially some mathematical terms, is sloppy at times.\n- Line 71: Theorem 2.3 -> Lemma 2.3\n- Line 126: $U(a, theta_t)$ -> $U(a_t, theta_t)$\n- Line 157: I don't understand what $\\pi_t = BR(...)$ means. The function $BR(...)$ returns an element of the set $A$, while $\\pi_t$ is a distribution over $A$. Did you mean a one-hot vector here?\n- Line 178: $U(a, \\mu)$ is not defined."}, "questions": {"value": "Please address the concerns and questions raised in the Weaknesses section. Additional comments and questions are below.\n\nComments:\n- The paper would be much stronger if it had a matching lower bound. I suspect that the current approach of using NTP to model the adversary will not be able to do much  better than the $O(\\sqrt{T}log(T))$.\n- Conceptually, for this line of work on the combination of transformers and online learning, I think that using transformers to learn an online algorithm (i.e Hedge) is much more interesting than trying to use transformers to learn a model of the adversary. \n\nQuestions:\n- The paper's sections on bounded context length seems interesting. I regret that I did not have more time to look more closely into its proofs. But just from the results alone, they don't seem particularly surprising, given that the down-stream task is adversarial online learning that requires the information of *all* previous rounds. In addition, a summary statistics of previous rounds can be efficiently encoded in a sum (just like Hedge). Can the authors comment on the novelty of these results with bounded context length: can they be derived independently outside of online learning context, for an arbitrary problem where the useful information is arbitrary far away from the current step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fzrdQE8DcW", "forum": "M5R7wW9CBm", "replyto": "M5R7wW9CBm", "signatures": ["ICLR.cc/2026/Conference/Submission22255/Reviewer_38CK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22255/Reviewer_38CK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932745963, "cdate": 1761932745963, "tmdate": 1762942137761, "mdate": 1762942137761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether next-token prediction models can be adapted for adversarial online decision-making tasks with low regret. The authors investigate robustification by modifying a trained model to achieve low adversarial regret while staying close to its original distribution. The paper's main finding is a contrast that this is always possible for models with an unbounded context, but generally impossible for models with a bounded context (like standard transformers). The authors then provide a workaround for the bounded case, prove the robust model can be implemented in a transformer, and validate their claims with illustrative experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Conceptually neat bridge from next-token prediction to no-regret decision making with clear, interpretable constructions\n- Mix of positive guarantees, a sharp bounded-context impossibility, and a feasible workaround that clarifies trade-offs\n- Transformer realizability plus empirical sketches make the theory more concrete and suggest a practical pathway"}, "weaknesses": {"value": "- Empirical validation is toy-level. The binary prediction game with Ber(1/3) to Ber(2/3) switch demonstrates the idea but doesn’t establish practicality for large-scale LLMs. Stronger baselines and realistic tasks would help.\n- The bounded-context workaround is somewhat unsatisfactory. Algorithm 2 gets $O(1/\\sqrt{\\Delta})$ regret by expanding context to $L+\\Delta$. This suggests the extra $\\Delta$ tokens act as memory for a standard bounded-memory regret algorithm; the robustification of the original bounded model feels less integrated than in the unbounded case.\n- Tension between theoretical TV-closeness and practical training. The goal is an exponentially TV-close(D), but training on D must produce behavior that differs on adversarial sequences. The experiments report small Next-Token TV (not exponential). Please clarify how theoretical indistinguishability maps to what’s actually enforced and measured in training."}, "questions": {"value": "Please address the concerns raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2zoS45yoMB", "forum": "M5R7wW9CBm", "replyto": "M5R7wW9CBm", "signatures": ["ICLR.cc/2026/Conference/Submission22255/Reviewer_UGWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22255/Reviewer_UGWM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963123529, "cdate": 1761963123529, "tmdate": 1762942137538, "mdate": 1762942137538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies training an LLM-style next-token predictor on sequences of opponent actions, when does predict to best respond give adversarial no-regret? It shows (1) there exist next-token models  such that a quantal best response to the model exactly simulates Hedge and gets low regret. (2) In the unbounded-context case, every model can be robustified to produces a new model that is low-regret while staying TV-close to the original model. (3) In the bounded-context case with size $L$, this robustification is in general impossible. (4) A transformer can implement the unbounded robustifier with $L+4$ layers and small width increase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes formulate a new question “Is next-token prediction enough to get no-regret?” It gives analysis showing that it is possible but only if we can modify the model on rare bad prefixes. The Polya-urn to QBR to Hedge reduction serves as the key technique for this problem.\n\n2. Robustification theorem is tight. In unbounded context we can turn any model into a low-regret one when the prediction quality is essentially preserved.\n\n3. The authors also provide impossibility for bounded context. Some $L$-bounded models can’t be made low-regret without leaving the class. This gives a nontrivial separation. \n\n4. The authors also show that we you can implement the robustifier with a standard transformer (+4 layers) makes the theory relevant to practice, and the experiments give some evidence to this idea."}, "weaknesses": {"value": "1. The paper studies full information setting where after each interaction, the state $\\theta_t$ chosen by the adversary is revealed to the learner. It is unclear for the more challenging bandit feedback case.\n\n2. The detect high regret and switch to Polya logic assumes access to the entire past to compute both the main model’s regret and the Polya/Hedge benchmark. That’s why it works in unbounded context. This is exactly what breaks in realistic LLMs that cap context. The paper proves this limitation, but it also means the positive result hinges on an assumption modern models routinely violate. \n\n3. The main claim ensure TV-close but the experiments themselves show that even two transformers trained on the same Bernoulli process can have large sequence-level TV, only next-token TV is small. That suggests the formal closeness guarantee may be much stricter than what current training can realize.\n\n4. The overall technical contribution is limited. It combines existing online-learning ideas, not in a new learning algorithm."}, "questions": {"value": "1.  Lemma 2.2 picks $U$ after seeing the model. Do your positive robustification results still hold if $U$ is fixed before seeing the model, or do we need to know $U$ to build the switch rule? \n\n2. Since full-sequence TV is very unforgiving, is it better to restate Theorem 3.1 in terms of expected next-token TV (your Table 2 metric) and prove that’s what’s actually preserved by Algorithm 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TpzCALLFXB", "forum": "M5R7wW9CBm", "replyto": "M5R7wW9CBm", "signatures": ["ICLR.cc/2026/Conference/Submission22255/Reviewer_1pVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22255/Reviewer_1pVk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987823795, "cdate": 1761987823795, "tmdate": 1762942137258, "mdate": 1762942137258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}