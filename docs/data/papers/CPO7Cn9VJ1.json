{"id": "CPO7Cn9VJ1", "number": 11429, "cdate": 1758198797015, "mdate": 1759897576215, "content": {"title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance", "abstract": "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model’s inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.", "tldr": "", "keywords": ["Large Language Model", "Reinforcement Learning", "Policy Optimization", "Multi-Agent Collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1463369a8fea18cb02a3921f0513438bd4d0a8a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Expert-Assisted Policy Optimization (EAPO), a reinforcement learning (RL) framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex tasks, particularly mathematical reasoning. EAPO augments the policy model's action space with a \"consult experts\" action, allowing the model to seek assistance from external experts (stronger LLMs) during training on-demand. This is optimized end-to-end with verifiable rewards, incorporating mechanisms like parallel expert querying, acceptance rate annealing, and consultation penalties to internalize knowledge and reduce reliance on experts over time. During evaluation, the policy reasons independently. Experiments on AIME 2024/2025 and AIMO 2025 show EAPO outperforming baselines like self-exploratory RL, expert-assisted workflows, and distillation methods, with gains of ~5 points on average accuracy and improved stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tEAPO creatively treats expert consultation as a learnable action within the RL policy, enabling adaptive, on-demand guidance during training. This addresses key RL challenges in reasoning tasks: sparse rewards and inefficient exploration.\n2.\tThe experiments are comprehensive, evaluating on challenging math benchmarks with Pass@32 and variance metrics. EAPO consistently outperforms baselines (e.g., +4.91% over self-exploratory RL on average). Ablations on expert parallelism (up to K=3), expert size (14B vs. 32B), and policy scaling (7B to 14B) provide insightful evidence of benefits.\n3.\tThe paper emphasizes reproducibility with detailed setups (e.g., models like DeepSeek-R1-Distill-Qwen-7B as policy, QwQ-32B as expert), hyperparameters (Table 4), prompts (Appendix C), and vLLM deployment (Appendix D)."}, "weaknesses": {"value": "1.\tWhile effective on AIME/AIMO, the evaluation is confined to mathematical tasks with verifiable rewards. Generalization to other reasoning domains (e.g., code, commonsense, or multi-modal) is not explored, despite claims of \"complex reasoning.\" The paper acknowledges this in limitations but lacks even preliminary cross-task experiments. This narrows the claimed impact on \"large reasoning models (LRMs).\"\n2.\tEAPO's gains hinge on high-quality experts (e.g., 32B > 14B in Table 2). If experts are noisy or biased, the policy might internalize errors, exacerbating reward hacking— a known RLHF issue cited but not deeply mitigated here. Ablations on noisy experts or homogeneous vs. heterogeneous pools could strengthen robustness claims. Additionally, the fixed expert pool (up to 3 replicas) limits diversity; integrating more varied experts (e.g., specialized for sub-tasks) could be discussed.\n3.\tTraining involves multi-turn interactions with experts, potentially increasing costs (e.g., parallel queries but up to T=10 turns). While annealing reduces this, no direct comparison of training efficiency (e.g., wall-time or FLOPs) vs. baselines is provided. For larger policies/experts, scalability might be challenging, especially since vLLM is used but not benchmarked against alternatives.\n4.\tThe mechanistic intuitions (Section 2.2) on alleviating sparse rewards and information gain are intuitive but lack formal proofs or bounds (e.g., on variance reduction or convergence). Related work connections (e.g., to HRL or self-distillation) are solid, but EAPO's novelty could be sharper by quantifying how it improves over them (e.g., via regret bounds)."}, "questions": {"value": "1.\tCould you provide results on non-math tasks (e.g., coding or planning) to assess generalization? The limitations mention cross-task studies—any preliminary data?\n2.\tHow sensitive is EAPO to expert quality? E.g., what if experts are weaker than the policy or provide conflicting advice?\n3.\tDid you experiment with dynamic expert pools (e.g., routing to specialized experts) or more than K=3 parallelism?\n4.\tThe reward function includes a 0.1 partial credit for correct format but wrong answer—how does this affect optimization vs. pure 0/1 rewards?\n5.\tIn Appendix E cases, how were examples selected? Are there failure modes where EAPO over-consults or under-internalizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2I9w1m8XTV", "forum": "CPO7Cn9VJ1", "replyto": "CPO7Cn9VJ1", "signatures": ["ICLR.cc/2026/Conference/Submission11429/Reviewer_Ka6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11429/Reviewer_Ka6W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408202915, "cdate": 1761408202915, "tmdate": 1762922543421, "mdate": 1762922543421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes a novel reinforcement learning framework called EAPO (Expert-Assisted Policy Optimization) that allows policy models to dynamically request the assistance of more powerful expert models during training, integrating expert feedback into their sampling trajectory. The empirical studies show that EAPO consistently outperforms strong baseline methods, including self-exploratory reinforcement learning, expert distillation, and multi-agent collaboration in math problems, demonstrating its effective knowledge transfer and inference capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Formulating expert consultation as an optimizable and annealing action in reinforcement learning is a novel idea. It goes beyond static workflows and distillation.\n2. Experiments are comprehensive. This paper compares extensive baseline methods including self-exploratory methods and distillation methods on three complex reasoning benchmarks. Results are statistically and qualitatively convincing.\n3. The presentation of this paper is clear, with intuitive explanations of complex mechanisms."}, "weaknesses": {"value": "1. EAPO mainly aims to use powerful models for inference in RL processes. In this case, why not directly use large models for inference, which would consume fewer resources.\n2. EAPO provides access to significantly stronger expert models. It’s unclear how EAPO performs when the expert is only marginally better or when experts are noisy. An ablation on expert quality would strengthen robustness claims.\n3. Parallel expert querying during training increases inference and memory overhead. The paper lacks an analysis of training-time cost against gain trade-offs. The article lacks an analysis of the training process time, such as the inference time for each training step.\n4. While promising, all experiments are in mathematical reasoning. Demonstrating EAPO on non-math domains (e.g., code, science) would bolster its generality.\n5. EAPO provides a form of intermediate guidance, but the paper does not discuss how this compares to the more established concept of process supervision. (e.g., PRMs)"}, "questions": {"value": "1. How does EAPO performance change if the expert model is weaker, for example, the same size as policy or only slightly better? Could EAPO still learn useful strategies for exploration?\n2. Do the authors test EAPO on non-math tasks such as HumanEval? Does EAPO generalize to other structured reasoning domains?\n3. How does EAPO compare to methods that use learned reward models trained on step-level annotations (e.g., PRMs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ct3RdJGcYk", "forum": "CPO7Cn9VJ1", "replyto": "CPO7Cn9VJ1", "signatures": ["ICLR.cc/2026/Conference/Submission11429/Reviewer_oegD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11429/Reviewer_oegD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877656308, "cdate": 1761877656308, "tmdate": 1762922542679, "mdate": 1762922542679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework called Expert-Assisted Policy Optimization (EAPO), which integrates external expert models (e.g., larger LLMs) during policy training. The goal is to leverage expert feedback to provide richer rewards, accelerate convergence, and improve learning stability compared to standard RL optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is well written and motivation is clearly stated.\n2. Experiments support the effectiveness of their proposed RL framework."}, "weaknesses": {"value": "1. The \"expert\" idea is not novel. Many prior works have explored using larger or teacher models to guide smaller student models. Similar approaches have also been studied, such as using supervised data (human or model-generated) to improve RL. This paper lacks a clear discussion and comparison with those related methods.\n2. The paper does not explicitly state: how the framework decides whether experts is required at time t; how to determine which instances are simple, hard, or complex at the beginning of training\n3. Notation typos. The subscript in Line 121-123 is not consistent with Line120; Line 124 should use index t-1 based on the equation in Line 126; the meaning of q_t in Line 235 is unclear."}, "questions": {"value": "1. What is the Expert-Assisted Workflow method mentioned, and how does it differ from your proposed EAPO?\n2. Have you considered the compute cost of using experts? Calling larger models can be expensive, and standard RL might generate more trajectories within the same compute budget.\n3. It seems the main benefit of using experts is to make the initial reward signals denser so that more meaningful trajectories are used for model updates. Could you show how the initial rewards change after introducing experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ExDQEHLUtT", "forum": "CPO7Cn9VJ1", "replyto": "CPO7Cn9VJ1", "signatures": ["ICLR.cc/2026/Conference/Submission11429/Reviewer_EoSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11429/Reviewer_EoSa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762922447775, "cdate": 1762922447775, "tmdate": 1762922542142, "mdate": 1762922542142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Expert-Assisted Policy Optimization (EAPO), a framework that integrates multi-turn interactions with external experts throughout the training process. Unlike prior methods where policy models reason in isolation, EAPO enables the policy to adaptively determine when and how to consult experts, thus obtaining richer reward signals and more reliable reasoning trajectories. Experiments on mathematical reasoning benchmarks like AIME 2024, AIME 2025, and AIMO 2025 demonstrate that EAPO outperforms baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow, and the figures are clear and helpful.\n\n2. The paper’s ablation studies, focusing on expert parallelism and expert model size, validate EAPO’s key designs: parallel expert queries outperform sequential ones and self-exploratory RL, while larger experts (e.g., QwQ-32B) outperform smaller ones."}, "weaknesses": {"value": "1. This paper only validates EAPO’s performance on mathematical reasoning benchmarks, including AIME and AIMO, while excluding other domains’ datasets like HLE and GPQA.\n\n2. The expert model-related overhead is relatively high. EAPO adopts a multi-expert parallel reasoning mechanism to improve information coverage and relies on large-scale expert models to ensure the quality of guidance, which undoubtedly increases the consumption of computing resources during model training and deployment, limiting its practicality in resource-constrained scenarios.\n\n3. EAPO is highly dependent on the capabilities of expert models. Experiments show that using smaller-scale expert models will lead to a significant decline in EAPO's performance. Moreover, when the expert is just a single model or is identical to the policy model, it may be difficult to effectively assist policy model optimization due to the lack of diverse perspectives.\n\n4. In terms of feedback mechanism design, EAPO is similar in idea to related works that rely on reward models to provide feedback. It does not form a significant difference in core feedback logic or framework design, resulting in insufficient novelty compared with existing reinforcement learning-assisted optimization methods.\n\n5. Minor issues in Section 2: In Line 122, how to generate the expert assistance $o_i$\nis unclear, in Line 126, $\\pi_\\theta(\\tau_t, \\alpha_t | H_{t-1}) = \\pi_\\theta^\\tau(\\tau_t | H_{t-1}) \\cdot \\pi_\\theta^\\alpha(\\alpha_t | H_{t-1}, \\tau_t)$ is problematic, since the formula is given $H_{t-1}$ instead of $H_t$, and in Line235, the $q_i$ is undefined."}, "questions": {"value": "1. How does EAPO perform on other domains’ datasets like HLE and GPQA?\n\n2. How robust is EAPO to low-quality or noisy expert models? For example, when experts provide incorrect or meaningless guidance, does the framework’s performance degrade gracefully? And how could you solve this problem?\n\n3. How dependent is EAPO on the expert models? What happens if the experts are fewer, weaker, or partially misaligned? Specifically, if the expert model is identical to the policy model, how do performance and stability change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xQnNPRvEtX", "forum": "CPO7Cn9VJ1", "replyto": "CPO7Cn9VJ1", "signatures": ["ICLR.cc/2026/Conference/Submission11429/Reviewer_CHBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11429/Reviewer_CHBo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11429/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762959335533, "cdate": 1762959335533, "tmdate": 1762959335533, "mdate": 1762959335533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}