{"id": "YLk1awtmAS", "number": 22697, "cdate": 1758334546530, "mdate": 1759896851880, "content": {"title": "On Training Mixture-of-Experts: A Social Choice Perspective", "abstract": "Mixture-of-Experts (MoE) training is notoriously difficult, caught between fostering expert specialization and ensuring balanced computation. We introduce a novel perspective to recast MoE training through the lens of social choice. From this perspective, we attribute the training difficulty to Arrow's Impossibility Theorem. Inspired by principles from social choice theory, we then present a novel approach, Regulated Mixture-of-Experts (RMoE), to alleviate these training difficulties. RMoE consists of a phased curriculum for the load-balancing loss and stateful fusion for expert weighting. Extensive experiments on the GLUE and DomainBed benchmarks show that RMoE significantly outperforms standard MoE and dynamic routing baselines. Our work provides a new lens for understanding MoE training and offers a practical framework for building more stable and powerful models. Our code is available at https://anonymous.4open.science/r/R-MoE-E3DC.", "tldr": "Attributing the MoE training difficulty to Arrow’s Impossibility Theorem through a social choice perspective, we introduce RMoE, a framework featuring phased curriculum and stateful fusion.", "keywords": ["Mixture-of-Experts", "Social Choice Theory", "Arrow's Impossibility Theorem", "Curriculum Learning", "Expert Specialization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb43c615446f42cdfc577f7d5b2027629e3df8da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes MoE training through the lens of social choice theory, arguing that the difficulty in balancing task performance and load balancing can be attributed to Arrow's Impossibility Theorem. The authors propose RMoE, which combines a phased curriculum for the load-balancing loss weight and stateful fusion using momentum for expert weighting. They demonstrate improvements over baselines on GLUE and domain generalization benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts an interesting interdisciplinary connection between MoE training and social choice theory, which offers new insights into understanding routing collapse. \n\n2. The experimental results show consistent improvements over baselines across multiple benchmarks. \n\n3.  the paper includes extensive ablation studies and analysis of expert specialization patterns."}, "weaknesses": {"value": "1. The social choice framing feels more like a loose analogy than a rigorous theoretical foundation. While the paper invoke Arrow's Impossibility Theorem, they don't provide a formal proof of how it applies to MoE training - the mapping between voting systems and routing is imprecise. The actual solutions proposed  are fairly standard techniques that don't really emerge from social choice principles. For instance, curriculum-based approaches for MoE have been explored in [1] and progressive training strategies in [2].\n\n2. The technical contributions are incremental. The phased curriculum is essentially scheduling the auxiliary loss weight, which has been explored in various forms (e.g., gradual unfreezing, warm-up schedules). The stateful fusion mechanism is just applying EMA to routing scores, similar to momentum-based methods in optimization. Neither innovation requires the social choice perspective to motivate or understand.\n\n3. The experimental setup has significant limitations. All experiments use relatively small models, making it unclear if the approach scales to production-scale MoE models like Mixtral-8x7B [3] where routing collapse is more problematic. \n\n[1] Lewis M, Bhosale S, Dettmers T, et al. Base layers: Simplifying training of large, sparse models[C]//International Conference on Machine Learning. PMLR, 2021: 6265-6274.\n[2] Zhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114.\n[3] Jiang A Q, Sablayrolles A, Roux A, et al. Mixtral of experts[J]. arXiv preprint arXiv:2401.04088, 2024."}, "questions": {"value": "1. Can you provide a formal proof showing how Arrow's theorem applies to MoE routing? \n\n2. How does RMoE perform on larger-scale MoE models like [1]? \n\n[1] Jiang A Q, Sablayrolles A, Roux A, et al. Mixtral of experts[J]. arXiv preprint arXiv:2401.04088, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W7Np4qRycU", "forum": "YLk1awtmAS", "replyto": "YLk1awtmAS", "signatures": ["ICLR.cc/2026/Conference/Submission22697/Reviewer_vmUk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22697/Reviewer_vmUk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722712449, "cdate": 1761722712449, "tmdate": 1762942341894, "mdate": 1762942341894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper connects the training of MoE to the social choice theory, where there is a conflict between task efficiency (utilitarian) and load balancing (fairness). It proposes two strategies that help to train MoEs better: a) phased curriculum where the coefficient of load balancing loss is decayed linearly b) momentum-based weight fusion of expert outputs. It is shown that both strategies help to achieve better performance and specialization of experts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple to implement and shows better results compared to baseline like DynMoE \n- The problem of MoE routing is relevant to the community, especially for training at scale. \n- Experiments are conducted in both language and vision domains"}, "weaknesses": {"value": "- The connection to social choice theory seems weaker. Impossibility theorem mentioned in abstract is not elaborated elsewhere in the paper and it is harder to make connection to MoE training \n- Second-best seems like a theoretical concept that applies when all objectives cannot be satisfied. It feels like inspiration rather than a technical solution for a given problem. Could you tell why it is relevant here? \n-  Are these results using multitask training from GLUE tasks? The method would be convincing if trained in multitask fashion. One baseline to beat here would be training a single expert on all data. \n- The proposed method should be shown at scale for it to be practical. Eg. pretraining on 1B tokens with model size of 1B or lower. If the focus is on downstream tasks, then approaches where experts are parameter efficient modules (https://arxiv.org/abs/2306.03745) should be compared. \n-   Add more baselines like SoftMoE (https://arxiv.org/abs/2308.00951), DeepSeek MoE (https://arxiv.org/pdf/2401.06066), auxiliary free load balancing (https://arxiv.org/pdf/2408.15664v1) to make the work more comprehensive."}, "questions": {"value": "- How is the moving average of routing scores done? Is it across the same tokenID appearing through training? Or are you averaging as you move across sequence length? \n- How do you handle moving averages at inference time? The text in lines 314 and 315 is incomplete. \n- Figure 3 doesn’t provide information about stable convergence of the proposed method over the baseline. Could you elaborate more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ehhW2HFIgt", "forum": "YLk1awtmAS", "replyto": "YLk1awtmAS", "signatures": ["ICLR.cc/2026/Conference/Submission22697/Reviewer_8fPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22697/Reviewer_8fPZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862258084, "cdate": 1761862258084, "tmdate": 1762942341487, "mdate": 1762942341487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper redefines the routing problem in MoE as a social choice problem: the input token is regarded as the \"agent\", the expert as the \"candidate\", and the router as a social welfare function that aggregates preferences. The author points out that the trade-off between task loss and load balancing loss in MoE training is similar to the conflict between efficiency and fairness in social choice, and borrows Arrow's impossibility theorem to explain the theoretical root cause of training difficulties. Based on this, they proposed the RMoE framework, which consists of two mechanisms: phased courses (gradually increasing the weight of load balancing losses) and state fusion (using EMA to smooth the expert fusion weights). Experiments were conducted on GLUE and DomainBed, demonstrating that RMoE outperformed multiple baseline"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Providing code for reproducing the experiments is commendable.\n\n2.  Linking the routing problem of Mixture-of-Experts (MoE) with social choice theory offers a brand-new theoretical perspective for understanding routing crashes and training instability.\n\n3.  The two proposed mechanisms (phased training and state fusion) are elaborated in detail."}, "weaknesses": {"value": "1. The experiments were based on BERT-base and ViT-Small, and their scalability was not verified on larger-scale models such as MoE with undreds of billions of parameters.\n\n2. The essence of phased learning is to dynamically adjust the loss weights, which has been widely applied in multi-objective optimization and phased learning. EMA smoothing in state fusion is also very common in time series models."}, "questions": {"value": "1. Could you provide a rigorous argument for directly mapping the MoE training problem to the conditions of Arrow's impossibility theorem (such as independence and non - dictatorship)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2yNvoxSq3R", "forum": "YLk1awtmAS", "replyto": "YLk1awtmAS", "signatures": ["ICLR.cc/2026/Conference/Submission22697/Reviewer_7ovw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22697/Reviewer_7ovw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898976040, "cdate": 1761898976040, "tmdate": 1762942341128, "mdate": 1762942341128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Regulated Mixture-of-Experts (RMoE), a framework for improving the training stability of MoE models. The authors frame the trade-off between task performance and load balancing as a social choice problem, attributing the difficulty to Arrow's Impossibility Theorem. They propose two main components: a \"Phased Curriculum\" for scheduling the load-balancing loss and Stateful Fusion which uses an EMA to smooth expert weights. Experiments on GLUE and DomainBed show improvements over baseline MoE and DynMoE models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The high-level perspective of connecting MoE training to social choice theory is creative and provides an interesting narrative."}, "weaknesses": {"value": "Limited Technical Novelty: The core technical contributions are essentially reinterpretations of well-established techniques.\n\nThe \"Phased Curriculum\" is a simple linear annealing schedule for an auxiliary loss weight, a common practice in machine learning (e.g., β-annealing in VAEs).\n\nStateful Fusion is an application of Exponential Moving Average (EMA) to introduce momentum and stabilize training, a concept that is neither new nor unique to this work. Its conceptual overlap with existing momentum-based methods is significant.\n\nSuperficial Theoretical Grounding: The connection to Arrow's Impossibility Theorem is presented as a high-level analogy rather than a rigorous, formal framework. The paper fails to formally map the components of MoE training (tokens, experts, router) to the axioms required by the theorem (e.g., non-dictatorship, independence of irrelevant alternatives). Consequently, the theoretical framing feels like a post-hoc justification for heuristic design choices, rather than a principled foundation that guides the method's development."}, "questions": {"value": "Q1 Can you better articulate the novelty of the proposed mechanisms beyond being applications of loss scheduling and EMA smoothing? What distinguishes them fundamentally from prior work using these concepts?\n\nQ2 Can you provide a more formal proof of how MoE training violates the axioms of Arrow's theorem? Following that, how do the specific designs of Phased Curriculum and Stateful Fusion mathematically address or relax these axioms to \"escape\" the impossibility result?\n\nQ3 Could you justify the omission of critical baselines like \"Loss-Free Balancing\"? Furthermore, can you provide a complexity analysis of RMoE to demonstrate its viability for models with thousands of experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kYz6xi8Lad", "forum": "YLk1awtmAS", "replyto": "YLk1awtmAS", "signatures": ["ICLR.cc/2026/Conference/Submission22697/Reviewer_wwgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22697/Reviewer_wwgW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991480289, "cdate": 1761991480289, "tmdate": 1762942340828, "mdate": 1762942340828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study designed a two-stage mechanism to regulate the expert selection problem in MoE, improving model performance, and attempted to explain it using social choice theory, which is quite an interesting approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work tries to capture the dynamic process in global and local optimization in MoE expert selection by considering the two stage mechanism. Such two process sounds common in other domains, especially in optimization field. I think this idea is worthy of further study."}, "weaknesses": {"value": "1, The author's discussion in the text is misleading. From the perspective of our sequential social choice framework, the MoE training objective represents a classic social dilemma, characterised by a conflict between two competing social welfare objectives: utilitarianism (efficiency) and egalitarianism (fairness) (Sen, 1977; 1986). The author has inserted their own viewpoint into the references, misleading readers into thinking that it is the original author's opinion, which is inappropriate.\n\n2, The proposed controlled mixture-of-experts RMoE has a certain novelty, but the working mechanism of its training method needs to be further clarified. Perhaps this process is closer to simulated annealing, for example, the interaction mechanism between the first stage Phased Curriculum and the second stage Stateful Fusion in the training process.\n\n3, The improvement in experimental performance is limited. The performance improvement compared to the baseline model is relatively limited."}, "questions": {"value": "I personally strongly disagree with using social choice theory to explain the expert selection problem in MoE, because these are two completely different issues with clearly distinct mechanisms. In social sciences, we need to emphasise efficiency and fairness, as fairness can affect efficiency. However, in MoE, if efficiency can be guaranteed, that is, if performance can be ensured, why emphasise fairness? Could the author explain how fairness in MoE affects efficiency? To be more specific, if fairness can guarantee efficiency, then simply distributing tokens equally to each expert would suffice, so why don't we do this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gULnLf7wAD", "forum": "YLk1awtmAS", "replyto": "YLk1awtmAS", "signatures": ["ICLR.cc/2026/Conference/Submission22697/Reviewer_bYYm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22697/Reviewer_bYYm"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136337655, "cdate": 1762136337655, "tmdate": 1762942340585, "mdate": 1762942340585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}