{"id": "3axBqFqDgk", "number": 3722, "cdate": 1757505980835, "mdate": 1759898073296, "content": {"title": "It Takes Two: Your GRPO Is Secretly DPO", "abstract": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). \nIt is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead.\nIn this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, \nwhich reveals a fundamental connection to Direct Preference Optimization (DPO). \nMotivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO)—a configuration previously deemed infeasible. \nWe provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, \ndespite using only $1/8$ of the rollouts and reducing training time by over $70\\\\%$.", "tldr": "GRPO with 2 rollouts", "keywords": ["Reinforcement Learning", "Large Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f5fc9eb73f1d1057a40acb4d7d2346a2e49c44e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reinterprets Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO) as a form of contrastive learning. Based on this insight, the authors propose 2-GRPO, an extremely lightweight variant that uses only two rollouts per prompt instead of large groups. , They show the 2-GRPO is able to preserve gradient estimation and the same optimization direction as standard GRPO. Through theoretical analysis, they prove that 2-GRPO implicitly normalizes advantages and maintains stability, and empirically show across multiple reasoning benchmarks that it matches the performance of 16-GRPO while largely cutting training time and computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work theoretically and empirically showed that 2-GRPO can be compatible with large GRPO while reduce the rollout."}, "weaknesses": {"value": "Although 2-GRPO achieves comparable performance to standard GRPO with substantially lower computational cost, it also presents several limitations. From Table 1 and figure 1,2, the overall performance is dropped. Especially, AIME drops up to 9.56% and AMC dropped up to 4.85%."}, "questions": {"value": "1. Is there any way to reduce the drop in performance? Could we increase the number of epochs for 2-GRPO to match the performance of standard GRPO? If so, approximately how many additional epochs would be required?\n\n2. How can the theory be generalized to continuous reward settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NKDzJBlIGj", "forum": "3axBqFqDgk", "replyto": "3axBqFqDgk", "signatures": ["ICLR.cc/2026/Conference/Submission3722/Reviewer_G2em"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3722/Reviewer_G2em"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761518640180, "cdate": 1761518640180, "tmdate": 1762916947228, "mdate": 1762916947228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes GRPO with group size of 2 and finds that it is able to achieve similar performance to group size of 16 on math datasets while using 70% wall clock time. Furthermore, a contrastive learning framework is used to analyze GRPO's advantage estimates under differing group sizes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- A theoretical analysis under a constrastive learning framework for 2-GRPO is provided\n- Established math datasets and benchmarks are used for post-training and evaluation"}, "weaknesses": {"value": "- My primary concern with the work is that while a 70% wall clock time improvement is significant, it appears that experimentally all that was done was changing the group size to 2\n- I would need to see a rather elaborate experiments section to verify that G=2 is equally performant and significantly faster on a wider variety of post training tasks to fully believe the claim"}, "questions": {"value": "- Please address weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "90bEDNldUt", "forum": "3axBqFqDgk", "replyto": "3axBqFqDgk", "signatures": ["ICLR.cc/2026/Conference/Submission3722/Reviewer_W4J6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3722/Reviewer_W4J6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163215694, "cdate": 1762163215694, "tmdate": 1762916946610, "mdate": 1762916946610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a variant of the GRPO algorithm which takes group size to be 2. The paper proved that similar to DPO, GRPO with group size 2 can be considered as a contrastive loss. The paper then derives a few property of 2-GRPO, and perform experiments on standard math reasoning tasks and demonstrates the gain in efficiency with small performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The observation that GRPO’s within-group normalization induces a signed “positive vs negative” split is a useful lens and does connect to generic contrastive forms.\n\n2. The compute focus is valid given rollout cost dominates; the table shows large wall-clock savings."}, "weaknesses": {"value": "1. The paper proves that both objectives admit gradients of a contrastive form (Def. 3.1 / Props. 3.2–3.3), but stops there. There is no mapping of corresponding terms in DPO to GRPO’s objective. This is just a nominal equivalence.\n\n2. The assumption of no clipping is rather strong. I understand it's for the convenience of the derivation, but in reality it plays an important role on shaping the gradient distribution. \n\n3. Honestly most of the observations / derivations in this paper is either quite well known or elementary (e.g., properties of Bernoulli random variables).\n\n4. The results actually show that taking group size 4 induces both huge save in training time and no performance gap w.r.t. group size 8 or 16, that makes the choice of group size 2 less ideal. \n\n5. This paper is less principled than many previous papers with adaptive group size. E.g., [1]\n\n[1] Zhang, Ruiqi, et al. \"SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning.\" arXiv preprint arXiv:2506.09016 (2025)."}, "questions": {"value": "With group size 2, what is the percentage of prompts that have no gradients? How does this change along training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6RRKNA9OtO", "forum": "3axBqFqDgk", "replyto": "3axBqFqDgk", "signatures": ["ICLR.cc/2026/Conference/Submission3722/Reviewer_ZREz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3722/Reviewer_ZREz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762316044439, "cdate": 1762316044439, "tmdate": 1762916946133, "mdate": 1762916946133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors perform theoretical analysis and experiments in support of a variant of GRPO with group size 2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) A mixture of theory and practice."}, "weaknesses": {"value": "(-) I think you're missing a log in Eq. 2. Similarly, the expectation in Eq. 5 clashes with the finite-sample group.\n\n(-) On line 272, I think you mean variance not bias. Also, I don't think you meant proportional on line 276. More broadly, it's not clear to me why Prop 4.1 implies that 2-GRPO performs normalization. I think there's also an extra nabla on line 286.\n\n(-) The rationale for increasing Q to compensate for lower G in Sec. 4.2 seems to rely on a strong unstated assumption: equal variance across all prompts. I believe the same is true for Sec. 4.3 -- let me know if I've misunderstood."}, "questions": {"value": "1. Why is the gradient zero outside of the clipping range as you write above Eq. 8?\n\n2. Can you explain the justification for Prop. 3.2? It also seems like you might be missing a log in Eq. 10?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u2KuNivuao", "forum": "3axBqFqDgk", "replyto": "3axBqFqDgk", "signatures": ["ICLR.cc/2026/Conference/Submission3722/Reviewer_Ako8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3722/Reviewer_Ako8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762324843155, "cdate": 1762324843155, "tmdate": 1762916945871, "mdate": 1762916945871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}