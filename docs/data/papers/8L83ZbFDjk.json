{"id": "8L83ZbFDjk", "number": 13936, "cdate": 1758225504472, "mdate": 1763701114774, "content": {"title": "Conformal Prediction for Long-Tailed Classification", "abstract": "Many real-world classification problems, such as plant identification, have extremely long-tailed class distributions. In order for prediction sets to be useful in such settings, they should (i) provide good class-conditional coverage, ensuring that rare classes are not systematically omitted from the prediction sets, and (ii) be a reasonable size, allowing users to easily verify candidate labels. Unfortunately, existing conformal prediction methods, when applied to the long-tailed setting, force practitioners to make a binary choice between small sets with poor class-conditional coverage or sets with very good class-conditional coverage but that are extremely large. We propose methods with guaranteed marginal coverage that smoothly trade off between set size and class-conditional coverage. First, we introduce a new conformal score function, coined prevalence-adjusted softmax, that targets macro-coverage, a relaxed notion of class-conditional coverage. Second, we propose a label-weighted conformal prediction method that allows us to interpolate between marginal and class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet-300K  and iNaturalist-2018, two long-tailed image datasets with 1,081 and 8,142 classes, respectively.", "tldr": "", "keywords": ["conformal prediction", "uncertainty quantification", "long tail", "class imbalance", "fine-grained image classification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b6528486a38cd2a93dbcb014beef4f049e02595.pdf", "supplementary_material": "/attachment/c9865d95e30e70f9de0680df23ecebc0ec64edaa.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of constructing useful prediction sets using conformal prediction (CP) in long-tailed classification settings, such as plant identification, where rare classes are underrepresented. Standard CP achieves marginal coverage but often fails to cover rare classes adequately, while class-conditional CP methods produce excessively large sets. The authors propose two approaches: (1) a new conformal score function called prevalence-adjusted softmax (PAS) and its weighted variant (WPAS) to target macro-coverage (the unweighted average of class-conditional coverages), derived from oracle-optimal sets that balance set size and coverage; (2) INTERP-Q, a simple interpolation between standard and classwise CP quantiles to trade off set size and class-conditional coverage. They evaluate these methods on long-tailed image datasets, demonstrating improved trade-offs compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces novel conformal score functions (PAS and WPAS) specifically tailored for macro-coverage in long-tailed settings, grounded in theoretical derivations of oracle-optimal prediction sets. This extends prior work on CP beyond marginal or basic class-conditional guarantees, addressing a practical gap in applications like biodiversity monitoring where rare classes are critical. The paper is clear, with well-defined notation, algorithms, and metrics, making the contributions accessible."}, "weaknesses": {"value": "The interpolation in INTERP-Q is simple but lacks deeper analysis of why linear weighting works well, and why it can only guarantee a conservative coverage bound ($1-2\\alpha$) while being close to $1-\\alpha$ in practice. \n\nAdditionally, comparisons could include more recent long-tail methods (e.g., Ding et al. (2023))."}, "questions": {"value": "1. Can this method be combined with other scores, such as RAPS and SAPS?\n2. In figure 2, there are cases where test label distribution is different from training and validation distribution, how this affects the results, especially considering PAS that is based on class normalization.\n3. Can this method be extended to regression problems with class imbalance, where the continuous target variable has a highly skewed distribution with certain value ranges, much more frequent than others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UpuzbDYXfi", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Reviewer_mwex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Reviewer_mwex"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826410486, "cdate": 1761826410486, "tmdate": 1762924440149, "mdate": 1762924440149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (pt. 1)"}, "comment": {"value": "Here, we provide some information that is worth sharing with all reviewers. We address more specific comments in individual responses to each reviewer.\n\n**Truncated dataset** (Reviewer mYkq, pAFA): _tl;dr: We created the truncated datasets to ensure that we can properly evaluate class-conditional metrics._\nIn an ideal world where we had infinitely large test sets for Pl@ntNet-300K and iNaturalist, there would be no need to create truncated versions. Unfortunately, we have finite test sets, and this creates a challenge when evaluating _class-conditional_ performance of the various methods. Due to the long-tailed distribution, there are many classes for which we have only a handful of test examples (as low as 1). If we ask for coverage of 0.90 but we only have one test example, it is difficult to tell if we are achieving the correct coverage, as the empirical coverage computed on one test example is either 1 or 0. To ensure that we have enough data to compute class-conditional metrics, we create truncated versions where each class has 100 test examples (this 100 threshold was chosen somewhat arbitrarily, but felt to us to be enough to allow for good estimation). However (and we admit this is not well-explained in the original main paper), we still assume that the ultimate test distribution we care about is the same as the train/val distribution. So to compute marginal metrics using the balanced test set, we compute the class-conditional metric and take a weighted average according to the class prevalences in the train set. When we recreate Figure 3 on the truncated datasets (see Figure 7 of the Appendix), we get results that are very similar to the full dataset evaluation, so we defer it to the Appendix. However, in the simulated decision-maker experiments in Section 3.3, we find it necessary to use the truncated dataset because we are no longer reporting aggregations of class-conditional metrics (as in Figure 3) but rather the class-conditional metrics themselves. We have added text to our paper to better describe this.\n\n**Distribution shift / robustness to imperfect prevalence estimation** (Reviewer pAFA, 5apV, mwex): \n * Conformal prediction methods fundamentally rely on the assumption that calibration and test data share the same distribution (or are exchangeable). Developing conformal approaches that can accommodate distribution shifts is an important challenge, but a separate objective from the one addressed in this paper. Some work has been done on label shift in conformal prediction settings, for instance, Podkopaev and Ramdas (2021), but we believe it falls outside the scope of our current work.\n    Furthermore, distribution shift is not a significant problem in the citizen science applications we are motivated by. The data collected by the user, which is used for model training and calibration, and the test examples the model is deployed on come from the same (citizen driven) data distribution.\n* What if we don’t perfectly know the p(y) of the test distribution?\n>    * Running PAS with an imperfect estimate $\\hat{p}(y)$ can be viewed as running WPAS where the weight on class y is $w(y) = p(y)/\\hat{p}(y)$. In other words, running PAS with $\\hat{p}$ approximates the sets that achieve the optimal trade off between the $p(y)/\\hat{p}(y)$-weighted macro-coverage and set size (while achieving the desired marginal coverage guarantee). As $\\hat{p}(y)$ gets closer to $p(y)$, PAS will more closely approximate the sets that optimally trade off _unweighted_ macro-coverage and set size. \n>    * In our experiments, since we know that the train, calibration, and test datasets are all from the same distribution, we estimate $\\hat{p}$ on the train data, since it is more plentiful than the calibration data. If the train distribution does not match the test distribution, we can instead estimate $\\hat{p}$ on the calibration dataset (which, under the standard conformal prediction assumption, is the same as the test distribution)\n\n **PAS coverage guarantees:** Our key insight is that to optimally trade off set size and macro-coverage (average class-conditional coverage), we should threshold on p(y|x) / p(y). Two ways of setting the threshold is to achieve $1-\\alpha$ macro-coverage or to achieve $1-\\alpha$ marginal coverage. These two thresholds are different in general. We choose to set the threshold to achieve $1-\\alpha$ marginal coverage, as marginal coverage is an important desiderata in many settings (and is a goal of almost all conformal prediction methods). We have updated our paper to emphasize that PAS is not designed to create prediction sets with $1-\\alpha$ macro-coverage."}}, "id": "Jsgrtqt0Wb", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699722288, "cdate": 1763699722288, "tmdate": 1763699722288, "mdate": 1763699722288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies conformal prediction for very imbalanced multi class problems where a few classes have many examples and many classes have very few. The goal is to produce prediction sets that keep marginal coverage guarantees while improving coverage for rare classes without exploding set size. The authors offer two main ideas. First, a new score called prevalence adjusted softmax and its weighted version that aim directly at macro coverage, with an option to upweight classes of interest such as endangered species. Second, an interpolation procedure called INTERP Q that blends classwise and standard conformal thresholds to trade off set size and class conditional coverage, with a conservative marginal guarantee. They test on Pl@ntNet 300K and iNaturalist 2018 and report better coverage for tail classes at small or moderate set sizes, plus a study with simple human decision models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Targeting macro coverage with a simple change to the score is a neat idea. It connects the oracle form of the optimal set for macro coverage to a practical score based on p hat of y given x divided by the estimated prevalence. The weighted version lets users push coverage toward special subsets like at risk species.\n\nThe paper is easy to follow. The problem is well motivated with plant identification. The two approaches are separated and labeled. Table 1 is a good map of methods and guarantees.\n\nSelective set prediction in long tailed regimes is common in biodiversity and medicine and also in open world recognition. A method that improves tail coverage while keeping sets short is directly useful for real labeling workflows and can slow collapse of rare labels in human in the loop systems."}, "weaknesses": {"value": "PAS relies on p hat of y given x and an estimate of label prevalence. In real systems there is often label shift between train, calibration, and test. The paper does not test robustness under such shift, even though label shift directly changes the p of y term that PAS divides by.\n\nThe 1 minus 2 alpha lower bound is likely conservative, as the authors note, but the paper does not quantify the realized marginal coverage gap across settings or give a simple correction to hit a target level.\n\nMost results use softmax scores from a standard ResNet trained with cross entropy, with one mention of focal loss in the appendix. Since the approach is driven by score quality, the work would benefit from a broader check across stronger long tail learners such as logit adjusted training and from alternative scores such as APS or label ranking scores, even if set sizes grow\n\nThe human models are an expert verifier and a random guesser, plus mixtures. That is a useful first look, but real users are neither."}, "questions": {"value": "How sensitive is PAS to misspecified prevalence. If the true p of y differs from the training estimate, can you still expect macro coverage gains.\n\nCan you provide a practical scheme to choose tau on the calibration fold to meet a specific marginal coverage while improving class conditional coverage.\n\nHave you tried teaching a small correctness predictor on the calibration set and using that as the score inside PAS or as a rank corrector.\n\nMacro coverage can look good while a handful of classes are still far below target. Can you add plots of the full distribution of per class coverage, not just the fraction below fifty percent.\n\nFor a practitioner who wants sets of average size at most three while lifting average coverage of at risk species above a target, can you give a small recipe that picks alpha and lambda and, for INTERP Q, tau."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9x3c0RcBgz", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Reviewer_5apV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Reviewer_5apV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982048238, "cdate": 1761982048238, "tmdate": 1762924439705, "mdate": 1762924439705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response (pt. 2)"}, "comment": {"value": "**Conformal scores and competing methods**:\n * LAC score (for Reviewer mYkq): we do compare against the LAC score, but we refer to it as the *softmax* score. In the literature, both LAC and softmax are used as names for this score. We have added a note in our paper to highlight that what we call the softmax score is also known as LAC\n * APS/RAPS/SAPS score : It has been observed in several works on many-class classification settings that APS and its variants produce larger sets than softmax (e.g., Ding et al., 2023). The main reason to use APS is to get approximate X-conditional coverage. We assume that this is not an important goal in our setting. \n * Ding et al. (2023)'s method (for Reviewer mwex): this method is called Clustered in our experiments (see for instance Fig. 3). It does not perform well in long-tailed settings, as it defaults to Standard CP for the many classes in the tail. \n * RC3P method by Shi et al. (2024): We are currently working on implementing this baseline and hope to have results in the coming days. However, we expect this method to suffer from similar problems as Classwise, as it similarly has to estimate parameters (a score and rank threshold) for each class.\n\n**Hyperparameter selection:**\n * [WPAS parameter] $\\lambda$ (at-risk species weight) should be chosen based on how much more you value a correctly identified instance of an at-risk species than a not-at-risk species. For instance, if it is 10x more valuable to correctly identify an at-risk species, then set $\\lambda=10$. This depends on the application. \n * [INTERP-Q parameter] For the interpolation parameter $\\tau$, choosing $\\tau=0.75$ seems to provide a good balance in our experiments. More generally, this parameter could be tuned in a more systematic way, possibly on left out data in the same manner used to tune hyperparameters in other conformal methods such as RAPS. This held out valuation can be combined with the kneedle technique from [1] to identify the best trade off point. To get a theoretical guarantee of $1-\\alpha$ coverage, INTERP-Q can be at the $\\alpha/2$ level. \n\n**Summary of changes to paper**. We describe the changes we have made to incorporate reviewer feedback. These changes are highlighted in **red** in the paper pdf.\n* **Tightness of INTERP-Q coverage bound**: One reviewer questioned the tightness of the $1-2\\alpha$ coverage guarantee of INTERP-Q. We have added an example of distribution for which this lower bound is almost attained (up to $\\alpha^2$). See the updated commentary following Proposition 3 and the corresponding formal statement in Appendix B.3.\n* **Better explanation of truncated dataset**: We have updated the discussion about the truncated dataset to clarify the purpose of its construction: having correct estimates of class-conditional coverages.\n* **PAS guarantee**, we have emphasized that PAS does not target directly a macro coverage guarantee but better handles the trade-off of macro-coverage and set size. In particular, we have clarified the objective or our methods and replaced $1-\\alpha$ by $\\beta$ in the optimization problem (8) and Proposition 1 and 2 as it seems to mislead readers.\n* **Improved evaluation visualization**: We added a $y=1-\\alpha$ line in Figure 5 to show when the decision accuraries (which equals the classwise coverage for $\\gamma=100\\%$) goes below this targeted level (linked to Reviewer mYkq's \"Evaluation Metric\" remark)\n* Other minor edits have been added throughout the paper to clarify various points.\n\n[1] Satopää et al., Finding a “Kneedle” in a Haystack: Detecting Knee Points in System Behavior, 2011."}}, "id": "JubQLTYGKS", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699964712, "cdate": 1763699964712, "tmdate": 1763699964712, "mdate": 1763699964712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper applies conformal prediction (CP) to an extremely long-tailed classification setting. The standard CP tends to ensure coverage of common classes and under/miss coverage of the rare ones, whereas classwise CP covers each class but produces unusable prediction sets. The authors address this challenge by 2 approaches: a) PAS / WPAS -> the idea here is to, instead of trusting the raw model probabilities, recalibrate label scores by their rarity, so that rare classes get a boost. The authors claim this results in smaller prediction sets compared to classwise CP, and is also fairer to rare classes compared to standard CP. b) INTERP-Q -> instead of score recalibration, this approach uses a controllable parameter that provides a cutoff threshold that balances between standard CP and classwise CP thresholds. It gives the user a controllable knob that can decide how much to protect tail classes versus how large a prediction set can be.\nEmpirically, they compare against standard baselines and the evidence supports their claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strength\n\na) The main paper is well motivated, mostly clear and easy to follow.\n\nb) It tackles conformal prediction in the extreme long-tailed scenario, which is practically important.\n\nc) The class coverage vs prediction set size tradeoff as a problem formulation itself seems novel. The two proposed approaches also appear reasonably original. \n\nd) Empirical studies are convincing, and their human decision maker simulation experiment seems interesting to me."}, "weaknesses": {"value": "Weaknesses\n\na) I could understand the working of PAS/WPAS and INTERP-Q, but I couldn't clearly find the motivation for utilizing either/or both of them. The two methods seem to address different parts of the pipeline, but the paper does not clearly explain a practical guideline when a practitioner should pick PAS/WPAS, INTERP-Q, or use them together.\n\nb) The experimental details in the appendix mention utilizing a truncated version with n-core filtering with n = 101. I am curious: doesn't this contradict the problem the authors are trying to solve? The paper is motivated by the extreme long tail, but the analysis is done only on this truncated subset. It would be helpful to clarify whether the main conclusions still hold for the truly rare classes that were filtered out.\n\nc) Based on my understanding, PAS / WPAS is motivated by the hypothesis that reweighting by class prevalence is optimal for trading off average per-class coverage and set size. However, in the implementation, PAS simply recalibrates standard split CP scores, which only guarantees marginal coverage, not class-conditional coverage. So the claim that there are fewer under-covered long-tailed classes seems to be empirical rather than a strict guarantee to me. Similarly, INTERP-Q appears to be only loosely bound to the standard finite-sample marginal guarantee. Neither approach provides a strict class-conditional guarantee, which makes the strength of claims less clear.\n\nd) The paper only evaluates using splits where calibration and test share the same long-tailed label frequencies. It would be interesting to see whether the proposed methods hold up when the test distribution differs from calibration.  I believe stress-testing robustness to even basic shifts would better support the practicality claims of the paper.\n\ne) Comparison with standard CP, classwise CP, and the clustered variant is good. But I would like to see how the method fares against some recent methods that seem to have similar motivation/methodologies [1][2].\n\n\n[1] Liu, Shuqi & Huang, Jianguo & Ong, Luke. (2025). Conformal Prediction Meets Long-tail Classification.\n\n[2] Yuanjie Shi, Subhankar Ghosh, Taha Belkhouja, Janardhan Rao Doppa, and Yan Yan. (2024). Conformal prediction for class-wise coverage via augmented label rank calibration (RC3P). NeurIPS 2024."}, "questions": {"value": "Please refer weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HoT0518dJA", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Reviewer_pAFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Reviewer_pAFA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983937570, "cdate": 1761983937570, "tmdate": 1762924439182, "mdate": 1762924439182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing conformal prediction methods often trade off between poor class-conditional coverage and overly large prediction sets. This paper proposes a new non-conformity score, PAS (and its weighted variant WPAS), which aims to interpolate between marginal and class-conditional conformal prediction. The goal is to achieve better coverage–efficiency balance under long-tailed label distributions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tAddresses the practically relevant problem of conformal prediction under long-tailed label distributions.\n\n2.\tThe proposed PAS/WPAS scores are simple and easy to implement.\n\n3.\tThe experiments offer preliminary evidence that the proposed method improves coverage fairness under long tailed setting."}, "weaknesses": {"value": "1.\tTruncated dataset setup: The test datasets are balanced by truncating rare classes and retaining those with more than 100 samples per class. The choice of this threshold is not explained. Why 100? In more realistic scenarios where both calibration and test sets are long tailed, how would the proposed method perform?\n\n2.\tMotivation: The new non-conformity score (PAS) is motivated by an oracle analysis showing that the optimal set depends on p(y|x)/p(y), but this only characterizes an ideal solution rather than a provably better practical formulation. Overall, the motivation appears ad-hoc, and it remains unclear what concrete limitation of existing scores (e.g., APS, LAC) PAS resolves under long-tailed distributions. \n\n3.\tBassline: The paper does not include [1], which also targets class-conditional coverage under imbalanced settings, which appears highly relevant to this work. In addition, standard non-conformity scores such as APS [2] and LAC [3] are omitted. \n\n4.\tScore comparison: Since APS emphasizes X-conditional coverage and LAC minimizes the expected set size, including them would provide a more informative comparison of the adaptiveness–efficiency tradeoff that PAS/WPAS aims to improve. It would also be helpful to clarify whether a weighted combination of APS and LAC could already achieve a similar trade-off, and how PAS/INTERP-Q compares in that respect.\n\n5.\tEvaluation Metric: The study reports FracBelow50% and UCG but omits the more intuitive Under-Coverage Ratio (UCR) used in [1], which reflects the fraction of classes that fail to meet the target coverage and would provide a more informative evaluation.\n\n[1] Yuanjie Shi, Subhankar Ghosh, Taha Belkhouja, Jana Doppa, and Yan Yan. Conformal prediction for class-wise coverage via augmented label rank calibration. NeurIPS 2024.\n\n[2] Yaniv Romano, Matteo Sesia, and Emmanuel J. Candes. Classification with valid and adaptive coverage. NeurIPS, 2020.\n\n[3] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. J. Amer. Statist. Assoc 2019."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bmJM9IvSwt", "forum": "8L83ZbFDjk", "replyto": "8L83ZbFDjk", "signatures": ["ICLR.cc/2026/Conference/Submission13936/Reviewer_mYkq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13936/Reviewer_mYkq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762623147034, "cdate": 1762623147034, "tmdate": 1762924438790, "mdate": 1762924438790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}