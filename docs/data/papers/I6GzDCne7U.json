{"id": "I6GzDCne7U", "number": 21966, "cdate": 1758324172849, "mdate": 1759896893515, "content": {"title": "DeepScaleR: Effective RL Scaling of Reasoning Models via Iterative Context Lengthening", "abstract": "Recent advances in large reasoning models (LRMs) such as OpenAI's o1 and Deepseek-R1 have demonstrated that reinforcement learning (RL) with outcome-based supervision can significantly enhance the reasoning abilities of language models. However, these improvements have so far relied on massive model scales and compute budgets, leaving open the question of whether RL-based scaling can be made both effective and efficient at smaller scales. In this work, we introduce DeepScaleR-1.5B, a 1.5B parameter model trained using reinforcement learning with a novel iterative context lengthening strategy. Our method begins with shorter context windows and progressively extends them throughout training, enabling the model to first learn to reason efficiently before learning to reason longer. This approach yields substantial performance gains with dramatically reduced computational cost. DeepScaleR-1.5B achieves 43.3% Pass@1 on the AIME2024 math benchmark—a 14.3 percentage point improvement over its base model and on par with OpenAI's o1-preview—while requiring a fraction of the compute. We provide a full training recipe, including dataset, code, hyperparameters, and training methodology, demonstrating that small models can be effectively scaled into strong math reasoners via RL.", "tldr": "", "keywords": ["reinforcement learning", "large reasoning model", "LLM reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8254bd94a935b0b417c30b3c0c2a86535045e3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This approach yields substantial performance gains with dramatically reduced computational cost. DeepScaleR-1.5B achieves 43.3% Pass@1 on the AIME2024 math benchmark—a 14.3 percentage point improvement over its base model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.DeepScaleR-1.5B achieves 43.3% Pass@1 on the AIME2024 math benchmark.\n2.The GRPO algorithm is advanced."}, "weaknesses": {"value": "1.The performance for 1.5B-LLM is not in the sota range, for example nvidia-1,5b\n2.These is not very much novelty in the proposed algorithm.\n3.These is not evluation on the code dataset in the experiments as code is also a good task for the reasoning ability of LLMs.\n4.There is big gap netween the performacne of the proposed LLM and the performance of the sota LLMs on 1.5B paramater scale."}, "questions": {"value": "1.Why does the 1.5B-parameter LLM fail to achieve state-of-the-art performance, particularly when compared to models like NVIDIA's 1.5B?\n2.What novel contributions does the proposed algorithm offer beyond existing methods?\n3.Why is there no evaluation of the proposed method on code-related datasets, despite code being a strong indicator of reasoning ability in LLMs?\n4.Why is there a significant performance gap between the proposed 1.5B-parameter LLM and current state-of-the-art LLMs at the same scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VcLRBNWGRO", "forum": "I6GzDCne7U", "replyto": "I6GzDCne7U", "signatures": ["ICLR.cc/2026/Conference/Submission21966/Reviewer_P2gr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21966/Reviewer_P2gr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923240426, "cdate": 1761923240426, "tmdate": 1762942000077, "mdate": 1762942000077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DeepScaleR, an efficient and effective training recipe for reasoning models. Specifically, it proposes high-quality data curation and iterative context lengthening, which gradually extends the context window during RL training (from 8K to 16K to 24K) to help the model learn efficient short reasoning before longer reasoning. Evaluation shows the effectiveness of context scheduling, as the trained 1.5B model achieves 43.3% Pass@1 on AIME2024, a 14.3% improvement over the base model and comparable to OpenAI’s o1-preview. This paper aims at reporting a RL-based training recipe that enables models to achieve good reasoning performance efficiently."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow.\n\nThe idea of iterative context lengthening scheduler is straightforward.\n\nThe method leads to a 1.5B model that has shows good performance in reasoning benchmarks."}, "weaknesses": {"value": "Limitation of the model and experimental results. Although the claim is that the technique can enable small model with efficient training to have reasoning ability, the current technique is applied only to train a 1.5B model. It is currently not clear whether the length scaling can be universally effective for other model configurations, i.e. different sizes or different architectures. Furthermore, is this technique applicable for models with larger size, i.e. 7B model.\n\nQuestion regarding length cutting: during training, by cutting at i.e. ctx length = 8k, do you explicitly let the model generate the final answer, i.e. by appending the final <think> token after it reaches 8k output length? How is this step done?\n\nQuestion regarding the 24k ctx length ablation. Why does the plot show almost no improvement (figure 4) with static context length during training?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOHtcYaxaz", "forum": "I6GzDCne7U", "replyto": "I6GzDCne7U", "signatures": ["ICLR.cc/2026/Conference/Submission21966/Reviewer_ZJL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21966/Reviewer_ZJL6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972154730, "cdate": 1761972154730, "tmdate": 1762941999720, "mdate": 1762941999720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge that training large reasoning models with Reinforcement Learning is computationally expensive and believed to be ineffective for smaller models. The authors propose iterative context lengthening, a training strategy that acts as an implicit curriculum. Instead of training at a large, fixed context (e.g., 24K), iterative context lengthening starts with a short context (8K) to force the\nmodel to learn efficient reasoning, then progressively increases the context length (to 16K, then 24K) as performance plateaus. Using this method, their 1.5B parameter DeepScaleR model achieves 43.3% Pass@1 on AIME2024, a 14.3% gain over its base model, matching o1-preview. This was achieved with a 2.6x reduction in compute cost compared to a direct 24K training baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple and Effective Method: Iterative context lengthening is an intuitive, simple, and highly effective training strategy that provides a more stable and efficient curriculum than direct\nlong-context training.\n- Strong Performance and Efficiency: The 1.5B model achieves a significant +14.3% absolute\ngain on AIME2024, demonstrating that small models can be scaled with RL. This is achieved\nwith 2.6x less training compute and results in a model that is also more efficient at inference time.\n- Good Ablations: The ablation study (Figure 4) clearly proves iterative context lengthening's\nsuperiority over a direct 24K baseline."}, "weaknesses": {"value": "- Questionable Base Model Choice: The base model selection of a Qwen-2.5B-Math model is a concern, as this model series is known for potential test-set contamination on math benchmarks. This makes it difficult to definitively attribute the +14.3% AIME gain solely to the iterative context lengthening technique rather than the base model's pre-existing (and potentially \"tainted\") capabilities. The claims would be far more convincing if the authors either:\n  - Replicated the experiment with a different base model (e.g., the Qwen3-0.6B used for the\nCOUNTDOWN task or Gemma-3-1B).\n  - Evaluated the base and final trained models on a contamination-resistant benchmark,\nsuch as LiveMathBench, to confirm the gains are from reasoning and not memorization.\n\n- Unclear Mechanism for \"Shorter Reasoning\": The paper's claim that a constrained window \"Encourages shorter reasoning\" is not well substantiated. A constrained window merely truncates long responses, filtering them from the gradient update. This doesn't necessarily teach the model to be concise. It's plausible that the RL algorithm (GRPO, which is known to increase response length) still favors longer reasoning paths, which are then simply cut off. This would result in fewer valid, complete responses within the constrained window, not more efficient ones.\n  - To truly support this claim, the authors should show that the 8K-trained model produces a\nhigher percentage of valid, complete responses (i.e., those reaching a final answer) within\na fixed 8K/16K/24K context than the base model. The data in Table 1, which only shows\naverage token length, is insufficient proof of this efficiency gain.\n\n- Ambiguous Test-Time Scaling Evaluation: The test-time scaling analysis in Figure 5 is missing a critical detail: the maximum context window used for generating the 64 samples. It is unclear if this was capped at the 24K training limit or was uncapped.\n  - A more insightful comparison, given the paper's theme, would be to evaluate how both the base model and DeepScaleR perform at test-time when the context window is expanded beyond the training limit with scaling at test time (e.g., progressively to 32K or\n64K).\n\nI am happy to increase my score if all the concerns are resolved."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XBDt2Ddcmi", "forum": "I6GzDCne7U", "replyto": "I6GzDCne7U", "signatures": ["ICLR.cc/2026/Conference/Submission21966/Reviewer_ky9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21966/Reviewer_ky9p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987754111, "cdate": 1761987754111, "tmdate": 1762941999184, "mdate": 1762941999184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}