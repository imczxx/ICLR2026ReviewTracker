{"id": "INnZIRw2XE", "number": 10157, "cdate": 1758162464955, "mdate": 1763117188623, "content": {"title": "Scaling Open-world Multiple Object Tracking", "abstract": "Multiple Object Tracking (MOT) has traditionally relied on expensive, exhaustively annotated datasets, limiting scalability and generalization.\nTo address these limitations, we propose \\textbf{\\ourmodel}, a transformer-based association module for MOT, explicitly designed to leverage large-scale, sparsely annotated video data. At the core of our approach is \\textit{Chain Contrastive Learning}, a novel contrastive strategy that maintains local discriminability while capturing long-range temporal coherence. Specifically, our approach constructs positive pairs in a chained manner across consecutive frames, promoting transitive consistency and local discriminability simultaneously. Our model additionally features a multi-scale spatiotemporal attention mechanism that effectively integrates contextual information across space and time, ensuring robust associations even in challenging scenarios. Notably, our method consistently improves performance as the amount of training video data increases, demonstrating robust scalability.\nOur tracker is designed as a plug-and-play module that seamlessly synergizes with any object detector, achieving state-of-the-art zero-shot performance across multiple large-scale MOT benchmarks, including TAO, BDD100K, SportsMOT and OVT-B. Code will be made public.", "tldr": "", "keywords": ["Open-vocabulary tracking", "multiple object tracking", "open-world"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d16628641826d859214cb2b446a565c4af16a025.pdf", "supplementary_material": "/attachment/cf1cb5beabab2b6b917c03a41db5d8e864c9310b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ScaleTrack, a transformer-based framework for open-world multiple object tracking (MOT) designed to scale efficiently on video datasets. The core contribution is the Chain Contrastive Learning (CCL), a temporal contrastive strategy that links positive samples across consecutive frames, maintaining both local discriminability and long-range temporal consistency. The model incorporates pseudo-negative sampling (from SAM/Detic proposals) and a multi-scale spatiotemporal attention module for robust association. Extensive experiments on TAO, BDD100K, SportsMOT, and OVT-B show consistent zero-shot performance gains over prior methods (e.g., MASA, GLEE, OVTrack), demonstrating strong scalability and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Chain Contrastive Learning effectively bridges local and global temporal contexts, addressing the scalability limitations of prior pairwise or global contrastive approaches.\n2. The ScaleTrack is evaluated on multiple benchmarks, including ablation studies and scaling analyses (chain length, frozen backbone, etc.) to show its effectiveness."}, "weaknesses": {"value": "- Besides open-world MOT datasets, the authors are encouraged to evaluate the proposed method on standard MOT benchmarks such as MOT17, MOT20, and DanceTrack to more comprehensively demonstrate its association capability and general applicability.\n- It is recommended to directly compare ScaleTrack with methods that do not use Chain Contrastive Learning (CCL), such as MeMOTR and MOTIP, to quantitatively verify the effectiveness of the proposed CCL module.\n- The authors could further transfer or plug the proposed CCL mechanism into other existing architectures to verify its universality and adaptability beyond the presented framework.\n- An efficiency comparison (inference speed, FLOPs, or training cost) should be added to better illustrate the practical scalability and computational trade-offs of the proposed approach."}, "questions": {"value": "refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qpz20xezaD", "forum": "INnZIRw2XE", "replyto": "INnZIRw2XE", "signatures": ["ICLR.cc/2026/Conference/Submission10157/Reviewer_wXyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10157/Reviewer_wXyn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621421239, "cdate": 1761621421239, "tmdate": 1762921527556, "mdate": 1762921527556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "SVCHH54wYh", "forum": "INnZIRw2XE", "replyto": "INnZIRw2XE", "signatures": ["ICLR.cc/2026/Conference/Submission10157/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10157/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763117187785, "cdate": 1763117187785, "tmdate": 1763117187785, "mdate": 1763117187785, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a novel approach called chain contrastive learning to capture local discriminative features and long-range temporal coherence features from large-scale, sparsely annotated video data for open-world multople object tracking. Comprehensive experiments are conducted to effectiveness of the proposed method and its superiority to other state-of-the-art algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of learning effective feature from sparsely annotated video data for open-world multiple object tracking is of importance in real-world application.\n2. Experiment part is solid and demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The writing and organization of the paper is moderate which makes it not easy to follow and understand the motivation of the proposed idea and method.\n2. What are the challenges of \"leverage large-scale, sparsely annotated video data\".\n3. Line 40, the author states that \"...appearance association module as the most viable path toward generalizable MOT\". How to come to this \nconclusion, any evidence to support it?\n4. Line 76, the author states that \"As the data size increases...causing the learning to plateau.\" How to demonstrate this problem?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "spu1weroKD", "forum": "INnZIRw2XE", "replyto": "INnZIRw2XE", "signatures": ["ICLR.cc/2026/Conference/Submission10157/Reviewer_Te2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10157/Reviewer_Te2h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843642895, "cdate": 1761843642895, "tmdate": 1762921526849, "mdate": 1762921526849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chain Contrastive Learning to maintain local discriminability while capturing long-range temporal coherence. The performance seems good on many datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The performance is good and the experiment is extensive."}, "weaknesses": {"value": "- The paper claims to “scale up” OW-MOT, but it is unclear in which aspect this scaling occurs. The proposed approach does not appear to expand the category space, task complexity, or range of scenarios typically associated with open-world settings. Instead, the work primarily modifies the contrastive learning formulation. Therefore, the claim of scaling up OW-MOT seems overstated and should be clarified or reframed to accurately reflect the scope of the contribution.\n\n- The paper mentions experiments on partially labeled videos, but these scenarios are not clearly demonstrated or discussed. It remains unclear what proportion of the training data is labeled versus unlabeled, and how the proposed method effectively leverages the unlabeled portions. If it is about pseudo-negative samples, this can be done simply by selecting proposals other than positive by thresholding, similar to QDTrack [1]. The only data scaling comparison in Figure 3 was not conducted with other SOTA works.\n\n- The proposed chain formulation, which accumulates pairwise similarities across frames via summation, appears conceptually similar to existing holistic temporal association strategies. For instance, QDTrack already encourages transitive consistency across multiple positives over entire video sequences. The novelty and benefit of the proposed method over such established approaches are therefore not well justified.\n\n- Initially, I had the feeling that the chain could be extended to infinite length via online processing; however, taking a look at Table 5. It seems that the chaining methodology only supports a handful number of samples. So it does not make a significant difference to batch processing in Figure 1a.\n\n[1] Fischer, et al. Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking.TPAMI 2023."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DAvqjZgP5h", "forum": "INnZIRw2XE", "replyto": "INnZIRw2XE", "signatures": ["ICLR.cc/2026/Conference/Submission10157/Reviewer_duKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10157/Reviewer_duKa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941944149, "cdate": 1761941944149, "tmdate": 1762921526307, "mdate": 1762921526307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new method for open-world multiple object tracking. It proposes a new contrastive strategy for better contrastive learning. A transformer-based model is built utilizing this strategy. Experiments on multiple public benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of selecting samples from constrained frames for contrastive learning makes sense.\n2. The implementation of the chain constrastive learning strategy is reasonable.\n3. The proposed method is effective on multiple public benchmarks."}, "weaknesses": {"value": "The main concern is limited methodology contribution. The main contribution is the chain contrastive learning strategy, which is a straightforward modification of existing strategies. The corresponding ScaleTrack generally adopts existing modules. Hence the contribution on the methodology part is not enough from my view."}, "questions": {"value": "I am curious about the universality of the proposed contrastive learning strategy. Could it be applied to other trackers, besides the ScaleTrack? It would be great if related experiments are included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "897tsbT5LY", "forum": "INnZIRw2XE", "replyto": "INnZIRw2XE", "signatures": ["ICLR.cc/2026/Conference/Submission10157/Reviewer_7EJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10157/Reviewer_7EJo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970451923, "cdate": 1761970451923, "tmdate": 1762921525819, "mdate": 1762921525819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}