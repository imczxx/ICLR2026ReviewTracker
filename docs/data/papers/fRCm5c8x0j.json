{"id": "fRCm5c8x0j", "number": 14773, "cdate": 1758243372276, "mdate": 1759897350108, "content": {"title": "Improving Attributed Long-form Question Answering with Intent Awareness", "abstract": "Large language models (LLMs) are increasingly being used to generate comprehensive, knowledge-intensive reports. However, while these models are trained on diverse academic papers and reports, they are not exposed to the reasoning processes and intents that guide authors in crafting these documents. We hypothesize that enhancing a model's intent awareness can significantly improve the quality of generated long-form reports. We develop and employ structured, tag-based schemes to better elicit underlying implicit intents to write or cite. We demonstrate that these extracted intents enhance both zero-shot generation capabilities in LLMs and enable the creation of high-quality synthetic data for fine-tuning smaller models. Our experiments reveal improved performance across various challenging scientific report generation tasks, with an average improvement of +2.9 and +12.3 absolute points for large and small models over baselines, respectively. Furthermore, our analysis illuminates how intent awareness enhances model citation usage and substantially improves report readability.", "tldr": "", "keywords": ["deep research", "long form question answering", "attributed question answering", "RAG", "supervised fine-tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed4b4015573063df3ec475e6ea9189529be4271a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an _intent-aware writing framework_ for attributed long-form question answering.  It augments both inference and training with explicit paragraph- and citation-level intent tags that describe the purpose of text segments (e.g., \"background,\" \"motivation,\" \"cause–effect\").   Experiments on three benchmarks—SQA-CS-V2, DeepScholar, and ResearchQA—show consistent gains  (+2.9 for large LMs, +12.3 for small ones)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation linking human writing processes to LLM report generation, supported by consistent empirical evidence.\n2. Simple but effective method with broad applicability—plug-and-play intent tagging improves attribution, readability, and transparency."}, "weaknesses": {"value": "1. The conceptual novelty is limited — effectively a structured form of rationale or CoT distillation.\n2. Intent schema is static and handcrafted, not learned or generalized across domains.\n3. Lacks comparison to stronger baselines such as _CoT + Answer_ or _rationale distillation_."}, "questions": {"value": "1. How does intent-aware SFT compare to _CoT + Answer_ distillation under equal data and compute budgets?\n2. If the model could _self-summarize intents_ from non-intent data (i.e., induce tags automatically) before SFT, would this remain effective—transforming the contribution from distillation to curation innovation?\n3. Can the approach generalize to less-structured domains such as policy or humanities writing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vs2Fbgrk2o", "forum": "fRCm5c8x0j", "replyto": "fRCm5c8x0j", "signatures": ["ICLR.cc/2026/Conference/Submission14773/Reviewer_yQe7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14773/Reviewer_yQe7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625674776, "cdate": 1761625674776, "tmdate": 1762925127885, "mdate": 1762925127885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an intent-aware report generation framework that can be used for both prompting and fine-tuning. The framework includes paragraph intent and citation intent (sentence-level), explaining the intent of including the citation or paragraph. In prompting, the model first generates the intent, and then generates the actual paragraph or citation. In fine-tuning, the student model is trained on data generated with a larger teacher model. Results show this improves citation precision / recall. Analysis shows that LMs capture the general idea of how people use intents, but distribution could vary among LMs. Small-scale user study shows that this method improves readability of the generated report."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow. I can read the paper without constantly referring to details in the Appendix.\n2. The proposed intent-aware prompting / fine-tuning methods improve Citation Precision and Recall. \n3. Clear ablation study showing the importance of each component. \n4. Interesting analysis showing how LMs generate reports with different proportions of intents. There is also potential for this to be an analysis / evaluation tool in the future as mentioned by the authors. I almost feel like this is the most exciting part of the paper. \n5. The human study including three PhD students reading actual generated reports is also interesting, showing the potential of this framework for more readable reports. Although I wish larger scale experiments could be done in this direction (improving readability)."}, "weaknesses": {"value": "1. Baselines are lacking. For example, I think at least a simple Chain-of-Thought (CoT) prompting baseline should be included. For fine-tuning, training on CoT generated by teacher models should also be included. There should be some additional baselines, like STaR (Zelikman et al., 2022) for example. \n2. The improvements mainly come from Citation Precision / Recall. For SQA-CS-V2 (Table 2), only Claude improves on Rubrics with intent-aware prompting. However in Table 3, Claude does not improve on Nugget Coverage. The same can be said for fine-tuning results (Table 4). It is also unclear why this is the case, given the paragraph-only variant (Table 5) also improves about 3 points in Citation Precision and Recall, but only 0.1 in Rubrics and -1.2 in Answer Precision. \n3. There is no significance test of any sort. Given some of the performance gains are pretty small, I think statistical significance is needed for judging the effectiveness of the method. \n4. Following 2., I think the claims that “performance” or \"capability\" in generating better reports seem to be overstatements, given the gain mostly come from better citation usage. I don’t know exactly how the “Overall” score is computed (not mentioned in the paper), but most of the gains in overall score seem to come from citation P/R. I think it would be better if the authors are more straightforward about this in the abstract and introduction. Combining my previous points, I almost feel like the advantage of using this framework is not in the performance, but in better citation usage and potentially better readability as shown in the case study. \n4. Minor Points:  \na. Some inconsistencies in experiments. Why not include o3 in Table 3? Why not include Deepscholar Bench and ResearchQA in Table 4? These should be explained.  \nb. In the paragraph “intent awareness improves citation usage”, from L370 to L372, the claim that intent-aware prompting “significantly improves the portion of retrieval candidates used” is not accurate. The only gain that I think is significant is on gemini-2.5-pro. Also, the claim of “without precision loss” is also not supported for the Qwen models.  \n\nReference: \nZelikman, Eric, et al. \"Star: Bootstrapping reasoning with reasoning.\" Advances in Neural Information Processing Systems 35 (2022): 15476-15488."}, "questions": {"value": "1. How is \"Overall\" score computed for all the datasets? \n2. Why not include o3 in Table 3? Why not include Deepscholar Bench and ResearchQA in Table 4? (Same as mentioned in 5. in Weakness)\n3. Is more citation always better? For example, if there are multiple works showing RAG helps factoid QA, is citing 10 of them better than just citing 5 of them? I wonder how much the claim that \"increased citation usage denotes that the model can cover more diverse points\"  (L372-373) is true."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6yHKNYT1YR", "forum": "fRCm5c8x0j", "replyto": "fRCm5c8x0j", "signatures": ["ICLR.cc/2026/Conference/Submission14773/Reviewer_yUc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14773/Reviewer_yUc4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871306350, "cdate": 1761871306350, "tmdate": 1762925127508, "mdate": 1762925127508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper augments the training data for language models with intents on paragraph and citation levels. The citation intents originate from ACL-ARC while the paragraph intents originate from existing literature. The paper is grounded in previous literature in writing intentions. The experiments include both prompting approaches with commercial closed models as well as training for open-weight models, and found improvements across different scientific writing benchmarks. Furthermore, citation scores generally improves with more citation coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an original approach to improving the citation quality by explicitly prompting and training models to generate the paragraph and citation intentions.\nThe motivation is well-grounded in previous literature in writing intentions, and show clear gains on several types of models and benchmarks.\nThe approach can be useful for the community as citation and trustworthiness is a critical flaw in modern large language models.\nFinally, the paper is overall well-written with clarity, and the figures are well designed and easy to understand."}, "weaknesses": {"value": "The paper could benefit from additional validation on the paragraph intent. Although the human studies in 4.3 shows that humans may find the intention useful for when reading and understanding the paragraph, it’s unclear if models are generating the correct intent type.\n\nFurthermore, additional discussion on how different distribution of paragraph intent types lead to differences in results could be insightful.\n\nFinally, there are no ablations on how much citation intent helps versus paragraph intents help, so ablating each component would improve the understanding on the importance of these part.\n\nTypo: Table 1 “he” → “The”"}, "questions": {"value": "When generating the citations for the training data, does the model get the cited article/text as part of the input?\n\nAre there o3 results for DeepScholarBench and RQA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hECJgvul1T", "forum": "fRCm5c8x0j", "replyto": "fRCm5c8x0j", "signatures": ["ICLR.cc/2026/Conference/Submission14773/Reviewer_Rq6k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14773/Reviewer_Rq6k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953548661, "cdate": 1761953548661, "tmdate": 1762925127078, "mdate": 1762925127078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles long-form, citation-heavy scientific report generation by large language models (LLMs). The authors argue that while current “deep research” systems can retrieve dozens or hundreds of sources and stitch together long answers, they still struggle with (i) organizing material into a coherent narrative, (ii) making clear rhetorical moves (e.g., motivate, compare, explain causality), and (iii) citing sources in a disciplined, honest way.\n\nThe core hypothesis is: if we make models explicitly consider their writing and citation intent while generating, those outputs will improve in structure, attribution quality, and readability"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of incorporating citation intention into the prompts and training data of research agents is spot-on. The argument for a need of intention in writing and research citations is well-established. The experiments are designed to thoroughly test the potential of intent-aware generation (isolating effects of prompt-based inference techniques and finetuning strategies)."}, "weaknesses": {"value": "1. **Unclear and (potentially) unreproducible training data**: Section 3.4 mentions that training data comes from a \"larger teacher model\". There lacks disclosure on details of the choice and ablation of the teacher model. How exactly is the synthetic data generated? Does the teacher model ever hallucinates tags? Without these details, the paper currently suffer from reproducibility and credibility issues. \n\n2. **Experiment results analysis**: Each evaluation metric reflects a different component of citation quality -- but there's a lack of discussion on how intent-aware inference and training improves individual components, and what it means to the qualitative citation quality. Table 2 shows very marginal improvement on the SQA-CS-V2 task. \n\n3. **Limited participants in case study**: There's only 3 participants who contributed to the case study in Section 4.3. This is an extremely small sample size that makes the result analysis weak."}, "questions": {"value": "1. Benchmarks like AstaBench and DeepScholar-Bench use LLM-based rubric scoring and automatic verifiability checks. What LLM judges are chosen? Is the judge model from a different family than the synthetic data generation model? \n\n2. How do you explain the difference in the impact of intent-awareness between eval metrics? For example, in Table 2 and Table 5, \"Rubrics\" metric and \"Answer P\" metric almost stayed the same, where the aggregated advantage of intent-awareness seems to only come from CitationP and CitationQ. How should we interpret this? \n\n3. How did you ensure that human participants are not biased? How did you recruit human participants? \n\n4. Why did you choose temperature 1.0 for inference? This can lead to unstable performance especially if you only run each evaluation metrics once. Can you please show us the mean and std of Table 4 across multiple different seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLgY6zDo5V", "forum": "fRCm5c8x0j", "replyto": "fRCm5c8x0j", "signatures": ["ICLR.cc/2026/Conference/Submission14773/Reviewer_N4jP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14773/Reviewer_N4jP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984648252, "cdate": 1761984648252, "tmdate": 1762925126670, "mdate": 1762925126670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}