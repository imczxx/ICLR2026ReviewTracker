{"id": "bz9fu4cL3W", "number": 22063, "cdate": 1758325532534, "mdate": 1763693894899, "content": {"title": "The Vendiscope: An Algorithmic Microscope For Data Collections", "abstract": "The evolution of microscopy, beginning with its invention in the late 16th century, has continuously enhanced our ability to explore and understand the microscopic world, enabling increasingly detailed observations of structures and phenomena. In parallel, the rise of data-driven science has underscored the need for sophisticated methods to explore and understand the composition of complex data collections. This paper introduces the $\\textit{Vendiscope}$, the first $\\textit{algorithmic microscope}$ designed to extend traditional microscopy to computational analysis. The Vendiscope leverages the Vendi scores -- a family of differentiable diversity metrics —- and assigns weights to data points based on their contribution to the overall diversity of the collection. These weights enable high-resolution data analysis at scale. We demonstrate this across biology and machine learning (ML). We analyzed the 250 million protein sequences in the protein universe, discovering that over 200 million are near-duplicates and that ML models like AlphaFold fail on proteins with Gene Ontology (GO) functions that contribute most to diversity. Additionally, the Vendiscope can be used to study phenomena such as memorization in generative models. We used the Vendiscope to identify memorized training samples from 13 different generative models spanning several model classes and found that the best-performing generative models often memorize the training samples that contribute least to diversity. Our findings demonstrate that the Vendiscope can serve as a powerful tool for data-driven science, providing a systematic and scalable way to identify duplicates and outliers, as well as pinpointing samples prone to memorization and those that models may struggle to predict—even before training.", "tldr": "We present the Vendiscope, a scalable algorithmic microscope that enables the systematic detection of outliers, duplicates, model failure modes, and memorization across domains.", "keywords": ["Algorithmic Microscopy", "Data-Driven Science", "Vendi Scoring"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c50c8f8100d8be4cff6b1f030c11c6913bcab1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the concept of algorithmic microscopes, tools designed to reveal hidden structure in both dataset composition and model behavior. An algorithmic micro.scope emphasizes understanding - helping researchers understand the contents of their data andwhere their models fail."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.They demonstrate the Vendiscope's contribution scores can help identify outliers and near-duplicates in linear time.\n2.They show how the same framework can be used to evaluate machine learning models -both predictive and generative.\n3.They apply the Vendiscope to the 250 million sequences composing the protein universe.where it identifies >80% redundant data points at a 90% similarity threshold."}, "weaknesses": {"value": "1. Adding a figure to show the overview of the framework will help readers to quickly understand your contribution.\n    2. The Vendiscope's outputs are fundamentally dependent on the choice of the similarity kernel and the data embeddings. The claim that the method works \"in any domain where similarity can be defined\" is true in principle, but the practical utility and interpretability of the results are highly contingent on the quality and semantics of the chosen similarity function. An analysis of this sensitivity is crucial.\n    3. The selected generative model architectures are not the recent published works. As mentioned in  A4.2, all of them are published before 2021. I believe the author should compare more methods in the recent two years.\n    4. More explanation about figures is needed. For example, how to understand the statements that “the most common images generated by each model are those that are increasingly similar to the training data.” shown in Figure 13?"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EuEQriA3DD", "forum": "bz9fu4cL3W", "replyto": "bz9fu4cL3W", "signatures": ["ICLR.cc/2026/Conference/Submission22063/Reviewer_vUBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22063/Reviewer_vUBK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400364723, "cdate": 1761400364723, "tmdate": 1762942041460, "mdate": 1762942041460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for performing analysis of datasets to uncover factors such as data samples that contribute least to diversity.  Specifically, this paper leverages Vendi scores, a metric of diversity based on a given similarity metric for a data set.  The proposed methodology is evaluated on several data sets.  Results on protein data show that the method can identify rare sequences, for which models such as AlphaFold perform most poorly.  Redundant protein sequences can be clustered, showing strong performance relative to the domain baseline of MMseqs2.  In the domain of image generative models, the proposed method can identify near-duplicates in CIFAR-10 and in generated images, and with respect to generated images, models that do not generate near-duplicates produce lower quality output as measured by human judgment.  As well, near-duplicates can be found in cross-matches between CIFAR-10 and generated images, indicating memorization, and the paper demonstrates a strong negative correlation between training examples with low score (contributing least to data set diversity) and likelihood of being reproduced/memorized by the models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Demonstrates applicability over three different domains (one in appendix).  Findings have relevance/applicability to each domain, eg strong clustering results on protein sequences relative to MMseqs2, and proper evaluation metrics in the presences of duplicates for image generative modeling."}, "weaknesses": {"value": "Would be interested to see (possibly deferred to the appendix) slightly more discussion on any potential issues in generating the Vendi scores.  For instance, how was it decided whether to use q=0.1 or 0.5, and how sensitive were the results to the value of q.  Or, how sensitive are the results to the particular chosen data embedding?"}, "questions": {"value": "see weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ySJsXi8ph5", "forum": "bz9fu4cL3W", "replyto": "bz9fu4cL3W", "signatures": ["ICLR.cc/2026/Conference/Submission22063/Reviewer_TgHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22063/Reviewer_TgHy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942546288, "cdate": 1761942546288, "tmdate": 1762942040624, "mdate": 1762942040624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewers"}, "comment": {"value": "Thank you to the reviewers for their thoughtful comments. The reviewers made a number of great suggestions. We have incorporated these into our draft and uploaded an updated version of the paper. \n\nIn particular, reviewers F2E2, TgHy, and vUBK all noted that the Vendiscope can be sensitive to the choice of kernel function, embedding model, and Vendi Score order $q$. We have therefore added 3 additional figures.\n\nFigure 6: A synthetic 2D experiment highlighting how the Vendiscope measures rarity for different choices of $q$. We show that for all finite values of $q$, the Vendiscope measures rarity similarly, with small values of $q$ being the most sensitive to individual samples. For $q=\\infty$, the Vendiscope focuses on the distance of samples from the largest mode. We have updated Section 2.3 to include this analysis.\n\nFigures 7 and 8: Demonstration of how the choice of embedding and kernel function will lead to different Vendiscope rankings on ImageNet classes. Embeddings generated from DINOv2 and Inception focus on semantics, whereas alternative methods focus on colors.\n\nWe have also updated the explanation for certain figures in the text. All updates are highlighted in red. \n\nWe look forward to continuing to engage with the reviewers throughout the rebuttal period!"}}, "id": "qYCv2nnLYL", "forum": "bz9fu4cL3W", "replyto": "bz9fu4cL3W", "signatures": ["ICLR.cc/2026/Conference/Submission22063/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22063/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22063/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763694402050, "cdate": 1763694402050, "tmdate": 1763694402050, "mdate": 1763694402050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the concept of algorithmic microscope, termed vendiscope, that uses probability-weighted vendi scores to measure data-point contribution to the diversity in the dataset. Using this framework, the paper investigates the diversity of protein sequences and Alphafold’s inability to predict rare proteins, and also highlights on the generative model’s memorization aspect on CIFAR-10 data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper is overall easy to understand. \n\n+ The paper conducted a rich amount of experiments."}, "weaknesses": {"value": "- The main weakness lies in the contribution of the paper. Using vendi scoring to quantify the diversity of datasets does not seem like a significant contribution. The paper neither leads to any new finding using this scheme. It is well known that Alphafold struggles to predict structure for proteins with low homologs. The generative model’s memorization aspect is also well-known. \n\n- It is not clear why CIFAR-10 is used for images, it is a small image dataset with limited diversity. The images are of very small resolution and there is no scene-centric images. I think ImageNet would have been the best choice. \n\n- Given the choice of small dataset for images, I am not sure which UniProt dataset it used for Proteins. For comparison with protein sequence clustering, only MMSeq2 was used despite the existence of many other widely-used methods like CD-Hit or deep embedding based methods.\n\n- The paper claims that it performed experiments on proteins, images, and materials (L478) where it clearly did not do anything on materials. These makes the work seem incomplete.\n\n- The organization of the paper is poor. The related work, which is supposed to serve as the background for the work, should come before method."}, "questions": {"value": "1. Why CIFAR-10 for images instead of ImageNet?\n2. Which UniProt dataset was used?\n3. How does the protein sequence clustering application work compared to the deep embedding clustering approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rChoybofJq", "forum": "bz9fu4cL3W", "replyto": "bz9fu4cL3W", "signatures": ["ICLR.cc/2026/Conference/Submission22063/Reviewer_YqP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22063/Reviewer_YqP7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974339297, "cdate": 1761974339297, "tmdate": 1762942039996, "mdate": 1762942039996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Vendiscope, an algorithmic microscope that quantifies each datapoint’s contribution to dataset diversity using probability-weighted Vendi Scores. It efficiently detects duplicates, rare samples, and memorization patterns across large datasets like UniProt and CIFAR-10. The approach offers a scalable, domain-agnostic tool for understanding data composition and model performance beyond conventional accuracy metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) This paper introduces a unified, scalable framework to quantify each datapoint’s contribution to dataset diversity across multiple domains.\n\n(2) It effectively bridges data analysis and model diagnosis by revealing redundancy, rarity, and memorization patterns using a single interpretable metric."}, "weaknesses": {"value": "See questions"}, "questions": {"value": "(1) Is there any justification or evidence of how the kernel type or hyperparameters affect the interpretability and stability of Vendiscope scores, as a positive semi-definite similarity kernel is assumed?\n\n(2) It is not quantified how this approximation impacts accuracy or reproducibility of the Vendiscope rankings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j2zmFdHSrD", "forum": "bz9fu4cL3W", "replyto": "bz9fu4cL3W", "signatures": ["ICLR.cc/2026/Conference/Submission22063/Reviewer_F2E2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22063/Reviewer_F2E2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208610059, "cdate": 1762208610059, "tmdate": 1762942038727, "mdate": 1762942038727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}