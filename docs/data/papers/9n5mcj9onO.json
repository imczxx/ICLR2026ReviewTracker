{"id": "9n5mcj9onO", "number": 24732, "cdate": 1758359780159, "mdate": 1759896752105, "content": {"title": "Bridging the Perception Gap: Probe-Guided Data Optimization Framework for Robotic Imitation Learning", "abstract": "Imitation learning allows robots to acquire complex manipulation skills from human demonstrations. However, traditional data collection methods often haven't account for the \"perceptual gap\" between humans and robots, which leads to models that don't perform as expected. To solve this, we introduce Policy-Intent Probe (PIP). This method trains a proxy model with a small amount of demonstration data, then quantifies the model's perceptual capabilities based on its policy distribution. Based on the model's feedback, we divide the task's operational space into a Robust Operation Zone (ROZ) and a Non-Robust Operation Zone (NROZ). By standardizing the trajectories in the NROZ and waiting until the robot enters the ROZ to perform precise operations, we have optimized the data collection trajectories. Furthermore, aided by PIP, we can correctly carry out subtask segmentation. We can supplement data collection based on subtask complexity, which enhances the model's generalization and robustness. By cleaning only the subtask data containing anomalous trajectories or failure, we minimize data loss. Based on an empirical evaluation on three real-world tasks, we proved that perceptual capabilities can affect a task's success rate and that arbitrary subtask decomposition can lead to negative consequences. Our model-in-the-loop data optimization framework can significantly boost the success rate of long-horizon precision manipulation tasks, enhance model robustness, and increase data collection efficiency.", "tldr": "A new method that optimizes the data collection process using a proxy model. This approach bridges the perceptual gap between humans and robots, improving data collection efficiency, model robustness, and success rates.", "keywords": ["Imitation Learning", "Manipulation", "Proxy Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8649d84de2279b3cfd28f462e218d2d41953cea0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a **Policy-Intent Probe (PIP)** method that addresses the \"perceptual gap\" between humans and robots in imitation learning. The key insight is that traditional data collection ignores what robots can actually perceive, leading to poor real-world performance. PIP analyzes a proxy model's action distribution using KDE to identify which workspace regions the robot can reliably operate in (Robust Operation Zones) versus where it struggles (Non-Robust Operation Zones). Using this feedback, the authors propose a model-in-the-loop data optimization framework that: (1) optimizes demonstration trajectories by standardizing movements in uncertain regions and performing precise operations only in reliable regions, (2) intelligently segments tasks at perceptually-stable points to ensure smooth subtask transitions, and (3) enables efficient subtask-level data augmentation and cleaning. Experiments on three real-world manipulation tasks show significant improvements in success rates and horizon scores compared to traditional data collection methods, demonstrating that accounting for robot perceptual capabilities is crucial for effective imitation learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important but often overlooked issue in robotic imitation learning - the mismatch between human perception during demonstration and robot perception during execution. The PIP method provides a principled way to quantify and visualize this gap through policy distribution analysis.\n- Practical model-in-the-loop framework: The ROZ/NROZ workspace categorization based on policy confidence is intuitive and actionable. This framework directly translates model insights into concrete data collection strategies, forming an efficient feedback loop rather than relying on blind data accumulation."}, "weaknesses": {"value": "- The method involves several empirical decisions that lack theoretical grounding or ablation studies, which raises concerns about reproducibility and generalizability:\n  - *Action Chunk Sampling* (lines 173-176): The number of samples $N$ for KDE analysis is not specified, nor is there ablation showing how $N$ affects the reliability of ROZ/NROZ classification.\n  - *Key Action Extraction* (lines 176-182): Why is the midpoint action ($a_{t+\\frac{H}{2}}$) chosen as representative? No justification is provided for this choice over alternatives.\n  - *ROZ/NROZ thresholds*: The criteria for classifying workspace regions as robust vs. non-robust are not explicitly defined. E.g., what KDE spread or entropy values (lines 302-304) iconstitute the boundary?.\n\n- While individual components (PIP, trajectory optimization, subtask decomposition) are explained, the paper lacks a clear, step-by-step description of the complete workflow (Fig.1 currently is not clear enough).\n\n- Ambiguous evaluation metrics. E.g., the *Horizon Score* definition in Sec.C.1 uses unclear notation (what are $x_i$ and $\\mu$ specifically?). The relationship between execution time and the score formula is not intuitive, making it difficult to interpret results.\n\n- No videos or supplementary materials are provided to visualize the claimed improvements (e.g., smoother trajectories, successful failure recovery, differences between ROZ and NROZ behaviors), which would significantly aid in understanding the practical benefits."}, "questions": {"value": "- This work relies on analyzing action distributions from stochastic policies (flow-matching). How would PIP apply to more deterministic policy architectures?\n\n- The current KDE analysis is limited to 2D visualization after PCA projection. For future scenarios with higher-dimensional action spaces (e.g., 20+ DoF dexterous hands, or the human muscle system described at [https://clonerobotics.com/android](https://clonerobotics.com/android) and [1]), would 2D projection still capture sufficient information for reliable ROZ/NROZ classification? Have you considered alternative dimensionality reduction methods or higher-dimensional density estimation techniques that could better preserve the structure of complex action manifolds?\n\n[1]. Wei, Y., Zhuang, S., Zhuang, V., & Sui, Y. Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning. In The Thirteenth International Conference on Learning Representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review is needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sHLH14b21W", "forum": "9n5mcj9onO", "replyto": "9n5mcj9onO", "signatures": ["ICLR.cc/2026/Conference/Submission24732/Reviewer_xJ9A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24732/Reviewer_xJ9A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710182179, "cdate": 1761710182179, "tmdate": 1762943179629, "mdate": 1762943179629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a data optimization framework to close the perception gap between human demonstrations and a robot’s visual-policy competence. It introduces Policy-Intent Probe (PIP), a lightweight proxy model trained on a small set of dataset, whose action distribution is used to diagnose where the policy is perceptually reliable versus fragile. Using this probe, the authors partition the task workspace into a Robust Operation Zone (ROZ) and a Non-Robust Operation Zone (NROZ), with additional techniques to standardize, segment, and clean trajectories. Across three real-world long-horizon manipulation tasks, the probe-guided loop improves success rates, robustness, and data efficiency compared to unguided imitation-learning collection."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Performing data curation is important for the robotic imitation learning problem. The paper proposed some techniques to identify the OOD predictions in the model and use these techniques to standardize, segment, and clean trajectories. \n\n- The paper validates the idea on real, long-horizon manipulation with measurable gains in success and data efficiency, suggesting practical relevance beyond simulator benchmarks."}, "weaknesses": {"value": "- The PIP, definitions of ROZ and NROZ, and subtask decomposition techniques in this paper are mostly based on human heuristics, which makes the pipeline computationally inefficient (querying human feedback) and biased. The overall engineering burden is nontrivial.\n\n- The motivations for designing each component in the pipeline are not well explained, making it unclear what specific questions they are tackling in the robotic imitation learning scenario. See the question part for details.\n\n- Most specific concepts and detailed methods are not clear from the texts, e.g., perceptual gap, ROZ and NROZ, subtask-level data augmentation. See the question part for details.\n\n- Since the paper focuses on empirical studies, including preliminary open-source code and checkpoints in the submission would strengthen the conclusions."}, "questions": {"value": "- line 166 - line 168: What is the perceptual gap between robots and humans in particular? Why are human expectations always good? Any specific formulations of the “perceptual gap” or examples?\n- “Key Action Extraction & Dimensionality Reduction”: The action extraction and dimensionality reduction usually incur errors in the analysis. What’s the reason for not sticking with the original action space and comparing some quantitative metrics?\n- Fig 2: What is the purpose of conducting KDE analysis on PCA results? How to trade-off between the model prediction errors and variances in the data? Any quantitative comparisons?\n- Sec 3.2: Is the perceptual gap here referring to the distribution shift problem in imitation learning [1]?\n- Sec 3.3, Sec Sec 3.4 (b), and Sec 3.5: the definition of ROZ and NROZ,  subtask decomposition, and subtask-level decompositions are purely built on human heuristics, which can be computationally inefficient and biased.\n- Sec 3.4 (b): This heuristic might not be correct if there are multiple optimal paths leading to the desired goal. In this case, even in the ROZ, the entropy of the policy might be high. The maximum entropy RL exactly aims at learning this kind of policy [2, 3].\n- Sec 3.5.1: It is not clear what is “subtask-level data augmentation” from the texts.\n\n[1] Ross, Stéphane, Geoffrey Gordon, and Drew Bagnell. \"A reduction of imitation learning and structured prediction to no-regret online learning.\" In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference Proceedings, 2011.\n\n[2] Levine, Sergey. \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv preprint arXiv:1805.00909 (2018).\n\n[3] Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" In International conference on machine learning, pp. 1861-1870. Pmlr, 2018."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sxDxcuuaml", "forum": "9n5mcj9onO", "replyto": "9n5mcj9onO", "signatures": ["ICLR.cc/2026/Conference/Submission24732/Reviewer_gaoE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24732/Reviewer_gaoE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876810349, "cdate": 1761876810349, "tmdate": 1762943179175, "mdate": 1762943179175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores how imitation learning pipelines can account for the robot's capabilities during the data collection process. The policy intent probe (PIP) models the demonstration data coverage. The PIP is used to decompose the state space into robust and non-robust operation zones (ROZ and NROZ). This decomposition enables targeted subtask-level data collection and cleaning. Finetuning results on VLA models demonstrate the effectiveness of using the PIP model in the data collection process."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Improving the data collection process for VLAs is an important problem studied by this paper.\n\n1. The data collection process produces substantial gains over a baseline data collection process in Table 2."}, "weaknesses": {"value": "1. The paper lacks many details about the MIL-DOF framework, which is significant since this is how the PIP is used in the data collection process. Firstly, the paper does not describe the subtask-level data augmentation in Section 3.5.1. What does it mean to perform targeted data collection (L340)? How does this targeting manifest in the 100 episodes collected in Section 4? How is the subtask complexity measured (L340)? What does it mean to augment the model's capability on harder subtasks (L345)? What is proactive augmentation of data in OOD regions (L348)? How are such regions determined? These are only a few of the examples of important details omitted throughout Section 3.5. The same issues are in Section 3.5.2 with the subtask-level data cleaning. Overall, the details in this section are crucial for understanding how PIP is used in the data collection process, which I believe is the primary contribution of this paper. \n\n1. The figures lack details. There is no y-axis in Fig 6a. The illustrations in Fig6b,c are never defined. What are the lines of Fig 7c describing? \n\n1. The paper continually refers to \"model's perceptual capabilities,\" but this term is never clearly defined, and I could not infer its definition from its usage in the paper. At first, I thought it referred to the gap between the visual perception of the demonstrator and the robot. But this didn't make sense in the context of the PIP, which only relies on the action distribution. The paper needs to clarify which parts of the data collection the framework is solving.\n\n1. The lack of clarity also holds throughout Section 4. How are subtasks manually segmented in Plane B? What does it mean to \"consider the model's perception capabilities\" (L418)? \n\n1. Partially due to the method and experimental setting lacking many important details, the paper does not sufficiently analyze why the new data collection process improves the performance. Why does the method help in high-precision operations, and how does the PIP specifically help with this?\n\n1. The framework is insufficiently evaluated on only 3 tasks with 100 episodes per task. The proposed framework must be evaluated on additional tasks to compare with prior work. Furthermore, since the contribution is for data collection, it is crucial to compare how the performance changes as the data budget changes. Not only performance under the fixed 100-episode budget.\n\n1. The main paper does not define what the key metrics of horizon and success score are. Horizon score is not a standard term from prior literature and must be defined in the main text."}, "questions": {"value": "1. What do the authors mean by \"plane\" in Section 4.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yis0Xlindd", "forum": "9n5mcj9onO", "replyto": "9n5mcj9onO", "signatures": ["ICLR.cc/2026/Conference/Submission24732/Reviewer_426B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24732/Reviewer_426B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936021945, "cdate": 1761936021945, "tmdate": 1762943178857, "mdate": 1762943178857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new paradigm to collect and select data for robot imitation learning. The approach trains a second model with a subset of the data and then checks the models \"perceptual capabilities\" by sampling many actions and computing a KDE on a subset of the action chunk. If the ground truth action has a low probability, the state is categorized as \"non-robust operation zone\". The data trajectory in the non-robust zone is then \"regularized\", but its not specified what that means. The categorization is used to decompose the task into subtask and use this decomposition to filter outlier trajectories. Both methods are evaluated on training the pi0 and pi0.5 model on 3 real world robotic tasks. The authors report significantly improved performance using such regularization and data selection techniques."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The real robot results seem to be promising. \n- The performance of SOTA VLA models is significantly improved."}, "weaknesses": {"value": "- The paper is very hard to follow and it is unclear what is really happening. Most information of what the algorithm really does is missing, e.g., how exactly is the non-robust zone defined, how exactly is the task decomposition done and how is that used to gather more data. The description is done on a very high level, more technical details need to be added.\n- The whole story is about the models perceptual capabilities, but the methods that are used do not test perceptional capabilities at all. The KDE rather tests which samples are outliers and where we have more variance in the demonstration. The whole motivation of the approach is not convincing. \n- The KDE approach seems very adhoc as we only us a subset of the action chunking to compute the likelihood. The paper proposes to use a \"representative action\" for the KDE, but this choice seems arbitrary. Why not use the PCA on the full action chunk? Why not use better density estimation methods such as normalizing flows?"}, "questions": {"value": "- After reading the paper, I really do not know how the algorithm works. An algorithm box would be very helpful. Also, an exact mathematical definition of when to detect the non-robust zone and how to do the task segmentation would be helpful. This needs to be specified much more formally, \"entropy being extremly low or high\" is not an exact statement. \n- I am not convinced whether the proposed algorithm has anything to do with the perecptual capabilities of the model. There can be tons of other reaons why a ground truth sample has low likelihood in the KDE, such as outliers, not enough training iterations etc... I would rather say that if the perceptional capabilities are limited the ground truth sample should still be covered by the distribution, just the distribution gets a higher variance. Yet, high variance could also be explained by different solutions in the demonstration space. The authors need to justify much better why the algorithm checks the perceptual capabilities (or change the story accordingly.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8akRVkaShG", "forum": "9n5mcj9onO", "replyto": "9n5mcj9onO", "signatures": ["ICLR.cc/2026/Conference/Submission24732/Reviewer_iDCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24732/Reviewer_iDCT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237257866, "cdate": 1762237257866, "tmdate": 1762943178560, "mdate": 1762943178560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}