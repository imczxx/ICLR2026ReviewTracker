{"id": "HdEpZE3wFa", "number": 2518, "cdate": 1757131742450, "mdate": 1763515169259, "content": {"title": "Beyond Skeletons: Learning Animation Directly from Driving Videos with Same2X Training Strategy", "abstract": "Human image animation aims to generate a video from a static reference image, guided by pose information extracted from a driving video. Existing approaches often rely on pose estimators to extract intermediate representations, but such signals are prone to errors under occlusion or complex poses. Building on these observations, we present DirectAnimator, a framework that bypasses pose extraction and directly learns from raw driving videos. We introduce a Driving Cue Triplet consisting of pose, face, and location cues that captures motion, expression, and alignment in a semantically rich yet stable form, and we fuse them through a CueFusion DiT block for reliable control during denoising. To make learning dependable when the driving and reference identities differ, we devise a Same2X training strategy that aligns cross-ID features with those learned from same-ID data, regularizing optimization and accelerating convergence. \nExtensive experiments demonstrate that DirectAnimator attains state-of-the-art visual quality and identity preservation while remaining robust to occlusions and complex articulation, and it does so with fewer computational resources. Our project page is at https://directanimator.github.io/.", "tldr": "", "keywords": ["Generative Models", "Human Motion Synthesis", "Representation Alignment", "Pose-Free Animation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf8887a35844a8562be73219d7cbf43c1455a91b.pdf", "supplementary_material": "/attachment/30cba38b1e9af6212ecfc4031a75db3968acdc9a.zip"}, "replies": [{"content": {"summary": {"value": "Human image animation generates videos from static images using motion cues from driving videos, but traditional pose-based methods suffer from errors inherent in pose extraction.\nDirectAnimator eliminates pose extraction and learns directly from raw videos through a Driving Cue Triplet—pose, face, and location cues—fused via a CueFusion DiT block.\nA Same2X training strategy aligns features, enabling stable and efficient learning.\nDirectAnimator achieves state-of-the-art performance on the Unseen (internet-crawled) dataset and competitive results on the TikTok dataset."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By conditioning directly on raw videos, the model effectively addresses errors that arise during pose extraction.\n2. Inspired by REPA, REPA-E, and SRA, it performs latent alignment, which improves both training speed and overall performance.\n3. Incorporating a face cue helps maintain facial identity consistency, leading to more natural and expressive animations."}, "weaknesses": {"value": "1. It is difficult to identify clear advantages over StableAnimator[1] from the comparison videos presented on the project page.\n\n2. Although the paper claims to propose a Human Image Animation (HIA) method that overcomes the limitations of skeleton-based pose extraction, it appears that the training process relies on results generated by StableAnimator, which itself depends on skeleton pose extraction.\n\n3. Minor Suggestion: In Equation (4), it seems that the parameter D, which is mentioned in Table 3, is missing.\n\n[1] Tu, Shuyuan, et al. \"Stableanimator: High-quality identity-preserving human image animation.\", CVPR 2025."}, "questions": {"value": "1. Regarding the dependency on StableAnimator [1] mentioned in Weakness 2, what is the reason that the proposed method is not considered to be bound by the performance of StableAnimator?\n\n2. Even though cross-ID training starts from a model pretrained on Same-ID data, why does using the S2X loss in Eq. (4)—which guides the latent representations to follow those of the Same-ID trained model—result in faster and better training? Could you provide an explanation?\n\n3. In L207, a low-pass filter is used to remove high-frequency information. However, in Figures 2–5, the pose cues appear to have only lost color information while retaining most appearance details. Could you clarify why this happens?\n\n4. In L222, spatial alignment is adopted from StableAnimator, but I could not find an explanation of this in the StableAnimator paper after a quick scan. Could you specify the exact section or provide a brief description? Also, with the location cue, would your method be capable of performing HIA on chibi characters, similar to what TCAN[2] demonstrated?\n\n[1] Tu, Shuyuan, et al. \"Stableanimator: High-quality identity-preserving human image animation.\", CVPR 2025.\n\n[2] Kim, Jeongho, et al. \"Tcan: Animating human images with temporally consistent pose guidance using diffusion models.\", ECCV 2024."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "I’m worried that the human images shown in Figure 5 (row 2), which come from the Unseen (internet-crawled) dataset, might raise privacy concerns and may not be appropriate to include in the figure."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jus0IU5oT3", "forum": "HdEpZE3wFa", "replyto": "HdEpZE3wFa", "signatures": ["ICLR.cc/2026/Conference/Submission2518/Reviewer_wkJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2518/Reviewer_wkJD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727636763, "cdate": 1761727636763, "tmdate": 1762916264610, "mdate": 1762916264610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DirectAnimator, a framework for Human Image Animation (HIA) that seeks to overcome the instability and limited expressiveness of traditional skeleton-based driving signals. The core contribution is the replacement of explicit pose estimators with a Driving Cue Triplet (Pose Cue, Face Cue, and Location Cue) derived directly from raw driving video frames. The method also proposes a Same2X training strategy to stabilize cross-identity animation by aligning internal feature representations. The authors report state-of-the-art performance on several quantitative metrics. Despite the promising results, the methodological concerns outlined in the weaknesses section are fundamental and require a major overhaul and deeper analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of replacing sparse, error-prone keypoint estimation with a dense, segmented, and filtered foreground (Pose Cue) is a clever and potentially more robust approach to motion transfer. It directly addresses artifacts common in skeleton-based methods。\n2. The quantitative results (lower FVD, higher SSIM, etc.) reported in the ablation and comparison studies suggest that DirectAnimator achieves high visual quality and identity preservation, outperforming several recent strong baselines。"}, "weaknesses": {"value": "1. The Same2X training strategy is somewhat similar to the approach used in X-Portrait, which also involves generating pseudo driving samples for cross-identity training. While effective, this core idea of leveraging synthetic data for generalization is not entirely novel in the field.\n2. There appears to be a low visual quality in both the video demonstrations and the illustrated figures within the paper. Specifically, I observe that identity preservation in the figures is insufficient, and the clothing in the demo video still exhibits noticeable artifacts."}, "questions": {"value": "1. Please provide examples and analysis of the specific failure modes of DirectAnimator, particularly under conditions where the input segmentation model (Grounded SAM) produces poor results (e.g., severe occlusion, complex background, or extreme motion blur).\n2. Could you provide a clear and obvious sample of the spatial mismatch challenge, such as a case with a large-scale driving video paired with a small-scale reference (source) image? My understanding is that, during the cross-ID training stage, the pseudo driving cues generated by the StableAnimator are designed to follow the body shape of the original driving video (which shares the identity of the reference image). If this is the case, how does this process fundamentally address the spatial alignment problem? If the StableAnimator already effectively resolves the spatial mismatch problem, how does the proposed method (DirectAnimator) differ, and is its primary function essentially a distillation of the StableAnimator architecture or its capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a5tk2KLSIx", "forum": "HdEpZE3wFa", "replyto": "HdEpZE3wFa", "signatures": ["ICLR.cc/2026/Conference/Submission2518/Reviewer_e8ax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2518/Reviewer_e8ax"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775378945, "cdate": 1761775378945, "tmdate": 1762916264242, "mdate": 1762916264242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method for human animation, with its primary contribution being an alignment training approach that facilitates the disentanglement of motion and identity (Same2X training strategy). Furthermore, to enable fine-grained motion transfer including detailed elements such as facial expressions, the method incorporates several carefully designed visual cues (face cue, location cue and pose cue). Comprehensive experiments validate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The idea to disentangle the motion and the idnetity in the feature space by distillation is interesting.\n\n+ The experimental results show that the proposed method is promising. The videos presented in the supplementary material are SOTA."}, "weaknesses": {"value": "+ I appreciate the insight behind the Same2X training strategy, though I have concerns regarding the reliability of the pseudo data used to support the cross-identity training process.\n\n+ while the transfer of facial expressions is highlighted as a key contribution in the Introduction (Lines 62–65), there appears to be no dedicated metric evaluating pose or facial landmark accuracy. It is important to note that identity preservation (measured by FIS and FTS) is not equivalent to expression or pose transfer. Therefore, relying solely on FIS and FTS is insufficient to fully validate this aspect of the method.\n\n### Some suggestions\n\n+ The contribution of the paper is not clearly stated (this is also one of the major concern). Although the method demonstrates strong performance, the novelty remains unclear. The introduction of visual cues seems more like an engineering refinement rather than a core research contribution. It would be helpful to explicitly distinguish what fundamentally makes this approach apart from prior human animation methods.\n\n+ The presentation could also be improved. The connection between the proposed method (Lines 74–90) and the challenges outlined earlier (Lines 60–73) is not well corresponded. The Introduction section would benefit from further refinement to better link the **problem statement** with the **proposed solution**.\n\nI will increase the score if the presentation is improved."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCI4JZyMpV", "forum": "HdEpZE3wFa", "replyto": "HdEpZE3wFa", "signatures": ["ICLR.cc/2026/Conference/Submission2518/Reviewer_ebkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2518/Reviewer_ebkb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972438142, "cdate": 1761972438142, "tmdate": 1762916264040, "mdate": 1762916264040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DirectAnimator, a human image animation framework that bypasses explicit pose estimation by directly learning from raw driving videos. The method introduces a Driving Cue Triplet (pose, face, location cues) integrated via a CueFusion DiT block, and a Same2X training strategy that aligns cross-identity features using same-identity representations. Experiments demonstrate state-of-the-art performance on TikTok and Unseen datasets in visual quality and identity preservation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ This paper is well written and easy to follow.\n\n+ Directly using raw video pixels as driving signals effectively avoids error accumulation from pose estimation in the Human Image Animation (HIA) domain.\n\n+ The Same2X alignment loss significantly accelerates cross-identity convergence (6.7× faster) and enhances generalization compared to previous work like REPA.\n\n+ Experimental results show DirectAnimator outperforms recent SOTA methods across multiple metrics (FID, FIS, FTS, FVD)."}, "weaknesses": {"value": "- While innovative in full-body HIA, the paper overlooks prior face animation works that already bypass explicit pose estimation, such as LIA [1*] which uses latent space motion trajectories, and AniTalker [2*] which employs identity-disentangled motion representations. These methods demonstrate alternative \"pose-free\" approaches that should be discussed.\n\n- Components of the Driving Cue Triplet build on existing ideas, face cropping resembles X-Dyna [3*], and foreground segmentation has been used in prior video generation works.\n\n- Cross-ID training relies on pseudo cues from StableAnimator and Face-Adapter, potentially inheriting their biases without sufficient analysis of error propagation.\n\n- Cross-dataset evaluation on diverse real-world benchmarks (e.g., UCF101, FineGym) with complex motions is missing and limits claims of generalizability.\n\n- Analysis of inference speed compared to skeleton-based methods is missing, despite the additional preprocessing costs of segmentation and filtering.\n\nReferences:\n\n[1*] Learning to Animate Images via Latent Space Navigation. ICLR, 2022.\n\n[2*] AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding. MM, 2024.\n\n[3*] X-Dyna: Expressive Dynamic Human Image Animation. CVPR, 2025."}, "questions": {"value": "- How does the model perform when the driving video contains heavy motion blur or low resolution, given that the pose cue relies on segmentation and filtering?\n\n- What are the limitations of this work, are there any failure cases analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vGqv4D2gGO", "forum": "HdEpZE3wFa", "replyto": "HdEpZE3wFa", "signatures": ["ICLR.cc/2026/Conference/Submission2518/Reviewer_8a4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2518/Reviewer_8a4X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012315193, "cdate": 1762012315193, "tmdate": 1762916263640, "mdate": 1762916263640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}