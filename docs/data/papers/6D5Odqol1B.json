{"id": "6D5Odqol1B", "number": 9234, "cdate": 1758115977651, "mdate": 1763725164484, "content": {"title": "CASteer: Cross-Attention Steering for Controllable Concept Erasure", "abstract": "Diffusion models have transformed image generation, yet controlling their outputs for diverse applications, including content moderation and creative customization, remains challenging. Existing approaches usually require task-specific training and struggle to generalise across both concrete (e.g., objects) and abstract (e.g.,4 styles) concepts. We propose CASteer (Cross-Attention Steering), a training-free framework for controllable image generation using steering vectors to influence a diffusion model’s hidden representations dynamically. CASteer precomputes concept-specific steering vectors by averaging neural activations from images generated for each target concept. During inference, it dynamically applies these vectors to modify outputs only when necessary, either removing undesired concepts from images where they appear or adding desired concepts to images where they are absent. This selective activation ensures precise, context-aware adjustments without altering unaffected regions. This approach enables precise control over a wide range of tasks, including removing harmful content, interpolating between desired attributes, replacing objects, all without model retraining. CASteer outperforms state-of-the-art techniques while preserving unrelated content and minimising unintended effects. Code is provided in the supplementary", "tldr": "", "keywords": ["steering", "diffusion", "control", "erasure"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/161d90de290b235c9452997948179d7863f30336.pdf", "supplementary_material": "/attachment/6638b0d5762aceb0fc9a217ba058ec416e1d5a81.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CASteer, an activation steering method for concept erasure in diffusion models. It extracts steering vectors from cross-attention layers in the diffusion model for the concept to be erased and applies them during inference for erasure. Experiments are conducted for erasure of abstract concepts (nudity, violence) and objects (snoopy, mickey) on I2P benchmark and COCO dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed activation steering method for concept unlearning is training-free. \n\nAdapting the method to novel concepts is straightforward and does not require retraining. \n\nExperiments on I2P benchmark for SDv1.4 model shows the effectiveness of the proposed method over a comprehensive set of prior methods."}, "weaknesses": {"value": "The proposed method is not new as it applies the idea of activation steering which is well-established in LLMs to concept unlearning in diffusion models. So, the novelty is limited. \n\nThere is no theoretical justification on why operating in the latent space of diffusion models is better. It does not fully justify why unlearning in the cross-attention activation space (latent) is fundamentally better than guidance or prompt-based methods. \n\nIs it sensitive to the noise seed of the diffusion model? Are the steering vectors extracted for random noise seeds?\n\nHow does the method work if there is a mismatch between the number of steps used in steering vector extraction and during inference?\n\nMissing ablation study on steering with only one vector per CA layer in SDv1.4.\n\nConcept erasure based on guidance or steering is susceptible to adversarial attacks via concept addition or subtraction. Is this robust to adversarial attacks based on concept arithmetic [1] ?\n\nResults are only reported for few concepts (nudity, violence, snoopy, mickey). Missing erasure results on art styles (e.g., van gogh) or more abstract concepts (e.g., summer, mosaic style etc).\n\nComparisons are only reported for SDv1.4 model. Missing comparisons to prior methods on more recent models such as SDXL, SANA or SD3.5\n\nPaper can be better organized. Important ablations from supplementary need to be included in the main paper or the appendix instead of supplementary paper.\n\n[1] Petsiuk et. al. Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models, ECCV 2024\n\nMinor: \nReference to tables in main paper (e.g., table 15) is missing."}, "questions": {"value": "See weaknesses above.\n\nHow are the multiple prompts generated?\n\nDo the steering vectors generated for SDv1.4 work for SD v1.5 across different checkpoints of the same model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ffvcgbRI0A", "forum": "6D5Odqol1B", "replyto": "6D5Odqol1B", "signatures": ["ICLR.cc/2026/Conference/Submission9234/Reviewer_dKH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9234/Reviewer_dKH9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598206992, "cdate": 1761598206992, "tmdate": 1762920889572, "mdate": 1762920889572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed Cross-Attention Steering (CASteer), a training-free framework for concept erasure in diffusion models using steering vectors to influence hidden representations dynamically. Specifically, CASteer cleverly designs an algorithm for constructing steering vectors for new concepts and leverages these steering vectors to suppress unwanted image features without retraining. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The writing is fluent and logically coherent, exhibiting strong readability.\n* The proposed method is highly efficient, requiring no training or fine-tuning while achieving excellent performance.\n* The proposed method exhibits strong generalization and is applicable to various text-to-image models that incorporate cross-attention mechanisms.\n* Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "* As shown in Figure 1, CASteer computes a steering vector for the output of every cross-attention layer at every timestep in the generation process and applies a correction. Could this be somewhat excessive? Would it be possible to experimentally analyze whether the number of corrected timesteps and CA layers can be reduced to improve efficiency?\n* The reviewer examined the examples provided in the main paper as well as the prompt set in the appendix, and observed that the subjects used when constructing the prompts are relatively simple. This may lead to high variability in the generated outputs, which in turn could make the steering vector estimation unstable. For example, in Figure 1, when using “dog” as the subject, the model generates a brown dog as the positive sample and a white dog as the negative sample; this could cause the steering vector to incorrectly treat other attributes of the dog as part of the target concept. Would using more specific subjects yield a more stable and robust steering vector?\n* It would be very helpful to provide some positive visual examples of erased abstract concepts, such as “Van Gogh style.”"}, "questions": {"value": "See 'Weaknesses'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jSi5f90n9J", "forum": "6D5Odqol1B", "replyto": "6D5Odqol1B", "signatures": ["ICLR.cc/2026/Conference/Submission9234/Reviewer_ezmt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9234/Reviewer_ezmt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909073370, "cdate": 1761909073370, "tmdate": 1762920889088, "mdate": 1762920889088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CASteer, a training-free method for concept erasure in text-to-image diffusion models. Given an input prompt and a concept prompt, CASteer guides image generation to follow the input prompt while avoiding visual content associated with the concept prompt. The method computes steering vectors from the difference between cross-attention outputs when prompting the model with and without the concept. These vectors are applied at inference to steer generation in all the cross attention layers of the diffusion transformers. Experiments show that CASteer significantly outperforms prior baselines and is able to remove both abstract (e.g. nudity, inappropriate content) and concrete concepts (e.g. Mickey, Spongebob) and to generalize across models such as Stable Diffusion v1.4, SDXL, and SANA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The works presents a novel *training-free* approach to solve the problem of content erasure which improves over existing baselines both in term of performances and of practicality (no need to run an ad-hoc training). \n2. I find the idea of using steering vectors to *remove* concepts rather than *adding* them very interesting.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "I only have some minor concerns:\n\n1. The *extension to multiple concepts* (L272–L276) is not experimentally validated. It would be useful to assess performance as the number of erased concepts increases and/or their individual steering vectors substantially differ from each other.\n2. Equation 4 introduces *cosine-similarity weighting* between the text prompt and the erased concept; the paper does not analyze how crucial this weighting is in practice. An ablation could clarify whether simpler weighting performs comparably.\n3. Some *figures* could be improved. Figures 2 and 4 could benefit from clearer labeling of rows/columns and spacing can be increased among the sub captions of Figure 3."}, "questions": {"value": "1. How does CASteer perform when erasing multiple unrelated concepts or when steering vectors are different from each others?\n2. How critical is cosine-similarity weighting? Could fixed or learned scalars achieve similar results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "al2XYzu5xD", "forum": "6D5Odqol1B", "replyto": "6D5Odqol1B", "signatures": ["ICLR.cc/2026/Conference/Submission9234/Reviewer_vj4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9234/Reviewer_vj4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929131675, "cdate": 1761929131675, "tmdate": 1762920888569, "mdate": 1762920888569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CASteer, a training-free framework for erasing specific concepts from diffusion models. The core idea is to compute steering vectors that represent the direction of an unwanted concept in the model's cross-attention layers. During inference, these vectors are subtracted from the model's activations without requiring any model retraining. The authors did comprehensive empirical experiments to demonstrate the effectiveness of the proposed method across various concepts, prompts, and model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- a training-free framework using precomputed steering vectors to remove concepts from diffusion models.\n- comprehensive empirical evaluations across various concepts and models."}, "weaknesses": {"value": "- limited novelty regarding method, with strong assumptions on steering vectors’ linear compositionality.\n- ad-hoc parameter selection - steering vector scaling hyperparameters are fixed empirically.\n- no discussion of computational or memory trade-offs when constructing per-layer, per-step steering vectors.\n- somewhat limited improvements, for example, CASteer with clipping only surpassing second-based model Receler by 1.42%, which is not significant.\n- did not compare with unlearning baselines, such as [1,2]. \n\n[1] Wu, Yongliang, et al. \"Unlearning concepts in diffusion model via concept domain correction and concept preserving gradient.\n\n[2] Alberti, Silas, et al. \"Data unlearning in diffusion models.\" arXiv preprint arXiv:2503.01034"}, "questions": {"value": "- the hyperparameter $β=2$ is used for all experiments. How does the method's performance and stability vary with this parameter, especially across different concepts or model architectures?\n- the paper uses a large number of prompt pairs (50 for concrete, 196 for abstract concepts). How sensitive is the method to the quality and diversity of these prompt pairs? poorly chosen set of prompts could lead to an imprecise steering vector.\n- for multi-concept erasure, how do you resolve conflicting steering directions? is orthogonality assumptions still valid?\n- the results on using steering vectors for adding concepts (and style transfer) are rather limited, can you discuss why certain applications fails with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YEYhV6DLqT", "forum": "6D5Odqol1B", "replyto": "6D5Odqol1B", "signatures": ["ICLR.cc/2026/Conference/Submission9234/Reviewer_ZNEe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9234/Reviewer_ZNEe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987972634, "cdate": 1761987972634, "tmdate": 1762920888136, "mdate": 1762920888136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common answer"}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nWe genuinely think the reviewers for their work on helping us improve the paper. There were several nice suggestions and questions that made us dig deeper be it theoretically or empirically. While we apologize for the lengthy responses to Reviewer ZNEe and dKH9, we felt that it was needed to respond to each of their concerns, usually accompanied by one or more experiments, or a Theorem.\n\nWe made several additions and changes in the manuscript, all in blue text. In the individual rebuttals, we link to those changes and results. In particular, a non-comprehensive list of these changes is:\n\n- Adding the appendix to the main paper, instead of in supplementary.\n\n- Showing results for timing and memory performance.\n\n- Doing all the asked ablations (e.g., different number of diffusion steps, evaluating the choice of β, sensitivity to prompt choice, cosine similarity, more visual results such as in Van Gogh style, steering only some layers, robustness to seed, mismatch on number of steps, steering multiple concepts, comparing with unlearning methods such as DoCo, etc). While we note that most of the reviewers already said that our method is **comprehensively evaluated**, we believe that the new set of experiments make the experimental setup even more stronger.\n\n- We theoretically justified some of the choices be it in forms of Theorems  (see Appendix Sec. L for Theorem L.1 and Corollary L.1.1) or linking to the previous works (assumptions on steering vectors’ linear compositionality and orthogonality).\n\nConsidering the work done, we hope that the reviewers carefully revise the changes and the rebuttal. We are excited to further discuss with the reviewers in continuing to improve our paper."}}, "id": "Xcfw0A9vLS", "forum": "6D5Odqol1B", "replyto": "6D5Odqol1B", "signatures": ["ICLR.cc/2026/Conference/Submission9234/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9234/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission9234/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725071564, "cdate": 1763725071564, "tmdate": 1763725586991, "mdate": 1763725586991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}