{"id": "89RGVbULYt", "number": 2233, "cdate": 1757038732511, "mdate": 1759898161481, "content": {"title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning", "abstract": "Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19\\% in accuracy while reducing average reasoning length by 81\\% under an 8K window. The paradigm also benefits security and creative tasks.", "tldr": "", "keywords": ["Efficient Reasoning;Large Reasoning Models;Test-time-scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79b29309430fa14b793cd127b0b5e45dab411177.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper propose Think-with-Me, a test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process.\n- The approach achieves a better balance between accuracy and reasoning length under limited context windows compared with a series of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed idea is interesting and demonstrates improvements in both performance and reasoning length compared to the original model.\n\n- The paper is overall well-written and clearly presented."}, "weaknesses": {"value": "- The baselines used in the experiments are relatively simple. There exists a substantial body of work focusing on reducing reasoning length while maintaining or improving accuracy; more relevant and competitive baselines should be included for a fairer comparison.\n\n- The proposed interactive reasoning framework involves multiple models (and possibly human) interactions and introduces pauses during inference. Therefore, in addition to reasoning length and accuracy, the evaluation should also consider the actual total inference time and deployment cost (e.g., computation and resource usage). The core concern is not merely how long the reasoning chain is, but whether the proposed method reduces overall inference time and resource requirements.\n\n- Novelty: The core mechanism described in Algorithm 1 (Line 12) appears closely related to early stopping methods (e.g., Dynamic Early Exit in Reasoning Models). The paper does not fully clarify the differences between the proposed approach and these existing works. The authors should also explicitly discuss distinctions from other related adaptive reasoning or calibration methods such as SEAL and s1: Simple Test-time Scaling . Furthermore, comparisons with earlier multi-agent or multi-model interaction methods like think-critic are recommended."}, "questions": {"value": "- In Proposition 1, is the claim always guaranteed to hold? Why does the uncertainty necessarily decrease under the proposed formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVptrkHCoP", "forum": "89RGVbULYt", "replyto": "89RGVbULYt", "signatures": ["ICLR.cc/2026/Conference/Submission2233/Reviewer_Zmbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2233/Reviewer_Zmbd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961698549, "cdate": 1761961698549, "tmdate": 1762916155819, "mdate": 1762916155819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Think-with-Me, a test-time interactive reasoning framework designed to improve the efficiency of Large Reasoning Models (LRMs). The method pauses the model's reasoning process at transitional conjunctions (e.g., \"so\", \"wait\") and introduces external feedback from humans or LLMs to adaptively extend or terminate reasoning. They use Group Relative Policy Optimization (GRPO) to adapt the model to this interactive framework. Experiments on several reasoning benchmarks show that their method improves the trade-off between accuracy and efficiency within an 8K-token context window."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well-written and easy to understand.\n* This paper addresses an important issue in optimizing the test-time reasoning efficiency."}, "weaknesses": {"value": "1. My main criticism is **on the design of the experiments**:\n   a. In the observation experiments of Section 3.1 in Figure 1, the authors conduct their analysis on DeepSeek-R1-Distill-Qwen-32B, showing in Figures 1(a) and 1(b) the prevalence of conjunction tokens in the reasoning traces of **o1-like models**. However, when investigating the influence of these tokens, they switch their experimental model to Qwen2.5-72B-Instruct, which, as far as I know, does **not possess long reasoning ability**. In summary, this inconsistency makes the experimental methodology appear odd and the observation seems less reliable.\n   b. The main experiment shown in Table 1 also seems **unfair**, since their method uses external powerful models such as Qwen2.5-72B-Instruct, or even involves human feedback, while other baselines such as DeepSeek-R1 and QwQ-32B operate autonomously. This raises the question of whether the improvements come from their post-training process or from the additional external hints.\n2. **The use of GRPO is not well-motivated**. There is no clear explanation of why reinforcement learning is necessary here, nor any ablation comparing GRPO with simpler alternatives such as SFT on synthetic intervention traces. A comprehensive comparison across different methodologies would enhance the rigor and insight of their approach.\n3. The theoretical claim (Proposition 1) that feedback reduces conditional entropy seems trivial and does not explain why the designed interactive intervention helps reasoning. Moreover, there is insufficient theoretical or experimental evidence to suggest that intervening at conjunctions like \"so\" or \"wait\" leads to better performance than intervening elsewhere.\n4. The human-in-the-loop setting might not be scalable for large datasets or real-time inference. The paper does not estimate human cost or latency under realistic deployment conditions."}, "questions": {"value": "1. Regarding weakness 1.a, can the experiments in Figure 1 be carried out on the same model?\n2. Regarding weakness 1.b, can the authors compare with other baselines, such as also providing external suggestions from another powerful model, or directly inserting these interactive feedbacks into a non-finetuned model, or making modifications to make those feedbacks resemble the reasoning models' own intermediate criticism of their reasoning traces?\n3. Regarding weakness 2, can the authors compare the use of GRPO with other post-training techniques, such as SFT on synthetic traces?\n4. Can their method be applied to longer context windows beyond 8k tokens while maintaining the advantages shown in Table 1?\n5. Additionally, I wonder whether reasoning tasks are the most suitable application for the proposed interactive framework. For example, if one wants to use an LRM to solve a mathematical problem but lacks sufficient knowledge to solve it, will they have the ability to judge the correctness of intermediate reasoning results or even provide useful suggestions or feedback? Perhaps this framework is more suitable for applications like creative writing, where humans can more easily judge whether the current results align with their intentions. Please correct me if I am wrong.\n6. Since LLMs or humans might not have the ability to process long contexts, would it be better to summarize the intermediate reasoning trace before sending it for judgment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quqgq3tyUG", "forum": "89RGVbULYt", "replyto": "89RGVbULYt", "signatures": ["ICLR.cc/2026/Conference/Submission2233/Reviewer_7csP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2233/Reviewer_7csP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962176969, "cdate": 1761962176969, "tmdate": 1762916155650, "mdate": 1762916155650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the LLM long2short problem by introducing external feedback. Specifically, at logical connectives, a LLM or human annotator provides prompts indicating whether the current reasoning process is complete (i.e., has derived the correct answer) and sound (i.e., whether the reasoning path needs correction). This feedback guides the model to adaptively perform test-time scaling. In comparisons within an 8K context window, this approach demonstrates a notable reduction in the length of output tokens."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clear, and easy to follow.\n2. The proposed method achieves a significant improvement in compressing reasoning length.\n3. It provides both LLM-based and human-in-the-loop feedback mechanisms, enhancing the scalability of the approach."}, "weaknesses": {"value": "1. Several existing works, for example [1] have already explored, from various perspectives, the use of external feedback frameworks—such as human feedback, model feedback, or verifiers—to improve LLM training, and these approaches can be adapted to address the long2short problem. Therefore, the empirical motivation of this paper needs to be further strengthened.\n2. The paper's two key observations are not new: the first observation has been similarly articulated in numerous works since [2], and the second observation has also been mentioned in some recent works, such as [3].\n3. The paper lacks sufficient comparison with relevant baselines. Since[2], numerous long2short approaches have emerged—too many to list exhaustively here (see recent surveys for a comprehensive overview). Whether inference-time or training-based, the methods cited in the paper are not strongly representative of the state of the art. The authors should conduct careful comparisons against closely related approaches to properly demonstrate the advantages of their proposed method.\n\n\n[1] Self-Refine: Iterative Refinement with Self-Feedback\n[2] s1: simple test-time scaling\n[3] Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fWEBdjPP53", "forum": "89RGVbULYt", "replyto": "89RGVbULYt", "signatures": ["ICLR.cc/2026/Conference/Submission2233/Reviewer_NoWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2233/Reviewer_NoWM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986566595, "cdate": 1761986566595, "tmdate": 1762916155319, "mdate": 1762916155319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an innovative reasoning-enhanced framework, \"Think-with-Me,\" designed to address common issues in large reasoning models (LRMs), such as \"overthinking\" and \"overshoot\" during the reasoning process. The framework incorporates an external feedback intervention mechanism that identifies key transitional words (such as \"so\") as intervention points during reasoning. This allows the reasoning process to pause, enabling human or LLM agents to provide feedback, dynamically adjusting the depth and direction of reasoning, reducing redundant reasoning steps while maintaining or improving accuracy. To ensure the model can effectively respond to external feedback, the authors employed the Group Relative Policy Optimization (GRPO) method to fine-tune the model, enhancing its adaptability in interactive reasoning modes. Experimental results show that Think-with-Me performs excellently across various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1、The study fully adheres to a rigorous logical chain of scientific exploration. The team first systematically analyzed the model’s reasoning behavior, identified that \"transitional words\" can serve as intervention nodes, and validated their effectiveness. This rigorous preliminary exploration laid a theoretical foundation for the design of Think-with-Me, forming a scientific closed loop.\n2、The experimental validation is comprehensive, covering a wide range of tasks and comparing with various mainstream baseline models and efficient reasoning methods.\n3、The paper provides extremely detailed appendices, including complete training details, reward function design, case studies, and even schematic diagrams of the interactive interface. This openness not only greatly enhances the transparency and credibility of the work, but also provides a valuable foundation for subsequent research and reproduction."}, "weaknesses": {"value": "1、lthough the authors provide additional experimental details in the appendix, they do not release the source code, resulting in low reproducibility.\n2、his approach relies on an external feedback mechanism, which may introduce new risks. If the LLM proxy generates incorrect feedback and the target model lacks a built-in correction mechanism, performance degradation or error propagation could occur; the authors offer no further discussion on this issue."}, "questions": {"value": "1、In the Think-with-Me framework, feedback comes from humans or LLM proxies. If human subjectivity is too strong or the LLM's performance is poor, will the resulting erroneous feedback affect subsequent reasoning? If this problem exists, it should be pointed out in I.1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cVS0bj18wS", "forum": "89RGVbULYt", "replyto": "89RGVbULYt", "signatures": ["ICLR.cc/2026/Conference/Submission2233/Reviewer_HGp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2233/Reviewer_HGp3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138280197, "cdate": 1762138280197, "tmdate": 1762916154969, "mdate": 1762916154969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}