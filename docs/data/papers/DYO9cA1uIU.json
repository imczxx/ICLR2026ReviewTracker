{"id": "DYO9cA1uIU", "number": 5157, "cdate": 1757857477140, "mdate": 1763715787919, "content": {"title": "LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models", "abstract": "Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.", "tldr": "", "keywords": ["Large Multimodal Models", "Model Compression", "Fourier Domain", "Matrix Approximation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5b49a6432263738c6b9cca7b8100244f99304a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain to exploit de-correlation and conjugate symmetry, leading to more compact and accurate weight representations. Additionally, authors introduce PolarQuant that separately discretizes amplitude and phase in polar coordinates. Moreover, in order to eliminate the need for large-scale calibration data, authors derive an optional diagonal calibration scheme that approximated the Hessian with row/column means."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper, for the first time, integrates low-rank + quantization optimization in the fourier domain. \n\n- The proposed method using fourier approximation is elegant and has solid mathematical justification.\n\n- Clear pseudocode for algorithms are provided (line 233 - 252).\n\n- Superior performance achieved across different benchmarks."}, "weaknesses": {"value": "- The novelty is limited somehow and while Fourier-domain decomposition is interesting, it may be seen as a direct extension of existing LoRA + quantization frameworks.\n\n- Comparison with related baselines (QLoRA[1]) is underdeveloped.\n\n- Some qualitative or visualization results may enhance the presentation.\n\n- The ODC heuristic (row/column averaging) lacks strong theoretical or empirical justification. It’s unclear when this approximation fails or how sensitive the method is to distribution shifts.\n\n[1] Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). *QLoRA: Efficient Finetuning of Quantized LLMs."}, "questions": {"value": "- Referring to line 066 *\"we observe that the weight matrices of LMMs in the frequency domain have a more **compact** spread of singular values as compare to spatial domain.\"* , what is \"compact spread of singular values\"\n\n- The proposed method achieve exceptional performance in hallucination benchmark and can you elaborate the reason\n\n- Since important contribution of the paper is reducing the computational and memory costs, it would be better to move analysis to the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "41OMfTiOhC", "forum": "DYO9cA1uIU", "replyto": "DYO9cA1uIU", "signatures": ["ICLR.cc/2026/Conference/Submission5157/Reviewer_kWwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5157/Reviewer_kWwk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447260012, "cdate": 1761447260012, "tmdate": 1762917915267, "mdate": 1762917915267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets on efficient Large Multimodal Model (LMM). They bring Fourier approximation to decomposes the weight matrices into a\nlow-rank plus quantized weight, designing a efficient LMM framework called LLaVA-FA. They also design PolarQuant which is an amplitude-and-phase polar codec to quantize complex matrix, and an optional diagonal calibration (ODC) scheme to approximate Hessian Matrix. Extensive experiment results prove the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is written in a very high quality, the figures analyze the problem and illustrate the idea very clearly, especially Figure 1 and 3. \n2. I think this paper targets on an important problem, the efficient LMM. \n3. The idea is interesting and reasonable. I am happy to see Fourier approximation can be applied to LMM since it really has some good characteristics. \n4. The experiments are abundant and clear, proving the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Just one discussion. This paper choose Fourier approximation, and can we consider other type of approximation? I am happy to see more comparison results. \n2. The authors can have more discussions about the limitations and future work."}, "questions": {"value": "See weaknesses, especially discussions about the limitation and future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qlCz9JlcVK", "forum": "DYO9cA1uIU", "replyto": "DYO9cA1uIU", "signatures": ["ICLR.cc/2026/Conference/Submission5157/Reviewer_YXsM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5157/Reviewer_YXsM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480036221, "cdate": 1761480036221, "tmdate": 1762917914696, "mdate": 1762917914696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLaVA-FA, an integrated compression pipeline that merges low-rank and quantization in Fourier space, accompanied by a concrete algorithmic recipe and a complexity-aware design. More specifically, LLaVA-FA applies a 2D Discrete Fourier Transform (DFT), factorizes the largest spectral components with a low-rank complex SVD, quantizes the residual using a polar-coordinate codec (PolarQuant), and optionally weights the reconstruction objective with a diagonal calibration derived from row and column statistics (ODC). The experiments demonstrate favorable performance at small scales, accompanied by measured efficiency gains in latency, FLOPs, and KV-cache usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed LLaVA-FA is well-motivated, and supported by a clear theoretical framing.\n\n2. Efficiency evidence with concrete measurements. Specifically, latency, FLOPs, KV-Cache usage, and TTFT are reported, which aligns well with the goals of a model compression study."}, "weaknesses": {"value": "1. Limited ablation for ODC and calibration choices. The paper mentions that ODC removes the need for large calibration sets, but no direct comparison or ablation is presented to isolate this effect. Adding such results would make the claim more convincing.\n\n2. There are some fairness concerns regarding the baseline comparisons in Table 1. The amount of training data varies across methods, and some baselines use fewer samples than LLaVA-FA while achieving comparable performance.\n\n3. Discussion on a few related works seem to be missing. For instance, in the line of LMM efficiency “CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers, ICML 2024,” and in the line of vision-language weight compression “UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers, ICML 2023.”"}, "questions": {"value": "1. Do the authors plan to release their models?\n\n2. The current results are mainly on small-scale models. Would the authors consider including results on larger models to demonstrate scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OMTq1oGgzl", "forum": "DYO9cA1uIU", "replyto": "DYO9cA1uIU", "signatures": ["ICLR.cc/2026/Conference/Submission5157/Reviewer_tSjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5157/Reviewer_tSjP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848331887, "cdate": 1761848331887, "tmdate": 1762917914291, "mdate": 1762917914291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLaVA-FA, a compression framework for large multimodal models (LMMs) that addresses the limitations of existing methods, where decoupled low-rank decomposition and quantization often lead to compounded reconstruction errors. LLaVA-FA employs Fourier approximation to integrate low-rank decomposition and quantization within the frequency domain, leveraging two essential properties of the Fourier transform: de-correlation, which reduces spectral redundancy, and conjugate symmetry, which nearly halves parameter storage.To handle complex matrices in the frequency domain, the paper introduces PolarQuant—a polar-coordinate quantization method that discretizes amplitude and phase separately to preserve complex structure. It also proposes an Optional Diagonal Calibration (ODC) scheme, which approximates the full Hessian with row/column means to avoid reliance on large-scale calibration data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes LLaVA-FA, a compression framework for large multimodal models (LMMs) that addresses the limitations of existing methods, where decoupled low-rank decomposition and quantization often lead to compounded reconstruction errors. LLaVA-FA employs Fourier approximation to integrate low-rank decomposition and quantization within the frequency domain, leveraging two essential properties of the Fourier transform: de-correlation, which reduces spectral redundancy, and conjugate symmetry, which nearly halves parameter storage.To handle complex matrices in the frequency domain, the paper introduces PolarQuant—a polar-coordinate quantization method that discretizes amplitude and phase separately to preserve complex structure. It also proposes an Optional Diagonal Calibration (ODC) scheme, which approximates the full Hessian with row/column means to avoid reliance on large-scale calibration data."}, "weaknesses": {"value": "1. The paper proposes performing low-rank decomposition and quantization in the frequency domain via the Fourier transform, but the experimental section lacks comparisons with existing solutions in the spatial domain, limiting the credibility of its claimed competitiveness.\n2. The method shows limited generalization capability, as experiments are only conducted on 3B and 7B-scale LLMs (Qwen-2.5) without evaluation on larger-parameter models."}, "questions": {"value": "1. How does the proposed method perform when extended to models beyond Qwen-2.5?\n2. How is the calibration matrix C constructed in the paper, and has there been any ablation study on the amount of calibration data used?\n3. Could you elaborate on the compression time required for models of different sizes?\n4. The algorithm flow in the paper provides an option to disable ODC—how does the performance change when ODC is not used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wHrYNF5jmz", "forum": "DYO9cA1uIU", "replyto": "DYO9cA1uIU", "signatures": ["ICLR.cc/2026/Conference/Submission5157/Reviewer_xZPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5157/Reviewer_xZPt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874708037, "cdate": 1761874708037, "tmdate": 1762917914012, "mdate": 1762917914012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}