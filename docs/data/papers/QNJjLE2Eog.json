{"id": "QNJjLE2Eog", "number": 10221, "cdate": 1758164359850, "mdate": 1762926784343, "content": {"title": "Towards open environments and instructions: general vision-language navigation via fast-slow interactive reasoning", "abstract": "Vision-Language Navigation (VLN) aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods.\nTo this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.\nTowards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. \nRecent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. \nInspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework.\nThe fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory.\nThe slow-reasoning module analyze the memories generated by the fast-reasoning module. \nThrough deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning, it continually improves the accuracy and generalization ability of fast decisions. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios. Extensive experiments demonstrate the superiorities of our method.", "tldr": "", "keywords": ["Vision-and-Language Navigation，General Scene Adaptation，Fast-slow thinking"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9a7efab2561d608168807aca222c99f38ae6ff7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a fast–slow navigation framework. The fast module generates actions based on real-time observations and instructions, while the slow reasoning module refines the historical data into structured knowledge by analyzing key successes and failures. The framework appears both practical and intuitively sound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Current navigation methods often lack robustness and perform poorly in out-of-distribution (OOD) environments. The dual-system framework may offer a promising solution to this challenge by enhancing the generalization ability of navigation systems."}, "weaknesses": {"value": "I am looking for some experiments testing on real mobile robots. the current testing results is trying to show it has good performance on out of distribution data. I think this is the key motivation of the paper. how about training with offline dataset but testing in real world. like is it solving the simtoreal problem simultaneously?"}, "questions": {"value": "1. In the section “Experience Library Capacity (K)”, I wonder whether the choice of K depends on the type of scene. For instance, dynamic environments such as streets change rapidly, while more static settings like offices remain stable. The update frequency and the length of K of the slow system might therefore need to be adapted to the scene dynamics.\n2. Have you tested the framework on real mobile robots to validate its effectiveness beyond simulation?\n3. Have you observed any hallucinations in the system—either visual hallucinations in perception or LLM-related hallucinations in text-based reasoning or instruction interpretation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "25SupdV1aB", "forum": "QNJjLE2Eog", "replyto": "QNJjLE2Eog", "signatures": ["ICLR.cc/2026/Conference/Submission10221/Reviewer_BNmG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10221/Reviewer_BNmG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760500638788, "cdate": 1760500638788, "tmdate": 1762921581953, "mdate": 1762921581953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "b2gHlmV9cq", "forum": "QNJjLE2Eog", "replyto": "QNJjLE2Eog", "signatures": ["ICLR.cc/2026/Conference/Submission10221/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10221/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762926783023, "cdate": 1762926783023, "tmdate": 1762926783023, "mdate": 1762926783023, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the General Scene Adaptation for Vision-Language Navigation (GSA-VLN) task, aiming to improve generalization to unseen environments and diverse instruction styles. The authors propose Slow4Fast-VLN, an interactive fast–slow reasoning framework inspired by dual-process cognition. A DUET-based policy handles fast reasoning, while a LLaMA3.2-vision module conducts slow reflection to extract structured experience knowledge. Experiments on GSA-R2R show improvements over GR-DUET."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focuses on open-world generalization in VLN, a direction gaining traction in embodied AI and multimodal reasoning. The use of dual-process cognition (fast and slow reasoning) to structure VLN decision processes is interesting and aligns with ongoing efforts to bring cognitive inspiration into LLM-augmented agents.\n\n- The use of GSA-R2R, which extends R2R with out-of-distribution scenes and diverse instruction styles, is suitable for testing generalization.\n\n- The paper includes ablations (FSR and ISC modules, experience library capacity) and a qualitative case study that helps readers understand the intuition behind the slow-reflection process."}, "weaknesses": {"value": "- While the fast–slow reasoning framework is well-motivated conceptually, its technical realization mainly combines existing components (DUET backbone + LLM-based reflection + attention fusion). The interaction between fast and slow reasoning is implemented as an experience retrieval and attention fusion module, which feels incremental rather than a fundamentally new reasoning paradigm.\n\n- The reported improvements over GR-DUET are small (typically +1–2% SR), which may fall within noise levels given the scale of evaluation. There is no clear demonstration that these gains stem from the proposed fast–slow interaction, rather than from using a larger or more capable LLM (LLaMA3.2-vision).\n\n- The claim of interactive reasoning remains somewhat concerns: the slow reasoning appears to be executed offline between episodes, not in real time. It is unclear whether the learned experience embeddings truly generalize to unseen scenes or simply act as additional memory augmentation.\n\n- The instruction style conversion module seems to rely heavily on LLM rewriting; however, the paper lacks analysis on how conversion quality affects navigation accuracy. The mechanism might introduce confounding variables (e.g., LLM paraphrasing fidelity) that are not controlled or discussed."}, "questions": {"value": "- Can the authors show quantitative evidence that the slow reasoning directly improves future fast decisions (e.g., ablations without memory fusion)?\n\n- How is the LLM’s contribution isolated from the architecture itself? Would smaller models achieve similar gains?\n\n- Why are more recent LLM-based VLN methods not included as baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZFg8JupLCq", "forum": "QNJjLE2Eog", "replyto": "QNJjLE2Eog", "signatures": ["ICLR.cc/2026/Conference/Submission10221/Reviewer_ejei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10221/Reviewer_ejei"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840540646, "cdate": 1761840540646, "tmdate": 1762921581534, "mdate": 1762921581534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Slow4Fast-VLN**, a novel *fast–slow interactive reasoning framework* for the **General Scene Adaptation Vision-Language Navigation (GSA-VLN)** task. Traditional VLN methods assume a closed world where training and testing share similar environments and instruction styles, which limits adaptability to unseen domains. The authors design an interactive dual-process architecture inspired by human cognition: a **fast reasoning module** (a policy network executing actions in real time) and a **slow reasoning module** (an LLM-based reflection system that analyzes history logs, extracts generalizable experiences, and refines fast policies). Additionally, the paper proposes an **instruction style conversion mechanism** using Chain-of-Thought prompting to normalize diverse user and scene instructions into a unified format. Experiments on the **GSA-R2R** benchmark demonstrate performance gains across in-distribution (residential) and out-of-distribution (non-residential) settings, showing improvements over GR-DUET and other adaptation baselines. Ablation, qualitative case studies, and sensitivity analyses further illustrate the benefits of the fast–slow interaction and experience library design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Innovative Cognitive Framework:**\nThe idea of *interactive* fast–slow reasoning—where System 2 (slow reflection) directly updates and empowers System 1 (fast policy)—is an elegant adaptation of dual-process cognition to embodied VLN. Unlike previous “parallel” designs, this model establishes a genuine feedback loop, forming a “slow-reflection-feeds-fast-action” mechanism.\n\n**Integration of LLM Reflection in Embodied Learning:**\nThe slow reasoning module uses structured Chain-of-Thought (CoT) prompts to distill experiences from navigation logs (Eq. 6). This implementation is well-motivated, and the structured experience vector E = [St, Cs, Rs, Tn, ηs, f] ⊺ provides an interpretable schema for extracting transferable knowledge from trajectory histories.\n\n**Comprehensive Evaluation:**\nResults across basic, scene-style, and user-style instructions (Tables 1–3) are extensive. The model improves SR by +1.5 % (ID) and +2.2 % (OOD) over GR-DUET under basic instructions, and consistently outperforms all baselines across five user personas and scene-style inputs. Ablations on both the **Fast–Slow Reasoning (FSR)** and **Instruction Style Conversion (ISC)** components are systematic (Table 4).\n\n**Clarity of Case Study:**\nThe qualitative example (Figure 3) clearly demonstrates how slow reasoning refines spatial understanding (e.g., identifying the “blue painting” landmark) and converts failure logs into structured spatial strategies that subsequently shorten trajectory length and reduce error by 80 %.\n\n**Relevance to Open-World VLN:**\nBy addressing both *scene generalization* and *instruction variability*, the paper aligns well with recent goals in embodied AI for open-environment robustness, bridging cognitive modeling and LLM-based navigation."}, "weaknesses": {"value": "**1. Conceptual Novelty—Limited Algorithmic Depth:**\nWhile the interactive fast–slow loop is well presented, it remains conceptually similar to existing *dual-process* or *meta-reflection* frameworks in VLN (e.g., MiC 2024, VLN-Copilot 2024, CogDDN 2025; SE-VLN 2025; NavCoT 2025) and in general LLM reasoning (e.g., Fast-Slow LLM 2025). The “interaction” here mainly consists of attention-based fusion (Eq. 7–9) between retrieved experience embeddings and visual features—a modest engineering extension rather than a fundamental algorithmic innovation.\n\n**2. Motivation and Design Ambiguity:**\nThe paper justifies the fast–slow split primarily through analogy to human cognition but provides limited empirical or theoretical grounding for *why* this two-stage interaction specifically enhances adaptation. It remains unclear how the “slow reasoning” differs functionally from a conventional memory-augmented replay or knowledge distillation step. The “instruction style conversion” component also appears ad hoc, with little ablation on prompt reliability or conversion accuracy.\n\n**3. Missing Discussion of Fast–Slow Discrepancy:**\nThe fast module reasons over latent embeddings, whereas the slow module reasons over symbolic textual reflections derived from visual descriptions. The discrepancy between these representation spaces (latent vs. linguistic) is never analyzed—raising questions about alignment, error propagation, and information loss during experience encoding.\n\n**4. Incomplete Theoretical and Computational Analysis:**\nKey parameters (e.g., retrieval threshold τ or attention dimension d) are mentioned without sensitivity analysis. The computational overhead of LLM reflection (frequency, token cost) is not reported, leaving unclear whether the method is practical for real-time or large-scale deployment.\n\n**5. Writing and Presentation Issues:**\nThe paper occasionally repeats background concepts (System 1/2 definitions, cognitive analogy) and lacks concise mathematical exposition. Equations (2–9) are descriptive rather than formally derived, and variable names are sometimes inconsistent. The introduction would benefit from clearer positioning relative to prior fast–slow reasoning work in embodied contexts."}, "questions": {"value": "1. **Interaction Frequency:** How often is slow reasoning invoked during training? After each episode or periodically? How do you control LLM inference cost versus learning benefit?\n2. **Representation Alignment:** How is semantic consistency ensured between textual experiences (E) and visual feature embeddings (Fv)?\n3. **Instruction Style Conversion Robustness:** How often does the system reject LLM-converted instructions due to low confidence?\n4. **Comparison to Mic, VLN-copilot, SE-VLN, and CogDDN:** These works also use cognitive dual systems. How does Slow4Fast differ beyond experience-attention fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AV48Cd4vIh", "forum": "QNJjLE2Eog", "replyto": "QNJjLE2Eog", "signatures": ["ICLR.cc/2026/Conference/Submission10221/Reviewer_1e2w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10221/Reviewer_1e2w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022165951, "cdate": 1762022165951, "tmdate": 1762921580756, "mdate": 1762921580756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}