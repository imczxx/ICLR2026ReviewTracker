{"id": "RXCRKAcv3B", "number": 23198, "cdate": 1758340820809, "mdate": 1759896827285, "content": {"title": "Generative Value Conflicts Reveal LLM Priorities", "abstract": "Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs *between* values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written ``user prompt'' and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.", "tldr": "We introduce ConflictScope, an automated pipeline that generates value conflict scenarios to evaluate how LLMs prioritize different values.", "keywords": ["LLM alignment", "value alignment", "evaluation", "moral dilemmas"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ded0e1392a6d5a74ab3e88a42ec3fbdf42d5f11b.pdf", "supplementary_material": "/attachment/8c4b13f25d837b1adb03f055644f483a90e4ccaa.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge that existing LLM alignment datasets rarely contain genuine value conflicts, making it difficult to study how models behave when forced to make trade offs. CONFLICTSCOPE, the proposed system, is desigedn to generate conflict scenarios given a set of user defined values. LLMs are evaluated on open ended outputs, rather than multiple choice questions, which can be brittle. The system simulates a user prompt, and the target LLM is then prompted and its output judged against the conflicting actions. Model preferences are aggregated using a bradley terry model to produce a final ranking of the models value priorities. The paper also includes some work on steering via system prompt, to align a model to a predefined value system.\n\n1. Scenarios generated by CONFLICTSCOPE were deemed to be more morally challenging\n2. Models moved away from supporting protective values and toward supporting personal values in an open-ended eval setting. \n3. Value alignment increased by an average of 14% via system prompt with value ordering"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a gap in existing alignment datasets, which rarely elicit true value conflicts. The ability to generate scenarios from a set of user defined value definitions (top down) makes the approach generalizable and potentially very useful.\n\nSimilarly the focus on open ended evaluation means that the approach more closely simulates realistic LLM usage and is more reliable than multiple choice, which can be brittle and sensitive to prompt nuances and ordering effects.\n\nInteresting experiment results."}, "weaknesses": {"value": "The approach relies heavily on an LLM (GPT4) for filtering, simulating the user, and judging the target model's response. The filtering is validated against human judgments with high precision, but the reliance on a foundational LLM for evaluation is a dependency and introduces some bias and reproducibility questions."}, "questions": {"value": "How would CONFLICTSCOPE work for multi turn interactions? \nDid you try to mitigate for bias and reproducibility when using GPT4 as llm judge?\nDo you have a theory as to why the model tend to shift from protective values to personal values in the open ended eval setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jZ1RKT4G8J", "forum": "RXCRKAcv3B", "replyto": "RXCRKAcv3B", "signatures": ["ICLR.cc/2026/Conference/Submission23198/Reviewer_5iqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23198/Reviewer_5iqJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947003332, "cdate": 1761947003332, "tmdate": 1762942555650, "mdate": 1762942555650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The current work introduces an automatic pipeline by which one can conduct an evaluation of value prioritization in LLMs. The extensibility and modularity of this pipeline are what makes it shine, and resultant findings delve into the alignment issues surrounding conflicting value judgements and how LLMs act in the presence of these conflicts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. the motivation of all 3 research questions is well explained\n2. use of multiple value sets - comprising of HHH, personal-protective, and ModelSpec - is good to ensure a holistic bucket of value set\n3. Despite the use of a closed LLM for their judge, the inclusion of a human study to validate the effectiveness and accuracy of their chosen judge is a welcome sight.\n4. both multiple choice and open ended evaluations are present\n5. large selection of both closed and open LLMs in experimental methodology."}, "weaknesses": {"value": "1. it would have been good to see another generative model chosen as the judge, particularly an open-weights and smaller size model"}, "questions": {"value": "1. Are there any plans to conduct a user study with the methodology to see how well it does when users supply their own value sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GvKZFT61RG", "forum": "RXCRKAcv3B", "replyto": "RXCRKAcv3B", "signatures": ["ICLR.cc/2026/Conference/Submission23198/Reviewer_MtAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23198/Reviewer_MtAF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958449229, "cdate": 1761958449229, "tmdate": 1762942555449, "mdate": 1762942555449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CONFLICTSCOPE, a generative evaluation framework that probes large language models (LLMs) under value conflict scenarios—situations where two moral or behavioral principles (e.g., autonomy vs. harmlessness) directly oppose each other. The framework automatically generates, filters such scenarios across three predefined value sets to reveal models’ implicit value priorities. Experiments on multiple LLMs show that models favor protective values in structured tasks but autonomy in open-ended ones. Furthermore, applying explicit value-order system prompts increases alignment consistency by about 14%, suggesting that value preferences can be modulated through targeted prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Meaningful motivation.\nThe paper addresses an important and underexplored question, how large language models (LLMs) behave when faced with value conflicts, i.e., when two  principles (such as autonomy and harmlessness) are in opposition. This framing moves beyond standard safety or alignment metrics and provides a more realistic lens for analyzing model decision tendencies.\n\n2. Reasonably experimental coverage.\nThe study includes roughly 3,000 automatically generated conflict scenarios across three distinct value domains (HHH, Personal–Protective, and ModelSpec). This scale provides sufficient empirical grounding to support its observations, with both multiple-choice and open-ended evaluations.\n\n3. Interesting findings.\nThe results reveal notable behavioral patterns: models tend to prioritize protective values (harmlessness, safety) in structured multiple-choice tasks, but shift toward personal autonomy in open-ended text generation. Moreover, specifying desired value hierarchies improves alignment consistency by about 14%, suggesting that model value expression can be modulated through prompting."}, "weaknesses": {"value": "1. Limited methodological novelty and technical depth.\nThe contribution lies mainly in defining a new evaluation perspective, value conflict, and constructing an automated scenario-generation pipeline. However, the framework itself relies entirely on existing LLM prompting. As a result, the work reads more as a benchmark or dataset study rather than a methodological advance suitable for a main-track research paper.\n\n2. Lack of principled grounding for selecting value pairs.\nThe paper enumerates three value sets, HHH, Personal–Protective, and ModelSpec, but provides no theoretical or empirical rationale for why specific value pairs (e.g., autonomy vs. harmlessness, honesty vs. helpfulness) are chosen or considered “likely conflicts” in real-world contexts. From section 3.1, the conflict pairs are largely randomly instantiated rather than derived from sociological or psychological evidence of common moral dilemmas.\n\n3. Reliance on LLM-as-Judge without human validation\nIn Section 3.1, all evaluation steps, including conflict validation, preference labeling, and response scoring are fully automated using LLM-as-judge without human verification or agreement check, which raises concerns about the objectivity and reliability of the results."}, "questions": {"value": "1. On the selection of value conflict pairs (Section 3.1).\nThe author states that conflict scenarios are generated by pairing values within the three sets (HHH, Personal–Protective, ModelSpec). However, it remains unclear how these pairings were determined to represent genuine conflicts. Were they randomly combined, or was there any filtering criterion ensuring that the paired values are indeed in tension in real-world ethical reasoning.\n\n2. On the lack of methodological clarity for target-ranking steering (Section 4).\nThe paper reports a “14% improvement in target ranking” with value-orderings in system prompt. However, Section 4 does not describe how this improvement was   measured, no definition of the metric, no example of the steering prompt, nor any breakdown of case studies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "waJaCYx6Ti", "forum": "RXCRKAcv3B", "replyto": "RXCRKAcv3B", "signatures": ["ICLR.cc/2026/Conference/Submission23198/Reviewer_bwD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23198/Reviewer_bwD5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991779183, "cdate": 1761991779183, "tmdate": 1762942555284, "mdate": 1762942555284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method (CONFLICTSCOPE) to learn how LLMs prioritize competing values. They build dilemmas from a user‑specified value set, apply (LLM) filters, then run interactions with a simulated user. They use a judge model to map responses to actions and a Bradley–Terry model to form value rankings. The authros find that models shift from protective to personal values. They also show that including an explicit value hierarchy in the system prompt improves alignment with a target ranking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Studying how models resolve value conflicts is an important problem (and is in contrast with existing work that focuses more on single values)\n\n+ It's nice that they have some human validation experiments\n\n+ Clear experimental results"}, "weaknesses": {"value": "- The labeling of scenario impossibility feels like it could be inconsistent; the filter description says it should be practically impossible to take both actions but the prompt asks the judge to return true if it is possible to take both actions (scenarios are retained only when all dims are labeled true?). This seems like it could result in both doable which seems to water down the dilemma definition.\n\n- The study seems a bit limited by the single-turn aspect. In practice, many conflicts are negotiated via (multi-turn) interaction, like clarification, reframing, de-escalation, etc. It would be more realistic to study if the shift to personal values still holds when the model can interact over turns.\n\n- The mapping V: D \\times A \\to A assumes unique representations and (strict?) dilemmas. A lot of conflicts allow for notions of good enough (satisficing), abstaining, compromise, etc. The formalism feels restrictive."}, "questions": {"value": "- Can the authors clarify the boolean pass/fail rule for the scenario impossibility filter and show retained itesm that would have been excluded under the definition?\n\n- There seems to be a disagreement between the steering effect definition divided by a(Rd,Rt) and Fig. 3's caption with correpsonds to dividing by 1-a(Rd,Rt). How would steering change if you report both normalizations?\n\n- Can the authors replicate the environment-model swap results for Personal-Protective and ModelSpec, and not just HHH? I'm curious what the differences (if there are any) would tell us"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJMHD8zQJ2", "forum": "RXCRKAcv3B", "replyto": "RXCRKAcv3B", "signatures": ["ICLR.cc/2026/Conference/Submission23198/Reviewer_ushm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23198/Reviewer_ushm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998219730, "cdate": 1761998219730, "tmdate": 1762942555080, "mdate": 1762942555080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}