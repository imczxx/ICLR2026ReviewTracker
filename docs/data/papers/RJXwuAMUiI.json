{"id": "RJXwuAMUiI", "number": 15789, "cdate": 1758255261706, "mdate": 1759897281907, "content": {"title": "The Effect of Attention Head Count on Transformer Approximation", "abstract": "Transformer has become the dominant architecture for sequence modeling, yet a detailed understanding of how its structural parameters influence expressive power remains limited. In this work, we study the approximation properties of transformers, with particular emphasis on the role of the number of attention heads. Our analysis begins with the introduction of a generalized $D$-retrieval task, which we prove to be dense in the space of continuous functions, thereby providing the basis for our theoretical framework. We then establish both upper and lower bounds on the parameter complexity required for $\\epsilon$-approximation. Specifically, we show that transformers with sufficiently many heads admit efficient approximation, whereas with too few heads, the number of parameters must scale at least as $O(1/\\epsilon^{cT})$, for some constant $c$ and sequence length $T$. To the best of our knowledge, this constitutes the first rigorous lower bound of this type in a nonlinear and practically relevant setting. We further examine the single-head case and demonstrate that an embedding dimension of order $O(T)$ allows complete memorization of the input, resulting in the approximation entirely achieved by the feed-forward block. Finally, we validate our theoretical findings with experiments on both synthetic data and real-world tasks, illustrating the practical relevance of our results.", "tldr": "", "keywords": ["Transformer", "Approximation Theory", "Lower Bound"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85501a6541e2b2ea93b627ef32f0c2989220a27d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper shows that target functions that can be concisely expressed in a given form can be efficiently approximated with transformers with a sufficient amount of heads, and inefficiently so when the number of heads is lower than a given threshold."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and explained. Intuition is provided after most mathematical definitions and results which makes it easy to follow the reasoning and the motivation.\n2. The introduction of the class of generalized D-retrieval tasks and proving that they are dense in the space of continuous functions is quite neat. It does remind me of the Kolmogorov-Arnold representation theorem, although there are a few details that might be of concern (see Weaknesses).\n3. The experiments demonstrate the predictions from theory indeed do appear in practice."}, "weaknesses": {"value": "1. Theorem 1 is purely a density result. It says nothing as to how the intrinsic dimensionality $D$ and the smoothness of $f$ depend on the target function $F$.\n2. This is a problem because the authors then go on to study how $D$ affects the required number of heads for a transformer to approximate $F$ well. However, if the link between the $F$ and $D$ is not established, it is difficult to be convinced whether the conclusions of the paper actually say anything of practical value. For instance, there could be trivial constructions that require a very large $D$ and $f$ with very high Lipschitz constant. Then all that the inner functions $f_i$ would do is to embed the sequence into a $D$-long vector and then let the MLP do the heavy lifting. But then that MLP might have to be extremely large. Overall, this interplay does not seem to be taken good care of.\n3. The experiments on real datasets do not study the interplay between the number of layers and the number of heads. How were the depths used (two and four layers) chosen? Additional experiments with varying depth possibly depth vs number of heads plot could be nice to have.\n4. On line 131 you say that “we omit layer normalization, noting that its removal does not harm the approximation capacity of the model”. However, layer norm (depending on where it is applied) might limit the ability of softmax to be “picky” and to approximate hardmax. Similarly, on line 151 you say that $\\beta$ can be chosen “arbitrarily large in order to make the softmax attention mechanism approximate a hardmax.” However, for both numerical stability and learnability, we do not want arbitrarily large activations in models. I understand that these are necessary assumptions for the theory to work in the limit but I think you should be more upfront with that they are significant and an important departure from how models work in practice."}, "questions": {"value": "Minor comments:\n\n1. On line 182 you use the term “intrinsic dimension” for the first time before it is defined, it feels a bit handwavy at this point. Perhaps you can explain what it means.\n2. There seems to be a mix in the notation in Figure 1 and the experiments section: both $h$ and $H$ seem to refer to the number of heads in the model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XoJ0rAAkYB", "forum": "RJXwuAMUiI", "replyto": "RJXwuAMUiI", "signatures": ["ICLR.cc/2026/Conference/Submission15789/Reviewer_vXWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15789/Reviewer_vXWe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390423856, "cdate": 1761390423856, "tmdate": 1762926021666, "mdate": 1762926021666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how the approximation properties of single-layer transformers on sequence-to-vector tasks vary with the number of attention heads. To investigate this question, they first introduce a new 'generalized D-retrieval' task, which they prove to be dense in the space of continuous sequence-to-vector functions. They then theoretically find that the number of parameters necessary to approximate their D-retrieval task is small when the number of attention heads $\\geq D$ and scales at least with $O\\left(1 / \\epsilon^{c T}\\right)$ where $T$ is the sequence length if the number of attention heads $< D$.\nThey validate their theoretical findings empirically on toy as well as more realistic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The story of the paper was clear and the paper was well-organized. The introduction  of the generalized D-retrieval task and the connection to the necessary number of heads of a single-layer transformer to efficiently approximate it is original. In particular, the lower bound on the number of parameters necessary to achieve a certain accuracy is interesting and useful. The experiments were well designed and presented."}, "weaknesses": {"value": "The presentation of the experimental results could be improved. In particular, it is not clear to me why they used the minimal validation NMSE achieved across multiple random seeds on the synthetic tasks, while using the training accuracy on the real datasets and the analysis of the results on the real datasets was kept very short, making it hard to evaluate the applicability of these results to real-world problems."}, "questions": {"value": "1. In line 190, why do you make the assumption $\\left|S_i\\right| \\geq \\frac{1}{4} T$? Where is this assumption needed?\n2. Should the input space of $F$ in line 195 depend on $T$? If yes, could you quickly explain this.\n3. Could you expand a bit on Assumption 2? Why or why not do you think they are reasonable assumptions?\n4. Could you explain the following sentence in slightly more detail:\n> As T increases, the number of relevant elements to distinguish grows linearly with T , yet they are compressed into an n-dimensional vector whose size does not scale with T.\n5. Could you explain the following sentence in more detail:\n> NMSE, equivalent to $1−R^2$, corrects for the variance shrinkage of maxima as T grows, thus enabling fair comparison across lengths.\n6. Could you expand on why you used the minimal validation NMSE achieved across multiple random seeds on the synthetic tasks, while using the training accuracy on the real datasets?\n7. Do you have further thoughts on how one might be able to verify that the experiments on real datasets indeed identify the 'correct' intrinsic dimension of the task? Might it be useful to train multiple slightly different transformers (e.g., different numbers of layers) on the same task to check whether the discovered intrinsic dimension is constant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R33NaH6rP4", "forum": "RJXwuAMUiI", "replyto": "RJXwuAMUiI", "signatures": ["ICLR.cc/2026/Conference/Submission15789/Reviewer_YCdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15789/Reviewer_YCdB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981713658, "cdate": 1761981713658, "tmdate": 1762926021261, "mdate": 1762926021261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the task of approximating sequence-to-vector mappings with a particular structure, where the output is a function of $D$ summary statistics of the input sequence with length $T$. The authors prove that with a number of heads $h \\geq D$, a Transformer with a single attention layer can efficiently approximate this class of mappings. On the other hand, with $h < D$, and an embedding dimension $n \\ll T$, approximating up to $\\epsilon$ error requires $(1/\\epsilon)^{\\Omega(T)}$ parameters. When the embedding dimension is $n \\geq Td$ where $d$ is the input token dimension, the Transformer can efficiently approximate these mappings by memorizing the input sequence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting setting where one can clearly prove the benefits of multiple heads for certain long-context tasks. It shows a nice tradeoff where one either needs to use as many heads as the intrinsic dimension of the mapping, or needs to use an embedding dimension that grows with the sequence length; in practical settings the former is always preferred for long-context problems. The authors also demonstrate the relevance of their introduced task by showing that it is a dense subset of the class of continuous sequence-to-vector mappings."}, "weaknesses": {"value": "* A key weakness of the results is that they only apply to a single-layer Transformer, i.e. a Transformer using only one layer of attention. It is unclear how the lower bounds would change if one were to use multiple layers with fewer heads. \n\n* It would be nice if the authors could provide some intuition on the necessity of Assumption 2. Some of the items in this assumption seem a bit specific, e.g. requiring non-zero coordinates in the gradient of $F\\_0$.\n\n* It would also be nice if the authors could include a brief intuition of the proof strategy, especially the strategy for proving the lower bound, in the main text."}, "questions": {"value": "* There seems to be a sudden transition in between cases 1 and 2 of Theorem 2, i.e. even with $h = D - 1$ one has a lower bound $M \\geq 1/\\epsilon^{\\Omega(T/(nD))}$, while with $h = D$ we suddenly get $M \\leq 1/\\epsilon^{\\gamma}$. Is there an intuition on this sudden change?\n\n* Similarly, what is the reason for the difference between $M \\leq 1/\\epsilon^{\\gamma}$ for Case 1 and $M \\leq 1/\\epsilon^{1 + \\gamma}$ for Case 3 in Theorem 2?\n\n* A relevant work that could be discussed is [1]. The authors there consider a sparse sequence-to-sequence task, and in one of their results they separate the approximation power of a single-head and multi-head attention in Transformers. The task studied in [1] could also be of interest here, as it allows to move from sequence-to-vector mappings to sequence-to-sequence ones where the length of the output sequence can grow with the input.\n\n* It would be nice if $C_{d,D,T}$ could be made explicit, at least in terms of $T$, since long inputs are of particular interest here.\n\n\n[1] A. Mousavi-Hosseini et al. \"When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective\". NeurIPS 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b6Dbv4cwdt", "forum": "RJXwuAMUiI", "replyto": "RJXwuAMUiI", "signatures": ["ICLR.cc/2026/Conference/Submission15789/Reviewer_SJw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15789/Reviewer_SJw1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144688854, "cdate": 1762144688854, "tmdate": 1762926020781, "mdate": 1762926020781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of the expressivity and approximation efficiency of single-layer Transformer networks, focusing specifically on the role of the number of attention heads, $h$. It introduces a function class, the generalized $D$-retrieval task, which models problems that combine multiple sequence elements determined by minimizing different component functions. The authors establish the generality of this task in Theorem 1 by proving that the class is dense in all continuous sequence-to-vector mappings. \n\nThe core of the paper is then captured in Theorem 2, which provides a set of sharp approximation bounds:\n* Positive Result (Thm 2.1): A transformer with $h=D$ heads can efficiently ($\\epsilon$-approximation in $L_\\infty$ norm) approximate $D$-retrieval tasks with constant per-head embedding dimension ($n=2$) and MLP size $M$ scaling independently of sequence length $T$.\n* Negative Result (Thm 2.2): If the number of heads $h$ is insufficient ($h<D$), and the sequence length $T$ is much larger than the per-head embedding dimension $n$, the MLP size $M$ must grow exponentially in $T$.\n* Positive Result (Thm 2.3): If the embedding dimension $n$ is scales linearly with the $T$, the full sequence can be effectively \"memorized\" in the attention output, and the task can be completed efficiently without an exponentially large FFN.\n\nThe theoretical findings are validated with experiments. The synthetic task clearly demonstrates a sharp phase transition in learning difficulty at $h=D=4$. Experiments on simple image and text retrieval tasks show that performance increases as the number of heads (with fixed parameters per head) increases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work provides a fundamental theoretical basis that cleanly captures the trade-offs on the expressive power of attention heads. The model is deliberately structured to reveal that when $h<D$, heads are forced to encode multiple retrieval roles simultaneously, creating an information bottleneck that MLP can only overcome with exponential complexity in $T$. Taken together, the bounds demonstrate sharp trade-offs between difference scaling regimes depending on head and embedding dimension capacity. The use of a simple, illustrative toy example greatly aids in explaining the bounds.\n\nA key achievement is how the work uses the concept of intrinsic dimension $D$ to distinguish performance at $h$ vs. $h+1$ heads. This analysis moves beyond earlier approximation results that either focused on universal capacity (where one large head is enough) or analyzed rank restrictions in isolation. The demonstration that $h<D$ leads to a $T$-dependent performance scaling is a useful distinction.\n\nThe synthetic experiments offer strong validation. They demonstrate a real threshold in model performance exactly at the theoretical intrinsic dimension D=4. This empirical finding strongly suggests that the theoretical representational gap translates directly into a learning difficulty gap in practice."}, "weaknesses": {"value": "The theoretical section lists model assumptions (Assumption 1) and target function constraints (Assumption 2) that appear to apply to the entire set of theorems (Thm 2.1, 2.2, 2.3). While it is appreciated that the bounds are in a consistent setting, it would improve the presentation and clarity of the paper if the authors would specify the minimal subset of assumptions required for each individual bound. For example, does Theorem 2.1 strictly require the Hessian constraints of Assumption 2.3? This small clarification would make the theoretical scope of each result easier to grasp.\n\nThe derivation of the lower bound in Theorem 2.2 relies on Lemma 4, which bounds all weights in the MLP by 1 (Assumption 1.3). This weight constraint simplifies the analysis, but is (to my knowledge) nonstandard in theoretical work or practical models. It would be valuable to discuss whether the lower bound can be maintained by instead considering a joint bound on the network's width and its weight norm in the lower bound, instead of a strict magnitude bound on every entry. This would align the result more closely with well-known complexity bounds for feed-forward networks (e.g., those from the line of work including [Yehudai-Shamir '19](https://arxiv.org/abs/1904.00687)). \n\nWhile the proofs appear largely correct upon detailed inspection, the proof of Theorem 2.2 is dense and hard to follow, involving numerous variables and recursive definitions that must be kept in the reader's head. I recommend the authors consider adding visual aids or diagrams that illustrate the construction of the adversarial sequences $W_1$ and $W_2$ and the partitioning of weights across the sequence indices. More descriptive notation would also help.\n\nThe experiments on real tasks show that increasing the number of heads $h$ (with a constant embedding dimension $n$ per head) helps performance. However, this is a key confounder, as the total embedding dimension $E=nh$ increases linearly with $h$, and thus the raw parameter count of the attention layers increases. Given the theoretical claim is about the specialization enabled by multiple heads, is it possible to design an experiment to disentangle the effect of the number of heads $h$ from the total parameter count?"}, "questions": {"value": "Why did the synthetic experiments focus exclusively on the case where $D=4$ (4-retrieval task)? It would be valuable to know how the critical performance threshold changes as a function of $D$. Did preliminary experiments suggest the sharpness of the transition is invariant to $D$?\n\nThe restriction to single-layer transformers is a noted limitation. While a rigorous multi-layer theory is beyond the scope of this paper, it would be highly insightful to share a conjecture or intuition about what is expected in that regime. Is the intrinsic dimension D of a task expected to be distributed across layers, or does the bottleneck effect remain a strong limiting factor at every layer? Insights based on the real-world experiments, which use deeper architectures, would be helpful to include in the discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "92cmpYLRBj", "forum": "RJXwuAMUiI", "replyto": "RJXwuAMUiI", "signatures": ["ICLR.cc/2026/Conference/Submission15789/Reviewer_FaEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15789/Reviewer_FaEJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146791416, "cdate": 1762146791416, "tmdate": 1762926020111, "mdate": 1762926020111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}