{"id": "LZ6Osh3DDD", "number": 24218, "cdate": 1758354232296, "mdate": 1763301254133, "content": {"title": "Derivative-Free Optimization via Monotonic Stochastic Search", "abstract": "We consider the problem of minimizing a differentiable function $f:\\mathbb{R}^d \\to \\mathbb{R}$ using only function evaluations, in the zeroth-order (derivative-free) setting. We propose three related monotone stochastic algorithms: the \\emph{Monotonic Stochastic Search} (MSS), persistent Monotonic Stochastic Search (pMSS), and MSS variant with gradient-approximation (MSSGA). MSS is a minimal stochastic direct-search method that samples a single Gaussian direction per iteration and performs an improve-or-stay update based on a single perturbation. For smooth non-convex objectives, we prove an averaged gradient-norm rate $\\mathcal{O}(\\sqrt{d}/\\sqrt{T})$ in expectation, so that $\\mathcal{O}(d/\\varepsilon^2)$ function evaluations suffice to reach $\\mathbb{E}||\\nabla f(\\theta^t)||_2 \\le \\varepsilon$, improving the quadratic dependence on $d$ of deterministic direct search while matching the best known stochastic bounds. In addition, we propose a practical variant, pMSS, that reuses successful search directions with sufficient decrease, and establish that it guarantees $\\liminf{t\\to\\infty}||\\nabla f(\\theta^t)||_2 = 0$ almost surely. Since MSS relies solely on pairwise comparisons between $f(\\theta^t)$ and $f(\\theta^t+\\alpha_t s_t)$, it falls within the class of optimization algorithms that assume access to an exact ranking oracle. We then generalize this framework to a stochastic ranking-oracle setting satisfying a local power-type margin condition, and demonstrate that a majority vote over $N$ noisy comparisons preserves the $\\mathcal{O}(d/\\varepsilon^2)$ gradient complexity in terms of iteration count, given suitably designed oracle queries. MSSGA uses finite-difference directional derivatives while enforcing monotonic descent. In the smooth non-convex regime, we show that the best gradient iterate converges almost surely at a rate of $o(1/\\sqrt{T})$ almost surely. To the best of our knowledge, this result provides the first $o(1/\\sqrt{T})$ almost-sure convergence guarantee for gradient-approximation methods employing random directions. Furthermore, our analysis extends to the classical Random Gradient-Free (RGF) algorithm, establishing the same almost-sure convergence rate, which has not been previously shown for RGF. Finally, we show that MSS remains robust beyond the smooth setting: when $f$ is continuously differentiable, the iterates satisfy $\\liminf{t\\to\\infty}||\\nabla f(\\theta^t)||_2=0$ almost surely.", "tldr": "", "keywords": ["Derivative-Free Optimization", "Zeroth-order optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72513d136f496071aa7ef2f71731ef6a728eec56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed two algorithms for zeroth-order optimization: the Monotonic Stochastic Search (MSS) algorithm and its gradient-approximation variant (MSSGA), and established their convergence properties for non-convex, convex, and strongly convex settings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The article is relatively well-written, with appropriate discussions and citations of relevant work."}, "weaknesses": {"value": "1. The upper complexity bounds achieved in the article are all known, and although a slightly different algorithm is used, this does not constitute sufficient novelty for the article to be accepted by ICLR. For example, in lines 107-109, the authors state, “The key difference, however, is that our algorithm enforces monotonic improvement by rejecting any update that does not lead to a smaller value of the objective function.” However, I do not believe this is an innovative point; it is simply a straightforward approach. Furthermore, for the stochastic setting (when the returned gradient oracle has noise), I am uncertain whether such a strategy remains viable.\n\n2. The writing of the article is poor. For instance, in Section 1, the \"Our Contribution & Related Work\" section is overly lengthy and lacks emphasis, spanning two pages yet making it difficult to identify the core contributions of the paper and how they differ from previous work. As a standard for a qualified paper, I believe this paragraph needs to be completely rewritten.\n\n3. There are no experiments presented, and I doubt the practical value of the algorithms proposed in the article."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWlE09ltH0", "forum": "LZ6Osh3DDD", "replyto": "LZ6Osh3DDD", "signatures": ["ICLR.cc/2026/Conference/Submission24218/Reviewer_AUH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24218/Reviewer_AUH2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470198000, "cdate": 1761470198000, "tmdate": 1762943002234, "mdate": 1762943002234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new class of **monotonic stochastic search (MSS)** algorithms for **derivative-free optimization (DFO)**.\nUnlike classical random search or evolutionary strategies, MSS imposes a *monotonic descent constraint* on noisy function evaluations, thereby improving stability under stochastic perturbations.\n\nThe authors analyze three major settings:\n\n1. **Smooth nonconvex functions** — MSS achieves sublinear convergence in expectation and almost surely, with\n   [\n   \\mathbb{E}|\\nabla f(x_T)| = O(\\sqrt{d}/\\sqrt{T}),\n   ]\n   without assuming convexity or PL-type conditions.\n\n2. **Convex functions** — The algorithm guarantees function value convergence\n   [\n   \\mathbb{E}[f(x_T)] - f^* = O(d/T).\n   ]\n\n3. **Strongly convex functions** — A faster geometric rate is achieved,\n   [\n   \\mathbb{E}[f(x_T)] - f^* = O!\\big((1 - \\mu/(dL))^T\\big).\n   ]\n   Here, the PL inequality is used only as a consequence of strong convexity, not as an independent assumption.\n\nOverall, the paper provides a unifying stochastic framework that recovers known DFO rates while improving robustness to noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive Theoretical Coverage**\n   The paper systematically treats nonconvex, convex, and strongly convex regimes in a unified manner, providing clear asymptotic rates for each case.\n   The inclusion of the **nonconvex L-smooth case without PL assumptions** is particularly commendable.\n\n2. **Novel Monotonicity Principle**\n   The “monotonic stochastic search” idea—using noisy evaluations to enforce descent direction without explicit gradients—is both conceptually simple and practically valuable.\n   It bridges classical stochastic approximation and derivative-free optimization.\n\n3. **Mathematical Rigor**\n   Proofs are clean and self-contained.\n   The paper references classical results (Nesterov, 2013; Ghadimi & Lan, 2016) appropriately while extending them to stochastic zeroth-order settings.\n\n4. **Clarity of Structure**\n   Each assumption and theorem is clearly labeled and motivated. The algorithmic structure is easy to follow.\n   The division of results (nonconvex / convex / strongly convex) is pedagogically clear.\n\n5. **Relevance and Generality**\n   DFO remains a vibrant area for large-scale simulation-based learning and black-box optimization.\n   This work offers a theoretically grounded yet computationally feasible method."}, "weaknesses": {"value": "1. **Limited Empirical Validation**\n   The experiments are minimal, mainly synthetic quadratic functions and low-dimensional benchmarks.\n   Demonstrations on higher-dimensional or noisy black-box tasks (e.g., reinforcement learning, hyperparameter tuning) would strengthen the impact.\n\n2. **Mild Novelty in Algorithmic Design**\n   While the monotonicity mechanism is interesting, it resembles prior stochastic line search or acceptance–rejection DFO strategies.\n   The novelty is thus more in the **analysis** than in the **algorithm itself**.\n\n3. **Dependence on Smoothness Constants**\n   The theoretical guarantees assume global L-smoothness and bounded variance of the function evaluations—standard but relatively strong assumptions for DFO.\n\n4. **No Adaptive Mechanism for Query Efficiency**\n   The paper could discuss how to reduce the dependence on the dimension (d), since rates scale as (O(\\sqrt{d})) or (O(d)), which is suboptimal for high-dimensional problems.\n\n5. **Strongly Convex Analysis Relies on PL-type Result**\n   Although acceptable as a corollary of strong convexity, the use of the PL inequality should be more clearly separated as a *derived property*, not an assumption."}, "questions": {"value": "1. Can the monotonic stochastic search idea be combined with adaptive sampling (e.g., covariance adaptation or coordinate selection)?\n2. How robust is MSS to biased noise or nonstationary stochasticity in function evaluations?\n3. Would it be possible to extend the analysis to nonsmooth (but Lipschitz) objectives?\n4. Can the dependence on (d) be improved via random subspace or low-rank approximation techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kjNTQjdJy4", "forum": "LZ6Osh3DDD", "replyto": "LZ6Osh3DDD", "signatures": ["ICLR.cc/2026/Conference/Submission24218/Reviewer_aFUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24218/Reviewer_aFUP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021757338, "cdate": 1762021757338, "tmdate": 1762943001754, "mdate": 1762943001754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed two stochastic zeroth-order optimization algorithms for smooth/nonsmooth optimization, MSS and MSSGA, which are based on DDS and gradient approximation. Convergence rates under nonconvex, convex and strongly convex scenarios are provided. Also asymptotic convergence result in the non-Lipschitz smooth case is provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithms are very simple, which should be easy to implement in practice.\n2. The propsoed algorithms achieved good convergence guarantees and matched existing best results."}, "weaknesses": {"value": "1. While the proposed MSS/MSSGA algorithms are elegant and minimalistic, the proposed algorithms' complexities do not outperform existing ones, it lacks a discussion on the motivation of the study.\n2. There lacks a thorough theoretical/empirical comparison on the proposed algorithms with closely related works, for example STP and GLD as authors mentioned. It is not clear what is the advantage of the proposed algorithms.\n3. The writing is a bit sloppy, for example, the \"Our Contribution & Related Work\" part is very lengthy and full of notations, which is hard to follow and identify the detailed contributions, I suggest a revision."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AG2uOyJUhR", "forum": "LZ6Osh3DDD", "replyto": "LZ6Osh3DDD", "signatures": ["ICLR.cc/2026/Conference/Submission24218/Reviewer_iWzL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24218/Reviewer_iWzL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030503539, "cdate": 1762030503539, "tmdate": 1762943001516, "mdate": 1762943001516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Modifications of the paper"}, "comment": {"value": "We thank the reviewers for their valuable comments and for taking the time to review our paper. The new additions in the revised version of the paper are as follows:\n\n1. We have substantially rewritten the “Our Contributions & Related Work” section to clarify the main contributions.\n\n2. We added two new subsections: 2.2 and 2.3.\n\n3. We removed the convex and strongly convex settings and replaced them with the more novel contributions presented in Sections 2.2 and 2.3."}}, "id": "cHU7yLIsJ7", "forum": "LZ6Osh3DDD", "replyto": "LZ6Osh3DDD", "signatures": ["ICLR.cc/2026/Conference/Submission24218/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24218/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24218/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763300728688, "cdate": 1763300728688, "tmdate": 1763300728688, "mdate": 1763300728688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies derivative-free optimization where only function evaluations are available. The authors propose two algorithms: Monotonic Stochastic Search (MSS) and MSS with Gradient Approximation (MSSGA). At each iteration, MSS samples a single random direction s_t from a distribution D and moves to the point that minimizes f among $\\theta_t, \\theta_t +\\alpha_t s_t$ where $\\alpha_t$ is a step size. MSSGA additionally uses finite differences to approximate the directional derivative. The main results show that MSS requires $d/\\epsilon^2$ samples for non-convex and smooth problems (Thm. 2), MSSGA uses $\\frac{d}{\\epsilon}$ for smooth and convex optimization (Theorem 5), and $d \\log \\frac{1}{\\epsilon}$ for strongly convex objectives (Theorem 6). The paper shows a convergence result for potentially nonsmooth (but still differentiable) objectives in Thm. 7."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MSS uses only one new function evaluation per iteration and enforces monotonicity, this is an advantage over competitor algorithms (e.g. Stochastic Three-Point method).\n2. The proofs are clear and mirror GD-style analysis, e.g. Lemma 1 gives GD-like expected decrease that straightforwardly leads to the $\\sqrt{d}/\\sqrt{T}$ bound.\n3. The paper provides an almost surely o(1/\\sqrt{T}) rate for the best iterate in the smooth non-convex case (Thm. 4) for MSSGA provided the smoothing sequence $\\gamma$ decays appropriately, and a similar result is shown for MSS in Remark 2."}, "weaknesses": {"value": "1. While using only one function evaluation instead of two is attractive, this is (a) a constant improvement, and (b) seems to actually show up in the convergence analysis. Comparing your Lemma 1 against Lemma 3.5 from [1], both have the same form of linear progress in (\\alpha|\\nabla f|) minus a quadratic penalty. STP works with normalized directions (i.e. $\\mathbb{E}|s|^2=1$), this corresponds to putting $\\mu_D=\\sqrt{2/(\\pi d)}$ in their lemma. If we rescale your Gaussian ($s_t\\sim\\mathcal{N}(0,I)$) to that normalization (i.e., divide by $\\sqrt{d}$) and matches the stepsizes, your linear‑term constant becomes (1/\\sqrt{2\\pi d}), i.e., a factor of 1/2 smaller than STP due to using only one side instead of ($\\pm s$). Since STP uses two evaluations per iteration and MSS uses one, the per‑function‑evaluation constants essentially tie. In other words, if we accept the convergence analysis in both papers, then the one function evaluation of MSS is cancelled out by having to do more iterations overall. If you include some experimental comparison, or improve the analysis, then you could still show an advantage of MSS over STP.\n2. I am not 100% sure what novelty is really claimed here, especially in the almost sure convergence results. Or in the proofs. Can you please make that more clear? The proof of MSS is very similar to the proof of STP in [1]. Also, the contributions section is currently rather difficult to read and very long, if you could shorten it to bullet points to better quantify what separates your work from prior work that'd be great.\n3. While most proofs are clear, some of the notation is a bit difficult to parse (like $A_{\\theta^t}^{--}, A_{\\theta^t}^{++}, A_{\\theta^t}^0$) maybe name these sets differently instead of using this many sub/superscripts?\n\nAs it stands, I lean towards rejecting this manuscript, but am open to changing my mind if my concerns are addressed.\n\n[1] Bergou, E. H., Gorbunov, E., & Richtarik, P. (2020). Stochastic three points method for unconstrained smooth minimization. SIAM Journal on Optimization, 30(4), 2726-2749."}, "questions": {"value": "1. Can you please address my concerns in the weaknesses section? In particular, a comparison with STP that takes into account *total complexity* rather than just per-step complexity while matching the distribution of noise used.\n2. Can you clarify if there are new technical tools used for MSS compared to prior work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EDsi8nZtL7", "forum": "LZ6Osh3DDD", "replyto": "LZ6Osh3DDD", "signatures": ["ICLR.cc/2026/Conference/Submission24218/Reviewer_Nhg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24218/Reviewer_Nhg5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058379358, "cdate": 1762058379358, "tmdate": 1762943001347, "mdate": 1762943001347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}