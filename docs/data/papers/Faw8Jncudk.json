{"id": "Faw8Jncudk", "number": 12844, "cdate": 1758210831074, "mdate": 1763081876845, "content": {"title": "C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging", "abstract": "Immunohistochemical (IHC) images reveal detailed information about structures and functions at the subcellular level. However, unlike RGB images, IHC datasets pose challenges for deep learning models due to their inconsistencies in channel count and configuration, stemming from varying staining protocols across laboratories and studies. Although existing approaches build channel-adaptive models, they do not perform zero-shot evaluation across IHC datasets with unseen channel configurations. To address this, we first introduce a structured view of cellular image channels by grouping them into either context or concept, where we treat the context channels as a reference to the concept channels in the image. We leverage this view to propose Channel Conditioned Cell Representations (C3R), a framework that learns representations that transfers well to both in-distribution (ID) and out-of-distribution (OOD) datasets which contain same and different channel configurations, respectively.  C3R is a two-fold framework comprising a channel-adaptive encoder architecture and a masked knowledge distillation training strategy, both built around the context-concept principle. We find that C3R outperforms existing benchmarks on both ID and OOD tasks, while yielding state-of-the-art results on CHAMMI-ZS; a zero-shot-style adaptation of the CHAMMI benchmark. Our method opens a new pathway for cross-dataset generalization between IHC datasets, with no need for retraining on unseen channel configurations.", "tldr": "C3R builds strong representations for generalization across IHC datasets with unseen channel configurations, outperforming existing methods on both in-distribution and out-of-distribution tasks without retraining.", "keywords": ["Representation learning", "Microscopy imaging", "Multi-channel imaging"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/25ea04e0148a8d81b9b39bf138e8b1fe478b59a9.pdf", "supplementary_material": "/attachment/17ca85dce4154725d2a4accb765fb94dfa05c05d.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an architecture and a training framework to create microscopy image models that can be reused across different imaging configurations with different channels. The main idea of the architecture is to group channels in two sets: context channels and concept channels, which is biologically motivated by the way some imaging experiments are conducted. Context channels serve as a reference frame to observe variations in the information collected in concept channels.\n\nThe proposed method was trained on the HPA dataset and evaluated in out of distribution tasks. Out of distribution in this paper is focused on channels observed during training, which is a rather narrow definition. The presented experimental results indicate promising performance of the proposed approach, including evaluations in the JUMP dataset, and in the CHAMMI benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of grouping channels in context and concept is interesting, and biologically motivated.\n* Based on this idea, the paper proposes an inductive bias to the architecture of ViTs and model training.\n* The paper makes a good presentation of previous work.\n* The experiments use established datasets to evaluate performance and investigate the properties of the models."}, "weaknesses": {"value": "The paper has several issues and the experimental results do not support the conclusions and claims of this paper.\n\n###  Context / Concept\n\n* The main technical limitation of the proposed approach is that the definition of context and concept channels needs to be manually defined. The decision may be very arbitrary in practice.\n* The quantification of context and concept seems to match the hypothesis, but it is not 100% supported by the data (as discussed in the Supplementary material). The separation can be artificial, and even if it’s quantitative, the paper is not presenting a solid method to automatically identify it in practice.\n\n### Training\n* The models and experimental evaluation are based on training with a single dataset (line 328). \n* With a single training dataset, it is unclear how the model learns generalizable representations to other channel configurations. Specifically, the HPA dataset seems to have only one concept channel during training. \n* The formulation of the model follows standard channel-wise token processing, originally proposed in the Channel-ViT paper and followed by others (Eq. 4 to 9). This architecture can be trained with varied numbers of channels. It is unclear why the proposed model is only trained with the fixed channels in HPA. \n* It is not completely clear if the training algorithm is SSL or weakly supervised. There is no clear definition of the objectives and losses used in their models and the baselines. \n* The focus on out-of-domain generalization is narrow to different channels only, when there are many other aspects of OOD generalization that can be studied. This includes different cell lines, treatments, batches and so on. The experimental designed is not clean in testing these types of OOD evaluations because of the restriction of training only on one dataset.\n\n### Baselines\n\n* Missing baseline evaluations. The Base-CP and Base-SC models are retrained with the same HPA data but with different channel settings to match the target dataset. Evaluation of models tailored to the channels and with the data of the specific task is important (specialized models).\n* Missing baselines. Other pre-trained models such as OpenPhenom or DINOv2 were not included in the evaluation. The former is channel adaptive, and the other can be adapted as presented in Figure 2 (line 182). It is unclear why these models were not used.\n* Baseline models seem suboptimal or impaired. ChannelViT and DiChaViT were trained with two channels at a time (line 1029). The models have the capacity to train with variable channel lengths, but the study is limiting this by arbitrarily choosing only 2, while the proposed models get to see 4 channels.\n* DINO4Cells and SubCell were said to be the pre-trained available models (lines 350 and 351), but Table 2 indicates they are re-trained to match the JUMP-CP channel configuration. It is unclear how to interpret these results.\n* Base-SC is trained with antibody loss, which is a supervised or weakly supervised loss (lines 328-332). It does not make sense to use a supervised loss with a single channel.  \n* The results in Table 3: the models are supposed to be trained with a 4 channel dataset, but the baselines only get to see 3 channels (line 331). In addition, the target dataset is 5 channels, so what channels are used and what channels are dropped is unclear, and it is an unfair comparison.\n\n### CHAMMI benchmark\n\n* CHAMMI is already a zero-shot benchmark, which uses K-NN evaluation for all 9 tasks (IID and OOD), and can be evaluated even with ImageNet pretrained models without any training. The statement that CHAMMI needs zero-shot style adaptation is incorrect, especially under the presented experimental setup (training only with HPA).\n* The introduction of CHAMMI-ZS (zero shot) is highly misleading, because a specialized, two-layer MLP is actually being trained (lines 341-344). Zero-shot evaluation means no training, resulting in a conceptual and experimental mistake. \n* The correct way of using CHAMMI for zero-shot evaluation is to compute features in all images and run the kNN evaluations. Those are the results that should be reported, specially if the paper claims generalization to unseen channels.\n\n\n### Incorrect terminology\n* Immunohistochemistry Channels => only one of the channels in HPA is IHC, the other channels are fluorescent channels. None of the channels in JUMP are IHC. This is the wrong terminology to refer to the datasets used in the presented experiments.\n* Models do not have channels. The input images have channels, the models are functions that only have parameters. Statements such as “encourage the channels” (lines 093, 295) do not make sense, because the channels are fixed data points rather than parameters.\n* In an experiment without re-training or fine-tuning, what gets frozen are the parameters of the model not the features (lines 334-340, 344, 424, etc). \n\n### Other issues\n* Figure 4 should keep together the bars of original and flip, rather than mAP and kNN. The quantities that need to be compared are split apart in the current layout. Also, the scale is misleading, giving the impression that the difference is large when it is actually very small.\n* In Table 4, why is the Base-HPA worse than h_c alone in the HPA tasks? Is the baseline suboptimal? Base-HPA is supposed to get all the information needed to solve the HPA tasks in almost the same way as h_c. Is there any explanation for this difference?\n* Figure 3 displays 5 channels (3 blue and 2 red) but in practice the experiments are all with 4 channels.\n* Typo in line 203: $p_k^c$ when it should be $p_c^k$.\n* Sloppy notation. In equation 8, the x parameters are missing the i and j super index respectively (e.g., $\\hat{x}^i_{c1}$)."}, "questions": {"value": "* If the concept/ context idea is robust enough, why there is not a method to automatically detect the type?\n* Why are all the experiments trained only on HPA? Does the method fail if trained on other datasets? This severely limits the potential of this study and the correctness of many reported experiments.\n* Why is the method not trained with varied numbers of channels but rather with 4-fixed channels from HPA?\n* Is there a limitation that prevents the model to be trained with disparate number of channels? In theory not, but why was this not tested?\n* What is an antibody loss and what are the definitions of the training objectives? This is not clearly defined, and it is unclear whether the models are self-supervised or supervised.\n* What does it mean to re-train baseline models DINO4Cells and SubCell to match the JUMP-CP channel configuration? When is this necessary?\n* When dropping channels from the model (e.g., 3-channels for JUMP-CP), which ones are dropped and which ones are used and why?\n* When adapting the proposed model trained with HPA, how are the assignments of context / concept channels made? Was this quantitative in CHAMMI, for instance? Where is this reported and why these choices?\n* In the CHAMMI evaluation, is an MLP trained for each subset of the benchmark? The proposed model can be channel-adaptive, but an MLP is not. The details of how this is done are unclear.\n* Why not conduct real zero-shot in CHAMMI? Why there is a need to train a two-layer MLP if the features are obtained with a frozen model? Why not just run the k-NN evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tttPuOlKcb", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Reviewer_bZAw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12844/Reviewer_bZAw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782516723, "cdate": 1761782516723, "tmdate": 1762923641654, "mdate": 1762923641654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Hfds8IOmxs", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763081876165, "cdate": 1763081876165, "tmdate": 1763081876165, "mdate": 1763081876165, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces  C3R, a novel framework for representation learning fluorescence microscopy images. The authors identify an inherent context–concept structure in microscopy channels, where some channels (e.g., nucleus, ER) provide structural context while others convey experiment-specific information (typically about the protein under study). Building on this insight, C3R combines (i) a Context–Concept Encoder (CCE) that processes channel groups separately before joint integration, and (ii) a Masked Context Distillation (MCD) pretraining scheme that promotes robustness to missing or unseen channels. The method enables zero-shot generalization across datasets with novel channel configurations. Experiments on HPA, JUMP-CP, and CHAMMI-ZS benchmarks show that C3R outperforms prior methods (DINO4Cells, SubCell, ChannelViT, DiChaViT) both in in-distribution (ID) and out-of-distribution (OOD) settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important and relevant problem of fluorescence microscopy representation learning \n2. Overall well-written and easy to read. \n3. Conceptually original idea (context/concept channel separation) \n4. Comprehensive ablation study"}, "weaknesses": {"value": "1. Conceptually, it is not so clear how to distinguish between context and concept channels. First, this distinction depends on the study design: the same channel might provide context for one assay and content for another. For instance, nuclei are context information for subcellular localization screens, while they provide the relevant readout when studying cell cycle, nucleoli, aneuploidy or cell death.  \n2. Related to this, I am not entirely convinced by the analysis and results provided in section 3.1. First, the authors make no distinction between intra-image heterogeneity (if for instance localization patterns are less marked or feature higher inter-cell variability) and inter-image heterogeneity (if perturbations affect the localization more). Second, the authors show only summary metrics for $P_c$ and $H_c$, thereby occluding differences between context channels. Indeed, the nucleus channel might heavily influence both metrics, while the ER could be more similar to a content channel. Finally, whether a channel provides context or content depends on which biological process I am interested in (see my point 1). This is a problem, especially since, as the authors have demonstrated, switching channel categories further degrades performance. \n3. The SSL MCD contribution is relatively incremental, and the validation (Table 5) sometimes depends on specific conditions, limiting robustness. \n4. The work does not leverage recent state-of-the-art SSL methods such as DINOv2/3 or MocoV3, relying instead on iBoT, which may restrict comparative performance. \n5. The approach lacks scaling to be considered truly generalizable; it appears more as a proof of concept than a fully deployable solution. \n6. There appears to be some confusion between Immunohistochemistry (IHC) and Immunofluorescence (IF). \n7. I would have expected a short explanation of the downstream tasks and biological objectives. Indeed, the datasets and associated tasks might not be known to the ICLR community."}, "questions": {"value": "1. The authors use the term Immunohistochemical (IHC) throughout the text, which can be easily confounded with a modality used in digital pathology, usually not based on fluorescence. It would be better to use an unambiguous term, such as Immuno-fluorescence (IF) throughout the text. \n2. Could the authors propose a systematic rule for assigning channels between context and concept? \n3. Did the authors try SOTA SSL methods (e.g., DINOv2/3, MocoV3)? Is there a rationale for choosing iBoT? \n4. In Figure 2, could the authors add a legend to facilitate the distinction between Concept and Context channels? Additionally, it is not very clear that Concept channels exhibit higher variance than Context channels from the UMAPs.  \n5. In Figure 4, it is noted that flipping the channels during inference degrades the metrics. Did the authors also train a model with flipped channels to assess the impact on performance? \n6. Figure 6: Can the authors explain how the error bars are calculated, and why are they only displayed for the HPA-Loc dataset? Moreover, the  depth evaluation was performed by fixing the number of layers at 4. Can the authors justify this choice? Figure 6 seems to indicate that the optimal depth is 2. \n7. Figure 6 seems to indicate that, overall, larger networks perform better. Have the authors tested more complex architectures to further improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BscoZwzqWO", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Reviewer_WCdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12844/Reviewer_WCdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904767484, "cdate": 1761904767484, "tmdate": 1762923641334, "mdate": 1762923641334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification on the Context-concept principle:"}, "comment": {"value": "The concept-context split is not arbitrary and not introduced by us. It reflects how immunofluorescence experiments are commonly designed and interpreted: (i) landmark/structural (context) channels that provide spatial reference for segmentation and registration; and (ii) readout (concept) channels whose signal is expected to vary with the biological factor of interest (antibody target, compound/MoA, genotype, pathogen). We adopt the roles declared by the assay; parity/entropy are reported only as consistency checks, not as a decision rule.\n\n\n**1)  This practice is codified across canonical IF resources and datasets.**\n-   Human Protein Atlas (HPA) [1]. Each sample pairs the protein of interest with three reference markers: nucleus (DNA/DAPI), microtubules (tubulin), and endoplasmic reticulum (ER; calreticulin), that serve as structural landmarks for interpreting localization.\n-   Cell Painting / JUMP-CP [2]. The assay separates positional references (typically DNA and ER) from phenotype-bearing channels (RNA, AGP, mitochondria).\n-   WTC-11 [3]. In cell-cycle assays, the nuclear/DNA channel is the readout (concept) because DNA content/organization encodes G1/S/G2/M; auxiliary stains (membrane/cytoplasmic markers or transmitted-light) are context for delineating cells.\n-   CM4AI-Bridge2AI (ICC-IF) [4]. Proteins of interest are labelled with specific antibodies (readout), while cells are co-stained with DAPI (nuclei), anti-tubulin (microtubules), and anti-calreticulin (ER) as structural/positional markers (context)\n-   Rabies diagnostics: The direct fluorescent antibody test detects rabies nucleocapsid antigen on fixed tissue/monolayers (the readout) and is interpreted relative to the structural background/counterstain (the context)\n-   Across HPA, Cell Painting/JUMP-CP, WTC-11, CM4AI, and diagnostic IF, biologists predefine channel roles to ensure interpretability. Our paper exploits this established convention; we are not redefining which channel is which.\n\n**2) Task dependence is expected and supports our framing**\n-   Protein localization (e.g., HPA): nucleus/DAPI, microtubules, and ER provide context to situate the protein channel.\n-   Morphological profiling (Cell Painting/JUMP-CP): DNA/ER act as context, whereas RNA/AGP/mitochondria are readouts carrying perturbation phenotypes.\n-   Cell-cycle assays (e.g., WTC-11 cell-cycle): DNA is the readout, and structural labels (membrane/cytoplasm/brightfield) provide context.\n-   We follow the assay’s intent rather than imposing a universal label. The fact that roles can flip across assays is a property of biology and study design, not evidence of arbitrariness.\n    \n\n**3) Manual is not arbitrary; it means assay-specified by domain experts**\n\n-   Manual here means roles are specified a priori in protocols and dataset documentation (e.g., HPA’s four-channel schema; Cell Painting’s fixed five-channel design; rabies FAT antigen vs. background) to ensure reproducibility and interpretability. These conventions have been validated over years of use.\n    \n\n**4) Parity/entropy are sanity checks, not the basis of assignment**\n\n-   Landmark channels typically appear more stable/unimodal, while readouts vary by design. We include parity/entropy to corroborate the expected roles; they do not define them. The definition comes from the assay’s biological specification.\n\n**5) What happens when a new IF dataset arrives?**\n\n-   We apply a biology-first policy that mirrors standard practice:\n-   If the protocol/SOP designates a stain as a reference/landmark, we treat it as context.\n-   If it is a target/readout stain, or documentation is ambiguous, we treat it as concept and validate via ablations to show conclusions are stable.\n   \n**6) Why our approach avoids per-task re-training (contrast with prior models)**\n-   Many recent microscopy foundation/SSL models learn strong representations but do not explicitly exploit the landmark-vs-readout principle; as a result, they are typically trained per task. DINO4Cells and SubCell have to train separate models for task, whereas C3R uses one model and routes information by biological role (context vs. concept) that is already defined by the assay. This is the core practical advantage: no separate model per task is needed precisely because we exploit the universal, assay-level role assignment that biologists specify.\n\nReferences:\n\n[1] Thul, Peter J., et al. \"A subcellular map of the human proteome.\" _Science_ 356.6340 (2017): eaal3321.\n\n[2] Chandrasekaran, Srinivas Niranj, et al. \"JUMP Cell Painting dataset: morphological impact of 136,000 chemical and genetic perturbations.\" _BioRxiv_ (2023): 2023-03.\n\n[3] Viana, Matheus P., et al. \"Integrated intracellular organization and its variations in human iPS cells.\" _Nature_ 613.7943 (2023): 345-354.\n\n[4] Rincon, John, et al. \"Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care.\" _arXiv preprint arXiv:2505.14757_ (2025)."}}, "id": "lsRyQKHt9X", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12844/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763033363117, "cdate": 1763033363117, "tmdate": 1763033363117, "mdate": 1763033363117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of applying deep learning to microscopy (IHC) images, which have inconsistent channel counts and configurations across datasets. This inconsistency normally prevents a model trained on one dataset from working on another without retraining.\n\nThe authors introduce the \"context-concept principle,\" dividing channels into:\n* Context: Stable structural references (e.g., Nucleus).\n\n* Concept: Variable, experiment-specific information (e.g., Protein).\n\nBased on this, they propose C3R, a framework with two parts:\n\n* Context-Concept Encoder (CCE): A new architecture that processes these channel groups separately before merging them, enabling generalization to unseen channel configurations in a zero-shot manner.\n\n* Masked Context Distillation (MCD): A training strategy that forces the model to learn from a limited set of context channels, improving representation robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses a real issue in the field of image representation learning for microscopy images: modern image representation learning architectures are designed for natural RGB images, which always come in the three-channel format, where there is no significantly distinct information across channels.\n\n* The paper introduces a new and intuitive way to structure microscopy data by grouping channels into \"context\" and \"concept\". The model's success should speak to the power of grouping channels accordingly.\n\n* Using community-accepted benchmarks to evaluate results against other models"}, "weaknesses": {"value": "* The paper claims that its framework is the first of its kind to demonstrate strong zero-shot evaluation for this problem, but I found the following paper that purports to do the same, as is also similarly channel adaptive [1].\n\n* The model's ability to evaluate a new OOD dataset relies on the assumption that the new dataset's channels can also be separated into \"context\" and \"concept\" groups. While the authors note this is true for the most common public IHC datasets (HPA, JUMP-CP, WTC-11, etc.), they have not explored datasets that may not follow this principle.\n\n\n[1] L. Phillips and R. Donovan-Maiye, \"CellRep: A multichannel image representation learning model,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Jun. 2025, pp. 4312–4318."}, "questions": {"value": "* Have you explored methods to learn this context-concept grouping automatically?\n\n* MCD's success implies context channels are redundant, yet this robustness fails on the OOD JUMP-CP dataset. Does this mean the rules of context redundancy are not generalizable, and what does that imply for the 'context-concept principle' on new, unseen datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3o78uAptzm", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Reviewer_FdJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12844/Reviewer_FdJv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977366010, "cdate": 1761977366010, "tmdate": 1762923640970, "mdate": 1762923640970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce a structured approach to training a channel adaptive model that can generalize well in zero-shot settings that are both in-distribution (having the same channels) and OOD (different channels during training and inference). They first introduce the Context-Concept principle by segregating channels into context channels (structural channels) vs concept channels (non-reference information). They propose Channel Conditioned Cell Representation (C3R) that incorporates the structural separation during training that generalizes well to ID and OOD at inference time. They also show their model performs state of the art on CHAMMI-zero-shot adapted dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Improved training strategy: Masked Context Distillation with concept context segregation is intuitive and adds biological data structure prior to training to an extent. \n* Comprehensive evaluation and improved performance on several benchmark datasets (HPA, JUMP-CP, CHAMMI-ZS) in both in distribution and OOD settings."}, "weaknesses": {"value": "* The assignment of concept or context groups are rather arbitrary or manually labeled subjectively. \n* Random channel masking across Student/Teacher (i.e. channel masking as an augmentation baseline) is missing and the contribution of MCD (masking context channels specifically) is unclear. \n* The contribution is very specific to IHC/fluorescence microscopy images and the subjective nature of context/concept selection makes it not generalizable and scalable to operate as a methodology.\n * CHAMMI-ZS benchmark doesn't seem to be truly zero-shot as there is a dataset specific MLP heads used for evaluating on the dataset."}, "questions": {"value": "* What was the procedure for the segregation of context vs concept channels? It feels rather arbitrary than a specific methodology. The per group parity and entropy metrics do not necessarily capture the contextual or conceptual nature of the channels.\n* Have you compared results on Masked Context Distillation with masking just concept channels or allowing masking all channels between student and teacher (Table 5 doesn't seem to answer that question clearly)? How much does Masking just the context contribute to performance compared to masking as an augmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TnsAYhPHyG", "forum": "Faw8Jncudk", "replyto": "Faw8Jncudk", "signatures": ["ICLR.cc/2026/Conference/Submission12844/Reviewer_6hTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12844/Reviewer_6hTV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762420235425, "cdate": 1762420235425, "tmdate": 1762923640318, "mdate": 1762923640318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}