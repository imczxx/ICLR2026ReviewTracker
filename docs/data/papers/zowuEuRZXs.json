{"id": "zowuEuRZXs", "number": 6384, "cdate": 1757976998582, "mdate": 1759897918251, "content": {"title": "MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding", "abstract": "Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce MultiDiffNet, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.", "tldr": "We introduce MultiDiffNet, a diffusion-based framework that generalizes to unseen subjects, supported by a new benchmark suite and evaluation protocol.", "keywords": ["Brain-Computer Interfaces", "Neural Decoding", "EEG", "Diffusion Models", "Representation Learning", "Benchmarks", "Cross-Subject Generalization", "Interpretability"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb7154db3d95d2372da9be684fa852bf752032fa.pdf", "supplementary_material": "/attachment/b411eb73e881cf4379e1bff9421a76c60dabd488.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MultiDiffNet, a complex, diffusion-based framework for EEG decoding. The authors' goal is to improve generalization across unseen subjects by learning a latent space optimized for classification, reconstruction, and contrastive objectives, thereby avoiding the need for explicit data augmentation. The paper also presents a new benchmark suite of four EEG tasks with standardized data splits and proposes a new \"trend-level\" statistical framework for evaluating results in low-trial settings. The central claim is that this approach achieves state-of-the-art generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's most significant and durable contribution is the curation and release of a standardized benchmark suite for EEG decoding . Establishing a rigorous, subject-disjoint evaluation protocol is a valuable service to the community that will foster more reproducible research.\nThe authors have conducted an exceptionally detailed ablation study, investigating over 100 configurations."}, "weaknesses": {"value": "The central narrative of achieving superior generalization is not supported by the paper's own results. In 50% of the tasks (P300 and Motor Imagery), simpler baselines outperform MultiDiffNet on the crucial unseen-subject test set. The proposed complex model does not justify its existence with consistent, clear-cut performance gains.\nThe proposed MultiDiffNet architecture is substantially more complex than the baselines it fails to consistently outperform. The ablation study reveals that removing the decoder, a core component of the multi-objective framework, can actually improve generalization, suggesting the authors' own understanding of the model's mechanics is incomplete. The added complexity does not bring a corresponding, reliable benefit."}, "questions": {"value": "1) Given that your model is outperformed by simpler baselines on half of the benchmark tasks, how can you maintain the claim of achieving \"state-of-the-art generalization\"? Please provide a more nuanced and accurate assessment of your model's capabilities and limitations.\n\n2) The finding that removing the decoder can improve performance severely undermines the paper's premise that a reconstruction objective is key to learning a generalizable latent space. How do you explain this result, and what does it imply about the actual source of any performance gains you do observe?\n\n3) Could you provide a compelling argument for why the community should adopt a significantly more complex model that offers, at best, task-specific performance improvements over simpler, faster, and more established architectures like EEGNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BdpVFSFDTG", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_omEe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_omEe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760526068908, "cdate": 1760526068908, "tmdate": 1762918670801, "mdate": 1762918670801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new model for decoding brain signals.  The method proposes a three-loss-based model using classification loss, reconstruction loss, and contrastive loss. Then they compare their method with five different models over 4 EEG tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes tackling generalization for the EEG task, which is a challenging yet important task."}, "weaknesses": {"value": "The paper lacks clarity:\n- Figure 3 is unreadable; all the captions and axis labels are too small, making it hard to understand what we are looking at.\n- The appendix is hard to follow, but all the ablation study is inside it. Since there are two additional pages available, it would be easier to follow if the ablation study were included within the main paper.\n- In the results table, bold is used only when the method's author is the best, and no bold is used when the proposed method is worse than competitors. making the table hard to read. Additionally, MultiDiffNet suffers a considerable performance loss on P300 and MI. 8% and 7% respectively. If the text says \"slightly\" below, I think it represents a significant decrease.\n- The introduction is very short. Since there is some space available, it could be interesting to have a better introduction to the literature to provide stronger motivation. Right now, there is no clear motivation for the paper.\n\nIf the paper's pipeline is interesting, improving the writing and clarity will enhance the contribution."}, "questions": {"value": "Why the score for Imagined speech is so low?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cGXmWfNgmr", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_n1ee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_n1ee"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916872812, "cdate": 1761916872812, "tmdate": 1762918670077, "mdate": 1762918670077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MultiDiffNet improves cross-subject generalization on several EEG decoding tasks using a latent space trained jointly for classification, reconstruction, and contrastive objectives. On SSVEP, unseen-subject accuracy rises from 81.08% with EEGNet to 84.72%, and to 85.25% with Temporal Masked Mixup. On imagined speech, it lifts seen-subject accuracy from 11.26% to 17.57% and unseen from 10.61% to 12.12%. Motor imagery shows competitive seen accuracy and better unseen accuracy than several baselines, though still below EEGNet on seen splits. P300 results lag strong baselines. The model consistently narrows the seen-vs-unseen gap, and extensive ablations identify which components matter most. \n \nContributions. The paper introduces a multi-objective diffusion framework that avoids synthetic augmentation by learning a compact shared latent space with an EEGNet-style encoder, subject-wise latent normalization, a lightweight classifier head, and two mixup strategies including a new Temporal Masked Mixup. It also releases a unified benchmark across four tasks with standardized subject- and session-disjoint splits, and proposes a trend-level statistical reporting scheme tailored to low-trial EEG to surface small but consistent gains. Together these pieces form a reproducible setup for subject-agnostic EEG decoding and clarify evaluation practices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: Joint multi-objective diffusion with subject-wise latent normalization and Temporal Masked Mixup for cross-subject EEG. \nQuality: Standardized subject/session-disjoint protocol, strong baselines under one schedule, plus thorough ablations and trend-level stats. \nClarity: Architecture and objectives are explicit; losses, mixup algorithm, and normalization are clearly defined. \nSignificance: Delivers unseen-subject gains on SSVEP and imagined speech; provides a reusable benchmark and reporting scheme."}, "weaknesses": {"value": "1 Claims do not match results. The paper says it consistently narrows the seen vs unseen gap across tasks, yet on P300 and Motor Imagery it trails EEGNet and the gap can widen. \n\n2 Subject-wise normalization may bias comparisons. You normalize the latent space with per-subject statistics. For unseen users it is unclear how those statistics are obtained. If any calibration trials are used, every baseline must get the same treatment. \n\n3 The paper labels the classification loss “CE/MSE” without stating which is used in the main results. The reconstruction aligns the decoder to the DDPM output rather than the raw signal; if that target is updated jointly and not frozen, training can chase a moving target. State the exact loss used, whether the denoised target is gradient-detached, and the training order. \n\n4. Accuracy is presented with a binary confusion-matrix formula while some tasks are multi-class, and the “win rate” stars are not defined. Provide a clear multi-class metric definition, add macro and micro F1, define the unit of a “win” and the pairing, and include confidence intervals plus a formal test or Bayes factor for the key head-to-head claims."}, "questions": {"value": "1. when authors say the method consistently narrows the seen vs unseen gap across tasks. The tables show mixed outcomes, especially on P300 and Motor Imagery. Please explain, and it would be better to add a short failure-mode analysis for those two tasks with concrete fixes you tried.\n\n2.For unseen subjects, how did authors obtain their per-subject statistics? If any calibration trials are used, every baseline should get the same procedure. Please report two settings for all methods: zero-calibration and few-shot calibration.\n\n3 What exactly is the loss and training flow?\nState which classification loss you used in the main results. Clarify what the reconstruction is aligned to, and whether the denoised target was kept fixed during decoder updates. A small stability plot comparing “frozen target” vs “jointly updated target” would settle concerns.\n\n4 Are the evaluation definitions rigorous and transparent?\nSeveral tasks are multi-class, yet accuracy is described with a binary definition and “win rate” is not clearly defined. Please add the exact multi-class metric, report macro and micro F1 alongside accuracy, define what counts as a “win,” and include confidence intervals plus a simple significance or Bayesian test for the key head-to-head claims."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The current ethics statement is insufficient as it fails to address core concerns like the validity of informed consent for aggregated data reuse and the potential for misuse of the neural decoding technology."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BUkssNAJQT", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_hFQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_hFQT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927048589, "cdate": 1761927048589, "tmdate": 1762918669598, "mdate": 1762918669598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper relates to generalizable representation learning for brain EEG date. The objective is to learn well separable clusters, even for unseen data. They go about solving this problem by having a diffusion module, an encoder, a decoder, and a classifier. Additionally, they have 4 different losses:\n- 1.Recon losses:\n  - a. A simple diffusion based gaussian denoising/recon. Loss\n  - b. Labeled and unlabeled mixing and recon loss\n- 2. Encoder outputs\n  - a. Contrastive Supcon Loss\n  - b. Classification loss\n\nThe results show that their work performs really well on unseen data, especially by having separable clusters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "They show decent results on their chosen datasets and beat a lot of SOTA results by a decent margin, especially on the unseen categories."}, "weaknesses": {"value": "- The paper is very procedural: It is about applying some already very well known techniques via multiple losses and techniques. There is no hypothesis that they are trying to prove or disapprove; It’s a benchmarking paper at best. On the theoretical side they have combined a lot of well-known losses. But no questions have been answered like why this way is the best way to solve the given problem. They claim that other generative synthetic augmentations are not scalable, but no computational-time-cost vs performance benchmarks were provided. \n\n- This paper has a wording problem: What do they exactly mean by compactness? Is it related to compression, rate-distortion stuff? Or do they mean that the classes are well separable and class prototypes are compact? For the latter, they haven’t compared the separability of their work against other and shown that their is better at separation. Showing better performance doesn’t mean that the class-separation is better as well. \n\n- The paper is only 7 pages, which although not a requirement, shows that a lot more thoughtful experiments could be added."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "snkkxquV8W", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_KQi5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_KQi5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148556285, "cdate": 1762148556285, "tmdate": 1762918669091, "mdate": 1762918669091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work presents a model named MultiDiffNet, an EEG decoder that aims to learn a compact and structured latent space for EEG decoding with multi-objective training. The idea is to use the EEG signal generated from a diffuser and the reconstructed signal from the latent representation via a decoder to mix with the original EEG signal in mixup fashion. Such augmentation will avoid other synthetic augmentations that have the tendency to result in unreal EEG. The model is trained to jointly optimize both EEG generation/reconstruction (diffusion, and decoder), downstream classification, and supervised contrastive loss. Evaluation was carried out on 4 different EEG decoding tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using reconstructed EEGs (from diffusion, decoder-based reconstruction) for augmentation is fine, particularly for EEGs, as one could expect the reconstructed EEGs are less subject to subject-specific noise and eventually improve generalization across subjects.\n\nHowever, this idea is not new, but the proposal to jointly train the diffusor & the decoder for EEG generation/reconstruction jointly with the encoder in multi-objective is."}, "weaknesses": {"value": "{\\bf Method:}\n\nThe approach that trains jointly generation/reconstructor and the encoder seems overkilled. I understand the motivation to unify them in multi-objective training, but I do not see the benefits in terms of performance and generalization (there are no experiments to showcase that). At the same time, I am also concerned about the stability of training such a model. In the early phase, when the diffusion and reconstructor are not yet in good state, the reconstructed EEGs are bad, would they be any good for data augmentation? The design of loss scheduler may mitigate this with different weights on different components, but I don't think it would resolve this issue. \n\nThe so-called temporal masked mixup that is claimed to be new in the work is actually similar to CutMix augmentation, which I don't think is considered novel. \n\n{\\bf Evaluation:}\n\nIn my opinion, the experimental design for evaluation is not yet appropriate to showcase the advantage of the work. As the work in fact focuses on generating EEGs for augmentation, I would expect comparison with existing augmentation methods to showcase its advantage. The unified multi-objective training also warrants the comparison with two-stage training.\n\nI am not convinced with the choices for baselines. Most of them are >5 years old and do not represent the recent advance in this topic. Also, I wonder if training these baselines without \"method-specific tuning\" is a reasonable way for a fair comparison. Different models and architectures may favor different training procedures and one-for-all receipt will hide the true potential of a model.\n\nI appreciate the effort in standardizing evaluation which I believe is important for reproducible research. However, I would defense for LOSO evaluation which is probably better suit for academic setting & small datasets like these. Particularly, the amount of training data will be maximized with LOSO, which is important in deep learning era. It also facilitates the reproducibility and comparability between different works as well.  \n\nI don't know how to interpret the claim \"state-of-the-art generalization\". Is the the gap between seen-subject and unseen-subject performance or the absolute performance themselves? In any case, neither of the interpretations is backed by the results in Table 1. The results show a mix where the proposed method wins on SSVEP and Imagined Speech, but it underperforms other methods on MI and P300 tasks, particularly the latter showing a big gap on unseen-subject classification. I am afraid that one could draw a conclusion confidently from these results."}, "questions": {"value": "- What are the benefits of multi-objective training over two-stage training, particularly in terms of performance and generalization?\n- Could you comment on the stability of the training? In the early phase, when the diffusion and reconstructor are not yet in good state, the reconstructed EEGs are bad, would they be any good for data augmentation?\n- How the loss weight scheduler was design given that the weight for diffusion and contrastive training is much smaller than the classification loss?\n- Given most of the baselines are >5 years old, how one could position the results in this work in literature, particularly the SOTA on these decoding tasks?\n- Would the way the baselines models were trained ensure fair comparison? And how?\n- How could we interpret the mix results in Table 1 to support the claim \"SOTA generalization\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "36qAw5Qvy2", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_LGZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_LGZ7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762374366846, "cdate": 1762374366846, "tmdate": 1762918668590, "mdate": 1762918668590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to addressing a key challenge in EEG-based brain-computer interfaces: poor generalization across subjects due to inter-subject variability and limited data. The proposed MultiDiffNet framework integrates diffusion models (DDPM) with a shared latent space optimized for classification, reconstruction, and contrastive learning. It also provided a curated benchmark suite across four EEG tasks (SSVEP, P300, Motor Imagery, and Imagined Speech) with standardized subject/session-disjoint splits."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach avoids common pitfalls like artifact introduction in GAN-based or diffusion-based synthesis. The Temporal Masked Mixup is a nice extension of standard mixup, preserving temporal structure in EEG signals.\n\nBy standardizing datasets and enforcing subject/session-disjoint splits, it addresses inconsistencies in prior works.\n\nThe trend-level statistical framework mitigates p-value limitations in high-variance settings, promoting evidence-based claims.\n\nAblations covering decoder inputs, classifier heads, encoder/decoder variants, loss combinations, which is relatively comprehensive."}, "weaknesses": {"value": "No runtime or parameter count comparisons; given EEGNet's lightweight design, how does MultiDiffNet's added DDPM/decoder components affect efficiency for real-time BCIs?\n\nSome hyperparameters such as embedding dimensions, attention pool specifics are underspecified in the main text. This could hinder replication. Mixup integration points are mentioned but results aren't fully tabulated, which point works best per task?\n\nP300 results are mixed with MultiDiffNet underperforms baselines on unseen accuracy. The paper attributes this to \"ceiling effects,\" more discussion is needed."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H7y8HLbnJI", "forum": "zowuEuRZXs", "replyto": "zowuEuRZXs", "signatures": ["ICLR.cc/2026/Conference/Submission6384/Reviewer_gmfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6384/Reviewer_gmfV"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission6384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762633949922, "cdate": 1762633949922, "tmdate": 1762918668139, "mdate": 1762918668139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}