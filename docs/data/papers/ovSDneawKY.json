{"id": "ovSDneawKY", "number": 864, "cdate": 1756821321786, "mdate": 1759898238403, "content": {"title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation", "abstract": "Existing AI-based film generation systems can generate high-quality videos, but struggle to design expressive camera language and establish cinematic rhythm. This deficiency leads to templated visuals and unengaging narratives. To address these limitations, we introduce FilMaster, an end-to-end automated film generation system that integrates real-world cinematic principles to generate professional-grade, editable films. Inspired by professional filmmaking, FilMaster is built on two key cinematic principles: (1) camera language design by learning cinematography from extensive real-world film references, and (2) cinematic rhythm by emulating professional post-production workflows. For camera language, our Multi-shot Synergized Camera Language Design module introduces a novel scene-level Retrieval-Augmented Generation (RAG) framework. Unlike shot-level RAG which retrieves references independently and often leads to visual incoherence, our approach treats an entire scene, comprising multiple shots with a shared spatio-temporal context and narrative objective, as a single, unified query. This holistic query retrieves a consistent set of semantically similar shots with cinematic techniques from a large corpus of 440,000 real film clips. These references then guide an LLM to synergistically plan coherent and expressive camera language for all shots within that scene. To achieve cinematic rhythm, our Audience-Aware Cinematic Rhythm Control module emulates professional post-production, featuring a Rough Cut assembly followed by a Fine Cut process that uses simulated audience feedback to optimize the integration of video and sound for cinematic rhythm. Extensive experiments show superior performance in camera language and cinematic rhythm, paving the way for generative AI in professional filmmaking.", "tldr": "", "keywords": ["video generation", "filmmaking"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62c19eaab0f0d415ee59957b6891e35f7d0bc571.pdf", "supplementary_material": "/attachment/947d1db0516647855c632cabedcc2242c443c54a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes FilMaster, an automated system for film generation that integrates cinematic principles—specifically, camera language design and cinematic rhythm control—into the video generation pipeline.\nIt introduces:\n\n* A Multi-shot Synergized Camera Language Design module using a scene-level Retrieval-Augmented Generation (RAG) from a 440k film dataset to plan expressive, coherent shots.\n\n* An Audience-Aware Cinematic Rhythm Control module that simulates post-production editing with “audience feedback” and multi-track sound design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Expressive camera work: Produces more natural and film-like results; shows improvement in “camera language.”\n\n* Comprehensive multi-stage pipeline: Covers both pre- and post-production, integrating video, audio, and editing.\n\n* Quantitative and qualitative validation: Outperforms baselines on FilmEval and user studies."}, "weaknesses": {"value": "* Pipeline complexity: The system is overly intricate, requiring multiple LLMs and retrieval steps; real-time practicality is unclear.\n\n* Manual intervention unclear: The amount of human selection or post-curation is not specified—e.g., how many outputs are generated and chosen.\n\n* Failure cases not discussed: The paper lacks analysis of when and why the method fails (e.g., scene inconsistency, unnatural transitions).\n\n* Audio-visual mismatch: Lip sync and timing issues occasionally appear, affecting realism.\n\n* Aesthetic awkwardness: Some close-up (“in-your-face”) camera shots feel forced or unnatural compared to simpler long-shots like in LTX-Video."}, "questions": {"value": "* Could modules (e.g., video generation, editing) be replaced with other backbones besides Kling? How modular is the system?\n\n* What level of human intervention or curation is required—how many outputs are generated per scene, and how is the best one selected?\n\n* What are the typical failure cases (e.g., incoherent motion, desynced audio, unrealistic rhythm)?\n\n* Have you evaluated the generalization to unseen domains (e.g., documentaries, dialogue-heavy content)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AizBd2EMLn", "forum": "ovSDneawKY", "replyto": "ovSDneawKY", "signatures": ["ICLR.cc/2026/Conference/Submission864/Reviewer_399a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission864/Reviewer_399a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760716004208, "cdate": 1760716004208, "tmdate": 1762915631832, "mdate": 1762915631832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a fully automated, end-to-end clip-level video generation agent. By providing multiple reference images and an initial prompt as input, the system invokes different models to iteratively modify both the prompt and the video. During the Coordination stage, the agent also retrieves suitable audio from an audio library for adaptation. The construction of cinematic language, agent-level video editing (such as acceleration and scene switching), and audio synchronization are particularly novel aspects of the approach. The experimental results, as well as the videos included in the supplementary materials, are impressive and refreshing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. An automatic end-to-end clip-level video generation agent with impressive performance.\n2. The Coordination Stage could edit the order and duration of the generated videos, which is reasonable as inter-clip videos do not have strict temporal order constraint. The audio fusion manner is also intuitive and suitable for agent like methods (retrieval and synchronized).\n3. The whole paper is well writen and easy to understand."}, "weaknesses": {"value": "The performance of this paper is great, my main concern lies on fair comparison with existing methods. This paper utilizes Kling 1.6 as the video generation model, which is much better than existing open-source model that previous method used, such as CogVideoX and LTX-Video. So is the performance improvement simply caused by the basic ability of Kling? It is recommended to have a fair comparison with MovieAgent to see the contribution of this paper."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wN8ZH3375V", "forum": "ovSDneawKY", "replyto": "ovSDneawKY", "signatures": ["ICLR.cc/2026/Conference/Submission864/Reviewer_rwQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission864/Reviewer_rwQb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760958402173, "cdate": 1760958402173, "tmdate": 1762915630591, "mdate": 1762915630591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FilMaster, an end-to-end automated film generation system designed to address key limitations in existing AI-based video generation. FilMaster proposes a Multi-shot Synergized Camera Language Design module which enables scene-level RAG that learns cinematography from movie references. To achieve a professional narrative, FilMaster further introduces Audience-Aware Cinematic Rhythm Control module for post-production workflow. The visualization experiments provide qualitative evidence for the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The overall method is easy to follow.\n- The visualizations in the supplementary materials offer clear qualitative support for the method's effectiveness."}, "weaknesses": {"value": "**1. Limited Novelty**\n\nI do not typically raise concerns about novelty lightly, but I must state that the technical contribution of this paper is highly limited. At its core, the proposed method is a relatively straightforward application of RAG. Crucial generative capabilities, such as identity preservation and high-fidelity video synthesis, appear to be inherited from the underlying foundational models used, rather than being novel contributions of the FilMaster framework itself. While the academic community certainly welcomes simple yet effective methods, such contributions usually offer a fundamental insight into the problem being studied. I do not believe this paper achieves that. Instead of solving a core technical challenge, the work primarily focuses on orchestrating existing components. For these reasons, the paper reads more like a well-executed technical project than a piece of novel research.\n\n**2. Subjective Evaluation**\n\nThe primary evaluation is based on a user study, which, while valuable, is inherently subjective. Although the authors supplement this with an automatic evaluation using Gemini, this approach is also a form of subjective assessment. The paper would be much stronger if it included more objective and popular metrics. For example, text-video similarity scores could be used to measure the faithfulness of script elaboration, and a quantitative metric for identity consistency across shots would provide more robust evidence of the system's capabilities.\n\nI understand a potential reason why authors did not discuss these metrics might be that these objective metrics would primarily test the performance of the base video generation model, not the proposed framework. However, this argument circles back to the core issue of novelty. If the main contribution is limited to the design of using cinematic language, and the measurable technical improvements are attributable to the underlying models, then the novelty of the framework itself is insufficient.\n\n\n**Conclusion**\n\nThe paper is methodologically sound, but its contribution feels incremental. It does not present significant shortcomings, but conversely, it lacks any clear and compelling advantages that would distinguish it from prior work. The contribution feels unsubstantial and does not appear to meet the bar for acceptance.\n\nI‘m willing to reconsider my score if the authors can provide a convincing rebuttal that thoroughly addresses my concerns."}, "questions": {"value": "Please See Weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The authors have commendably identified two significant ethical considerations for their work. First, there is the issue of societal biases from the source films. Second, the potential for misuse as a generative tool warrants serious consideration."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NMePzi4xQT", "forum": "ovSDneawKY", "replyto": "ovSDneawKY", "signatures": ["ICLR.cc/2026/Conference/Submission864/Reviewer_uN4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission864/Reviewer_uN4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964960165, "cdate": 1760964960165, "tmdate": 1762915630262, "mdate": 1762915630262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims at automated film video generation, i.e., automatically generating film videos given a text and sets of reference images for characters and locations as input. The proposed framework, named \"FilMaster\", attempts to surpass existing methods in terms of camera language and cinematic rhythm. The core contributions lie in the introduced Multi-shot Synergized Camera Language Design and Audience-aware Cinematic Rhythm Control, where the former is designed for camera language by introducing a shot-level RAG, and the latter module serves as a post-production process to refine the \"Rough Cut\" to \"Fine Cut\" for cinematic rhythm. FilMaster is evaluated on the proposed FilmEval benchmark and outperforms recent methods (Anim-Director, MovieAgent, and LTX-Studio) in both camera language and cinematic rhythm, which is further evidenced through a user study. The paper also presents ablation studies regarding the two proposed modules."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and underexplored area—bridging cinematic principles and AI-based film generation. The topic is both academically relevant and practically impactful.\n2. The authors explicitly ground their work in film principles (camera language, cinematic rhythm, audience perception, etc.) and emulate professional filmmaking workflows. This fills an existing academic gap between generative modeling and film studies.\n3. The scene-level retrieval and coordinated camera planning improve shot-level incoherence in previous RAG-based methods."}, "weaknesses": {"value": "1. While the system design is well-engineered, it lacks a deep technical innovation or theoretical insight at the algorithmic level. I would prefer to see more technically substantive modules rather than a purely workflow-oriented system.\n2. The system’s multi-stage pipeline (retrieval -> shot planning -> rough cut -> audience feedback -> fine cut -> sound production) introduces fragility. The paper doesn’t investigate how errors propagate or how robust the system is when upstream stages fail.\n3. Despite providing reference images, generated scenes may still lack consistency across shots, especially for partially occluded or unseen parts of reference subjects/scenes. The paper does not discuss this, though it is crucial for multi-shot storytelling.\n4. Some generated results in the supplementary video exhibit noticeable audio–video misalignment, including unsynchronized sound effects and inaccurate lip movements.\n5. The workflow depends on GPT-4o, Gemini, and Kling video generators. This makes reproducibility extremely difficult and limits transparency."}, "questions": {"value": "It remains unclear whether the system can produce varied cinematic styles or if retrieval biases constrain creativity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yqYiPIJ842", "forum": "ovSDneawKY", "replyto": "ovSDneawKY", "signatures": ["ICLR.cc/2026/Conference/Submission864/Reviewer_Pb14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission864/Reviewer_Pb14"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832934713, "cdate": 1761832934713, "tmdate": 1762915629970, "mdate": 1762915629970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}