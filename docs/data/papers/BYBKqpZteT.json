{"id": "BYBKqpZteT", "number": 13287, "cdate": 1758216054629, "mdate": 1763738748912, "content": {"title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "abstract": "Dynamical systems modeling is a core pillar of scientific inquiry across natural and life sciences. Increasingly, dynamical system models are learned from data, rendering identifiability a paramount concept. For systems that are not identifiable from data, no guarantees can be given about their behavior under new conditions and inputs, or about possible control mechanisms to steer the system. It is known in the community that \"linear ordinary differential equations (ODE) are almost surely identifiable from a single trajectory.\" However, this only holds for dense matrices. The sparse regime remains underexplored, despite its practical relevance with sparsity arising naturally in many biological, social, and physical systems.\n In this work, we address this gap by characterizing the identifiability of sparse linear ODEs. Contrary to the dense case, we show that sparse systems are unidentifiable with a positive probability in practically relevant sparsity regimes and provide lower bounds for this probability. We further study empirically how this theoretical unidentifiability manifests in state-of-the-art methods to estimate linear ODEs from data. Our results corroborate that sparse systems are also practically unidentifiable. Theoretical limitations are not resolved through inductive biases or optimization dynamics. Our findings call for rethinking what can be expected from data-driven dynamical system modeling and allows for quantitative assessments of how much to trust a learned linear ODE.", "tldr": "We investigate the identifiability challenges of  linear ordinary differential equations for sparse system matrix starting from a single trajectory.", "keywords": ["dynamical systems", "identifiability", "sparsity"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d3eeffe40bd6e0658d981a91433f6c8a2c48d09.pdf", "supplementary_material": "/attachment/0028c8819ea4968b8d9fb92316c90eff96f436a2.zip"}, "replies": [{"content": {"summary": {"value": "This paper revisits the identifiability of linear ODE systems $dx(t)/dt = Ax(t)$ when the system matrix $A$ is sparse. The authors show that, unlike the dense case where almost all systems are identifiable from a single trajectory, sparsity introduces a positive probability of unidentifiability. They define a sparse–continuous ensemble and prove a sharp phase transition: when sparsity exceeds roughly $1-ln n /n$, systems become globally unidentifiable with high probability. The paper also introduces a trajectory-level metric to quantify how close a trajectory is to being unidentifiable. Simulations confirm that identifiability deteriorates with higher sparsity, both theoretically and in practice, using SINDy and Neural ODE estimators"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Provides a clear theoretical characterization of when sparse linear ODEs lose identifiability.\n2. Decomposing failure probability into global vs. trajectory unidentifiability (Eq. 2) is conceptually clarifying, and the distance $d_A(x_0)$ is a helpful practical proxy.\n3. The sharp threshold result is elegant and connects to known results in random matrix theory.\n4. Writing is well structured, and assumptions are transparently stated."}, "weaknesses": {"value": "1. The strong assumptions (noise-free, single continuous trajectory, full observability) limit real-world applicability.\n2. For readers less familiar with random matrix theory, it would be helpful to include a brief intuition box or paragraph below Lemma 3 explaining why the identifiability transition occurs precisely at $1-ln n/n$. A short, high-level explanation would make this elegant result more accessible and highlight its connection to classic random graph thresholds.\n3. In the main text, the normalized Hamming distance is described as divided by $n$, but Appendix C.2 defines it as normalized by $n^2$. Please clarify which version was used, and make it consistent."}, "questions": {"value": "1. In Fig. 4, for small dimensions (e.g., $n=3,5$) SINDy sometimes achieves low normalized Hamming distance even at very high sparsity. Could you comment on this regime?\n2. In Section 2 (“Discussion of assumptions and limitations”), you might consider citing a related work on identifiability under hidden confounders: Wang et al. (2024) Identifiability analysis of linear ODE systems with hidden confounders."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wHziOaRxx", "forum": "BYBKqpZteT", "replyto": "BYBKqpZteT", "signatures": ["ICLR.cc/2026/Conference/Submission13287/Reviewer_7pZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13287/Reviewer_7pZ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760622026501, "cdate": 1760622026501, "tmdate": 1762923958814, "mdate": 1762923958814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Practical relevance of our results"}, "comment": {"value": "All reviewers raised questions about the relevance of our problem setting and the assumptions we make. In particular, our assumptions of continuous, noise-free, fully observed trajectories have been regarded as limiting the scope/applicability of our findings. Indeed, those would be limiting if this was a method proposal, where the method only works on such observations. However, for analyzing unidentifiability, they are **the exact opposite of limiting**: Without these assumptions, no system would ever be identifiable, i.e., unidentifiability would be trivially given. Only once we have eliminated unidentifiability due to, e.g., undersampling, finite samples, and partial observations, can we make non-trivial claims about the inherent unidentifiability of the underlying systems.\n\nLet us elaborate by splitting the discussion of our assumptions into two parts:\n**(A)** The assumptions of **continuous** trajectories, **noise-free** observations and **full observability** are assumptions about our ability to observe/measure the system. For these, our setting corresponds to the “best case” idealized setting where (in principle) infinite data (both in terms of sampling frequency as well as in terms of noise instantiations) are available about all relevant parts of the system. As we’ll discuss one-by-one below, violating any of those assumptions will add additional identifiability issues, which are due to limitations of our observation process (not inherent to the system) that one could in principle overcome. Hence, instead of “our results only apply under those assumptions,” our results show unidentifiability to “still occur under these assumptions,” even when all solvable sources of unidentifiability due to limited measurement processes have been overcome. In short, these form the appropriate starting point for an identifiability analysis. \n**(B)** The assumption of linear systems and sparsity are indeed assumptions about the underlying dynamics. We elaborate on the relevance of these assumptions below.\n\nTo summarize, by showing unidentifiability under those assumptions, our results transfer to all subsequent modifications for real-world modeling (but not vice versa) like discrete (scarce) observations, noise, non-linearities, partial observation. Therefore, our assumptions are not made for convenience (or in order to be able to prove something) but form the appropriate starting point for identifiability analysis.\n\n Let us go through the assumptions one-by-one for details.\n\n**(A1) Continuous trajectories**: Any real-world measurement device will have a maximum sampling frequency, so all real-world data is necessarily discrete in time. Discrete observations add trivial unidentifiability issues around aliasing (cf. aliasing in the Nyquist sampling theorem). Hence, the continuous observation case is the best-case scenario - the limit of infinite sampling frequency - where we expect any remaining unidentifiability to be due to the fundamental problem setting, not our insufficient sampling rate.\n\n**(A2) Noise-free observations**: Again, real-world data are almost always noisy. And again, in the presence of noise, system identification becomes inherently more difficult, namely probabilistic: multiple different systems may be compatible with the observed data, each with a different likelihood under the assumed noise model (for example, the standard assumption of i.i.d. additive Gaussian noise in classical machine learning often used to model “measurement errors”). Within this probabilistic framework, our noise-free setting can be interpreted as the best case scenario, where unidentifiability is not just due to suboptimal measurement devices. It can also be interpreted as the asymptotic limit of observing infinitely many trajectories for a single initial condition such that the “noise can be averaged out” to recover the noise-free setting. From this perspective, our results state under which conditions collecting more observations allows (in the asymptotic limit) the identification of the system. Or put differently, we show that there are cases in sparse systems where, even if more data was collected or where the noise level is reduced by other means, the true system remains unidentifiable."}}, "id": "Ul82pxzbv9", "forum": "BYBKqpZteT", "replyto": "BYBKqpZteT", "signatures": ["ICLR.cc/2026/Conference/Submission13287/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13287/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13287/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737315823, "cdate": 1763737315823, "tmdate": 1763737524176, "mdate": 1763737524176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical study of the identifiability of autonomous, linear and noise-free ODEs from a single trajectory, focusing on systems where the drift matrix is *sparse*. The authors distinguish between two notions of unidentifiability: (i) system-level (or global) unidentifiability, where the system is unidentifiable regardless of the initial condition, and (ii) trajectory-level unidentifiability, which occurs only for initial states lying in invariant subspaces of the system. \n\n*For system-level unidentifiability*, they show that in sparse systems it is implied by rank deficiency (i.e., zero-eigenvalue degeneracy, Lemma 1), derive a lower bound on its probability via the occurrence of system matrices with two zero columns (Lemma 2), and extend the analysis (Lemma 3) using random graph theory to identify a dimensionality-dependent sparsity threshold governing unidentifiability. \n\n*For trajectory-level unidentifiability*, they prove that the probability of “unlucky” initial conditions lying in invariant subspaces is zero, so identifiability depends solely on the system-level property. They further analyze near-invariant initial conditions, introducing a notion of distance to invariant subspaces, and showing that the indistinguishability time between trajectories of two systems agreeing on such a subspace increases inversely with this distance (Lemma 4). \n\nTheoretical findings are finally supported by numerical experiments, which rely on neural network (Neural ODE) and symbolic regression (SINDy) methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides strong context through a clear discussion of related work, assumptions, and limitations. This situates the contribution well within the system identification and machine learning communities and makes its relevance easy to grasp.\n2. The work extends classical results on the unidentifiability of linear systems to the practically important case of sparse systems, filling a notable gap in the existing theory.\n3. The proofs are well structured and technically interesting. \n4. The experimental section is well designed and directly supports the theoretical claims. The authors also apply two widely used system-identification methods (Neural ODE and SINDy) to test the practical implications of their results. The exposition of these empirical findings is clear and coherent (albeit limited, see Weekness 2 below).\n5. The appendix is comprehensive, providing detailed proofs, additional empirical results, and clear information for reproducibility.\n\nOverall, the paper is well written, technically sound, and contributes meaningful theoretical insight into the identifiability of sparse linear systems."}, "weaknesses": {"value": "1. The most immediate limitation lies in the restriction to linear and noise-free systems. While the authors are transparent about this assumption, it considerably narrows the scope of applicability to real-world or data-driven settings.\n\n2. The results on empirical unidentifiability (Section 5.3) are interesting but somewhat limited. The authors do not explicitly connect their theoretical findings on trajectory-level unidentifiability with their empirical unidentifiability results. For example, it would be interesting to examine how the two system-identification methods considered behave for initial conditions close to invariant subspaces, especially since the trajectory length should influence distinguishability in that regime (as suggested by Lemma 4).\n\n3. While the use of a Bernoulli mask to model sparsity is theoretically convenient, the authors neither discuss nor acknowledge (or at least I don't find such a discussion/acknowledgement) that real-world sparse networks often exhibit \"structured sparsity\" (e.g., hub nodes or community structure). The Bernoulli mask (i.e. Erdős–Rényi) model does not capture such degree statistics, which may limit the generality of the conclusions."}, "questions": {"value": "1. Why did you title Section 3 “Global Unidentifiability” instead of “System-level Unidentifiability”?\nPut differently, why introduce the term system-level unidentifiability later in Section 3 rather than consistently using global unidentifiability throughout? This shift in terminology is somewhat confusing.\n\n2. In Appendix B, you discuss the sparsity of real-world gene regulatory networks.\nWhat are the node degree statistics of these networks, and can they be reproduced by your sparse–continuous random matrix model? If not, how might structured or heavy-tailed degree distributions affect your theoretical results?\n\n3. Within your setup, is there a way to study how the closeness of an initial condition to an invariant subspace influences the performance of Neural ODE and SINDy? In particular, does the length of the observed trajectory or the inter-observation interval play a significant role in this regime, as suggested by Lemma 4?\n\n4. In Figure 4, SINDy appears to match the true sparsity pattern almost perfectly for the 3-dimensional case across all sparsity levels.\nHow should these empirical findings be interpreted in light of your sharp sparsity threshold for global unidentifiability, and the corresponding empirical verification in Fig. 2? In other words, why does SINDy succeed in this low-dimensional setting even in regimes predicted to be unidentifiable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "odP21LECEl", "forum": "BYBKqpZteT", "replyto": "BYBKqpZteT", "signatures": ["ICLR.cc/2026/Conference/Submission13287/Reviewer_h45Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13287/Reviewer_h45Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669014467, "cdate": 1761669014467, "tmdate": 1762923958501, "mdate": 1762923958501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors prove an identifiability result for a class of linear dynamics governed by a sparse-continuous random matrix. I went through the proofs in detail and they are reasonable, although this is outside my personal area of research so I can just say that the techniques and arguments seem reasonable. My only feedback is whether the authors can better motivate this class of sparse-continuous matrices and in what way they are relevant to machine learning. Of course identifying dynamics from time series is of interest, but a bit more discussion about why one should care about this is necessary."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Seems like a correct theoretical paper."}, "weaknesses": {"value": "See  my response to the summary - the relevance of this class of matrices needs to be better motivated. I couldn't tell whether this is an impactful result and the authors chose this problem because its important, or if the authors just chose a class of matrices for which they knew how to prove something. if they can help me understand that better i will raise the score."}, "questions": {"value": "no further questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yrxquSBJlP", "forum": "BYBKqpZteT", "replyto": "BYBKqpZteT", "signatures": ["ICLR.cc/2026/Conference/Submission13287/Reviewer_i8tC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13287/Reviewer_i8tC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928291167, "cdate": 1761928291167, "tmdate": 1762923958156, "mdate": 1762923958156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}