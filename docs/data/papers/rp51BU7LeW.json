{"id": "rp51BU7LeW", "number": 9231, "cdate": 1758115840105, "mdate": 1759897736509, "content": {"title": "V-MAGE: A Game Evaluation Framework for Assessing Vision-Centric Capabilities in Multimodal Large Language Models", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in visual-text processing. However, existing static image-text benchmarks are insufficient for evaluating their dynamic perception and interactive reasoning abilities. We introduce Vision-centric Multiple Abilities Game Evaluation(V-MAGE), a novel game-based evaluation framework designed to systematically assess MLLMs’ visual reasoning in interactive, continuous-space environments. V-MAGE features five distinct video games comprising over 30 carefully constructed evaluation scenarios. These scenarios are set in free-form, visually complex environments that require models to interpret dynamic game states and make decisions based solely on visual input, thereby closely reflecting the conditions encountered by human players. To ensure robust and interpretable comparisons across models, V-MAGE employs a dynamic Elo-based ranking system that accounts for varying difficulty levels and task diversity. Benchmarking state-of-the-art MLLMs against human baselines reveals that while leading models approach human-level performance in simple tasks, their performance drops significantly in complex scenarios requiring advanced reasoning and task orchestration. This persistent performance gap highlights fundamental limitations in current MLLMs’ ability to perform real-time, vision-grounded interactions. Through extensive analyses, we demonstrate the utility of V-MAGE in uncovering these limitations and providing actionable insights for improving the visual and reasoning capabilities of MLLMs in dynamic, interactive settings.", "tldr": "A game-based evaluation framework designed to assess MLLMs’ vision-centric reasoning capabilities.", "keywords": ["Multimodal Large Language Models", "Benchmark", "Game"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a9a511fc67a415f9cf3734aeab7e462f8b29cff.pdf", "supplementary_material": "/attachment/67c610bfbe6cbc229b746eee928f5744270b9f84.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces V-MAGE, a game-based evaluation framework for MLLMs that targets vision-centric abilities under dynamic, interactive conditions. V-MAGE comprises five video games with 30+ levels, provides visual-only inputs (continuous state spaces, difficulty tiers), and evaluates models through a modular game–agent–model pipeline. To compare heterogeneous tasks, it adopts an adaptive ELO-based ranking for interpretable cross-game scoring. Experiments on state-of-the-art MLLMs (with human references) show that models can handle simpler scenarios but degrade markedly on complex tasks requiring perception, temporal reasoning, memory, and action orchestration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured, with clear motivation against text/grid baselines and a readable exposition of the ELO-based scoring. Figures and sectioning make the contributions and gaps intuitive.\n\n2. The framework is practical: lightweight game environments, standardized interfaces, and an extensible level design lower the barrier to adoption and invite community contributions (new games, agents, or metrics) while supporting reproducibility.\n\n3. The core idea—using controllable, visual-only video games with continuous states to evaluate vision-centric, interactive reasoning—is timely. The modular game–agent–model design plus an adaptive ELO scheme pushes beyond text/grid reductions and enables comparable, cross-level scoring."}, "weaknesses": {"value": "1. The paper does not convincingly explain why these five games, beyond high-level descriptions. It remains unclear how each game uniquely probes a specific vision-centric or interactive capability, or whether the set is minimally sufficient/covering.\n\n2. Several state-of-the-art closed-source models (e.g., GPT-o1/o3, Gemini 2.5, Claude family) are absent, leaving open whether the reported gaps persist under the strongest available systems. This weakens external validity and limits the benchmark’s positioning.\n\n3. Most importantly, becuase these games are widely available online, strong performance may partly reflect prior exposure rather than genuine vision-centric capability. The paper does not present leakage audits or controls to rule out memorization effects."}, "questions": {"value": "1. What a priori criteria led to these five games (vs. others), and how does each map to a concrete capability taxonomy (e.g., perceptual binding, spatiotemporal prediction, visual working memory, causal control)?\n\n2. Are there counterexamples you considered but excluded—why?\n\n3. Could you include some current top-tier closed model (e.g., GPT-o1/o3, Gemini 2.5, Claude) or a carefully controlled proxy comparison?\n\n4. Please report performance under standardized prompting regimes: zero-shot, k-shot ICL (vary k), and CoT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sLDqvIST8j", "forum": "rp51BU7LeW", "replyto": "rp51BU7LeW", "signatures": ["ICLR.cc/2026/Conference/Submission9231/Reviewer_4tME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9231/Reviewer_4tME"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018272821, "cdate": 1761018272821, "tmdate": 1762920888704, "mdate": 1762920888704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces V-MAGE, an interactive and visually rich evaluation framework focused on dynamic interaction and vision-centric reasoning (also serving as a sandbox for vision agent development). Using V-MAGE, the authors evaluated various public MLLMs with ELO scores, highlighting a significant gap between model performance and human proficiency in complex tasks. They further analyzed why current MLLMs perform poorly in video game tasks, including deficiencies in fundamental visual capabilities, reasoning challenges during prolonged interactions, and issues like anchoring bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the inability of existing static image-text benchmarks to evaluate MLLMs’ dynamic perception and interactive reasoning; it is both an evaluation framework focusing on \"dynamic interaction + vision-centric reasoning\" and a sandbox for vision agent development.\n2. Adopts a dynamic ELO ranking system to resolve score scale differences across games, enabling standardized cross-model comparison and accurate measurement of incremental progress in non-linear scoring tasks.\n3. Quantifies MLLMs’ performance via ELO scores (e.g., near human level in simple tasks, large gaps in complex tasks) and analyzes poor performance causes (visual capability deficiencies, reasoning bottlenecks, etc.), providing clear directions for MLLMs optimization."}, "weaknesses": {"value": "1. Only 5 human participants were invited to establish the performance baseline; the small sample size may affect the representativeness and robustness of the human benchmark.\n2. In-depth analysis of model errors (e.g., manual annotation of error types) focuses mainly on GPT-4o, with insufficient coverage of error characteristics of other models, limiting the generalizability of conclusions."}, "questions": {"value": "1. What specific mechanisms does V-MAGE’s ELO rating system use to address the issue of score scale differences across different games?\n2. Model error analysis focuses mainly on GPT-4o, and no equally in-depth error annotation is conducted for other models (e.g., Qwen2.5VL-72B, InternVL2.5-78B); are there technical or resource constraints behind this choice?\n3. The paper mentions that V-MAGE’s game levels have the \"visually irreducible\" feature; how is this feature reflected in specific levels (e.g., Race or SuperMario)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3y9zePjiP3", "forum": "rp51BU7LeW", "replyto": "rp51BU7LeW", "signatures": ["ICLR.cc/2026/Conference/Submission9231/Reviewer_Xs7D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9231/Reviewer_Xs7D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636260943, "cdate": 1761636260943, "tmdate": 1762920888296, "mdate": 1762920888296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark V-MAGE to measure how well MLLMs like GPT4o or Gemini can perform real-time visual reasoning in fixed-screen and platformer video games - simulated environments that are reminiscent of interactive and continuous spaces."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I personally find this work possesses the following strong aspects:\n\n1. A refreshing angle to tackling a widely recognized issue.\n2. Good awareness on shortcut prevention as in Chap 3.2 - It’s quite commendable to see the authors make sure the visual inputs cannot be easily converted to simple texts."}, "weaknesses": {"value": "I find the work at its current stage suffers from the following major weak points.\n\n1. **(Soundness) Ambiguity in the ELO-Based Evaluation Metric.** Although I personally have a brief knowledge of ELO ranking in chess and go, the description of the ELO ranking mechanism used in this work is not fully intuitive for readers unfamiliar with it. In fact, the current metric is inconsistent on what constitutes a “win” or “loss” in each game - whether success corresponds to surviving longer (e.g. Flappy Bird), completing levels without failure (e.g. Super Mario), or achieving higher raw scores (e.g. Pong). The criteria for scoring the same ELO score are different across the games included in this study. We would need a clearer definition of **unified evaluation standards** for better transparency and interpretability.\n\n2. **(Presentation) Limited Robustness of Human Baselines.** The reported human baselines may lack robustness, as individual skill levels and familiarity with specific games can strongly influence performance. This unpredictable variability in skill weakens the reliability of the human-based reference baselines shown in Figure 4 and 6. To counter this, I encourage the authors might consider incorporating **Tool-Assisted Speedruns (TAS)** or other controlled, reproducible playthroughs to establish a more stable *theoretical upper bound* on human-level performance. For example, many TAS runs for Super Mario Bros in video format can be found on: https://tasvideos.org/1G .\n\n3. **(Contribution) Unclear Downstream Usage of V-MAGE.** While V-MAGE is proposed as an evaluation benchmark, its broader practical utility is not well demonstrated. To be specific, *it is unclear to me how improved performance on V-MAGE translates to advancements in real-world or practical multimodal reasoning tasks.* Can the samples in V-MAGE be integrated into **post-training or reinforcement learning pipelines** (e.g., SFT, DPO, or GRPO) to enhance visual reasoning? Unfortunately, this work has not yet demonstrated much concrete proof for its relevant downstream usage."}, "questions": {"value": "Please find my major concerns in the Weakness section. Out of the three concerns, I suggest the authors prioritize the 1st and the 3rd, given the time limit for rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jyhV3CJSqj", "forum": "rp51BU7LeW", "replyto": "rp51BU7LeW", "signatures": ["ICLR.cc/2026/Conference/Submission9231/Reviewer_aqeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9231/Reviewer_aqeE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762221313246, "cdate": 1762221313246, "tmdate": 1762920887923, "mdate": 1762920887923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces V-MAGE, a game-based evaluation framework designed to assess the vision-centric reasoning and interaction capabilities of multimodal large language models (MLLMs). The benchmark consists of five visually rich video games with over 30 dynamic levels, requiring models to perform perception, temporal reasoning, and decision-making based solely on visual inputs. V-MAGE employs a dynamic ELO-based ranking system for consistent model comparison. Experimental results across leading MLLMs (e.g., GPT-4o, Gemini, Qwen-VL) and human baselines reveal a clear gap between models and humans in dynamic, vision-grounded tasks, highlighting current MLLMs’ limitations in temporal reasoning and interactive planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a well-designed evaluation framework. In particular, the introduction of a dynamic ELO-based ranking system provides a unified and interpretable metric across heterogeneous game environments, effectively mitigating the bias caused by varying score scales and task difficulty levels.\n\n- The study offers comprehensive experimental details, including environment configurations, prompt templates, and implementation settings in the appendix. This high level of transparency ensures that the results are fully reproducible and facilitates fair benchmarking and future extensions."}, "weaknesses": {"value": "- **Overemphasis on Visual-Only Modality**: The evaluation framework focuses exclusively on visual input, omitting textual or memory-based information that is often present in real-world multimodal scenarios. This design choice, while emphasizing visual reasoning, may cause a distributional mismatch with practical multimodal applications that rely on both linguistic and contextual cues.\n\n- **Simplified Game Environments**: Although the selected games (e.g., FlappyBird, Pong) introduce dynamic visual elements, they remain relatively low in semantic complexity. As a result, the benchmark may not fully capture the semantic understanding and strategic planning challenges encountered in more realistic interactive environments."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3WTZpMQ9DL", "forum": "rp51BU7LeW", "replyto": "rp51BU7LeW", "signatures": ["ICLR.cc/2026/Conference/Submission9231/Reviewer_4GB9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9231/Reviewer_4GB9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762445145989, "cdate": 1762445145989, "tmdate": 1762920887523, "mdate": 1762920887523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "## Summary  \nThis paper proposes V-MAGE, a game-based, vision-centric evaluation suite for Multimodal Large Language Models (MLLMs). The framework spans 5 video games and over 30 levels, featuring a minimal agent wrapper, multi-frame inputs, and an ELO-style comparative ranking system for evaluating models across heterogeneous tasks. Experiments cover a range of state-of-the-art models, report notable human–model performance gaps, and include unit tests for core visual abilities (positioning, tracking, visual grounding, and timing). Additionally, the paper analyzes anchoring bias in model inference and presents a hand-annotated error taxonomy. Sections on ethics and reproducibility are included, and the appendices provide details on implementation procedures and prompt designs.  \n\n\n## Strengths  \n- **Clear Problem Framing**: The paper compellingly argues that grid-based or text-reducible games fail to adequately stress vision-centric competencies of MLLMs, while free-form video games serve as an appropriate stress test for such abilities.  \n- **Comprehensive Decomposition and Coverage**: The framework includes 5 distinct games, a multi-level design, and unit tests that map to interpretable sub-skills—with weaknesses in tracking and timing being particularly insightful.  \n- **Cross-Task Comparability**: The ELO-based ranking system delivers a unified, scale-robust metric for comparing models across diverse tasks, while still retaining per-level diagnostic capabilities to identify fine-grained performance differences.  \n- **Robust Error and Bias Analysis**: The quantification of anchoring bias and qualitative breakdown of model errors go beyond mere leaderboard reporting, providing actionable insights to guide the community in addressing specific model limitations.  \n- **Human Baselines and Ablation Studies**: Incorporating human gameplay baselines and a “perception bypass” ablation (via textual descriptions of game states) strengthens the claim that both visual perception and downstream reasoning jointly constrain model performance.  \n\n\n## Weaknesses  \n1. **Insufficient Root Cause Analysis**: While the paper effectively documents the existence of model limitations (e.g., poor tracking ability), it lacks a deeper exploration of their underlying causes. For instance, are these deficits attributable to architectural constraints (e.g., inadequate temporal modeling in vision encoders), gaps in training data (e.g., limited exposure to dynamic visual sequences), or inference inefficiencies (e.g., suboptimal frame sampling or context window limitations)?  \n2. **Limited Connection to Real-World Model Capabilities**: The paper only evaluates model performance through game-specific scores, with insufficient analysis of how these scores correlate with the models’ abilities in real-world vision-centric scenarios  \n\n\n## Questions  \nQ1: Could you supplement the ablation analysis to disentangle whether model limitations stem from the agent framework (e.g., agent-side processing bottlenecks) versus inherent deficiencies in the MLLMs themselves? (Addressed to Weakness 1)  \nQ2: Could you demonstrate the connection between models’ performance on V-MAGE and their capabilities in real-world tasks to further validate the benchmark’s effectiveness? (Addressed to Weakness 2)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "See Summary"}, "questions": {"value": "See Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0CheAAk0h7", "forum": "rp51BU7LeW", "replyto": "rp51BU7LeW", "signatures": ["ICLR.cc/2026/Conference/Submission9231/Reviewer_J2KG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9231/Reviewer_J2KG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762908222415, "cdate": 1762908222415, "tmdate": 1762920887085, "mdate": 1762920887085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}