{"id": "TXZ54qxdAF", "number": 21787, "cdate": 1758321747894, "mdate": 1759896902915, "content": {"title": "Weak-to-Strong Generalization with Failure Trajectories", "abstract": "Weak-to-Strong generalization (W2SG) is a new trend to elicit the full capabilities of a strong model with supervision from a weak model. While existing W2SG studies focus on simple tasks like binary classification, we extend this paradigm to complex interactive decision-making environments.  \nSpecifically, we fine-tune a strong model with trajectories of intermediate actions generated by a weak model. Motivated by the human learning process, we propose to generalize not only success knowledge but also failure experience so that the strong model can learn from the failed trajectories accumulated by weak models. To effectively and efficiently elicit the potential of strong agents, we further construct ``trajectory trees,\" a hierarchical representation that organizes weak model-generated action trajectories, coupled with Monte Carlo Tree Search (MCTS) to optimize the strong model. Through theoretical analysis, we provide formal guarantees for the effectiveness of our method in improving W2SG performance. Our empirical evaluations demonstrate substantial improvements in reasoning and decision-making capabilities across diverse task domains, validating the scalability and robustness of our proposed framework. Our code is available at: https://anonymous.4open.science/r/5A51.", "tldr": "", "keywords": ["Weak-to-Strong Generalization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/148f607fcfd1f79da6da59c6d784cddea49d58a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether the Weak-to-Strong generalization (W2SG) paradigm can be extended from binary classification to sequential decision-making environments. The core idea is to fine-tune a stronger model using trajectories sampled by a weaker model. In order to achieve this, the authors propose to construct _trajectory trees_ (which are game/decision trees applied to text generated by LLMs) from the weak model rollouts, and use this tree to select which paths should be used to tune the stronger model. They present two approaches: select preference pairs for DPO from divergence points in the tree (TreeDPO), or use MCTS to identify paths with strong performance for SFT (W2SG with MCTS). For TreeDPO, they prove a theorem that, under strict assumptions, guarantees performance improvement over a strong SFT baseline. Finally, the empirical results on WebShop, Science World, and AlfWorld suggest W2SG methods  can improve the strong model's performance. \n\n**Recommendation:**\\\nEven though this topic falls quite far outside my area of expertise, I am an expert on exploration in sequential decision-making environments and reinforcement learning.  With this perspective, I recommend to reject this paper. It appears to lack in its narrative and framing within existing literature: it is framed as an W2SG approach but seems to not be aligned in the overall objective. Furthermore, it is currently not possible to judge the significance of the empirical results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The W2SG paradigm seems very interesting, and its extension to sequential decision-making environments, the central focus of this paper, is highly relevant. \n- The main approach, to construct game trees and uses those to select better fine-tuning candidates, is very interesting."}, "weaknesses": {"value": "- The framing of the paper appears slightly confused. From my understanding, the goal of the W2SG paradigm seems to be around aligning strong models to the preferences and intentions of a weaker model (whilst ideally not losing performance). However, the approaches in this paper don't seem to share this goal. Instead, they seem solely focused on improving the strong agent's performance. As such, the proposed methods don't seem to be part of the core objective of the W2SG paradigm, but the paper _is_ framed in this way. Instead, the approaches are very reminiscent of the field of exploration and MCTS in reinforcement learning, where the objective is to use diverse, exploratory trajectories to improve the performance of a policy in a sequential decision making environment. However, the paper is not framed in this way, and seems to lack positioning within this particular field of research. Therefore, the framing of the paper seems to fit neither of these two paradigms in its current state. \n- It is not possible to judge the significance of the empirical results. There is no mention in the paper of how significance is tested, or how many/which seeds were used. As such, whether the proposed methods actually improve over any of the baselines is currently not possible to judge."}, "questions": {"value": "- How do the proposed approaches align with the core alignment objective of the W2SG paradigm?\n- How is the significance of the empirical results judged? How many seeds were used and how did you avoid accidental bias during seed selection? \n- Theorem 1: It appears to me, that this theorem proves that under the assumption that the underlying algorithm improves performance (Assumption 3), the algorithm will improve performance. But this seems not very surprising. How is this interpretation wrong, and/or in what way is Theorem 1 significant or novel? \n\n\n\n**Questions that did not heavily impact decision:**\n- Line 278: You introduce node statistics but I cannot find where these are defined?\n- Line 285: You state MCTS is used to create a dataset of paths, but it is unclear to me how the proposed approach can generate more than one path? Doesn't the result of the MCTS iterations produce only a single path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oM16RFF4lj", "forum": "TXZ54qxdAF", "replyto": "TXZ54qxdAF", "signatures": ["ICLR.cc/2026/Conference/Submission21787/Reviewer_yTwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21787/Reviewer_yTwq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920804338, "cdate": 1761920804338, "tmdate": 1762941931038, "mdate": 1762941931038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the Weak-to-Strong Generalization (W2SG) framework to decision-making tasks for LLM agents. Instead of relying on human-labeled data, the method allows a strong LLM to learn from full action trajectories generated by a weaker agent, reducing the need for human supervision. The authors represent these trajectories as hierarchical trees that merge shared prefixes between successful and failed trajectories. This structure identifies divergence points where decisions lead to different outcomes, enabling preference-pair sampling guided by the tree rather than random (as standard DPO).\n\nThe authors propose to use Offline Monte Carlo Tree Search (MCTS) to sample high-quality trajectories used to optimize the strong model. The paper also provides a theorem showing that, under certain assumptions, the policy minimizing the proposed TreeDPO objective can outperform models trained with supervised fine-tuning (SFT). Empirical results on three agent benchmarks, demonstrate that W2SG under weak supervision improves both average reward and success rate over SFT baselines, approaching or surpassing the performance of expert-trained models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The extension of the W2SG framework to sequential decision-making for LLM agents is timely and relevant, it offers a way to reduce the need of human supervision in interactive reasoning tasks.\n\nThe use of hierarchical trajectory trees is well-motivated and effectively captures shared paths between successful and failed rollouts, enabling more informative preference sampling than standard DPO.\n\nThe paper includes a theoretical result showing that optimizing the proposed TreeDPO objective can outperform supervised fine-tuning (SFT) under certain assumptions.\n\nIncorporating offline MCTS to extract high-quality trajectories strengthens the training signal for the strong model and proves effective across the three evaluated agent benchmarks."}, "weaknesses": {"value": "1. The approach relies heavily on the quality and diversity of trajectories generated by the weak model; limited exploration could constrain the effectiveness of the strong modelâ€™s improvement.\n\n2. The theoretical result provides an interesting insight, but its practical implications are unclear. It seems that when the weak policy \\pi_w^\"sft\"  diverges substantially from the optimal policy \\pi^*, the SFT-trained strong model could perform better. The paper does not analyze conditions under which TreeDPO is beneficial or when weak learners/trajectories should be discarded. This could make the theorem more actionable. \n\n3. Evaluation is done only on three environments, and a single model family (Llama)."}, "questions": {"value": "See weaknesses and also:\n\nQ1. Could you clarify which aspects of Theorem 1, are specific to the tree-based formulation? I can see the weak-to-strong policy relationship clearly but not the tree related."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LJXFBgrnJj", "forum": "TXZ54qxdAF", "replyto": "TXZ54qxdAF", "signatures": ["ICLR.cc/2026/Conference/Submission21787/Reviewer_jPwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21787/Reviewer_jPwv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139846226, "cdate": 1762139846226, "tmdate": 1762941930765, "mdate": 1762941930765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for Weak-to-Strong Generalization (W2SG) in complex decision-making tasks using large language model (LLM) agents. The key idea is to fine-tune a strong model using both successful and failed action trajectories generated by a weaker model. To organize these trajectories, the authors introduce a \"trajectory tree\" structure that captures shared prefixes and divergence points between success and failure paths. Two learning strategies are proposed: (1) Tree-guided Direct Preference Optimization (Tree-DPO) based on structured contrastive pairs, and (2) Monte Carlo Tree Search (MCTS) to extract high-quality paths for imitation. The paper provides theoretical guarantees and empirical results across multiple environments, demonstrating that strong models trained via this method can outperform standard supervised fine-tuning baselines and approach expert-level performance using only weak supervision."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "a. Introduces trajectory trees for organizing weak model trajectories and identifies divergence points to train strong models. It is a novel application of structured contrastive learning.\n\nb. Provides a formal performance guarantee for the Tree-DPO method, showing that weak supervision can outperform expert-labeled SFT under specific assumptions.\n\nc. Demonstrates strong performance gains across multiple complex environments using only weak model trajectories."}, "weaknesses": {"value": "a. The approach relies on the weak model producing sufficiently diverse and informative trajectories. Performance may degrade if the weak model is too underperforming.\n\nb. The MCTS variant lacks a formal performance analysis (unlike Tree-DPO).\n\nc. While the trajectory tree representation is compelling, an ablation study comparing with unstructured contrastive pairs would help quantify its specific contribution."}, "questions": {"value": "a. Can the authors elaborate on how the method performs when the weak model is significantly weaker?\n\nb.  Could the authors quantify the computational cost of trajectory tree construction and MCTS? How scalable is the method to tasks with longer action horizons or larger vocabularies?\n\nc. Is there a possibility of iterated W2SG, e.g., bootstrapping progressively stronger models via self-distillation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LSvbkPBeiS", "forum": "TXZ54qxdAF", "replyto": "TXZ54qxdAF", "signatures": ["ICLR.cc/2026/Conference/Submission21787/Reviewer_SFUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21787/Reviewer_SFUW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762763900618, "cdate": 1762763900618, "tmdate": 1762941930521, "mdate": 1762941930521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}