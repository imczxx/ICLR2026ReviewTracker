{"id": "64Ea2Dx0JJ", "number": 15426, "cdate": 1758251231066, "mdate": 1759897307728, "content": {"title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?", "abstract": "Vision-language models (VLMs) have recently demonstrated strong performance in image geolocation, identifying images' location to a precision that now surpasses specialized systems. This capability poses a serious privacy threat, as these widely accessible models may enable malicious actors to infer sensitive locations from photos casually shared online. While prior work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate extent of location information to disclose. To evaluate this ability, we introduce a benchmark challenging VLMs to interpret latent social cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their abilities to precisely geolocate images, models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.", "tldr": "A benchmark for evaluating VLMs' contextual privacy judgment in geolocation tasks.", "keywords": ["Benchmarking", "NLP datasets", "Evaluation Methodologies", "Privacy", "Geolocation", "VLM", "Contextual Integrity"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39b7dcca829fff2ee9faad5649c349aae67729fd.pdf", "supplementary_material": "/attachment/fc103a344b8d21d943abc23d359402ea2e7dfcf3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VLM-GeoPrivacy, a benchmark designed to test whether VLMs respect contextual integrity when disclosing image locations.\nIt evaluates 14 leading VLMs on 1,200 real-world images annotated for context, intent, and appropriate disclosure granularity, showing that current models frequently over-disclose sensitive locations and fail to align with human privacy norms.\nEven advanced models like GPT-5 and Gemini-2.5 achieve only about 50% agreement with human judgments and are easily manipulated by iterative or adversarial prompts.\nThe authors conclude that future multimodal systems must incorporate context-aware privacy reasoning rather than blanket disclosure limits to protect users from location-based privacy risks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Evaluate many frontier models, providing a comprehensive view.\n+ The framing of the paper is good. The motivation is clear. I like the idea that “models should be aware of whether the geo-information in specific images can be revealed.” The framing about three possible use cases in section 2.1 is good.\n+ The questions are constructed based on some existing regulations. The authors used an iterative design of annotation for better refinement."}, "weaknesses": {"value": "- The conversion from the Nissenbaum’s Contextual Integrity and some established regulations to Q1-7 lacks some supports. [See my Q1]\n- Some designs look ad-hoc: Phi-3.5 for filtering, GPT-4o-mini for labeling, GPT-4.1-mini for the judge. [See my Q2]\n- As the dataset is the core part of this paper. Information about annotation process should be clearer. [See my Q3]"}, "questions": {"value": "1. Line 155-156: “we found that adding these concrete, intermediate questions improves annotator consistency.” I want to know that how much do you think that the previous questions are deliberate guidance to annotators towards a desired result? The Krippendorff’s alpha = 0.83 is good. Do you test the alpha when Q1-6 are not given? I think people’s intuition should also be considered. Another possible test can be first letting annotators do Q7, then inviting them to give explanations using Q1-6 to see the contributions of Q1-6 in human decisions.\n2. Do you have human verification on Phi-3.5-Vision filtering results? A better way may be using multiple VLMs and do majority voting? Do you use GPT-4o-mini for classifying images to the several privacy-sensitivity categories instead of Phi-3.5? Why changing to GPT-4o-mini for this task?\n3. Line 238-244: I am still confused. Do you only annotate 400 images out of the 1,200? Where did you recruit annotators? How many images did each of them annotate? What was the average working hours and how much was paid per hour? What is the demographic information of the annotators? I am asking the last question because I think there locations, nationalities, birthplaces can affect their familiarity of the images, thereby affecting Q1. Can you also provide the location distribution of your 1,200 images?\n4. Does “context” refer to Q1, Q4, Q5, and Q6?\n5. Do you ask models Q1-Q7 in a consecutive context or separately?\n6. Should elaborate more on the differences between the three privacy leakage metrics, i.e., Loc, Vio, and Over-Disc.\n7. Do you compare models’ own Q7 granularity answers and their free-form generation extracted granularity answers? Is there any relationship? Currently from Table 2 the two accuracies vary a lot.\n\nMinor suggestions and typos:\n1. Duplicated references: Line 565 to 571; Line 648 to 656.\n2. Table 1: middle image, Q4, “buit” -> “but”.\n3. Fig. 2: font size is too small.\n4. Table captions should appear before tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1RAOuSWcs", "forum": "64Ea2Dx0JJ", "replyto": "64Ea2Dx0JJ", "signatures": ["ICLR.cc/2026/Conference/Submission15426/Reviewer_RmhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15426/Reviewer_RmhQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761005190557, "cdate": 1761005190557, "tmdate": 1762925702412, "mdate": 1762925702412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLM-GEOPRIVACY, a benchmark to test whether vision–language models (VLMs) respect contextual integrity when disclosing location from images. It assembles real-world photos with seven annotations (e.g., sharing intent, visibility of people, acceptable disclosure granularity), evaluates multiple generation regimes (vanilla, iterative reasoning, adversarial prompting), and measures over/under-disclosure and policy violations. Experiments on many VLMs reveal strong geolocation ability yet systematic over-disclosure, with few-shot, context-matched exemplars partially mitigating harm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and important problem: contextualized privacy in image geolocation is underexplored yet high-impact for deployment safety.\n2. Evaluates a diverse set of VLMs under vanilla, chain-of-thought, and adversarial setups, yielding informative failure patterns.\n3. Generally well-written with transparent descriptions of proposed dataset and metrics, making the study easy to follow."}, "weaknesses": {"value": "1.\tInsufficient baselines weaken claims of “strong geolocation.” The paper compares only across VLMs; it lacks head-to-head evaluation against dedicated geolocation systems (e.g., retrieval-based pipelines) on the same test set. I suggest that the authors can add specialized geolocation and classical CV baselines [R1][R2], or report directly comparable numbers from prior work with a careful discussion of any differences.\n\n[R1] Ma, Wanlun, et al. \"LocGuard: A location privacy defender for image sharing.\" IEEE Transactions on dependable and Secure Computing 21.6 (2024): 5526-5537.\n\n[R2] Clark, Brandon, et al. \"Where we are and what we're looking at: Query based worldwide image geo-localization using hierarchies and scenes.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n2.\tSingle-judge dependence for free-form granularity grading risks bias. Using GPT-4.1-mini as the sole judge—while also being a system under evaluation—invites same-family preference and style-matching artifacts. It will be better to introduce at least one independent judge from a different provider/architecture and report inter-judge agreement; include tie-breaking rules.\n3.\tLimited human auditing. The reported manual check (300 samples, ~96% agreement) is too small to ensure robustness across models and prompts. I recommend to expand human verification to ≥1,000 samples, stratified by model, prompt regime, and predicted label; report confidence intervals and error breakdowns.\n4.\tCultural and legal context sensitivity is under-analyzed. Heuristics (e.g., children/indoor/political rallies → restrict disclosure) may vary across jurisdictions and norms. It is recommended to add boundary-case discussions (e.g., public interest in mass protests vs. participant risk), collect cross-region annotations, and report inter-cultural agreement for key questions.\n5.\tData licensing and release plan remain unclear. The ethics statement says the dataset will be released under CC BY-NC 4.0, but the data-sourcing section lists platforms including Flickr and Shutterstock without clarifying whether the latter is redistributable under CC BY-NC. If not, please state explicitly whether all non-CC BY-NC–redistributable items were actually excluded and describe the filtering procedure.\n6.\tInference-time settings are not comparable or statistically characterized. Models use different reasoning modes/budgets; decoding uses temperature 0.7 without multiple runs. I suggest that the authors can standardize reasoning budgets, run multiple seeds, and report mean±std (or CIs) for all key metrics; consider deterministic decoding (temperature 0) for safety-critical refusal metrics.\n7.\tGeoparsing pipeline may induce systematic bias. Location strings are extracted by a specific LLM and geocoded via a single API, which can over-resolve ambiguous or homonymous toponyms. The authors should report parsing/geocoding failure rates, ambiguity handling, and ablations with alternative extractors (regex/lexicons) and geocoders; analyze sensitivity of street/city/region accuracy to these choices."}, "questions": {"value": "see above comments"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The ethics statement says the dataset will be released under CC BY-NC 4.0, but the data-sourcing section lists platforms including Flickr and Shutterstock without clarifying whether the latter is redistributable under CC BY-NC. If not, please state explicitly whether all non-CC BY-NC–redistributable items were actually excluded and describe the filtering procedure."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zBCh27dfo1", "forum": "64Ea2Dx0JJ", "replyto": "64Ea2Dx0JJ", "signatures": ["ICLR.cc/2026/Conference/Submission15426/Reviewer_duDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15426/Reviewer_duDY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761007238718, "cdate": 1761007238718, "tmdate": 1762925701418, "mdate": 1762925701418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a visual contextual integrity benchmark for geolocating with VLMs. The benchmark consists of 1200 images with ground-truth location labels and seven human-labeled contextual questions/cues that define the privacy context of the image. Using this benchmark, various VLMs are evaluated for i) judging/replicating the fine-grained privacy context of the image, and ii) providing the appropriate level and accurate geolocation information. The evaluation shows that current models are heavily miscalibrated in terms of visual contextual privacy. Preliminary experiments with few-shot examples show promise for either inference-time or training-time improvements on contextual integrity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Important and timely problem.\n- Sound methodology for constructing the benchmark. Especially appreciated are the efforts made to calibrate the labels and labeling questions well.\n- I believe the benchmark will enable targeted research on improving CI for VLMs.\n- Interesting and sound evaluation. Promising results with few-shot prompting. Kudos for evaluating different levels of adversaries for location inference."}, "weaknesses": {"value": "- The paper focuses solely on geolocation, which is easy to evaluate. However, as shown by prior work [1] (btw a missing relevant citation in this paper) VLMs are capable of inferring other private attributes from images as well, such as sex, age, or income.\n- This is maybe half a question: The paper currently defines the appropriate privacy context for each image according to global guidelines. However, in practice I could imagine that a user might not intend to share their location through a given image, independently of which privacy context it would fall into in the framework of contextual integrity. How could one account for that? Is CI the right tool in this case, or is it maybe indeed better if models were to simply refuse to do geolocation (or other private attribute inferences)?\n- The paper would benefit from a discussion of mitigating inference risks, both on the users' side and on the providers' side.\n- The paper does not comment explicitly in the main part on the geographic/cultural distribution/bias of neither the images nor the concrete instantiation of the contextual integrity framework. I would assume that at least some of the labels (both location and privacy context) would change depending on the given cultural interpretation of the framework. I believe to a certain extent this is already accounted for implicitly (e.g., Q1), but I wonder if the authors can add more to this. Obviously, this work is a first step, and a benchmark aimed explicitly and cultural variations and diversity is definitely more in the scope of follow-up work.\n\n**References**\n\n[1] Tömekçe et al., Private Attribute Inference from Images with Vision-Language Models. NeurIPS 2024."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ytId3XzADV", "forum": "64Ea2Dx0JJ", "replyto": "64Ea2Dx0JJ", "signatures": ["ICLR.cc/2026/Conference/Submission15426/Reviewer_rnXc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15426/Reviewer_rnXc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761068983171, "cdate": 1761068983171, "tmdate": 1762925700987, "mdate": 1762925700987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VLM-GEOPRIVACY, a 1.2 K-image benchmark for testing whether vision-language models (VLMs) respect contextual integrity when describing locations. Each image is labeled for visual recognizability, subject visibility, and an “appropriate disclosure” level (e.g., refuse / city / exact place). Fourteen open- and closed-source VLMs are evaluated in multiple-choice and free-form settings.\nResults show models often over-disclose (~50 % cases) and fail to judge privacy context correctly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Problem is socially relevant: privacy and location disclosure are important deployment issues.\n\n* Benchmark and labeling are carefully designed\n\n* Evaluation covers many models and prompt styles, producing a clear quantitative picture.\n\n* Writing and visuals are clear"}, "weaknesses": {"value": "The paper presumes that location disclosure is inherently undesirable and that the visual context alone suffices to infer disclosure appropriateness. In practice, location sharing on social media is often strategically self-disclosing—users may intentionally reveal or ambiguously hint at places for social signaling, identity performance, or prestige. Without modeling user intent, audience, or platform norms, the proposed notion of “contextual integrity violation” collapses into a moralized prior rather than an empirically grounded construct.\n\nTherefore, while the question (“can VLMs respect contextual integrity?”) is interesting, the methodology has limited interpretive value and limited insights:  Models are optimized to output the most probable, semantically specific description given their training distribution. So, it would actually be unsurprising — and even expected — if a VLM provides accurate and detailed location descriptions.  Over-disclosure is thus an expected by-product of likelihood maximization, not evidence of moral failure. The current experimental setup measures this natural behavior rather than revealing a new deficiency."}, "questions": {"value": "How do you know the person truly didn’t want to share their location? (i.e. How do you label your data? and how can you be confident/certain that your judgments about “intent to disclose” are actually correct?) Human intent is complex — even people can’t reliably judge it (read peoples' minds), let alone models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jHv4BmUfTq", "forum": "64Ea2Dx0JJ", "replyto": "64Ea2Dx0JJ", "signatures": ["ICLR.cc/2026/Conference/Submission15426/Reviewer_eR7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15426/Reviewer_eR7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908750278, "cdate": 1761908750278, "tmdate": 1762925700658, "mdate": 1762925700658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}