{"id": "ASPC2Ut0CB", "number": 8506, "cdate": 1758088531058, "mdate": 1763132763949, "content": {"title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models", "abstract": "Physical spatiotemporal forecasting poses a dual challenge: The inherent stochasticity of physical systems makes it difficult to capture extreme or rare events, especially under \\textit{data scarcity}. Moreover, many critical domain-specific metrics are \\textit{non-differentiable}, precluding their direct optimization by conventional deep learning models.\nTo address these challenges, we introduce a new paradigm, \\textbf{\\textit{Spatiotemporal Forecasting as Planning}}, and propose \\textbf{\\method{}}, a framework grounded in Model-Based Reinforcement Learning. First, \\method{} constructs a novel Generative World Model to learn and simulate the physical dynamics system. This world model comprises a deterministic base network and a probabilistic Multi-scale Top-K Vector Quantized decoder. It not only provides a single-point prediction of the future but also generates a distribution of diverse, high-fidelity future states, enabling \"imagination-based\" simulation of the environment's evolution.\nBuilding on this foundation, the base forecasting model acts as an \\textbf{\\textit{Agent}}, whose output is treated as an action to guide exploration. We then introduce a \\textbf{\\textit{Planning Algorithm based on Beam Search}}. This algorithm performs forward exploration within the learned world model, leveraging the non-differentiable domain metrics as a \\textbf{\\textit{Reward Signal}} to identify high-return future sequences. Finally, these high-reward candidates, identified through planning, serve as high-quality pseudo-labels to continuously optimize the agent's \\textbf{\\textit{Policy}} through an iterative self-training process.\nThe \\method{} framework seamlessly integrates world model learning with reward-based planning, fundamentally addressing the challenge of optimizing non-differentiable objectives and mitigating data scarcity via exploration in its internal simulations. Comprehensive experiments on multiple benchmarks show that \\method{} not only significantly reduces prediction error (e.g., up to 39\\% MSE reduction) but also demonstrates exceptional performance on critical domain metrics, including physical consistency and the ability to capture extreme events.", "tldr": "", "keywords": ["World Model", "Spatio-temporal data mining"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b8aeae04a7b152c535ac77373bdbdd72c27c4c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose training a world model with RL. They conduct experiments on diverse forecasting benchmarks and systems."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using RL to train a probabilistic world model is interesting, and at times, the manuscript hints at a good motivation for why one would want to do this. It appears that the benchmarking domains are varied and the method is applicable to a broad range of architectures."}, "weaknesses": {"value": "The description of key parts of the system is severly lacking and not well-motivated. E.g.,\n- Equation 1 is the \"theoretical cornerstone\" of the proposed framework, yet (1) $D$ is not defined up until this point, (2) the supposed world model that is being trained $\\mathcal{M}_\\phi$ is not even in the equation, (3) the equation does not include any mechanism to ensure the world model actually approximates a target dynamical system (the latter of which is also not even part of the equation).\n- The \"codebook\" (equation 2) is never motivated.\n- Even though the paper initially states the research problem is training a generative model that maximizes non-differentiable rewards, the actual training process minimizes standard differentiable metric (reconstruction loss in equation 3).\n- The training objective contains terms that are never described (\"codebook loss\" and \"commitment loss\" in equation 3).\n- Notably, and contrary to the manuscript's own stated goal, the world model is not trained with RL; instead opts for an end-to-end training loop (line 244).\n- The paper initially claims to address rare events and data scarcity but these motivations does not seem to have impacted the framework design itself, or at least it is never stated how the framework addresses these challenges.\n- These statements are incompatible:\n\t- \"our agent learns from an external, high-dimensional, and non-differentiable evaluation function.\"\n\t- \"Our core idea is to employ the non-differentiable metric S(·) as a reward function, R,\"\n\t- They are incompatible because a metric is typicallly a scalar function (otherwise the paper does not specify what a metric is in this case).\n- The proposed RL approach is never connected to the standard world modelling objective of predicting the future.\n\nFurthermore the paper's presentation is one of its weakenesses, emphasizing form over substance. It makes heavy use of bold and italics, and makes very strong claims about the proposed method (e.g., \"paradigm shift\", \"creative\", \"paradigm that enables any model to learn directly from these true real-world objectives\"), and sometimes employs unorthodox language that obscures the true technical meaning (\"The climax of the loop is the reward evaluation and self-training mechanism.\").\n\nIn its present state, it would be very hard for me to recommend this paper for acceptance because I could not understand the methodology, even if the method appears general based on how many systems are features in the experiments. I would suggest to the authors the following:\n1. Add a section that summarizes and motivates each of the technical components of the system and revise the mathematical formulation of the core problem that you aim to solve.\n2. Rewrite the manuscript avoiding subjective observations and language, focusing instead on mathematical clarity and soundness."}, "questions": {"value": "If possible, please address the weaknesses described above, and please point out any errors I may have made in my assessment of your manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IJDhfVKL8s", "forum": "ASPC2Ut0CB", "replyto": "ASPC2Ut0CB", "signatures": ["ICLR.cc/2026/Conference/Submission8506/Reviewer_d6hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8506/Reviewer_d6hr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602729462, "cdate": 1761602729462, "tmdate": 1762920376068, "mdate": 1762920376068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose training a world model with RL. They conduct experiments on diverse forecasting benchmarks and systems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using RL to train a probabilistic world model is interesting, and at times, the manuscript hints at a good motivation for why one would want to do this. It appears that the benchmarking domains are varied and the method is applicable to a broad range of architectures."}, "weaknesses": {"value": "The description of key parts of the system is severly lacking and not well-motivated. E.g.,\n- Equation 1 is the \"theoretical cornerstone\" of the proposed framework, yet (1) $D$ is not defined up until this point, (2) the supposed world model that is being trained $\\mathcal{M}_\\phi$ is not even in the equation, (3) the equation does not include any mechanism to ensure the world model actually approximates a target dynamical system (the latter of which is also not even part of the equation).\n- Even though the paper initially states the research problem is training a generative model that maximizes non-differentiable rewards, the actual training process minimizes standard differentiable metric (reconstruction loss in equation 3).\n- Notably, and contrary to the manuscript's own stated goal, the world model is not trained with RL; instead opts for an end-to-end training loop (line 244).\n- The paper initially claims to address rare events and data scarcity but these motivations does not seem to have impacted the framework design itself, or at least it is never stated how the framework addresses these challenges.\n- These statements are incompatible:\n\t- \"our agent learns from an external, high-dimensional, and non-differentiable evaluation function.\"\n\t- \"Our core idea is to employ the non-differentiable metric S(·) as a reward function, R,\"\n\t- They are incompatible because a metric is typicallly a scalar function (otherwise the paper does not specify what a metric is in this case).\n- The proposed RL approach is never connected to the standard world modelling objective of predicting the future.\n\nFurthermore the paper's presentation is one of its weakenesses, emphasizing form over substance. It makes heavy use of bold and italics, and makes very strong claims about the proposed method (e.g., \"paradigm shift\", \"creative\", \"paradigm that enables any model to learn directly from these true real-world objectives\"), and sometimes employs unorthodox language that obscures the true technical meaning (\"The climax of the loop is the reward evaluation and self-training mechanism.\").\n\nIn its present state, it would be very hard for me to recommend this paper for acceptance because I could not understand the methodology, even if the method appears general based on how many systems are features in the experiments. I would suggest to the authors the following:\n1. Add a section that summarizes and motivates each of the technical components of the system and revise the mathematical formulation of the core problem that you aim to solve.\n2. Rewrite the manuscript avoiding subjective observations and language, focusing instead on mathematical clarity and soundness."}, "questions": {"value": "If possible, please address the weaknesses described above, and please point out any errors I may have made in my assessment of your manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IJDhfVKL8s", "forum": "ASPC2Ut0CB", "replyto": "ASPC2Ut0CB", "signatures": ["ICLR.cc/2026/Conference/Submission8506/Reviewer_d6hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8506/Reviewer_d6hr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602729462, "cdate": 1761602729462, "tmdate": 1763131968252, "mdate": 1763131968252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel framework, Spatiotemporal Forecasting as Planning (SFP), which reframes forecasting as a model-based planning approach. Instead of optimizing differentiable surrogate losses such as MSE, the authors propose to treat forecasting as a planning task over imagined futures generated by a learned generative world model. This model (a conditional VQ-VAE) produces multiple high-fidelity rollouts conditioned on latent “actions,” representing potential evolutions of the physical system. A beam-search planner evaluates these imagined trajectories using non-differentiable, domain-specific metrics acting as rewards.\nThe central idea is that this process directly aligns optimization with the true scientific evaluation criteria, rather than with proxy pixel-wise errors. Empirical results on weather, turbulence, and combustion datasets demonstrate significant improvements on critical non-differentiable metrics while maintaining or improving standard prediction accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Recasting forecasting as planning provides a coherent link between model-based approaches and scientific forecasting, addressing the issue of optimizing proxy losses like MSE.\n- The technical integration is quite elegant; the framework combines a generative world model, beam-search planning, and pseudo-label self-training into a unified, closed-loop learning process.\n- Results are consistent across diverse datasets and metrics, showing large improvements on physically meaningful metrics.\n- The figures are clear and effective (especially the architectural diagrams that illustrate the training loop), which clearly support the exposition of the approach.\n- The idea of alleviating the bottleneck linked to the optimization of non-differentiable evaluation metrics via RL is quite novel..."}, "weaknesses": {"value": "- ... but in fine, the final method presented here is ambiguous; the final policy loss consists in the L2 loss between \"the projection of the action chosen by the policy in the current state to the reward space\" (if I understand equation 5 correctly) and the optimal reward (_single reward, not accumulated_) obtained at the predicted next step. That's really not standard (in contrast to what is claimed in the caption of Figure 3), and I'm wondering how this boils down to optimizing the expected return in a Markov process, as it would be with standard RL approaches.\n- Beam search is used as the planning algorithm within the learned model... but why not simply use standard, efficient RL algorithms to efficiently explore the model and plan?\n- Beam search is sensitive to the horizon and prone to myopic expansions. There is limited analysis on the effect of beam width B, and no analysis on the horizon and stochastic branching. Fig. 7 (right) begins to address B, but the rationale for choosing beam search over alternatives (e.g., MCTS or merely RL) is not developed.\n- The authors describe the learned dynamics as $p\\_\\phi(\\mathbf{y}\\_{t+1}\\mid \\mathbf{a}\\_t)$ in Section 3 as a process that only depends on the current action. This formulation is not standard. In contrast, further in the paper, the VQ-VAE is presented as a process that learns the dynamics of a Markov chain (the transition function learned only depends on the current \"conditioned\" state), so no explicit action enters the model. Therefore, this is unclear what the role of the latent actions in the planning phase. This mismatch affects both the MBRL framing and the planning interpretation.\n- $S(.)$ is used on full imagined futures, but its mathematical definition is missing or postponed. The authors sometimes treat rewards as independent of the current state (equation 1) or action (line 280), which is non-standard. Again, this inconsistency spoils the theoretical exposition of the framework.\n- The authors link stability improvements to the proposed framework and later compare it to Direct Preference Optimization (DPO). However, DPO is motivated in text generation with human preference pairs; here it is not clear how preference pairs are constructed and what model class for the policy is optimized (DPO's policies are usually LLMs). The comparison misses a clear alignment of assumptions.\n- The current exposition of the training settings clearly lacks discussion on the chosen hyperparameter. For instance, not all hyperparams are detailed or even mentioned, and their choice is not really motivated (e.g., the codebook implies a latent discrete latent space of size 1024; is this enough?)."}, "questions": {"value": "1. Can you provide a clear explanation of the transition function of the learned model, with full conditionals? Is the world model $p\\_\\phi(y\\_{t+1}\\mid s_t,a_t)$, $p\\_\\phi(y\\_{t+1}\\mid s_t)$, or $p\\_\\phi(y\\_{t+1}\\mid a_t)$?\n2. What exactly is an action here, as the model learned via VQ-VAE is a Markov chain?\n3. What exactly is the function $S(.)$ which defines the rewards? Over which horizon, how aggregated, and how stochasticity is handled. If $S$ ignores $s_t$ or $a_t$, explain why this is acceptable.\n4. Why beam search over MCTS or RL? What are the rollout lengths (horizon) used with the beam search? Is that horizon allowed to change between trajectories? If yes, how do you handle that?\n5. What is the projection $\\mathcal{P}$ concretely (equation 5)? Is it learned or fixed? How does mapping from latent action to state ensure the update remains physically meaningful? And how does this projection relate to the optimal reward of equation 4?\n6. How are preference pairs constructed? What is the policy class used by DPO in your setting? If DPO collapses, is that due to a mismatch between preference construction and the stochastic generator, or due to optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AbEsVw7iyc", "forum": "ASPC2Ut0CB", "replyto": "ASPC2Ut0CB", "signatures": ["ICLR.cc/2026/Conference/Submission8506/Reviewer_T6Qh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8506/Reviewer_T6Qh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655530393, "cdate": 1761655530393, "tmdate": 1762920375633, "mdate": 1762920375633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Spatiotemporal Forecasting as Planning (SFP), a framework that redefines forecasting as a planning problem using a learned VQ-VAE world model. Instead of relying on differentiable losses like MSE, SFP employs beam search in the latent space and evaluates imagined futures with non-differentiable domain metrics such as CSI and TKE. The top-performing rollouts are then used as pseudo-labels to iteratively train a forecasting policy, aligning model learning with physically meaningful evaluation criteria. Experiments across several datasets and architectures show consistent gains, highlighting the framework’s potential."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths:**\n\n**1. Good Writing:** The paper is clear, concise, and well-organized. Its explanations of tasks and evaluation approach are logically structured, facilitating straightforward comprehension of the methodology and results.\n\n**2. Well-motivated problem framing:** The paper addresses a significant problem in spatiotemporal forecasting i.e. the mismatch between differentiable training objectives (e.g., MSE) and the non-differentiable metrics (e.g., CSI, TKE) actually used for scientific evaluation. By explicitly incorporating these metrics into the optimization process through planning and self-training, the work tackles a practically important challenge.\n\n**3. Comprehensive Evaluation:** The experimental design is broad and well-executed, spanning multiple datasets and diverse architectures. The authors conduct detailed ablations, beam-width sensitivity studies, and cross-dataset generalization tests, providing a convincing empirical case for the robustness of their framework. The inclusion of both visual (spatial) and spectral analyses strengthens the credibility of reported improvements."}, "weaknesses": {"value": "**Weaknesses:**\n\n**1. Novelty:** While the paper presents a seemingly novel framing of forecasting as planning, the actual methodological novelty is limited, as most of its core components have appeared in prior literature. The authors should explicitly clarify how their approach differs from BeamVQ [1], which also integrates a VQ-VAE–based latent world model with beam search to improve physical spatiotemporal forecasting under data scarcity. At present, the overlap in core design elements like latent quantization, discrete search, and top-K rollout selection is substantial, making it unclear what specific algorithmic or conceptual advance SFP introduces beyond re-contextualizing these ideas under a “planning” interpretation. The paper should delineate whether its novelty lies in the training dynamics, policy and world model, or the reward formulation based on non-differentiable scientific metrics.\n\n**2. Metric Bias:** I am a bit concerned as the method would induce a metric bias in learning (*“When a measure becomes the target, it ceases to be a good measure”*). In SFP, the reward is entirely defined by domain metrics such as CSI or TKE, which while scientifically meaningful, are still proxies for the underlying physical fidelity of spatiotemporal dynamics. By directly optimizing these metrics through beam search and self-training, the system is inherently susceptible to metric bias or metric hacking, wherein the model learns strategies that inflate the chosen score without genuinely improving predictive or physical realism. For example, a TKE-based reward can unintentionally encourage the model to add random high-frequency noise that mimics turbulence energy without capturing real fluid dynamics, leading to visually chaotic but physically meaningless predictions that still score highly."}, "questions": {"value": "1. Since pseudo-labels are derived from model-generated rollouts, how do you prevent confirmation bias i.e., the model reinforcing its own mistakes through iterative imitation?\n\n2. Beam search would favor narrow, high-probability trajectories. Has the authors experienced any diversity collapse in the latent space or over-confident selection of single-mode rollouts? If yes, what would they propose to avoid that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LkA94yzOdd", "forum": "ASPC2Ut0CB", "replyto": "ASPC2Ut0CB", "signatures": ["ICLR.cc/2026/Conference/Submission8506/Reviewer_q6AA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8506/Reviewer_q6AA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920700845, "cdate": 1761920700845, "tmdate": 1762920374982, "mdate": 1762920374982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the task of spatiotemporal  forecasting, which is usually learnt by optimizing for surrogate loss objectives is posed as a planning problem where in the reward signal comes directly from the downstream metrics of importance. These metrics, being non-differentiable are modeled as the reward for a model-based RL style optimization wherein the  agent plans in imagination (i.e. using a generative world model). More concretely, the approach - Spatio Temporal Forecasting (SFP) learns a generative world model to simulate the dynamics of the physical system and then the current policy / agent leverages the this learnt world model to simulate rollouts. Rollouts leading to higher downstream metrics are rewarded accordingly and as training data to  train the agent's policy. SFP shows improvement consistent improvement over the baseline of  optimizing the MSE, across different model architectures, across 3 tasks, showing an average improvement of 39% (in terms of MSE reduction). The authors also provide interesting analysis on the approach"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The application of world modeling and planning in imagination in the context of spatiotemporal forecasting is novel according to the best of my knowledge, and very interesting.\n* The experimental results are significant, showing a consistent improvement over the baselines across multiple tasks. \n* The experiments is well structured. I really liked how it has been been written around the three RQs discussed at the start. The analysis included is also very insightful\n* The approach is a simple plug and play and seems to work with already widely studied backbones for SFT.\nThe approach is more sample efficient than the MSE optimization baseline."}, "weaknesses": {"value": "* Several key experimental details seem to be missing - for eg. The choice of the non-differentiable reward functions S(.) (see Questions for more)\n* I believe a brief discussion about what the each of the CSI, TKE error and SSIM metrics mean / represent could improve the paper. \n* The inclusion of strong generative baselines such as diffusion based modeling might be relevant here, but seems to be missing from the discussion"}, "questions": {"value": "* What is the value of K for the Top-K VQ-decoder.\n* What is the total reward signal used out of the three metrics ? Is it the CSI, TKE error or SSIM? Or some sort of ensemble of all three? Is it an ensemble of all metrics?\n* In Figure 7 - how does \"w/o self-training\" work? How does the policy learn without any training?\n* Can the authors elaborate on the setup for the DPO experiments? How are the positive and negative example pairs obtained?\nWhat is ORI in Table 3 appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xjml7Sv2dC", "forum": "ASPC2Ut0CB", "replyto": "ASPC2Ut0CB", "signatures": ["ICLR.cc/2026/Conference/Submission8506/Reviewer_d4cr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8506/Reviewer_d4cr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762342899474, "cdate": 1762342899474, "tmdate": 1762920374599, "mdate": 1762920374599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}