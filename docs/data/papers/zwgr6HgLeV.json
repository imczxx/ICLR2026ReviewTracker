{"id": "zwgr6HgLeV", "number": 9970, "cdate": 1758153415417, "mdate": 1759897682650, "content": {"title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Representation Learning from Video", "abstract": "Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation to learn robust visual representations, similar to human visual processing. Our learning objective is derived from variational inference. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation  including RSP and CropMAE, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a step toward more biologically plausible computer vision systems that process visual information in a manner more aligned with human cognitive processes.", "tldr": "", "keywords": ["Representation learning", "Brain-inspired", "hippocampus", "video"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d055b19ab1669ac863c0473a9e0099edc52cb8eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PhiNet v2, a brain-inspired Transformer-based model for self-supervised video representation learning. Unlike existing methods (e.g., RSP, CropMAE), it learns from sequential video inputs without heavy data augmentation or masking, drawing on neuroscientific insights (temporal prediction hypothesis, Complementary Learning Systems) and variational inference. Experiments on DAVIS, VIP, and JHMDB benchmarks show competitive or superior performance compared to SOTA baselines, with enhanced robustness to noise and simpler architecture (no auxiliary MAE module)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Eliminates reliance on strong augmentation/masking, improving generalizability and practicality.\n2. Strong empirical results in this manuscript outperform baselines on key tasks and demonstrate robustness to noise and batch size variations."}, "weaknesses": {"value": "The **MOST IMPORTANT** weaknesses: \n1. Biological plausibility is limited. Actually, the model presented in this paper is engineering-driven, with unclear alignment to actual neural mechanisms.\n2. Patch-based ViT processing may not faithfully mimic human visual systems, lacking biological realism.\n\nThe Minor weakness:\n1. Ablation studies could better clarify the relative importance of individual components (e.g., symmetric loss vs. EMA).\n2. Typos:\n    - PhiNetv2\" (Table 3) lacks a space; should be \"PhiNet v2;\n    - \"reprentation learning\" (Section D) should be \"representation learning\""}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njMzgprO44", "forum": "zwgr6HgLeV", "replyto": "zwgr6HgLeV", "signatures": ["ICLR.cc/2026/Conference/Submission9970/Reviewer_JDrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9970/Reviewer_JDrW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810367388, "cdate": 1761810367388, "tmdate": 1762921409532, "mdate": 1762921409532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the ICLR 2025 PhiNet-v1 model, a biologically inspired self-supervised temporal prediction framework grounded in complementary learning systems (CLS). PhiNet-v2 introduces two key innovations: (1) training directly from continuous video streams rather than masked frame augmentation, and (2) incorporating stochastic latent conditioning and divergence-based alignment inspired by variational inference and hippocampal uncertainty-based prediction. The model aims to more closely reflect biological principles of predictive coding and fast/slow memory systems while remaining competitive with state-of-the-art recent transformer-based video SSL models (VideoMAE, CropMAE, RSP, DynST) with ~+1–3% gains on common benchmarks. The model demonstrates improved stability in online and continual streaming settings, an appealing property in light of CLS-style consolidation and hippocampal rapid plasticity. The model has the potential of providing a future “predictive-coding-style” foundation model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tIncorporation of stochastic latent variables and divergence alignment provides robustness and more principled predictive uncertainty modeling compared to purely deterministic latent forecasting.\n2.\tDemonstrates that biologically motivated design principles (continuous temporal prediction, fast vs. slow learning pathways) can be competitive with modern masked-video transformer pipelines.\n3.\tClear incremental improvements over PhiNet-v1, with empirical gains attributable to the stochastic latent pathway and the unmasked temporal learning regime.\n4.\tExtensive comparisons to current SOTA video-SSL frameworks reinforce the value of predictive learning without reliance on heavy augmentations or masking.\n5.\tCareful lesion experiment to identify the relative contribution of video input, temporal prediction, and variational inference."}, "weaknesses": {"value": "1.\tConceptually, temporal predictive coding is not new; related ideas appear in Lotter et al. (PredNet), predictive coding models from Rao & Ballard, and hippocampal predictive map literature. A more explicit comparison and positioning would help.\n2.\tThe anatomical labels (EC, CA3, CA1) are likely metaphorical for different system components rather than neuroscientifically validated. The biological mapping is interesting but would benefit from clearer qualification and evidence discussion.\n3.\tWhile technically well-executed, the core contributions are incremental rather than fundamentally transformative (strong engineering iteration more than a conceptual leap).\n4.\tClaims around “biological plausibility” should be treated cautiously; real hippocampal circuits perform episodic indexing and relational reasoning, not only next-latent forecasting."}, "questions": {"value": "1.\tIs the CLS analogy literal, or mainly a framing for dual-timescale optimization (EMA long-term memory vs. rapid plasticity in the online encoder)? What empirical signatures support the neuroscientific mapping?\n2.\tThe “fast adaptation per movie” mechanism is intriguing—does this correspond to episodic memory formation? What is the magnitude of the adaptation effect, and is the performance improvement significant if the online updates are disabled?\n3.\tIs there explicit generative replay or only EMA alignment as a surrogate for cortical consolidation? How the encoder and predictor are updated, and what their weights are updated to encode, are not entirely clear -- is it a way to remember the movie frames that have just been seen to predict the next frame's latents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6yIvsH9Rtv", "forum": "zwgr6HgLeV", "replyto": "zwgr6HgLeV", "signatures": ["ICLR.cc/2026/Conference/Submission9970/Reviewer_MxHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9970/Reviewer_MxHU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941026796, "cdate": 1761941026796, "tmdate": 1762921409182, "mdate": 1762921409182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PhiNet v2, offering self-supervised video representation learning method using hippocampal inspired predictive coding models. The author tries to fit the proposed SSL method into the brain theory hippocampus guided representation learning. Despite the conceptual novelty, the paper’s experiments and analysis doesn’t solidly support the proposed thesis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Make analogy to DG-CA3-CA1 system is inspiring direction to integrate with self-supervised representation learning. \n- The paper demonstrates a system that would lead to better video representation learning without heavy masking or augmentation, only via the inspiration of predictive coding and the brain-like architecture.\n- The paper demonstrates significant performance gain on the continued learning tasks in small scale datasets."}, "weaknesses": {"value": "- Does the brain inspired approach lead to better representation alignment with the brain? E.g., measuring whether there is any improvement to the alignment to brain representation using BrainScore. \n- In the brain system, DG serves as a sparse index that would guide better pattern-completion / retrieval from CA3 memory. However, in the architecture design, CA3 is merely implemented as two-layer feedforward neural network rather than a recurrent Hopfield Net. Then how would we know it’s functionally corresponding to CA3 as a memory for retrieving future patterns? \n- Lack of analysis of how sparse the component that represents DG is and whether sparsity would actually make the model works better / worse. \n- Limited novelty beyond existing SSL works. The most important novelty of the paper is to demonstrate that SSL can be achieved via video. However, the paper didn’t directly compare with more recent video representation learning work like V-JEPA (even on the same scale)."}, "questions": {"value": "- How is the sparsity in the output of DG being controlled? Why there is not an architecture that make the DG layer overcomplete and sparse. \n- Will the EC representation (the main backbone) be affected if the output is forced to have sparsity using loss without overcomplete?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBNrYD3IeR", "forum": "zwgr6HgLeV", "replyto": "zwgr6HgLeV", "signatures": ["ICLR.cc/2026/Conference/Submission9970/Reviewer_uw2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9970/Reviewer_uw2y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989884273, "cdate": 1761989884273, "tmdate": 1762921408829, "mdate": 1762921408829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PhiNet v2, a brain-inspired, Transformer-based self-supervised learning model that learns robust visual representations from video sequences without relying on strong data augmentation or masking, aligning more closely with human visual processing. By formulating a variational inference–based objective grounded in hippocampal circuitry and Complementary Learning Systems theory, PhiNet v2 achieves competitive or superior performance to state-of-the-art methods like RSP and CropMAE on video understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) It learns robust representations without relying on strong data augmentations or masking strategies.\n(2) By formulating a variational inference–based objective grounded in hippocampal circuitry and Complementary Learning Systems theory, PhiNet v2 bridges neuroscience principles with probabilistic machine learning.\n(3) PhiNet v2 shows great improvement over PhiNet v1 on the DAVIS dataset, as illustrated in Tab. 4."}, "weaknesses": {"value": "(1) The absolute performance of PhiNet v2 is too weak. It can not compete with the well-established representation learning methods, such as DINOv2, DINOv3, and EVA-02.\n(2) The idea of introducing transformer into the PhiNet family is not surprising. This is a natural and necessary step—other representation learning methods have long been using Transformers.\n(3) PhiNet v1 has experimental results on ImageNet and Cifar. PhiNet v2 should also give results on these classical benchmark and compare with PhiNet v1."}, "questions": {"value": "I believe exploring brain-inspired learning methods is highly meaningful. However, the current results of these approaches are still quite poor. What is the biggest challenge the authors are currently facing—GPU resources, perhaps? And what is the long-term vision for the PhiNet series from the authors' team?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mxtpS8s5cc", "forum": "zwgr6HgLeV", "replyto": "zwgr6HgLeV", "signatures": ["ICLR.cc/2026/Conference/Submission9970/Reviewer_kZog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9970/Reviewer_kZog"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154427065, "cdate": 1762154427065, "tmdate": 1762921408477, "mdate": 1762921408477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}