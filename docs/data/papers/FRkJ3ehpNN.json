{"id": "FRkJ3ehpNN", "number": 7458, "cdate": 1758023028553, "mdate": 1759897851887, "content": {"title": "Robust Test-time Video-Text Retrieval: Benchmarking and Adapting for Query Shifts", "abstract": "Modern video-text retrieval (VTR) models excel on in-distribution benchmarks but are highly vulnerable to real-world *query shifts*, where the distribution of query data deviates from the training domain, leading to a sharp performance drop. Existing image-focused robustness solutions are inadequate to handle this vulnerability in video, as they fail to address the complex spatio-temporal dynamics inherent in these shifts. To systematically evaluate this vulnerability, we first introduce a comprehensive benchmark featuring 12 distinct types of video perturbations across five severity degrees. Analysis on this benchmark reveals that query shifts amplify the *hubness phenomenon*, where a few gallery items become dominant \"hubs\" that attract a disproportionate number of queries. To mitigate this, we then propose HAT-VTR (Hubness Alleviation for Test-time Video-Text Retrieval), as our baseline test-time adaptation framework designed to directly counteract hubness in VTR. It leverages two key components: a *Hubness Suppression Memory* to refine similarity scores, and *multi-granular losses* to enforce temporal feature consistency. Extensive experiments demonstrate that HAT-VTR substantially improves robustness, consistently outperforming prior methods across diverse query shift scenarios, and enhancing model reliability for real-world applications.", "tldr": "Query shifts cause video-text retrieval models to fail by creating 'hubs'—overly dominant videos. We introduce a benchmark to analyze this vulnerability and propose HAT-VTR, a test-time method that suppresses hubs for robust retrieval.", "keywords": ["Video-Text Retrieval; Test-time Adaptation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77956337dbcfc821522266f9cafff574de880662.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors point out that existing video-text retrieval models exhibit performance degradation when the distribution of query data behaves differently from the training domain. While some solutions have been introduced, they are mainly focused on the image applications, and not applicable for videos due to the complex spatio-temporal dynamics. To systematically evaluate the vulnerability, the authors have introduced their own benchmark revealing the vulnerability arises from a phenomenon called hubness phenomenon. This phenomenon appears when few gallery items become dominant behaving as a hub. To tackle this, the authors introduce a test-time adaptation framework which consists of two key components. First, a hubness suppression memory refines the similarity scores. Second, the multi-granular loss is designed to keep the temporal feature consistent. HAT-VTR is evaluated on four main datasets, which exhibit strong performances."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Problem is well driven with clear motivations. The authors have used their own benchmark to uncover where the vulnerability comes from. The analysis becomes a clear evidence of hubness phenomenon. \n- The proposed model has been directly derived from the analysis with the benchmark. The framework is straightforward and is based upon two main purposes. \n- Extensive experimental results supports the efficacy of HAT-VTR."}, "weaknesses": {"value": "- Some claims are controversial. For instance, the candidate component barely contributes to final performance (see Table 6). Adding candidate (row 2 and row 4) affects minimal compared to Rerank. This is unexpected as ‘candidate selection’ associates extensive computations.   \n- Too many temperature parameters: $\\tau$ $\\alpha$, $\\beta$, t in Eq 7 and 8 requires extensive tuning/searching. Still, there are more hyperparameters including those specified as hyperparameters and those not specified (e.g., fraction r) . It is unclear how the proposed model is sensitive to these combinations unless reported. Some of the methods are based on heuristics. \n- HAT-VTR requires some computational cost, which only has been reported in Appendix."}, "questions": {"value": "Q1. What is the rationale behind using the first B rows in line 256? \n\nQ2. Any details on posterior reranking? Isn’t the reranking based on some heuristic? How do you *demote* hubs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tpc5FIOMRi", "forum": "FRkJ3ehpNN", "replyto": "FRkJ3ehpNN", "signatures": ["ICLR.cc/2026/Conference/Submission7458/Reviewer_tpJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7458/Reviewer_tpJo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841608359, "cdate": 1761841608359, "tmdate": 1762919572158, "mdate": 1762919572158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies robust video–text retrieval (VTR) under query shift and contributes (i) MLVP, a benchmark with 12 video perturbation types × 5 severities spanning low/mid/high-level spatio-temporal corruptions, and (ii) HAT-VTR, a test-time adaptation (TTA) baseline that counters hubness via a Hubness Suppression Memory (HSM) and multi-granular losses for temporal consistency (with a Reliable Memory for stability). Experiments report consistent gains across query-shift settings, including scenarios where both queries and gallery drift."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a clear diagnosis linking robustness failures under perturbations to amplified hubness (illustrated via k-occurrence distributions), introduces a well-scoped MLVP benchmark spanning 12 perturbation types × 5 severities that moves beyond image-only corruptions to video dynamics, proposes a simple plug-in TTA—HAT-VTR with Hubness Suppression Memory and multi-granular (global/frame) losses—that integrates cleanly with dual-encoder VTR, and demonstrates broad, consistent improvements over prior TTA methods under both query-shift and query+gallery-shift scenarios."}, "weaknesses": {"value": "1.\tEnd-to-end cost of the interactive TTA loop (HSM updates + multi-granular adaptation) is not characterized; please report per-round latency, peak memory, and wall-clock on a commodity GPU and CPU, and discuss amortized cost over sequence length.\n2.\tComparisons to classical retrieval debiasing / hubness-reduction baselines are missing; adding such baselines would better isolate HSM’s contribution.\n3.\tThe effect of online adaptation on in-distribution retrieval is unclear; include a no-shift control row to quantify any degradation and implement a switch to disable TTA when drift is not detected.\n4.\tHyperparameter sensitivity is under-analyzed—queue size K, mixture weights/temperatures, and stability–plasticity controls likely govern convergence and variance; provide ablations, convergence diagnostics, and seed variance."}, "questions": {"value": "1. What is the per-batch adaptation time and memory for HAT-VTR vs. TCR on a single 3090/A10?\n2. Does adaptation degrade R@K on clean test data? Any drift detector to gate adaptation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "norUvIfpiS", "forum": "FRkJ3ehpNN", "replyto": "FRkJ3ehpNN", "signatures": ["ICLR.cc/2026/Conference/Submission7458/Reviewer_drGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7458/Reviewer_drGP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937938828, "cdate": 1761937938828, "tmdate": 1762919571803, "mdate": 1762919571803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the domain shift of the text-video retrieval by investigating the spatial-temporal axises. Accordingly, this work proposes a   new benchmark that performs video perturbation from low/mid/high-levels. Also a new method is proposed to enhance the test-time adaptation. Both the benchmark and the method admit the necessity of improving the generalization ability of text-video retrieval methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Studying the generalization and robustness of the text-video retrieval is meaningful and urgent. The proposed method introduces a valuable benchmark. I'd like to support this work if the benchmark could be properly released to the academic communities. \n\n* The proposed method studies different levels of the perturbation when devising the dataset, which is inspiring. \n\n* The proposed benchmark and method are well-motivated."}, "weaknesses": {"value": "* There might be some logical issues at presentation. This work starts from the query shift but operates on the video side, which is confusing. Are there any rationales that might be missing to bridge the two? \n\n* It seems that there is no discussion on the query generalization method applied when devising the benchmark. \n\n* It seems that the proposed benchmark is applied on very limited methods and datasets, which might lack generalizability. \n\n* There lacks the discussion of the generaliability for existing foundation models on the proposed perturbations."}, "questions": {"value": "* What might be the efficiency cost of the proposed method, such as the memory usage and the latency cost. \n\n* Is that possible to provide some empirical evidence to show that the improvement on the hubness phenomenon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1Wx0TI4H7", "forum": "FRkJ3ehpNN", "replyto": "FRkJ3ehpNN", "signatures": ["ICLR.cc/2026/Conference/Submission7458/Reviewer_qdkY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7458/Reviewer_qdkY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983637967, "cdate": 1761983637967, "tmdate": 1762919570635, "mdate": 1762919570635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Modern Video-Text Retrieval (VTR) models demonstrate strong performance on standard benchmarks but are highly vulnerable to real-world distribution shifts in query data, which cause a significant performance drop and amplify the \"hubness\" phenomenon where a few gallery items become dominant hubs for an disproportionate number of queries. To systematically evaluate this robustness issue, this paper first introduces a comprehensive benchmark with 12 types of video perturbations across five severity levels. In response, the authors propose HAT-VTR, a test-time adaptation framework designed to directly mitigate hubness. This framework leverages a Hubness Suppression Memory to refine similarity scores and a Multi-Granularity Loss to enhance temporal feature consistency. Extensive experiments show that HAT-VTR significantly improves robustness, consistently outperforming prior methods across diverse query shift scenarios and thereby increasing model reliability for practical applications. Overall, the paper presents an interesting and valuable contribution, and the reviewer expresses a willingness to raise their score pending satisfactory responses to their questions and concerns"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.   The authors have clearly identified and articulated a critical issue in video-text retrieval: the problem of overcoming the query gap between training data and real-world application scenarios. They provide a fairly detailed introduction to related work and effectively contrast their approach with TCR techniques from image-text retrieval, highlighting the distinctions of applying such technology in the video domain.\n\n 2.    The authors' idea of designing a Hubness Suppression Memory module to identify and mitigate the influence of hub points in the embedding space is both intuitive and sound.\n\n3.    The authors present their work through a well-structured and clearly organized writing flow, which enhances the readability and coherence of the paper."}, "weaknesses": {"value": "1.     While addressing the query gap between training data and real-world scenarios is indeed a critical issue in video-text retrieval, the authors have overlooked relevant works in their related work section. For instance, approaches like FreestyleRet, which attempt to construct multi-style queries in a data-driven manner to address this task, represent an alternative direction beyond Test-Time Adaptation and should be discussed for a more comprehensive literature review.\n\n 2.   Although the authors have constructed a robustness benchmark with three levels of perturbations for the video modality, it would be interesting to explore whether similar robustness constructions can be applied from the perspective of the text modality. Specifically, I am curious if the authors have considered or could incorporate various types of noise and perturbations into the textual queries to further enhance model resilience.\n\n3.    While the authors clearly explain the weak transferability of TCR (Test-Time Contamination Remediation) techniques to video domains, it is notable that their triplet-based loss functions are inherited directly from the TCR framework. A more in-depth analysis would be beneficial to clarify which specific improvements or modifications in the TCR components contribute most significantly to the performance gains observed in the video-text retrieval task."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dpBQx5Znk2", "forum": "FRkJ3ehpNN", "replyto": "FRkJ3ehpNN", "signatures": ["ICLR.cc/2026/Conference/Submission7458/Reviewer_2w2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7458/Reviewer_2w2B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985622116, "cdate": 1761985622116, "tmdate": 1762919569620, "mdate": 1762919569620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}