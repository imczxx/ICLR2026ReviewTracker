{"id": "wb05ver1k8", "number": 7150, "cdate": 1758009701188, "mdate": 1763488750360, "content": {"title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "abstract": "While Large Language Models (LLMs) show immense promise as planners for embodied AI, their stochastic nature and lack of formal reasoning capabilities prevent the strict safety guarantees required for physical deployment. Current approaches fall short: they either rely on other unreliable LLMs for safety checks or simply reject unsafe plans without offering a path to success. This work bridges this critical gap by introducing the **Verifiable Iterative Refinement Framework (VIRF)**, a neuro-symbolic architecture that shifts the paradigm from a passive safety gatekeeper to an active safety collaborator. **Where prior verifiers simply reject failures, our framework provides causal, pedagogical feedback that teaches the LLM *why* its plan was unsafe, enabling intelligent repairs rather than mere avoidance.** Our core contribution is a novel **tutor-apprentice dialogue**, where a deterministic **Logic Tutor**, grounded in a formal safety ontology, provides causal and explanatory feedback to an LLM **Apprentice** planner. This pedagogical interaction allows the apprentice to perform intelligent, creative plan repairs, resolving safety conflicts rather than merely avoiding them. **To ground this dialogue in verifiable truth**, we introduce a scalable knowledge acquisition pipeline that synthesizes a comprehensive safety knowledge base from real-world documents, a process that simultaneously reveals and corrects significant blind spots in existing benchmarks. On a new suite of challenging home safety tasks, VIRF achieves a **perfect 0% Hazardous Action Rate (HAR)**, completely eliminating unsafe actions while attaining a **77.3% Goal-Condition Rate (GCR)**—the highest among all baselines. It does so with remarkable efficiency, requiring only **1.1 correction iterations** on average. By acting as a verifiable **safety scaffold**, VIRF demonstrates a principled and robust pathway toward building embodied agents that are not just capable, but fundamentally trustworthy.", "tldr": "We propose a hybrid neuro-symbolic architecture where a formal logic verifier tutors an LLM planner, enabling the generation of verifiably safe plans for embodied agents.", "keywords": ["neurosymbolic AI", "hybrid AI", "formal reasoning", "large language models", "AI safety", "verifiable AI", "embodied AI", "robotics"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4032230e9c6faa5f7dd6690f77ef545c550bff96.pdf", "supplementary_material": "/attachment/f4559d3e70c872c9d16302e1074798cc50416d1f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VIRF (Verifiable Iterative Refinement Framework), a neuro-symbolic architecture for safe embodied AI planning. The system combines an LLM planner with an OWL 2 Description logic based formal verifier that provides structured feedback when plans are unsafe. The work makes three claimed contributions: (1) a semi-automated RAG-based workflow for building safety ontologies from documents, (2) VLM-Cascade, a perception pipeline for generating rich semantic scene graphs, and (3) a tutor-apprentice verification loop where the \"Logic Tutor\" provides pedagogical feedback to guide plan refinement. Experiments on SafeAgentBench show 0% Hazardous Action Rate (HAR) apart from other improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive system: The paper presents a complete end-to-end system with careful engineering across perception, reasoning, and planning components.\n\n- Thorough evaluation strategy: The decoupled evaluation with golden ABox to isolate perception from reasoning is methodologically sound. The multiple ablations (VIRF-RAG, VIRF-Manual, VIRF-Reject) provide insights into component contributions.\n\n- Honest about limitations: Section 6 acknowledges brittleness of symbol grounding, static knowledge base, and sim-to-real gap. The discussion of the 7B model failure establishes a \"lower bound of competence.\"\n\n- Practical knowledge engineering: The semi-automated RAG workflow with human-in-the-loop validation addresses a real bottleneck. The claim of building 97 axioms in two working days demonstrates efficiency.\n\n- Neuro-symbolic integration: Combines formal logic verification, RAG-based knowledge synthesis, and multi-modal perception effectively.\n\n- Strong conceptual narrative: The “tutor-apprentice” framing is intuitive and appealing."}, "weaknesses": {"value": "1. Unfair experimental setup: Baseline methods are inadequately defined, making it impossible to verify fair comparison. The paper doesn't state whether baselines have access to the safety knowledge base, if not, the 0% HAR achievement is not so useful in my opinion.\n\n2. Novelty overclaimed: The distinction from VeriPlan[1]  is overstated. Both systems provide rule-based feedback derived from formal verification; VIRF's \"pedagogical\" framing is primarily a presentation difference.\n\n3. Missing critical comparisons with recent work: The paper claims its RAG-based workflow for extracting rules from natural language documents is a key contribution. However, recent work has explored similar approaches. For instance, the trajectory prediction literature has demonstrated LLM-powered RAG frameworks for automated rule extraction from natural language[2,3] (e.g., extracting safety rules). This paper doesn't position itself within this emerging paradigm of LLM-based rule extraction or explain what methodological advances it offers beyond these parallel efforts.\n\n4. Incomplete related work on neural-symbolic integration: The paper positions itself as bridging neural planning with formal reasoning, but doesn't adequately survey the landscape of recent approaches combining LLMs with structured knowledge representations. Several concurrent works explore different formalization strategies (temporal logic, probabilistic priors, ontologies) for similar safety-critical applications. A more thorough comparison would strengthen the contribution.\n\n5. Incomplete technical descriptions: How does VLM-Refine actually work? What makes it \"refinement\" vs. just another VLM call? The two-stage vs. three-stage inconsistency needs resolution.\n\n6. Metrics poorly defined: What is FPR/FNR measuring exactly? The \"positive class\" is unclear. HAR validation against independent annotations is missing. Also no experiment isolating the effect of the pedagogical causal explanation vs. simple rejection.(But these are minor issues but will make paper strong)\n\n7. No validation of Pillar 1 design: Extensive justification of BFO and compositional modeling in appendices, but zero ablations showing these choices matter empirically.\n\n8. Argument for SGG: Claims about \"closed-set brittleness\" and \"semantic anemia\" ignore recent work explicitly addressing these problems in open-vocabulary scene understanding.. The paper does not empirically compare VLM-Cascade to any role-playing LLM SGG [4] system. Without that, it’s unclear if VIRF’s perception improvements (Table 2 / Table 9) come from true architectural innovation or just careful prompt engineering.\n 9. The \"Foundational Design Choices and Knowledge Core Architecture\" (based on which pillar 1 is based) section is conceptually strong but lacks empirical validation. Claims such as “ensures correctness” and “enables blind-spot discovery” are overstated and not experimentally demonstrated. The ontology’s impact on generalization, causal reasoning, and safety coverage should be supported with quantitative audits or ablations.\n\n[1]Lee et al: VeriPlan: Integrating Formal Verification and LLMs into End-User Planning\n\n[2]Cai et al: Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM\n\n[3]Manas et al: Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification\n\n[4] chen et al: Scene Graph Generation with Role-Playing Large Language Models"}, "questions": {"value": "1. Baseline implementation: Can you provide complete definitions and citations for Impulsive, Thinker, and Committee baselines in main text? Specifically, what does \"Committee (SAFER-like)\" mean, is this your implementation or the actual SAFER system?\n\n2. Information access fairness: Do the baseline methods have access to your safety knowledge base? If not, how can you claim superiority when VIRF has privileged information? If yes, why do they fail when VIRF succeeds?\n\n3. VLM-Cascade stages: Is your method two-stage or three-stage? Resolve the inconsistency between main paper and Appendix C. Provide concrete details on how \"refinement\" works. Refinement is major novelty as claimed but details about that is misisng even in appendix .\n\n4. Pillar 1 validation: What ablations validate your ontological design choices (BFO vs. alternatives, layered vs. flat, compositional modeling)? These are claimed contributions but never empirically tested.\n\n5. Positioning within LLM-based rule extraction paradigm: Your RAG-based workflow for extracting safety rules from natural language documents shares conceptual similarities with recent work using LLMs to automatically extract rules (e.g., traffic rules (kinda KB), system constarints or documentation of robot model) for regularizing systems. How does your approach differ methodologically? What specific advances does your framework offer beyond automated rule extraction and integration? The literature shows successful deployment of LLM-RAG pipelines for generating training labels from natural language rule descriptions [2,3]  how does your ontology construction advance beyond this? Even if domain of driving or formal logic robotics are different but if they tackle same limitation needs to be discussed.\n\n6. HAR validation: How was ground truth for \"hazardous\" determined? Inter-annotator agreement? Can you validate against independent safety annotations rather than your own ontology?\n\n7. SGG comparison: Why no comparison against open-vocabulary SGG methods that address the exact limitations paper criticizes?\n\n8. Metric definitions: Please clearly define FPR/FNR—what is the positive class? Is VL per-action or per-plan? What constitutes \"goal achievement\" for GCR?\n\n9. RAG necessity beyond document retrieval: The VIRF-RAG ablation shows that RAG-only knowledge achieves 11% HAR. However, this doesn't address whether the RAG pipeline itself adds value over directly prompting the LLM with safety queries. Could you compare against using the LLM's parametric knowledge for safety judgments vs. your formalized RAG-to-OWL pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tp6xU8F2Dr", "forum": "wb05ver1k8", "replyto": "wb05ver1k8", "signatures": ["ICLR.cc/2026/Conference/Submission7150/Reviewer_fsQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7150/Reviewer_fsQj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417203748, "cdate": 1761417203748, "tmdate": 1762919316542, "mdate": 1762919316542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical safety problem of using Large Language Models (LLMs) as planners for embodied AI agents. The authors argue that existing methods, which rely on LLM self-correction or simple verifiers, are insufficient for providing formal safety guarantees. They propose the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture based on a \"tutor-apprentice\" model. In this framework, a deterministic \"Logic Tutor\" (powered by an OWL 2 ontology and a Pellet reasoner) provides structured, causal, and pedagogical feedback to an LLM \"Apprentice\" (the planner, e.g., Gemini 2.5).\n\nThe framework's contributions are threefold: (1) A RAG-based, human-in-the-loop workflow to synthesize a formal safety knowledge base from real-world documents. (2) A VLM-Cascade perception pipeline to generate a rich semantic scene graph (ABox). (3) The core tutor-apprentice refinement loop that uses diagnostic feedback to guide the planner toward verifiably safe solutions. The authors evaluate their framework in a modified version of the SafeAgentBench (kitchen scenarios) within the AI2-THOR simulator, claiming to achieve a perfect 0% Hazardous Action Rate (HAR) while maintaining high task completion (GCR)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the critical and significant problem of ensuring verifiable safety for LLM-based planners in embodied AI. Its primary contribution is the novel \"pedagogical dialogue\" framework, where a deterministic Logic Tutor actively collaborates with and refines the LLM planner's output, moving beyond simple plan rejection to provide causal, diagnostic feedback."}, "weaknesses": {"value": "**1. Insufficient Engagement with Relevant Prior Work.**\n\nThe literature review is notably incomplete and overlooks several highly relevant and representative benchmarks in the field of trustworthy embodied AI. The paper fails to discuss, compare against, or even cite key works such as EarBench, Hazard Challenge , IS-Bench , and LabSafety Bench [1-4]. This omission suggests an inadequate survey of the domain. Furthermore, some of the core ideas presented, such as the use of safety rules, bear a strong resemblance to concepts already introduced in existing works, which is not acknowledged.\n\n**2.Concerns regarding Framework Complexity and Efficiency**\n\n The proposed VIRF framework, which is essentially a tutor-student model, introduces significant architectural complexity. Embodied agents, particularly in dynamic environments, operate under stringent real-time constraints. The paper suggests a verification loop for action plans, but it is unclear if this complex process (plan-verify-diagnose-refine) must be executed before every single action. If so, the accumulated latency would likely be unacceptable for practical deployment.\n\n**3.Lack of Evaluation on Mainstream VLA Models**\n\nThe experiments are conducted using general LLMs like Qwen and Gemini 2.5 as the planner. This is a significant disconnect from the current embodied AI landscape, which is increasingly dominated by end-to-end Vision-Language-Action (VLA) models. The authors do not adequately explain or demonstrate how this text-centric framework would be integrated with or applied to VLA-based agents. Moreover, the primary task planner used for the main results, Gemini 2.5, is an extremely powerful model. This raises concerns about the generalizability of the framework's benefits, as it is unclear whether the \"tutor\" is genuinely effective or if the strong performance is largely attributable to the \"student\" (Gemini 2.5) being exceptionally capable to begin with.\n\n**4.Insufficient and Narrow Experimental Validation**\n\n The empirical evidence provided is not sufficient to fully support the paper's claims.\n\n- Limited Datasets: The evaluation is confined to a subset of kitchen scenarios from SafeAgentBench. This lacks the diversity needed to test the framework's robustness. The field has multiple benchmarks (e.g., the aforementioned EarBench, Hazard Challenge, IS-Bench) that model different types of hazards and environments, which the authors should have included for a comprehensive evaluation.\n\n- Limited Planners: The planner scaling analysis is limited to only the Qwen series and Gemini. To claim broad applicability, the framework should be tested across a wider and more diverse set of open-source and proprietary LLMs.\n\n- Missing VLA Evaluation: As stated in point 3, the complete absence of VLA model evaluations is a major experimental gap.\n\n**5.Absence of Sim2Real Validation**\n\nThe entire evaluation is conducted within the AI2-THOR simulator. While simulation is a crucial first step, there is no validation of the framework's performance on a physical robot (e.g., a robotic arm). It is uncertain whether the proposed method can handle the noise, uncertainty, and complexities of the real world, or if its utility is confined to the simulation environment.\n\n**6. Writing and Formatting Issues:** \n\nThe paper exhibits inconsistent citation practices, frequently confusing `\\citet` and `\\citep`. This results in a citation format that is difficult to read (e.g., \"Wei et al. (2023)\" vs. \"(Wei et al., 2023)\") and detracts from the paper's professionalism.\n\n**Reference:**\n\n[1]Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents.\n\n[2]Hazard challenge: Embodied decision making in dynamically changing environments. \n\n[3]Is-bench: Evaluating interactive safety of vlm-driven embodied agents in daily household tasks.\n\n[4]LabSafety Bench: Benchmarking LLMs on safety issues in scientific labs."}, "questions": {"value": "- The main results (0% HAR) are achieved with Gemini 2.5, an extremely capable planner. The performance with Qwen-72B is significantly worse  and the 7B model fails. How can the authors de-confound the contribution of the VIRF \"tutor\" from the raw capability of the Gemini 2.5 \"student\"? Is it possible that a simpler safety mechanism combined with a powerful planner like Gemini 2.5 would achieve similarly strong results?\n\n- The decoupled evaluation uses a \"golden ABox,\" meaning the core reasoning is tested with perfect perception. It is different from real world scenarios.Given the framework's reliance on a multi-stage, complex pipeline (VLM-Cascade for ABox generation, Pellet reasoner for verification, LLM for refinement), how do the authors assess its viability for a physical Sim2Real deployment? What is the expected resilience of this fragile pipeline to real-world sensor noise, perception failures, and calibration errors that are absent in the simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HgOUjBAEWi", "forum": "wb05ver1k8", "replyto": "wb05ver1k8", "signatures": ["ICLR.cc/2026/Conference/Submission7150/Reviewer_fKL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7150/Reviewer_fKL1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634897702, "cdate": 1761634897702, "tmdate": 1762919315958, "mdate": 1762919315958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hybrid architecture which includes a neuro-symbolic element to perform determinitstic, formal safety verification to an LLM planner. This is tantamount to a dual system in Dual Process Theory by Kahneman. It therefore leverages both the generative ability of generative models and the logical robustness of a neuro-symbolic system. It further includes a tutor-apprentice refinement loop."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides an original framework that combines generative model planner with a neuro-symbolic tutor grounded in domain knowledge and Rich Semantic Scene Graphs. The framework and the new challenge scenarios are novel contributions to the field. The experiments are well-designed and provide a detailed comparative analysis with other methods and its own's ablation study. The paper is well written, with clear explanation of components and good experimental design to bring out the key insights from the evaluation."}, "weaknesses": {"value": "Not exactly a weakness but the experimental result seems to show that VIRF Reject and Manual's performance are closer to VIRF full than VIRF RAG to VIRF Full. To an extent, this questions the usefulness of the RAG-synthesized component. But overall, there is no notable weakness with the submission."}, "questions": {"value": "How scalable is the neuro-symbolic algorithm/framework with more complex query and scenes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dY7kWEKobO", "forum": "wb05ver1k8", "replyto": "wb05ver1k8", "signatures": ["ICLR.cc/2026/Conference/Submission7150/Reviewer_joDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7150/Reviewer_joDr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658638802, "cdate": 1761658638802, "tmdate": 1762919315375, "mdate": 1762919315375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture designed to enable safe and verifiable planning for embodied AI systems. The central idea is to bridge the gap between the stochastic creativity of Large Language Model (LLM)-based planners and the deterministic reasoning of symbolic systems. VIRF implements a tutor-apprentice paradigm, in which a Logic Tutor grounded in a formal ontology provides causal, verifiable feedback to an LLM Apprentice planner. Unlike existing approaches that either rely on probabilistic self-correction or shallow feedback from external tools, VIRF employs pedagogical, causal dialogue that explains why a plan fails and guides refinement.\n\nThe framework integrates three pillars: (1) a semi-automated knowledge acquisition pipeline that constructs a scalable, evidence-based safety ontology; (2) a VLM-Cascade Perception module that builds a rich semantic scene graph for logical reasoning; and (3) the Verifiable Tutor-Apprentice loop, which iteratively refines unsafe plans using structured diagnostic reports derived from formal logic proofs.\n\nExperiments on SafeAgentBench and new knowledge-driven challenge scenarios show that VIRF achieves a 0% Hazardous Action Rate (HAR) and a 77.3% Goal-Condition Rate (GCR), outperforming baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Technical quality and novelty: VIRF represents a principled integration of symbolic verification (OWL 2 DL + Pellet reasoner) with generative LLM planning. Its neuro-symbolic design, particularly the causal feedback loop that translates logical proofs into natural language tutoring, is a significant conceptual advance over corrective or rejection-based paradigms.\n- Motivation and problem framing: The authors convincingly argue that self-referential LLM safety paradigms lack formal verifiability, motivating the need for an independent symbolic system. The paper’s framing around “safety through pedagogy” provides a fresh and human-like cognitive analogy (System 1 vs. System 2).\n- Experimental validation: VIRF achieves a perfect 0% HAR while maintaining strong task success (GCR 77.3%), demonstrating that rigorous safety need not reduce efficacy. The ablation studies (VIRF-RAG, VIRF-Manual, VIRF-Reject) isolate the contribution of each component effectively, and the perception ablation validates the design trade-offs between accuracy and latency."}, "weaknesses": {"value": "I fully agree with the paper’s vision of integrating large language models (LLMs) with symbolic reasoning to achieve verifiable, trustworthy embodied AI. However, I think two structural constraints in symbolic tool integration that VIRF has not yet convincingly overcome.\n\n- Limited empirical realism and unclear practical validation: While VIRF is architecturally sound and conceptually compelling, its empirical persuasiveness remains limited relative to its implementation complexity. All evaluations are confined to the AI2-THOR simulator, and it is unclear whether the designed scenarios genuinely capture the level of physical risk or environmental uncertainty that would necessitate human-in-the-loop safety reasoning. The paper reports a “0 % Hazardous Action Rate,” yet the simulated setting may not reflect truly hazardous or high-stakes conditions that test real-world robustness. Moreover, integrating multiple components, such as the OWL 2 DL reasoner, Pellet verifier, and RAG-based ontology pipeline, appears computationally demanding, but evidence of end-to-end stability or deployment feasibility beyond simulation is limited. Without more realistic validation, the framework’s contribution risks converging conceptually with existing neuro-symbolic planners [1, 2, 3].\n- The framework’s dependence on a manually curated and expert-audited ontology, while ensuring formal soundness, introduces a structural limitation on adaptability and continual learning. The authors acknowledge this “static knowledge core” issue (Section 6, Appendix I) but stop short of implementing any concrete mechanism for dynamic ontology updating or automated rule induction. Without such capabilities, VIRF may struggle to generalize to unseen environments or novel object types, especially when safety rules must evolve beyond the predefined TBox. Recent advances in adaptive symbolic solvers and continual neuro-symbolic learning [4] demonstrate that online modification of symbolic rules is feasible and could strengthen this framework’s long-term scalability.\n\n[1] Generalized planning in pddl domains with pretrained large language models. Tom Silver et al.\n\n[2] CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning. Xinrui Lin et al.\n\n[3] Llm+ p: Empowering large language models with optimal planning proficiency. Bo Liu et al.\n\n[4] NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains. Wonje Choi et al."}, "questions": {"value": "- Does the submission provide any evaluation or discussion regarding VIRF’s performance in realistic human-in-the-loop or physical-world scenarios involving hazardous actions or perception noise? Is there any analysis of how the current OWL-based verification pipeline would scale when integrated into a real robotic control loop?\n\n- Does the submission address the potential for dynamic knowledge evolution within VIRF? In particular, is there any discussion on extending the Logic Tutor or RAG pipeline to enable self-revision, such as autonomously verifying and incorporating new safety axioms during deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KS3VZFnyJS", "forum": "wb05ver1k8", "replyto": "wb05ver1k8", "signatures": ["ICLR.cc/2026/Conference/Submission7150/Reviewer_f2Q4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7150/Reviewer_f2Q4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750773184, "cdate": 1761750773184, "tmdate": 1762919314562, "mdate": 1762919314562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}