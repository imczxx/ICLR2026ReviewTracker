{"id": "GL4jwXuR65", "number": 3158, "cdate": 1757344386148, "mdate": 1759898105367, "content": {"title": "Learning Hippo: A Hippocampal Neural Architecture for Deep Learning", "abstract": "Current neural networks exhibit known limitations due to their fragility against corrupted data and the requirement for large amounts of data for training. These limitations are in deep contrast to the efficiency and robustness of many biological systems such as the human brain. In this work, we present \\textit{Hippo-1}, a biological neuron model along with a specific architecture inspired by the CA3 region of the hippocampus. Our biological model is based on a justified compartmental simplification of the neuron which allows us to preserve essential non-linear dendritic integration and on a specific interaction between excitatory and inhibitory neuron populations through a shunting inhibition mechanism. \nTo validate the approach, we evaluated the performance of three simple \\textit{Hippo-1} models (named High, Medium, and Low) on standard image classification tasks (i.e., MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, Tiny ImageNet), comparing them with Multi-Layer Perceptron (MLP) networks of comparable parametric complexity. The experimental results demonstrate that \\textit{Hippo-1} not only achieves competitive or superior performance compared to the standard MLPs but also shows a superior robustness to noise (Gaussian and salt-and-pepper), with a more gradual performance degradation as input corruption increases. Gains in adopting Hippo-1 models increase with the complexity of the task, being approximately 5\\% on Cifar-10, around 10\\% on Cifar-100 and as high as 50\\% on Tiny-ImageNet. In fact, our work shows that integrating targeted neuroscientific principles, rather than a complete morphological simulation, represents an effective strategy for building more resilient models, combining biological plausibility with computational efficiency.", "tldr": "We introduce Hippo-1, a neural architecture inspired by the hippocampal CA3 region, which uses compartmental simplification and shunting inhibition to achieve superior performance and robustness to noise compared to standard MLPs.", "keywords": ["Deep Learning", "Neural Architecture", "Hippocampus", "CA3 Region", "Compartmental Models", "Shunting Inhibition", "Dendritic Integration", "Robustness to Noise"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32e17645558be051d4add5d7b3221cc6213f8ac7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new architecture, inspired by the hippocampus, in which various neurons (or is it neural projections from a given neuron?) are divided into multiple compartments, such that each compartment affects target neurons at a different stage of processing (?).\n\nBased on multiple small-scale image classification experiments, the proposed model seems to outperform simple one-hidden-layer multi-layer perceptrons (MLPs) for a given number of trainable parameters, without increasing computational requirements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The architecture seems novel. The results are intriguing. The review of hippocampal function is informative."}, "weaknesses": {"value": "The main problem is that the architecture is not precisely described, making it impossible to reproduce (or understand), and thus preventing acceptance. \n\nHere are a few questions:\n\n- Where are the weights? The architecture  seems to have some kind of trainable connection weights, judging from the main text. However, no such trainable weights appear in Equations 1-5 - only the inhibitory-specific slope parameters s are shown. As a result it is impossible to know at which of the successive \"stages\" (if any) trainable weights are applied. \n\n- We are told that there is a \"gamma\" parameter denoting an \"attenuation coefficient\" for inhibition. Where is it applied? \n\n- In line 234 we are told that further layers should learn \"nonlinear combinations\" of the concatenated inhibitory and excitatory portions of H. But in line 210 we are told that the input H is just split into two parts for inhibition and excitation. Which is it?\n\n- Table 1 suggests the initial linear layer has dimension different from the \"dimensionality\" of E/I neurons. This suggests that some sort of projection occurs between the linear layer and the E/I neurons. But no such projection is reported in the equations, which instead suggest the input is simply partitioned and then a series of nonlinearities is applied to each neuron/channel in parallel.\n\n- More generally it is never clear what counts as a single neuron, a whole group of neurons, or a single channel (a connection between neurons)\n\n- l. 277: \"hidden dimensions h and c of the bio-inspired modules\" - what are those? They don't appear in the equations.\n\nBasically, the architecture must be explained precisely, *with equations and/or pseudocode* describing the whole process, and clearly defining *all* symbols (and also clearly differentiating scalars, vectors and matrices). At present, the description, such as it is, is not ready for publication.\n\nThere are two additional caveats:\n\n- It's not clear how fair the comparison with MLP is. The HIPPO model includes a lot more nonlinearities and multiplicative interactions. It also had its hyperparameters carefully tuned. No such tuning is reported for the MLPs. At the very least, MLPs with two layers (but similar number of overall parameters) should be tried.\n\n\n- It is interesting that the proposed model does not seem to require more computation (per parameter) than MLPs. However, the specific metric of \"accuracy gain per flop\" is described as dividing the accuracy gain by the flops, which seems strange (e.g. if they had exact same accuracy, but very different flops, the result would be 0, indicating no difference, which would not reflect the very different efficiencies. Admittedly it is not obvious how to jointly compare two independent dimensions. \n\nMinor:\n\n- Fix the citations, using correct parenthetical style (the authors may find the following latex command helpful: \\renewcommand{\\cite}{\\citep}  )\n\n- l. 271 \"when a new maximum was not reached...\" - a new maximum of what?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Mii9OHmMc5", "forum": "GL4jwXuR65", "replyto": "GL4jwXuR65", "signatures": ["ICLR.cc/2026/Conference/Submission3158/Reviewer_4kxP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3158/Reviewer_4kxP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587550310, "cdate": 1761587550310, "tmdate": 1762916576488, "mdate": 1762916576488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to propose a biologically-inspired architecture for deep learning, inspired by hippocampal pyramidal neurons, with the goal of improving performance on specific tasks and the robustness of deep learning models to input perturbations. First, the authors propose their core model architecture which involves separate excitatory and inhibitory components/channels which interact through several additive and multiplicative steps, improving the expressivity of the architecture over simple artificial neural network (ANN) units typically found in multi-layer perceptrons (MLPs). Then, the authors evaluate their method on several image classification benchmarks and compare its performance with that of MLPs in terms of classification accuracy and robustness to corruption in input image pixels (with Gaussian or salt-and-pepper noise). The authors find that on more complex classification tasks (CIFAR-100 & TinyImageNet), their method outperforms MLPs while typically using fewer FLOPS overall and also working better with less training data. Overall, the claim is that architectures with a certain degree of bio-inspiration can outperform typical ANNs and serve as a promising replacement for typical deep learning models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* An implementation of biological neural computations in artificial neural network models could improve our understanding of how the brain works and which features of neural computation contribute towards improving performance and robustness.\n* The proposed model is more efficient in terms of number of FLOPS (and thus, the amount of compute) and typically has fewer parameters than the chosen competitor model."}, "weaknesses": {"value": "* Why do the authors consider an MLP, that too one with a single hidden layer alone and just different numbers of units, as the only baseline comparison? There are several other architectures, even just MLPs (although CNNs, ViTs, etc. are highly performant) that perform better than the MLP baseline considered here, and are also quite parameter-efficient or at least comparable. In particular, models like simple CNNs could very reasonably outperform Hippo-1 with far fewer parameters (and the CNN has bio-inspired origins too). There is also no comparison to bio-inspired architectures such as spiking MLPs, e.g., https://arxiv.org/abs/2203.14679 or others.\n* On a related note, the authors have considered only a single architecture with the MLP as a base and not adapted their model to work with CNN-style layers. This severely limits the applicability of the framework introduced.\n* Only image classification tasks have been considered, that too, with an MLP-like architecture. To make a convincing case for a new architecture, I would expect additional tasks (e.g., regression, sequence prediction, etc.) and also for the framework/modifications to be integrated into more modern models and other architectures such as CNNs, RNNs, or Transformers – and to test whether there are actually any meaningful improvements.\n* While the authors have tested the robustness of their model to some input perturbations, e.g., Gaussian and salt-and-pepper noise, they have not evaluated whether their models are robust to more sophisticated perturbations such as those generated through adversarial attacks, which can be specifically optimised to increase misclassification in a model-specific way. Claims about improved robustness would, in my opinion, require testing against sophisticated threats that deep learning models could face in the real world.\n* It is unclear how important the several hyperparameters are to the Hippo-1 architecture, as there is no reported robustness study to changes in hyperparameter values. Furthermore, it does not seem like an equivalent hyperparameter optimisation scheme was considered for the MLP baseline, which calls into question the fairness of the comparisons reported here.\n* A critical weakness is that the actual accuracy values reported, for both the MLP and Hippo-1, are extremely low in comparison to typical model accuracies on these datasets. In fact, I have in my experience trained simple MLPs that perform quite a bit better than the reported baseline MLP on datasets such as CIFAR-10. To be precise, the reported accuracies for Hippo-1 on CIFAR-10, CIFAR-100 are 51.5%, 24.1% – these are really low, vanilla fully-connected networks have been shown to perform far better (https://arxiv.org/pdf/2007.13657).\n* There is no discussion of a large amount of relevant work on bio-inspired architectures for deep learning and image classification. I am not providing an exhaustive list here but references to fundamental papers on CNNs, the dataset publications, papers on gating and other mechanisms added to ANNs and inspired by the brain, etc. are missing.\n* Implementation details are very sparse and I am unable to get a good understanding of the architecture from Section 3. There is also no code provided.\n* Finally, there are several formatting and presentation issues, the first of which could be quite serious in my opinion:\n    * There are 3 garbled references with incorrect information: (1) \"Quantitative morphometry...\" by Ascoli et al. should have authors Ascoli, Brown, Calixto, Card, et al. (https://pubmed.ncbi.nlm.nih.gov/19496174/), but the listed authors are those on another paper by Ascoli et al. (https://pubmed.ncbi.nlm.nih.gov/20445069/); (2) \"Robust resting-state...\" by Kopsick et al. should have authors Kopsick, Tecuati, Moradi, Attili, et al. (https://pubmed.ncbi.nlm.nih.gov/37663748/), but the listed authors are from another Kopsick et al. paper (https://link.springer.com/article/10.1007/s10827-024-00881-3); (3) Stingl, Draguhn & Both has incorrect issue information (https://pubmed.ncbi.nlm.nih.gov/39162833/).\n    * Template text has not been removed from the appendix.\n    * The paper seems heavily LLM-edited overall – although, the authors acknowledge that LLMs were used to translate the paper into English and polish the text. Typical papers contain several subsections and paragraphs, and references are not just scoped to one literature review section but are spread throughout. Here, the overall structure, lack of paragraphs, and lack of justifications/references for several claims overall leads me to believe that a significant amount of text could have been the output of an LLM.\n\nMy assessment is that the paper represents an initial step towards what should be a more comprehensive investigation, and currently, the paper lacks crucial justifications and has an undefined contribution – it neither advances performance for deep learning as claimed (since I believe the comparisons are not rigorous enough), nor does it yield much insight into neural computation. Overall, I believe that there are several critical weaknesses as discussed, which unfortunately make me strongly opposed to accepting the paper in its current form. I would encourage the authors to improve upon their work by considering these points and attempting to sharpen their work through more rigorous experimentation and better presentation."}, "questions": {"value": "* Why is the performance of Hippo-medium quite consistently worse than Hippo-low?\n* Could the authors comment on training times for Hippo-1 versus MLPs? The times reported for Hippo-1 are so much slower than the MLPs. Further, in some cases Hippo-high has been given more params than the MLP, but the performance is very close (e.g., CIFAR-10, CIFAR-100). Could the authors comment on this?\n* Why do the authors consider a hippocampus-inspired architecture for visual tasks? In general, I feel there is a lack of justification for the model proposed here, and there are no references or explanations for why the authors consider the specific 5-way channel partition and implementation that they have explored here. Are there any references to prior work or indication that such computation is indeed performed in Hippocampal neurons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z0Skn8szlO", "forum": "GL4jwXuR65", "replyto": "GL4jwXuR65", "signatures": ["ICLR.cc/2026/Conference/Submission3158/Reviewer_Vrmw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3158/Reviewer_Vrmw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988297205, "cdate": 1761988297205, "tmdate": 1762916576207, "mdate": 1762916576207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hippo-1, a neural network architecture inspired by the CA3 region of the hippocampus, incorporating excitatory/inhibitory (E/I) populations and multiplicative shunting inhibition within multi-compartment neurons. The authors argue that this design preserves key dendritic integration principles while remaining computationally tractable. They report that Hippo-1 outperforms parameter-matched MLPs on several image-classification benchmarks (MNIST, CIFAR-10/100, Tiny-ImageNet), exhibits improved robustness to Gaussian and salt-and-pepper noise, and provides an accuracy gain per FLOP on harder datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an interesting and timely problem: what advantages might biological structure confer on neural networks? Conceptually, the paper is interesting because it explores how dendritic-compartment interactions might improve learning performance and efficiency. \n\nIt is good to see an exploration of robustness and efficiency, in addition to just task performance. The experiments also cover a range of image datasets."}, "weaknesses": {"value": "Hippo-1’s accuracy-per-FLOP is negative on simple datasets (MNIST, Fashion-MNIST) and only becomes positive where MLPs already perform poorly (CIFAR/Tiny-ImageNet). This suggests that the “improvement” largely reflects MLP inadequacy rather than a fundamental advance. The claim that the model scales better with complexity therefore lacks a solid baseline; comparisons with alternative architectures, such as convolutional connected networks for example would be more informative.\n\nThe paper does not articulate why or how the proposed architectural inductive biases drawn from biology benefit the tasks evaluated. The mechanisms drawn from hippocampal neurons, namely compartmentalized dendritic integration and multiplicative shunting inhibition between excitatory and inhibitory populations, are biologically well-motivated, but the authors do not analyze their computational role. The gains may just reflect a more expressive nonlinearity rather than a meaningful hippocampal computation. The authors could strengthen their case by performing ablations on shunting inhibition, E/I separation, or compartmental depth.\n\nThe paper is motivated by the hippocampal trisynaptic loop, in particular CA3’s role in it. Hippocampal CA3 is primarily associated with associative memory, pattern completion, and sequence learning, yet the evaluation focuses exclusively on image classification. The choice of image datasets is not justified and does not probe the motivating computational role of the architecture. A more appropriate test would involve memory-centric or reconstruction tasks. Relatedly, the architectural changes are primarily dendritic compartmentation. As such, the works’ motivation appears to be disconnected and it should focus more on dendritic computation. \n\nThe introduction has no citations.\n\nThe paper omits key references integrating dendritic computation and EI structure into ANNs ( Chavlis & Poirazi 2025, Poirazi & Mel 2001, Cornford 2021, Jones & Kording 2021 among others)."}, "questions": {"value": "See weaknesses. Minor points are:\n\n- Can the multiplicative inhibition be negative, changing the polarity of the excitatory channel?\n- Inhibition is only multiplicative here, why not also subtractive? \n- What are the layers referred on line 235? There is only ever one, correct?\n- The model description should be improved. There is also a tanh in figure 1 but not in section 3. Section 4 also introduces new parameters defining the model (d_out, hidden_dim). Why is n_stages even mentioned in table 1, it is always 1 and should be removed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WULC9k73BP", "forum": "GL4jwXuR65", "replyto": "GL4jwXuR65", "signatures": ["ICLR.cc/2026/Conference/Submission3158/Reviewer_vDKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3158/Reviewer_vDKG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994231579, "cdate": 1761994231579, "tmdate": 1762916575928, "mdate": 1762916575928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}