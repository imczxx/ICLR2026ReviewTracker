{"id": "DwerBtrre2", "number": 21154, "cdate": 1758314345646, "mdate": 1763349794368, "content": {"title": "AHELM: A Holistic Evaluation of Audio-Language Models", "abstract": "Evaluations of audio-language models (ALMs)---multimodal models that take interleaved audio and text as input and output text---are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets---including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering---to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 6th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs will be available online. AHELM is intended to be a living benchmark and new datasets and models will be added over time.", "tldr": "A fully transparent, standardized, and automatic benchmark that measures 10 aspects most relevant to audio language models", "keywords": ["Audio language models", "Holistic", "Benchmark", "Transparency", "Stereotypes"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3515e7e68147440a0cd9698349a2d56a94b7567d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduce AHELM, a holistic benchmark for audio-language models (ALMs). The benchmark assesses model performance across ten key dimensions: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. The authors evaluate 14 systems, including both open-source and closed-API models, uncovering previously unreported weaknesses and providing valuable insights for future model development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The primary contribution of this work lies in integrating previously underexplored evaluation dimensions for audio-language models (bias, fairness, robustness, toxicity and safety) into a unified assessment framework. This holistic design broadens the scope of evaluation beyond traditional perception and reasoning metrics.\n* The introduction of two new datasets, CoRE-Bench (multi-turn conversational audio reasoning) and PARADE (stereotype probing), fills critical gaps in existing benchmarks.\n* The discussions are well-reasoned and provide a solid understanding of the broader challenges of current audio-language models."}, "weaknesses": {"value": "* The paper lacks sufficient discussion of related work and omits direct comparisons with existing audio-language model benchmarks such as AIR-Bench[1], MMAU[2], Dynamic-SUPERB[3,4], and VoiceBench [5], which already explore evaluation in multi-aspect overlapped with this work. A comparative analysis against these frameworks would help clarify AHELM’s distinct contributions and positioning.\n* The originality of the benchmark is somewhat limited, as most evaluation tasks are directly adopted from existing datasets rather than newly designed or substantially modified.\n* The coverage and completeness of certain evaluation aspects remain constrained. For instance, in the bias and fairness dimension, only gender is considered, leaving out other critical attributes such as accent, age, and ethnicity. Similarly, in multilinguality evaluation, only two language pairs are included for speech translation, which restricts the generalizability of the findings. Lastly, robustness focuses only on additive noise without considering reverberation, clipping, etc.\n* [Minor] From a “holistic evaluation” perspective, it would be beneficial to include a broader range of ALMs. Currently, the open-source category includes only the Qwen-family models. Incorporating other systems, such as the Audio-Flamingo series, and older model like SALMONN and WavLLM, would yield a more representative comparison and provide valuable insights into the progress and evolution of the community’s efforts over time.\n\n[1] Yang, Qian, et al. \"AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension.\" ACL 2024. 2024.\n\n[2] Sakshi, S., et al. \"MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark.\" ICLR 2025. 2025.\n\n[3] Huang, Chien-yu, et al. \"Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech.\" ICASSP 2024, 2024.\n\n[4] Huang, Chien-yu, et al. \"Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks.\" ICLR 2025. 2025.\n\n[5] Chen, Yiming, et al. \"Voicebench: Benchmarking llm-based voice assistants.\" 2025."}, "questions": {"value": "* Why are aspects such as hallucination detection, instruction-following ability not included? These capabilities are increasingly important for evaluating multimodal large language models [6,7] and would further broaden the coverage of this benchmark.\n* The reliance on mean win rate (MWR) as the primary aggregation metric may limit benchmark stability and reproducibility, since results can shift as new models or datasets are added. How do the authors plan to mitigate this issue to ensure long-term comparability and transparency for benchmark users?\n\n[6] Kuan, Chun-Yi, et al. \"Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models.\" Interspeech 2024. 2024.\n\n[7] Gao, Yiming, et al. \"IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models.\" 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CuKf2mdU5V", "forum": "DwerBtrre2", "replyto": "DwerBtrre2", "signatures": ["ICLR.cc/2026/Conference/Submission21154/Reviewer_qDSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21154/Reviewer_qDSB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912116457, "cdate": 1761912116457, "tmdate": 1762941512638, "mdate": 1762941512638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NdUtquZaJt", "forum": "DwerBtrre2", "replyto": "DwerBtrre2", "signatures": ["ICLR.cc/2026/Conference/Submission21154/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21154/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763349793413, "cdate": 1763349793413, "tmdate": 1763349793413, "mdate": 1763349793413, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AHELM, a benchmark aiming to comprehensively evaluate audio-language models (ALMs). The proposed benchmark covers 10 aspects of ALMs by aggregating existing benchmarks and two newly proposed datasets. The evaluation reveals limitations of current ALMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Introducing two new datasets.\n- The paper writing is clear enough."}, "weaknesses": {"value": "### Missing References\nWhile the authors claim that AHELM provides a holistic evaluation framework, it overlooks several important aspects and lacks sufficient references to prior work. For instance, multiple benchmarks have already been proposed to assess different dimensions of reasoning in ALMs [1–4]. Although these benchmarks target different aspects compared to the proposed CoRe-Bench, it remains essential for the authors to acknowledge and cite these works to improve the completeness and contextual grounding of the paper.\n\nFurthermore, the current version of the paper may give readers the impression that the proposed PARADE benchmark is the first to evaluate bias and stereotypes in ALMs. This framing is inaccurate, as several previous studies have already explored bias and stereotyping in ALMs [5,6]. The authors are strongly encouraged to revise the related work section to properly acknowledge these contributions and clarify how PARADE differs from or extends them.\n\nFinally, to strengthen the coverage and accuracy of references, the authors may refer to a recent survey on ALM evaluation [7], which provides a comprehensive overview of related efforts. Overall, I believe the current version of the paper insufficiently recognizes prior research, which weakens its scholarly positioning.\n\n\n\n[1] Sakshi et al., \"MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark\", ICLR 2025\n\n[2] Yang et al., \"SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information\", Interspeech 2025\n\n[3] Ma et al., \"MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix\", arXiv preprint\n\n[4] Yang et al., \"SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models\", arXiv preprint\n\n[5] Lin et al., \"Listen and Speak Fairly: a Study on Semantic Gender Bias in Speech Integrated Large Language Models\", SLT 2024\n\n[6] Lin et al., \"Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models\", SLT 2024\n\n[7] Yang et al., \"Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey\", EMNLP 2025\n\n### Coverage of ALMs\n\nThe current coverage of ALMs is quite restricted. I believe it will be much better if the authors can report performances of more ALMs.\n\n### Experimental Results\n\nI would suggest the authors to move the results of ALMs on the CoRe-Bench and PARADE to the main text of the paper. In addition, please consider to include statistical measures to demonstrate the significance of the reported results.\n\nIn addition, since the proposed benchmark highly relies on existing ones, the insights is not quite novel, which will undermine the contribution of this paper."}, "questions": {"value": "I have listed my concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iPMBzF9lUu", "forum": "DwerBtrre2", "replyto": "DwerBtrre2", "signatures": ["ICLR.cc/2026/Conference/Submission21154/Reviewer_STuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21154/Reviewer_STuJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934340079, "cdate": 1761934340079, "tmdate": 1762941511736, "mdate": 1762941511736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents AHELM, a benchmark designed for evaluating audio-language models (ALMs) across ten dimensions: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. It assesses 14 ALMs and three ASR+LM baselines. Experimental results show that Gemini 2.5 Pro achieves the best overall performance but exhibits unfairness in certain settings. Additionally, the authors also found that simple ASR+LM pipelines perform competitively in several areas, particularly in robustness and certain emotion recognition tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark covers ten clear-defined aspects that cover a wide range of speech and audio tasks drawn from existing datasets and benchmarks. In addition, the authors curated two new datasets to address specific aspects not covered by existing resources.\n- The authors standardized the evaluation setup by using fixed zero-shot prompts and consistent decoding constraints, which helps reduce variance across different models.\n- The plan to release prompts, generations, and a leaderboard is aligned with reproducibility and community adoption."}, "weaknesses": {"value": "- In the abstract, the authors claim there is a lack of standardized ALM benchmarks and that existing ones cover only one or two aspects. This is doubtful. Benchmarks such as AIR-Bench and Dynamic-SUPERB already cover more than a few aspects.\n- As a benchmark paper, it does not review prior work on ALM benchmarks. The related work section only discusses ASR benchmarks.\n- The framework highlights \"adaptation\" as a key part, but all experiments are zero-shot. The reason to include adaptation in the framework is unclear.\n- The ten aspects are clearly defined, but the rationale is not. Why do these ten yield a holistic evaluation? Is there prior work or information that supports this grouping?\n- The 14 ALMs tested are mainly Gemini, GPT, and Qwen, and Qwen is the only open-source model. The paper says it selects \"popular state-of-the-art\" models for a fair comparison, which I do not consider an appropriate statement. The criteria for \"state of the art\" are not stated, and several open-source models from the community are missing.\n- Many findings are not new. For example, the first two points were already observed in Dynamic-SUPERB Phase-2. For the emotion result, note that MELD extends EmotionLines, a text-only sentiment dataset, so its spoken content naturally carries cues for emotion.\n- The use of an LLM as a judge in this paper is questionable. The reported agreement rate is only around 50%, which makes the LLM-based scores unreliable. Moreover, using GPT-4o as the evaluator raises reproducibility concerns. Even with the temperature fixed at 0, GPT-4o outputs can vary due to hidden randomness or version updates. Since OpenAI frequently modifies its models, it is unclear how long the version used in this paper will remain accessible. Although the authors attempt to fix the inference configuration, the evaluation setup is non-reproducible."}, "questions": {"value": "Please refer to my comments in the weakness section. I also have a few minor comments:\n- Figure 1 is never mentioned in the main text. It appears only once in the caption of Table 1, and it is unclear whether this is intentional.\n- There are several typos in the paper, such as \"Samping Rate\" in Table A3, \"unaswerable\" in Section E.4.4, and \"µmale and µmale\" in Appendix H.\n- Figure 4 is difficult to read because it includes too many models with limited colors, and the data points for different models overlap heavily."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "Since AHELM includes data used to evaluate bias, fairness, toxicity, and safety, I believe the paper should undergo an additional ethics review to ensure that potential ethical concerns are properly addressed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hqJJQVjJvD", "forum": "DwerBtrre2", "replyto": "DwerBtrre2", "signatures": ["ICLR.cc/2026/Conference/Submission21154/Reviewer_4EZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21154/Reviewer_4EZE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946988771, "cdate": 1761946988771, "tmdate": 1762941511059, "mdate": 1762941511059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}