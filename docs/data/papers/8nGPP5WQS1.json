{"id": "8nGPP5WQS1", "number": 14839, "cdate": 1758244594270, "mdate": 1759897346232, "content": {"title": "FUSION-BASED ERROR-FEEDBACK AUGMENTATION FOR LOW-RESOURCE RADIOLOGY REPORT GENERATION", "abstract": "Radiology report generation (RRG) is critical for assisting clinical diagnosis, yet current methods struggle to effectively integrate multi-view images and longitudinal patient data while operating under constrained annotation resources. Existing approaches often rely on large-scale supervised datasets and lack adaptability in low-resource settings. To address these challenges, we propose FEFA, a novel approach that combines a multi-expert image fusion module with an error-feedback augmentation strategy powered by large language models. Our fusion module dynamically combines current and prior images using tailored attention and gating mechanisms, producing compact and informative representations. Furthermore, the error-feedback mechanism enables self-correction during training by incorporating error analyses from previous stages. Experiments on MIMIC-CXR show that FEFA achieves state-of-the-art performance with only 8% of the supervised data, attaining 94% of the clinical efficacy score of the best existing method while outperforming all other competitors. Our work demonstrates significant improvements in data efficiency and model adaptability for real-world clinical scenarios.", "tldr": "", "keywords": ["Radiology report generation (RRG)", "low-resource settings", "multi-expert image fusion", "error-feedback augmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0fc1a186eb28cb6a854823b9bc61f664fa5d44c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FEFA (Fusion-based Error-Feedback Augmentation), a novel approach for low-resource radiology report generation (RRG). It introduces two core mechanisms: (1) Multi-expert Image Fusion Module: Using self-attention, cross-attention, and gating mechanisms to dynamically fuse multi-view input images (frontal, lateral, prior); (2) Error-feedback Augmentation: Incorporating sample-level error analysis and common error lists generated by a large language model (LLM) to enable self-correction during training. Experiments on MIMIC-CXR show that FEFA achieves 94% of the clinical efficacy score of the best existing method with only 8% of the supervised data, outperforming all other competitors. The work demonstrates significant improvements in data efficiency and model adaptability for real-world clinical scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe combination of multi-expert fusion and error-feedback augmentation provides both technical and practical benefits.\n2.\tExperimental results demonstrate the effectiveness of FEFA with limited data and outperform other baselines.\n3.\tThe paper provides a theoretical framework with the UID theory, even though it requires further quantification.\n4.\tThe approach is highly relevant for low-resource medical AI, which is an important and under-explored area."}, "weaknesses": {"value": "1.\tThe error-feedback augmentation method needs more in-depth analysis to show its robustness in noisy environments or when dealing with imbalanced error feedback.\n2.\tThe impact of curriculum learning could be more clearly defined and validated independently from other components.\n3.\tThe UID theory is an interesting hypothesis but requires statistical validation to substantiate its role in improving the model's interpretability and performance."}, "questions": {"value": "1.\tCan the error-feedback mechanism handle noisy or inconsistent error feedback, and how does it affect the model's performance in such cases?\n2.\tHow does curriculum learning contribute to the model's improvement in low-resource settings? Are the results independent of the other modules?\n3.\tWould the error-feedback strategy still be effective if the model is trained with a larger dataset? How would it scale?\n4.\tCan you provide more quantitative analysis or statistical validation for the UID theory claims?\n5.\tHow would FEFA perform when applied to datasets with more heterogeneous, real-world data or in cross-domain applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ti4dnpnSRP", "forum": "8nGPP5WQS1", "replyto": "8nGPP5WQS1", "signatures": ["ICLR.cc/2026/Conference/Submission14839/Reviewer_11TT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14839/Reviewer_11TT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693043167, "cdate": 1761693043167, "tmdate": 1762925192251, "mdate": 1762925192251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes FEFA, a multi-expert image fusion module that additionally incorporates error-feedback augmentation, with the augmentation strategy utilizing a large language model (LLM). FEFA compares favourably against the state-of-the-art MAIRA-2, using only 8% of the supervised data. In particular, a multi-expert fusion module that addresses MAIRA-2's issues with fusion by concatenation, is proposed. However, major details especially as pertaining to the LLM are missing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- (Originality) Proposal of multi-expert image fusion module, comprising three separate experts to handle various combinations of radiological views\n\n - (Quality) Detailed justification, description and reporting of Uniform Information Density metrics for quantifying understandability and supporting the use of attention-based fusion\n\n - (Clarity) Provided case studies in the Appendix\n\n - (Significance) Empirical finding on possible detrimental impact of lateral radiological image views, without prior frontal images"}, "weaknesses": {"value": "- Severe lack of details about the downstream LLM (even its architecture; it is just referred to as \"an LLM\" in the Appendix), especially regarding parameter settings and LLM fine-tuning in the curriculum learning and error-aware SFT stages (Figure 3); only broad objective formulae (Equations 6-8) are provided\n\n - Lack of direct comparison for FEFA against competing SOTA methods on the same training datasets (Table 2), with no attempt to minimally match training scales, at least for the lowest scale (40k)\n\n - Again, while the claim of achieving 94% of MAIRA-2 performance using only 8% of the supervised data appears impressive, the tradeoff for most of the competing models (as from Table 2) is not presented, for the direct comparison on low-resource datasets (Table 3). As such, the true contribution of FEFA for low-resource modelling could be further justified\n\n - Section 4.3 might be edited for clarity, as it is not immediately clear as to what ablations (a), (b) and (c) represent, or that FEFA is essentially equivalent to MAIRA-2 + (a), (b), (c) and also test-time feedback; since the ablations appear additive, combinations such as error-feedback augmentation without curriculum learning also do not appear to be explored"}, "questions": {"value": "1. In Section 3.3, for multi-expert image fusion, it is stated that fused image features are output, if only current images (CF or CF+CL) are available. However, it is then stated that the output is a fixed-length representation. It might be clarified as to whether the Self-Att(.) and Cross-Att(.) dimensionalities are designed to be equal in size, and how this output size was determined.\n\n2. Related to the above, in Section 4.4, it is then stated that the fused representation contains no more raw information than concatenation, which appears to imply that the size of the fused representation is equal to the maximum size of concatenated views (three views). However, it also appears that the objective should be to maximize FEFA performance (on the NLG and CX-14 metrics), and not to constrain input dimensionality. If so, it could be discussed as to whether a larger fused representation might be appropriate.\n\n3. Given the observation on lateral views, it might be clarified as to whether these lateral views are used for clinical diagnosis in current medical practice, and if so, what conditions rely more on lateral views for diagnosis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "88I7dbgomh", "forum": "8nGPP5WQS1", "replyto": "8nGPP5WQS1", "signatures": ["ICLR.cc/2026/Conference/Submission14839/Reviewer_fkaR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14839/Reviewer_fkaR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742340217, "cdate": 1761742340217, "tmdate": 1762925191693, "mdate": 1762925191693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a fusion module using attention and gating mechanisms which shows superior performance over the simpler concatenation scheme used in the state-of-the-art model (MAIRA-2). Additionally, an error-feedback augmentation training strategy is introduced for more robustness.\n\nThe approach and specifically the fusion module could be a technical contribution. I'm willing to increase my score if the generalization either on other models or at full training split is convincing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is easy to follow.\n- The method outperforms MAIRA-2 at low resource training splits.\n- Comprehensive ablation study."}, "weaknesses": {"value": "- While the training strategy is promising in low resource training splits, the generalization at full training split is unknown."}, "questions": {"value": "- Would the fusion scheme work with multiple prior images? For example, would the approach be able to use prior lateral image if available?\n- Would this approach work with other models? What architectures would be applicable?\n- Would a reinforcement fine-tuning (RFT) better for error awareness? It seems more prevalent as a method to increase robustness.\n- How is the error-feedback augmentation generated? Is it by another specific LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9C0gl5KaJ", "forum": "8nGPP5WQS1", "replyto": "8nGPP5WQS1", "signatures": ["ICLR.cc/2026/Conference/Submission14839/Reviewer_TrBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14839/Reviewer_TrBt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972239425, "cdate": 1761972239425, "tmdate": 1762925191356, "mdate": 1762925191356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FEFA, a method designed for low-resource radiology report generation (RRG). The framework integrates:\n\n- A multi-expert image fusion module combining current, lateral, and prior images with attention and gating mechanisms.\n\n- An error-feedback augmentation (EFA) strategy inspired by large language model (LLM) self-correction, introducing feedback from earlier training stages as additional textual prompts.\n\nExperiments on MIMIC-CXR show that FEFA achieves ~94% of the full-data MAIRA-2 clinical efficacy score while using only 8% of supervised data. The paper claims improvements in data efficiency and model adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Timely and relevant topic — Tackling low-resource RRG aligns well with the current push toward efficient and clinically reliable medical AI.\n\n2.Novel integration of LLM feedback — Applying error-feedback augmentation for radiology generation is an interesting direction.\n\n3.Reasonable empirical results — The reported gains on MIMIC-CXR under low-resource settings are consistent and moderately significant.\n\n4.Good presentation and writing quality — Figures and tables are clear, methodology is described in a readable manner."}, "weaknesses": {"value": "1.Lack of technical depth and originality (Major):The multi-expert fusion design appears to be a modest extension of MAIRA-2’s concatenation scheme. The attention/gating mechanism lacks architectural novelty and seems incremental. The error-feedback augmentation mainly reuses textual prompts derived from model outputs — similar ideas have been extensively explored in Self-Refine (Madaan et al., 2023), Reflexion (Shinn et al., 2023), and LEMA (An et al., 2023). There is no theoretical or algorithmic innovation specific to the medical domain. The method is essentially a composition of known modules without deep insight into why these components interact effectively.\n\n2.Weak experimental validation (Major): Experiments are only conducted on a single dataset (MIMIC-CXR), which severely limits generalizability. No external dataset (e.g., IU-XRay, CheXpert-Plus) or real-world validation is provided. The claimed “8% data efficiency” lacks ablation under different data sampling strategies — it’s unclear whether the gain persists under non-uniform splits.\n\n3.Unclear evaluation methodology: No human expert evaluation or radiologist assessment is provided to validate clinical soundness. NLG metrics (BLEU, ROUGE, METEOR) remain low compared to state-of-the-art models.\n\n4.Weak connection between error feedback and performance gain: The claimed “self-correction” ability is not convincingly demonstrated. No quantitative analysis shows that the model reduces specific error types after feedback integration.\n\n5.Limited theoretical grounding (Minor): The Uniform Information Density (UID) analysis is interesting but speculative; it lacks rigorous proof or empirical correlation with model performance."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8xCtHuQ9Vh", "forum": "8nGPP5WQS1", "replyto": "8nGPP5WQS1", "signatures": ["ICLR.cc/2026/Conference/Submission14839/Reviewer_g6KK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14839/Reviewer_g6KK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987854851, "cdate": 1761987854851, "tmdate": 1762925191015, "mdate": 1762925191015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}