{"id": "Jhx82FkPXe", "number": 18778, "cdate": 1758290801729, "mdate": 1759897081610, "content": {"title": "Decoding Large Language Diffusion Models with Foreseeing Movement", "abstract": "Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.", "tldr": "In this paper, we propose FDM (Foreseeing Decoding Method) to more accurately decode outputs of large language diffusion models and FDM-A for the acceleration.", "keywords": ["Large Language Diffusion Model", "Decoding Strategy", "Decoding Acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/633ba831694ac104ed06ba8b03d0044e7f7be32c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to decode text diffusion models by balancing the global confidence $p(x_t | q)$ and local confidence $p(x_t | q, x_{t-1})$. Tokens are sampled by top-K most confident from the local confidence, above a given threshold, then reranking with product of local and global confidence. They find that at early stages of sampling, it's helpful to fall back to global confidence if local confidence is not high. At later stages of sampling, local confidence is reliable. Results on a few math, coding, and reasoning benchmarks demonstrate that the method both improves accuracy and also offers speedups."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The results seem reasonable, achieving a good accuracy and speed via a seemingly simple method."}, "weaknesses": {"value": "The writing is confusing. See questions for typos. I believe the method is simple: consider the top-K tokens at each position that are unmasked with a high-enough probability under reverse model. Then rerank the tokens at each position using the product of the reverse model and $p(x_t|q)$. Please explain if this is incorrect.\n\nAdditionally, I find the use of $p(x_t|q)$ to be unjustified. The paragraph before equation 7 does not motivate equation 7, and equation 7 is probably not a good approximation of $p(x_T|q,x_t)$."}, "questions": {"value": "1. Equation 2 has typos. This should be the same as equation 5 right?\n2. Line 167 in Algorithm 1: Should this be for xt in Candidate?\n3. Can you propose a rigorous hypothesis on why accuracy gets worse with larger K in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kdo4XFzs1o", "forum": "Jhx82FkPXe", "replyto": "Jhx82FkPXe", "signatures": ["ICLR.cc/2026/Conference/Submission18778/Reviewer_nFF5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18778/Reviewer_nFF5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969135855, "cdate": 1761969135855, "tmdate": 1762999999427, "mdate": 1762999999427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Foreseeing Decoding Method (FDM) for diffusion LLMs, which ranks candidate unmasking actions by combining local confidence (current-step token uncertainty) with an estimate of global confidence (future impact) derived from the modelâ€™s training objective; a width-K \"foreseeing\" search and threshold $\\gamma$ keep compute manageable. An accelerated variant, FDM-A, adaptively switches between exploration (FDM) and fast local decoding using stage thresholds to trade off quality and speed. Across GSM8K, ARC, HumanEval, and Countdown on several LLDMs (LLaDA, LLaDA-1.5, LLaDA-MoE, MMaDA-MixCoT), FDM shows improvements over heuristic decoding methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a reasonable and interesting approach for improving the decoding ability of discrete diffusion models, by combining a search-based strategy that considers longer-term effects.\n- This paper further proposes an accelerated version that restricts exploration to critical steps, significantly saving computational cost.\n- FDM and FDM-A both show performance improvement on standard benchmarks, with FDM-A also demonstrating consistent speedups"}, "weaknesses": {"value": "- More proofreading is needed. There seem to be quite a few typos/mistakes in the writing, which caused a lot of confusion while reading. For example:\n    - In Eq (1), I suppose the decomposition should be: $p(x\\_0)\\prod \\_{t=1}^Tp(x\\_t \\| q, x\\_{0:t-1})$. This also affects subsequent equations\n    - The paper says in Section 4.1 that \"we also incorporate a dynamic pruning strategy that retains only candidate tokens whose confidence exceeds the predefined threshold $\\gamma$\", but in Algorithm 1 line 167-170, candidates with confidence larger than $\\gamma$ are instead removed\n- The proposed method introduces many new heuristic hyperparameters (e.g., thresholds, search width, stage divisions), making it less practical. While the paper shows some ablation study results in Section 5.2, they are still somewhat limited. Questions remain: How are the stage division coefficients determined? Why not consider $n$ (the number of foreseeing steps) larger than 1? How well can the hyperparameters found on one dataset transfer to other model-dataset combinations?"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vLShSMBjV1", "forum": "Jhx82FkPXe", "replyto": "Jhx82FkPXe", "signatures": ["ICLR.cc/2026/Conference/Submission18778/Reviewer_PvZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18778/Reviewer_PvZV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184350177, "cdate": 1762184350177, "tmdate": 1762999999419, "mdate": 1762999999419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Foreseeing Decoding Method (FDM) and its accelerated variant FDM-A to improve inference in Large Language Diffusion Models (LLDMs). The key idea is to combine local confidence (token-level certainty) and global confidence (future impact of current decoding) when determining decoding order. FDM integrates these two components via a search-based strategy, while FDM-A adaptively applies deep exploration only when necessary, achieving a strong trade-off between performance and efficiency. Experiments across multiple benchmarks show that FDM and FDM-A outperform existing heuristic and dynamic decoding approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies a critical issue in LLDM decoding: sensitivity to token order and proposes a principled solution.\n2. The accelerated variant (FDM-A) is well designed and shows impressive efficiency gains without sacrificing accuracy.\n3. Extensive experiments across benchmarks (GSM8K, HumanEval, ARC, Countdown) validate both scalability and effectiveness."}, "weaknesses": {"value": "1. The proposed method introduces several additional hyperparameters (e.g., $\\eta$, $K$, $n$, $\\gamma$), which may be difficult to tune in real-world applications. It would be helpful to discuss their sensitivity and provide guidelines or heuristics for practical tuning.\n2. Equations (7) and (8) should be explained in more detail, particularly regarding their derivation and intuitive interpretation.\n3. Since Equations (4), (7), and (8) involve approximations, it would strengthen the paper to include a discussion on the possible upper and lower bounds of the resulting errors or their theoretical implications.\n4. In line 239, the deeper analysis and observation should be explained further to make the conclusions more convincing and transparent.\n5. It would be useful to clarify why the proposed method was not evaluated using standard SOTA evaluation frameworks such as `lm-evaluation-harness`, which would enhance reproducibility and comparability with prior work.\n\n\n## Minor Issues:\n1. In Algorithm 1, the variable `list` is used but not introduced or defined."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q7THodwODA", "forum": "Jhx82FkPXe", "replyto": "Jhx82FkPXe", "signatures": ["ICLR.cc/2026/Conference/Submission18778/Reviewer_GjSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18778/Reviewer_GjSM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227883365, "cdate": 1762227883365, "tmdate": 1762999999647, "mdate": 1762999999647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}