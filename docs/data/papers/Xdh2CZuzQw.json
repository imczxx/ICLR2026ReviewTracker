{"id": "Xdh2CZuzQw", "number": 19300, "cdate": 1758295180042, "mdate": 1763751499730, "content": {"title": "Beyond the Final Layer: Attentive Multi-Layer Fusion for Vision Transformers", "abstract": "With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task-aware approach for unlocking their potential in probing-based adaptation.", "tldr": "Intermediate layer fusion with attention outperforms standard probing by capturing complementary features across the network hierarchy.", "keywords": ["Probing", "Vision Transformers", "Intermediate layer fusion", "Transfer Learning", "Representation Learning", "Parameter efficiency"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e8047508aaa96f3330bca413b57092739308764.pdf", "supplementary_material": "/attachment/5d9f070012426a63a6df00198792beb820fdf6b9.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies probing-based adaptation, proposes a task-dependent attentive probing method on Vision Transformers (ViTs) to achieve better downstream task performance compared to the linear probing baseline. The key is to utilize intermediate layer representations (including CLS and average-pooled token embeddings), aiming at extracting complementary information therein."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and is easy to follow.\n- The experiment coverage is large, over 19 datasets and three model families.\n- As claimed in the abstract and introduction, utilizing intermediate layer representation consistently outperforms the linear probing baseline by a substantial margin.\n- The additional module scales only in the number of layers, not in the number of image patches, which remains reasonable even for high-resolution inputs and large base models."}, "weaknesses": {"value": "- The main weakness is the practicality of the proposal. Though the method beats linear probing, its performance is on par with attentive probing on all last-layer tokens (AAT). No significant gain is observed when comparing with this baseline, and thus raises the question of why and when a user should switch to the proposal rather than probing use only the last layer token representations.\n\n- This fact also questions the *motivation* of the proposal from the paper, where one would *expect benefits in some specialized domains, such as satellite imagery or medical images, due to the necessity of low-level structural cues contained in earlier layer representations*. From Table 1, (1) the benefits of the proposal are predominantly on tasks where “baseline accuracy is near saturation” (Line 362), while (2) on specialized / structured tasks, it does not always beat AAT. Both points contradict the motivation directly. With the additional result that on natural single domain tasks, the proposal usually performs worse than AAT, they together raise questions about the scope of the proposal on when it is practically useful, and further question the usefulness of intermediate layers."}, "questions": {"value": "- In Section 4.2 (line 302), the proposal is described as having “markedly less variance” compared to AAT. However, from the figure, the var looks basically the same as AAT.\n\n- In Section 4.3 (line 407), it is claimed that “the benefits are greatest for datasets outside the pretraining domain, where the CLS token alone often proves to be insufficient”. However, in the table, consistent gains are obtained on natural mult-domain tasks, which brings a direct contradiction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "198cn1HQ12", "forum": "Xdh2CZuzQw", "replyto": "Xdh2CZuzQw", "signatures": ["ICLR.cc/2026/Conference/Submission19300/Reviewer_FHUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19300/Reviewer_FHUv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580797297, "cdate": 1761580797297, "tmdate": 1762931253064, "mdate": 1762931253064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the question: “Can we do better than probing only the last layer of a frozen ViT?” The authors argue that task-relevant information is distributed across the depth of a ViT and not fully captured by the final CLS token. They therefore propose an attentive multi-layer fusion probe that, for each downstream task, attends over CLS and average-pooled (AP) tokens extracted from all transformer layers and learns to weight them via a lightweight cross-attention module. This keeps the backbone frozen and trains only the probe. Evaluated on 19 diverse datasets and three model families (CLIP, DINOv2, supervised ViTs), the method consistently improves over standard last-layer linear probing and is more stable than attentive pooling over all last-layer tokens (CLS + AP). The analysis of the learned attention maps shows that tasks close to pre-training focus on later layers, whereas domain-shifted/specialized tasks draw more from intermediate AP tokens, supporting the paper’s main claim about the usefulness of intermediate representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Timely and relevant problem.** Attentive probing is becoming increasingly important as full/PEFT fine-tuning gets more expensive and also changes the backbone; probing instead tells us what the pre-trained model already knows. Positioning the paper as “probing as evaluation of representational potential” is spot on.\n2. **Clear, well-written, easy to follow.** The method is essentially “extend attentive probing from one layer to all layers, over CLS+AP,” but the authors explain it cleanly, with a good schematic (Fig. 1) and a nice notation for layer subsets (last, mid+last, quarterly, all).\n3. **Broad experimental sweep.** 19 datasets, three major pre-training paradigms (CLIP / DINOv2 / supervised), and small–base–large variants. This gives the reader confidence that the effect is not cherry-picked.\n4. **Good reproducibility.** Code + detailed appendix are promised and the paper already contains enough detail to reimplement the probe. \n5. **Insightful analysis sections (Sec. 4.4–4.5)**. The heatmap analysis that shows when intermediate layers are attended to (domain shift, structural/satellite/medical tasks) and what token types matter (AP used across more layers than CLS) is genuinely informative — I have not seen this exact “which layers matter for which kind of shift” angle before. This is a real contribution to our understanding of probing, not just to accuracy"}, "weaknesses": {"value": "Major:\n\n1. **Not the first multi-layer attentive probing**. The paper presents the idea as if “attentive probing = last layer” and “we are the first to go across layers.” But there is earlier work (e.g. Psomas et al., 2025) that already applies attentive probing independently to multiple layers of an encoder and reports that attentive probes beat linear probes at basically every depth. What seems novel here is specifically the joint, cross-attentive fusion of CLS+AP across all layers. That narrower novelty should be stated more explicitly.\n2. **Gains over the strongest attentive baseline are small/inconsistent**. The central technical claim is “attend over all layers > attend over last layer.” But looking at the exact numbers (Figure 5): for CLIP models the mean gain is ≈1 pp, for DINOv2 it can even go slightly down, and for supervised ViTs it is roughly on par. This is not the kind of margin that kills alternative designs — it says “this is a nice refinement,” not “this is the new default.” So the paper somewhat *overstates the strength of the result*.\n3. **Single attentive baseline (CAE - Chen et al., 2024)**. The whole paper is built around one attentive pooling choice (the CAE-style cross attention). Yet the attentive-probing literature has already several flavours (AIM [1], EP (Psomas et al., 2025) / multi-query attentive pooling, heavy V-JEPA-style [2] pooling). Since the core contribution here is “multi-layer attentive fusion,” the natural question is: does this generalise to other attentive poolings? If yes, show 1–2 of them; if not, at least discuss why. Right now the reader can wonder whether the results are a quirk of CAE.\n4. **AIM and CAE are very close**. AIM [1] and the employed CAE-style probing both rely on a learned query and effectively absorb W_Q. The paper could likely drop W_Q and simplify the probe, or at least comment on this — especially since one of the selling points of probing is being lightweight.\n5. **Missing experiment that would strengthen the story**. A simple table/plot with “independent linear probe per layer, CLS vs AP,” followed by “independent CAE probe per layer” would vividly show why we need this layer fusion and whether the proposed layer-fusion performance is an upper bound when compared with independently linear (CLS or AP) or attentive (CAE) probing performance.\n6. **Backbone diversity could strengthen the story.** All results are on CLIP / DINOv2 / supervised ViT. But masked-image-modeling or generative models (e.g. DiT) are the types of models probing papers ([3]) report large attentive-probe gains on — adding one of these could give the paper the extra empirical “punch” it currently lacks. \n\n[1] El-Nouby, Alaaeldin, et al. \"Scalable pre-training of large autoregressive image models.\" arXiv preprint arXiv:2401.08541 (2024).  \n[2] Bardes, Adrien, et al. \"Revisiting feature prediction for learning visual representations from video.\" arXiv preprint arXiv:2404.08471 (2024).    \n[3] Przewięźlikowski, Marcin, et al. \"Beyond [cls]: Exploring the true potential of Masked Image Modeling representations.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.   \n\nMinor:\n1. Metric choice obscures the effect. Most plots are in “absolute accuracy gain over last-layer CLS linear probe.” This is fine for the headline (“probing deeper helps”), but it becomes confusing once we want to compare attentive-vs-attentive or last-layer attentive vs multi-layer attentive.\n2. GAP (global average pooling) might be better/more commonly used than AP."}, "questions": {"value": "1. Related to weakness 3: Can you run the exact same multi-layer fusion but with EP (Psomas et al., 2025) or AIM [1] as the per-layer attentive block to show that the gains are not CAE-specific?\n2. Related to weakness 4: In your setting, could you remove W_Q (as AIM effectively does) without hurting performance? If not, why is your query different?\n3. Related to weakness 5: Could you add this experiment + discussion about it. \n4. Related to weakness 6: Could you add a MIM backbone (e.g. MAE or CAPI)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FmHLBDkwc7", "forum": "Xdh2CZuzQw", "replyto": "Xdh2CZuzQw", "signatures": ["ICLR.cc/2026/Conference/Submission19300/Reviewer_XzCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19300/Reviewer_XzCg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870472286, "cdate": 1761870472286, "tmdate": 1762931252407, "mdate": 1762931252407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of transferring large-scale vision foundation models to downstreams tasks. The authors find that linear probing is restricted to last-layer representations which limits the performance as task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To address the issue, the authors propose an attentive probing mechanism that learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions to dynamically fuses representations from all layers of a Vision Transformer. Empirical results on multiple datasets and pretrained foundation models demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are extensive. It is appreciated that the authors have conduct experiments on different foundation models and various fine-grained classification datasets.\n2. The writing is clear and the paper is easy to follow. The overall structure is well organized and the idea is presented in a coherent manner."}, "weaknesses": {"value": "1. The baseline of the paper is inappropriate. The authors seems to only compare the proposed method with linear probing, which is problematic as linear probing itself is more like an evaluation protocol to test the generalization ability of foundation models. In other words, linear probing can hardly be regarded as a transfer learning method which aims to persuit state-of-the-art performance. In the transfer learning context, better baselines could be full fine-tuning and efficient fine-tuning methods like LoRA.\n2. The proposed method seems heuristic as the authors seem to leverage different settings for different experiments which means the proposed method does not have a general solution which could work for different test cases. It is suggested to develop the method in a systematic way.\n3. While the authors have provided abundant results on fine-grained classification, additional results on ImageNet-1K experiments and dense prediction tasks like semantic segmentation would help to better understand the behavior of the proposed method."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xwa3T4DdFg", "forum": "Xdh2CZuzQw", "replyto": "Xdh2CZuzQw", "signatures": ["ICLR.cc/2026/Conference/Submission19300/Reviewer_5xNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19300/Reviewer_5xNU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974844515, "cdate": 1761974844515, "tmdate": 1762931251574, "mdate": 1762931251574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method to aggregate representative tokens (like [CLS] from multiple layers. The authors claim that the intermediate tokens would have rich information relevant to downstream tasks based on the previous researches. The authors propose an attentive probing method and represent performance gain on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The evaluation is broad and through.\n2. Incorporating AP token is interesting.\n3. The attentive mechanism shows meaningful improvement over linear model."}, "weaknesses": {"value": "1. The model size (parameters of attentive probing) is huge compared to linear model. Also, it will increase quadratically  with base model's depth.\n2. For fine graned tasks, all token from the last layer performs better."}, "questions": {"value": "1. How can one find whether  attentive probing performs better than all tokens method? Is applying and evaluation the only method for real applications?\n2. How to overcome the potential overfitting problem?\n3. Not in cases, the overhead of attentive probing over linear model is not justifiable. How can one decide which method would be better fit considering the overhead and risk of overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HcdRDRVeYx", "forum": "Xdh2CZuzQw", "replyto": "Xdh2CZuzQw", "signatures": ["ICLR.cc/2026/Conference/Submission19300/Reviewer_MGWs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19300/Reviewer_MGWs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994737456, "cdate": 1761994737456, "tmdate": 1762931251071, "mdate": 1762931251071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their careful reading of the paper and for many constructive comments. The reviews unanimously highlight three key strengths:\n- **Relevance and Contribution:** Reviewers recognized the work as \"timely and relevant\" (XzCg) and found the incorporation of AP tokens \"interesting\" (MGWs), noting that the method yields \"meaningful improvement\" (MGWs) and \"consistently outperforms the linear probing baseline\" (FHUv).\n- **Thorough Evaluation:** Reviewers commended the experimental rigor, describing it as \"broad and thorough\" (MGWs), \"extensive\" (5xNU), and a \"broad experimental sweep\" (XzCg) that covers \"large experiment coverage\" (FHUv).\n- **Clarity:** The manuscript was praised as \"clear, well-written\" (XzCg), \"easy to follow\" (FHUv), and presented in a \"coherent manner\" (5xNU).\n\nWe are grateful for this consensus. In this general response, we address the shared observation regarding the All-Tokens (AAT) baseline to clarify the specific scope and utility of our method. We agree with the observation that attending to all tokens in the last layer (AAT) can outperform our multi-layer probe on specific fine-grained datasets. However, our goal is not to propose a specialized solver for fine-grained recognition, but to provide an **adaptive probing strategy that harnesses the additional information contained in intermediate features** and is robust across any unknown downstream task. The empirical evidence in Table 1 demonstrates that our Multi-Layer Fusion is the superior choice for general application:\n- **Best Mean Rank (1.45 vs. 2.74)**: Across all 20 datasets and 9 models, our probe achieves the best mean rank (1.45) among all methods. In contrast, AAT ranks substantially lower (2.74) due to its instability.\n- **Universal Improvement (20/20)**: Our method yields positive gains over the standard linear probe on every single dataset. Conversely, AAT exhibits high variance: while it peaks on texture-heavy tasks, it suffers from negative transfer on others (e.g., Country-211, PASCAL VOC), performing worse than a simple linear probe.\n- **Interpretable Attention Heatmaps**: The learned query token exposes which layers each task relies on, providing a transparent view of how downstream tasks tap into the model’s representational hierarchy.\n- **Computational Complexity**: Our method scales with network depth ($\\mathcal{O}(|L|^2)$), while AAT scales with patch tokens ($\\mathcal{O}(P^2)$). Since $|L| \\ll P$ (typically $|L|=12$ vs. $P \\approx 200$), our approach is order of magnitude more efficient, making it a more feasible attention mechanism for high-resolution applications where $P$ is large.\n\n### **Revisions/Additions during rebuttal**  \nare visible in **blue ink the updated manuscript**: \n- **ImageNet-1K** (Table 1): We added ImageNet-1K results for each setting. \n- **Additional experiments with Masked Autoencoder** (Appx. A.6): We evaluated Masked Autoencoder (MAE), which trains via patch reconstruction without CLS tokens in its loss. While AAT performs best, our Multi-Layer fusion still is second-best and achieves $22.4-24.7$ [pp] gains over last-layer probes, demonstrating that spatial and hierarchical aggregation are orthogonal. Our method (currently) targets CLS-focused models (CLIP, DINOv2, supervised ViTs).\n- **Comparing per-layer linear and attentive probe performance** (Appx. A.8): Per-layer comparison confirms that intermediate layers contain complementary information and additionally reveals that they encode rich spatial information distributed across patch tokens, which cannot be fully captured by summary tokens alone.\n- **Finetuning comparison** (Appx. A.13): While Finetuning is more powerful, our method approaches its performance with 36 times lower training time.  \n- **Multi-layer fusion across attention probe architectures** (Appx. A.14): Four alternative attention architectures all benefit from multi-layer fusion, confirming robustness to probe design.\n\n**Conclusion:** Our work introduces a **simple adaptive approach to utilize complementary task-relevant information hidden in intermediate layers**. Hierarchical fusion consistently improves over standard probing (20/20 datasets) while providing interpretable insights into which layers matter for different domains. Our method offers greater stability than AAT across diverse tasks and scales efficiently with depth rather than resolution, making it robust when downstream task characteristics are unknown."}}, "id": "rSuKPN0fsQ", "forum": "Xdh2CZuzQw", "replyto": "Xdh2CZuzQw", "signatures": ["ICLR.cc/2026/Conference/Submission19300/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19300/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission19300/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763751926212, "cdate": 1763751926212, "tmdate": 1763751926212, "mdate": 1763751926212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}