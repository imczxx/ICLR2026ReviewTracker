{"id": "k3qeF8tBfm", "number": 19147, "cdate": 1758293855580, "mdate": 1763704482389, "content": {"title": "Unbiased Gradient Low-Rank Projection", "abstract": "Memory-efficient optimization is critical for training increasingly large language models (LLMs). A popular strategy involves gradient low-rank projection, storing only the projected optimizer states, with GaLore being a representative example. However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. Aiming to tackle this problem, this paper investigates the layerwise sampling technique for debiasing low-rank projection mechanisms. In particular, an instantiation of the paradigm gives rise to a novel and unbiased low-rank optimization method built upon GaLore's mechanism and the Muon algorithm, named GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the convergence guarantees of the base Muon algorithm while preserving the memory efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and pretraining also demonstrate non-trivial improvements over GaLore and even better performance than full-parameter training, validating the method's effectiveness. Further investigation shows that the improvement of this technique comes from a more uniform distribution of knowledge inside layers, leading to more efficient utilization of the model parameter space and better memorization.", "tldr": "", "keywords": ["Optimization", "Memory-Efficient Training", "Memorization Ability"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b0715e9016896fe3d31e6571689695a8d10e2ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GUM, a new low-rank optimization algorithm aiming to achieve unbiased gradient updates for memory-efficient LLM training. GUM combines the low-rank gradient projection mechanism of GaLore with the Muon optimizer, augmented by a layerwise-sampling debiasing technique. The key idea is to probabilistically perform full-rank updates on a subset of layers, ensuring that the overall gradient estimation remains unbiased in expectation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a new approach to mitigating the bias introduced by low-rank projections through layerwise sampling. This idea extends beyond GaLore and establishes a general theoretical analysis for constructing unbiased low-rank optimizers.\n\n - The authors provide a formal convergence theorem (Theorem 1) under non-convex settings, showing that GUM retains the same convergence rate as Muon. The theoretical derivations are logical and clear."}, "weaknesses": {"value": "- W1. Unbiased gradient statement as key contribution is overstated. This property requires taking the expectation over stochastic sampling. Moreover, as the full gradient evolves along the training trajectory, it is unclear to which the step of the full gradient the unbiasedness claim refers. Therefore, the claim of unbiasedness appears to be overstated and lacks rigorous justification.\n\n - W2. According to Theorem 1, the optimal choice of q  is 0.5, which coincides with the hyperparameter used in the experiments (Line 290). However, using a constant q implies that the full-rank updates dominate the space complexity, matching that of full fine-tuning. This significantly limits the contribution of the work. It would be more meaningful to design an algorithm with a diminishing q, aligning better with the paper’s goal of memory-efficient optimization.\n\n - W3. In Section 5.1, the authors do not use q = 0.5; instead, they only update two layers with full-rank gradients. This further indicates that the proposed method is not an unbiased gradient estimator for low-rank projection under efficient memory constraints, undermining the claimed theoretical properties.\n\n - W4. Insufficient training epochs for convergence. For the 8B-parameter model with rank = 512, the reported number of epochs appears insufficient for convergence. The authors are encouraged to extend training to at least 5 epochs to provide a more convincing empirical evaluation."}, "questions": {"value": "- Since the authors position GoLore as the closest approach to building an unbiased algorithm, the absence of direct empirical comparison makes the conclusions about landscape properties and convergence rates unconvincing.\n - What is the key innovation in using the optimizer Muon for low-rank adaptation during fine-tuning in this work? The paper does not clearly state why this optimizer provides unique benefits compared to standard choices (e.g., AdamW)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XrhqNocmIM", "forum": "k3qeF8tBfm", "replyto": "k3qeF8tBfm", "signatures": ["ICLR.cc/2026/Conference/Submission19147/Reviewer_SdfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19147/Reviewer_SdfL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674502433, "cdate": 1761674502433, "tmdate": 1762931162464, "mdate": 1762931162464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors have introduced GaLore Unbiased with Muon (GUM). It is a new memory efficient algorithm for LLM pretraining and fine-tuning. It is a follow-up work of Galore, where Galore projects gradients into a low-rank subspace to reduce the memory cost. However, one limitation of Galore is that it suffers from a biased gradient estimate. In this paper, the authors address this problem using the layerwise sampling debiasing technique. It performs full-rank updates on some layers and low-rank rank updates on the remaining ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper can be summarized as follows:\n\n1. Memory efficiency of LLMs is a very important and popular area of research. The biased gradient updates in low-rank optimizer is a well-known issue. Addressing this is interesting to the machine learning community.\n\n2. The authors have given a rigorous proof for the unbiasedness and convergence. As far as I can see, all the proofs look correct to me.\n\n3. The authors have conducted experiments on both fine-tuning and pre-training on multiple LLMs."}, "weaknesses": {"value": "The weaknesses of this paper are summarized as follows:\n\n1. This paper does not have an ablation study. It would be better to test by changing the sampling probability and projection ranks.\n\n2. Also, the model pretraining mainly focuses on the small models, such as LLaMA-60M, LLaMA-130M, and LLaMA-350M. \n\n3. The improvements are from 0.3% to 1.1%. These improvements are relatively small. The authors may consider showing whether or not these margins are consistent across longer training or larger models."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eVr6UTaGYE", "forum": "k3qeF8tBfm", "replyto": "k3qeF8tBfm", "signatures": ["ICLR.cc/2026/Conference/Submission19147/Reviewer_9aRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19147/Reviewer_9aRW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698709044, "cdate": 1761698709044, "tmdate": 1762931161889, "mdate": 1762931161889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a critical flaw in popular memory efficient optimizers like GaLore: their low-rank gradient projection mechanisms are biased, leading to performance gaps and a lack of convergence guarantees. To solve this, the authors propose a debiasing technique based on layer-wise sampling. Their method, GUM, randomly samples a few layers to perform a compensating full-rank update, while all other layers perform a scaled low-rank update. This ensures the gradient estimate is unbiased in expectation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical Guarantees: It proves that GUM is an unbiased estimator and, as a result, matches the convergence guarantees of its base optimizer (Muon). This directly addresses a major theoretical weakness in GaLore. The synthetic experiment in Figure 1, where GaLore fails to converge but GUM succeeds, provides a stark practical example of this theoretical advantage.\n\n2. Analysis: The paper provides a plausible explanation for GUM's success, linking its high rank updates to a higher stable rank and a more uniform singular value distribution in the model weights, which implies better knowledge distribution and memorization.\n\n3. Performance: GUM consistently outperforms the GaLore baseline in extensive experiments. Fine tuning: On 7B-9B scale models, GUM shows clear improvements over GaLore on both instruction following (IFEval) and reasoning (GSM8K) tasks. Pretraining: On LLaMA models up to 350M, GUM outperforms GaLore on commonsense reasoning benchmarks."}, "weaknesses": {"value": "1. Limited pre-training scale. The pre-training results, while promising, are on relatively small models (up to 350M). Demonstrating the performance win over full rank AdamW on the larger 8B scale models would make the claims even more conclusive.\n\n2. The method introduces new hyperparameters, namely the sampling probability $\\gamma$ (or number of full rank layers) and the sampling period $K$. The authors rightly note in their limitations that this sampling can introduce high variance, which leads to instability and requires more careful tuning."}, "questions": {"value": "1. You admit in your limitations that GUM's sampling \"introduces high variance,\" \"leads to instability,\" and \"requires more careful tuning\". Why is optimizer that's hard to tune a practical improvement over GaLore?\n2. In Fig. 4, with (K=200) projector refreshes, the residual $\\chi_t$ exceeds ~60–80% within fewer than 20 iterations after each refresh, suggesting severe projector staleness rather than inherent bias. Could this be largely fixed by more frequent refresh? Please report GaLore with smaller (K) (e.g., (K=20)) under matched compute and compare against GUM (quality vs. wall clock/FLOPs).\n3. Please report per step FLOPs, wall clock to a fixed validation metric, and SVD costs (including workspace) for GaLore vs. GUM across the same hardware. Provide time vs quality curves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HPm7kvmI0S", "forum": "k3qeF8tBfm", "replyto": "k3qeF8tBfm", "signatures": ["ICLR.cc/2026/Conference/Submission19147/Reviewer_QBWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19147/Reviewer_QBWt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950859185, "cdate": 1761950859185, "tmdate": 1762931160755, "mdate": 1762931160755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GaLore Unbiased with Muon (GUM), a memory-efficient optimization method for training large language models. GUM debiases low-rank training using layerwise sampling. While prior methods such as GaLore reduce optimizer state memory by projecting gradients into a low-rank subspace, they introduce inherent optimization bias and lack convergence guarantees, leading to degraded or unstable performance, especially under noisy gradients. GUM resolves this by integrating layerwise unbiased sampling with the Muon optimizer, periodically applying full-rank updates to mathematically cancel projection bias in expectation while retaining low-rank efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method highlights and shows an important limitation in GaLore-style low-rank PEFT methods - that most low-rank optimization methods introduce biased gradient estimations during training. This paper also rightly highlights the challenges in analyzing the convergence properties of such methods.\n2. Theoretical Contribution and convergence analysis of GUM is an important contribution. The motivating example to show why GaLore might fail in an extremely noisy setting highlights an important issue."}, "weaknesses": {"value": "1. Some ablations on the probability of full-rank updates, sampling period, or rank of low-rank updates could be insightful.\n2. Inherent issues with Muon as an optimizer haven’t been fixed or addressed \n- i) for large hidden layers, computational overhead from Newton-Schulz Updates would be significant; \n- ii) Muon has only been studied for dense hidden linear layers and stability and efficiency will degrade in a sparse training regime;\n- iii) Although Muon improves conditioning without second moments, storing intermediate matrices and running iterative updates might still use more memory than other optimizers. \n- iv) how does Muon perform for larger LLMs (30B/70B scale)?\n\n3. Experiments lack a performance comparison to other low-rank SOTA methods, especially the ones that could be potentially unbiased due to their recovery, random selection, or error feedback mechanisms, like: 1) LDAdam [1], 2) APOLLO [2], 3) GreedeLore [3] 4) FRUGAL [4] 5) SubTrack++ [5], etc.\n- i) Pre-training experiments are conducted on very small models, and can not be used to show the effectiveness of proposed methods in larger (7B+), and longer training regimes.\n- ii) Although Fira is included in pre-training baselines as one potentially unbiased low-rank method; it is not SOTA anymore, and methods like LDAdam [1], APOLLO [2], or SubTrack++ [5] has been shown to surpass its performance. \n- iii) None of the mentioned unbiased low-rank techniques are included in fine-tuning experiments for a fair comparison and thorough evaluation of the proposed method. \n\n [1] Robert et al., 2025. LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics.\n\n [2] Zhu et al., 2025. APOLLO: SGD-like Memory, AdamW-level Performance.\n\n [3] Chen et al., 2025. Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees.\n\n[4] Zmushko et al., 2024. FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training.\n\n[5] Rajabi et al., 2025. SubTrack++: Gradient Subspace Tracking for Scalable LLM Training."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RvgfL4E8BA", "forum": "k3qeF8tBfm", "replyto": "k3qeF8tBfm", "signatures": ["ICLR.cc/2026/Conference/Submission19147/Reviewer_Et17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19147/Reviewer_Et17"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013343654, "cdate": 1762013343654, "tmdate": 1762931159775, "mdate": 1762931159775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}