{"id": "wKi4Jeqqrb", "number": 14618, "cdate": 1758240180033, "mdate": 1759897359272, "content": {"title": "ReTrace: Reinforcement Learning-Guided Reconstruction Attacks on Machine Unlearning", "abstract": "Machine unlearning has emerged as an inevitable AI mechanism to support GDPR requirements such as revoking user consent through the \"right to be forgotten\". \nHowever, existing approaches often leave residual traces that make them vulnerable to data reconstruction attacks. \nIn this work, we propose ReTrace, the first reconstruction attack framework that uniquely formulates unlearned data recovery on large-scale deep architectures as a reinforcement learning (RL) problem. \nBy treating residual unlearning traces as reward signals, ReTrace guides a generator to actively explore the input space and converge toward the forgotten data distribution. \nThis RL-guided approach enables both instance-level recovery of individual samples and distribution-level reconstruction of unlearned classes. \nWe provide a theoretical foundation showing that the RL objective converges to an exponential-tilted distribution that amplifies forgotten regions. \nEmpirically, ReTrace achieves up to 73.1\\% instance-level recovery and reduces FID and KL scores beyond state-of-the-art baselines, UIA (IEEE S\\&P 2024) and HRec (NeurIPS 2024). \nStrikingly, on the challenging task of text unlearning, it improves BLEU scores by nearly 100\\% over black-box baselines while preserving distributional fidelity, demonstrating that RL can recover even high-dimensional and structured modalities. Furthermore, ReTrace demonstrates effectiveness across both convolutional (ResNet) and transformer-based models, with Distil-BERT as the largest architecture attacked to date. These results show that current unlearning methods remain vulnerable, highlighting the need for robust and provably private mechanisms.", "tldr": "", "keywords": ["Machine Unlearning", "Reinforcement Learning", "Reconstruction Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abe8398bca79027c21c2743d0114dc7e101c6073.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ReTrace, which seeks to reconstruct unlcearned training samples assuming access to the model before and after unlearning. ReTrace uses a reinforcement learning to learn a sample that is likely to minimize the loss functions of the two models before and after unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Understanding and objectively measuring the effectiveness of unlearning is an important problem. The paper presents a novel and interesting use of RL in this space. Overall, I think as a measurement method, there are some benefits to ReTrace as the results seem to suggest that exact unlearning is more effective than approximate unlearning, which is a nice, though somewhat expected result."}, "weaknesses": {"value": "The paper claims to evaluate both ResNet and DistilBERT, but the majorit of results are in the image domain. Average attack success rates is only around 50% for exact unlearning and 60% for approximate unlearning, and worse on the text tasks, so the attack is not that  effective. As an actual attack, ReTrace doesn't seem that realistic: as it assumes that an adversary can access both pre- and post-unlearning models while running a costly RL process. The paper would have been stronger if the authors could have further compared the relative strenghts of different unlearning proposals using ReTrace and used their method to explain some of the differences."}, "questions": {"value": "I found some results confusing: some trace score patterns in Figure 2 and Figure 6 are confusing as the trace score doesn't necessarily correlate with being unldearned or not ‚Äîfor example, (0,0) has a very low trace score despite being unlearned, while other points like (1,0) and (1,1) show high scores even though they are not unlearned. The bottom-right case (4,4) is inconsistent across black-, gray-, and white-box settings -- an explanation would be nice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SLnTtsdWby", "forum": "wKi4Jeqqrb", "replyto": "wKi4Jeqqrb", "signatures": ["ICLR.cc/2026/Conference/Submission14618/Reviewer_tMmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14618/Reviewer_tMmg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849186730, "cdate": 1761849186730, "tmdate": 1762924997286, "mdate": 1762924997286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReTrace, a new reconstruction attack that reinterprets the problem of recovering unlearned data through the lens of reinforcement learning (RL). Instead of relying on static optimization or inversion techniques, ReTrace uses discrepancies between the original and unlearned models as reward signals to guide a pretrained generator toward regions of the input space that likely correspond to forgotten data. The approach integrates multiple trace signals‚Äîchanges in predictions, losses, and gradients‚Äîacross different access levels (black-, grey-, and white-box) and performs reconstruction through RL-guided latent exploration followed by a candidate refinement step. Through both theoretical analysis and experiments on multiple datasets, the paper shows that ReTrace can recover semantically meaningful data at both the instance and distribution levels, highlighting residual vulnerabilities in existing unlearning methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- **Novel and creative formulation**. This paper presents a genuinely original framing of reconstruction attacks. It adapts the RL-GAN-Net idea from Sarmad et al. (2019)‚Äîoriginally proposed for conditional image generation‚Äîand re-purposes it for unlearning data recovery. Using reinforcement learning to guide a generator‚Äôs latent exploration based on unlearning traces is both conceptually interesting and technically innovative.\n\n- **General and modular framework**. The approach unifies different attack settings (black-, grey-, and white-box) under a single reward formulation, where prediction, loss, and gradient discrepancies can be combined or omitted depending on model access. This modularity makes the framework broadly applicable and easy to adapt to new unlearning scenarios.\n\n- **Empirical evidence of residual traces after unlearning**. The experiments clearly demonstrate that models subjected to unlearning still leak identifiable information, confirming the practical relevance of the attack.\n\n- **Largest-scale model targeted to date**. The paper extends reconstruction attacks beyond CNNs to transformer architectures, reporting results on DistilBERT, which the authors state is the largest model attacked to date in the unlearning literature. This demonstrates the framework‚Äôs scalability and suggests that the proposed RL-based formulation might have the potential to generalize to high-dimensional transformer settings.\n\n- **Timely and relevant contribution**. The work addresses the emerging area of machine-unlearning security‚Äîa topic of growing importance for model safety, privacy, and regulatory compliance‚Äîand provides a concrete framework for analyzing these vulnerabilities.\n\n- **Theoretical grounding supporting intuition**. The paper includes a concise theoretical analysis that explains why the proposed RL formulation works, strengthening the intuition behind the method."}, "weaknesses": {"value": "I like this paper ‚Äî it‚Äôs a creative and well-motivated idea. That said, there are a few points I would like to raise and hopefully discuss with the authors.\n\n- **On the ambiguity in the RL formulation**. Section 3.2 introduces the RL framing with definitions of state, action, and policy, but these descriptions are somewhat abstract and internally inconsistent when mapped to the actual image-generation setup. The text defines the action as ‚Äúsampling or refining a candidate x from the generator,‚Äù implying that the policy acts in the data space, while simultaneously describing the policy œÄœï as ‚Äúoutputting candidates from the latent space,‚Äù implying it acts over z. This leaves it unclear whether the RL agent‚Äôs action space is x or z. Appendix D.1 later reveals that œÄœï is in fact a small two-layer MLP producing latent vectors z that are then passed through a pretrained DCGAN G to obtain x = G(z). This architectural detail is crucial for understanding the proposed RL loop but is only specified in the appendix under the Experimental Setup Section. I would strongly recommend that the authors move this clarification into the main text so readers can immediately understand what components are being optimized and how gradients flow.\n\n- **On the mathematical clarity and internal consistency**. While the paper‚Äôs theoretical framing is interesting, I found the mathematical presentation scattered and internally inconsistent. Symbols are introduced but never used or formally defined. For example, T(x) is defined once (Eq. 5) and never referenced again. The trace score s(x), which seems to be the central quantity guiding the policy updates, is described conceptually but never expressed mathematically. In addition, a reward r(x) is defined (Eq. 6); if I understood correctly, it corresponds to ‚àís(x), but this relationship is never made explicit. In Eq. (7), it is also unclear what pœï denotes‚Äîwhether it is simply the policy distribution œÄœï or a learned variant of the prior p0. The term Dpub is briefly defined as ‚Äúa publicly available dataset with a similar distribution,‚Äù but its operational role remains vague. Is Dpub the same dataset used to pretrain the DCGAN generator, or is it only used for the KL regularization term? Clarifying this would help connect the regularized RL objective to the actual implementation.\n\n- **on distribution-level comparison with baselines**. While the paper reports FID and KL metrics for ReTrace across datasets and access levels, it does not provide corresponding values for baseline methods (e.g., UIA, HRec). Since FID and KL are the primary metrics used to evaluate distribution-level reconstruction quality, the lack of direct comparison makes it difficult to assess whether ReTrace actually improves over existing approaches in recovering the overall deleted-data distribution."}, "questions": {"value": "I would appreciate it if you could also answer my questions: \n\n**Q1**. I might have missed it in the paper, but I don‚Äôt fully understand‚Äî as also mentioned in my weakness section‚Äîwhether the optimization in Equation (7) aims to maximize the trace score ùë†(ùë•) or the reward ùëü(ùë•) and what the motivation is for defining both and how they are related.\n\n**Q2**. My understanding is that, according to Equation (9), the instance-level reconstruction step selects a single top-scoring candidate via arg max ùë†(ùë•) and refines that sample. If that is correct, could the authors clarify how the multiple instances shown in Figure 4(a) are reconstructed? A related question: did the refinement step lead to a significant improvement in reconstruction quality?\n\n**Q3**. I noticed that DCGAN is used as the generative model throughout the experiments. Could the authors elaborate on the reasoning behind this choice? Given the many stronger generative models introduced in recent years (e.g., StyleGAN, diffusion models), wouldn‚Äôt using a more advanced generator potentially improve reconstruction quality?\n\n**Q4**. In the appendix, it‚Äôs mentioned that the DCGAN produces 32 √ó 32 images. If I understood correctly, some of your evaluation datasets (e.g., Food-101) are higher-resolution. Could you clarify how this resolution mismatch is handled? In particular, how are comparisons with baselines such as UIA and HRec made‚Äîdo these methods also operate at 32 √ó 32 resolution, or were their outputs downsampled to ensure fairness?\n\nAs I mentioned before, I like the core idea and find it creative and promising. However, I would need the authors to clarify the points raised in the weakness section‚Äîparticularly by adding distribution-level comparisons against baselines‚Äîand address the questions above before I could confidently give this paper a clear accept."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kh3cpyeK89", "forum": "wKi4Jeqqrb", "replyto": "wKi4Jeqqrb", "signatures": ["ICLR.cc/2026/Conference/Submission14618/Reviewer_MuvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14618/Reviewer_MuvD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956315409, "cdate": 1761956315409, "tmdate": 1762924996634, "mdate": 1762924996634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an attack on machine unlearning using RL-based data reconstruction. The work uses residual learnings as rewards and is comprehensively evaluated on several data samples using blackbox access on standard benchmark datasets. The findings indicate the feasibility of performing such attacks on large scale datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- important problem and timely issue\n- well-carried out attack framework without much prior assumptions\n- good comparison on several benchmarks and good theoretical foundation"}, "weaknesses": {"value": "- comparison is limited to a few models and datasets and can easily be expanded broader\n-  Convolution Transpose,instance-wise unlearning,  or Masked Small Gradients methods could have also been studied\n- would have been great to mention computational costs and complexities"}, "questions": {"value": "The paper presents a good study and evaluation of hidden traces and connections leading to success in attacks against unlearning models. The work is well presented, though some of the popular models and approaches for unlearning have not been explored. I wonder if the comparison can benefit from the larger set of models and methods studied in \"Deep Unlearn: Benchmarking Machine Unlearning for Image Classification\" in EuroS&P'25. \n\nWhat defenses can be used to mitigate the attacks mentioned in the paper?\n\nIt would be great to discuss the complexities of the attack and if it will be realistic to carry out against large datasets and bigger models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wq26ewfDGv", "forum": "wKi4Jeqqrb", "replyto": "wKi4Jeqqrb", "signatures": ["ICLR.cc/2026/Conference/Submission14618/Reviewer_gE9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14618/Reviewer_gE9i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120252562, "cdate": 1762120252562, "tmdate": 1762924996227, "mdate": 1762924996227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RETRACE, a reconstruction attack on machine unlearning that frames recovery of deleted data as a reinforcement-learning (RL) problem. The method extracts ‚Äúunlearning traces‚Äù by contrasting a pre-unlearning model  with a post-unlearning model at prediction, loss, and gradient levels, and uses these as rewards to train a generator. The theory claims the RL objective converges to an exponential-tilted policy that emphasizes high-trace regions; experiments report strong instance-level recovery and improved distributional alignment (lower FID/KL) versus UIA and HRec, plus preliminary text results with DistilBERT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new solution based on RL to address a newly proposed threat model against machine unlearning. \n2. The paper considered both instance-level machine unlearning and class-unlearning setting in their approach.\n3. Authors consider three levels of access to the original model and retrained model (black-box, grey-box, and whitebox), and perform most their evaluation on the three settings."}, "weaknesses": {"value": "1. Some notations are used without proper definitions and clarifications. For example in line 141, $x_{\\pi_{(j)}} is not defined.\n2. Some of the details of the proposed approach are missing, which makes it confusing for the reader. What is the initial $z$ that you use in PPO? The details are missing while I think it might affect the outcome. I think the paper would benefit more from a pseudocode instead of figure 1 that does not provide any useful information.\n3. From assumptions 1 and 2 of the paper, it seems authors assume access to the population data of the forget class, which is a more restrictive assumption of access to the population data of the original training set as in prior work [1].\n4. In general, while this paper builds on an earlier paper for its setting [1], I don‚Äôt believe the setting of the attack specifically is specifically relevant to unlearning, but it is more in general about getting two models, one of which is trained on a subset of the training data used for the other, and try to reconstruct that data. So I think it is basically a privacy attack using model differentiation.  That is also why in [1], they only use the retrained model for their evaluations. I believe the assumption about gaining access to the original model in the setting of unlearning is not realistic.\n5. Although the link for the code has been added and reviewers are referred to in the reproducibility statement, the repository was empty and only the score computations method (equation 5) was provided.\n6. Your generative model relies on a DCGAN for generating the data that is missing. However, the pretrained DCGAN data is already trained on the unlearned classes and is capable of generating samples from the unlearned class. Therefore, it wouldn‚Äôt be surprising if DCGAN is capable of generating samples whose MSE from the original forget sample fall within the same range as the variance of the samples of that class. Basically, if I want to rephrase what you point out about the MSE value it would be sth like: you have a sample from the ‚ÄòBed‚Äô class that has been removed from the training set. Then you use your method to generate the image of a sample that can be considered a ‚ÄòBed‚Äô, but not necessarily the same sample that was unlearned. Given that DCGAN is able to generate images of ‚ÄòBed‚Äô achieving that would not be very surprising. I think some of the confusion about this could be resolved once you respond to weakness 2, specially if based on assumption 1 of the paper, the adversary starts with a prior for the unlearned class.\n7. I think the provided figures are not very informative as they are now. For example, looking at figure 8, I don‚Äôt see any significance on the values of the selected squares. Even in figure 2 the advantage of white-box over black-box is not clear from the figure. In white box all the values seem to be larger, not only the forgotten samples.\n8. The results in the table are not accompanied by standard deviations. For example in table 1 the difference in CS score or MSE score for white-box vs grey-box is only at most 0.02, which might simply be smaller than the variance of the data.\n9. Your theoretical results rely on the assumption that the expectation of the score assigned to the samples from the forgotten samples are strictly larger than this expectation for the retained data. However, the score that you defined in equations 2,3, and 4 do not seem to necessarily follow this assumption. For example, if you train a model on the retained data and train a model on the whole data, the loss of these models, should be more similar on the retrained data compared to the forget samples. I think this assumption should be at least accompanied by some empirical observations.\n\n[1] Bertran, M., Tang, S., Kearns, M., Morgenstern, J. H., Roth, A., & Wu, S. Z. (2024). Reconstruction attacks on machine unlearning: Simple models are vulnerable. Advances in Neural Information Processing Systems, 37, 104995-105016."}, "questions": {"value": "1. In the setting of the problem, it is assumed that the adversary has access to the auxiliary public dataset $D_{\\mathrm{pub}}$. However, it is not clear from the paper what the initial $z$ in the experiment is? It has to be clarified what initial $z$ is chosen in the optimization (the initial state). Could the authors please elaborate on the details (with specific examples on the dataset and forgotten sample ideally)?\n2. Have you tested your method on a class that DCGAN has not been trained on? What would happen in that case?\n3. Could the others provide some plots on the number of iterations used in PPO and how the metrics they use (e.g., MSE) changes along this optimization? \n4. Could the authors provide normalized values in figure 2 to show-case the improvement due to more information in the white-box setting. I would suggest computing the average value for all the 25 images in the patch and then reporting the ratio of the values for the red squares over the computed average value for the corresponding patch.\n5. To maximize equation 7, as mentioned in line 214, you need to maximize the reward given in equation 6. For that you need to minimize the differences (due to negative signs). But this would mean samples that the retrained model and original model would act similar on (which would be the retained data) and for example for the retrained data, we would expect equation 2,3, or 4 achieve the smallest values. So why the model should converge to the forget samples. Could the authors please address this confusion I had when reading the approach?\n6. In line 836, you mention exact unlearning method is implemented by ‚Äúfine-tuning the model on the remaining data for the same number of epochs as the original training‚Äù. Why not training the model from scratch instead of fine-tuning the model on the remaining data. In practice the exact unlearning models are derived by fine-tuning the model from scratch because the fine-tuned model still contains information about the forget data and is not equivalent to the retrained model. \n7. Currently the authors only evaluate the effectiveness of their method that rely on SGD update (either gradient descent on remaining samples or gradient ascent on the forget samples). It would be interesting to see the effectiveness of the attack on the two following settings that are shown to be more successful than GA:\n    - Using sparsification methods that only perform SGD updates on a subset of the parameters [1,2].\n    - Unlearning methods that do not rely on SGD on either of the remaining sets and forget sets [3,4].\n\n[1] Jia, J., Liu, J., Ram, P., Yao, Y., Liu, G., Liu, Y., ... & Liu, S. (2023). Model sparsity can simplify machine unlearning. Advances in Neural Information Processing Systems, 36, 51584-51605.\n[2] Fan, C., Liu, J., Zhang, Y., Wong, E., Wei, D., & Liu, S. (2023, October). SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. In The Twelfth International Conference on Learning Representations.\n[3] Chen, M., Gao, W., Liu, G., Peng, K., & Wang, C. (2023). Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7766-7775).\n[4] Ebrahimpour-Boroojeny, A., Sundaram, H., & Chandrasekaran, V. Not All Wrong is Bad: Using Adversarial Examples for Unlearning. In Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8kfUwudoA", "forum": "wKi4Jeqqrb", "replyto": "wKi4Jeqqrb", "signatures": ["ICLR.cc/2026/Conference/Submission14618/Reviewer_FHN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14618/Reviewer_FHN1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216291712, "cdate": 1762216291712, "tmdate": 1762924995681, "mdate": 1762924995681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}