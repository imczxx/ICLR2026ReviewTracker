{"id": "DILQqCQIJ3", "number": 25082, "cdate": 1758363898436, "mdate": 1759896734945, "content": {"title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "abstract": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs --- In particular, we show that direct fine-tuning on traditional non-reflective datasets leads to limited gains. To fully leverage LRMs’ inherent reasoning abilities, we propose **CALM** (Corrective Adaptation with Lightweight Modification), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop **STORM** (Smart Thinking Optimization Reasoning Model), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.", "tldr": "How do you get a 4B model to perform like a 671B giant? Don't force it, guide it. Our CALM framework uses gentle hints to teach a small LRM to think smart, before unleashing its full potential with RL to create STORM.", "keywords": ["Large Reasoning Models", "Tool Use", "Domain Adaptation", "Reasoning Alignment", "Optimization Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13758ce7d709f7d4b538171811b4aba529b326df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CALM (Corrective Adaptation with Lightweight Modification), a framework that refines large reasoning models (LRMs) for optimization modeling (OR) tasks through an automated Reasoner–Intervener collaboration loop. By detecting and correcting reasoning flaws via lightweight interventions, CALM generates high-quality expert trajectories to train STORM, achieving near–DeepSeek-R1 performance with a 4B model on multiple OR benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel self-refining adaptation framework (CALM) that enhances LRM reasoning without relying on large teacher models.\n- The proposed Reasoner–Intervener loop effectively identifies and corrects reasoning flaws, producing high-quality training trajectories with minimal intervention.\n- Experiments on multiple OR benchmarks demonstrate strong performance—STORM (4B) nearly matches the 671B DeepSeek-R1, showing impressive parameter efficiency."}, "weaknesses": {"value": "- The paper does not include a comparison with the typical teacher-generated data paradigm, where large LRMs (e.g., DeepSeek-R1) generate reasoning trajectories to train smaller models. This omission makes it unclear whether CALM’s self-refining pipeline achieves comparable data quality and efficiency to large-model supervision.\n- The experiments use only the Qwen3-4B model, without testing other architectures (e.g., Llama, Phi) or larger scales (8B, 14B). This limits the understanding of CALM’s generality and scalability across different model families and sizes.\n- The paper does not evaluate whether CALM/STORM training affects the model’s original general abilities (e.g., math reasoning, coding, or general instruction following). It remains unclear if domain specialization leads to performance degradation on non-OR tasks."}, "questions": {"value": "- Can the authors provide or discuss a baseline where a large reasoning model (e.g., DeepSeek-R1) generates the training data, to compare against CALM’s self-refinement approach in terms of data quality, performance, and cost?\n- Have the authors tested CALM on larger or different LRM architectures to verify whether its self-refining mechanism generalizes beyond Qwen3-4B?\n- Have the authors examined the impact of CALM training on the base model’s general capabilities (e.g., math, code, GPQA)? Does specialization in optimization modeling cause catastrophic forgetting or trade-offs in other domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xFin9kj6y", "forum": "DILQqCQIJ3", "replyto": "DILQqCQIJ3", "signatures": ["ICLR.cc/2026/Conference/Submission25082/Reviewer_DQtT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25082/Reviewer_DQtT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758667764, "cdate": 1761758667764, "tmdate": 1762943318492, "mdate": 1762943318492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets optimization modeling (OR tasks like LP/ILP) as a reflective reasoning problem for large reasoning models (LRMs). The authors observe that standard SFT on non-reflective QA pairs often hurts complex problem performance by discouraging multi-step, code-driven reasoning.\n\nThey propose CALM (Corrective Adaptation with Lightweight Modification): within an executable environment (natural language + Python + solver), an Intervener LLM inserts very short, pinpoint hints into the model’s ongoing reasoning, correcting two pervasive errors: (1) Code/solver underuse or distrust and (2) OR modeling gaps (e.g., LP vs ILP, missing/incorrect constraints, implementation drift). The corrected “gold” traces supervise SFT to calibrate behavior while preserving reflective style. Then RL (GRPO) continues training in the same executable loop, rewarding correct/optimal solutions to reinforce computation-driven reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Treats optimization modeling explicitly as a reflective reasoning problem, avoiding the way standard QA-style SFT suppresses multi-step thought and the code–solver loop.\n\nUses very small, pinpoint hints (few-token edits) to convert flawed chains into “gold” traces—preserving the model’s native reasoning style while correcting errors.\n\nKeeps a full trail from error → micro-hint → fix → solver-verified result, which supports failure analysis and produces instructive, teaching-style exemplars.\n\nValidated across NL4Opt, MAMO-Easy/Complex, IndustryOR, and OptMath; gains are especially strong on the harder subsets, making the claims more persuasive."}, "weaknesses": {"value": "All CALM data synthesis and the two-stage training pipeline start from one base LRM—Qwen3-4B-Thinking-2507—used as the Reasoner, with Gemini-2.5-Pro as the Intervener. The paper does not report adapting multiple base LRMs to test generality of the pipeline.\n\nIn the reproducibility statement, the authors say they plan to release code and models, implying they are not available yet, which limits independent verification and adoption.\n\nThe overall “reflect–feedback–revise with executable code/solver” paradigm has been explored in prior art such as Reflexion and Self-Refine for iterative self-correction, PAL for code-centered reasoning, and the NL4Opt line for NL-to-OR modeling; several MILP auto-formulation works also adopt prompt-/template-based pipelines. Hence, while the paper integrates these ideas neatly, the framework itself is conceptually close to existing paradigms."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKlP1znNiR", "forum": "DILQqCQIJ3", "replyto": "DILQqCQIJ3", "signatures": ["ICLR.cc/2026/Conference/Submission25082/Reviewer_BMgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25082/Reviewer_BMgt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786305534, "cdate": 1761786305534, "tmdate": 1762943317975, "mdate": 1762943317975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to adapt Large Reasoning Models (LRMs) for automated optimization modeling.\n\n The authors identify that conventional instruction-tuning methods - based on static, non-reflective datasets - undermine the multi-step reasoning patterns inherent in LRMs.\n\nThey propose CALM (Corrective Adaptation with Lightweight Modification), a hint-based intervention framework that corrects reasoning flaws while preserving the model’s reflective reasoning flow. Through CALM, the authors curate high-quality reasoning trajectories and use them to fine-tune an LRM, followed by reinforcement learning to produce STORM, a 4B-parameter model that achieves 68.9% average accuracy across five optimization benchmarks - matching the performance of a 671B model.\n\nThe work includes a taxonomy of reasoning flaws, an interpretable correction protocol, and detailed ablation and behavioral analyses showing that CALM’s hint-based adaptation leads to more sample-efficient reinforcement learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Original idea of aligning adaptation with native reasoning instead of retraining from scratch.\n\nMethodological soundness: clear formalization of the reasoning loop and rigorous evaluation.\n\nPractical impact: dramatic performance/parameter efficiency gains (4B → 671B equivalence).\n\nHigh interpretability: the taxonomy and hint examples make the improvement mechanism transparent.\n\nThorough analysis: ablations and behavioral plots (code-usage ratio, response length, flaw frequency) offer unusually strong introspection for this domain.\n\nReproducibility: detailed appendices and explicit commitment to code release."}, "weaknesses": {"value": "Limited dataset size in the CALM curation phase (112 “golden” trajectories) may raise questions about scalability; a brief discussion on automating large-scale generation would strengthen the paper.\n\nDomain scope: experiments are confined to optimization modeling; it would be valuable to show transfer to another structured-reasoning domain (e.g., planning or theorem proving).\n\nComparative baselines: although strong, the study lacks direct comparison to contemporaneous reflective-alignment frameworks like START or CoRT under identical setups.\n\nIntervener dependence: using Gemini-2.5-Pro as the expert may raise reproducibility concerns if closed-source models differ in reasoning behavior.\n\nSome claims of “expert-level performance” might benefit from human expert evaluation rather than relying solely on pass@1 accuracy."}, "questions": {"value": "How sensitive is CALM to the choice of Intervener model? Could a smaller or open-source Intervener reproduce similar quality in the curated trajectories?\n\nThe current pipeline uses a discrete number (≤ 5) of interventions. Have you explored adaptive stopping criteria or uncertainty-driven intervention scheduling?\n\nSince CALM focuses on hint-based reasoning correction, could its framework generalize to non-code-based reasoning tasks (e.g., logical proofs, multi-hop QA)?\n\nIn reinforcement learning, the reward function is binary on final correctness. Have you considered incorporating intermediate reflection-quality rewards to further stabilize training?\n\nFinally, how does STORM perform under zero-shot generalization to unseen industrial problem types not represented in the benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkgYmvrj1O", "forum": "DILQqCQIJ3", "replyto": "DILQqCQIJ3", "signatures": ["ICLR.cc/2026/Conference/Submission25082/Reviewer_k153"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25082/Reviewer_k153"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903888455, "cdate": 1761903888455, "tmdate": 1762943317484, "mdate": 1762943317484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CALM (Corrective Adaptation with Lightweight Modification), a framework for aligning large reasoning models (LRMs) with optimization modeling tasks. The key idea is that simply fine-tuning LRMs on non-reflective datasets — i.e., problem–solution pairs without intermediate reasoning — harms their performance on complex problems by suppressing their native multi-step reasoning ability. To address this, CALM introduces a “Reasoner–Intervener” collaboration pattern where an expert model identifies reasoning flaws and provides minimal corrective hints (fewer than 3% of tokens). The resulting corrected trajectories are used to fine-tune the base LRM, followed by reinforcement learning to achieve full adaptation. The final model, named STORM (Smart Thinking Optimization Reasoning Model), achieves 68.9% average accuracy across five optimization benchmarks, matching the performance of a much larger 671B model while using only 4B parameters. The paper combines an insightful problem diagnosis, a clean methodological design, and strong empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths lie in the depth and coherence of the study. The authors don’t just propose a new method—they build a full story around understanding why current adaptation methods fail, and how to fix that in a principled way. The CALM framework is well-motivated and the “Reasoner–Intervener” pattern is intuitive and scalable. The two-stage training pipeline (SFT then RL) is validated thoroughly, with strong ablation studies that isolate each component’s contribution."}, "weaknesses": {"value": "The weaknesses are relatively minor. The evaluation, while comprehensive, depends heavily on Qwen and Gemini models; testing on a different family (e.g., DeepSeek or Llama) would help show generality. Some parts of the taxonomy (especially the seven triggers) could be streamlined — the paper spends many pages on classification detail that could be summarized. There is also limited theoretical discussion about why minimal interventions produce such a large effect; the explanation is intuitive but mostly empirical. Lastly, while the results are impressive, it’s not entirely clear how much of the performance gain comes from the data filtering versus the interventions themselves."}, "questions": {"value": "How sensitive is the performance of STORM to the 2.6% intervention ratio? Would increasing or decreasing the intervention intensity change the outcome significantly?\n\nCould CALM be generalized to domains beyond optimization modeling, or does it rely heavily on the availability of an executable solver for immediate feedback?\n\nHave the authors verified whether Gemini-2.5 as the Intervener introduces stylistic bias into the reasoning traces?\n\nHow do you ensure that the reinforcement learning stage does not overfit to the benchmarks used for calibration?\n\nIs there a qualitative difference between trajectories produced after CALM fine-tuning and those after RL, beyond shorter length and higher correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "22FnA9qmvL", "forum": "DILQqCQIJ3", "replyto": "DILQqCQIJ3", "signatures": ["ICLR.cc/2026/Conference/Submission25082/Reviewer_MAsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25082/Reviewer_MAsL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762381226526, "cdate": 1762381226526, "tmdate": 1762943317047, "mdate": 1762943317047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}