{"id": "kl8CrfUvw3", "number": 16995, "cdate": 1758271014203, "mdate": 1763127641114, "content": {"title": "Matrix-Driven Detection and Reconstruction of LLM Weight Homology", "abstract": "Recently, concerns about intellectual property in large language models (LLMs) have grown significantly,\nparticularly around the unattributed reuse or replication of model weights.\nHowever, existing methods for detecting LLM weight homology fall short in key areas, including recovering the correspondence between weights and computing significance measures such as $p$-values.\nWe propose Matrix-Driven Instant Review (MDIR), leveraging matrix analysis and Large Deviation Theory.\nMDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on homologous weights without requiring full model inference.\nWe demonstrate that MDIR reliably detects homology even after extensive mutations, such as random permutations and continual pretraining with trillions of tokens.\nMoreover, all detections can be performed on a single consumer PC, making MDIR efficient and accessible.", "tldr": "", "keywords": ["Large Language Models", "Weight Homology", "Similarity Detection", "Large Deviation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc6a8704e7eaa94e1f0fba20972733e199a4f732.pdf", "supplementary_material": "/attachment/3335badda05361b1921947c4a9b73adb9f8fa64c.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MDIR, a method for detecting weight homology between LLMs using matrix analysis and Large Deviation Theory. The approach analyzes weight matrices directly to identify if models share common ancestry through transformations like pruning, fine-tuning, or permutations, providing rigorous p-value estimates without requiring model inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The mathematical framework connecting invariant transformation groups, polar decomposition, and Large Deviation Theory to LLM weight detection appears novel, with detailed theoretical derivations provided in the appendices.\n\n- The method demonstrates interpretability by reconstructing specific transformations (e.g., Figure 8a shows layer correspondence in Llama-3.2-1B pruning), which could provide insights beyond binary homology detection."}, "weaknesses": {"value": "- The theoretical foundation relies on assumptions that may not hold in practice. The method assumes training dynamics preserve G-coordinates under \"idealized conditions (infinite numerical precision and G-invariant optimizers)\" (line 131), but modern training uses Adam/AdamW (not G-invariant) and low-precision formats (fp16/bf16). While the authors note \"up to 1% discrepancy\" between fp32 and fp64 (line 480), there is no systematic analysis of how Adam optimization or quantization affects the invariance assumptions. The claim that \"α remains constant at its initialization value\" (line 134) would benefit from empirical validation across different optimizers, learning rates, and training durations.\n\n- The evaluation of false positive and false negative rates appears limited. The study tests only 25 models primarily from 4 families (Llama, Qwen, DeepSeek, RWKV) without systematic analysis of negative pairs from diverse architectures. The threshold p < 2×10^{-23} is stated but not justified through ROC analysis or comparison with alternative thresholds. The paper would benefit from: (1) testing on models deliberately trained to be similar (e.g., same architecture and data, different seeds) to establish false positive rates, (2) varying the similarity threshold to construct precision-recall curves, and (3) explaining whether the extreme p-values (10^{-104}) reported are necessary or if detection could work with less extreme thresholds.\n\n- The robustness to adversarial evasion is not empirically tested. Section 5 acknowledges \"potential ways of evading detection\" as future work, but the paper only evaluates models not designed to avoid detection. Critical untested scenarios include: (1) deliberately retraining embedding layers with higher learning rates to break the orthogonal relationship, (2) applying non-orthogonal transformations before fine-tuning, (3) adding strategic noise to weights. The claim that the method is \"exceedingly difficult for adversaries to bypass\" (line 71) would be strengthened by adversarial experiments. Additionally, the changed tokenizer case (Section 3.2) assumes shared tokens retain aligned embeddings, which may not hold if embeddings are retrained.\n\n- The experimental comparison with existing methods appears incomplete. Section E.1 compares with REEF on only 2 model pairs through visual inspection, without quantitative metrics (accuracy, F1, precision-recall). The ablation study (Section 4.3) tests only same-seed initialization on two small datasets, but does not systematically vary: (1) amount of training data (would 100B tokens break detection?), (2) different learning rates, (3) continued pretraining duration, (4) mixing weights from multiple models. The computational cost is stated as \"single consumer PC\" but no wall-clock time or memory comparisons with baselines are provided. For the GQA transformation group construction (Section 3.1), the paper states this is a \"sufficient subset\" but does not prove completeness—unexplored transformations could enable evasion.\n\nI will reconsider my score in the rebuttal."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2CF5N5Z5WD", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Reviewer_zuU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Reviewer_zuU1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760495750601, "cdate": 1760495750601, "tmdate": 1762927011722, "mdate": 1762927011722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Added Supplementary Material"}, "comment": {"value": "We apologize for the oversight in not including the supplementary code in the submission.\nNow, we have uploaded supplementary material, including the source code and the full-resolution Figure 5(c), in hope to address some of the concerns.\nAll experiments in the paper are reproducible via the provided code.\nWe are actively conducting additional experiments to address the reviewers’ concerns. Meanwhile, we greatly appreciate any specific suggestions on further experiments that would be most useful. \nWe aim to address each concern raised in the reviews in the next few days."}}, "id": "DJF2eMbdYP", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763128213493, "cdate": 1763128213493, "tmdate": 1763128213493, "mdate": 1763128213493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies model weight 'homology'. In particular, they propose a method to infer whether a model parameterized by theta_a is derived from another model theta_b. Authors consider a broad range of what constitutes 'derivation', including continual pretraining, finetuning, pruning etc (5 in total, and a combination of all). \n\nThey propose a mathematically grounded method called MDIR to compare the similarity between the matrices of parameters, leveraging SVD and polar decomposition. Instead of setting a threshold for this similarity metric, or learning it using some ground truth, they further use Large Deviation Theory (LDT) and random matrix theory to estimate p-values (where the null hypothesis is that both models are not homologous). \n\nAuthors then consider 25 open-source models and use MDIR to estimate whether each combination of these models can be considered homologous with respect to a certain p-value. They find their method to give strong signal for models with a known homology relationship, while giving no signal for independently developed models. \n\nThey further add an ablation experiment studying whether their method picks up on weight initialisation versus training data similarity, confirming that it is the former. \n\nAs a **disclaimer,** I want to clarify that, as a reviewer, my mathematical background likely does not suffice to thoroughly critique or fully appreciate the proposed method in Section 3. I therefore focus most of my review on the motivation, experimental setup, results and conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of identifying model homology is interesting, and the paper proposes a novel way, grounded in mathematics, to address this. \n- The fact that the method allows for the computation of a p-value a priori (no ground truth is needed) is valuable, and remains overlooked by other methods. \n- The ablation experiment in section 4.3 is very nice, and cleanly distinguishes between initialization and training data similarity."}, "weaknesses": {"value": "- Authors do not consider any baselines, including some representation-based methods from prior work. I understand from the introduction that these methods \"generally lack the ability to reconstruct the weight correspondence mapping\", but when it comes to vizualizing the similarity metric as in Figure 3a, these methods could be presented as a valuable baseline. Moreover, to justify the mathematical complexity of the method, some more naive baselines (e.g. norm of the difference between the weights) would further ground the significance of the results. \n- While I find Section 2.2 easy to follow, Section 3 quickly becomes hard to understand. The paper could be appreciated by a wider audience if the most relevant pieces of the method remain in the main body of the paper, while the other pieces could be put in the appendix. On that note, it might also be useful to summarize the core of the method either at the end or at the start of section 3. \n- Sections 4.1 and 4.2 would benefit from more elaborate description of the results and what they mean."}, "questions": {"value": "- What do authors mean by \"Independently developed models\" on line 366? \n- In figure 3B, could you explain why there seems to be homologous relationship between Qwen-2.5-14B and Pang-Pro-MOE? \n- An interesting set of additional experiments would be to use the pretrained models from section 4.3 and apply multiple transformations to their weights (e.g. continued pretraining, SFT+DPO, pruning) and evaluate how the similarity metric and p-value compare across transformations, and for e.g. dataset size or hyperparameters in the transformations. Such results would further illustrate the robustness of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oMQ26apPcn", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Reviewer_h2j3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Reviewer_h2j3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316650489, "cdate": 1761316650489, "tmdate": 1762927011243, "mdate": 1762927011243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Robustness: Adversarial Noise Incapacitates Models Before Evading MDIR"}, "comment": {"value": "We are writing to address Reviewer EF5y’s concern regarding the robustness of MDIR to adversarial perturbations in the embedding layers, as well as Reviewer h2j3's concern on missing baselines. We appreciate the opportunity to address this point with evidence from some experiments.\n\nWe directly tested the robustness of MDIR by injecting increasing levels of Gaussian noise into the embedding (and unembedding, when they are not tied) matrices of two models: Qwen3-0.6B-Base and Llama-3.2-3B. The noise was scaled as  `noise_level * RMS(original_matrix)` to ensure fair comparison across scales. We evaluated:\n- Our method’s trace and normalized trace (the trace divided by the model dimension)\n- Two baseline similarity measures: naive cosine similarity and REEF’s Central Kernel Alignment (CKA)\n- The functionality of the model via LAMBADA perplexity and MMLU accuracy\n\nThe results are summarized below:\n\n### Qwen3-0.6B-Base\n| Noise Level | Trace (MDIR) | Normalized Trace | Cosine Sim. | CKA (REEF) | LAMBADA PPL | MMLU Acc |\n|-------------|--------------|-------------|-------------|------------|--------------|----------|\n| 0x (original)          | 1024         | 1       | 1       | 1      | 9.6          | 50.4     |\n| 0.1x        | 1023.98      | 0.99998     | 0.995       | 0.997      | 11.0         | 49.7     |\n| 0.3x        | 1023.82      | 0.9998      | 0.958       | 0.973      | 24.7         | 40.0     |\n| 1x          | 1021.96      | 0.998       | 0.707       | 0.582      | 31,412       | 27.1     |\n| 3x          | 1005.6       | 0.982       | 0.316       | 0.291      | 4e16       | 25.7     |\n\n### Llama-3.2-3B\n| Noise Level | Trace (MDIR) | Normalized Trace | Cosine Sim. | CKA (REEF) | LAMBADA PPL | MMLU Acc |\n|-------------|--------------|-------------|-------------|------------|--------------|----------|\n| 0x (original)         | 3072         | 1            | 1               | 1           | 3.95          | 54.1     |\n| 0.1x        | 3071.77      | 0.99992     | 0.995       | 0.9986     | 3.98         | 53.7     |\n| 0.3x        | 3069.88      | 0.9993      | 0.958       | 0.985      | 4.36         | 51.3     |\n| 1x          | 3048.44      | 0.992       | 0.707       | 0.721      | 37.9         | 26.0     |\n| 3x          | 2859.64      | 0.931       | 0.316       | 0.455      | 5e12       | 24.9     |\n\nResults show that: firstly, MDIR is more robust than baselines. Even at 3x noise (subsequent fine-tuning and quantization are unlikely to produce changes at this scale), MDIR retains more than 93% of maximum possible trace value, while naive cosine similarity and CKA drops significantly to around 0.3~0.5. This demonstrates that MDIR preserves structural signal far better than standard similarity metrics under strong perturbation.\n\nMore crucially, noise destroys model functionality before detection fails. At 1x noise, both models suffer catastrophic performance collapse (MMLU drops to near-random). Yet, MDIR still reports a normalized trace greater than 0.99, indicating detection remains highly confident.\n\n---\n\nMeanwhile, we are planning to conduct more experiments, such as continual pre-training after modifying an existing model's tokenizer, to verify that MDIR remains operational under these scenarios. Will this experiment address your concern? We look forward to your valuable suggestions on experimental design."}}, "id": "a9tZiLw1gc", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763134461863, "cdate": 1763134461863, "tmdate": 1763135008218, "mdate": 1763135008218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Kernelized Dynamics Pruning (KDP), a new method for pruning layers in large language models by viewing their forward computation as a discrete-time dynamical system. It observes that consecutive layers often produce highly similar representations, indicating redundancy. To exploit this, KDP projects representations into a kernel space where nonlinear transformations become approximately linear, allowing simpler modeling. A linear operator and an inverse mapping network then replace entire Transformer blocks. The authors present a theoretical error bound showing that multi-layer dynamics can be linearly approximated in this kernel space and prove that it provides superior fitting capacity compared to the original representation space. Extensive experiments on fifteen benchmarks demonstrate that KDP effectively prunes models while maintaining performance, without requiring fine-tuning. Overall, KDP provides a geometric and theoretically principled framework for simplifying internal model dynamics and reducing redundancy in large language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: Previous methods rely either on vendors implanting specific proprietary key-value pairs, or similarity measures of representations/weights. This work operates along the line of the second method, but also identifies the transformation involved (based on known symmetries of the weight matrices), which allow for a more fundamental detection of similarity. Moreover, they improve the statistical soundness of evaluating detection through a better evaluation of p-value based on large deviation theory arguments.\n- Quality: The theoretical part seems solid. The experiments are extensive, showing the method for a large number of examples. The algorithm is computationally efficient and runs on a laptop, allowing broad use. The appendix also includes comparison with previous methods (REES).\n- Clarity: The problem to solve is well posed and the writing is understandable and linear.\n- Significance: The method seems to improve significantly the state-of-the-art of identifying unattributed reuse of LLMs."}, "weaknesses": {"value": "As a general comment, it is not completely clear to me the extent to which weight relationships are included in the algorithm. The authors list a few of them, saying the list is not exhaustive. This is fine, as it is probably hard to write down an exhaustive list, but then I would expect that at least mathematically we have a clear statement of which groups of transformations satisfy the hypotheses of the method. The authors say that “we don’t need to characterize all totally invariant transformations; We need a subset with sufficiently large dimension,  which is enough for subsequent analysis with high confidence”. This is in principle explained in section 3.1, but I don’t see this specific point clearly adressed. Since this is a key point of the papers, the authors might consider refining this part to improve the manuscript."}, "questions": {"value": "- Along the line of the comments above, why are transformations from theta_A to theta_B being considered linear (cfr. line 176) ?\n- Just below, the authors show that Uq, Uk and Uv must be orthogonal matrices, but they don’t say anything about Uo. Should we consider this to be a generic matrix?\n- The subset of W transformations is declared to be sufficient on lines 196-200. Is this proven?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "26ed6X0ToF", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Reviewer_6fc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Reviewer_6fc4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738083603, "cdate": 1761738083603, "tmdate": 1762927010856, "mdate": 1762927010856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Robustness: Mathematical Proof of Preservation under Scaling and Permutation"}, "comment": {"value": "We write this section in hope to address reviewer's concern on the robustness of our method.\n\n---\n\nFollowing Section 3.2, we let $E$ and $E'$ denote the vocabulary embeddings of model $A$ and $B$, respectively. We demonstrate that the final results remain invariant when an adversary applies both scaling and permutation to the matrix $E'$. Specifically, we consider the transformed embedding $E'' = \\alpha E' P'$, where $P' \\in \\mathrm{Perm}(\\mathrm{EmbDim})$ is a permutation matrix, and $\\alpha > 0$ is a scaling factor.\n\n## Lemma\nDefine the trace and the corresponding optimal permutation as follows:\n\n$$\n\\begin{aligned}\n    T(E, E') &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left(E'^{\\text{T}} E\\right)^{\\text{T}}\\right), \\quad \\\\\n    \\Pi(E, E') &= \\arg\\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left(E'^{\\text{T}} E\\right)^{\\text{T}}\\right).\n\\end{aligned}\n$$\n\nSuppose an adversary applies a simultaneous scaling and permutation transformation to $E'$, yielding $E'' = \\alpha E' P'$, where $P' \\in \\mathrm{Perm}(\\mathrm{EmbDim})$ is an arbitrary permutation matrix, and $\\alpha > 0$ is a positive scaling coefficient. In this case, the following properties hold:\n\n$$\nT(E, E'') = T(E, E'), \\quad \\Pi(E, E'') = P'^{\\text{T}} \\Pi(E, E').\n$$\n\nThus, such adversarial modifications are futile.\n\n## Proof\nWe first analyze the trace $T(E, E'')$:\n\n$$\n\\begin{aligned}\n    T(E, E'') &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left((E''^{\\text{T}} E)^{\\text{T}}\\right)\\right) \\quad \\\\\n    &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left(((\\alpha E' P')^{\\text{T}} E)^{\\text{T}}\\right)\\right) \\quad \\\\\n    &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left(\\alpha E^{\\text{T}} E' P'\\right)\\right).\n\\end{aligned}\n$$\n\nSince scaling by $\\alpha > 0$ does not affect the orthogonal factor in polar decomposition ($\\mathrm{Ortho}$), we can simplify further:\n\n$$\n\\begin{aligned}\n    T(E, E'') &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}\\left((E^{\\text{T}} E') P'\\right)\\right) \\\\\n    &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P \\cdot \\mathrm{Ortho}(E^{\\text{T}} E') P'\\right) \\\\\n    &= \\max_{P \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(P' P \\cdot \\mathrm{Ortho}\\left(E^{\\text{T}} E'\\right)\\right).\n\\end{aligned}\n$$\n\nLet $R = P' P$. Since $P'$ is a fixed permutation matrix, as $P$ ranges over all permutations in $\\mathrm{Perm}(\\mathrm{EmbDim})$, so does $R$. Substituting $R$ into the expression, we obtain:\n\n$$\n\\begin{aligned}\n    T(E, E'') &= \\max_{R \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(R \\cdot \\mathrm{Ortho}\\left(E^{\\text{T}} E'\\right)\\right) \\\\\n    &= \\max_{R \\in \\mathrm{Perm}(\\mathrm{EmbDim})} \\mathrm{Tr}\\left(R \\cdot \\mathrm{Ortho}\\left((E'^{\\text{T}} E)^{\\text{T}}\\right)\\right) \\\\\n    &= T(E, E').\n\\end{aligned}\n$$\n\nNext, we examine the optimal permutation $\\Pi(E, E'')$. Recall that the trace achieves its maximum when $R = \\Pi(E, E')$. From the substitution $R = P' P$, it follows that:\n\n$$\nP' P = \\Pi(E, E') \\implies P = P'^{-1} \\Pi(E, E') = P'^{\\text{T}} \\Pi(E, E').\n$$\n\nThus, the optimal permutation for $E''$ is given by:\n\n$$\n\\Pi(E, E'') = P'^{\\text{T}} \\Pi(E, E').\n$$\n\nThis completes the proof.\n\n---\n\nWe plan to insert this theoretical proof, as well as the previous experimental results, into the Appendix. We welcome reviewers to offer suggestions on the writing style and logical coherence."}}, "id": "9tBqozT5pt", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763136302894, "cdate": 1763136302894, "tmdate": 1763136302894, "mdate": 1763136302894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To detect LLM weight homology, this paper proposes Matrix-Driven Instant Review (MDIR), leveraging matrix analysis and Large Deviation Theory to identify weight reuse, reconstruct transformation relationships between weights, and provide rigorous p-value estimates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is novel and contributes to the protection of open-source model IP.\n- Experiments demonstrate the robustness and practicality of the proposed method."}, "weaknesses": {"value": "- The paper's core motivation relies on an \"idealized\" assumption (Lines 106-107). Is there related experimental evidence or research to support this?\n- MDIR depends on first reliably computing $U$ from the embedding layers $E$ and $E'$. If an adversary intentionally injects large-scale, non-orthogonal noise into or retrains only the embedding layers, it could lead to an incorrect $U$ calculation, causing all subsequent layer detections to fail. There is a lack of discussion on the lower bounds of MDIR's robustness.\n- There are potential overclaims, such as an undefined \"consumer PC\" and no discussion on computational resource consumption.\n- The method is not applicable to closed-source models. In real-world scenarios, malicious users often provide API services, which limits its application.\n- There are clarity issues with the paper's presentation, such as excessively small text in figures (e.g., Figures 4, 5, 6, and more) and an illogical presentation of text content in Section 4.2."}, "questions": {"value": "- If multiple transformations (e.g., permutation and quantization) exist in the model simultaneously, can MDIR reconstruct them?\n- Why is there no specific pattern in Figure 5(c)? The pattern in Figure 4(c) is also unclear. Is this related to the properties of MLPs?\n- Is this method applicable to model merging scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKP0XW1HMc", "forum": "kl8CrfUvw3", "replyto": "kl8CrfUvw3", "signatures": ["ICLR.cc/2026/Conference/Submission16995/Reviewer_EF5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16995/Reviewer_EF5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997374545, "cdate": 1761997374545, "tmdate": 1762927010470, "mdate": 1762927010470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}