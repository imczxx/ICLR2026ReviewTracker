{"id": "YULeQtSyiW", "number": 16259, "cdate": 1758262427137, "mdate": 1759897251526, "content": {"title": "Task-Related Token Compression in Multimodal Large Language Models from an Explainability Perspective", "abstract": "Existing Multimodal Large Language Models (MLLMs) process a large number of visual tokens, leading to significant computational costs and inefficiency. Instruction-related visual token compression demonstrates strong task relevance, which aligns well with MLLMs‚Äô ultimate goal of instruction following. Previous works generally assume that visual tokens achieve better vision‚Äìlanguage alignment in the shallow layers of LLMs, which have led to task-related token compression being primarily applied in intermediate LLM layers. In contrast, our study reveals that with proper selection, task-related token compression is feasible at the input stage of LLM with negligible performance loss. This new paradigm significantly reduces task-irrelevant visual tokens and its model-agnostic design enables application without modifying the LLM architecture. Specifically, we suggest that explainability methods for transformer-based architechtures can evaluate the global importance of each visual token with respect to the given instruction, which can effectively guide the task-related token compression for MLLMs. Furthermore, we propose to learn a mapping from the attention map of the first LLM layer to the explanation results, thereby avoiding the need for a full inference pass. Interestingly, this mapping can be learned using a simple and lightweight convolutional network, whose training is efficient and independent of MLLMs. Extensive experiments on 11 image and video benchmarks across three leading MLLMs (Qwen2-VL, LLaVA-OneVision, and VILA1.5) demonstrate the remarkable effectiveness and strong generalization of our approach. Additionally, our new compression paradigm achieves faster inference with reductions in both prefilling time and KV-cache memory.", "tldr": "", "keywords": ["multimodal large language model; token compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e194f09a5a3162e1bd0fdb49abd8dec43224604.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a visual token compression method for Multimodal Large Language Models (MLLMs) based on explainability approaches, enabling task-related token compression at the LLM input stage. The core innovation lies in using explainability methods to evaluate the importance of visual tokens and training a lightweight network to predict these importance scores. Evaluations on 3 MLLM models and 11 image/video benchmarks show that the method maintains high performance while reducing computational complexity, prefilling time, and KV-cache usage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It achieves an innovative paradigm shift, as the study challenges the previous view that shallow-layer visual tokens are indispensable.\n2. The authors conducted comprehensive experiments to demonstrate the superiority of their method compared to other existing approaches."}, "weaknesses": {"value": "Requires more discussion and ablation analysis; see Questions for details."}, "questions": {"value": "1. Why is the network depth set to 5 layers? No ablation analysis is provided to justify this choice.\n2. Can relevance prediction be improved using more advanced architectures without sacrificing efficiency?\n3. The motivation explanation in Section 3.1 is insufficiently clear. Specifically, the causal link indicated by \"therefore\" in Lines 186‚Äì187 is not clear enough.\n4. Why is one layer of attention sufficient? The paper mentions \"first-layer attention suffices\" but provides no theoretical or empirical analysis to support this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2xddTs6caq", "forum": "YULeQtSyiW", "replyto": "YULeQtSyiW", "signatures": ["ICLR.cc/2026/Conference/Submission16259/Reviewer_66M8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16259/Reviewer_66M8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519601838, "cdate": 1761519601838, "tmdate": 1762926411737, "mdate": 1762926411737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to compress visual tokens for accelerating inference of multi-modal large language models (MLLMs).\n\nSpecifically, it transfers the explainability methods for transformer-based architectures to visual token pruning.\nThe explainability methods often keep a relevance map that could be used to measure the importance of visual tokens.\n\nTo validate the effectiveness of the method, experiments on various benchmarks (MME, MMStar, MMVet, Video-MME, MVbench, and MMBench-V) are conducted, demonstrating improvements over previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper writes clearly and is easy to follow.\n\n(2) It is interesting that the relevance map from explainability methods can accurately identify important visual tokens."}, "weaknesses": {"value": "(1) To derive the relevance map, it requires the ground truth labels to calculate gradients. Thus, the paper proposes a lightweight model to distill knowledge from the derived relevance maps. However, it is challenging to make it generalizable to various data, as the lightweight model is small and the size of training data is also much smaller than that used for MLLMs.\n\nIt would be better to evaluate the model performance on various benchmarks, like GQA, SQA, VQAv2, VizWiz, and MMB.\n\n\n(2) For the lightweight model, it takes the first-layer attention scores as inputs.\n     Why not use the original visual tokens as inputs?     \n     Is it helpful to use multiple-layer attention scores?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "87VVCu3j2i", "forum": "YULeQtSyiW", "replyto": "YULeQtSyiW", "signatures": ["ICLR.cc/2026/Conference/Submission16259/Reviewer_y6e3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16259/Reviewer_y6e3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814778027, "cdate": 1761814778027, "tmdate": 1762926411282, "mdate": 1762926411282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a task-related token compression paradigm for Multimodal Large Language Models (MLLMs) to address the high computational cost and inefficiency associated with a large number of visual tokens in the LLM input. The authors first use an explainability method (e.g., gradient-weighted multi-head attention) to assess the global importance of each visual token relative to a given instruction, generating a \"ground truth\" importance score. Building on this insight, they train a lightweight convolutional network to predict this importance score based solely on the LLM's first layer attention map. Experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to understand. Figure 1 and 2 are intuitive to understand the motivation and the big picture of the proposed method.\n\n2. The authors successfully demonstrate the feasibility and efficiency of performing task-related token compression at the LLM input stage with negligible performance loss, providing a new, more efficient compression strategy.\n\n3. Experiments validate the effectiveness across three distinct MLLM architectures (Qwen2-VL, LLaVA-One Vision, VILA1.5) and 11 diverse image and video benchmarks, showcasing its robustness and wide applicability."}, "weaknesses": {"value": "1. The lightweight convolutional network is trained using only the first-layer attention map ($A^0$). A more in-depth analysis is needed to justify how such a small network, using only shallow information, can accurately and robustly predict the global, task-specific importance score, which typically requires information propagated through multiple LLM layers.\n\n2. This train-based methods may overfit to the training data. I wonder the performance on some OCR related benchmarks (TextVQA, Chartqa, DocVQA, OCRBench), since they are more challenging to validate the effectiveness of the proposed method."}, "questions": {"value": "The paper mentions training using only 10K image samples. Please elaborate on the data sampling strategy (e.g., are they sampled from the test benchmarks? Are they general-domain? How is diversity ensured?) Is the small scale of the training data a potential limitation when generalizing to open-world or other benchmarks scenarios beyond the 11 evaluation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lRIYmTZnSD", "forum": "YULeQtSyiW", "replyto": "YULeQtSyiW", "signatures": ["ICLR.cc/2026/Conference/Submission16259/Reviewer_M6Sq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16259/Reviewer_M6Sq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894709067, "cdate": 1761894709067, "tmdate": 1762926410513, "mdate": 1762926410513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach for task-aware visual token compression in multimodal large language models (MLLMs), aiming to eliminate instruction-irrelevant tokens at the LLM input stage to enhance inference efficiency‚Äîwithout modifying the underlying architecture. The key idea is to utilize explainability techniques to assign relevance scores to visual tokens with respect to a given instruction, guiding the compression process accordingly. Furthermore, the authors demonstrate that a lightweight convolutional network can be trained to map first-layer attention maps to explainability-derived importance scores, enabling token importance prediction without a full forward pass."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work tackles a critical bottleneck in MLLMs that high computational and memory overhead from large numbers of visual tokens by introducing an instruction-aware token compression paradigm that operates without architectural changes.\n2. The proposed compressors are lightweight, transferable, and require minimal retraining across diverse MLLM architectures, underscoring strong generalizability.\n3. Evaluation spans 11 benchmarks for both images and videos, with comprehensive ablations, efficiency analyses, and baseline comparisons."}, "weaknesses": {"value": "1. The paper relies heavily on empirical findings and visual evidence, offering limited theoretical grounding. The mathematical formulation (e.g., the relevance propagation equation for ùëÖt in Section 3.2) lacks a deeper analysis of properties such as linearity, gradient behavior, and attribution faithfulness, especially under ambiguous or multi-factor instructions."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b2qKgORrXt", "forum": "YULeQtSyiW", "replyto": "YULeQtSyiW", "signatures": ["ICLR.cc/2026/Conference/Submission16259/Reviewer_RQ2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16259/Reviewer_RQ2d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762423438874, "cdate": 1762423438874, "tmdate": 1762926410103, "mdate": 1762926410103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}