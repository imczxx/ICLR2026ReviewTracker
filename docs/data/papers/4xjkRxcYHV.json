{"id": "4xjkRxcYHV", "number": 7632, "cdate": 1758029965072, "mdate": 1763466939120, "content": {"title": "A Dual-View Contrastive Learning Framework for Heterogeneous Graph Representation Learning", "abstract": "Heterogeneous graph representation learning leverage the rich semantics and complex structural relationships within heterogeneous graphs. However, existing methods often fail to capture long-range semantic dependencies and localized structural patterns simultaneously. Therefore, we propose a novel Dual-View Contrastive Learning framework (DVCL) for heterogeneous graph representation learning. Specifically, the Graph Schema View Module (GSVM) is conducted to model the structural dependencies by leveraging a relational graph neural network with type-aware message passing and adaptive residual connections. Then, the Semantic Meta-Path Mamba Module (SMPMM) is designed to capture high-order semantic dependencies through a globally enhanced Mamba backbone, equipped with multi-resolution fusion and directional positional encodings. Moreover, a dynamic bidirectional contrastive learning is constructed to integrate the semantic view and structural view, treating each view as a learnable augmentation of the other to ensure robust and complementary representations. Extensive experiments on four datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods, in terms of classification and clustering tasks.", "tldr": "", "keywords": ["Multi-instance/Multi-view Learning", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3fd3e481eaac57079e6957d7ae785c6e6ec268d.pdf", "supplementary_material": "/attachment/f71c2821ac88806a7a65d3c1babe3248a60c0c84.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on heterogeneous graph representation learning, aiming to simultaneously capture long-range semantic dependencies and localized structural patterns, which are often overlooked by existing methods. The authors propose a novel framework named DVCL, which integrates a Graph Schema View Module (GSVM) for structural modeling via relational graph neural networks and a Semantic Meta-Path Mamba Module (SMPMM) for high-order semantic modeling using a Mamba-based sequence encoder. A bidirectional contrastive learning mechanism aligns these views to produce robust representations. Experimental results on four datasets show that DVCL outperforms state-of-the-art methods in node classification and clustering tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe use of Mamba architecture for meta-path sequences in heterogeneous graphs is innovative, as it addresses long-range dependency modeling with linear complexity.\n2.\tComprehensive evaluations on four datasets provide strong empirical support, and the inclusion of ablation studies and the hyperparameter analysis validates component contributions.\n3.\tThe paper is well-structured, with clear module descriptions and algorithm outlines. Figure 1 gives a very detailed overview of the proposed model."}, "weaknesses": {"value": "1.\tThe hyperparameter analysis only examines embedding dimension $h$, temperature $\\tau$, and loss weight $\\lambda$, ignoring critical parameters like the number of Mamba layers.\n2.\tNo theoretical or experimental analysis of convergence, complexity bounds, or robustness guarantees is included.\n3.\tThe authors provide complexity analysis in Section 2.5, but the paper does not discuss practical scalability on very large graphs or compare with baselines empirically.\n4. Although meta-path mamba module is novel to me, the motivation of using it should be clarified in introduction."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XUegeIHYfL", "forum": "4xjkRxcYHV", "replyto": "4xjkRxcYHV", "signatures": ["ICLR.cc/2026/Conference/Submission7632/Reviewer_A832"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7632/Reviewer_A832"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477506835, "cdate": 1761477506835, "tmdate": 1762919711144, "mdate": 1762919711144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DVCL, a contrastive heterogeneous graph representation learning approach. DVCL leverages two views: one from the local heterogeneous neighborhood captured by a Graph Schema View Module (GSVM), another from high-order metapath-based dependencies captured by a Semantic Meta-Path Mamba Module (SMPMM). In experiments, DVCL outperforms recent supervised, generative, and contrastive methods on some representative heterogeneous graph datasets for node classification and node clustering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Heterogeneous graph representation learning is a fundamental task that could benefit a wide spectrum of downstream applications.\n2. This paper is easy to read. It is well-written and clearly organized overall."}, "weaknesses": {"value": "1. The novelty of the proposed method is quite limited. There are already quite a few studies that have proposed contrastive methods for heterogeneous graph representation learning, particularly the schema view + metapath view strategy.\n2. The technical quality of this paper is not strong enough. The key innovations and advantages of the proposed method are not clear enough. Also, some important tasks are missing, such as link prediction."}, "questions": {"value": "Based on the review above and some other issues, the reviewers have the following questions or comments.\n1. DVCL and HeCo are quite similar in that they both adopt a schema view + metapath view contrastive strategy. The main differences seem to be just the choices of the respective view encoders. Simply swapping the encoder modules to other neural architectures (proposed by other researchers) does not contribute much to the paper novelty. More elaboration on this is needed.\n2. The literature review is missing some important related studies, including DMGI [1], PT-HGNN [2], CPT-HG [3], and HGCML [4]. The authors may need to include them and discuss how DVCL is different and why their proposed designs are better.\n3. The experiments only cover two node-level tasks, which may not comprehensively reflect different methods' capabilities. Including link-level tasks (e.g., link prediction) or even graph-level tasks can make the experimental results more convincing.\n4. There are quite a few typos or grammatical errors, including but not limited to\n    * Page 1 Line 013: \"leverage\" => \"leverages\"\n    * Page 1 Line 046: missing space in \"… Liao et al. (2022).In order to …\"\n    * Page 2 Line 065: extra period in \"Consequently,.multi-perspective …\"\n    * Page 2 Line 065: \"helerogeneous\" => \"heterogeneous\"\n    * Page 2 Line 066: missing space in \"relation-specificviews\"\n    * Page 2 Line 088: extra space in \"… message passing , an SMPMM … \"\n    * Page 2 Line 095: \"Deep View-aligned Contrastive Learning (DVCL)\" => \"Dual-View Contrastive Learning (DVCL)\"\n    * Page 4 Line 165: \"stagel\" => \"stage\"\n    * Page 6 Line 306: \"Datasets.we …\" => \"Datasets. We …\"\n    * Page 9 Line 444 & Page 17 Line 873 & Page 17 Line 914: \"Clssifications\" => \"Classifications\"\n\n[1] Chanyoung Park, Donghyun Kim, Jiawei Han, Hwanjo Yu: Unsupervised Attributed Multiplex Network Embedding. AAAI 2020: 5371-5378\n\n[2] Xunqiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, Hui Wang: Pre-training on Large-Scale Heterogeneous Graph. KDD 2021: 756-766\n\n[3] Xunqiang Jiang, Yuanfu Lu, Yuan Fang, Chuan Shi: Contrastive Pre-Training of GNNs on Heterogeneous Graphs. CIKM 2021: 803-812\n\n[4] Zehong Wang, Qi Li, Donghua Yu, Xiaolong Han, Xiao-Zhi Gao, Shigen Shen: Heterogeneous Graph Contrastive Multi-view Learning. SDM 2023: 136-144"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZNxfXo5Bq", "forum": "4xjkRxcYHV", "replyto": "4xjkRxcYHV", "signatures": ["ICLR.cc/2026/Conference/Submission7632/Reviewer_S2mY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7632/Reviewer_S2mY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643200180, "cdate": 1761643200180, "tmdate": 1762919710729, "mdate": 1762919710729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the limitations of heterogeneous graph representation methods that struggle to capture both long-range semantic dependencies and localized structural patterns, this paper introduces a Dual-View Contrastive Learning (DVCL) framework that jointly models structure-aware and semantics-aware representations for heterogeneous graphs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dual-view contrastive framework effectively integrates structural and semantic modeling.\n  \n2. The module design is clear, and each component contributes significantly."}, "weaknesses": {"value": "- Figure 1 is poorly drawn; the nodes are distorted, labels are inconsistent, and the layout is overcrowded.\n  \n- The improvement over baselines such as HGMS is minor, generally within two to three points, and some compared methods are outdated.\n  \n- Figure 3 lacks comparison with the strongest baseline HGMS, reducing the credibility of visualization.\n  \n- The method relies heavily on predefined meta-paths, limiting generalization to graphs without clear semantic paths and lacking automatic meta-path discovery.\n  \n- The experimental setup is limited, focusing only on node-level tasks and missing newer baselines and broader downstream evaluations."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ryVwngi7y", "forum": "4xjkRxcYHV", "replyto": "4xjkRxcYHV", "signatures": ["ICLR.cc/2026/Conference/Submission7632/Reviewer_gr1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7632/Reviewer_gr1c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911197757, "cdate": 1761911197757, "tmdate": 1762919709936, "mdate": 1762919709936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}