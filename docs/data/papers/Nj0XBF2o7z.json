{"id": "Nj0XBF2o7z", "number": 11880, "cdate": 1758204452423, "mdate": 1763527555931, "content": {"title": "Seesaw: Accelerating Training by Balancing Batch Size and Learning Rate Scheduling", "abstract": "Increasing the batch size during training --- a “batch ramp'' --- is a promising strategy to accelerate large language model pretraining. While for SGD, doubling the batch size can be equivalent to halving the learning rate, the optimal strategy for adaptive optimizers like Adam is less clear. As a result, any batch-ramp scheduling, if used at all, is typically tuned heuristically. This work develops a principled framework for batch-size scheduling and introduces Seesaw: whenever a standard scheduler would halve the learning rate, Seesaw instead multiplies it by $1/\\sqrt{2}$ and doubles the batch size, preserving loss dynamics while reducing serial steps. Theoretically, we provide, to our knowledge, the first finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression, and we extend this equivalence to normalized SGD, a tractable proxy for Adam, under a variance-dominated regime observed in practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla scale using a constant (critical) batch size, Seesaw matches cosine decay at equal FLOPs while reducing wall-clock time by $\\approx 36\\%$, approaching the theoretical limit implied by our analysis.", "tldr": "We introduce a new learning rate scheduler and batch size ramp up scheme that reduces serial run time in LLM pretraining.", "keywords": ["optimization", "batch size", "cbs", "scheduler", "llm", "pretraining"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fef23299b99728fbdbc7e5eeb16c811fb63a1fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a technique to trade off a decrease in LR for an increase in batch size for LLM training.  The recipe is an application of the square-root scaling rule previously proposed for Adam training: whenever you double the batchsize, you should increase the LR by \\sqrt(2).  Here, the idea is that when we would normally drop the LR by 2 according to its decay schedule, we immediately apply the square-root scaling rule, with the net effect that the LR gets decreased by only \\sqrt(2), and the batch size is doubled.  Experiments on 150M, 300M, and 600M models trained to a compute-efficient 20 tokens-per-parameter show this approach can equal the loss of the baseline recipe, but using fewer steps (which you can calculate in advance)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "It makes sense to revisit ideas like \"Don't decay the LR, increase the batch size\" in the context of modern LLMs.  Companies are competing to train the next generation of models as quickly as possible; if we can train faster, without spending additional FLOPs, it's of definite benefit.  \n\nTesting a simple recipe, as in the paper, is definitely the next step; if it works, people can immediately apply it.  Moreover, the ability to deterministically calculate the number of steps in advance is useful for resource planning.  I.e., the schedule is not based on, e.g., online measurements of the gradient noise scale as in https://arxiv.org/abs/2411.00999, but rather are motivated in advance from the LR schedule.\n\nI would absolutely encourage the authors to keep working in this direction!"}, "weaknesses": {"value": "In its current form, the paper is only half-baked, in terms of writing, experimental rigor, theory, awareness of prior work, etc.  The paper gave me the feeling that the ICLR reviewers were doing the work of proofreading the paper, rather than mentors or co-authors, which was unsettling.\n\n- The idea that Adam is basically normalized SGD, and that this necessitates dividing the learning rate by the square root of the batch size adjustment, was previously articulated in, e.g., in \"Hilton - Batch size-invariance for policy optimization - 2110.00641v3\", where (citing Hardin 2017), they similarly note \"Adam divides the gradient by a running estimate of the root mean square gradient\".  Hilton et al. divide this estimate into the square root of {gradient-mean-squared plus gradient-variance}, and show how the batch size reduces the variance, and essentially leads to the square root LR adjustment.  Since this paper wasn't discussed or cited, I'm not sure what the connections are, or the extent to which the theory as presented goes beyond this.\n\n- Regarding soundness, I was concerned why we tested the extreme values of the equivalence (Figure 2) at 2x the CBS?  Especially since they're much closer at 256, it makes me suspect that they're even closer at 128, yet this wasn't presented.\n\n- Also regarding soundness: I just feel there isn't the depth and breadth of experiments that we would normally find in an ICLR paper.  Beyond this, I feel like there's not enough depth and breadth to convince me to use this approach in my own training.\n\n- Limitations were not discussed, e.g., not acknowledging that state-of-the-art models are trained with non-zero weight decay, to higher tokens-per-parameter, etc.  That this was only on one dataset with one tokenizer with one optimizer, etc.\n\n- The claim in the abstract that we \"*approach the theoretical limit implied by our analysis*,\" is misleading, as approaching this limit is not, like, a sign the model is training well or whatever, but just that the continuous-time approximation that you used returns a number that is close to the actual number of steps, right?  I mean, you could run a simple dry-run simulation to exactly determine how many steps the model will take given your algorithm, it's not like \"abstract-level-ITALICIZED-claim\" significant that your continuous-limit version provides a close answer, right?  The significant thing is that the losses match, but, like I said, that's got nothing to do your continuous-time version.  Unless I'm really misunderstanding something, which is possible, because...\n\nThe paper is poorly written and organized, and not from like a non-native-speaker grammar perspective, but from a thinking-and-planning-out-the-paper-clearly perspective.  Some points on these lines:\n\n- It's confusing as heck to use α and β to represent the adjustments to the LR and batch size --- used in the product that should remain constant --- and to use them as actual values that get substituted into this equation, essentially α := √α, β := (√α)^2. You know what I mean?  Like, setting β = α is needed in order to satisfy α = √β.  It's almost non-sensical.\n\n- \"where the schedulers are equivalent in terms of loss as long as we keep the product α√β fixed\" – what schedulers???  The one where B doesn’t change and the one where it does?\n  - Let’s use α and β to be the adjustments to the LR and batch size, respectively.\n  - Case 1: α = C and β = 1.  I.e., the points where the LR schedule would drop by C and we don’t change the batch size.  α√β = C * √1 = C.  (e.g., C=2 would give us the abstract of the paper)\n  - Case 2 (proposed): α = √C, and β = something.  To maintain the invariant, β = C, only this way will α√β = √C√C = C as before.\n  - As far as I can tell, that’s our only option for β in order to keep the product fixed.  But then we say, of all the ways we could adjust the batch size, i.e., of all the β values, the most aggressive we can use must satisfy α = √β, i.e., β = α^2.  So if α = √C, β = C.  But isn’t this our only option to have an equivalent scheduler?\n  - I REALLY wanted to understand what you’re saying here, but I just could not.  Maybe on a second reading of the paper... but I'm just a reviewer, make my life easier please!\n\n- Not enough context prior to the experiments\n  - E.g., there's a section on “Assumption 3”, but this assumption is only mentioned parenthetically before this point\n\n- I don’t really understand why all the content that was collected into Section 3 is in there:\n  - Intuition\n  - Theoretical results, with lots of pointers to other parts of the paper.\n  - The formal algorithm\n  - Like, you say, “the NSGD update rule, which is a crucial component of designing Seesaw”, but it seems to me that using a simple square-root scaling rule would have been sufficient here.\n\n- When you present your main findings (Table 1), maybe provide some interpretation?  Like, what is the take-home message here?\n\n- Too much required for understanding is in the appendices.\n\nI could go on.\n\nNitpicks:\n- For the Taylor expansions, it would have been helpful to define x_0, x_1 and x_2, and to define the noise terms precisely.  Should there be some expectations here?  Can you cite the specific section of Malladi that “shows this argument”?\n- Might be worth pointing out somewhere that actually changing the batch size in a fine-grained way is challenging for modern large-scale LLM GPU deployments… although perhaps not on other hardware…\n- “Understanding batch size ramp up schemes during training has been a topic of interest in recent years”.  Really?  I mean, people have used it, but has \"*understanding* ramp-up\" been a topic of interest?\n- Wrong use of \\citet vs \\citep, e.g., “Recently, (Meterez et al., 2025) have used” should instead by \\citet\n- Typo: “We further empirically comapre* Seesaw”\n- Table 1: “Note that the dynamics match” --- do you mean the final losses?  Might be good to highlight (e.g., color/italicize/shade/bolden) the CBS cells somehow.\n- Says \"trained using AdamW\", but then weight decay = 0.0, so isn’t this just vanilla Adam?\n- For the CBS, do you get the numbers directly from Zhang, or do you use their power law estimates based on tokens, or something else?"}, "questions": {"value": "- Suppose I apply the Merrill et al approach at exactly those points where I drop the LR by 2x.  That is, I double the batch size, and then scale the LR back up by √2 (after dropping it by 2).  So the net change to the LR is to change by √2/2 = 1/√2.  Doesn’t this result in the exact prescription from this paper: whenever you would drop the LR by 2x, you instead double the batch size and decrease the LR by 1/√2?  If so, is the key difference from the Merrill approach just the timing of when you do this adjustment?  How does their proposed timing differ from yours?\n\n- For Figure 3, what does “Seesaw” mean in terms of α and β?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dHyRGbBuED", "forum": "Nj0XBF2o7z", "replyto": "Nj0XBF2o7z", "signatures": ["ICLR.cc/2026/Conference/Submission11880/Reviewer_gioo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11880/Reviewer_gioo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428271681, "cdate": 1761428271681, "tmdate": 1762922896601, "mdate": 1762922896601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Seesaw, a principled batch size scheduling algorithm designed to reduce wall-clock training time for LLM pre-training. The key insight is to replace LR decay with slower decay and increasing batch size. Specifically, when a standard schedule would decrease LR by factor $\\alpha$, Seesaw instead decays LR by $\\sqrt{\\alpha}$ while increasing batch size by $\\alpha$.\n\nThe authors motivate this rule through theoretical analysis of SGD on noisy linear regression, extending to Normalized SGD (NSGD) as a proxy for Adam. Under a \"variance-dominated\" regime, analysis on NSGD results in the Seesaw scheduler. Experiments on 150M-600M parameter models at Chinchilla scale demonstrate ~36% wall-clock time reduction while matching baseline validation loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong empirical results.** The paper's main contribution is its compelling empirical validation: Seesaw achieves approximately 36% reduction in wall-clock training time while matching the baseline's final validation loss. For practitioners with access to parallel compute resources (for processing larger batch sizes), this translates directly to significant cost savings. Another advantage is its simplicity, as it can be implemented as a straightforward drop-in replacement for standard learning rate schedules.\n2. **Principled derivation of a simple heuristic.**  The authors bridge theory and practice by grounding their scheduler in a principled theoretical framework rather than relying on pure empirical tuning. This approach elevates batch size scheduling from ad-hoc experimentation to a more systematic methodology.\n3. **Rigorous validation of theoretical predictions and failure modes.** The evaluation of where its theory succeeds and where it breaks down strengthens the paper. Figure 2 validates the stability condition derived in Lemma 3, confirming that overly aggressive schedules ($\\alpha < \\sqrt{\\beta}$​) lead to performance degradation as predicted. Figure 3 demonstrates the failure of the \"variance-dominated\" assumption at very large batch sizes, where Seesaw no longer tracks the baseline. This transparency about limitations adds credibility and helps practitioners understand the method's applicability boundaries."}, "weaknesses": {"value": "1. **The NSGD proxy inadequately represents Adam.** The choice of Normalized SGD appears driven by mathematical tractability rather than fidelity to Adam's behavior. NSGD's global L2 normalization differs fundamentally from Adam's coordinate-wise adaptivity. A strong consensus in recent literature suggests that SignSGD, which respects the sign-based nature of Adam's updates, is a much better conceptual proxy. Since Seesaw's square-root scaling rule is specifically tailored to the analysis on NSGD, it's unclear whether similar insights hold for SignSGD or Adam. The paper should explicitly acknowledge this limitation and discuss why the NSGD-derived heuristic succeeds despite this mismatch.\n\n2. **Oversimplified theoretical foundation.** The noisy linear regression setting vastly oversimplifies LLM training's non-convex, high-dimensional landscapes. More critically, the \"variance-dominated\" assumption (Assumption 3) appears overly strong and may only hold near the end of training. The paper does not sufficiently justify why analyzing this simplified setting provides a good proxy for LLM training dynamics, even though the empirical results suggest the heuristic works in practice. \n\n3. **Performance degradation at large batch sizes.** While overall results are strong, Table 1 shows Seesaw slightly underperforming the cosine baseline at large batch sizes (e.g., $B=1024$ for all 150M, 300M, 600M models). This gap along with Figure 3 suggest diminishing benefits as batch size grows beyond the variance-dominated regime. This ceiling on applicability could limit the method's utility for larger-scale training runs in practice.\n\n4. **Proof presentation and limited technical novelty.** The proofs in Appendix A suffer from unclear notation. Multiple symbols (e.g. $Q$, $\\lambda$, $\\Lambda$) are used without proper definition, making the derivations difficult to follow. Presumably these relate to an eigendecomposition $H=Q\\Lambda Q^T$ with $\\lambda = \\text{diag}(\\Lambda)$, but this is never explicitly stated. Additionally, the theory specifically analyzes a factor of 2.0 step decay but does not provide a proof for general drops by factor $\\alpha$, limiting its generality. Beyond these presentation issues, the technical contribution itself is limited—the proof is straightforward and follows the same approach as Meterez et al. (2025).\n\n5. **Inconsistent visualization in Figure 1.** The figure uses log scale for tokens but linear scale for steps, which is inconsistent and potentially misleading. The log scale can obscure small but meaningful differences in sample efficiency, while the linear scale visually exaggerates the speedup. Using consistent linear scaling for both plots would provide a more transparent and fair comparison of the method's performance."}, "questions": {"value": "1. Could the authors elaborate on why NSGD was chosen as a proxy for Adam, given that recent literature increasingly favors SignSGD as a more faithful conceptual model? Would a theoretical analysis based on SignSGD yield the same $\\sqrt{\\alpha}$​ scaling rule, or would different dynamics emerge?\n\n2. The theory analyzes the specific case of a factor 2.0 decay. Could the authors provide a proof for the general case of decay by factor $\\alpha$?\n\n3. The practical implementation discretizes the continuous cosine decay (e.g., using $\\alpha=1.1$ in Table 1). How sensitive is the method's performance to this discretization factor? Would finer-grained approximations better track the cosine curve, and are there practical trade-offs to consider in choosing $\\alpha$?\n\n4. Could the authors revise Figure 1 to use consistent linear scaling for tokens to enable fairer visual comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n2l5EqUOqE", "forum": "Nj0XBF2o7z", "replyto": "Nj0XBF2o7z", "signatures": ["ICLR.cc/2026/Conference/Submission11880/Reviewer_nmrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11880/Reviewer_nmrR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656781824, "cdate": 1761656781824, "tmdate": 1762922896230, "mdate": 1762922896230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed SEESAW scheduling method replaces standard learning rate halving by concurrently multiplying the learning rate by \\sqrt{\\frac{1}{2}}  and doubling the batch size. This reduces serial steps while preserving the loss trajectory. Experiments on 150M-600M models with C4 and Chinchilla scaling show SEESAW matches cosine scheduling's final loss while achieving near-theoretical-maximum speedups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper establishes a non-asymptotic equivalence between learning rate decay and batch size growth under SGD, extending it to NSGD via an equivalence family where the product \\alpha \\sqrt{\\beta} is conserved. This links theoretical insight to practice, forming an actionable framework for designing training protocols.\n\n2. The proposed algorithm features a remarkably simple structure and achieves true zero intrusion, meaning it can be seamlessly integrated into existing training pipelines without requiring any modifications to the model architecture, optimizer, or other components. Its plug-and-play nature makes it highly accessible and easy to adopt in practice.\n\n3. The theoretical upper bound derived in the paper aligns closely with experimental results."}, "weaknesses": {"value": "1. The theoretical derivation relies heavily on Assumption 3; however, Figure 3 shows that as the batch size increases, Seesaw diverges from cosine scheduling, indicating a fundamental limitation in the method's applicability when this assumption breaks down.\n\n2. The paper lacks comprehensive experimental validation across a broader range of conditions. It evaluates only three medium-scale models on a single dataset and omits comparisons across diverse downstream tasks, additional optimizers, or extensive hyperparameter settings. More extensive experiments would be necessary to convincingly demonstrate the generalization and robustness of the proposed method.\n\n3. The notion of acceleration is measured in terms of reduced serial steps rather than actual wall-clock time. While the paper reports the ratio of achieved serial step reduction to the theoretical limit, it does not provide end-to-end wall-clock evaluations under realistic distributed training scenarios, including cross-node throughput, communication overhead, and memory constraints. Furthermore, extreme members of the proposed equivalence family exhibit instability in practice, raising concerns about their usability."}, "questions": {"value": "Please refer to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OUxbsQZQiS", "forum": "Nj0XBF2o7z", "replyto": "Nj0XBF2o7z", "signatures": ["ICLR.cc/2026/Conference/Submission11880/Reviewer_fg72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11880/Reviewer_fg72"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885190551, "cdate": 1761885190551, "tmdate": 1762922895508, "mdate": 1762922895508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a theoretically grounded approach to batch size scheduling, specifically showing that decay schedules can be replaced by a combination of decay schedules and batch size schedules (e.g. instead of decaying by a factor of 2 you can scale LR by 2 and decay by a factor of sqrt(2)). This allows you to either exploit more hardware or (more realistically) exploit the hardware you have more effectively.\n\nThey conduct small experiments up to 600M \"Chinchilla\" that demonstrate that the theory works out, showing both that their schedule is equivalent and also that a more aggressive schedule (in terms of higher effective LR) won't work, or works less well."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall I think this is a nice paper: It is generally very well written. It's a nice example of trying to deploy actual theoretical insights with practical advice, and I think it's a fairly creative way of going about it. I think one could have gotten to the same conclusions from older SDE theory about Adam LR's relationship to batch size, but it's good to have this new approach too.\n\na nice extension of the theory from Meterez, et al 2025 (and others) coupled with some small but very convincing experiments demonstrating the idea. \n\nThe experiments show that their theory holds up remarkably well.\n\nThis isn't a world-shattering paper but, modulo my concerns, it is a solid contribution and one i'd be keen to test myself."}, "weaknesses": {"value": "It is easy to say that it would be nice to see bigger experiments, but that doesn't feel necessary here.\n\nOne small thing is that the authors appear to have moved their theoretical results to the end of the paper late in the drafting process: assumption 3 is referenced in 3.1 and 4.2 but not defined until section 5. This is confusing but easily remedied.\n\nI think it's a little much to claim a 36% \"wall clock\" speedup since it would need ~proportionally more compute for those phases (or considerably smaller wall clock gains from improved MFU)... I get the point that it's possible to use more compute in those circumstances when you might otherwise be constrained by CBS (though there are other ways to do that too through model parallelism)\n\nI am quite confused by how Lemma 3 is a refutation of the approach proposed by Merrill et al... They're trying to hold effective LR constant. They increase B by \\beta = 2, which would reduce the effective LR by sqrt(2) (consistent with your analysis of NSGD and other prior results with SDE etc.) But they then increase the LR by a factor sqrt(2), and so alpha=sqrt(2), and so the effective LR is held constant? Right?\n\nThe absence of weight decay in the experiments makes sense given its weird interaction with LR, but it's not realistic. In particular, decoupled weight decay means LR impacts steady state of the weight norms, and so you would end up with different results... Does the theoretical analysis transfer to this more realistic setting?"}, "questions": {"value": "Clarification about Lemma 3's relationship to Merrill et al 2025 would be helpful. I feel like I'm missing something.\n\nsome experiments with actual weight decay would be helpful since that is how LLMs are usually trained. \n\nI'm also curious about behavior at sub-CBS (e.g. 150M with B=128).\n\nIs it possible to exploit the empirical observation from Merrill et al 2025 that their measured CBS seems to increase as training progresses? Or is your contention that their estimate of CBS is inherently flawed?\n\nrelatedly, it would be nice to see this holding for the overtrained ( say 4x Chinchilla) regime too. I wouldn't expect to see all scales to a multiple of chinchilla. Given that many real models are overtrained, it would be nice to see. (This may be asking too much and that's ok.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "j0dyTCpolX", "forum": "Nj0XBF2o7z", "replyto": "Nj0XBF2o7z", "signatures": ["ICLR.cc/2026/Conference/Submission11880/Reviewer_r7Qn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11880/Reviewer_r7Qn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11880/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895451692, "cdate": 1761895451692, "tmdate": 1762922894899, "mdate": 1762922894899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General answer to the reviewers"}, "comment": {"value": "We would like to thank the reviewers for their thorough feedback. We have addressed bulletwise the concerns of each reviewer, and we will use this general statement to mention the main changes provided to the manuscript. We have colored the main changes in the manuscript in blue.\n\nIn terms of changes, both experimental and theoretical, we now provide:\n- Experiments showing that for a 150M model trained at 4x Chinchilla, across 256, 512 and 1024 (at sequence length 1024) batch sizes, Seesaw and cosine still have matching final losses\n- Weight decay experiments for 150M in Appendix E\n- Downstream eval results for the 150M models on PIQA, HellaSwag and ARC Easy in Figure 11, Appendix G, showing that cosine and Seesaw achieve similar downstream performance across multiple seeds\n- In Section 5, we have now added the generalized Theorem 1 and Corollary 1 for arbitrary decays (instead of just halving/doubling), as well as the corresponding proof in Appendix A"}}, "id": "XO3NgV2caO", "forum": "Nj0XBF2o7z", "replyto": "Nj0XBF2o7z", "signatures": ["ICLR.cc/2026/Conference/Submission11880/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11880/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission11880/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763528170865, "cdate": 1763528170865, "tmdate": 1763528170865, "mdate": 1763528170865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}