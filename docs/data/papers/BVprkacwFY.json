{"id": "BVprkacwFY", "number": 13404, "cdate": 1758217443879, "mdate": 1759897439979, "content": {"title": "FrontierCO: Real-World and Large-Scale Evaluation of Machine Learning Solvers for Combinatorial Optimization", "abstract": "Machine learning (ML) has shown promise for tackling combinatorial optimization (CO), but much of the reported progress relies on small-scale, synthetic benchmarks that fail to capture real-world structure and scale. A core limitation is that ML methods are typically trained and evaluated on synthetic instance generators, leaving open how they perform on irregular, competition-grade, or industrial datasets. We present FrontierCO, a benchmark for evaluating ML-based CO solvers under real-world structure and extreme scale. FrontierCO spans eight CO problems, including routing, scheduling, facility location, and graph problems, with instances drawn from competitions and public repositories (e.g., DIMACS, TSPLib). Each task provides both easy sets (historically challenging but now solvable) and hard sets (open or computationally intensive), alongside standardized training/validation resources. Using FrontierCO, we evaluate 16 representative ML solvers---graph neural approaches, hybrid neural–symbolic methods, and LLM-based agents---against state-of-the-art classical solvers. We find a persistent performance gap that widens under structurally challenging and large instance sizes (e.g., TSP up to 10M nodes; MIS up to 8M), while also identifying cases where ML methods outperform classical solvers. By centering evaluation on real-world structure and orders-of-magnitude larger instances, FrontierCO provides a rigorous basis for advancing ML for CO.", "tldr": "We evaluate 16 recent machine learning-based combinatorial optimization solver on challenging real-world instances from problems, results revealing the significant limitations of existing methods.", "keywords": ["Combinatorial Optimization", "Graph Neural Networks", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0dc68e197c6c3093e71844baa2cdcccb029f5f6c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a systematic, large scale benchmark for machine learning for combinatorial optimization field. The benchmark collected a wide range of datasets, and standardized training pipelines, and exhibited insightful findings of current ML4CO methods compared to standard solvers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A large scale and wide benchmark makes a lot of sense for ML4CO field. The selection of problems are reasonable. The best known solution is a good guidance for CO practitionerrs. And the findings about current ML vs solvers are important guidelines for future research."}, "weaknesses": {"value": "First of all, there is no clear weakness of the paper. There are some small concerns that I have:\n- The problem instances in table 1 are all collected from previous literature or competitions, which might limit the originality of the benchmark. \n- The standardized training and validation makes sense to some extent, but it may not be fair comparison. e.g., some methods may subsume scaling law and performs better with more training data, while other methods might be suitable for data scarcity but stagnant with more training data. Also, baselines such as GNNs seem incomparable to LLMs, as the modality is totally different. So it is not convincing to me if there exists a standardized training pipeline. \n- I think the deployment difficulty of various methods should also be considered, such as GPU hours, for real world applications."}, "questions": {"value": "- In equation 2, why is the primal gap set to 1 when $cost \\cdot c* < 0$?\n- There also exists other CO benchmarks, e.g., https://github.com/Thinklab-SJTU/ML4CO-Bench-101, so what is your core competitive strength as another CO benchmark paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9rWTSzxwtL", "forum": "BVprkacwFY", "replyto": "BVprkacwFY", "signatures": ["ICLR.cc/2026/Conference/Submission13404/Reviewer_zcmM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13404/Reviewer_zcmM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226500725, "cdate": 1761226500725, "tmdate": 1762924037213, "mdate": 1762924037213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a standardized benchmark for the evaluation of machine learning–based solvers for eight combinatorial optimization problems. It provides a comparison of 16 ML-based solvers against state-of-the-art classical methods. The study offers some insights, revealing both the current limitations and the future potential of ML-based solvers in this domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology of the paper is good and well presented.\n\n2. Developing standardized benchmarks for evaluating ML-based CO solvers is necessary for progress in this field.\n\n3. Proposed benchmark cover routing, graph, location, set, and scheduling CO problems.  However, another scheduling problem would be welcome."}, "weaknesses": {"value": "I believe that some of the claims in this paper are too strong and not supported by the current state of research in this domain.  \n\nIt is true, in general, that many neural solvers rely on attention mechanisms and suffer from well-known attention bottlenecks, and this paper points out that addressing these limitations is an important direction for future research. However, many works have already explored ways to mitigate these bottlenecks, which are entirely overlooked in this work. Some relevant works are listed below.\n\n1. The work by Luo et al. [1] specifically addresses the attention bottleneck problem in routing tasks. It directly challenges the claim in Section 5.1 that neural solvers are still far from being comparable to state-of-the-art classical solvers. In fact, their approach outperforms both LKH3 and HGS on large-scale CVRP instances.\n\n2. The scalability analysis in this work considers only a few (computationally expensive) models and concludes that NCO solvers have problems with scalability. An example in this work is LEHD, and it, as its name implies, relies on a Heavy Decoder architecture, which is very costly. However, there are significantly faster alternatives, for example, the method proposed in [2], and their experiments show that it is over 200× faster than LKH3, with only a minor drop in performance.\n\n3. The claim that most neural solvers are based on GNNs is not entirely accurate. Current SOTA neural solvers primarily employ attention mechanisms rather than GNNs - including the majority of models you evaluate in this work. \n\n4. The discussion and claims about capturing global structure using Euclidean and non-Euclidean problems are based on a single problem -  STP. I am not sure that is enough to conclude the ability of neural solvers to capture the global graph structure.\n\n5. I fully support the claim that most existing evaluations are limited to 2D Euclidean instances. However, unfortunately, this work did not take any steps to address this limitation - the benchmarks still exclude non-Euclidean instances for TSP and CVRP, despite the growing number of recent works focusing on this aspect. Including such instances would significantly strengthen the arguments and make the evaluation more comprehensive, especially given the stated goal of building a “real-world evaluation of NCO.” In practice, all real-world routing problems involve non-Euclidean distances. I therefore suggest adding the instances from [2], or any other real-world datasets, to your benchmark set.\n\n[1] Luo et al. Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems, ICLR 2024\n\n[2] Son et al. Neural Combinatorial Optimization for Real-World Routing, arXiv:2503.16159"}, "questions": {"value": "In addition to my remarks in the weaknesses section, I have the following questions:\n\n1. I did not fully understand your procedure for generating training instances. Let’s take the descriptions of the TSP and VRP training datasets as examples. You first mention that instances are uniformly sampled from a unit square. However, you later state that DIMACS instances with 1,000 nodes are used for the LLM dataset. Does this mean you created two separate datasets - one for “classic” NCO solvers and another for LLM-based methods?\n\n2. Additionally, you mention that 15 validation VRP instances are used “for LLMs,” but there is no information about how many training instances were generated, nor any details about the validation dataset for TSP. Why do TSP instances have 1,000 nodes, while CVRP instances go only up to 500 nodes? Do you consider 15 validation instances sufficient for a reliable evaluation?\n\nI would appreciate further clarification on these points, as well as on the training datasets for the other problems.\n\nAdditionally, I do not fully understand your claim that you provide regenerated training and validation datasets to eliminate inconsistencies found in previous works, and the limitation that existing models are trained on synthetic instances. Your TSP and CVRP datasets appear to be identical to those used in prior works - synthetic instances with node coordinates sampled from the unit square, demands drawn from [1, 9], and capacity set to 50. \n\n3. You mention that an asterisk (*) indicates that the method encountered out-of-memory or timeout issues. In that case, how was the optimality gap computed for those models if they did not produce any solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T489MOoXoF", "forum": "BVprkacwFY", "replyto": "BVprkacwFY", "signatures": ["ICLR.cc/2026/Conference/Submission13404/Reviewer_VRL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13404/Reviewer_VRL2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758837159, "cdate": 1761758837159, "tmdate": 1762924036711, "mdate": 1762924036711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FRONTIERCO, a benchmark for evaluating ML-based combinatorial optimization (CO) solvers under real-world structure and extreme scale. It targets three long-standing limitations in prior NCO evaluations: small “toy” instances, synthetic data that miss structural diversity, and inconsistent baselines/splits. The benchmark spans eight CO problems across routing, facility location, scheduling, graphs, and Steiner trees, drawing test instances from competitions and public repositories (e.g., TSPLib, DIMACS). For training/validation, the authors provide standardized synthetic generators for neural solvers and separate dev sets for LLM agents to avoid leakage; test evaluation is strictly on real-world instances. A unified protocol enforces a one-hour evaluation cap and a scale-invariant primal-gap metric with an explicit infeasibility policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[Problem motivation & clarity]  \nThe introduction tightly argues why synthetic, small-scale evaluations have over-estimated ML performance and motivates a real-world, frontier-scale benchmark. The easy/hard split, data provenance, and scale claims are clearly articulated, making the problem and goals unambiguous.  \n  \n[Benchmark design & practicality]  \nThe benchmark aggregates eight diverse CO tasks with real-world test instances and provides standardized BKS and synthetic training/dev resources. The one-hour cap and hardware normalization (single CPU core for all, single A6000 for neural solvers) produce a pragmatic, comparable setting. The scale-invariant primal-gap definition (including the infeasibility policy) is a strong choice for cross-task fairness.  \n  \n[Breadth and diagnostic depth of evaluation]  \nSixteen ML solvers spanning neural, hybrid, and LLM agents are compared to top classical solvers. Beyond averages, the paper probes why gaps occur: distribution shift from synthetic training to structurally diverse test sets; neural OOM/latency; and LLM variability/safety issues. The STP Euclidean vs. non-Euclidean experiment is a crisp test of “global structure” capture.\n  \n[Insightful, constructive negative results]  \nThe study shows that neural modules can improve weak baselines (e.g., DIFUSCO vs. 2-OPT; GCNN vs. SCIP) yet still trail strong classical SOTA, and it identifies promising pockets where LLM agents surpass classical SOTA (e.g., MIS-easy, CVRP-hard) while cautioning about instability. These balanced findings are valuable for the community."}, "weaknesses": {"value": "1. The paper’s treatment of metric edge cases lacks technical clarity. While the primal-gap policy is defined—including handling of negative or zero costs and infeasible outputs—there are few concrete examples that cover both minimization and maximization settings. This matters because tasks can differ in objective sign and scale, and without worked examples readers may interpret the same numeric gap differently across problems. The gap definition section would benefit from illustrative cases; as is, reproduction and extension to new tasks or cost conventions can be confusing.\n\n2. Reporting granularity is limited, creating questions about numerical consistency. The main text emphasizes aggregate gaps, but offers few per-instance distributions, confidence intervals, or paired significance tests. This makes it difficult to assess the reliability of observed improvements or claimed LLM “wins.” In the results tables and figures, richer dispersion statistics and paired comparisons on shared instances would substantiate robustness and provide clearer selection guidance for practitioners.\n\n3. Transparency around compute and training budgets is insufficient. Although the evaluation setup standardizes a one-hour cap and hardware, the paper provides only limited per-method training details for neural baselines (e.g., epochs, steps, seeds, hyperparameters) and sparse accounting of sampling budgets for LLM agents. This obscures fairness and cost-effectiveness comparisons across approaches. The evaluation and appendices should enumerate these budgets; without them, reproducibility and cost–benefit assessment are hindered.\n\n4. The scope and novelty of LLM-based algorithms are not framed sharply enough. Word-clouds and qualitative analysis suggest that the agents largely recompose known metaheuristics such as simulated annealing and large-neighborhood search. What is missing is an explicit separation between retrieval/recombination and genuinely novel algorithmic invention. In the Discussion (§5.3), clarifying this boundary would calibrate expectations for future “agentic” progress and reduce the risk of overstating algorithmic novelty.\n\n5. Finally, statistical robustness and safety considerations are underdeveloped. The LLM agents exhibit high variance across runs—sometimes generating very different strategies—and can trigger OOM or other resource issues during iterative “evolving.” The paper does not provide a systematic variance/seed study or a resource-safety protocol (e.g., per-sample CPU/GPU caps). These gaps, noted across the results and discussion, leave practical adoption uncertain without clearer guardrails on reliability and operational cost."}, "questions": {"value": "[Clarify and exemplify the metric]  \nProvide a boxed, self-contained definition of the primal-gap for both minimization and maximization tasks, with fully worked examples that cover negative and zero objective values as well as an infeasible output (e.g., explicitly show a case yielding gap = 1 with time = 3600s). Accompany this with a concise table mapping each task to its objective definition and sign convention so readers cannot misinterpret gap magnitudes across problems with differing scales or signs.  \n  \n[Make evaluation and training budgets fully transparent]  \nEnumerate complete training and sampling budgets so cost–performance trade-offs are clear and reproducible. For neural methods, report dataset sources and sizes, epochs/steps, batch sizes, learning-rate schedules, regularizers, number of seeds, total GPU hours, and peak memory. For LLM agents, disclose prompts/templates, sampling parameters (temperature, top-p), number of samples/iterations, wall-clock per sample, and aggregate CPU/GPU hours; include fair wall-clock comparisons to classical solvers under the same evaluation limits.  \n  \n[Strengthen statistical reporting and robustness]  \nReport mean ± standard deviation (or confidence intervals) per test set and run paired statistical tests (e.g., Wilcoxon) on shared instances to substantiate improvements. Release raw per-instance outcomes and add scatter plots (gap versus time), clearly marking infeasible or failed runs. Include sensitivity analyses: for neural models, vary model size and any reduction techniques; for LLM agents, vary the number of samples/iterations and compare tool-use versus no-tool settings.  \n  \n[Probe integration with strong classical components]  \nMove beyond “neural versus weak heuristic” and evaluate “neural + strong heuristic,” for example by plugging neural move-selection into LKH-3 or HGS variants. Measure whether such hybrids consistently lift near-SOTA classical solvers across tasks and scales, thereby clarifying the realistic contribution of learned components.  \n  \n[Expand structure-shift diagnostics]  \nGeneralize the Euclidean/non-Euclidean analysis beyond STP to graph problems such as MIS/MDS (e.g., SAT-induced versus random graphs) and to routing (metric versus non-metric variants). Report performance deltas, learning-curve contrasts, and any systematic failure modes to better characterize how models cope with structural distribution shifts.  \n  \n[Improve reproducibility aids]  \nRelease concise pseudocode for each evaluation harness (neural and LLM), including the hidden evaluation API signature to eliminate ambiguity in adapter interfaces. Publish all seeds, configuration files, and scripts required to regenerate figures and tables from raw outputs. Add a short “gotchas” section documenting environment pitfalls (e.g., Python/BLAS/MKL versions) that can materially affect results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qzOXYSxza6", "forum": "BVprkacwFY", "replyto": "BVprkacwFY", "signatures": ["ICLR.cc/2026/Conference/Submission13404/Reviewer_uR1C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13404/Reviewer_uR1C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957660560, "cdate": 1761957660560, "tmdate": 1762924036326, "mdate": 1762924036326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}