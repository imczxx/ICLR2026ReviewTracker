{"id": "NvKvW5k6Kk", "number": 25552, "cdate": 1758369050458, "mdate": 1763362004646, "content": {"title": "Improving Semantic Proximity in English-Centric Information Retrieval through Cross-Lingual Alignment", "abstract": "With the increasing accessibility and utilization of multilingual documents, Cross-Lingual Information Retrieval (CLIR) has emerged as an important research area. Conventionally, CLIR tasks have been conducted under settings where the language of documents differs from that of queries, and typically, the documents are composed in a single coherent language. In this paper, we highlight that in such a setting, the cross-lingual alignment capability may not be evaluated adequately. Specifically, we observe that, in a document pool where English documents coexist with another language, most multilingual retrievers tend to prioritize unrelated English documents over the related document written in the same language as the query. To rigorously analyze and quantify this phenomenon, we introduce various scenarios and metrics designed to evaluate the cross-lingual alignment performance of multilingual retrieval models. Furthermore, to improve cross-lingual performance under these challenging conditions, we propose a novel training strategy aimed at enhancing cross-lingual alignment. Using only a small dataset consisting of 2.8k samples, our method significantly improves the cross-lingual retrieval performance while simultaneously mitigating the English inclination problem. Extensive analyses demonstrate that the proposed method substantially enhances the cross-lingual alignment capabilities of most multilingual embedding models.", "tldr": "This paper identifies multilingual embedding gaps in cross-lingual retrieval, proposes scenario and Max@R metric, and introduces a training strategy combining JSD and InfoNCE loss, significantly improving cross-lingual alignment with minimal data.", "keywords": ["Cross-Lingual Alignment", "Information Retrieval", "Multilingual Embedding", "Cross-Lingual Information Retrieval"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/affa60edc455651502fa656bfe7244e0d6d0178b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles English‑centric bias and poor cross‑lingual alignment in multilingual retrieval models. When English and non‑English documents coexist, models often favor irrelevant English content. To address this, the authors propose a mixed‑language evaluation setup, a new metric Max@R to measure alignment quality, and a joint training strategy combining Jensen–Shannon Divergence for embedding alignment with InfoNCE for contrastive retrieval learning. Experiments on XQuAD and Belebele show significant performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors identify a critical limitation in existing CLIR models: when retrieving from multilingual document pools, multilingual embedding models tend to prioritize irrelevant English documents while overlooking relevant ones written in the same language as the query.\n- The authors propose a new evaluation metric, Max@R, to measure retrieval performance more effectively in mixed-language document environments.\n- The authors present an optimization approach that jointly uses embedding alignment and contrastive learning, which is straightforward and effective.\n- Experimental results demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "- The construction of alignment samples relies on translations generated by large language models, which may introduce translation bias or noise into the training data.\n- The constructed dataset contains only 2,800 samples, which may lead to potential overfitting of the model.\n- The proposed method is primarily trained with English queries. It is recommended to further explore retrieval performance using queries in a broader range of languages."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YM9pcLeI0m", "forum": "NvKvW5k6Kk", "replyto": "NvKvW5k6Kk", "signatures": ["ICLR.cc/2026/Conference/Submission25552/Reviewer_Mk7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25552/Reviewer_Mk7b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536964972, "cdate": 1761536964972, "tmdate": 1762943470356, "mdate": 1762943470356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses challenges in Cross-Lingual Information Retrieval (CLIR) and Multilingual Information Retrieval (MLIR) tasks where multilingual embeddings often exhibit biases, especially toward English.The authors introduce a new evaluation metric (Max@R) and propose a training strategy combining Jensen-Shannon Divergence (JSD) and InfoNCE contrastive loss to improve the cross-lingual alignment of embeddings. The method demonstrates significant improvements in retrieval performance even with a small dataset of 2.8K samples. Experiments on XQuAD and Belebele datasets across 10 languages and 4 models demonstrate consistent improvements and reduced language bias disparities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- 1. Proposing a cross-lingual embedding optimization method that combines JSD and InfoNCE, which is a novel and reasonable approach;\n\n- 2. The experiment covers multiple metric and text embedding models.The experimental results are sufficient and the analysis of the experimental results is also very convincing.\n\n- 3. Using only 2.8k samples to improve multiple pre-trained models is efficient and suggests the method is data-efficient for practitioners with limited parallel resources."}, "weaknesses": {"value": "- 1. In the methodology section, especially the description of the JSD semantic alignment process is too brief, and readers may need diagrams to understand the idea of distribution level alignment. Equation 6 is motivated only by satisfying distance axioms, but many divergence measures satisfy these properties. The paper does not explain why JSD over, say, Wasserstein distance, Bhattacharyya divergence, or direct embedding-space alignment (e.g., via cosine similarity matching or optimal transport on embeddings) would be superior.\n\n- 2. The lack of a more detailed description of the dataset has affected the intuitiveness of the experimental results. XQuAD and Belebele are QA datasets converted to retrieval by treating passages as documents. This is somewhat artificial; natural retrieval corpora with mixed languages would be more convincing.\n\n- 3. More detailed explanations are needed for some metric, such as numerical ranges, numerical size meanings, etc."}, "questions": {"value": "- Why is JSD in Equation 6 superior to alternative divergence measures for the retrieval task?\n\n- In Table 1, the Comp@10 values range from approximately 0.5 to 93.22 (e.g., multilingual-e5-base En+Zh: 0.50 vs. gte-multilingual-base En+Ar: 87.44). Are these values reported as percentages (0–100 scale), or as proportions (0–1 scale)?\n\n- Additionally, was the result averaged across multiple runs? If so, how many independent experiments were performed, and how were random seeds selected and fixed? Please provide clearer or additional explanations to improve reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2c4DrWlem", "forum": "NvKvW5k6Kk", "replyto": "NvKvW5k6Kk", "signatures": ["ICLR.cc/2026/Conference/Submission25552/Reviewer_gRdR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25552/Reviewer_gRdR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547785565, "cdate": 1761547785565, "tmdate": 1762943470159, "mdate": 1762943470159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new training strategy to enhance cross-lingual alignment in information retrieval, addressing the issue of language bias and semantic misalignment in multilingual document pools. The authors propose a method that combines Jensen-Shannon Divergence (JSD) and InfoNCE losses to improve semantic alignment and retrieval performance. By aligning semantic embeddings across languages and optimizing retrieval capabilities, their approach significantly reduces language bias and improves retrieval accuracy. Experiments across multiple languages and datasets demonstrate the effectiveness of the proposed method, showing substantial improvements in cross-lingual retrieval performance and reduced bias, thus providing a robust solution for enhancing semantic proximity in multilingual information retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of this work is relatively clear.\n2. The experimental validation in this work is relatively comprehensive."}, "weaknesses": {"value": "1. In the section “Introduction”, why does this paper deliberately emphasize English rather than other languages?\n\n2. In the section “Related work”, ① The description \"However, most prior studies assume either purely monolingual or entirely multilingual document pool, thus failing to adequately address biases and misalignments that arise when two languages coexist in a single document pool.\" seems ambiguous. The focus of this paper should also be on multilingual issues, and I am not clear about the differences between the current description and the methods mentioned. ② The descriptions \"While insightful, these works primarily emphasize alignment in general representation tasks, offering limited consideration of practical retrieval challenges associated with combined-language environments\" are vague and inaccurate, and the authors are advised to clarify them. ③ I also noticed that some works [1][2] seem similar to this paper, but they are not introduced or reviewed. What are the differences between [1] and this paper?\n\n3. In the section “Method”, how is the positive sample pair defined?\n\n**Reference**\n\n[1]  Zuo Y, Jiang W, Liu W, et al. Alignxie: Improving multilingual information extraction by cross-lingual alignment[J]. arXiv e-prints, 2024: arXiv : 2411.04794.\n\n[2]  Kargaran A H, Modarressi A, Nikeghbal N, et al. MEXA: Multilingual evaluation of English-centric LLMs via cross-lingual alignment[J]. arXiv preprint arXiv:2410.05873, 2024."}, "questions": {"value": "Please refer to the “Weakness” section for related questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8lU581xENG", "forum": "NvKvW5k6Kk", "replyto": "NvKvW5k6Kk", "signatures": ["ICLR.cc/2026/Conference/Submission25552/Reviewer_dUrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25552/Reviewer_dUrW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913978998, "cdate": 1761913978998, "tmdate": 1762943469990, "mdate": 1762943469990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the paper, the authors revisit the Cross Lingual Information Retrieval (CLIR) task through the lens of cross-lingual alignment evaluation. They argue that in a CLIR setup, the English unrelated documents, for a given query, are prioritized in the ranked list when compared with the related documents in the same language as the query. To alleviate this, they have proposed both new metrics and training setups, testing on standard benchmarks. The results show that the proposed setup achieves better semantic alignment in the CLIR task across many languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is clearly described. The formal presentation of the problem premise in Section 2 makes it easy to comprehend.\n\n* The demonstration through Performance Disparities, esp. for Chinese, is insightful.\n\n* The authors show semantic Representation Instability among languages. They show that the performance is worse for Arabic and Thai when compared with Spanish.\n\n* The authors also show high retrieval ranks for many languages. \n\nThese three key findings establish the motivation for a better setup.\n\nBased on the premise, the authors propose semantic alignment techniques to ensure better alignment between the query and the relevant English document. This is proposed by the application of InfoNCE between the query and the relevant document. JSD is deployed to minimize semantic differences between embedding vectors.\n\nThe authors report experiment results on datasets: XQuAD and Belebele using metrics like Complete@K. In total, 10 languages have been considered.\n\nThe results show increased cross-lingual alignment, showing the efficacy of the proposed training setup. \n\nIn section 5.4, the superior performance over LNCE is reported.\n\nMost of the standard embeddings, like multilingual-e5, gte-multimodal, bge, etc., have been considered."}, "weaknesses": {"value": "It is not clear why the existing metrics, like MAP, can not be used for CLIR evaluation. The new metric Max@R, needed a clearer comparison with existing IR evaluation platforms in the CLIR setup. The authors need to establish better why a new metric was needed and how the existing metrics (even with customization for the premise of the paper) were inadequate.\n\nThe setup \"We report the performance achieved when querying in each language within these environments.\" (line 159-160) need more details for reproducibility and because, importantly, the claims in this section hinge on the reliability of these results. I will be curious to also look into word-overlap-based and yet effective baselines like BM25 in the CLIR setup, empowered with translation resources like https://github.com/facebookresearch/fairseq/tree/nllb (No Language Left Behind: Scaling Human-Centered Machine Translation).\n\nThough a little older, I would still suggest comparison with Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and other multilingual variants)."}, "questions": {"value": "The motivation for a new metric needs clearer motivation.\n\nMore baselines, as already suggested, will further fortify the claim of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DLPOiIZL9l", "forum": "NvKvW5k6Kk", "replyto": "NvKvW5k6Kk", "signatures": ["ICLR.cc/2026/Conference/Submission25552/Reviewer_oW6S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25552/Reviewer_oW6S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994839877, "cdate": 1761994839877, "tmdate": 1762943469587, "mdate": 1762943469587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}