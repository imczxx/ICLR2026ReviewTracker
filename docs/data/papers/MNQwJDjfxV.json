{"id": "MNQwJDjfxV", "number": 11108, "cdate": 1758189440554, "mdate": 1763708315683, "content": {"title": "Learn to Select Node in Branch and Bound with Causality Modeling", "abstract": "Learning-based approaches have shown strong promise in Branch and Bound (BnB) node selection by using offline data. They typically model \\textit{correlations} from node features to node quality, selecting nodes based on predicted quality. However, this correlation modeling may encode spurious patterns rather than the true decision rationale. For example, it may associate node quality with lower bounds; but a node with lower bounds is not necessarily better due to overestimated relaxation, illustrating how this feature-level signal misleads decision. The true decision rationale lies in whether a node contains an optimal solution. To this, this paper proposes modeling the \\textit{causal} effect of optimal solution presence on node selection, moving beyond correlation modeling. We define the causal signal by BnB's optimality transitivity: if a node contains an optimal solution, then its parent must also contain that solution; consequently, optimal nodes tend to resemble their parents in feature representation. We implement this by contrastive learning, treating parent-child node pairs in which both nodes contain an optimal solution as positive samples and other pairs as negative; training the model to distinguish nodes containing an optimal solution from those do not. This enables learning intrinsic node representations centered on optimality, free from spurious correlations. Experiments show that our method significantly outperforms correlation-based approaches in efficiency, robustness, and generalization, achieving near-expert performance under limited data and distribution shift.", "tldr": "We propose a causality-based approach for Branch-and-Bound node selection, learning intrinsic signals of optimality beyond spurious correlations.", "keywords": ["node selection; branch and bound; mixed integer linear programming"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3ae05225a1a75252ceb7bb9801667d4336d46d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a causality-based framework for node selection in Branch-and-Bound (BnB) for MILP solving. The key idea is to model the causal effect of optimal-solution presence on node selection, leveraging the transitivity of optimality as a causal signal. The method is implemented via contrastive learning, where parent-child node pairs containing optimal solutions form positive samples. Experiments on several MILP benchmarks and real-world datasets demonstrate improvements over heuristic and learning-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Motivation and Strong Intuition:** The paper addresses an important weakness of correlation-based learning for BnB—its susceptibility to spurious patterns—and proposes a theoretically grounded causal approach.\n\n2. **Theoretical Analysis:** The causal formulation is carefully derived using Pearl’s do-calculus, giving the paper a solid theoretical foundation.\n\n3. **Comprehensive Experiments:** The paper provides extensive comparisons on synthetic and real-world MILP datasets, demonstrating consistent improvements and statistical significance.\n\n4. **Interpretability and Conceptual Novelty:** The idea of using “optimality transitivity” as a causal supervision signal is elegant and provides interpretability that prior works often lack.\n\n5. **Strong empirical generalization:** Transfer experiments show scalability to larger instances, which is uncommon in learning-based BnB studies."}, "weaknesses": {"value": "1. **Writing and Clarity Issues:** The writing is uneven and could benefit from proofreading. There are grammatical mistakes (e.g., in the abstract, *“those do not”* should be *“those that do not”*). The abstract feels overly dense and could better highlight the core innovation. Section 3.1's theoretical motivation jumps into equations without sufficient intuition, making it hard for non-experts in causal inference to follow. Additionally, the appendix is referenced heavily (e.g., for related work on contrastive learning), but without it, the main text feels incomplete. The paper would also benefit from an overview figure to visually summarize the proposed framework and clarify the overall workflow.\n\n2. **Innovation:** The core idea—using contrastive learning on ancestor-child pairs to capture similarity—bears resemblance to existing graph contrastive learning methods (e.g., GraphCL or SimCLR adaptations for trees/graphs). While applying causality to BnB is novel, the paper doesn't sufficiently differentiate from prior works, which also use pairwise comparisons. \n\n3. **Experimental Limitations and Questionable Claims:** The experimental section lacks comparisons with stronger and more recent GNN-based baselines—the latest included baseline appears to be from 2022. Generalization claims (e.g., under distribution shift) are asserted but not explicitly tested (e.g., via cross-dataset transfer). The \"near-expert performance under limited data\" in the abstract seems exaggerated, as baselines like SCIP often match or exceed on certain metrics (e.g., GISP wins). \n\n4. **Reproducibility Concerns:** The code link is anonymized, which is fine for blind review, but details like hyperparameters (e.g., GNN architecture beyond \"bipartite graph encoder\") are deferred to the appendix."}, "questions": {"value": "1. **Innovation:** Is the \"causal signal\" truly causal, or is it just a structural prior (transitivity) rebranded? The back-door adjustment is invoked theoretically, but the implementation doesn't explicitly control for confounders (e.g., node depth or LP bounds), raising doubts about whether it fully achieves causal identifiability. A deeper comparison to causal ML methods (e.g., counterfactual learning in RL) would strengthen the claim.\n\n2. **Experimental Limitations:**  Important ablations are also missing: for instance, how sensitive is performance to the temperature parameter τ in the InfoNCE loss? What if data augmentation is removed—does the method still hold under truly limited data? \n\n3. **Reproducibility Concerns:** Metrics like \"wins\" are defined vaguely—how are ties handled? More transparency on data splits (train/test sizes) would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PI6cW8Sk12", "forum": "MNQwJDjfxV", "replyto": "MNQwJDjfxV", "signatures": ["ICLR.cc/2026/Conference/Submission11108/Reviewer_dUgo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11108/Reviewer_dUgo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761266365641, "cdate": 1761266365641, "tmdate": 1762922284147, "mdate": 1762922284147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for selecting nodes in a branch-and-bound process by using the fact that optimality is transitive. This means that optimal nodes are similar to their parents in the sense that both contain the optimal point. The method uses this by training a contrastive objective that ensures the feature representations of optimal nodes are more similar to ancestor nodes than suboptimal ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well written and the motivation is clear. Further, the use of contrastive ML for branch and bound is generally novel."}, "weaknesses": {"value": "The paper has two main limitations: \nFirst, while the contrastive training does perform well, I do not think that this is due to the causal connections. Fundamentally, the argument behind the causal link inside optimal nodes is the transitivity of Points inside the sub-polytope. While this is obviously true, it is not necessarily true for the features used to represent the polytope: There is no fundamental relationship between the GNN representation and the children on a feature level, which is why this method needs to find those embeddings with a contrastive method in the first place. The core causal analysis idea relies on circular logic: \n1. Causality yields that ancestors and their optimal children have similar features \n2. Therefore, we train a contrastive model to ensure that ancestors and optimal children have similar features\n3. We observe that for those features causality holds\n\nIf you take a step back and look at what this method does, one can see that this method groups nodes in the optimal path into one cluster (or multiple clusters) and everything else into others. This is just a huge classifier with variable numbers of classes, which is exactly what the InfoNCE loss does. You don’t need to invoke causality for this (arguably this is the same idea as what Zhang et al. and Mattick et al. do implicitly using RL: They assign probabilities/Q-values for every path by following the branch and bound tree.). \nMind you, this not being causal is not a problem in isolation: Having a better way of getting node embeddings is not a bad thing, but I highly doubt that this is due to the exploitation of causal structures.\nFundamentally, I don’t believe that this method actually manages to get at the causal core of the problem. For instance, I don’t see any way to correct for the fact that the contrastive triplets where found using an existing diving oracle. It is very possible that this method implicitly solves an imitation learning problem like e.g. Labassi et al.\n\n\nThe second problem is regarding benchmarking. The instances used for benchmarking are tiny. Studying prior work this seems to be the case for many other methods as well, but this nevertheless makes me doubt the ability for  this model to scale to larger instances. For instance comparing against SCIP’s default node selector on such tiny instances is not reasonable since one of the prime considerations for SCIP is the speed at which the node selector can run. This complexity has already killed plenty of “smart” node selectors in the past (e.g. in the classical selector world “Ashish Sabharwal and Horst Samulowitz Guiding Combinatorial Optimization with UCT (2011)” was a research dead-end since it collapses at scale), so it is important to also test large instances.\nFurther, one should test a single trained model on all instance types. As far as I understood, you train a separate model on every instance type, but this yields unsound comparisons against e.g. SCIP which has to operate well on all problems. If you wanted to fairly compare “instance type for instance type” I think you would need to compare against dedicated e.g. TSP or MAXSAT solvers; after all, if you know that you will only be solving TSP then you would use a TSP solver instead of a general branch-and-bound method.\nAdditionally, you run your node selector on GPU while normal node selectors (and I assume baselines like SVM) run on CPU. You should run your own method on CPU as well, otherwise you are effectively comparing runtimes on two different systems.\n\nThere are some smaller things to note as well:\n- The bold markings in the tables are inconsistent: Sometimes the best node and time is highlighted, sometimes only the nodes (e.g. table 2) and sometimes neither (e.g. table 3, 5, 6, 7)\n- In the conclusion section (line 463) you also mention interpretability: This is not discussed anywhere before and – in my opinion -  also not really supported by the methods (How interpretable are high dimensional similarity comparisons really?)\n- Comparing solving times/node count for datasets that cannot be fully solved is dangerous: You have to only compare on the subset that was solved by all methods. Otherwise you will “reward” methods that give up on slow instances"}, "questions": {"value": "See bove."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZAYdCl02A", "forum": "MNQwJDjfxV", "replyto": "MNQwJDjfxV", "signatures": ["ICLR.cc/2026/Conference/Submission11108/Reviewer_YXQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11108/Reviewer_YXQf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460550615, "cdate": 1761460550615, "tmdate": 1762922283472, "mdate": 1762922283472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a representation learning-based approach (CausalM) for node selection within branch-and-bound. Node selection policies govern the exploration of the branch-and-bound search tree by recommending relaxations to consider next at any stage of the solution process. Traditionally, MIP solvers employ policies based on simple greedy hueristics; this work belongs to a line of recent research that explores various deep ML approaches for node selection.\n\nFundamentally, the authors argue that prior ML-based approaches, which extract training data from solver traces, suffer from sampling bias because these traces tend to favor nodes with particular features. As a result, the authors frame node selection as a causal decision-making problem, where interventions (expanding a particular active node) influence the distribution of whether the selected node contains on optimal solution in the current solve state. The paper invokes causal reasoning mainly as a conceptual framing and motivation rather than through an explicit causal model, introducing an ML architecture that directly predicts the probability that a node contains an optimal solution as a proxy for causal estimation. The efficacy of the base approach, and an augmented approach that is more data-efficient, are both demonstrated via experiments on MIP benchmarks considered in prior work [Labassi et al. 2022]."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel conceptual framing of node selection as a causal decision-making problem, directly addressing the bias that arises when relying on MIP solver traces for supervision signals in ML approaches to node selection.\n2. Empirically, CausalM is a marked improvement over the chosen ML-based baselines for node selection, consistently outperforming these methods in at least one of solve time, number of problems solved to optimality, or generated tree size (and often all three).\n3. The CausalM architecture and experimental setup are both clearly described in a high level of detail, enabling reproduction."}, "weaknesses": {"value": "1. While a causal approach is well-motivated, the connection to the proposed representation learning architecture is unclear. Namely, this approach does not actually estimate the causal target (which would require defining a causal graph or structural equations).\n2. There is no evidence provided (experimental or theoretical) to support the claim that the proposed ML approach achieves a lower confounding gap than prior correlation modeling approaches.\n3. The experiments do not include a comparison to state-of-the-art RL-based baselines (e.g. Zhang et al. 2025), which generally outperform the supervised learning approach of Labassi et al (2022)."}, "questions": {"value": "Could the authors elaborate on how CausalM’s learning pipeline would not also be considered “correlation modeling?” Additionally, I would appreciate further clarification on the concerns raised in the weaknesses.\n\n#### **Other comments**\n* Theorem 1 is an immediate property of the branch-and-bound tree construction and would be better presented as an observation than a contribution.\n* Page 1: “dominate” $\\mapsto$ “dominant”\n* Page 3: no space after “Recent work has also explored reinforcement learning”\n* Page 4: it is not clear what part of equation (7) is coming from Kallus and Zhou (2021); the line reads as though their paper introduced TV distance.\n* There is a significant amount of repeated information between tables 1/3 and 4/5.\n* Given that there is no causal estimation, terminology like “causal linkage,” “causal signal,” “causal supervision,” etc. are ambiguous."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "42l9toGVya", "forum": "MNQwJDjfxV", "replyto": "MNQwJDjfxV", "signatures": ["ICLR.cc/2026/Conference/Submission11108/Reviewer_LKJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11108/Reviewer_LKJB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900122116, "cdate": 1761900122116, "tmdate": 1762922282924, "mdate": 1762922282924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of learning a node selection rule for use in Branch and Bound solvers for mixed integer linear programs (MILPs).\nRoughly speaking, these solvers construct a tree that recursively partitions the original MILP problem into subproblems.\nEach node in this tree corresponds to a subproblem specified by an additional set of constraints, one introduced at each edge along the path to that node.\nOn each iteration, the BnB algorithm selects one of the nodes in the current tree to expand into new children nodes by imposing disjoint bounds on a fractional variable in the optimal solution to the LP relaxation at that node.\nTraditional BnB implementations use hand-designed heuristics for node selection, but a recent line of work has explored the idea of learning node selection policies that perform well for a given distribution over problem instances.\n\nThis paper argues that most prior work on learning node selection rules only learn correlations between node features and selection quality and therefore fail to control for latent confounding variables.\nInstead, motivated by causal modeling, this paper proposes a new approach to learning node selection policies that has a significantly different flavor from prior work: they learn a similarity function that takes as input a node and its ancestor where the learning objective is to make (node, ancestor) pairs that contain an optimal solution similar, and other pairs dissimilar.\nThe key observation that this technique exploits is the fact that if one node of the tree contains an optimal solution to the original MILP, then so do all of its ancestors.\n\nThe authors implement this approach using contrastive learning.\nEach node is embedded by a bipartite graph encoder with learned parameters, and then the similarity between two nodes is measured by the cosine similarity of their embeddings.\nThe parameters of the encoder are learned via InfoNCE.\n\nFinally, the authors conduct an extensive empirical evaluation of their proposed method and several baselines on several MILP benchmark and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem of learned node selection rules in BnB MILP solvers is an interesting problem with many applications.\nThe proposed method is significantly different from the prior work and the experimental evaluation shows that it performs well."}, "weaknesses": {"value": "I had a hard time understanding some of the core motivation of the paper, but this could be because I am relatively unfamiliar with causal analysis. I think the paper would be significantly improved by including additional discussion of why focusing on learning a similarity measure between nodes and their ancestors is consistent with a causality-motivated approach, while learning to predict scores from node features is not."}, "questions": {"value": "1. In Section 3 the discussion revolves around the conditional distribution of $y$ given that node $v$ is selected at step $t$ when the search tree at step $t$ is equal to $s_t$. What is the joint distribution across these variables? I was guessing that there is a distribution over MILP instances which are then solved using a baseline BnB implementation, and the nodes present in the trees produced by that solver are the ones that appear in the distribution. There is some discussion of this in Appendix A.2.1, but I didn't quite follow. It would be helpful to be a bit more explicit about what this distribution corresponds to. I had trouble following precisely some of the theoretical motivation.\n2. I don't quite understand the connection between optimality transitivity and causality. I agree that if one node contains an optimal solution then all of its ancestors must too. But it is less clear to me why this implies that learning a similarity function as proposed is accounting for latent confounding variables. It could be worth giving some intuition for the proof of Proposition 1.\n3. What is the oracle baseline in the experiments? Is it the result of selecting nodes in BnB in such a way that the size of the tree is minimized?\n4. It seems like when learning the similarity function, we might want to use the collection of all optimal solutions to the MILP when deciding the positive and negative examples. Does the data generation process use all optimal MILP solutions? If not, do we run the risk of learning bad similarity functions because we assign a label of 0 to some (node, ancestor) pairs where both contain an optimal MILP solution, just not one that we know of?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4x5JhGu14a", "forum": "MNQwJDjfxV", "replyto": "MNQwJDjfxV", "signatures": ["ICLR.cc/2026/Conference/Submission11108/Reviewer_LopH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11108/Reviewer_LopH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942655594, "cdate": 1761942655594, "tmdate": 1762922282183, "mdate": 1762922282183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}