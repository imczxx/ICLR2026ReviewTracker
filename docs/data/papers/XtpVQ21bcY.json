{"id": "XtpVQ21bcY", "number": 23989, "cdate": 1758351412138, "mdate": 1759896787607, "content": {"title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models", "abstract": "Vision-Language Large Models (VLLMs) faces significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache—thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 × KV cache compression and 3.16 × prefill acceleration, with negligible quality degradation. By seamlessly integrating with sparse attention optimization, our work unlocks scalable deployments for real-time multimodal applications.", "tldr": "", "keywords": ["Visual Language Models", "KV Cache", "Spatial-temporal Sparse Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/023035ab4c3203bd3741734680f3d1ed638c9f53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that previous sparse attention and KV cache methods are incompatible with the FlashAtten kernel and largely static. To address this, the authors propose a dedicated sparse attention mechanism that purifies spatial noise and reduces temporal redundancy in the KV cache."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The decomposition of spatial and temporal redundancy is good."}, "weaknesses": {"value": "I have several concerns regarding the claims and evaluations in this paper:\n\n1. Adaptability of attention-based methods\nThe authors claim that existing attention-based methods cannot adapt to FlashAtten. However, this statement is not entirely accurate. As far as I know, several existing approaches employ block-wise probing strategies that efficiently obtain attention scores. Examples include NSA, MoBA, SeerAttention, and ZipVL.\n\n[1] Hardware-Aligned and Natively Trainable Sparse Attention\n\n[2] MoBA: Mixture of Block Attention for Long-Context LLMs\n\n[3] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs\n\n[4] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification\n\n2. Static vs. dynamic sparsity\nThe paper further claims that current methods are static. This characterization is misleading, as it mainly applies to Top-k based methods. In contrast, Top-p based dynamic KV methods have already been explored, such as Twilight and ZipVL, which demonstrate dynamic sparsity in practice.\n\n[1] Twilight: Adaptive Attention Sparsity with Hierarchical Top-p Pruning\n\n[2] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification\n\n3. Lack of comprehensive comparisons\nThe experimental evaluation is insufficient. The method is only compared against StreamingLLM and H2O, which does not establish a strong case for state-of-the-art performance. More comprehensive benchmarks should be included, such as VideoMME, LongVideoBench, PerceptionTest, ActNet-QA, and NextQA. In addition, evaluation on prolonged video cases exceeding 20k context length is necessary. More extensive experiments on LLaVA-Video are also expected.\n\n4. Questionable necessity of cross-layer estimation\nThe proposed cross-layer estimation strategy seems unnecessary. Existing methods (e.g., SeerAttention) already adopt fine-grained token selection to skip computation without discarding tokens. This avoids excluding potentially important information, whereas cross-layer dropping may risk compounding information loss from earlier layers."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f94v8efzZv", "forum": "XtpVQ21bcY", "replyto": "XtpVQ21bcY", "signatures": ["ICLR.cc/2026/Conference/Submission23989/Reviewer_xK7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23989/Reviewer_xK7L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760488166451, "cdate": 1760488166451, "tmdate": 1762942887965, "mdate": 1762942887965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PureKV, a plug-and-play framework for Vision-Language LLMs that jointly optimizes KV-cache compression and attention sparsity, remaining compatible with efficient kernels (e.g., FlashAttention/sparse attention). Core to the method is a cross-layer importance estimator that reuses shallow-layer attention scores and weights them by the L2 norm of deep-layer V vectors to select salient past tokens without computing deeplayer attention matrices. In addition, the authors designed a Spatial-Temporal Sparse Attention (ST-SpAttn) mechanism that purifies KV cache by suppressing spatial background noise and temporal redundancy. Experiments on VideoLLaMA2 and Qwen2.5-VL report up to 5x KV-cache compression and 3.16x prefill acceleration with small quality degradation, plus ablations and rank-correlation evidence supporting the cross-layer estimator. Overall, the work targets practical, system-aware inference gains for video VLLMs by coupling cache selection with sparsity design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and easy to follow. It introduces a genuinely plug-and-play KV-cache framework that remains compatible with high-performance attention backends (e.g., FlashAttention). A lightweight cross-layer importance estimator—reusing shallow-layer attention and weighting deep-layer V by its L2 norm—efficiently ranks tokens, preserving speed while selecting the most salient cache entries."}, "weaknesses": {"value": "The proposed method requires obtaining attention scores for KV-cache importance estimation, which appears incompatible with FlashAttention, as the latter does not explicitly compute or expose attention matrices."}, "questions": {"value": "1. The explanation in Section 3.2 is somewhat unclear. The authors introduce Spatial-Temporal Sparse Attention (ST-SpAttn), claiming that it suppresses background noise and irrelevant visual distractions. However, the paper does not clearly explain why or how ST-SpAttn achieves this effect compared to full attention. Could the authors elaborate on the underlying mechanism and provide experimental evidence or visualizations to support this claim? Moreover, the section relies on qualitative descriptions without mathematical formulations or equations, which makes the method difficult to fully understand. \n\n2. In Section 3.2, the authors state that existing KV-cache pruning strategies overlook structural modifications—specifically, that the KV cache at position j in layer i aggregates information from the first j + 1 tokens in layer i − 1. However, this issue seems straightforward to address by simply retaining the original position IDs. Could the authors clarify why this is considered a significant challenge and explain why preserving position information alone would not resolve the problem? \n\n3. The experimental details are insufficient. For instance, the authors state that the proposed method reuses shallow-layer attention scores for deeper layers, but it remains unclear which specific shallow layers correspond to which deep layers. Clarifying this mapping or providing an illustrative example would help improve the reproducibility and understanding of the method. \n\n4. The authors only present results on multimodal video understanding benchmarks, which is insufficient to demonstrate the generality of the proposed method. It would strengthen the paper to include additional experiments on image understanding benchmarks, such as VQAv2 [1] and ChartQA [2]. \n\n5. In Table 4, the results for the full-cache baseline are missing, making it difficult to assess the absolute performance and relative improvements of the proposed method. \n\nReference: \n\n[1] Making the v in vqa matter: Elevating the role of image understanding in visual question answering. CVPR 2017. \n\n[2] ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. ACL 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AygnSZqt93", "forum": "XtpVQ21bcY", "replyto": "XtpVQ21bcY", "signatures": ["ICLR.cc/2026/Conference/Submission23989/Reviewer_vfdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23989/Reviewer_vfdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455632759, "cdate": 1761455632759, "tmdate": 1762942887689, "mdate": 1762942887689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a KV cache compression strategy and an attention module for video KV cache algorithms. The authors use several experiments to show the feasibility. Specifically, they reported that PureKV can achieve 5× compression and 3.16× prefill acceleration on VLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea makes sense, combining two modules together for video KV cache compression with theoretically grounded cross-layer correlation analysis.\n- The results are convincing, although more diverse video understanding tasks would strengthen the evaluation.\n- The method achieves genuine compatibility with modern attention accelerators."}, "weaknesses": {"value": "- The core techniques (attention-based importance scoring and sparse attention) are not novel individually; more critically, the selection of which layers to apply CLIE versus ST-SpAttn appears empirically driven rather than theoretically justified. A principled framework for determining optimal layer assignments would strengthen the contribution.\n- While the authors briefly mention audio-visual experiments in the appendix (AVSD dataset), these results deserve fuller integration into the main evaluation. Testing on multimodal models like the Qwen2-Audio or Qwen2-Omni series would better demonstrate generalizability across modalities.\n- The non-monotonic relationship between cache budget and performance (some tasks showing better results at 10% than 20% budget) warrants deeper investigation. Analyzing the distribution of retained tokens and their semantic properties could reveal why aggressive pruning sometimes outperforms moderate compression.\n- The evaluation focuses on single-query scenarios, but real-world video understanding often involves multi-turn dialogue. Demonstrating how PureKV handles iterative questioning about the same video content and whether cache can be effectively reused across queries would significantly strengthen practical applicability claims."}, "questions": {"value": "- Is there theoretical justification for the layer selection logic?\n- Why do some tasks perform better at 10% cache budget than 20%? What tokens are being retained/dropped that cause this? A more detailed analysis may help.\n- How does PureKV handle sequential questions about the same video? Can KV cache be reused across queries?\n- Please consider adding more experiments, such as LongVideoBench and VideoMME, and indicate whether the method can be generalised well to the audio or audio-visual domain (for example, by testing on Qwen-3 Omni)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RsFmsUmaTP", "forum": "XtpVQ21bcY", "replyto": "XtpVQ21bcY", "signatures": ["ICLR.cc/2026/Conference/Submission23989/Reviewer_ieie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23989/Reviewer_ieie"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597807264, "cdate": 1761597807264, "tmdate": 1762942887456, "mdate": 1762942887456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PureKV, a plug-and-play framework for the joint optimization of sparse attention and KV cache compression. PureKV features a lightweight token importance estimator that utilizes lower-layer attention scores and the L2 norm of value vectors to assess the importance of high-layer KV cache entries, ensuring compatibility with efficient attention mechanisms. Additionally, it presents a novel Spatial-Temporal Sparse Attention (ST-SpAttn) module tailored for video tasks, which purifies the KV cache by reducing spatial noise and temporal redundancy. Extensive experiments show that PureKV achieves up to 5× KV cache compression and 3.16× prefill acceleration, with negligible impact on quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The cross-layer importance estimation using lower-layer attention scores and value vector norms is both novel and empirically validated.\n2. The introduction of ST-SpAttn for video tasks is well-motivated and demonstrates clear benefits in both cache purification and acceleration.\n3. The experiments cover multiple VLLMs, tasks, and cache budgets, showing consistent improvements"}, "weaknesses": {"value": "1. Figure 1(c) could be further improved to more clearly illustrate the differences between dense attention and ST-SpAttn. \n2. While the empirical results are strong, the theoretical justification for why lower-layer attention scores are sufficient for high-layer importance estimation could be further strengthened, for example by providing more formal analysis or theoretical bounds.\n2. It would be beneficial to include an analysis of PureKV’s computational overhead in the ablation study. \n3. H2O and StreamingLLM are not the strongest baselines; please consider including more recent methods [AdaKV](https://arxiv.org/pdf/2407.11550), [PyramidKV](https://arxiv.org/pdf/2406.02069), [HeadKV](https://arxiv.org/pdf/2410.19258?) and [PruneVID](https://arxiv.org/pdf/2412.16117) for a more comprehensive comparison.\n4. The overall writing quality could be improved for greater clarity and readability."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gnavJp2Gfq", "forum": "XtpVQ21bcY", "replyto": "XtpVQ21bcY", "signatures": ["ICLR.cc/2026/Conference/Submission23989/Reviewer_n7gx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23989/Reviewer_n7gx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976215675, "cdate": 1761976215675, "tmdate": 1762942887108, "mdate": 1762942887108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}