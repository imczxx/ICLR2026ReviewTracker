{"id": "MGg4ymeH7R", "number": 4092, "cdate": 1757598837383, "mdate": 1759898053372, "content": {"title": "Dual-Phase Whitening for Test-Time Adaptation", "abstract": "When deploying machine learning models in real-world scenarios, a key challenge is distribution shift where test data differs from the training distribution, often degarding model performance. This problem is particularly challenging in test-time adaptation (TTA), where the model must adapt to unlabeled target data without access to source data or labels. To address this problem, we introduce a novel approach to facilitate target feature learning by utilizing dual-phase whitening (DPW) in connected with whitening Batch Normalization (WBN) and whitening contrastive learning schemes (WCL). WBN operates at the feature transformation level to enforce isotropic feature distributions by ZCA whitening, thereby reducing model dependence on domain-specific covariance structures and improving stability under distribution shifts. WCL extends standard contrastive learning by incorporating global feature whitening, which eliminates redundant feature correlations while enforcing a hyperspherical distribution that better preserves semantic relationships. By the dual-phase whitening, WBN handles low-level feature standardization while WCL optimizes global representation geometry. Thus, we can obtain more generalized features from dual-phase whitening. Our method achieves state-of-the-art performance on major benchmarks including VisDA-C, DomainNet-126, ImageNet-C and CIFAR-100C have several advantages over existing works.", "tldr": "DPW", "keywords": ["Test-Time Adaptation", "Dual-Phase Whitening", "Whitening Batch Normalization", "ZCA Whitening"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efd876c9c0a31803f9b550711823a5ab721d20bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the critical problem of distribution shift in real-world machine learning deployments, focusing on the test-time adaptation (TTA) setting where the model must adapt to unlabeled target data without access to source data. The authors propose a novel Dual-Phase Whitening (DPW) framework that jointly integrates Whitening Batch Normalization (WBN) and Whitening Contrastive Learning (WCL) to improve model robustness and generalization under distribution shifts. In the first phase, WBN performs feature-level ZCA whitening, enforcing isotropic representations and reducing sensitivity to domain-specific covariance structures. In the second phase, WCL introduces a global feature whitening mechanism within a contrastive learning framework, promoting decorrelated, hyperspherical feature distributions that preserve semantic consistency. Experiments on standard domain generalization and corruption benchmarks (VisDA-C, DomainNet-126, ImageNet-C, CIFAR-100C) demonstrate that the proposed DPW framework outperforms prior TTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses a well-known challenge—distribution shift in test-time adaptation and domain generalization. The idea of applying whitening to both normalization and contrastive learning is intuitive and aligns with recent trends toward decorrelated feature representations.\n\n2. The proposed approach is relatively easy to implement and integrates cleanly with standard frameworks like ResNet.\n\n3. The paper evaluates on multiple datasets, which provides reasonable empirical validation of the proposed approach.\n\n4. Some novelty in combining whitening techniques."}, "weaknesses": {"value": "1. Experiment results are not significant. Many competing methods are from 2019-2021. Performance improvements are often marginal, and sometimes under-perform than competing methods.\n\n2. Novelty is limited as the core components of this paper, whitening normalization and contrastive whitening, are well-established techniques."}, "questions": {"value": "1. Comparing with Contrast Test-Time Adaptation (AdaContrast) Chen et al. (2022), what is the exact difference/improvement? Please justify.\n\n2. Any theoretical insights on the proposed approach?\n\n3. How sensitive is the method to batch size especially under small target batches at test time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sbNxB6U2KG", "forum": "MGg4ymeH7R", "replyto": "MGg4ymeH7R", "signatures": ["ICLR.cc/2026/Conference/Submission4092/Reviewer_Bj5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4092/Reviewer_Bj5V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760456447202, "cdate": 1760456447202, "tmdate": 1762917174163, "mdate": 1762917174163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dual-Phase Whitening (DPW), a novel test-time adaptation (TTA) method designed to improve model robustness under distribution shifts. DPW integrates two complementary whitening strategies: Whitening Batch Normalization (WBN): Replaces standard BN with ZCA whitening to decorrelate features and enforce isotropic distributions. Whitening Contrastive Learning (WCL): Extends contrastive learning with a global whitening constraint to promote a hyperspherical feature distribution. The method builds on AdaContrast and is evaluated on major benchmarks like VisDA-C, DomainNet-126, ImageNet-C, and CIFAR-100-C, achieving state-of-the-art performance without requiring source data during adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Introduces a unified whitening framework (WBN + WCL) for TTA, addressing feature decorrelation and geometric structure simultaneously.\n\n(2) Outperforms existing TTA and UDA methods across multiple challenging benchmarks, often by significant margins.\n\n(3) Well-grounded in feature whitening theory, with clear explanations of how whitening improves generalization under domain shift.\n\n(4) Designed for real-world TTA constraints --- no source data, online processing, and small batch sizes."}, "weaknesses": {"value": "(1) The font in Figure 1 is very small. The authors should pay attention to these details.\n\n(2)  The combination of WBN and WCL adds hyper-parameters and implementation complexity compared to simpler TTA baselines.\n\n(3) Only closed-set adaptation is evaluated; open-set or partial-set scenarios are not addressed."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h2CIz2ME3e", "forum": "MGg4ymeH7R", "replyto": "MGg4ymeH7R", "signatures": ["ICLR.cc/2026/Conference/Submission4092/Reviewer_eQ9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4092/Reviewer_eQ9h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741741903, "cdate": 1761741741903, "tmdate": 1762917173991, "mdate": 1762917173991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DPW (Dual Phase Whitening), a method for test-time adaptation that combines Whitening Batch Normalization (WBN) with Whitening Contrastive Learning (WCL). In DPW, the whitening process in WBN normalizes and decorrelates features within each batch, while WCL imposes independence constraints on the embedding components. The paper reports performance gains in accuracy on different benchmarks: VisDA-C, DomainNet-126, ImageNet-C, and CIFAR-100-C."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The idea of integrating whitening operations (WBN and WCL) into test-time adaptation is interesting and could inspire further work.\n\n* The method is evaluated on many benchmarks (VisDA-C, DomainNet-126, ImageNet-C, CIFAR-100-C)"}, "weaknesses": {"value": "* The method is simple, but the paper is difficult to follow, with unclear descriptions that sometimes make it hard to distinguish the main contribution from auxiliary tricks or implementation details.\n\n* There are grammatical inconsistencies and awkward phrasing (e.g., line 17: “in connected with whitening Batch Normalization”; line 187: “the covariance matrix for mitigation of this issue”).\n\n* Several notational issues reduce clarity. For example : (i) Line 206: confusing use of index k, superscript t, and missing reference to layer index l. (ii) Line 211: awkward sentence (“B_t and use them for whitening the corresponding activations…”).  (ii) Ambiguity between WBN, BN, and matrix symbols (e.g., B, W in equations vs. B for “Batch”), especially in Eq. 2 and Eq. 3.\n\n* In some cases, reported results show only marginal improvements (the average is often about +1, Tables 1,2, 3). It is unclear whether this gain is statistically significant. Reporting statistical errors on repeated experiments would help."}, "questions": {"value": "* In Line 236: “We adopt the cosine similarity for distance calculation and implement it through mean squared error between normalized vectors”. Why is this necessary, and how does this affect the results?\n\n* In Line 246: What exactly does parameter p represent?\n\n* What are the exact formulas in Equations 10 and 11?\n\n* Line 412: “In the more challenging TTA setting…”. Which setting are you referring to?\n\n* How do you apply DPW  in Continual TTA where samples arrive in an online fashion (one after another)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5sL7g77aO", "forum": "MGg4ymeH7R", "replyto": "MGg4ymeH7R", "signatures": ["ICLR.cc/2026/Conference/Submission4092/Reviewer_hrRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4092/Reviewer_hrRh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846882331, "cdate": 1761846882331, "tmdate": 1762917173791, "mdate": 1762917173791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach for test-time adaptation (TTA) called Dual-Phase Whitening (DPW), which integrates Whitening Batch Normalization (WBN) and Whitening Contrastive Learning (WCL) to enhance model generalization under distribution shifts. The method aims to decorrelate features and enforce a spherical distribution in the embedding space, reducing domain-specific biases. Extensive experiments on benchmarks like VisDA-C, DomainNet-126, ImageNet-C, and CIFAR-100-C demonstrate state-of-the-art performance, outperforming existing TTA and UDA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of combining feature whitening at both batch normalization and contrastive learning levels is novel and well-motivated.\n\n2. This paper provides thorough ablation studies and comparisons, showing the contribution of each component (WBN and WCL).\n\n3. The approach is source-free and suitable for real-world deployment where source data is unavailable.\n\n4. This method is empirically strong, achieving SOTA results across multiple challenging benchmarks."}, "weaknesses": {"value": "1. The writing contains several typos and grammatical errors that occasionally hinder clarity (e.g., \"boyel\" instead of \"bicycle\" in Table 1, \"Adacontrast (baselinee)\" with an extra 'e', inconsistent capitalization in \"Whitening BN\" vs. \"whitening BN\").\n\n2. Some mathematical notations are not fully explained (e.g., Eq. 7 refers to Eq. 5 and 6 without clear connection).\n\n3. The figures (e.g., Figure 1, 2, 3) are referenced but not included in the submitted draft, making it difficult to fully assess the visual explanations."}, "questions": {"value": "1. How does DPW perform under very small batch sizes, and what are the limits of its stability?\n\n2. Could the method be extended to open-set or partial-set adaptation scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4q7Yhmy2X6", "forum": "MGg4ymeH7R", "replyto": "MGg4ymeH7R", "signatures": ["ICLR.cc/2026/Conference/Submission4092/Reviewer_SQDP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4092/Reviewer_SQDP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993200807, "cdate": 1761993200807, "tmdate": 1762917173604, "mdate": 1762917173604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}