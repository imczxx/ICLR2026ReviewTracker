{"id": "k381ITf2EG", "number": 4971, "cdate": 1757822697412, "mdate": 1759898002192, "content": {"title": "Supervised Disentanglement Under Hidden Correlations", "abstract": "Disentangled representation learning (DRL) methods are often leveraged to improve the generalization of representations. Recent DRL methods have tried to handle attribute correlations by enforcing conditional independence based on attributes. However, the complex multi-modal data distributions and hidden correlations under attributes remain unexplored. Existing methods are theoretically shown to cause the loss of mode information under such hidden correlations. To solve this problem, we propose Supervised Disentanglement under Hidden Correlations (SD-HC), which discovers data modes under certain attributes and minimizes mode-based conditional mutual information to achieve disentanglement. Theoretically, we prove that SD-HC is sufficient for disentanglement under hidden correlations, preserving mode information and attribute information. Empirically, extensive experiments on toy data and six real-world datasets demonstrate improved generalization against the state-of-the-art baselines. Codes are available at anonymous Github https://anonymous.4open.science/r/SD-HC-1FAD.", "tldr": "", "keywords": ["Disentangled representation learning", "Correlations", "Mutual information", "Causal graph"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77a229477f13be20fa1205370bc90411f59d2928.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel supervised disentangled representation learning method, SD-HC, designed to handle correlations between the sub-populations under one attribute and other attributes. The authors propose minimizing a mode-based conditional mutual information to achieve disentanglement while preserving crucial mode-level information that existing methods lose. They provide a theoretical proof that their approach is a sufficient condition for disentanglement under a specified causal data generation process and demonstrate its superior generalization performance through extensive experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies and formalizes the novel and important problem of hidden correlations in disentanglement, with the motivation being very clear (Fig. 1). The main theoretical contribution, proving the sufficiency of mode-based CMI for disentanglement under these conditions, is a significant step beyond prior work that only established necessity. The empirical evaluation is extensive and rigorous, spanning multiple datasets, OOD generalization tasks, and thorough ablations. The results convincingly demonstrate the superiority of the proposed method in scenarios with complex data distributions."}, "weaknesses": {"value": "The method’s success relies heavily on an initial unsupervised clustering step (k-means on pre-trained features), making it a two-stage rather than end-to-end approach. Because of this, its overall performance depends on how well that clustering work, something that can easily fail if the data isn’t clearly separable or the pre-training isn’t strong.\n\nIt also introduces an additional hyperparameter: the number of modes per attribute. The paper shows that getting this wrong significantly hurts performance, meaning users need to tune it carefully. While the authors offer some estimation strategies, it still adds extra complexity and room for error.\n\nAnother issue is conceptual. The paper’s 'modes' could just as easily be seen as sub-categories or hierarchical labels. The authors don’t justify why their approach is better than existing hierarchical or fine-grained learning methods.\n\nFrom a practical standpoint, the method may be computationally heavy: it requires pre-training, clustering for each attribute, and an adversarial training loop, making it hard to scale.\n\nFinally, the theoretical grounding assumes that attributes are independent causal factors, an unrealistic simplification in many real-world settings, although common in disentangled feature learning."}, "questions": {"value": "- The method's reliance on a two-stage process seems to be its main practical weakness. Have you considered methods for end-to-end learning that could jointly discover the modes (e.g., via deep clustering objectives) and optimize the disentanglement objective simultaneously? This could potentially make the method more robust and elegant.\n- How does the performance compare to a baseline that treats modes as distinct sub-classes in a standard classification task without any explicit disentanglement objective? This would help clarify whether the performance gain comes from discovering the modes, the specific CMI objective, or both.\n- In the causal graph, the key insight is that conditioning on $m_1$ blocks the backdoor path through $c^m$. What is the intuition for why $c^m$ cannot directly influence $x$? Is it plausible that the same unobserved confounder that affects the walking mode also directly affects the raw sensor readings (x) in a way not mediated by the mode $m_1$?\n- The discriminator parameter sharing strategy across modes proved highly effective. Could you provide more intuition for this? Is it primarily a regularization effect to combat data sparsity within each mode, or does it enforce a shared structure on what it means to be independent of other attributes, regardless of the specific mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "62bRfkM7BI", "forum": "k381ITf2EG", "replyto": "k381ITf2EG", "signatures": ["ICLR.cc/2026/Conference/Submission4971/Reviewer_nRdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4971/Reviewer_nRdb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749259963, "cdate": 1761749259963, "tmdate": 1762917799671, "mdate": 1762917799671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SD‑HC, a supervised disentangled representation learning method designed for settings where attributes are correlated and, where multi‑modal structure (modes) exists under a given attribute. It first estimates mode labels for each attribute value, then minimizes a mode‑conditioned conditional mutual information so that representation subspaces disentangle while preserving both attribute and mode information. The authors provide formal results that this mode‑based CMI suffices for disentanglement under their causal data‑generation assumptions, and report experiments on toy data and real‑world datasets indicating improved robustness to distribution and train-test correlation shifts compared with prior DRL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow.\n- The motivation is clear and important. Addressing hidden correlations in multimodal attributes is essential to make DRL practical.\n- The paper includes theoretical analysis and the empirical method itself is intuitive and simple."}, "weaknesses": {"value": "- The main results are reported with F1 and Accuracy, which do not directly assess disentanglement without any justifications. These metrics can be high even when representations are entangled. Although the authors also report MI and DCI-I at the end of Section 5, DCI-D, which is a direct measure for disentanglement and a more common disentanglement metric in the literature, is not provided. \n\n- While the advantage of using supervision (i.e., GT labels) in disentanglement representation learning is that it can guarantee identifiability of GT factors under mild assumptions [1] (which could not be guaranteed in unsupervised setting as shown in [2]), this paper formalizes disentanglement via statistical independence (Definition 2) and does not establish identifiability of the underlying factors.\n\n**Reference**\n\n[1] Locatello  et al., Weakly-Supervised Disentanglement Without Compromises, in ICML 2020.\n\n[2] Locatello  et al., Challenging common assumptions in the unsupervised learning of disentangled representations, in ICML 2019."}, "questions": {"value": "- In L183-185, it is correct that $I(z_2; a_2) < H(a_2)$ implies that $z_2$ loses attribute information for predicting $a_2$ but does $I(z_1; m_1) < H(m_1)$ imply $z_1$ loses mode information for predicting $a_1$ (not $m_1$)?\n- When the GT labels for mode information are not given (which is common), we should decide an arbitrary number of clusters, which could be differ from the GT number of modes. Are those theoretical guarantees still hold for wrong number of clusters? \n- In Table 18, under Test 1 where train–test correlations are identical, the baseline shows the highest accuracy, outperforming the proposed method. What explains this gap? While it is expected that the baseline benefits when the correlation structure matches training, the proposed method’s inferior performance appears to indicate a loss of predictive information, which seems to contradicts the paper’s claim that predictive information is preserved. Moreover, in realistic deployments with careful dataset curation, correlation shift may be minimal or absent; in such cases, the baseline could be preferable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z91Umx3TpD", "forum": "k381ITf2EG", "replyto": "k381ITf2EG", "signatures": ["ICLR.cc/2026/Conference/Submission4971/Reviewer_XBAQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4971/Reviewer_XBAQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970355778, "cdate": 1761970355778, "tmdate": 1762917799322, "mdate": 1762917799322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of supervised disentangled representation learning under hidden correlations in multi-modal data. Existing DRL methods often assume attribute independence, or at most handle observed attribute correlations via conditional mutual information. However, these methods can lose important mode information in the presence of hidden correlations. The authors propose SD-HC, which introduces mode-based conditional mutual information minimization to preserve both attribute and mode information. The paper provides theoretical guarantees (necessary and sufficient conditions for disentanglement under hidden correlations), introduces an architecture-agnostic framework, and validates the approach on toy and six real-world datasets. Results show consistent improvements over baselines in both accuracy and generalization under correlation shifts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical contributions are significant, establishing both necessary and sufficient conditions for disentanglement under attribute and hidden correlations. This is the first formal proof showing sufficiency of conditional mutual information (CMI) minimization in this context, and the work generalizes to multiple attributes, varying correlation strengths, and both uni-modal and multi-modal distributions. The causal formulation and mode-based approach are rigorous, showing a deep understanding of the interplay between attribute and mode information in disentanglement. The proposed SD-HC method demonstrates strong performance across diverse datasets, including toy data, image datasets (CMNIST, CFashion-MNIST), and real-world wearable human activity recognition datasets. The method consistently outperforms state-of-the-art baselines, including A-CMI, HFS, and other DRL approaches, particularly under distribution shifts and correlation changes. The ablation studies convincingly show the importance of the mode-based CMI and discriminator components, highlighting that the design aligns with the theoretical guarantees. The extensive experiments, visualization of learned representations, and robustness analysis under noise and correlation shifts add further credibility to the claims."}, "weaknesses": {"value": "While SD-HC provides strong theoretical and empirical contributions, the reliance on pre-estimated mode labels introduces an additional external step that may affect reproducibility and performance, particularly for datasets where the mode structure is unclear or highly complex. Although the paper shows some insensitivity to the number of modes, the selection of hyperparameters for mode estimation can still influence results, and the method’s dependence on clustering quality may limit applicability in high-dimensional or highly noisy settings.\n\nAnother limitation is the scalability and computational complexity of SD-HC. The framework requires adversarial training with mode-based CMI minimization, discriminators, and mode predictors, which may increase training time compared to simpler DRL methods. While the method is claimed to be architecture-agnostic, the experiments focus mainly on moderate-dimensional representations and small to medium-sized datasets; it remains unclear how SD-HC performs on very large-scale datasets or in real-time applications where computational resources are constrained."}, "questions": {"value": "Can the SD-HC framework be efficiently scaled to very large datasets or real-time applications, and what strategies could mitigate the increased computational complexity from adversarial training and mode-based CMI minimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i9TFjneqqk", "forum": "k381ITf2EG", "replyto": "k381ITf2EG", "signatures": ["ICLR.cc/2026/Conference/Submission4971/Reviewer_uBsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4971/Reviewer_uBsu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970891137, "cdate": 1761970891137, "tmdate": 1762917799020, "mdate": 1762917799020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first identifies the limitations of previous supervised DRL approaches, which fail to consider the mode information of attributes for prediction. It then provides a theoretical analysis that establishes a sufficient condition for DRL under correlated attributes. Building on this analysis, the paper proposes a new learning framework, SD-HC, designed to disentangle an attribute with multiple modes from other attributes. Empirical results on six datasets demonstrate the effectiveness of SD-HC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is clear and well-justified.\n\n2. The theoretical analysis in Section 3 is comprehensive and contributes meaningfully to understanding DRL under correlations.\n\n3. The ablation studies are thorough, and the performance of SD-HC is promising."}, "weaknesses": {"value": "1. The discussion on SD-HC framework (Section 4) is confusing. The purpose of batch shuffling and the insight for adopting adversarial training, which is known to be unstable, are not clearly explained. “Shuffling” or “pairing” is indeed a common approach to introduce weak supervision signals in DRL, but it remains unclear whether the shuffling in SD-HC serves a similar role. Moreover, could adversarial training be replaced with a more stable alternative? The motivation for seeking a Nash equilibrium in this context is also not entirely clear.\n\n2. The section on \"mode label acquisition\" should be clarified, and including an algorithm outline or figure can be helpful for readers to better  understand the procedure. As this component is critical for interpreting the experimental results, its current vague description may easily lead to misunderstandings.\n\n3. More detailed discussion on the insight of variants should be provided in Section 5.3, instead of simple descriptions."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ypB3dx0bC7", "forum": "k381ITf2EG", "replyto": "k381ITf2EG", "signatures": ["ICLR.cc/2026/Conference/Submission4971/Reviewer_bvj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4971/Reviewer_bvj9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762439862220, "cdate": 1762439862220, "tmdate": 1762917798669, "mdate": 1762917798669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}