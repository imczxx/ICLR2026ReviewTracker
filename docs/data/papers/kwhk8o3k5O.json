{"id": "kwhk8o3k5O", "number": 14241, "cdate": 1758231039596, "mdate": 1763039694033, "content": {"title": "Phys-Bench: A Physics-aware Benchmark with Multi-Body Interactions for 3D Dynamic Scene Understanding", "abstract": "We introduce Phys-Bench, a novel physics-aware benchmark for 3D dynamic scene understanding. This benchmark is designed to evaluate methods for reconstructing 4D scenes and understanding underlying physics from given videos, with a main\nfocus on the Dynamic Novel View Synthesis (DyNVS) task. \nWhile existing algorithms and benchmarks primarily focus on photorealistic reconstruction, they largely overlook physics understanding. This neglect is a critical limitation, as a true understanding of dynamic scenes requires models to reason about physical interactions, not just appearance. \nOur benchmark provides complex dynamic scenarios with rich multi-object interactions, featuring realistic collisions and force exchanges that are faithfully generated to strictly adhere to physical laws. \nFurthermore, it contains a diverse range of physical materials, such as liquid, gas, rheological substances, and textiles, which move beyond the rigid bodies prevalent in existing benchmarks. \nTo enable quantitative evaluation, we provide essential ground-truth information such as 3D particle trajectories and physics parameters and propose two novel metrics tailored to assessing physical realism. \nWe further evaluate existing Dynamic Novel View synthesis and physics parameter estimation method on our benchmark and reveal their overlooked limitations in physics understanding and multi-body dynamics handling. \nWe believe Phys-Bench will serve as a crucial foundation for advancing research in dynamic view synthesis, physics-based scene understanding, and the integration of deep learning with physical simulation, ultimately enabling more faithful reconstruction and interpretation of complex 3D dynamic scenes.", "tldr": "", "keywords": ["4D Gaussian Splatting", "Physics", "Dynamic Novel View Synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7397fc3721e1087578236428bbabf75295e20450.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a physics-aware benchmark for 3D dynamic scene understanding. It features 17 simulated scenes with diverse materials—liquids, gases, rheological substances, and textiles—showcasing complex multi-body interactions generated by accurate physical solvers. Experiments reveal that existing DyNVS models achieve visual realism but fail to capture true physics, making Phys-Bench a key resource for developing physics-aware scene reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work focuses on the physics-aware 3D scene understanding, which is a significant problem. This benchmark is useful for evaluating model's physics understanding ability. \n2. The benchmark includes interactions of many different materials with modern simulation techniques."}, "weaknesses": {"value": "1. Lack of visualizations. Can the author present videos of the benchmark?\n2. The benchmark only includes 17 scenes, which seems to be limited for evaluation. \n3. The proposed metrics are not well-justified. The author should provide more discussion about the selection of the metrics. Specifically, in TD, the authors don't explain how they match ground-truth points and reconstructed points. It's unclear how they compute this metric. Furthermore, both TD and AUOP are computed point-wise. This cannot well indicate the physical plausibility of the whole reconstructed 3D model. \n4. Justification of my current score:\n(1) Why not lower: this work is well-motivated, and I believe it's benchmarking a significant problem (physical plausibility) in the field of 3D reconstruction and understanding. \n(2) Why not higher: my main concern with this work is the limited scale of scenes. As a benchmark, it should provide a comprehensive evaluation for its target field. As for this paper, the authors do include different kinds of materials and different simulation techniques. However, the majority of this dataset is still scenes with several relatively simple objects and constrained spatial configurations. This limits its ability to fully test the generalization and robustness of learning-based methods, especially in more complex or cluttered environments. Moreover, it remains unclear how well the dataset covers real-world variability in lighting, geometry, and dynamic interactions, which are crucial for practical applications."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "knOuiLwS0z", "forum": "kwhk8o3k5O", "replyto": "kwhk8o3k5O", "signatures": ["ICLR.cc/2026/Conference/Submission14241/Reviewer_4CZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14241/Reviewer_4CZi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792044678, "cdate": 1761792044678, "tmdate": 1762924696162, "mdate": 1762924696162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8cwM2Pne22", "forum": "kwhk8o3k5O", "replyto": "kwhk8o3k5O", "signatures": ["ICLR.cc/2026/Conference/Submission14241/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14241/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039693370, "cdate": 1763039693370, "tmdate": 1763039693370, "mdate": 1763039693370, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark dataset for evaluating the physics awareness of methods targeting dynamic scene understanding. \nThis dataset involves materials including liquid, gas, rheological substances, and textiles. It also captures multi-object interaction. \nFor the physics awareness assessment, evaluation is made against ground truth 3D trajectories and physics materials."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall presentation is clear and easy to follow.\n\n2. The research problem this paper targets is insightful: how to evaluate physics realism beyond photorealism and extend it to more complex scenarios."}, "weaknesses": {"value": "1. The scale of the proposed dataset is limited (17 scenes only). With diverse materials and multi-object interaction, the benchmark is expected to be larger.\n\n2. Evaluation of physics awareness is not fully benchmarked in this paper. \n\na. A majority of experiments still focus on photorealism assessment. \n\nb. The proposed metrics only focus on 3D trajectories of primitives. Are matched trajectories equivalent to physics awareness? Will it be useful to consider future trajectory predictions as additional metrics?\n\nc. Experiments for physics parameter prediction are very limited. \n\n3. Only 4D reconstruction methods are tested. Relevant physics learning approaches are not evaluated."}, "questions": {"value": "1. In Figure 5, why is the number of trajectories different for different methods?\n\n2. For the calculation of TD, how is a predicted primitive trajectory matched to a GT trajectory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E2boMg6R5O", "forum": "kwhk8o3k5O", "replyto": "kwhk8o3k5O", "signatures": ["ICLR.cc/2026/Conference/Submission14241/Reviewer_HztB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14241/Reviewer_HztB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840733042, "cdate": 1761840733042, "tmdate": 1762924695386, "mdate": 1762924695386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Phys-Bench, a novel physics-aware benchmark designed for 3D dynamic scene understanding, focusing on the Dynamic Novel View Synthesis (DyNVS) task. Phys-Bench provides complex dynamic scenarios featuring multi-object interactions and diverse physical materials, including liquids, gases, rheological substances, and textiles. Two metrics are proposed to evaluate the motion reconstruction performance. Several state-of-the-art DyNVS algorithms and physical parameter estimation methods are evaluated on the proposed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This benchmark includes diverse materials types (liquid, gas, rheological, textile) and multi-body interactions. \n2. This proposed dataset provides comprehensive ground truth for various tasks, such as rgbs, depths, normals, and relighted images.\n3. By testing existing algorithms, this benchmark identifies the gap of the current models.\n4. The presentation is clear and easy to follow."}, "weaknesses": {"value": "There are several obvious weaknesses for this proposed benchmark, especially the experiment is not complete. \n1. Since this benchmark is mainly designed to evaluate the physical awareness of DyNVS models, experiments should focus more on this part. However, currently the most part of the experiments are still for photorealistic metrics. Metrics like TD and AUOP are only evaluated for two scens, which is far from completeness. What’s more, only PAC-NeRF is evaluated for parameter estimation task, and only for one scene, which is weak to support the authors’ arguments, and the number is too small to serve as a benchmark. \n2. The methods for DyNVS are limited only on reconstruction based methods. No generation based models are included, such as ReCamMaster (Bai et al. ICCV2025), TrajectoryCrafter(Yu et al. ICCV2025), NVS-Solver (You et al. ICLR2025). \n3. The size of the dataset is too small, only 17 scenes. Note I’m not saying only large dataset can make contribution for the community, but I’m addressing the underlying scheme how this benchmark can benefit future research. Since the authors only list the current methods’ performance, but not proposing any potential way to improve the algorithms, the underlying logic is thus `performing good performance on this benchmark means the model have good ability in understanding physics’. Nevertheless, the limited size of the dataset cannot provide enough diveristy to support this. \n4. The proposed metric TD is not novel. This is actually 3D end point error, which is widely used in 3D tracking evaluations. \n\nI’m open to modify my score based on other reviewers’ comments and the response from the authors."}, "questions": {"value": "1.as shown in the appendix B.2, is the camera number limited to 2 to 4? Are they too sparse? Can the baseline methods show better performance given more training views? I ask this question is to ensure the weak performances are mainly due to the incapability of modeling complex physics but not due to view sparsity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mqco9IaPFb", "forum": "kwhk8o3k5O", "replyto": "kwhk8o3k5O", "signatures": ["ICLR.cc/2026/Conference/Submission14241/Reviewer_S2ew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14241/Reviewer_S2ew"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877820173, "cdate": 1761877820173, "tmdate": 1762924694786, "mdate": 1762924694786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A new benchmark for dynamic 3D scenes, with an emphasis on not just realism but physics since many of the benchmarks in existence don't put an emphasis on this facet of 3D dynamic reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Does a good job of overviewing related benchmarks, giving a nice taxonomy that makes it clear what your benchmark brings to the table. I appreciate that you run existing methods on your benchmark to provide a starting point for methods down the line that want to use this benchmark."}, "weaknesses": {"value": "I don't have much in the way of a weakness but it would be slightly nicer if some more reference photos could be shown showcasing the diversity of the benchmark... like even just one small photo per scene and showcase a lot of the scenes to demonstrate the geometric complexity of what is being modeled."}, "questions": {"value": "The one thing I'd like to know more about is correspondences / texture: many of the models shown lack texture that you'd find in the real world. A monochromatic cube / fluid that moves or deforms might make it really difficult for a dynamic 3D reconstruction method to work effectively. Can I get some perspective on why this may not be an issue? In my mind this makes this simulation unnecessarily more challenging compared to real world capture data (even if you don't get a lot of the ground truth measurements you can get in sim)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hVxQSlFwKM", "forum": "kwhk8o3k5O", "replyto": "kwhk8o3k5O", "signatures": ["ICLR.cc/2026/Conference/Submission14241/Reviewer_DGcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14241/Reviewer_DGcp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882130958, "cdate": 1761882130958, "tmdate": 1762924694304, "mdate": 1762924694304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}