{"id": "vs0NxGAUO9", "number": 14039, "cdate": 1758227475489, "mdate": 1763037585283, "content": {"title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints", "abstract": "Learning high-quality, robust, efficient, and disentangled representations is a central challenge in artificial intelligence (AI). Deep metric learning frameworks tackle this challenge primarily using architectural and optimization constraints. Here, we introduce a third approach that instead relies on $\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple framework that uses a small monotonic multi-layer perceptron (MLP) head attached to any pre-trained encoder. Due to co-adaptation between encoder and head guided by contrastive loss and monotonicity constraints, MonoCon learns robust, disentangled, and highly compact embeddings at a practically negligible performance cost. On the CIFAR-100 image classification task, MonoCon yields representations that are nearly 9x more compact and 1.5x more robust than the fine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN classification accuracy. We also report a 3.4x more compact and 1.4x more robust representation on an SNLI sentence similarity task for a marginal reduction in the STSb score, establishing MonoCon as a general domain-agnostic framework. Crucially, these robust, ultra-compact representations learned via functional constraints offer a unified solution to critical challenges in disparate contexts ranging from edge computing to cloud-scale retrieval.", "tldr": "A new domain-agnostic framework for learning compact and disentangled representations using functional monotonicity constraints", "keywords": ["Deep metric learning", "monotonic networks", "contrastive learning", "supervised representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6bf51d4ac0169e78b7f8eba49d151834189c6a56.pdf", "supplementary_material": "/attachment/8a7f0195788b8b5fe157c4c73bf8e2f818eaba12.zip"}, "replies": [{"content": {"summary": {"value": "- MonoCon attaches a monotonic MLP head to a pretrained encoder and trains end-to-end with SupCon; the head’s output dimensionality matches the encoder and is kept at test time.\n- Monotonicity is enforced via non-negative weights, aiming to make representations compact and stable.\n- Benchmarks: CIFAR-10/100 and SNLI→STSb; metrics: 5-NN/Recall@k plus PCA-based proxies (effective dimensionality, reconstruction error).\n- Results: achieves large dimension reduction and lower reconstruction error with minimal accuracy loss, supported by correlation-structure analyses.\n- Positioning: uses monotonic constraints not for compliance or calibration, but as a general head for representation compression/regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple and general: a lightweight, plug-on head for existing encoders with a clear training/inference pathway.\n- Consistent compression: substantially reduces effective dimensionality while retaining accuracy across both vision and language tasks."}, "weaknesses": {"value": "- Missing comparisons along two crucial axes. To attribute gains to monotonicity (rather than capacity/regularization), the paper should compare monotonic MLP (proposed) vs DLN (lattice-based monotonic) to isolate the unique effect of monotonicity and to quantify the practical trade-offs (accuracy–compression–latency) under identical training and capacity.  In parallel, it needs (B) non-monotonic compact-representation baselines, (e.g., VICReg, Barlow Twins) to show added value over alternative compression strategies. Without both axes, it remains unclear whether improvements stem from monotonicity itself or from architectural/regularization choices.\n- Limited backbone/scale generalization: vision focused on ResNet, NLP on MiniLM, and small datasets (CIFAR, SNLI→STSb); needs tests on ViT/Swin/ConvNeXt, BERT/RoBERTa, and larger benchmarks (ImageNet/COCO, GLUE).\n- Proxy-metric dependence: heavy reliance on PCA-based proxies; lacks task-level robustness (e.g., OOD kNN, noise/corruption settings).\n- Statistical reliability: no multi-seed mean±std, making stability/variance unclear."}, "questions": {"value": "- Can you compare monotonic MLP (proposed) vs DLN (lattice-based monotonic) to isolate the unique effect of monotonicity and to quantify the practical trade-offs ?\n- Do the performance–compression trade-offs hold across more backbones and larger datasets (ViT/Swin/ConvNeXt; BERT/RoBERTa; ImageNet/COCO, GLUE)?\n- Will you report ≥3-seed mean±std for key metrics to establish stability?\n- Beyond PCA proxies, can you evaluate task-level robustness (OOD kNN, noise/corruption, downstream retrieval/classification) to verify the benefit of the monotonic head?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cnZr2RQxsC", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Reviewer_scD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14039/Reviewer_scD9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989588737, "cdate": 1761989588737, "tmdate": 1762924528538, "mdate": 1762924528538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "zKeN6NEUrC", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763037584296, "cdate": 1763037584296, "tmdate": 1763037584296, "mdate": 1763037584296, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MonoCon, a framework that extends deep metric learning encoder with a trainable MLP head. To enforce monotonicity, the weights of this head are squared, and it utilizes Leaky ReLU activation function. The authors claim these constraints produce more compact and robust representations compared to either the direct encoder outputs or an unconstrained MLP head. The method is evaluated on two vision datasets (CIFAR-10, CIFAR-100) and one NLP dataset (SNLI)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I don't see any notable strengths in the current form of the submission."}, "weaknesses": {"value": "**W1)** A major weakness is the absence of a theoretical grounding or clear intuition. The paper asserts that monotonicity leads to more compressed and robust embeddings but provides no formal analysis or convincing explanation for  why  this constraint should produce such benefits. \n\n**W2)** The authors evaluate on the CIFAR training set (line 208) rather than the standard test split, which fails to measure generalization. Constraining the network with monotonicity potentially helps against overfitting that the baseline experiences. The authors should evaluate in a proper setting that verifies generalization of the method. Larger-scale metric learning benchmarks, and ideally a second task, such as image retrieval should be added to truly confirm the useful properties of MonoCon.\n\n**W3)** The empirical results are unconvincing. In the low-dimensional regime (Table 3, dim=16), MonoCon underperforms the baseline on two out of three metrics. Furthermore, the deff​ metric appears disconnected from task performance: the baseline performs better at dim=16 despite a much higher $d_{eff}​$ (125), which questions the metric's practical relevance. To demonstrate broader applicability, the evaluation should also include modern architecture, such as vision transformer.\n\n**W4)** The paper fails to adequately position its contributions and lacks essential baseline comparisons.\n- **Novelty:** The discussion of novelty is insufficient. The paper does not clearly differentiate its contributions from prior works that apply monotonicity constraints to deep networks, such as \\[1\\].\n- **Baselines:** The paper mentions standard compression methods (e.g., PCA, product quantization) in the related work but provides no empirical comparison. Without these baselines, it is impossible to assess the method's effectiveness relative to established techniques.\n\n\\[1\\] Deep lattice networks and partial monotonic functions, You et al., NeurIPS, 2017\n\n**W5)** Although the extent of the use of LLM is disclosed in the Appendix, its heavy use makes reading the paper text more difficult. Factual information is often buried in convoluted sentences and vague language. For example, the sentence, \"We implement a differential learning rate strategy to flexibly switch between fine-tuning the encoder at a lower rate, to full-fledged co-adaptation between encoder and head at the same rate,\" is ambiguous and lacks concrete details about the switching mechanism or the rates used. This convoluted style hinders readability and comprehension.\nAdditionally, the in-text citations lack brackets, causing them to blend with the text and deviate from the author guidelines."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qu6QGc4zfD", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Reviewer_3mnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14039/Reviewer_3mnW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010223939, "cdate": 1762010223939, "tmdate": 1762924528079, "mdate": 1762924528079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MonoCon offers a simple yet generalizable solution to a critical bottleneck in representation compactness. Specifically, the authors demonstrate that MonoCon achieves up to a 9× reduction in effective dimensionality on the CIFAR-100 task and a 3.4× reduction on the SNLI task, while preserving 99% of the baseline performance. These impressive results are obtained by incorporating a lightweight monotonic multi-layer perceptron (MLP) head that can be seamlessly attached to any pre-trained encoder."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Effectiveness and generality of MonoCon: The proposed MonoCon framework is both simple and remarkably effective on the CIFAR-100 and SNLI tasks. The methodology achieves a substantial reduction in effective dimensionality—by up to 3.4× on SNLI—while maintaining 99% of the baseline performance. This demonstrates that the approach successfully balances representation compactness and accuracy. More importantly, the design appears architecture-agnostic, suggesting that the monotonic constraint can be seamlessly integrated into various model architectures, potentially broadening its applicability across domains.\n\n2. Insights from training dynamics: The analysis of training dynamics uncovers a self-organized co-adaptation between the encoder and the classification head, characterized by a rapid early-stage dimensional collapse followed by a gradual recovery phase. This finding is particularly intriguing, as it reveals the internal coordination between model components under monotonic constraints. The authors’ effort to visualize and interpret this phenomenon provides valuable insight into the representational evolution during training."}, "weaknesses": {"value": "1. **Motivation — Why does monotonicity work intuitively?** This is the key question the authors should have addressed, or at least attempted to answer, in this work. However, the paper lacks a dedicated section exploring the underlying intuition or theoretical rationale for why monotonic constraints improve learning. While the section on training dynamics briefly touches on this aspect, it merely reports empirical phenomena without delving deeper into *why* such behavior emerges. A more thorough analysis or discussion linking monotonic learning to potential cognitive constraints in human learning or mechanisms in the human brain would significantly strengthen the paper’s contribution.\n\n2. **Limited empirical validation.** Although the proposed idea is simple and appears effective on CIFAR-100 and SNLI, the empirical analysis is limited in scope. The authors should evaluate the proposed monotonic constraint across multiple architectures to verify whether the observed trend is consistent and reproducible under different model designs. Such validation would greatly enhance the generality and credibility of the findings.\n\n3. **Insufficient task diversity.** The experimental evaluation is currently restricted to CIFAR-100 and SNLI. To better demonstrate the effectiveness of the proposed representation compactness, additional experiments on larger and more diverse classification datasets, such as ImageNet, would be valuable. Broader evaluations across domains would help establish whether the approach truly scales and benefits memory efficiency in more complex, real-world settings.\n\n4. **Impact on generalization.** While the results show only a small performance drop on the tested datasets, it remains unclear how representation compactness affects generalization, particularly in *out-of-distribution (OOD)* scenarios. A deeper analysis of whether compact representations improve robustness or transferability under distribution shifts would make the work more comprehensive and impactful."}, "questions": {"value": "1. What is the underlying intuition behind the effectiveness of monotonicity in representation learning? Can the authors provide a theoretical or conceptual explanation for why imposing monotonic constraints leads to improved learning dynamics or compact representations?\n\n2. Does the proposed monotonic constraint generalize across architectures? Have the authors tested whether the observed performance and dimensionality reduction trends remain consistent under different model architectures and design choices?\n\n3. Can the proposed approach scale to larger and more diverse tasks? How does MonoCon perform on more complex datasets such as ImageNet or across domains beyond CIFAR-100 and SNLI? Does it maintain efficiency and performance advantages at scale?\n\n4. How does representation compactness influence model generalization and robustness? Does the compact representation learned via MonoCon improve or degrade performance in out-of-distribution or transfer learning settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CejCYsNJon", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Reviewer_EV8x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14039/Reviewer_EV8x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181622990, "cdate": 1762181622990, "tmdate": 1762924527490, "mdate": 1762924527490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thoughtful reviews. In the light of these comments, we have decided to withdraw and improve the manuscript."}}, "id": "06NKb2yHaL", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14039/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763037559953, "cdate": 1763037559953, "tmdate": 1763037559953, "mdate": 1763037559953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MonoCon - a monoticity-constrained (squared weights fed into a ReLU) MLP attached to representation models then trained using contrastive objectives such as SupCon to produce more compact representation spaces; with results verified on CIFAR-10, CIFAR-100 and SNLI."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well written and easy to follow; while the setup for MonoCon is easy to utilize.\n* The recall capabilities for highly restricted representation spaces as shown in Tab. 3, at least on CIFAR100, is very convincing.\n* I really like the study on compression and training dynamics conducted in order to understand practical impact of MonoCon; within the realm of extrapolatable context given the utilized datasets, the results are quite convincing."}, "weaknesses": {"value": "I do not believe that this paper is ready (yet) for publication, for the following primary reasons\n\nWhile one could argue that the contribution is too simplistic (squared weights + ReLU on a MLP head), I strongly believe that this could be a feature IF it would be generally applicable. Unfortunately, experiments were only conducted on very small-scale benchmarks and architectures (limited also only to convolutional networks for vision), and NO conclusion can be drawn how these insights hold up to any relevant scales useful for general representation learning scenarios (i.e. at the very least, studies on ImageNet would be great to see).\n\nMoreover, the relevance and connection of using a monotonic reprojection is introduced pretty much ad-hoc, with at most some pointers to some related literature. It's prudent for the authors to provide significantly more context as to why monotonic reprojections can helo compress representations down to the degree seen."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8KvrHfVyVx", "forum": "vs0NxGAUO9", "replyto": "vs0NxGAUO9", "signatures": ["ICLR.cc/2026/Conference/Submission14039/Reviewer_3PVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14039/Reviewer_3PVx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762695040569, "cdate": 1762695040569, "tmdate": 1762924526971, "mdate": 1762924526971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}