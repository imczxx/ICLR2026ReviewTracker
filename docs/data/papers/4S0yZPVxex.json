{"id": "4S0yZPVxex", "number": 9563, "cdate": 1758127802428, "mdate": 1759897712037, "content": {"title": "TR-MERGING: TRAINING-FREE ROUTER FOR MODEL MERGING", "abstract": "With the rapid advancement of deep learning, a wide variety of open-source models for different tasks have emerged. However, a single fine-tuned model often fails to meet users' diverse requirements. To address this limitation, model merging has been proposed as an effective approach to integrate the capabilities of existing models into a unified one. Among existing approaches, router-based methods have become representative baselines due to their strong performance; however, their reliance on a trainable router compromises the appealing advantage of traditional model merging—being completely training-free.In this paper, we propose a training-free router from a similarity-based perspective. Our method achieves performance on par with router-based approaches while eliminating the need for any additional training. We demonstrate the effectiveness of TR-Merging across multiple tasks in both computer vision (CV) and natural language processing (NLP), and demonstrate its flexibility in adapting to diverse requirements.", "tldr": "We propose a training-free model merging method specifically for router-based model merging.", "keywords": ["Model Merging", "Router", "LLM", "Training Free"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78b5848edc54a2ca68aabaf0947633bd51ff195c.pdf", "supplementary_material": "/attachment/9d244dbc8dd79c2fcd00621e670a6e3c7196e3c6.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose TR-Merging, a training-free router for model merging that dynamically selects and weights expert models based on semantic similarity between input data and task domains. The method leverages a pre-trained embedding model to compute cosine similarities, applies temperature scaling and top-k selection to enhance discriminability, and merges parameters without any additional training. The authors validate their approach across a range of NLP and CV tasks, demonstrating competitive or superior performance compared to training-based router methods, while maintaining computational efficiency and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a conceptually simple yet effective training-free routing strategy. It leverages LoRA to reduce memory overhead and avoids the often-overlooked costs and complexities associated with training additional routing networks.\n- This method has validated its effectiveness across multiple domains (NLP, CV) and presented robust empirical results.\n- The paper provides solid theoretical analysis, which enhances the credibility of the method."}, "weaknesses": {"value": "- The experimental details are considerably inadequate. Specifically, it fails to provide the specific values of temperature ($\\tau$) and sample size ($M$) for each domain, nor does it include corresponding ablation experiments to illustrate the effectiveness.\n- Following the approach outlined in the paper, if we directly select the model with the smallest similarity as the merged model—i.e., setting the coefficients of all other models to 0—could this potentially yield better performance?\n- The discussion on the specific advantages of LoRA is rather brief, and there is a lack of ablation experiments comparing full-parameter merging with LoRA-based merging. This is particularly notable given that all baselines employed in the paper utilize full-parameter settings as reported in their original sources.\n- There are issues with certain figures and textual descriptions:\n    - The captions for Figure 1, 2, 3 and Table 1, 2, 3, 4, 5 in the main text fail to provide useful information.\n    - There are some typos. For instance, in the \"Implementation detail\" section (Section 4.1), NLP tasks are incorrectly described as being trained using ViT."}, "questions": {"value": "My questions are listed with my weaknesses above.\n\nI'm excited to engage with the authors to clear up the aspects I don't fully understand and I'm optimistic that with some iteration this paper can be made stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mBQppMgOrH", "forum": "4S0yZPVxex", "replyto": "4S0yZPVxex", "signatures": ["ICLR.cc/2026/Conference/Submission9563/Reviewer_ykfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9563/Reviewer_ykfp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309536598, "cdate": 1761309536598, "tmdate": 1762921119027, "mdate": 1762921119027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "While existing high-performance model merging methods often utilize a router to dynamically select experts based on the input, this router typically requires an additional training phase, incurring further data and computational overhead. This paper proposes a training-free router based on embedding similarity to resolve this issue."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper correctly identifies and highlights the dependency on an additional training phase as a significant limitation of existing router-based model merging methodologies."}, "weaknesses": {"value": "1. Although the router itself is not trained, the method still necessitates access to the training data to compute domain similarity. This dependency remains a significant limitation. Validation is needed on how this method performs if it cannot access data from the original training distribution.\n2. The method's performance appears to be heavily dependent on the chosen pre-trained embedding model (the \"router\"). This dependency is likely to be a critical factor when the types of datasets or domains change.\n3. Furthermore, the process of generating these embeddings via a powerful pre-trained model (like CLIP) could introduce significant inference latency. This may result in the method being slower than existing approaches that use a lightweight, trained router, contrary to the paper's claims.\n4. The paper lacks sufficient analysis (e.g., ablation studies) of key hyperparameters that could significantly affect performance. This includes the sample size (M) used to represent each task domain and the temperature coefficient.\n5. The core technical components, such as temperature scaling and softmax normalization, appear to be standard, existing methods applied directly, rather than novel contributions.\n6. The number of compared SOTA merging techniques is insufficient. The comparison would be much stronger if it included other recent router-based merging methods.\n7. The overall readability and presentation of the paper are poor. The placement and formatting of figures and tables, as well as the prose, require significant revision for clarity."}, "questions": {"value": "1. Twin-Merging reportedly uses a very lightweight MLP-based router. How can TR-Merging, which uses a much heavier model like CLIP as its router, achieve faster inference times as reported in the tables? This result seems counter-intuitive and requires a detailed explanation of the experimental setup for measuring latency.\n2. It seems that the choice of the pre-trained model used as the router (e.g., CLIP vs. BGE vs. other alternatives) would have a substantial impact on performance. Could the authors comment on this dependency or provide analysis on using different embedding models as the router?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EE7wkR9nrP", "forum": "4S0yZPVxex", "replyto": "4S0yZPVxex", "signatures": ["ICLR.cc/2026/Conference/Submission9563/Reviewer_v3ys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9563/Reviewer_v3ys"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829352147, "cdate": 1761829352147, "tmdate": 1762921118618, "mdate": 1762921118618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TR-Merging, a method for merging multiple fine-tuned models by using a dynamic, input-dependent weighting scheme. The core idea is to bypass the need for an explicitly trained routing module, which is a key component in recent high-performance methods like Twin-Merging. Instead, TR-Merging employs a \"training-free router\" that calculates the cosine similarity between a new input's embedding and the pre-computed average embeddings of small data samples from each expert model's training domain. These similarity scores determine the weights for merging the expert models' parameters for the current inference task. The authors present experiments across NLP and CV tasks, arguing that their method achieves competitive performance with router-based approaches while being more efficient and simpler to implement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The central concept of using semantic similarity to guide the merging process is simple, elegant, and easy to understand. It provides a clever alternative to training a dedicated routing network.\n \nThe method shows competitive performance on several benchmarks when compared to established baselines, including a state-of-the-art method that requires router training. This suggests the direction has potential.\n\nCompared to methods requiring training a router, the proposed approach is conceptually simpler and avoids a complex training pipeline, provided a suitable embedding model is available."}, "weaknesses": {"value": "The paper's most significant flaw is the complete omission of crucial hyperparameter values (M, τ, K) and the methodology for their selection. Without this information, the experiments are not reproducible, and the validity of the results cannot be independently verified. It is impossible to know if the performance is robust or the result of careful, undisclosed tuning.\n\nThe core marketing of the method as \"training-free\" is questionable. The method's success is entirely dependent on a large, pre-trained embedding model. This dependency is not analyzed. The paper fails to investigate how performance changes with different or weaker embedding models. This makes the efficiency claims (e.g., \"no training cost\") incomplete, as the cost is simply outsourced.\n\nThe paper lacks a proper ablation study on its key components. What is the sensitivity to M, the number of samples? How does the choice of embedding model affect performance? Answering these questions is fundamental to understanding the method, not just demonstrating it works on one specific setup.\n\nThe claim of successfully merging generative and discriminative tasks is based on extremely thin evidence. The reported gains are marginal (+0.5%), and the experimental setup and baseline are not sufficiently detailed. This feels like a speculative claim rather than a demonstrated capability."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UNInnOxspg", "forum": "4S0yZPVxex", "replyto": "4S0yZPVxex", "signatures": ["ICLR.cc/2026/Conference/Submission9563/Reviewer_DovB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9563/Reviewer_DovB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889698374, "cdate": 1761889698374, "tmdate": 1762921118293, "mdate": 1762921118293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TR-Merging, a training-free router framework for model merging that integrates multiple fine-tuned models (experts) without additional training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of a training-free router is a simple yet impactful idea, addressing a key limitation of router-based merging (training cost).\n2. Evaluation across both CV and NLP domains, with comparisons to strong baselines (Fisher, Task Arithmetic, TIES, Twin-Merging)."}, "weaknesses": {"value": "1. The cosine-similarity-based routing is conceptually close to classical nearest-neighbor or retrieval-based gating. The paper would benefit from clearer distinction and analysis versus simple centroid-matching baselines."}, "questions": {"value": "1. How sensitive is TR-Merging to the choice of the embedding model used as the router?\n2. What happens if the expert domains are highly overlapping (e.g., MNLI vs. QNLI)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1sgKyGCI46", "forum": "4S0yZPVxex", "replyto": "4S0yZPVxex", "signatures": ["ICLR.cc/2026/Conference/Submission9563/Reviewer_eJCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9563/Reviewer_eJCb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174712897, "cdate": 1762174712897, "tmdate": 1762921117956, "mdate": 1762921117956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}