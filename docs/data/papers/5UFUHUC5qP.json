{"id": "5UFUHUC5qP", "number": 5643, "cdate": 1757925172796, "mdate": 1763446221090, "content": {"title": "GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning", "abstract": "Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for generative DyTAG tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation. To address these critical issues, we propose \\underline{G}enerative \\underline{D}yTA\\underline{G} \\underline{B}enchmark (GDGB), which comprises eight meticulously curated DyTAG datasets with high-quality textual features for both nodes and edges, overcoming limitations of prior datasets. Building on GDGB, we define two novel DyTAG generation tasks: Transductive Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG). TDGG transductively generates a target DyTAG based on the given source and destination node sets, while the more challenging IDGG introduces new node generation to inductively model the dynamic expansion of real-world graph data. To enable holistic evaluation, we design multifaceted metrics that assess the structural, temporal, and textual quality of the generated DyTAGs. We further propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation. Experimental results demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation. These findings establish GDGB as a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation. The dataset and source code are available at \\url{https://anonymous.4open.science/r/GDGB-3F25}.", "tldr": "We propose Generative DyTAGs Benchmark (GDGB), addressing poor textual quality in existing datasets and defining novel tasks (TDGG/IDGG) to advance robust and reproducible DyTAG generation research.", "keywords": ["Dynamic Text-Attributed Graph", "Dynamic Graph Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/763de443fb9c84b2d399bcb228c08037b5493547.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper highlights 2 key concerns with existing dynamic text-attributed graphs: a)Lack of high-quality textual attributes- either completely absent or limited to non-semantic quality, like only containing usernames or emails; b) Lack of standardized DyTAG generative tasks formulations and evaluation protocols: Existing evaluation frameworks don't incorporate textual characteristics.  To solve for this, the authors propose the following contributions\na) Eight DyTAG datasets covering diverse domains with rich textual semantic information\nb) Two novel tasks on these DyTAGs: Transductive dynamic graph generation and Inductive Dynamic graph generation with metrics converting Graph Structural Metrics, Textual Quality Metrics, and Graph Embedding Metrics. \nc) Baseline: Given this suite of datasets, the paper proposes GAG-General, which is an LLM-based multi-agent framework adopted for DyTAG generation tasks. \n\nCollectively, the paper proposes a robust benchmarking for DyTAG generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "a) There is a clear gap in standardized datasets for textual attributed graphs, which this paper attempts to fill.\nb) Novel Eight text attributed dynamic graph datasets proposed\nc) Comprehensive comparison against existing datasets in terms of richness and utility of these textual attributes to highlight their importance\nd) The paper clearly motivates the problem and reports a detailed analysis of these datasets."}, "weaknesses": {"value": "A) The paper somewhat dilutes its core contribution by introducing a Generative framework: GAG-General, which is adopted from existing work. Core contribution could only be a textual attributed benchmark with evaluation metrics. And this generative framework could be proposed as a baseline method, presenting it as a central contribution, overemphasizes an incremental component, as novelty here is very limited. \n\nB) Text quality and graph embedding metrics: There seems to be a dependency on the underlying LLM  on final performance. However, the paper provides no clear guidance on which model should be considered standard. Can the author guide which LLM should be used for evaluation, or can any one of them be used? The evaluation framework needs to be final, but the proposed one seems to be dependent on the underlying LLM, with no clarity on which to use.  There is a table 3 in the experiment section, but it's not clear what the conclusion is from it. \n\nAn evaluation framework that is dependent on underlying LLMs will lead to different researchers obtaining inconsistent results. Any new LLM version update or slight change in prompt can lead to different results. The authors of this paper should define a fixed or recommended LLM backbone with recommended hyperparameters during LLM inference and input for consistency, robustness, or a way to minimize the LLM-specific biases."}, "questions": {"value": "Overall, the paper is of significant use to the temporal graph learning community, which is in dire need of benchmark datasets, especially in dynamic attributed graphs and a consistent evaluation framework.   But paper in this current form needs clear positioning and a more robust evaluation framework. \n\nI will be open to accepting if authors can address these concerns meaningfully."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OugAixNWuD", "forum": "5UFUHUC5qP", "replyto": "5UFUHUC5qP", "signatures": ["ICLR.cc/2026/Conference/Submission5643/Reviewer_xhXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5643/Reviewer_xhXC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473392648, "cdate": 1761473392648, "tmdate": 1762918173323, "mdate": 1762918173323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GDGB introduces a high-quality benchmark for generative learning on dynamic text-attributed graphs (DyTAGs), accompanied by two novel tasks (TDGG and IDGG) and a multi-agent LLM framework (GAG-General). The datasets notably surpass prior ones in textual richness, and the holistic evaluation protocol covers structure, text, and time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written and easy to follow.\n2. The paper proposes the 1st dedicated generative DyTAG benchmark with rich, realistic node/edge texts across eight diverse domains.\n3. The TDGG and IDGG tasks is well defined, and the metrics (structure, text, embedding) give a comprehensive quality picture."}, "weaknesses": {"value": "1. IDGG new-node evaluation may lack direct semantic-drift or human-amenity checks."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nqUx0y6evD", "forum": "5UFUHUC5qP", "replyto": "5UFUHUC5qP", "signatures": ["ICLR.cc/2026/Conference/Submission5643/Reviewer_EtXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5643/Reviewer_EtXQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747233087, "cdate": 1761747233087, "tmdate": 1762918172931, "mdate": 1762918172931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GDGB presents a generative Dynamic Text-Attributed Graph benchmark with eight text-rich datasets, two tasks (transductive and inductive DyTAG generation), and metrics covering structure, time, and text. An LLM-based multi-agent framework, GAG-General, standardizes evaluation and delivers competitive results. Experiments show that high-quality text and structure are crucial, enabling rigorous assessment."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n\n2. The authors introduce several dynamic text-attributed graph datasets with higher text quality than existing benchmarks.\n\n3. The authors design an LLM-based multi-agent generative framework; an accompanying illustration would improve comprehensibility."}, "weaknesses": {"value": "### **Major Concern**\n\n1. **Claim of the “first” generative DyTAG benchmark.**\n   The paper claims to be the first generative DyTAG benchmark, yet it does not evaluate **existing TAG-generation or DyG-generation methods**. Instead, it varies only the LLM backbones for DyTAG generation. Without comparisons to prior generative baselines, the “first” or “state-of-the-art” claim is overstated. Please include representative TAG/DyG generative methods (or strong non-LLM baselines) and report side-by-side results.\n\n2. **Assessment of text quality and incomplete reporting on DTGB.**\n   The paper asserts that existing DyTAG datasets have poor text quality; however, the reported results on DTGB appear strong in both graph-embedding metrics and textual quality scores. Moreover, only **partial** DTGB results are shown.\n\n   * Please explain the selection criteria for the reported DTGB subsets.\n   * Provide complete results on all DTGB datasets for both generation tasks (transductive and inductive), using the same metrics and settings.\n\n3. **Direct comparison between the new datasets and existing ones.**\n   To establish the value of the introduced datasets, the main paper should present **explicit empirical generation comparisons** between DTGB and the newly proposed datasets. This will make the dataset-quality argument far more convincing.\n\n4. **Metric definitions and justification.**\n   Evaluation is central in generative work, yet the three metrics are not formally defined. Please:\n\n   * Provide precise mathematical definitions (inputs, outputs, normalization, and aggregation).\n   * Justify why each metric is appropriate for DyTAG generation and discuss potential limitations or failure modes.\n   * Clarify implementation details\n\n\n### **Minor Concern**\n\n   Could you please elaborate on concrete use cases for dynamic text-attributed graph generation?"}, "questions": {"value": "All my major and minor concerns are mixed in the Weaknesses part. Please kindly refer to it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HacOszioyJ", "forum": "5UFUHUC5qP", "replyto": "5UFUHUC5qP", "signatures": ["ICLR.cc/2026/Conference/Submission5643/Reviewer_iU4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5643/Reviewer_iU4e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886271752, "cdate": 1761886271752, "tmdate": 1762918172724, "mdate": 1762918172724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Summary"}, "comment": {"value": "We thank all reviewers and ACs for their time and constructive feedback. During the rebuttal period, we have uploaded a revised version of our manuscript in response to the reviewers’ concerns. All modifications are explicitly detailed in our point-by-point responses to each reviewer’s official comments. To clearly distinguish new content from the original submission, **all newly added texts, figures, and tables are highlighted in blue font**. To further clarify the scope of our revisions, we summarize the key updates below:\n\n---\n**For Reviewer iU4e:**\n1. We have added an additional feature-supportive dynamic graph generation baseline, TIGGER-I, and reported its comparative performance against GAG-General in terms of graph structural quality and graph embedding similarity. These results are now included in **Appendix D.11, Table 53** (previously Table 49 in the original submission).\n2. We have fully expanded the experimental evaluation to cover all eight DTGB datasets under both TDGG (transductive) and IDGG (inductive) generation settings. Comprehensive comparisons with GDGB are provided across three dimensions: graph structural quality, textual quality, and graph embedding consistency. The complete TDGG results are integrated into the updated **Tables 11–13**, while IDGG results are presented in **Tables 14–16 (see Appendix A.5)**.\n3. **Appendix B.3** has been enhanced to provide more precise mathematical definitions and implementation details of our evaluation metrics, with particular emphasis on the LLM-as-Evaluator framework—specifically, the prompt templates, scoring criteria, and post-processing logic for textual quality assessment.\n\n---\n**For Reviewer EtXQ:**\n1. We have added **Appendix D.14**, which introduces a cross-snapshot semantic drift analysis by computing BERT-based cosine similarity between consecutive temporal snapshots of the generated DyTAGs.\n2. We have added **Appendix D.15**, which presents a small-scale human evaluation to assess whether the generated DyTAGs satisfy human amenity.\n\n---\n**For Reviewer xhXC:**\n1. We have strengthened the descriptions in **Appendices B.3 and C.4** to explicitly clarify that the LLM-as-Evaluator for textual quality assessment consistently uses GPT as the backbone model, with fixed hyperparameters (e.g., temperature, max_tokens) and a standardized prompt template. We further emphasize that this evaluation LLM is independent of the LLM backbone used within GAG-General (which may vary across baselines).\n\n---\nWe hope this summary provides a clear and transparent overview of our revisions and facilitates a more informed re-evaluation of our work. We will continue refining the manuscript based on the ongoing discussion during the rebuttal process."}}, "id": "c8nsTkhNhn", "forum": "5UFUHUC5qP", "replyto": "5UFUHUC5qP", "signatures": ["ICLR.cc/2026/Conference/Submission5643/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5643/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission5643/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763447040889, "cdate": 1763447040889, "tmdate": 1763447158145, "mdate": 1763447158145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}