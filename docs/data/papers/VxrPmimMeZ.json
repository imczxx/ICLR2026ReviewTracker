{"id": "VxrPmimMeZ", "number": 7634, "cdate": 1758029973561, "mdate": 1759897842325, "content": {"title": "SOFTADACLIP: A SMOOTH CLIPPING STRATEGY FOR FAIR AND PRIVATE MODEL TRAINING", "abstract": "Differential privacy (DP) provides strong protection for sensitive data, but often reduces model performance and fairness, especially for underrepresented groups. One major reason is gradient clipping in DP-SGD, which can disproportionately suppress learning signals for minority subpopulations. Although adaptive clipping can enhance utility, it still relies on uniform hard clipping, which may restrict fairness. To address this, we introduce SoftAdaClip, a differentially private training method that replaces hard clipping with a smooth, tanh-based transformation to preserve relative gradient magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48% compared to Adaptive-DPSGD, and these reductions in subgroup disparities are statistically significant. These findings underscore the importance of integrating smooth transformations with adaptive mechanisms to achieve fair and private model training.", "tldr": "", "keywords": ["Fairness", "Differential Privacy", "Gradient Clipping", "Utility", "Deep Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb6b060d35146033a3ba5f388e9a6b64e6f310c3.pdf", "supplementary_material": "/attachment/7ea42c265c51922595eacb8de5de84852c7bded1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SoftAdaClip, a differentially private (DP) training strategy that replaces the conventional hard clipping in DP-SGD with a smooth tanh-based transformation, integrated into an adaptive clipping framework. The authors argue that hard clipping disproportionately suppresses gradients from underrepresented subgroups, contributing to fairness degradation during DP training. SoftAdaClip aims to preserve relative gradient magnitudes while maintaining sensitivity bounds necessary for DP. Experiments on three datasets (MIMIC-III, GOSSIS-eICU, Adult Income) show reduced subgroup disparities and often improved utility over DP-SGD and Adaptive-DP-SGD. The paper also analyzes clipping behaviors and conducts ablations to separate smoothing from adaptivity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a meaningful and timely challenge at the intersection of privacy and fairness, where trade-offs are often assumed unavoidable. The proposed method is conceptually simple yet intuitively motivated and appears compatible with standard training infrastructures. The empirical results show notable improvements in subgroup loss disparities across multiple real-world datasets, particularly in healthcare domains where fairness issues can have severe consequences. The statistical significance analysis is appreciated, and the ablation study helps clarify the distinct role of adaptivity. The work would likely be of interest to both privacy researchers and practitioners deploying DP in sensitive domains."}, "weaknesses": {"value": "Although promising, the novelty is incremental: the method mainly replaces a min() rescaling with a tanh-based one inside an existing adaptive clipping algorithm. The theoretical foundations stop at sensitivity bounding; there is no deeper optimization or fairness analysis (e.g., convergence, bias dynamics, subgroup gradient geometry, compatibility with Rényi-DP accounting). The fairness evaluation relies almost exclusively on loss gaps; no standard fairness metrics like Equal Opportunity or demographic parity gaps appear. The method struggles on low-gradient regimes, requiring manual threshold tuning, which undermines the claim of being a drop-in robust improvement. Presentation needs polishing: key equations and experimental setups feel buried, and figures require clearer labeling and narrative connection. References to prior fairness-aware DP methods are limited in experimental comparison."}, "questions": {"value": "Are there guarantees that replacing min() with tanh() preserves unbiasedness or improves optimization dynamics? Any theoretical characterization of how gradient direction distortion differs from hard clipping?\n\nIn Algorithm 2, how does adaptive update interact with tanh scaling? Could certain C trajectories amplify subgroup divergence?\nPrivacy accounting. Since tanh depends on gradient norm, is there any subtle impact on the DP accountant or amplification? Please clarify formally.\n\nWhy not include widely-used fairness metrics (AUC/F1 gaps per subgroup, equalized odds violations, accuracy parity)? Loss gaps alone may not reflect actual decision-level harms.\n\nWhy is DPSGD-Global-Adapt (Esipova et al., 2022) excluded from experiments? If code is unavailable, can closer replication or alternative strong baselines (e.g., subgroup-adaptive clipping techniques) be tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBCwE6LU6w", "forum": "VxrPmimMeZ", "replyto": "VxrPmimMeZ", "signatures": ["ICLR.cc/2026/Conference/Submission7634/Reviewer_aiqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7634/Reviewer_aiqC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656551409, "cdate": 1761656551409, "tmdate": 1762919712120, "mdate": 1762919712120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors highlight the problem of unfairness when performing private training using methods like DP-SGD, which employ methods like gradient clipping to bound sensitivity for adding differentially private noise. They propose replacing the hard clipping operation with a smooth tanh-based gradient operation, and demonstrate improvements in group fairness after performing differentially private training using their method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**[S1]** Very well-motivated question, highlighting how clipping in DP-SGD may lead to unfairness (as shown in previous works mentioned in the paper, viz. Esipova et al, Tran et al, Bagdasaryan et al)\n\n**[S2]** Using a tanh-based smooth transformation instead of hard clipping is less aggressive and lossy, still retaining gradient information about groups with large gradients.\n\n**[S3]** The design and description of the proposed method are very clearly and unambiguously done, including justifying the design of the gradient transformation and privacy-preserving clipping threshold updates.\n\n**[S4]** In-depth and frank discussion about the limitations of the work is included, including key points about sensitivity to hyperparameter tuning when small clipping thresholds are adopted.\n\n**[S5]** The use of a Wilcoxon signed-rank test with Bonferroni correction to illustrate the statistical significance of the improvements in loss gap is thoroughly appreciated and reflects good practices in presenting empirical results!\n\n**[S6]** Ablation study on adaptive thresholding is useful and clearly demonstrates the importance of pairing smooth transformation with adaptive thresholding."}, "weaknesses": {"value": "**[W1]** The authors provide a justification for using different hyperparameters for non-private and private training, which is convincing. However, the only concern I have is that this makes it difficult to fairly assess the true utility loss due to private training.\n\n**[W2]** **Needs significant editing.** The presentation of the work needs significant improvements and editing. For example, there are duplicate paragraphs in page 6 in Section 4.1, where paragraph 1 and the first part of paragraph 2 state the same thing in slightly different language, suggesting redundant text was left over while drafting. There is also a missing appendix reference in the last line right before Section 5.1 (\"dataset-specific results are provided in Appendix ??\").\n\n**[W3]** **Important missing baselines.** This work only looks at limited baselines, primarily at non-private clipping-based baselines. However, I feel like this work’s contribution cannot be truly assessed without meaningful comparison against important fair DP-SGD baselines like [1] and [2], which employ methods like Langrangian dual based fairness-constrained training, or Esipova et al (the comparison against Esipova et al is very limited, and for a venue like ICLR, it is not appropriate to simply defer a comparison to future work; *you must compare against all important and relevant baselines in your own work*) At the end of the day, while their work proposes improvements to the clipping paradigm used in DP-SGD, it is important to see if it actually contributes an overall fairer way of doing DP-SGD, or if it does not lead to any improvements in fairness when added to/compared against these fair DP-SGD baselines, rendering it functionally redundant. Put another way, while the proposed method might improve upon DP-SGD and Andrew et al., it is unclear if it will actually lead to meaningful improvements (if any at all) in fairness when compared against or integrated into existing sophisticated SoTA fair DP-SGD baselines like [1] or [2]. The absence of results against such baselines presents a weaker assessment. Put another way, this paper shows that the proposed method is fairer than non-fairness-aware baselines, but does it really outperform prominent and existing fair DP-SGD methods, or is it inferior to them/does not provide meaningful improvement in conjunction with them? This is an important question to answer for a venue of this stature.\n\n**[W4]** The experimental settings section is presented poorly. The models used for eICU and Income datasets are not described and are vaguely called the simple and complex models, with actual model descriptions deferred to the appendix, while **the experimental setting section instead spends most of its real estate talking about what related work does** (which is better discussed in related work or in the appendix as additional details, while priotizing mentioning the settings used in **this paper**). Therefore, the experimental setting section does not discuss what it is intended to do: properly and exhaustively describe the experimental settings used in **this** paper. I heavily implore the authors to improve the presentation of this section and include concrete details about **their** settings here (which should take precedence here!) instead of sending them to the appendix (in fact, you can actually send the related work discussion in this section to the appendix, but what has been sent to the appendix should actually be in the main text in this section!), especially for a submission to a venue like this\n\n---\n\n## References\n\n[1] Tran, Khang et al. “FairDP: Achieving Fairness Certification with Differential Privacy.” 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) (2023): 956-976.\n\n[2] Tran, Cuong et al. “Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach.” ArXiv abs/2009.12562 (2020): n. pag."}, "questions": {"value": "[Q1] Can you please address W1 and make sure that the best possible set of hyperparameters is used for each setting, perhaps via a hyperparameter search, to obtain the best possible non-private utility to compare against?\n\n[Q2] Can you, to the best of your ability, add more comparisons against SoTA baselines for better showcasing the efficacy of your method (as mentioned in W3; please feel free to include any more baselines than those included as well)? I believe this will make your paper significantly more convincing.\n\n[Q3] Pursuant to W4, can you please provide a much better drafted experimental settings section that focuses primarily on what **you** do in this paper, while making sure to exhaustively and unambiguously discuss all the settings/models, etc., used?\n\nIn short, I believe this paper has the potential to make a good contribution. However, in its current state, with its experiments, lack of comparison against important SoTA methods (or any in-depth comparison against even just any fair DP-SGD method at all), ambiguity in choice of hyperparameters for private/non-private settings, presentation, etc., it does not inspire strong support from my end. However, I’ll be more than happy to engage with the authors, and if there’s enough improvement, I would be happy to strengthen my support for the paper (contingent upon my concerns being satisfactorily addressed)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zyqStK2p6j", "forum": "VxrPmimMeZ", "replyto": "VxrPmimMeZ", "signatures": ["ICLR.cc/2026/Conference/Submission7634/Reviewer_Zq5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7634/Reviewer_Zq5M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791096276, "cdate": 1761791096276, "tmdate": 1762919711808, "mdate": 1762919711808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SoftAdaClip, an alternative to (Adaptive) Differentially Private (DP) SGD that replaces hard gradient clipping with a smooth, tanh-based transformation to better preserve relative gradient magnitudes.\nSimilar to prior work (Adaptive-DPSGD), the method adaptively adjusts the clipping threshold during training.\nSoftAdaClip demonstrates improved fairness by reducing subgroup disparities (measured in loss differences) across text and tabular datasets compared to both DP-SGD and Adaptive-DPSGD."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* SoftAdaClip introduces a simple yet elegant/effective modification to Adaptive-DPSGD.\n* The paper is very verbose, well-motivated, and easy to follow.\n* The evaluation is performed over different data modalities (text and tabular data).\n* The results are promising/consistent, showing clear improvement over DP-SGD and Adaptive-DPSGD in terms of the measured fairness metric (loss disparities)."}, "weaknesses": {"value": "## Weaknesses:\n* While SoftAdaClip is simple/elegant (which is great), the novelty feels somewhat incremental (the core idea boils down to 1 line of code) may not qualify as a substantial contribution for a top-tier conference.\nOverall, the work is promising, but it currently reads more like a strong workshop/early-stage research paper than a full conference paper.\nI encourage the authors to continue working on the paper.\n\n* The results reported in the abstract are slightly misleading -- it would be more appropriate to report average improvements over DP-SGD/Adaptive-DPSGD, rather than cherrypicking the best differences.\nAdditionally, from Table 1, while SoftAdaClip achieves lower loss than Adaptive-DPSGD, but this does not seem to lead to better accuracy/f1.\nThis is only briefly mentioned on the last page.\nGiven that accuracy/f1 are more practically important than loss, this deserves more discussion.\nFinally, accuracy per subgroup (as in (Bagdasaryan et al., 2019)) is not provided, which will be valuable for understanding fairness performance across subgroups.\n\n* The paper primarily focuses on two DP-SGD alternatives -- Adaptive-DPSGD and DPSGD-Global-Adapt. However, several other important works should be considered or at least be acknowledged (this is not an exhaustive list):\n\t* tempered sigmoid activations in DP-SGD [1]\n\t* private and fair classification with pre-traiing [2]\n\t* FairDP-SGD and FairPATE [3]\n\t* DP-SGD without clipping [4, 5]\n\n* In Table 4, the presentation of gradient norms seems confusing: it reports Before -> Diff (After) rather than Before -> After (Diff). Moreover, the reported clipping for SoftAdaClip appears larger than for the other methods, which seems inconsistent with the claim in Section 4.1 and Appendix B.3 that SoftAdaClip applies less clipping.\nThis is somewhat intuitive, since SoftAdaClip could clip some gradients more than hard clipping (as shown in Figure 1).\nIt would be helpful if the authors clarify this discrepancy. \n\n* While the paper is easy to follow, the writing is extremely verbose and contains repetitions across sections, which can make it harder to navigate.\n\t* Section 2 (Related Work) contains repeated information between the first subsection and Section 2.1.\n\t* Section 2.1 mixes background/preliminaries with related work. It would be clearer to separate these two aspects into distinct sections.\n\t* Section 4.1 (Gradient Behavior Analysis) discusses specific results (e.g., Table 4) that seem more appropriate for Section 5 (Results).\n\t* Section 5 (Results) begins by continuing discussion of the experimental setup, which should be fully contained in Section 4.\n\t* Sections 6 (Limitations) and 7 (Conclusion) take a full page, which could be better utilized by moving some experiments/tables from the Appendix.\n\n## Minor Weaknesses/Comments:\n* The variable \\epsilon in Algorithm 2 may be confusing, as \\epsilon is already used in the DP definition; consider using a different symbol.\n* References are inconsistently cited (e.g., \\citet vs. \\cite/\\citep).\n* Small typos/punctuation errors -- \"It would nevertheless be valuable to It would still be beneficial to,\" etc.\n\n\n## References:\n\n[1] Papernot et al., Tempered Sigmoid Activations for Deep Learning with Differential Privacy. In AAAI, 2021\n\n[2] Berrada et al., Unlocking Accuracy and Fairness in Differentially Private Image Classification. 2023\n\n[3] Yaghin et al., Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility. In RegML at NeurIPS, 2023\n\n[4] Bethune et al., DP-SGD Without Clipping: The Lipschitz Neural Network Way. In ICLR, 2024\n\n[5] Zhang et al., Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach. In ICLR, 2024"}, "questions": {"value": "## Questions/Suggestions for Improvements:\n* Appendix A states that demographic subgroups are balanced, but it is unclear whether the target labels are imblanced/balanced. Can the authors clarify the label distributions?\n* How do different imbalance ratios and different values of \\epsilon affect SoftAdaClip’s performance?\n* Have the authors evaluated performance on smaller subgroups (e.g., >20 groups) as in (Bagdasaryan et al. 2019)?\n* Why was the tanh function chosen for smooth clipping? Would other functions (e.g., sigmoid) work as well?\n* It is unclear whether the methods uses DP-Adam or DP-SGD as base for SoftAdaClip. From the code, it seems DP-Adam is used -- if so, is it defined in the paper (I only can see DP-SGD)? Are there any differences in performance between DP-Adam and DP-SGD?\n* What is the difference between the clipping parameters C and C_0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WbH8VBDNOl", "forum": "VxrPmimMeZ", "replyto": "VxrPmimMeZ", "signatures": ["ICLR.cc/2026/Conference/Submission7634/Reviewer_i9qR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7634/Reviewer_i9qR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866647627, "cdate": 1761866647627, "tmdate": 1762919711511, "mdate": 1762919711511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SoftAdaClip proposes a novel DP training method integrating a smooth tanh-based transformation into adaptive clipping. It is proposed to mitigate a disproportionate effect of DP on minority groups in terms of performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- tested on 3 real-world datasets: MIMIC-III (clinical text dataset), GOSSIS-1-eICU (structured healthcare dataset), Adult Income (tabular dataset).\n- tackles important problem of fairness under DP training"}, "weaknesses": {"value": "- the paper should include experiments demonstrating the efficacy of SoftAdaClip in mitigating fairness disparities in a standard vision task to support general application of the method."}, "questions": {"value": "How was the smooth function chosen? Why not using a different sigmoidal or smooth bounded function instead of tanh?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RxYp8dYLJ6", "forum": "VxrPmimMeZ", "replyto": "VxrPmimMeZ", "signatures": ["ICLR.cc/2026/Conference/Submission7634/Reviewer_SUNv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7634/Reviewer_SUNv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957405548, "cdate": 1761957405548, "tmdate": 1762919711124, "mdate": 1762919711124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a novel approach to building fair and private model training through adaptive clipping  using tanh transformation that preserves magnitudes of gradients. The paper addresses a clear problem of privacy-fairness tradeoff with a unique approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Mechanism design is clean and nicely presented, can be integrated well into the existing libraries and pipelines\n- tanh idea is also quite strong, from the literature I know DPSGD always focused on balancing noise multiplier with clipping bound without focusing on how the clipping is performed.\n- Empirically there is some evidence of improved fairness with same guarantees"}, "weaknesses": {"value": "Overall, while the paper looks great it lacks experimental evidence of the usefulness of the method, here are a couple of questions:\n- I believe the paper should include at least a basic example of applying proposed method to vision problems, even MNIST or CIFAR is enough. It is stated as out-of-scope in limitations, but it will still be very useful to have.\n- Additionally unbalancing these vision datasets and demonstrating how far the method can apply.\n- I would like to see how different epsilon changes the effect of the method. \n- I am also curious about selection of the noise multiplier (noise/clipping threshold) and how this selection affects the method. \n- Can we have different fairness metrics besides difference in loss? maybe accuracy on test sets? \n- Comparison with related work. The paper only compares with DPSGD and adaptive clipping (Andrew et al) but even adaptive clipping has different settings. It will be helpful to also compare with Bu et al 2024 (automatic clipping)."}, "questions": {"value": "Addressing weakness above will significantly help the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FWxbc9r2IQ", "forum": "VxrPmimMeZ", "replyto": "VxrPmimMeZ", "signatures": ["ICLR.cc/2026/Conference/Submission7634/Reviewer_3XfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7634/Reviewer_3XfE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971890259, "cdate": 1761971890259, "tmdate": 1762919710632, "mdate": 1762919710632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}