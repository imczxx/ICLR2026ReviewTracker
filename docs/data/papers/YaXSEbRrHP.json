{"id": "YaXSEbRrHP", "number": 23006, "cdate": 1758338038290, "mdate": 1759896836189, "content": {"title": "A Hierarchical Circuit Symbolic Discovery Framework for Efficient Logic Optimization", "abstract": "The efficiency of Logic Optimization (LO) has become one of the key bottlenecks in chip design. To prompt efficient LO, many graph-based machine learning (ML) methods, such as graph neural networks (GNNs), have been proposed to predict and prune a large number of ineffective subgraphs of the LO heuristics.  However, the high inference cost and limited interpretability of these approaches severely limit their wide application to modern LO tools.  To address this challenge, we propose a novel **H**ierarchical C**i**rcuit **S**ymbolic Discovery Framework, namely HIS, to learn a *lightweight* and *interpretable* symbolic function that can *accurately* identify ineffective subgraphs for efficient LO.  Specifically, HIS proposes a hierarchical tree structure to represent the circuit symbolic function, where every layer of the symbolic tree performs an efficient and interpretable message passing to capture the structural information of the circuit graph. To learn the hierarchical tree, we propose a circuit symbolic generation framework that leverages reinforcement learning to optimize a structure-aware Transformer model for symbolic token generation.  To the best of our knowledge, HIS is *the first* approach to discover an efficient, interpretable, and high-performance symbolic function from the circuit graph for efficient LO.  Experiments on two widely used circuit benchmarks show that the learned graph symbolic functions outperform previous state-of-the-art approaches in terms of efficiency and optimization performance. Moreover, we integrate HIS with the Mfs2 heuristic, one of the most time-consuming LO heuristics. Results show that HIS significantly enhances both its efficiency and optimization performance on a CPU-based machine, achieving an average runtime improvement of 27.22% and a 6.95% reduction in circuit size.", "tldr": "Chip Design; Logic Optimization; Symbolic Regression; Graph Neural Networks", "keywords": ["Electronic Design Automation; Logic Synthesis; Large Language Models;"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fbcf2242239835e16459f0055405426c5a0f207.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the bottleneck of low efficiency in LO heuristic algorithms for chip design, as well as the issues of high inference cost and poor interpretability in existing GNN-based optimization methods, the researchers propose HIS, a hierarchical circuit symbol discovery framework. This framework leverages a hierarchical symbolic tree structure and a structure-aware Transformer optimized with PPO to generate lightweight, interpretable symbolic functions for identifying invalid subgraphs. Evaluated on the EPFL and IWLS benchmarks, HIS outperforms SOTA methods like COG and CMO in terms of prediction recall. Furthermore, when integrated with Mfs2, it achieves an average runtime reduction of 27.22%, a circuit size reduction of 6.95%, and demonstrates 296x and 254x faster inference speed compared to COG.​"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The researchers tackle the critical problem of logic optimization in EDA, achieving notable advances on an NP-hard problem through the design of HIS.\n2. The study validates the balance between efficiency, accuracy, and interpretability on industrial-grade benchmarks. The experimental design is solid and demonstrates high reproducibility, achieving a 27.22% reduction in runtime and a 6.95% reduction in circuit size."}, "weaknesses": {"value": "1. The work lacks a mathematical proof that the hierarchical symbolic tree can equivalently approximate the message-passing capability of a GNN. It relies solely on the experimental observation that its performance with L=2 is comparable to that of a 2-layer GNN. The work lacks a theoretical analysis of the necessity of the operators (e.g., log, exp) included in the symbol library.\n2. The paper does not derive the generalization error bound for the symbolic function, such as how the sensitivity of the $\\beta / \\gamma$ parameters changes as the number of circuit nodes increases. While positioning itself as a paper that emphasizes \"interpretability,\" it provides little theoretical proof or analysis to substantiate this claim.\n3. This paper only validates the Mfs2 heuristic for logic optimization, without testing mainstream pre-mapping heuristics such as Resub and Rewrite. In my view, the practical value of this work in the broader context of LO problems would be significantly strengthened if it included at least three different LO heuristics. Otherwise, there is reasonable doubt that the proposed framework might be effective only for Mfs2.\n4. While the EPFL and IWLS benchmarks cover industrial-scale circuits, they do not include real circuit data from advanced process technologies (e.g., 7nm or 3nm). Therefore, I believe testing on real industrial data would provide more compelling evidence of the method's robustness.\n5. The work fails to investigate the impact of the computation tree depth L (using only L=2) on performance, thus making it impossible to determine if L=3 or L=4 could enhance the structure-capturing capability. It also omits an analysis of how circuit type (e.g., combinational vs. sequential logic) influences the effectiveness of HIS.\n6. It is recommended to include a comparison with more recent methods like Boolformer."}, "questions": {"value": "1. Please respond to the concerns I have raised in the 'weaknesses' section. If the revision can adequately address most of the critical issues, I would consider raising the score.\n2. In my personal opinion, the contribution of this paper lies more in its solid experimentation and reproducibility than in its theoretical advancement in ML. Considering this, EDA-focused conferences (such as DAC or ISCA, with an upcoming November deadline) would be a more suitable venue for it. Of course, I also believe that if the intention is to submit to DAC or ISCA, experiments related to scalability, as well as validation under advanced process technologies (below 7nm), would need to be supplemented."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YswTb61src", "forum": "YaXSEbRrHP", "replyto": "YaXSEbRrHP", "signatures": ["ICLR.cc/2026/Conference/Submission23006/Reviewer_KPds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23006/Reviewer_KPds"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633942996, "cdate": 1761633942996, "tmdate": 1762942473345, "mdate": 1762942473345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HIS (Hierarchical Circuit Symbolic Discovery Framework), a novel approach for efficient logic optimization in chip design. The key innovation lies in learning hierarchical symbolic functions that can capture circuit structural information while maintaining interpretability and computational efficiency. The framework uses structure-aware Transformers with reinforcement learning to generate symbolic trees that perform message passing similar to GNNs but with significantly lower inference cost. Experiments on EPFL and IWLS benchmarks demonstrate that HIS achieves runtime improvement and circuit size reduction when integrated with the Mfs2 heuristic, while being faster than GNN-based methods during inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) novel problem formulation: First GNN-free work to apply symbolic discovery to circuit graphs for logic optimization.\n(2) Good originality: proposing a hierarchical symbolic tree structure that independently learns to mimic GNN-style message passing.\n(3) Comprehensive Evaluation: The authors conducted a comprehensive empirical evaluation to validate their approach, including inference time, online/offline evaluation, QoR improvement etc.\n(4) Clear and Intuitive Presentation: The paper presents a clear writing style and visual aids."}, "weaknesses": {"value": "(1) Limited Analysis on Training Overhead: While the paper excellently highlights the inference speed-up, it lacks a analysis of the training time overhead, which can be substantial for reinforcement learning-based methods.\n(2)  Inconsistent Performance Gains: The experimental results do not consistently demonstrate the superiority of HIS over the state-of-the-art CMO. While HIS shows an advantage in some specific settings (e.g., HIS-Mfs2 on 'Hyp' and 'DesPerf'), it underperforms CMO in QoR-focused scenarios (e.g., 2HIS-Mfs2), raising concerns about the stability and real-world benefit of the proposed method.\n(3) The Claim of Interpretability is Not Substantiated by the Final Outputs: While HIS proposes a conceptually novel hierarchical framework, its final symbolic expressions seems are not demonstrably more interpretable than those from CMO."}, "questions": {"value": "(1) What is the computational cost of training the hierarchical symbolic trees compared to GNN based symbolic discovery method such as CMO?\n(2) Is the claimed interpretability advantage primarily from the hierarchical learning framework, or do the final, complex symbolic expressions offer more practical insights for domain experts than the simpler, non-hierarchical functions from methods like CMO?\n(3) The paper focuses solely on the Mfs2 heuristic. How does the HIS framework generalize to other critical LO operators with different graph structures and optimization goals, such as Refactor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IGErjYTHnD", "forum": "YaXSEbRrHP", "replyto": "YaXSEbRrHP", "signatures": ["ICLR.cc/2026/Conference/Submission23006/Reviewer_DPdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23006/Reviewer_DPdp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834510983, "cdate": 1761834510983, "tmdate": 1762942473143, "mdate": 1762942473143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Hierarchical Circuit Symbolic Discovery Framework (HIS) for efficient logic optimization. HIS aims to address the high inference cost and limited interpretability of existing machine-learning cost-prediction methods, such as GNNs. The framework leverages reinforcement learning to train a structure-aware Transformer model that automatically discovers a lightweight and interpretable symbolic function. This function, represented as a hierarchical tree, mimics the message-passing of GNNs to identify ineffective node-level transformations for pruning. Extensive experiments on two public benchmarks demonstrate that HIS outperforms state-of-the-art methods in prediction accuracy and optimization efficiency. When integrated with the Mfs2 heuristic, HIS significantly reduces runtime while maintaining or even improving optimization quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-identified the practical problems of GNNs:** The paper correctly pinpoints the critical bottlenecks— **high inference cost and lack of interpretability and** —that prevent state-of-the-art GNNs from being widely deployed in real-world EDA tools. This shows a clear understanding of industrial needs beyond just academic metrics.\n2. **Innovative methodology combining generation and RL:** The core idea of using a **generative model (Transformer) coupled with Reinforcement Learning** to discover an interpretable symbolic tree is highly innovative. It reframes the problem from a black-box prediction task to a structured search for a human-readable function, effectively bridging the gap between performance and interpretability.\n3. **Solid integration with industry tools and demonstrated gains:** The research is grounded in practicality. By seamlessly integrating with a standard EDA framework (ABC) and a critical, time-consuming heuristic (Mfs2), and demonstrating **significant, measurable improvements** (27.22% speedup, 6.95% size reduction), the work is far more **substantial and deployable** compared to many theoretical GNN studies that lack a clear path to real-world application."}, "weaknesses": {"value": "1. **Misleading figures and poor readability:** The writing and diagrams are currently a significant weakness and hinder understanding.\n    - **Figures:** Figure 1 mentions \"GNNs\" in a way that is confusing and seems disconnected from the HIS methodology. In Figure 2, the flow of \"Generation Process\" and \"Model Training\" are not expressive and hard to understand.\n    - **Writing:** The methodology section requires significant polishing. It needs to guide the reader through the complex pipeline in a simpler, more logical, and step-by-step manner. The current presentation is dense and difficult to follow.\n2. **Omission of training cost:** While the paper justifiably highlights the model's superior **inference efficiency**, it omits any discussion of the **training time and computational resources** required. For a comprehensive cost-benefit analysis, especially for industrial adoption, the substantial computational overhead of training the Transformer with RL must be considered and disclosed.\n3. **Non-actionable interpretability:** The claimed \"interpretability\" is superficial in its current form. While the final symbolic tree is indeed human-readable, the pipeline does **not leverage this interpretability to adjust or improve the model**. It is a one-way, post-hoc explanation. There is no feedback loop where a human expert can correct or guide the symbolic discovery process based on the generated formulas. \n4. **Lack of qualitative analysis:** The authors missed a key opportunity to use interpretability for a compelling **qualitative analysis**. The paper lacks a case study or an in-depth discussion that answers questions like: \"What did we learn from the discovered formulas?\" or \"Can we see a specific example where the symbolic tree correctly identified a complex structural pattern that a human would have missed?\" Including such an analysis would have powerfully demonstrated the unique advantage of their method over pure black-box approaches."}, "questions": {"value": "1. **Scalability of the Symbolic Tree:** The paper demonstrates the effectiveness of relatively small symbolic trees for integration into existing tools. Could the authors comment on the scalability of their approach? What would be the computational cost and potential performance implications of training a model to discover significantly larger and more complex symbolic expressions?\n2. **Scaling Laws and Potential for a Foundational Model:** A key question for the long-term impact of this methodology is its adherence to scaling laws. Do the model's performance and generalization capability improve predictably with increased model capacity and training data volume? If empirical evidence supported such scaling laws, it would justify the development of a large-scale, pre-trained symbolic model for EDA. Can the authors provide any insight or preliminary results on this front?\n3. **Sensitivity to the Symbolic Library:** The choice of the symbolic library is a strong prior in the discovery process. The results appear sensitive to this choice, as more complex operators (e.g., `log`, `exp`) are available yet absent from the discovered functions in Figure 5. A dedicated ablation study on the composition of the symbolic library is lacking. How do the results change if key operator types (e.g., aggregation functions) are removed? Such a study is crucial for understanding the true expressiveness of the discovered functions and the biases introduced by the library."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PTfQMcT456", "forum": "YaXSEbRrHP", "replyto": "YaXSEbRrHP", "signatures": ["ICLR.cc/2026/Conference/Submission23006/Reviewer_63LY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23006/Reviewer_63LY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852868447, "cdate": 1761852868447, "tmdate": 1762942472771, "mdate": 1762942472771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing logic optimization tools suffer from high inference costs and limited interpretability. The authors developed HIS, an effective logic optimization model that leverages hierarchical symbolic function representation and a corresponding group reward mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written, easy to follow\n* Authors performed various experiments including ablation studies.\n* The problem the authors aimed to address is practical and addresses a real demand in the field."}, "weaknesses": {"value": "* I haven’t read many AI papers on logic optimization, so I’m not very familiar with the field, but the number of baselines seems rather limited. It would be beneficial to include more general AI models as baselines, even if they were not originally designed for logic optimization, as long as they can be applied to this problem.\n* It appears that the experiments were conducted only once and the performance was reported based on that single run. To ensure that the model’s performance is not dependent on a specific random seed but is statistically meaningful, it is necessary to repeat the experiments multiple times and report the mean and standard deviation of the performance.\n* From Table 3, it can be seen that for the Hyp, Multiplier, Square, and Conmax circuits, the performance difference with or without group optimization is not very large, whereas for DesPerf and Ethernet, the difference is significant. This suggests that while certain components of the model are highly beneficial for some circuits, they contribute little to others. It would be helpful if the authors could provide an explanation of whether this variation is related to the characteristics of the circuits."}, "questions": {"value": "See the 'weakness' part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nAWPDsBYyM", "forum": "YaXSEbRrHP", "replyto": "YaXSEbRrHP", "signatures": ["ICLR.cc/2026/Conference/Submission23006/Reviewer_L1RN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23006/Reviewer_L1RN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997539790, "cdate": 1761997539790, "tmdate": 1762942472461, "mdate": 1762942472461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}