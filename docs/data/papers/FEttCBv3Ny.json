{"id": "FEttCBv3Ny", "number": 18703, "cdate": 1758290286592, "mdate": 1759897086606, "content": {"title": "Transporting Tokens: Optimal-Transport View of Parallel LLM Decoding", "abstract": "Autoregressive decoding is a primary bottleneck for large language models (LLMs), as its inherent sequentiality severely limits inference speed. While speculative decoding methods mitigate this via a draft-and-verification pipeline their effectiveness is severely constrained by dependency on draft model quality and availability. We rethink the generation pattern and introduces a novel theoretical perspective by reframing token generation as a predictable state transition process in probability space, formalized through Optimal Transport (OT) theory. We demonstrate that the temporal consistency of hidden states induces a stable transport map, enabling theoretically grounded multi-step prediction. Building on this insight, we develop SHAPE, an OT-based predictor that implements lightweight Sinkhorn iterations. Extensive evaluations across diverse models (e.g., Qwen, Vicuna, LLaMA, DeepSeek) and tasks (text, code, math) show that SHAPE achieves up to 5.23× speedup with minimal quality loss ($\\leq 1.2\\%$ accuracy drop), empirically validating our distributional transition hypothesis. This work establishes a new theoretical foundation for understanding autoregressive decoding and a practical path toward high-speed generation beyond token-wise limitations.", "tldr": "We reframe autoregressive decoding as a predictable state transition in probability space. Using Optimal Transport, we align hidden states across steps, enabling parallel decoding with up to 5.23× speedups and minimal accuracy loss.", "keywords": ["Parallel Decoding; Inference acceleration; Optimal transport"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f39dcfd29da0c2b93251a478abb59282a11de637.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel theoretical and practical framework for accelerating autoregressive decoding in large language models (LLMs) by reframing token generation as a probability distribution transition process governed by optimal transport (OT) theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 The paper elegantly bridges optimal transport theory and autoregressive decoding, formalizing token generation as a structured evolution of probability distributions. The stability analysis of hidden-state transitions (e.g., bounded OT distances) provides a rigorous foundation for multi-step prediction.\n\n2 SHAPE’s plug-and-play design requires no draft model, reducing deployment complexity. \n\n3 The Tree Rejection Sampling algorithm dynamically selects optimal paths with minimal overhead, balancing parallelism and quality."}, "weaknesses": {"value": "1 While comparisons with EAGLE and Medusa are included, recent methods like LookaheadDecoding or Jacobi iteration are absent. \n2 The speed is limited compared to Eagle-3.\n3 The OT theory focuses on single-step transitions, but SHAPE uses N=3 steps. A theoretical analysis of error accumulation in multi-step predictions would strengthen the claims."}, "questions": {"value": "1 How does SHAPE perform on models with >70B parameters or contexts >8K tokens? Are there memory or complexity bottlenecks?\n2 How does SHAPE handle distribution shifts between training data (ShareGPT, THUCNews) and unseen domains? Are there robustness guarantees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VpD58hYgsw", "forum": "FEttCBv3Ny", "replyto": "FEttCBv3Ny", "signatures": ["ICLR.cc/2026/Conference/Submission18703/Reviewer_7RBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18703/Reviewer_7RBM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217280542, "cdate": 1761217280542, "tmdate": 1762928404963, "mdate": 1762928404963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SHAPE (Step-ahead Hidden-state Accelerated Prediction Engine), a draft-free framework for parallel LLM decoding based on an optimal transport (OT) view of token generation. The authors conceptualize autoregressive decoding as a distributional transition in probability space, where hidden states evolve smoothly and predictably. By learning lightweight OT-guided predictors over hidden-state residuals and using a tree-based rejection sampling mechanism, SHAPE enables multi-token prediction without modifying model weights. Experiments on various models (Qwen, Vicuna, LLaMA, DeepSeek) and tasks (text, code, math) show up to 5.23× speedup with minimal quality loss (<1.2%), establishing a theoretically grounded and practical approach to efficient LLM inference."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper offers an explanation for temporal smoothness in hidden-state evolution for auto-regressive decoding, which is interesting and novel.\n- Built upon the findings, the authors design the method SHAPE that leverages the theory of optimal transport. This formulation is interesting and has theoretical proof.\n- The experiments are comprehensive using multiple base LLMs on several datasets to prove the effectiveness of the method."}, "weaknesses": {"value": "- The clarity of this paper needs to be improved. Some key experiment results are not clearly presented in this paper. For example, in Figure 1, what does the x-axis represent? On what dataset are the results obtained for each setting? Do you compute the average result for similarity?\n- The method relies heavily on observed temporal consistency of hidden states. However, this smoothness might not hold in tasks with abrupt semantic shifts (e.g., dialogue transitions, coding completions, or multimodal reasoning) or different decoding positions, which could lead to unstable or misaligned multi-step predictions. Note that this comment also greatly relates to the first one, where the authors should demonstrate the generalization of this property.\n- Please use the correct reference format. The current version negatively impacts the reading experience by using brackets."}, "questions": {"value": "- It would be helpful to present a decoding time breakdown for candidate generation and tree reject sampling. Since the similarity decreases as $n$ becomes larger, the tree rejection sampling time may also increase correspondingly. Readers would be curious about the trade-off.\n- What is the decoding configuration in this paper? For example, what is the decoding temperature used? Does SHAPE perform equally well when the temperature is adjusted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oNs08wkUsC", "forum": "FEttCBv3Ny", "replyto": "FEttCBv3Ny", "signatures": ["ICLR.cc/2026/Conference/Submission18703/Reviewer_QACb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18703/Reviewer_QACb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639032529, "cdate": 1761639032529, "tmdate": 1762928403949, "mdate": 1762928403949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes autoregressive decoding as a distributional transition in probability space and argues that strong temporal similarity of successive hidden states induces a stable (entropic) OT map between next-token distributions. Building on this, the authors propose SHAPE, a plug-in multi-token predictor: (i) a step-ahead hidden-state predictor (with residual + gating) optionally aligned via a low-dim OT layer solved by a few Sinkhorn iterations, and (ii) a tree rejection sampling scheme that accepts the longest valid prefix. Experiments on Qwen, Vicuna, LLaMA and DeepSeek across Alpaca/WikiLingua/MT-Bench and reasoning tasks report 5.23× speedups with ~1.2% accuracy deltas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work presents a method beyond draft-model speculative decoding. The paper is mostly well written.\n\nI appreciate the architecture diagrams and a concrete tree rejection sampling algorithm.\n\nExperiments cover wide choices of models and tasks."}, "weaknesses": {"value": "Section 2 first presents an empirical study that serves as the motivation of the paper. A hidden-state similarity lower bound $\\tau$ is asserted \"for the vast majority of steps\". These claims lack a rigorous support. From Figure 1, we also observe that $\\tau$ can be very different in different models and tasks. Then Section 2.2 consists of a large paragraph introducing optimal transport. The presentation is rather casual. It is recommended to write precise mathematics when talking about formulations.\n\nIt is unclear to me how the speedup is calculated. Does it include solving optimal transport with Sinkhorn method and the overhead in tree reject sampling?"}, "questions": {"value": "I think modeling the hidden-state distribution using optimal transport is a bit ad hoc. Although the semantic similarity suggests a limited change in distribution for consecutive or closely placed hidden states, why optimal transport is a necessary model remains unclear.\n\nAs claimed by the authors: \"The observed consistency suggests that transitions between consecutive token distributions are both\nsmall and structured.\" What are the structures reflected by the experiment in Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BgntFQnLP3", "forum": "FEttCBv3Ny", "replyto": "FEttCBv3Ny", "signatures": ["ICLR.cc/2026/Conference/Submission18703/Reviewer_nkXe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18703/Reviewer_nkXe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973933089, "cdate": 1761973933089, "tmdate": 1762928403281, "mdate": 1762928403281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}