{"id": "sa1a53cE6E", "number": 20381, "cdate": 1758305328122, "mdate": 1759896980953, "content": {"title": "ThermEval: A Structured Benchmark for Zero-Shot Evaluation of Vision-Language Models on Thermal Imagery", "abstract": "Vision-Language Models (VLMs) achieve strong results on RGB imagery, yet their ability to reason over thermal data remains largely unexplored. Thermal imaging is critical in domains where RGB fails, such as surveillance, rescue, and medical diagnostics, but existing benchmarks do not capture its unique properties. We introduce \\textbf{ThermEval-B}, a benchmark of 50,000 visual questionâ€“answer pairs for evaluating zero-shot performance of open-source VLMs on thermal imagery across tasks including modality identification, human counting, temperature reasoning, and temperature estimation. ThermEval-B integrates public datasets such as LLVIP and FLIR-ADAS with our new dataset \\textbf{ThermEval-D}, the first to provide per-pixel temperature annotations across diverse environments. Our evaluation reveals that while VLMs reliably distinguish raw thermal from RGB images, their performance collapses on temperature reasoning and estimation, and modality recognition becomes unreliable under false colormap renderings. Models frequently default to language priors or fixed outputs, exhibit systematic biases, or refuse to answer when uncertain. These recurring failure modes highlight thermal reasoning as an open challenge and motivate benchmarks like ThermEval-B to drive progress beyond RGB-centric evaluation.", "tldr": "We introduce ThermEval-B, a 50k-task benchmark for evaluating 14 vision-language models on thermal imagery. While models handle modality recognition, they struggle with reasoning and temperature estimation.", "keywords": ["VLMs", "Thermal Imagery", "Zero Shot Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5b9db730e1f7ea3d6602ab3707d1784843e62a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a thermal image understanding benchmark for VLMs. It includes 500 image-QA pairs where all images have pixel-level annotations and questions from templates. The authors then do zero-shot evaluation for multiple open-source VLMs. The results show that most VLMs can distinguish thermal images from RGB images but still suffer from color map understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors present a new thermal evaluation dataset with precise annotation.\n2. The paper provides an initial study on zero-shot VLM capabilities in the thermal image domain."}, "weaknesses": {"value": "1. Contribution In terms of ML/CV: While the benchmark can be important for a specific domain, is it important for the VLM field as a whole? What's the difference between lots of chart understanding / depth map understanding compared to the thermal images? The reviewer feels like it's a subdomain of the chart understanding and not as important/popular than benchmarking models' understanding on other modality inputs like optical flow or depth map from the computer vision perspective. As it's an machine learning conference, it would be great if the authors can address the problem from this perspective more.\n\n2. Experimental Design: The goal of the paper is to analyze VLM's capability on thermal images and provide valuable insights for practitioners and researchers to move forward; however, the reviewer believes that the analyses are just surface-level due to the insufficient depth of experimental design. For all tasks, the authors assume that VLMs already understand the definition of thermal images and thus did not provide clear definition on it but the reviewer suggests that putting the definition of thermal images can be a great ablation studies. For instance, in task 1, the authors found that BLIP-2 and PaliGemma-2 perform bad when fed with a simple prompt \"Is this a thermal image or an RGB image\" (in L177, Table 1). However, it's unclear if it's because these models just did not have the capability to understand this kind of modality or it's because the instruction is not clear enough. What if the prompts be \"The thermal images is captured.... where the resulting images are usually colored in xxx color map; the RGB images on the other hand... Given this definition, is the following image a thermal image or an RGB image?\" From a user perspective it does not make sense for us not to do some simple prompt tweaking and come up with a conclusion directly. For this task, the author did not test any adversarial RGB examples as test cases as well. For task 3 and beyond, when observing the model performing bad on such questions, have the authors tried to provided few-shot prompting or deeper analyses on why it didn't work? Finally, from a practical perspective, do we usually need to understand a scene from only thermal images or should be thermal plus RGB images? The authors can discuss more about the connection between these benchmark questions and the real-world application as well, especially when the questions are all easy and created from a template.\n\n3. Evaluation Protocol: The authors decided to use LLM-as-a-judge because the outputs often vary structurally (L314), resulting in 97% consistent compared to the human evaluation (L348). However, multiple libraries, such as VLLM, have support structure output for open-source VLMs and the authors can try to use that to make it better when it's a easy regression or yes/no question. For the results, it would be much better if the authors can do bootstrapping and report mean and standard deviation / confidence interval to ensure the reliability. Also, the authors did not run any proprietary models, lacking important baselines to compare."}, "questions": {"value": "Please read the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gcwV1YVJBE", "forum": "sa1a53cE6E", "replyto": "sa1a53cE6E", "signatures": ["ICLR.cc/2026/Conference/Submission20381/Reviewer_Ath4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20381/Reviewer_Ath4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766736734, "cdate": 1761766736734, "tmdate": 1762933830613, "mdate": 1762933830613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a benchmark to evaluate the general-purpose vision-language models (VLMs) on thermal images. The benchmark includes:\n- ThermEval-D: the first dataset with per-pixel temperature annotations across diverse environments.\n- ThermEval-B: A comprehensive visual question-answering benchmark.\nBesides, they conduct various evaluations for latest VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A novel idea that evaluate on thermal images. \n- The dataset with pixel-by-pixel temperature annotation (ThermEval-D) is unique in existing literature.\n- The design comprises seven levels, ranging from simple to complex.\n- A comprehensive and detailed evaluation and analysis.\n- Excellent reproducibility"}, "weaknesses": {"value": "- The authors were expected to conduct further analysis on some failure cases. \n- The evaluated models are mostly open-sourced model with small sizes. What about the closed-source product like Seed-VL and GPT-5? and large open-source models (like Qwen3VL-A22B)"}, "questions": {"value": "I agree with trying zero-shot testing on these types of images, but I think the gap between our model's vision and the human eye is huge. Heatmaps seem to be created by the human eye, and if we disregard accurate interpretation, they are naturally perceptible to humans. However, the model's vision is data-driven, and to my knowledge, our data is severely lacking in such thermal imaging samples. Therefore, it seems inevitable that future research will involve data in this area, perhaps even domain-specific VLM. I'd like to ask the authors how they view this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MTfk5rlKC0", "forum": "sa1a53cE6E", "replyto": "sa1a53cE6E", "signatures": ["ICLR.cc/2026/Conference/Submission20381/Reviewer_c8GH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20381/Reviewer_c8GH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897627950, "cdate": 1761897627950, "tmdate": 1762933830187, "mdate": 1762933830187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ThermEval, a comprehensive benchmark and dataset for evaluating VLMs on thermal imagery. It includes ThermEval-B, a 50k-sample benchmark across seven thermal reasoning tasks, and ThermEval-D, a 500-image dataset with per-pixel temperature annotations and body-part segmentation. The authors benchmark 14 open-source VLMs (e.g., LLaVA, Intern-VL, Qwen-VL) under strict zero-shot settings and reveal systematic weaknesses in thermal reasoning. Current models rely heavily on language priors, misread colorbars, and fail at temperature estimation even when they handle RGB tasks well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a under-explored but important domain for vision understanding, thermal imagery, in the context of multimodal reasoning.\n2. Well-structured benchmark with increasing task difficulty, covering both perception (modality, counting) and reasoning tasks (temperature estimation).\n3. Comprehensive experimental design: large-scale comparison of 14 VLMs under a unified zero-shot protocol.\n4. Clear failure analysis with concrete qualitative examples (e.g., fixed numeric biases, hallucination from priors).\n5. Ethical considerations and dataset release add reproducibility and impact."}, "weaknesses": {"value": "1. Evaluation is purely zero-shot; no fine-tuning or adaptation experiments on existing VLMs to show the dataset potential.\n2. The new dataset (ThermEval-D) is relatively small and limited to controlled environments, diversity and realism remain low.\n3. It heavily depends on an LLM-as-a-judge introduces parsing noise and interpretability issues.\n4. The authors didn't include proprietary or stronger close-sourced models like GPT-5/Gemini2.5, which limits the completeness."}, "questions": {"value": "This paper shows a meaningful limitation of current VLMs and contributes a benchmark that will likely inspire follow-up work in thermal-aware multimodal learning. \n\nDespite limited dataset scope and missing finetuning studies, I tend to accept this paper given its contribution."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper involves direct data collection from human participants for the ThermEval-D dataset, including per-pixel thermal imagery of identifiable body parts (forehead, chest, nose). The authors state that an Institutional Ethics Committee approved the study and that all data were anonymized.\n\nGiven these factors, an ethics review may be necessary that focused on human data collection, privacy safeguards, and responsible dataset release."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6lyLAmur4Z", "forum": "sa1a53cE6E", "replyto": "sa1a53cE6E", "signatures": ["ICLR.cc/2026/Conference/Submission20381/Reviewer_f9Nx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20381/Reviewer_f9Nx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953901446, "cdate": 1761953901446, "tmdate": 1762933829864, "mdate": 1762933829864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}