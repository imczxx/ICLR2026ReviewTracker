{"id": "PRu8Sybp1j", "number": 7404, "cdate": 1758020155772, "mdate": 1763672674355, "content": {"title": "An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes", "abstract": "Predicting individualized potential outcomes in sequential decision-making is central\n for optimizing therapeutic decisions in personalized medicine (e.g., which\ndosing sequence to give to a cancer patient). However, predicting potential out-\ncomes over long horizons is notoriously difficult. Existing methods that break the\ncurse of the horizon typically lack strong theoretical guarantees such as orthogonality\n and quasi-oracle efficiency. In this paper, we revisit the problem of predicting\n individualized potential outcomes in sequential decision-making (i.e., estimating\n Q-functions in Markov decision processes with observational data) through a\ncausal inference lens. In particular, we develop a comprehensive theoretical foundation\n for meta-learners in this setting with a focus on beneficial theoretical properties.\n As a result, we yield a novel meta-learner called DRQ-learner and establish\nthat it is: (1) doubly robust (i.e., valid inference under model misspecification),\n(2) Neyman-orthogonal (i.e., insensitive to first-order estimation errors in the nuisance\n functions), and (3) achieves quasi-oracle efficiency (i.e., behaves asymptotically\n as if the ground-truth nuisance functions were known). Our DRQ-learner is\napplicable to settings with both discrete and continuous state spaces. Further, our\nDRQ-learner is flexible and can be used together with arbitrary machine learning\n models (e.g., neural networks). We validate our theoretical results through\nnumerical experiments, thereby showing that our meta-learner outperforms state-of-the-art baselines.", "tldr": "", "keywords": ["Causal Machine Learning", "Doubly Robust Estimation", "Neyman-Orthogonality", "Markov Decision Process"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8170af1d475ebe23876e31216b929a3ff4b1a76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies off-policy estimation of the full Q-function  from trajectories generated by a different behavior policy. The authors propose  DRQ-learner, a two-stage learner: Stage-1 fits “nuisance” components; Stage-2 minimizes an EIF-based loss to debias the final Q function. Theoretically, they claim Neyman-orthogonality, double robustness, and quasi-oracle efficiency. Empirically, on Taxi-like settings they vary data size, horizon, and overlap and report that DRQ outperforms plug-in baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The EIF-based orthogonal loss directly targets $Q^{\\pi_e}$, and cancels 1st-order nuisance error, well-motivated.\n2. The paper is well-organized and clearly written."}, "weaknesses": {"value": "1. Most results are on Taxi-like setups; no continuous-control tasks.\n2. The paper varies $\\epsilon$ in $\\epsilon$-greedy policies but never provides a simple numerical overlap indicator, so readers cannot tell how low the overlap actually is.\n3. Several results appear to use oracle or near-oracle nuisance components, so the practical robustness of Stage-2 under ordinary estimation errors is not fully demonstrated."}, "questions": {"value": "1. Theorem 4 uses $\\Delta k$ without defining $k$; seems like a leftover typo and should be fixed.\n\n2. What exactly is the measure $p_b \\pi_e$ in the theorem 4? Why is this weighting natural for your analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qKkNh05oQz", "forum": "PRu8Sybp1j", "replyto": "PRu8Sybp1j", "signatures": ["ICLR.cc/2026/Conference/Submission7404/Reviewer_Pa5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7404/Reviewer_Pa5y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761345091158, "cdate": 1761345091158, "tmdate": 1762919526790, "mdate": 1762919526790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of estimating individualized potential outcomes in sequential decision-making, specifically the off-policy estimation of Q-functions in Markov decision processes (MDPs) using observational data. The authors introduce a novel meta-learner called the DRQ-learner, which is based on principles of causal inference and semiparametric efficiency theory. The key contribution of this work is a theoretically grounded framework that produces an estimator with three important properties: double robustness (the ability to provide valid inferences even when the behavior policy or the density ratio is mispecified), Neyman orthogonality (resilience to first-order errors in estimating nuisance functions), and quasi-oracle efficiency (asymptotic performance that matches that of an oracle with access to the true nuisance functions). In contrast to previous methods that either experience plug-in bias or struggle with the \"curse of horizon,\" the DRQ-learner utilizes a debiased loss derived from the efficient influence function. This allows for stable and accurate long-horizon estimation, even in scenarios with low policy overlap. Additionally, the method is flexible, capable of handling both discrete and continuous state spaces, and supports various machine learning models, such as neural networks, for estimating nuisance functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is strong due to its solid theoretical foundation and its clear synthesis of ideas from causal inference, orthogonal statistical learning, and off-policy reinforcement learning. The authors effectively reframe Q-function estimation as a causal estimand and identify the limitations of existing plug-in approaches, such as Q-regression and FQE, by examining plug-in bias. They also provide formal proofs of the key properties of their method. Empirical results from the Taxi environment support the theoretical claims, demonstrating consistent improvements over baseline methods, especially in scenarios with low overlap and long horizons. The findings confirm the advantages of Neyman orthogonality and the approach's applicability to both unrestricted and restricted model classes."}, "weaknesses": {"value": "- While the theoretical guarantees are strong, the practical implementation relies on accurate estimation of complex nuisances (e.g., the stationary density ratio), which may be challenging in high-dimensional or continuous settings despite the orthogonality property. \n\n- The experiments are limited to a single, relatively simple environment; broader validation across diverse MDPs (e.g., with partial observability or real-world medical data) would strengthen the empirical claims."}, "questions": {"value": "- What are the standard assumptions in Theorem 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zhdVsmAgxH", "forum": "PRu8Sybp1j", "replyto": "PRu8Sybp1j", "signatures": ["ICLR.cc/2026/Conference/Submission7404/Reviewer_V1QJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7404/Reviewer_V1QJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794979541, "cdate": 1761794979541, "tmdate": 1762919526405, "mdate": 1762919526405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of estimating individualized potential outcomes in sequential decision-making (i.e., estimating Q-functions in Markov decision processes with observational data) through a causal inference lens. They propose a novel meta-learner, termed the DRQ-learner, which enjoys beneficial theoretical properties: double robustness, Neyman orthogonality, and quasi-oracle efficiency. Empirically, the paper demonstrates that the DRQ-learner consistently outperforms baseline Q-regression and Q-learning methods in terms of the rMSE metric across a different settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Novelty intersection causal inference & orthogonal learning and MDPs.** The paper creatively bridges causal‐inference, orthogonal learning with off‐policy Q‐function estimation in MDPs. By leveraging orthogonal meta‐learner ideas, it provides a fresh perspective on learning in Markov decision processes.\n\n**Theoretical guarantees.** The authors present comprehensive theoretical analysis, including identification from multiple perspectives, derivation of efficient influence functions, and construction of an orthogonal loss. The resulting estimator is shown to possess several theoretical properties."}, "weaknesses": {"value": "**1. Motivation for the MDP framework.**\nWhile the proposed orthogonal learner is formulated within an MDP setting, the motivation for adopting this framework and the necessity of the Markov structure remain unclear. It would be helpful to clarify what advantages or practical challenges the MDP formulation addresses compared with alternative settings, such as DTR or contextual bandit settings.\n\n**2. Interpretation of theorems.** \nThe paper includes several lengthy and dense formulas. Some theoretical results would benefit from additional intuition and clearer interpretation to help readers understand the key ideas and implications behind the derivations.\n\n**3. Experiments.**\nThe experiments demonstrate that the proposed DRQ estimator outperforms strong baselines. However, several theoretical results, such as double robustness under misspecification of $\\pi_b$ or $w_{e/b}$, are not empirically verified.\n\nSee also questions below."}, "questions": {"value": "1. The authors mention the “curse of horizon” multiple times. Could you provide an explanation of this phenomenon and briefly clarify why it arises in other estimators? Additionally, do other baseline methods in the experiments also suffer from this issue?\n\n2. In Theorem 3, could the authors provide more intuition behind $\\phi_1$ and $\\phi_2$? Moreover, the connections among Eq (13)–(15) and Eq (16) are not clearly explained. The proof here is not clear and requires more interpretation.\n\n3. In Figure B3, DRQ does not perform best when $\\epsilon$ is large. How to explain it?\n\n4. There appear to be a few typos: a misplaced comma in Equation (15); inconsistent indices used in Eq (1): first $T_i$ and subsequent $nT$; and a typographical error (“si” → “is”) in Appendix C.4, line 1283. Besides, Several equations in the Appendix are too long to be fully displayed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e37LAzw4gt", "forum": "PRu8Sybp1j", "replyto": "PRu8Sybp1j", "signatures": ["ICLR.cc/2026/Conference/Submission7404/Reviewer_MFXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7404/Reviewer_MFXq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807740040, "cdate": 1761807740040, "tmdate": 1762919525708, "mdate": 1762919525708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors used a two-stage approach based on the nature of double machine learning to construct a semi-parametrically efficient estimator for the Q function under the MDP setting, which is helpful for individualized and robust decision making. The authors claim novelty by extending existing work that mainly focuses on value function estimation or is not doubly robust, to the case of doubly robust Q function estimation. The corresponding theory is provided in detail.\n\nOverall, this is a well-structured paper, but I find it lacks theoretical novelty (or at least does not emphasize it clearly) compared with existing papers, given that DML and doubly robust OPE have been extensively studied in the last 5-10 years. I think it is a good practice of applying these theories to Q function estimation, but it may not be solid enough for publication at ICLR. I would vote for a weak reject, and I believe this paper would be a good fit for theory-extension journals or conferences, although I’m open for discussion and won’t hold a very strong opinion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured and carefully written. The flow is clear, and the theoretical results (with a not very detailed look) generally look fine.\n\n2. For estimation construction, the authors propose a two-stage estimation that can get rid of modeling class restrictions as in Shi et al. (2021). I think this part is novel (to my knowledge), and the authors justify the performance and doubly robust property well through simulation studies."}, "weaknesses": {"value": "1. In general, the contents before Section 4.2 are well established in the related literature. In a standard MDP setting, it is not surprising to see these results. When I first read it, I expected some new perspectives, but it turns out the new contributions mainly start from Section 4.2. For example:\n\n- Line 128–130: The authors claim that \"these works do not focus on the MDP setting and are well known to suffer from the curse of horizon.\" To my knowledge, some of the works listed in this paragraph (such as Shi et al., 2021) are developed under the MDP framework. Maybe the authors meant that 'some' papers in this paragraph are not based on the MDP setting, but the sentence is somewhat misleading and should be rephrased.\n\n- Line 251–256: \"This causal lens, to our knowledge, has not been made explicit in estimating Q-functions.\" I think this causal perspective is already well established in related work, including papers the authors cited such as Uehara (2022), and has been used in many extensions studying settings with causal assumption violations (e.g., confounders, interference, etc). Thus, I don’t think the identification results are new or particularly contributive for this paper under a standard MDP setting.\n\n2. Based on point 1, I think the authors should highlight more clearly the theoretical challenges in establishing semi-parametric efficiency for the Q function compared with existing work (which already has such results for the value function). I don’t think there is a strong theoretical difference, but if there is, it should be emphasized to strengthen the contribution.\n\n3. Regarding estimation, although I appreciate the contribution of proposing a two-stage estimation process that can handle nonparametric function classes for DR-Q learner, I would like to hear more about the challenges when building upon existing work and how the authors address them.\n\n4. For the simulation part, since the authors highlight many comparisons with Shi et al. (2021), it would be great to also show comparisons with their paper on value function estimation. Because the proposed DR-Q learner is more flexible, showing this comparison would further strengthen the contribution."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AvkdAUtJfB", "forum": "PRu8Sybp1j", "replyto": "PRu8Sybp1j", "signatures": ["ICLR.cc/2026/Conference/Submission7404/Reviewer_9GGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7404/Reviewer_9GGk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812478718, "cdate": 1761812478718, "tmdate": 1762919524923, "mdate": 1762919524923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all Reviewers"}, "comment": {"value": "Thank you very much for the evaluation of our paper and your helpful feedback! We addressed all of them in the comments below. We also uploaded a revised version of our paper (see **rebuttal PDF**), where we **highlight key changes in blue color**.\n\nFollowing the feedback of the reviewers, our **main improvements** are the following:\n\n - **New experiments:** We **added a comprehensive set of new experiments** on the *Frozen Lake* environment to provide a broader empirical validation of our theoretical claims. In the experiments, we compare the theoretical properties of our method by varying (1) the dataset size, (2) the horizon length, and (3) the overlap level. The results confirm that our method achieves performance on par with state-of-the-art baselines and remains robust under challenging conditions, thereby **empirically supporting our theoretical guarantees** (see **new Figure 6** and expanded **Section 6)**.\n\n - **Clarification of theoretical novelty:** We now spell out explicitly why efficient influence functions (EIFs) from existing works are **not** applicable. Importantly, prior work (e.g., Kallus & Uehara, 2022) focuses on policy value estimation, which is a *scalar estimand*, while our work focuses on the Q-function, which is a functional estimand. Hence, this requires new orthogonal statistical learning theory.\n\n - **Added intuition for theoretical results:** We expanded the discussion around the main theorems to provide additional intuition and clearer explanations of the proof ideas (see revised **Section 5**).\n\n - **Improved organization and focus:** We **shortened and streamlined** the materials before Section 4.2 to focus the exposition on our **main theoretical and methodological contributions** in Section 5.\n\n - **Additional explanatory material:** We introduced a new appendix, which elaborates on background topics such as the *curse of horizon* and provides clarifying examples (see **new Appendix B**).\n\n - **Minor fixes.** We did many many fixes. We fixed typos, improved phrasing, and clarified ambiguous statements throughout.\n\nGiven these improvements, we are confident that our paper will be a valuable contribution to the causal machine learning literature and a good fit for ICLR 2026."}}, "id": "TFGtuF27J3", "forum": "PRu8Sybp1j", "replyto": "PRu8Sybp1j", "signatures": ["ICLR.cc/2026/Conference/Submission7404/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7404/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission7404/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763675467560, "cdate": 1763675467560, "tmdate": 1763675812248, "mdate": 1763675812248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}