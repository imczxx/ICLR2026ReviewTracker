{"id": "1aAFc4OANr", "number": 15809, "cdate": 1758255563273, "mdate": 1759897280672, "content": {"title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights", "abstract": "Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with selective state space models such as Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of these hybrid architectures via both inter-layer (sequential) and intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of each computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.", "tldr": "", "keywords": ["Hybrid Architecture", "Transformer", "Mamba", "Language Modeling", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24358bf22ad2a678dc49120330c3e7275ff85ad3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a hybrid architecture combining Transformer and Mamba modules to enhance inference speed and computational efficiency. The idea is novel and relevant, showing potential in bridging attention-based and state-space models. However, the paper lacks clarity on when and how each module is applied, as well as theoretical justification for their integration. The model design and experimental details are insufficiently explained, and the absence of thorough ablation and generalization studies limits the strength of the empirical claims. Overall, the contribution is promising but not yet fully convincing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel combination of Transformer and Mamba modules, aiming to exploit their complementary strengths in sequence modeling and efficiency.\n\nThe work addresses a relevant challenge in modern deep learning — improving inference speed and memory efficiency without significantly compromising accuracy.\n\nPreliminary results demonstrate that the proposed hybrid architecture can outperform standard Transformer baselines on certain tasks, suggesting its promise for long-sequence or resource-constrained applications."}, "weaknesses": {"value": "The paper does not clearly explain the theoretical motivation for combining Transformer and Mamba modules, nor provide criteria for when each should be activated.\n\nKey implementation aspects such as module interaction, feature fusion, and parameter sharing are not well described, which limits reproducibility.\n\nThe evaluation is narrow, lacking ablation studies, robustness analysis, and tests on diverse datasets to support the claimed generalization and efficiency improvements."}, "questions": {"value": "1. How does the model determine when to use the Transformer module versus the Mamba module? Is there a data-driven or theoretical basis for this choice?\n﻿\n2. Are the integration strategies between modules such as information flow, gating mechanisms, or parameter sharing fixed or adaptive during training?\n﻿\n3. Could the authors provide ablation studies to quantify the contribution of each module and demonstrate the stability of the hybrid architecture across tasks?\n﻿\n4. How does the proposed hybrid approach generalize to different sequence lengths and domains beyond the tested benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Km0S4o4Znd", "forum": "1aAFc4OANr", "replyto": "1aAFc4OANr", "signatures": ["ICLR.cc/2026/Conference/Submission15809/Reviewer_Lhf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15809/Reviewer_Lhf2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761134135482, "cdate": 1761134135482, "tmdate": 1762926039983, "mdate": 1762926039983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies hybrid language model architectures that mix Transformer attention with state-space models (SSMs), especially Mamba. It compares two ways to combine them: inter-layer (alternating Transformer and Mamba blocks) and intra-layer (splitting attention heads within a layer between Transformer and Mamba, then fusing the outputs). The paper compares quality, efficiency, and scaling, and gives design guidance on block ratios and intra-layer choices."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ Clear comparisons and practical guidance: The paper compares inter-layer vs intra-layer hybrids alongside Transformer, Mamba, and sliding-window attention, providing practical design insights.\n\n+ Thorough intra-layer ablations: The paper studies normalization, fusion choices, gating, output projection, and dimension splits for intra-layer hybrid design.\n\n+ Efficiency analysis: The paper shows that lower FLOPs lead to lower step time, higher throughput, and a smaller KV cache than a Transformer at the same scale."}, "weaknesses": {"value": "1. Novelty is limited: Similar hybrid ideas appear in prior work, and they provide a similar recipe and conclusion. Please clarify what is new here beyond existing hybrids and head-wise designs (e.g., what part of the architecture, training recipe, analysis, or design rules is truly novel).\n\n#### references\n- Griffin: Mixing Gated Linear Recurrences with Local Attention\n- MambaFormer: Hybrid Architecture for In-Context Learning\n- An Empirical Study of Mamba-based Language Models\n- Jamba: Hybrid transformer-mamba language models\n- Jamba-1.5: Hybrid transformer-mamba models at scale\n- Hymba: A hybrid-head architecture for small language models\n\n2. Scale: Results focus on 1B-parameter models, 8K training context, and fixed budgets (~60–73B tokens). Several prior hybrid papers (see above references) studied at larger scales (>1B). It would be helpful to demonstrate whether the design rules (e.g., block ratios, placement) still hold for larger models (e.g., 3B–7B) and for longer contexts (e.g., 32K+). Some scaling-law figure would strengthen claims.\n\n3. Hybrid variants not covered: The paper mainly studies head-wise intra-layer fusion, which is already explored in prior work. More experiments with other linear modules (e.g., RWKV, linear attention) or other fusion styles (e.g., sequence-wise fusion) would support the claim that the proposed “best recipe” generalizes across hybrid families.\n4. Scope of tasks: The main quality metrics are NLL and a small few-shot set. Downstream long-context tasks (e.g., QA, retrieval, summarization) are limited. Adding these would better validate the method."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kEFN2AFFPc", "forum": "1aAFc4OANr", "replyto": "1aAFc4OANr", "signatures": ["ICLR.cc/2026/Conference/Submission15809/Reviewer_xUqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15809/Reviewer_xUqE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764712389, "cdate": 1761764712389, "tmdate": 1762926039327, "mdate": 1762926039327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ChatGPT said:\n\nThe paper explores hybrid large language model architectures that integrate self-attention layer with state-space modules to optimize the trade-offs between model accuracy, computational efficiency, and long-context performance. Two main hybridization strategies are examined: inter-layer hybridization, which alternates self-attn blocks and Mamba blocks sequentially, and intra-layer hybridization, which fuses both mechanisms within a single layer through head-wise or sequence-wise splitting. Experiments conducted under controlled compute and data budgets (1B parameters, 8K context length, 4.5e20 FLOPs) compare these hybrids against baselines like Llama, SWA, and Mamba across language modeling quality, efficiency, long-context retrieval, and scaling behavior (including Mixture-of-Experts integration). Results show that both inter- and intra-layer hybrids outperform pure Transformer and Mamba models in perplexity and accuracy, with intra-layer hybrids achieving the best quality-efficiency balance. The hybrid models preserve Mamba’s linear scalability and cache efficiency while retaining the Transformer’s representational power, leading to strong long-context extrapolation and superior performance on benchmarks. Optimal configurations feature approximately a 1:5 Transformer-to-Mamba block ratio and Transformer placement in middle layers, while the addition of Mixture-of-Experts modules further enhances performance and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Thorough and systematic investigation: Rather than merely introducing a new hybrid model, the paper carefully examines the design space—including block ratios, layer placement, and fusion strategies.\n2. Meaningful insights for practitioners and following works: The design recommendations (e.g., middle Transformer placement, 1:5 ratio) provide actionable guidelines for future hybrid model development.\n3. The study shows that hybrid architectures integrate smoothly with Mixture-of-Experts (MoE) layers and retain strong generalization performance—a notable result given the growing adoption of MoE designs in modern large language models."}, "weaknesses": {"value": "Overall, this is a strong and well-executed paper, and the notes below are not weaknesses but rather some of my concerns/questions for further exploration. \n\n1. Limitations of Equal-FLOP Comparisons: While equal-FLOP evaluations provide a fair computational baseline, they may overlook important nuances in training behavior—such as differences in convergence speed, or stability across architectures. Future work could benefit from analyzing learning curves and optimization dynamics to better capture these distinctions.\n\n2. Narrow Task Scope and Generalization: The study focuses primarily on language modeling (ppl) and retrieval task, without assessing reasoning, code generation, or multimodal tasks. This leaves some uncertainty about the conclusion of this paper to broader domains and complex cognitive tasks. Evaluating such capabilities would strengthen the paper’s claims about overall versatility and robustness."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f8d4D7AZLb", "forum": "1aAFc4OANr", "replyto": "1aAFc4OANr", "signatures": ["ICLR.cc/2026/Conference/Submission15809/Reviewer_wjS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15809/Reviewer_wjS1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869488716, "cdate": 1761869488716, "tmdate": 1762926038785, "mdate": 1762926038785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates hybrid language models through systematic comparisons of different hybridization strategies and analyses of the key factors underlying their effectiveness. It compares inter- and intra-layer hybridization (and their variants) with pure models, benchmarking them on commonsense reasoning and retrieval tasks. The study finds that intra-layer hybridization is the most promising design choice and provides design insights through ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research question explored in this work, i.e., investigating optimal hybridization strategies, is important, especially given the growing interest in hybrid model architectures.\n\n2. The study finds that intra-layer hybridization is the most promising design choice, and the ablation study provides valuable reference for the community.\n\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The major concern of this work lies in the generality of its conclusion regarding the ranking between intra- and inter-layer hybrid models, i.e., whether the observed differences are due to the stacking strategy itself or influenced by other design factors.\n\nMore specifically, the following evidence suggests that the latter may have an even greater impact:\n\n(1) The intra-layer results in Table 5 also show some ambiguity. For instance, while the authors mentioned that normalization is critical, Hymba w/o group normalization achieves +1.5% higher accuracy than Hymba. In addition, other design choices also show mixed results. For example, it is unclear which fusion strategy, add or diff, is better, as they exhibit different rankings when out=1 and 2. It raises the question of whether the higher accuracy reflects genuinely better design choices or merely fluctuations. If it is the latter, it becomes difficult to claim that intra-layer designs will consistently outperform inter-layer designs across different model and data scales.\n\n(2) Furthermore, from the intra-layer results in Table 5, before tuning the design choices, the vanilla intra-layer model achieves around 53.9% accuracy, while the best accuracy for inter-layer models is 54.0% in Table 4. It is not entirely convincing why the authors chose to report 53.3% for inter-layer models in Table 2. This suggests that intra-layer models require additional architectural tuning to surpass inter-layer models, and the latter could also benefit from tuning, e.g., by adding extra normalization layers.\n\n2. Building upon the previous concern, another major issue is that the authors only report accuracy values with relatively small differences, without providing insight into why each design choice works. Without such analysis, it is difficult to determine which design choices should be preferred for different model or data scales, especially given the mixed results of strategies such as fusion and normalization.\n\n3. The authors mention that the final model choices in Table 2 are also based on efficiency. However, a missing component is the accuracy–efficiency frontier across model variants, which would help reveal which variants offer the best scaling.\n\n4. Almost all of the explored design choices have been discussed in prior works, and some conclusions have already been reported (e.g., Hymba also found that intra-layer stacking is better than inter-layer stacking). Without analyzing the underlying mechanisms behind each design choice, the technical novelty of this work remains limited.\n\n5. Only Mamba is considered as the hybrid operator. Given the emergence of linear attention mechanisms with stronger recall capabilities, it remains unclear whether the conclusions drawn in this work can be generalized to a broader range of hybrid models."}, "questions": {"value": "I have listed my questions in the weakness section. I'm willing to adjust my scores if my concerns are properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CMXL4VaPFm", "forum": "1aAFc4OANr", "replyto": "1aAFc4OANr", "signatures": ["ICLR.cc/2026/Conference/Submission15809/Reviewer_bLdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15809/Reviewer_bLdq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943286012, "cdate": 1761943286012, "tmdate": 1762926038390, "mdate": 1762926038390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares inter-layer and intra-layer Transformer-Mamba hybrid models across language modeling perplexity, few-shot tasks, long-context retrieval and efficiency, under equivalent number of FLOPs. In the 1B-parameter scale models, hybrid architectures outperform Transformer, SWA or Mamba, and show sub-quadratic memory and throughput scaling. Notably, the hybrid architectures also enjoy the length extrapolation gains from the Mamba component. The analysis provides some design guidance: quality peaks at 1:1 and quality/efficiency Pareto is achieved at 1:5, and etc. The method is also MoE capable and exhibits scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Thorough ablation/analysis of the model design, and the performance comparison with fixed number of FLOPs. The efficiency of hybrid models, in FLOPs, memory, and step-time, is evidenced through experiments beyond theory. \n- The design insights (quality at 1:1, Pareto optimal at 1:5) and placement rules, such as not putting Transformer at the front for inter-layer hybrid. For intra layer hybrid, norm, fusion and projection choices are pinned.\n- Scaling and MoE compatibility"}, "weaknesses": {"value": "Some minor weaknesses and questions are listed here and in the next section.\n\n- Missing citation of https://arxiv.org/abs/2402.04248 - one of the first works to investigate Inter-layer Hybrid Architectures and Positional Embeddings. This work also explored placing the Mamba layer in the front (while dropping the Positional Embeddings), which could be heavily related to the takeaway of not placing Transformer blocks at the front."}, "questions": {"value": "- What could be an intuitive way to understand the intra-layer hybrid architecture? \n- Shouldn't 'Transformer' in Figure 1 be Multi-Head Attention?\n- How sensitive is the intra-layer hybrid architecture to the hyperparameter settings?\n- Is it possible to verify the scaling at upto 3~7B scale for the best performing inter- and intra- layer models?\n- What are some possible failure modes - i.e. would there be specific tasks under which the hybrid architecture's benefit might diminish, or be inferior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kbc6MU2t3D", "forum": "1aAFc4OANr", "replyto": "1aAFc4OANr", "signatures": ["ICLR.cc/2026/Conference/Submission15809/Reviewer_FVoW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15809/Reviewer_FVoW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995925433, "cdate": 1761995925433, "tmdate": 1762926037898, "mdate": 1762926037898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}