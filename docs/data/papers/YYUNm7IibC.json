{"id": "YYUNm7IibC", "number": 7915, "cdate": 1758043122231, "mdate": 1759897822680, "content": {"title": "GradShield: Alignment Preserving Finetuning", "abstract": "Large Language Models (LLMs) pose a significant risk of safety misalignment after finetuning, as models can be compromised by both explicitly and implicitly harmful data. Even some seemingly benign data can inadvertently steer a model towards unsafe behaviors. To address this, we introduce GradShield, a principled filtering method that safeguards LLMs during finetuning by identifying and removing harmful data points before they corrupt the model's alignment. It removes potentially harmful data by computing a Finetuning Implicit Harmfulness Score (FIHS) for each data point and employs an adaptive thresholding algorithm.We apply GradShield to multiple utility fine-tuning tasks combined with different levels of harmful data, and evaluate the safety and utility performance of the resulting LLMs under various metrics. Our results show that GradShield outperforms all baseline methods, as it consistently maintains a low Attack Success Rate (ASR) of under $6\\%$, while preserving the utility performance.", "tldr": "", "keywords": ["large language model", "finetuning", "alignment", "defense", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07a03d6c705e10ead39d0d702e342ec7732b501c.pdf", "supplementary_material": "/attachment/09626aca3f4b976474c26412345a35c8d661ced5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a data filtration method to address the fine-tuning risk. Compared to typical guardrail filteration, the filtration performance is significantly increased."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The definition of  FIHS score looks interesting to me. The definition is constructed on a general safety function and measure the impact of one specific data point on the dataset. The definition seems to have some connection with influence function or shapley values. \n\n2. The final derivation of practical looks intincively correct to me in the first eye, though I have some comments on it (see the weakness part)\n\n3. Experiment is sort of comprehensive though it can be improved. \n\n4. The detection threshold is automatically selected by a proposed method."}, "weaknesses": {"value": "1. The FIHS score definition is not as fundamental as the SEAL score [1] in my view. Particularly, FIHS defines a safety score function S. Some casually designed safety function (e.g., a guardrail, and also the S that in you use in your experiment) may not accurately represent the oveall safety ability of the model (also for the S that you use in experiment) and can be circumvent by the adaptive attacker. The safety alignment loss use in SEAL should be a more reliable safety score function. I understand that it is also possible to directly apply the safety alignment loss here as the safety score. In that case, could you discuss more on how the FIHS becomes and how it connects with the data weight optimized by Seal?  Also, I would also mention that recently there are three  more very relevant papers Antibody[3], BDS [4], and Ref-Teacher[5]  that also explore weighting the fine-tuning data points. Please also consider to include them into your discussion.\n\n[1] SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection (ICLR2025)\n\n[3] Antibody: Strengthening Defense Against Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Gradient Influence\nhttps://openreview.net/forum?id=qur2ef8MqQ \n\n[4] Adaptive Defense against Harmful Fine-Tuning via Bayesian Data Scheduler  (NeurIPS2025)\n\n[5] Safety-Aligned Weights Are Not Enough: Refusal-Teacher-Guided Finetuning Enhances Safety and Downstream Performance under Harmful Finetuning Attacks  https://openreview.net/forum?id=OK2GR1guwv\n\n2. **Adaptive attack is not considered.** For detection based method, I typically have serious concern over its ability to circumvent adaptive attack, especially given my prior view that the FIHS score is not very fundamental. Particularly, I have two concerns here. \n\n* **Your safety score function seems to be overly casually designed.**  In Eq. (5), the safety score function is defined as the logits difference between unsafe token \"Sure\" and safe token \"I\".  This may not accurately reflects the safety capability of the model.  For example,  the of answer of a harmful query may not start with \"Sure, ..\" but actually start with \"I definitely can help you!\", and in this case. why fine-tuning on this harmful sample with increase the logit over \"Sure\" but decrease that of \"I\"? \n\n*  **Your safety score function can be bypassed by a stronger adaptive attackers.** Let's assume that the attacker have access to the proxy dataset, and know the safety score S. Then the attacker can optimize a harmful query/answer using a similar way with Virus[6]. Specifically, the attacker can replace the F1 in Eq. (3) of Virus [6] with the objective to minimize its FIHS score. As you assume, the FIHS score is differentiable,  the attacker should be able to directly minimize the FIHS score by data optimization. I suggest the authors to conduct such adaptive attack experiments and it is okay that the defense fail in this stronger attack settings. \n\n\n[6] Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation   \n\n\n3. Insufficient baselines. The authors should compare with SEAL[1] and Ref-Teacher[5]. Both are data filtration method and were first appeared before the ICLR2026 cycle.\n\n4. Some related work on harmful fine-tuning defense should be discussed.\n\nDetecting Adversarial Fine-tuning with Auditing Agents\n\nScaling Trends for Data Poisoning in LLMs\n\nUnleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models\n\nVirus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation\n\nNo, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data\n\nYour Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents\n\nEliciting Harmful Capabilities by Fine-Tuning on Safeguarded Outputs\n\nDeep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs\n\nSelf-Destructive Language Model\n\nCTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning\n\nVulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning\n\nLoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning\n\nTowards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning\n\nSEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection\n\nSafety alignment should be made more than just a few tokens deep\n\nBeware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs\n\nShape it Up! Restoring LLM Safety during Finetuning\n\nMitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization\n\nRefusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation\n\nAsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin\n\nDefending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment\n\nA Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space\n\nDetecting Instruction Fine-tuning Attack on Language Models with Influence Function\n\nYour Task May Vary: A Systematic Understanding of Alignment and Safety Degradation when Fine-tuning LLMs\n\nLocking Down the Finetuned LLMs Safety\n\nPanacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation\n\nSafe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets\n\nNavigating the safety landscape: Measuring risks in finetuning large language models\n\nESTIMATING WORST-CASE FRONTIER RISKS OF OPEN-WEIGHT LLMS\n\nFundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models\n\nWhen Style Breaks Safety: Defending Language Models Against Superficial Style Alignment \n\n** There may be more relevant works (I just list above some more recent work), and I suggest the authors to read and discuss all of the relevant works on harmful fine-tuning when revising the paper.**"}, "questions": {"value": "1. I am also curious how the cosine similarity between the two gradient in (4) looks like on the initial aligned model and how it  evolve with the fine-tuning rounds (though you only use the initial aligned model θ_0 only to compute the FIHS score). \n\n\n\n\nI will consider to change my score if the authors can sufficiently address my concern. I overall like this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jeCzInvddC", "forum": "YYUNm7IibC", "replyto": "YYUNm7IibC", "signatures": ["ICLR.cc/2026/Conference/Submission7915/Reviewer_xgCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7915/Reviewer_xgCV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372039989, "cdate": 1761372039989, "tmdate": 1762919939068, "mdate": 1762919939068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GradShield, a filtering framework designed to protect large language models (LLMs) from safety misalignment during fine-tuning. It addresses the problem that both explicitly harmful and seemingly benign data can inadvertently compromise model safety. GradShield operates by computing a Finetuning Implicit Harmfulness Score (FIHS) for each data point and using an adaptive thresholding algorithm to identify and remove potentially harmful samples before fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Safeguarding LLMs during API-based fine-tuning is an important and timely research area. It is increasingly common for developers to fine-tune LLMs on domain-specific datasets via APIs to improve their utility on specialized tasks, making this problem practically relevant.\n\n2. The proposed FIHS framework is theoretically grounded. Although I did not verify the correctness of the proof in detail, the theoretical formulation appears sound and well-motivated.\n\n3. The paper is clearly written and well-organized, making it easy to follow the overall methodology and contributions."}, "weaknesses": {"value": "1. Time inefficiency and limited practicality.\n \nThe major limitation, in my view, is the method’s computational cost. The approach requires computing gradients for **each node**, performing repeated safety and utility evaluations, fitting the resulting scores to two Gaussian models, and possibly fine-tuning the model multiple times on different subsets. This iterative process is impractical for API-based settings, where users typically expect fast responses and cannot afford repeated fine-tuning cycles.\n\n2. Unfair comparison with baselines.\n\nThe algorithm’s iterative filtering and fine-tuning steps, based on fixed safety and utility thresholds, give it a natural advantage over baselines that lack such adaptive refinement.As a result, the comparison may not be entirely fair, since the proposed method benefits from repeated optimization until desired thresholds are achieved.\n\n3. Missing relevant baselines.\n\nIt would strengthen the evaluation to include comparisons with LLM-based filtering or guard models, such as Llama Guard or frameworks that use LLMs as safety judges. These are widely used in practice for safeguarding fine-tuned models.\n\n4. Insufficient description of adaptive threshold computation.\n\nThe adaptive thresholding mechanism is a central component of the proposed approach, yet its presentation is limited. The paper currently provides a large algorithmic block without sufficient explanation of the design intuition or computational process behind it. A more detailed description (e.g., how thresholds are initialized, updated, and stabilized) would help readers better understand the method.\n\n5.Limited justification for token selection (“I” as aligned token).\n\nThe rationale for selecting “I” as the aligned or compromised token appears entirely empirical, and its motivation is unclear. Additional analysis or intuition explaining why this token meaningfully represents alignment would make the choice more convincing.\n\n6. Minor Comments\n\nPlease use consistent table formatting across the paper to improve readability.\n\nThe introduction could be strengthened by citing related works that highlight the trade-off between benign fine-tuning and safety degradation, such as:\n\n[1] Benign Samples Matter! Fine-tuning on Outlier Benign Samples Severely Breaks Safety.\n\n[2] What is in Your Safe Data? Identifying Benign Data that Breaks Safety."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PACJrgh9Ok", "forum": "YYUNm7IibC", "replyto": "YYUNm7IibC", "signatures": ["ICLR.cc/2026/Conference/Submission7915/Reviewer_mD8z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7915/Reviewer_mD8z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605105826, "cdate": 1761605105826, "tmdate": 1762919938344, "mdate": 1762919938344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the safety misalignment that arises during finetuning and proposes a filtering method that protects LLMs by identifying and removing harmful data points before they corrupt alignment. It computes the FIHS score and employs an adaptive thresholding algorithm. The paper shows that the proposed algorithm achieves a low attack success rate of under 6% while preserving utility performance."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem that has a significant effect on society.\n\n- The paper proposes a novel approach for selecting a threshold that automatically adapts to the user dataset by employing Gaussian models and applying the Likelihood ratio test to determine a harmful change in the data."}, "weaknesses": {"value": "- The presentation of the paper is confusing due to a lack of reasoning behind the techniques used and a vague description of the methods used. For example, why the two-component GMM is used should be highlighted more, and why binary search is used should clearly be explained. \n\n- The proposed algorithm is dependent on the safety score of the held-out dataset.\n\n- Requires only one pass for each datapoint, which can be high for large-scale datasets and considering the LLMs require 3-4 epochs to be finetuned.\n\n- The use of only one probing safety data point is inadequate and limited since it depends on which data point is selected. \n\n- Choosing \"I\" as “safe” and \"Sure\" as “unsafe” isn’t grounded in theory. To me, it seems ad. hoc choice. This selection is not generalizable since a prompt can start with \"I\" but can still be unsafe, for example, “I can help you do that.”"}, "questions": {"value": "- In line 181: why is a data point considered harmful if it raises the safety score?\n\n- $x_s$ belongs to $D_s$, which is a safety benchmark dataset with harmful prompts. What is the reason behind selecting $x_s$ as the safety data point even though it is harmful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FucGMb41Uw", "forum": "YYUNm7IibC", "replyto": "YYUNm7IibC", "signatures": ["ICLR.cc/2026/Conference/Submission7915/Reviewer_8ZBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7915/Reviewer_8ZBa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061719026, "cdate": 1762061719026, "tmdate": 1762919937993, "mdate": 1762919937993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}