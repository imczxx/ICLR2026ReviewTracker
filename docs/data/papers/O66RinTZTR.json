{"id": "O66RinTZTR", "number": 18204, "cdate": 1758285112010, "mdate": 1759897119801, "content": {"title": "Feature Warping-and-Conditioning for Representation-Guided Novel View Synthesis", "abstract": "We present a novel framework for diffusion‐based novel‐view synthesis that harnesses the rich semantic and geometric representations of VGGT—a transformer model trained for multi‐view geometry prediction. Unlike existing methods that either rely on explicit 3D models (e.g., NeRF) or monocular depth estimates for guidance, our approach reformulates view synthesis as a warping‐and‐inpainting task: first, VGGT features from multiple reference views are geometrically warped into a target pose; then, a diffusion U-Net generates the final image by attending to both warped features (for accurate reconstruction of visible regions) and semantically similar cues (for plausible inpainting of occluded areas). Through an empirical analysis of DINOv2, CroCo, and VGGT features, we demonstrate that VGGT’s multiscale attention consistently delivers superior geometric correspondence and semantic coherence. Building on these insights, we design a multi‐view synthesis architecture with dedicated warping‐and‐conditioning modules that inject VGGT features into the diffusion process. Our experiments show that this design yields marked improvements in both reconstruction fidelity and inpainting quality, outperforming prior diffusion‐based novel‐view methods on standard benchmarks and enabling robust synthesis from sparse, unposed image collections.", "tldr": "", "keywords": ["NVS", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/071a3839240f2df238bd10cabd599c259bf77a9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "a diffusion-based novel view synthesis method that leverages VGGT features through feature warping-and-conditioning. it achieves superior performance on RealEstate10K and DTU, with ablations confirming VGGT's critical role in maintaining geometric and semantic consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the idea of using VGGT features for conditioning in a diffusion-based warping-and-inpainting framework is novel and well-motivated.\n- the experimental setup is rigorous, with strong baselines, multiple datasets, and both interpolation and extrapolation settings.\n- well-organized, with clear explanations and illustrative visualizations.\n- the method advances the state of the art in novel view synthesis, particularly for extrapolative scenarios where generative capabilities are crucial."}, "weaknesses": {"value": "- does not discuss computational cost or inference speed, which are important for practical deployment.\n- while VGGT features are shown to be effective, the paper does not explore whether similar results could be achieved with other multi-view consistent features or through fine-tuning existing models.\n- the comparison with other warping-and-inpainting methods is limited and could be expanded."}, "questions": {"value": "- how does ReNoV scale with the number of reference views? is there a diminishing return or a computational bottleneck?\n- could the proposed feature conditioning mechanism be applied to other generative tasks beyond novel view synthesis?\n- were any alternative multi-view feature extractors besides VGGT considered? if so, how did they compare?\n- the method relies on an off-the-shelf geometry predictor. how sensitive is the performance to the quality of the estimated pointmaps and poses?\n- have you considered evaluating on dynamic scenes or video sequences, and if so, what challenges arise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rQfMQawZ4Q", "forum": "O66RinTZTR", "replyto": "O66RinTZTR", "signatures": ["ICLR.cc/2026/Conference/Submission18204/Reviewer_7QwP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18204/Reviewer_7QwP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840580600, "cdate": 1761840580600, "tmdate": 1762927950013, "mdate": 1762927950013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReNoV, a diffusion-based framework for novel view synthesis that reformulates the task as feature warping and inpainting. By leveraging VGGT for geometry-aware and semantically rich conditioning, the model synthesizes novel views without explicit 3D reconstruction or known camera poses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and elegant formulation connecting geometry and diffusion.\n\nInnovative use of VGGT features for semantic–geometric conditioning."}, "weaknesses": {"value": "Cannot handle dynamic scenes and performance is bounded by VGGT.\n\nMissing many SoTA NVS methods for comparison, e.g. SEVA, ViewCrafter, CausNVS, CAT3D, LVSM.\n\nThe paper shows 1-3 views as input, how many input and ouput views can the model handle? How does the computation cost scale with input/output view number?\n\nMissing the training details, e.g. how many GPU (hours)? What are the training settings, e.g., batch size, model size?"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TrQgkp2Ci6", "forum": "O66RinTZTR", "replyto": "O66RinTZTR", "signatures": ["ICLR.cc/2026/Conference/Submission18204/Reviewer_S48k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18204/Reviewer_S48k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876265778, "cdate": 1761876265778, "tmdate": 1762927949591, "mdate": 1762927949591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a diffusion-based novel-view synthesis framework that utilizes VGGT's multi-view geometry features for improved image reconstruction and inpainting. It conducts extensive ablation studies to demonstrate the effectiveness of implicit semantic and geometric conditioning, showing that the proposed method outperforms baseline models in terms of structural coherence and semantic fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of VGGT features enhances the model's performance in novel-view synthesis, as evidenced by quantitative results.\n2. Ablation studies provide clear insights into the contributions of different components, validating the proposed approach."}, "weaknesses": {"value": "1.\tThe proposed ReNoV framework relies heavily on existing warping-and-inpainting paradigms (e.g., GenWarp, Seo et al., 2024) and ControlNet-style dual U-Net architectures. The core innovation—integrating VGGT features into diffusion-based view synthesis—lacks sufficient technical distinctiveness:\n2.\tThe comparison with \"non-generative models\" (PixelSplat, MVSplat) in Tables 2–3 is misleading. Non-generative methods (e.g., MVSplat) are inherently limited in inpainting occluded regions, so outperforming them does not demonstrate ReNoV’s strengths relative to generative peers.\n3.\tComputational Efficiency: No analysis of inference time or memory usage is provided. Given that VGGT (a transformer-based geometry model) and Stable Diffusion 2.1 are both computationally heavy, ReNoV’s practicality for low-resource settings (e.g., edge devices) is unclear. The paper also does not compare efficiency to lightweight methods like NopoSplat.\n4.\tThis paper discusses the sparse view novel view synthesis. Is it able to synthesize novel views based on a single view, because in real-world scenarios, we usually can collect a single view image[1][2][3]. \n\n[1] Liu, Ruoshi, et al. \"Zero-1-to-3: Zero-shot one image to 3d object.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n[2] Yang, Y., Qiu, Z., Zhang, S., & Tan, M. Disparity Guidance and Spatial-Angular Interaction for Single-View-Based Light Field Synthesis. Available at SSRN 5034806.\n[3] Jiang, Lei, Gerald Schaefer, and Qinggang Meng. \"Multi-scale feature fusion for single image novel view synthesis.\" Neurocomputing 599 (2024): 128081."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Stkfla6Z3m", "forum": "O66RinTZTR", "replyto": "O66RinTZTR", "signatures": ["ICLR.cc/2026/Conference/Submission18204/Reviewer_4WZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18204/Reviewer_4WZN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905435504, "cdate": 1761905435504, "tmdate": 1762927948952, "mdate": 1762927948952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Representation-guided Novel View synthesis, a framework for diffusion-based novel view synthesis that operates on sparse and unposed image collections. The core idea is to reformulate NVS as a warping-and-inpainting task, guided by the rich geometric and semantic features from VGGT.  It first uses VGGT to estimate camera poses and extract multi-scale features from a set of reference images. A diffusion model, architecturally similar to ControlNet, then generates the final image. It is conditioned on both the warped features and the original reference features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Defining the task of scene-level reconstruction with sparse and unposed images as a warping-and-inpainting problem is intuitive. By leveraging existing methods such as VGGT to obtain a strong prior and then using diffusion models to generate the unknown regions, the approach is both reasonable and effective.\n2. By modeling a sparse collection of images without pre-existing camera poses, the method becomes more flexible in its application scenarios, which is largely attributed to the capabilities of VGGT.\n3. Both the visual and quantitative results show significant improvements over the baseline."}, "weaknesses": {"value": "1. The method directly combines the VGGT model with a diffusion model, injecting multi-view conditions into the diffusion model using a ControlNet-like architecture. This approach is already widely adopted and does not present any particularly creative or novel design.\n2. Heavy Dependency on VGGT: The entire framework's performance is fundamentally bottlenecked by the quality of the VGGT model. Any errors in VGGT's pose estimation or feature extraction will directly propagate and negatively impact the final generated image."}, "questions": {"value": "Nil"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7V4jftvgW2", "forum": "O66RinTZTR", "replyto": "O66RinTZTR", "signatures": ["ICLR.cc/2026/Conference/Submission18204/Reviewer_QMVB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18204/Reviewer_QMVB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968489261, "cdate": 1761968489261, "tmdate": 1762927948503, "mdate": 1762927948503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}