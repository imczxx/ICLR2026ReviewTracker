{"id": "hyJV4HYCnw", "number": 1114, "cdate": 1756842611107, "mdate": 1759898227317, "content": {"title": "Controllable Video Synthesis via Variational Inference", "abstract": "Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats.\nWe develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones.\nWe cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively.\nTo address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.", "tldr": "A video synthesis method that combines multiple controls via variational inference, enabling high controllability and diversity with 3D consistency.", "keywords": ["video synthesis", "generative modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11d771442c7a37bde38aedf4dfd400bf85d66541.pdf", "supplementary_material": "/attachment/0ff86c0f812b9e9256cd8b2f5e85cb467a6fe0a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an inference-time optimization framework built on top of a variational inference formulation. The method enables controllable video generation under one or multiple conditions such as text prompts, camera trajectories, and image inputs. It defines a product-of-experts target distribution that combines multiple pre-trained backbones (Wan2.2, Depth2V, VideoX-Fun, Flow2V, and VACE), each capturing a specific control modality. The optimization is performed using Stein Variational Gradient Descent (SVGD) to minimize the KL divergence between particle distributions and a sequence of annealed intermediate targets.\n\nA key component of the approach is a 3D-aware context-conditioning mechanism, which anchors background and camera information to improve geometric consistency and mitigate local optima. This is implemented using external pretrained models such as Cam2V and GEN3C, together with an inversion process to compute context latents. Spatially adaptive masks are employed to balance the influence of different conditioning sources during optimization.\n\nAlthough the method primarily represents an engineering effort, combining several pretrained systems and heuristic stabilizers, it is supported by a coherent theoretical interpretation through variational inference. The generated results demonstrate improved controllability, scene consistency, and visual quality compared to prior methods that rely on fixed conditioning. However, the computational cost is significant (approximately 40 minutes per video segment on an H200 GPU) making the approach resource-intensive."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong controllability and flexibility: Supports multiple input modalities (text, image/video, camera, and simulation) in a unified generation process, which most existing methods cannot handle jointly.\n- Conceptually interesting bridge: Combines heavy engineering composition (multiple pre-trained models) with a sound theoretical framing via variational inference and SVGD, giving the system a principled interpretation instead of a heuristic pipeline.\n- Experimentally validated: Demonstrates clear performance gains in controllability and consistency, reaching or surpassing state-of-the-art results."}, "weaknesses": {"value": "- Extremely high inference cost: Around 40 minutes per segment on a single H200 GPU makes the method impractical beyond research demos.\n- Complex pipeline: Involves many external pre-trained components (Cam2V, GEN3C, RF-Solver, segmentation, inversion), increasing maintenance cost and dependency on each model’s quality.\n- Bounded by backbone performance: While the method builds upon multiple pre-trained backbones, its optimization framework allows combining their complementary strengths, leading to performance exceeding individual baselines. However, it still remains conceptually bounded by the representational capacity of these experts.\n- Limited originality in modeling: The novelty lies in aggregation and inference formulation, not in new architectures or learning strategies. A trainable unified model using these modalities directly would be a more scalable direction.\n- Generalization uncertainty: The method inherits data biases from the underlying models, and may fail in domains not covered by them (e.g., robotics, human manipulation). This is a minor weakness, though every model has its own data biases, it is mentioned only for completeness.\n- Missing compositional generation references: Related works should include recent object-centric and compositional methods such as Learning to Compose [1], SlotDiffusion [2], SlotAdapt [3], and SlotAdaptVideo [4] (not a necessity, but an unpublished extension of [3] to the video domain).\n[1] Jung et al., Learning to Compose: Improving Object Centric Learning by Injecting Compositionality, ICLR 2024\n[2] Wu et al., SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models, NeurIPS 2023 \n[3] Akan & Yemez, Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation, ICLR 2025\n[4] Akan & Yemez, Compositional Video Synthesis by Temporal Object-Centric Learning, arXiv"}, "questions": {"value": "- I think one of the most important comparisons would be a runtime comparison across all methods. How does the proposed method perform relative to the baselines in terms of runtime?\n- The reported 40-minute inference time per segment is substantial. Which part of the pipeline dominates the runtime, such as model evaluations, inversion for context latents, or SVGD iterations? Are there identifiable opportunities for reduction?\n- The method relies on context conditionals derived from GEN3C outputs followed by inversion. Table 4 already includes results without context conditioning, but the question here concerns sensitivity to the quality of the reconstructed context rather than its removal. How sensitive is the overall optimization quality to errors or noise in this inversion process? In other words, does imperfect context reconstruction noticeably degrade consistency or controllability? Could the inversion be performed by an alternative method for comparison?\n- Since the framework integrates multiple pre-trained backbones, it would be useful to understand how performance changes if certain experts are removed. Does the optimization remain stable with a reduced set of backbones, or is it strongly dependent on the full ensemble? This could be evaluated by removing one or more backbones during optimization.\n- Could the proposed inference-time framework be extended to train a unified model capable of handling multi-modal conditioning directly, without iterative optimization? For example, can a unified model be trained using the proposed optimization scheme to eliminate the inference-time overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "amFOghxHkf", "forum": "hyJV4HYCnw", "replyto": "hyJV4HYCnw", "signatures": ["ICLR.cc/2026/Conference/Submission1114/Reviewer_c9w8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1114/Reviewer_c9w8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760812628225, "cdate": 1760812628225, "tmdate": 1762915682758, "mdate": 1762915682758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to extend the capabilities of video generative models during inference time by steering generation outputs to faithfully follow a range of user control signals. The authors formulate the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to collectively satisfy all task constraints. The paper presents compelling qualitative results showing videos that successfully follow various control inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important and practical problem of utilizing multiple expert models for controllable video synthesis.\n- The paper is well written and easy to follow.\n- The Appendix provides strong qualitative support, including diverse and convincing video examples demonstrating the proposed method’s effectiveness."}, "weaknesses": {"value": "1. **Incremental contribution**: The paper's novelty is incremental, not fundamental. It primarily applies the existing SVGD framework to a new task, conceptually mirroring prior variational-inference-based diffusion works [1, 2]. The annealed sampling is seen as an engineering heuristic, not a core algorithmic contribution. The work is a successful application of established methods.\n\n2. **Computation resources analysis**: Please provide computational usage comparison against other baselines. The paper requires a detailed comparison of runtime, computational cost against established baselines, particularly the MCMC-based samplers.\n\n3. **Limited experimental scope on long video generation**: To credibly demonstrate the method's scalability for long video generation, the paper must include comparisons against state-of-the-art long video generation sampling methods (e.g.,[3], [4]) on standardized benchmarks. The current evaluation feels more like an interesting, extended application of the proposed method than a systematic assessment. Also, please provide the qualitative video examples of generated long videos.\n\n4. **Diversity concern**: The diversity gains attributed to SVGD are quantitatively marginal. The role of the particle count ($L$) in the experiments and evaluation is unclear. It appears the study uses $L=2$, but utilizing such a small particle count is ambiguous and concerning as it carries a high risk of mode collapse.\n\n[1] Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models, Corso et al., ICLR 2024\\\n[2] Collaborative Score Distillation for Consistent Visual Editing, Kim et al., NeurIPS 2023\\\n[3] FIFO-Diffusion: Generating Infinite Videos from Text without Training, Kim et al., NeurIPS 2024\\\n[4] DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation, Cai et al., CVPR 2025"}, "questions": {"value": "Please refer to the Weakness Section. I am willing to increase my scores if my concerns are fully addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aozbfs1SrZ", "forum": "hyJV4HYCnw", "replyto": "hyJV4HYCnw", "signatures": ["ICLR.cc/2026/Conference/Submission1114/Reviewer_dzZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1114/Reviewer_dzZe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828331666, "cdate": 1761828331666, "tmdate": 1762915682614, "mdate": 1762915682614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variational inference framework for controllable video synthesis that unifies multiple user controls (text, image, camera, and 4D asset trajectories). The method composes several pretrained video models into a single target distribution and optimizes it via annealed Stein Variational Gradient Descent (SVGD). A context-conditioned factorization improves 3D consistency and mitigates local optima. Experiments show clear gains in controllability, visual fidelity, and diversity over baselines like Wan2.2, GEN3C, and PoE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Originality\n1. Compared with prior approaches that train separate models for different control conditions, this paper introduces a novel variational-inference framework for controllable video generation, which integrates multiple pretrained models into a unified and coherent compositional generative process.\n\n### Technical Quality\n1. The algorithmic design is rigorous: equations (1)–(6) are mathematically consistent and clearly linked to the pseudocode in Algorithm 1. Moreover, the paper provides a concrete framework that implements these theoretical derivations in practice, achieving a unified and controllable video generation model under the variational inference paradigm.\n2. The method is implemented carefully with detailed ablations, quantitative evaluations (LPIPS, 4 metrics from WorldScore, ViCLIP, GPT-4o scores), and qualitative comparisons.\n\n### Clarity\n1. The exposition is clear and well-structured.\n2. Implementation details and reproducibility statements are well documented in the appendix.\n\n### Significance\n1. The work addresses an important and timely challenge: combining multiple control modalities for coherent and physically consistent video generation.\n2. The proposed formulation is general, training-free and can be applied on top of any modern diffusion or flow-based video model."}, "weaknesses": {"value": "1. Computational and Memory Cost, and Scalability.\nWhile the paper claims that the proposed method enables efficient sampling, this efficiency should be supported by fair comparisons and quantitative evidence. The discussion of cost should not be limited to computational complexity — memory consumption also deserves attention. The paper should report more detailed runtime or memory statistics beyond the brief mention in Appendix F.\n\n2. Lack of Clear Motivation for Variational Inference.\nThe paper does not sufficiently explain why variational inference is preferable to the first category of methods mentioned in the Introduction — i.e., training a controllable video model directly on curated, multi-condition datasets. The authors should make explicit the advantages of adopting the variational-inference approach (e.g., reusability of pretrained models, reduced data curation cost) and clarify why fine-tuning a unified controllable model is not an adequate alternative (e.g. Full-DiT[1]).\n\n[1] Ju X, Ye W, Liu Q, et al. Fulldit: Multi-task video generative foundation model with full attention[J]. arXiv preprint arXiv:2503.19907, 2025."}, "questions": {"value": "My biggest concern is efficiency, and whether it can support industrial applications (because video generation is a matter of concern in the industry, and there are already some industrial applications).\n\n1. If the method indeed incurs substantial computational and memory costs due to maintaining multiple particles and combining several large backbone models, would this make it difficult to deploy or scale in industrial applications?\n\n2. In terms of computational efficiency, could the framework be accelerated using state-of-the-art distillation techniques as adopted in existing methods? For memory efficiency, instead of combining multiple distinct base models, is it possible to use a single backbone model with multiple condition-specific LoRA adapters, achieving similar compositional control with lower memory overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dYHk3DwnlW", "forum": "hyJV4HYCnw", "replyto": "hyJV4HYCnw", "signatures": ["ICLR.cc/2026/Conference/Submission1114/Reviewer_WdWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1114/Reviewer_WdWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988518994, "cdate": 1761988518994, "tmdate": 1762915682489, "mdate": 1762915682489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a variational-inference framework for controllable video synthesis that composes multiple pretrained video generators via a product-of-experts objective. The author propose \"3D‑aware context conditioning\"  to  generate “context conditionals” at every step to stabilize geometry and camera control. Experiments report stronger controllability and 3D consistency than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Casting multi‑constraint control as approximating a PoE target, then solving it with annealed forward‑KL and SVGD, is coherent and computationally attractive relative to MCMC‑based PoE sampling in prior work.\n2. Extensive experiments are conducted with multiple settings and ablation studies."}, "weaknesses": {"value": "## Major Concerns\n1. The main evaluation uses only 50 test samples constructed from 10 simulations, 8 camera trajectories, 13 scenes, with 2 samples per method (Sec. 4.1). While the setups are well‑controlled, this feels small for claims of generality.\n2. The proposed method aggregates multiple heavy backbones at inference. The wall-time, GPU usage should be also compared with baseline models and in ablation studies.\n3. The GAP between factorization in Eq3-5 to the score in Eq6 is huge. Can the author provide some intuitive explaination, despite there are derivations in the appendix.\n4. The adaptive foreground mask starts from image segmentation and is later replaced by a dynamic motion tracker once dynamics are sufficiently clear. What \"sufficiently clear\" is not clearly specified.\n\n## Minor Concerns\n5. There is '4D asset' in line 84, and '3D asset' in line 124. I think they all should be '4D'?\n6. The Figure 2 is not clear to me. This figure should show a more clear workflow of the pipeline.\n7. In the appendix, many \\cite should be \\citep. For example, in line 898.\n8. Reproductibility: In the implementation details part, it is better to provide the detailed hyperparameter chosen.\n9. Line 364: Cam2C should be Cam2V."}, "questions": {"value": "Can you share typical failures (e.g., fast camera roll, thin structures, specular backgrounds) and how they relate to the context conditional term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "45alGUHqQj", "forum": "hyJV4HYCnw", "replyto": "hyJV4HYCnw", "signatures": ["ICLR.cc/2026/Conference/Submission1114/Reviewer_fepN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1114/Reviewer_fepN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255991697, "cdate": 1762255991697, "tmdate": 1762915682324, "mdate": 1762915682324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}