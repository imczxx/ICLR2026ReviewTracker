{"id": "M47SWYubR5", "number": 9629, "cdate": 1758131207216, "mdate": 1763503573195, "content": {"title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "abstract": "While LLM agents can plan multi-step tasks, intervening at the planning stage—before any action is executed—is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap,  model gap, and  evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release \\texttt{Pre-Exec Bench}, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.", "tldr": "", "keywords": ["Large Language Model", "Agent", "Guardian", "Guardrail", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fb211af05acd04e11747473ed7875721e6197bb.pdf", "supplementary_material": "/attachment/9327222e504980719c9a8ba276be009cefc66f9d.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the safety of LLM agents by intervening during the \"planning stage\", before the agent executes the plans. The manuscript highlights three gaps in the current literature: the data gap, a model gap, and an evaluation gap. The paper proposes a contribution for each.\n\nTo close the data gap, the authors propose AuraGen, a synthetic data generation framework that introduces unsafe perturbations to safe plans, thereby creating a training corpus for planning-stage LLM agent safety approaches. AuraGen features four injection modes: Single-step perturbation, Multi-step corruption, New-branch diversion, and Bridged-branch diversion, ensuring a diverse set of unsafe plans can be generated.\n\nTo close the model gap, the authors propose Safiron, which features a unified adapter module that normalizes inputs and a guardian model that outputs a classification (plan: safe vs. unsafe), a risk categorization, and a textual explanation. Safiron is trained by supervised fine-tuning on AuraGen data, followed by RL (GRPO) to further optimize output behavior.\n\nLastly, to close the evaluation gap, the authors propose Pre-Exec Bench, a benchmark for evaluating the safety of agentic plans before execution.\n\nThe results show that AuraGen generates a balanced dataset featuring different kinds of risks. The proposed model, Safiron, is shown to outperform almost all existing models on classification accuracy, harmful detection precision, risk categorization, and explanation correlation.\n\n---\nNote: This paper lies outside my primary area of expertise. While I can evaluate the clarity and quality of the writing and presentation, I do not feel fully confident assessing the soundness of the methodological approach or situating it within the broader literature on agentic LLM safety. I therefore assign a low confidence score to my review."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is exceptionally well written and easy to follow. The motivation is clear, and the proposed approach appears sensible. Theere are multiple contributions — (a) a data generation framework, (b) a method achieving state-of-the-art results, and (c) a benchmark — which will likely be appreciated by the community. Beyond these core contributions, the paper is also very thorough and provides many insightful results in the main body."}, "weaknesses": {"value": "I appreciate the paper and strongly believe it should be published. I am somewhat unsure whether ICLR is the right venue, though, since the manuscript is highly applied and engineering-focused, and I see relatively little new theoretical or methodological innovation in agentic LLM safety (except perhaps the adapter/guardian decomposition). It is unclear how much of Safiron’s improvement is due to the adapter/guardian decomposition itself versus dataset or training choices; an ablation isolating the adapter’s contribution would be interesting."}, "questions": {"value": "X"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Iy36SWoCei", "forum": "M47SWYubR5", "replyto": "M47SWYubR5", "signatures": ["ICLR.cc/2026/Conference/Submission9629/Reviewer_xJJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9629/Reviewer_xJJZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562472762, "cdate": 1761562472762, "tmdate": 1762921163954, "mdate": 1762921163954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper centers on the safety of AI agents at the planning stage, prior to the execution of any actions—referred to as a proactive approach. The authors propose a data generation process named AuraGen, introduce a new model called Safiron, and develop a benchmark termed Pre-Exec Bench to address gaps in data, modeling, and evaluation. Their adapter-based Safiron pipeline consistently surpasses baseline performance on the Pre-Exec Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The contributions of the paper are clearly delineated, facilitating readers' understanding of the authors’ ideas. Additionally, the paper is logically organized.\n2. The data generation process appears reasonable, and the model training pipeline, which includes supervised fine-tuning (SFT) and reinforcement learning (RL), also seems sound.\n3. The paper clearly presents the training details of each module and provides comprehensive and straightforward experimental demonstrations."}, "weaknesses": {"value": "1. While the paper addresses data, model, and evaluation benchmarks comprehensively, the extensive experimental work may give the impression that the study lacks a specific focus or a distinct technical highlight.\n2. Additional points are discussed in the Questions section."}, "questions": {"value": "1. RM Model Learning: Why is the input from DeepSeek-R1 directly regarded as ground truth? If so, why is a separate Reward Model (RM) trained? Moreover, why isn’t the evaluation from DeepSeek-R1 directly used as a filtering criterion in the AuraGen process?\n2. It is observed that even the best-performing model within Safiron achieves relatively low accuracy in explanation correctness. Could a brief explanation be provided regarding the specific tasks in which the model underperforms most significantly?\n3. In the GRPO framework, explanation correctness does not contribute to the final reward calculation. This implies that the model could potentially optimize rewards by generating only the initial tokens. Could a brief explanation be offered as to why the GRPO training approach can further enhance model performance beyond SFT?\n4. Could you clarify the distinction between \"multi-step corruption\" and \"bridge branch diversion\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RCAFjmIFpB", "forum": "M47SWYubR5", "replyto": "M47SWYubR5", "signatures": ["ICLR.cc/2026/Conference/Submission9629/Reviewer_jKy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9629/Reviewer_jKy2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567162952, "cdate": 1761567162952, "tmdate": 1762921163732, "mdate": 1762921163732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on improving safe deployment of LLM-based agents by building tools designed for the “planning” phase of agentic execution. The authors argue that in contrast to post-execution guardrails, planning-based guardrails allow for pro-active safety evaluation. They identify a data, model, and evaluation gap as challenges to such a pro-active approach and propose solutions for each of these. To alleviate the data gap, the authors propose a multi-step synthetic data framework. For the model gap, the authors propose a guardian training method that combines an input-data adapter and a two-staged training strategy. Finally, to address the evaluation gap, the authors propose a new benchmark that uses their synthetic data engine and is enhanced by human annotators. The paper performs extensive empirical experiments on multiple models to show the efficacy of their approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[**significance**] Safe agent deployment is a timely and important research direction. Guardrails focused on the planning stage present an operationally practical opportunity.\n\n\n[**quality**]\n- Using a diverse and broad model pool is essential for robust evaluation of benchmark initiatives. The paper includes eight leading open-source models from different developers to address this (l288-289).\n- The paper does a great job at discussing and addressing the requirements for enhancing pro-active safe deployment. This reviewer especially appreciated the use of human annotators to improve the quality of benchmark annotations compared to “just” using language models. Small nit: “non-LLM [arbiters] guarantee reliability” (l297), there is no formal guarantee here. The cost discussion provided in Appendix I is also appreciated.\n- Detailed and comprehensive discussion of subsets of the experimental results.\n\n[**clarity**] Overall, the paper is well written. This reviewer does believe readability could be improved (see suggestions).\n\n\n[**originality**] While there is existing work highlighting the promise of auditing reasoning / chain-of-thought leading up to actions [1], work focusing exclusively on a “planning” stage is a useful contribution.\n\n\n[1] Korbac et al., Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, 2025"}, "weaknesses": {"value": "[**significance**] While safety in planning could be seen as a _necessary_ condition to safe deployment, it does make some strong assumptions preventing it from being a _sufficient_ condition. Specifically, the authors do not address the assumptions of (i) plan-to-action faithfulness [ 2, 3], or (ii) the possibility of dynamic adjustments based on environmental feedback. Additionally, (iii) this work assumes plans are fully specified, e.g., “the full sequence of actions to be taken” (l95), does not account for high-level actions that can be interpreted in a myriad of ways. If the goal is to model and improve safe deployment of AI agents, these should at least be discussed.\n\n[**quality**]\n- In section 5, the authors write “Real-world agentic systems [...] but distributionally realistic” (l286-288). While this holds for zero-shot generations, it does not necessarily hold for intervened trajectories, e.g., using the approach described in Section 3. Additionally, the authors fail to discuss the coverage or “realism” of the user queries used to generate the above planned trajectories.\n- In section 4, l195-196, the authors present a core component of their stated contribution: “a unified data adapter”. While this reviewer is sympathetic to the challenges of page limits, this is not an excuse to move the entire discussion of a core component to the Appendix. At the minimum, one would expect a high-level summary. For example, lots of space is used in lines 229-242 to (mostly) restate the basic GRPO algorithm.\n- Given the strong reliance and focus on synthetic data throughout this work, it is surprising to this reviewer how little discussion focuses on key insights and evaluation axes from the synthetic data literature. The related work section (in the Appendix) similarly is limited. For example, any formal discussion on diversity is missing. With one of the core stated objectives being pro-active coverage, this is damning. I would suggest the work of [4] for a detailed discussion on this topic and [5] for a practical implementation.\n\n\n[2] Yakovi and Goldber, Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? ACL 2020\n\n[3] Dibjit, Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning, EMNLP 2024\n\n[4] Havrilla et al., Surveying the effects of quality, diversity, and complexity in synthetic data from large language models, 2024\n\n[5] Davidson et al., Orchestrating synthetic data with reasoning, ICLR 2025"}, "questions": {"value": "Q1: In section 3 it is stated that AuraGen produces “diverse and controllable trajectories spanning a wide spectrum of risks” – where are these claims supported?\n\n\nQ2: in Section 4 it is stated that “RL complement it by optimizing for fine-grained safety objectives” (l206-207) – why does SFT not suffice here and why does RL solve this?\n\n\nQ3: In Section 5 it stated that “Overall, Pre-Exec Bench [...] first-class objective” (l255-256), could you explain what you mean by this?\n\n\nQ4: Section 6, Table 2 shows results for “Risk Cat. Acc.” - for the models **not** fine-tuned using Safiron variants, were the models provided with the risk category options and explanations? Similarly, for the “Expl. Corr.”, were models provided with explanation examples? I fear that the current setup overly biases results in favor of the Safiron method. Specifically, these models were trained on a distribution that is very similar to the one used at test time. It is thus reasonable to assume they’ll produce outputs that are more in line with the expected test test format(s) than untrained models. It would be useful to include a comparison of simply providing the untrained models with some in-context examples. Given the rapid development cycle of language models, in-context learning provides an operationally more favorable approach. Thus, for practical purposes, the authors would do well to compare Safiron to ICL alternatives.\n\n\nSuggestions:\n- The use of “trajectory” to refer to a plan is potentially confusing given pre-existing literature. Perhaps consider simply calling these plans instead?\n- However, readability could be further improved by providing a running example. It is also quite dense in information, especially in the results section, which makes it difficult to properly parse the presented results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zLvbYscPXH", "forum": "M47SWYubR5", "replyto": "M47SWYubR5", "signatures": ["ICLR.cc/2026/Conference/Submission9629/Reviewer_rUUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9629/Reviewer_rUUR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820754236, "cdate": 1761820754236, "tmdate": 1762921163489, "mdate": 1762921163489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets pre-execution (planning-stage) safety for LLM-based agentic systems, arguing that most existing guardrails act after an action is executed, which is too late for high-stakes scenarios. To close what the authors call the data gap, model gap, and evaluation gap, the paper proposes three components: (i) AuraGen, a controllable synthetic-data engine that first generates benign, tool-using agent trajectories from structured metadata, then injects category-labeled risks with four principled strategies (single-step, multi-step, new-branch, bridged-branch), and finally filters them with an automated reward model; (ii) Safiron, a “foundational guardrail” composed of a cross-planner adapter to normalize different agent outputs and a compact guardian model trained with SFT + GRPO to do (a) binary detection, (b) fine-grained risk typing, and (c) rationale generation; and (iii) Pre-Exec Bench, a new, human-validated benchmark specifically for planning-stage safety that covers diverse tools, trajectories, and injected risks. Experiments show that Safiron consistently outperforms strong proprietary and open-weight baselines on classification accuracy, harmful detection, and risk categorization, while the adapter enables cross-format generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on pre-execution / planning-stage safety for agents, which is still under-served compared to execution-time or dialog I/O guardrails. Framing “look at the whole trajectory before anything is executed” is both sensible and impactful for high-stakes domains.\n2. AuraGen is a clearly specified, controllable synthetic-data engine with four risk-injection strategies, a risk taxonomy, and automated RM-based filtering. This is more systematic than ad-hoc prompt-injection datasets and can be extended to new tools or environments.\n3. The adapter + Safiron split is a good modular design: the adapter unifies planner outputs in heterogeneous formats, and the guardian only has to do safety reasoning on a normalized schema. This increases transferability across agent frameworks (MetaGPT, AutoGen, etc.).\n4. Pre-Exec Bench is a nontrivial contribution—it is human-verified, covers ~1k benign and ~670 risky samples, uses a heterogeneous model pool to avoid style leakage, and directly targets the planning stage, which existing benchmarks do not."}, "weaknesses": {"value": "1. Even though the authors add two-phase human verification, the core training data and the benchmark seed are still LLM-synthesized, which raises the usual concern: will Safiron detect messy, partially malformed, or adversarially obfuscated real agent traces that do not follow the neat stepwise style AuraGen produces? A small “real logs” test set (e.g., from MetaGPT/AutoGen runs with user-created tools) would help.\n2. The paper claims the adapter “generalizes across input styles” (Fig. 6), but all styles seem to be synthetically created or LLM-generated. It is unclear how well the adapter handles idiosyncratic, schema-changing production systems (e.g., extra telemetry fields, partial tool arguments, JSON-with-errors). Adding a “noisy/partial JSON + extra fields” stress test would strengthen the claim.\n3. AuraGen’s usefulness depends heavily on the RM and the “classifier” filtering policy (Table 1). But the paper does not show ablation on RM quality (e.g., train with a weaker RM, or no RM, or RM without the five criteria) to prove that the final gains are not just from better filtering. A small ablation could make the pipeline more convincing.\n4. The paper gives a per-sample cost estimate and says Safiron is compact, but it does not provide end-to-end latency numbers for “agent -> guardrail -> environment” in a running system. A short table with average inference time on typical trajectories would be helpful.\n5. Pre-Exec Bench has 1,001 benign and 671 risky samples—good for research, but still relatively small and balanced across the 4 injection strategies (25% each) rather than reflecting real-world skew. This makes the reported numbers somewhat optimistic; adding a “realistic imbalanced” split would test over-flagging."}, "questions": {"value": "1. Can you report Safiron’s performance on non-synthetic agent traces collected from real AutoGen/MetaGPT runs (with user-written tools, occasional tool errors, and truncated steps)? Even a 200–300 sample subset would make the generalization claim much stronger.\n2. Your adapter experiments remove two styles and show good generalization (Fig. 6). How does the adapter behave on malformed or partially missing tool calls (e.g., missing arguments field, extra telemetry, JSON5, HTML-in-text)? Do you discard samples or try to repair them?\n3. Table 1 shows that the SVM filtering policy is best. Could you show an ablation where AuraGen uses (i) no RM, (ii) RM with a single criterion (e.g., causal consistency only), and (iii) RM with noisy labels, and then retrain Safiron? This would clarify how crucial the RM is.\n4. You weigh the correct risk category at 1.0 and mismatched category at 0.5 during RL. Which categories are most often confused (e.g., availability vs. goal-hijacking)? Could fine-grained label smoothing or a hierarchy of risks improve this?\n5. What is the average inference latency of the full pipeline (adapter + Safiron) on the Pre-Exec Bench input length, on a single A100/4090? Can it run inline for multi-agent systems where every plan is checked?\n6. If a trajectory contains a novel or composite risk not in your pool R, does Safiron tend to (a) label it benign, (b) over-flag it to a close category, or (c) refuse? Can you add an “unknown / abstain” option?\n7. You show that harmful:harmless ratios of 1:4–1:6 are best (Fig. 9). How sensitive is this to the synthetic corpus size—does the same ratio hold if you scale AuraGen from 20k to 200k?\n8. Pre-Exec Bench is said to be “held out” from model selection. Will you release the human instructions and debiasing protocol so others can reproduce the same verification standard?\n9. You compare to LlamaFirewall and LlamaGuard-3 in the appendix. Can you also compare to recent causal-influence prompting or runtime enforcement methods that operate over plans (e.g., AgentSpec, GuardAgent), evaluated on Pre-Exec Bench under the same metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YUG3ZZKTr6", "forum": "M47SWYubR5", "replyto": "M47SWYubR5", "signatures": ["ICLR.cc/2026/Conference/Submission9629/Reviewer_uXEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9629/Reviewer_uXEq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929326896, "cdate": 1761929326896, "tmdate": 1762921163130, "mdate": 1762921163130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}