{"id": "nAO9LcV7nE", "number": 22079, "cdate": 1758325724513, "mdate": 1759896887624, "content": {"title": "Emergent Dexterity Via Diverse Resets and Large-Scale Reinforcement Learning", "abstract": "Reinforcement learning in GPU-enabled physics simulation has been the driving force behind many of the breakthroughs in sim-to-real robot learning. However, current approaches for data generation in simulation are unwieldy and task-specific, requiring extensive human effort to engineer training curricula and rewards. Even with this engineering, these approaches still struggle to reliably solve long-horizon, dexterous manipulation tasks. To provide a seamless tool for robotic data generation in simulation, we introduce a simple framework that enables on-policy reinforcement learning to reliably solve an array of such tasks with a single reward function, set of algorithm hyper-parameters, no auto-curricula, and no human demonstrations. Our key insight is careful usage of diverse simulator resets for simplifying long-horizon exploration challenges. Our proposed system, OmniReset, automatically generates these resets with minimal human input and gracefully scales with compute to solve dexterous, contact-rich long-horizon tasks. OmniReset outperforms baselines on easier versions of our tasks, and scales to tasks with complexities beyond the reach of existing techniques. Finally, we use this data-generation methodology to create a large dataset of trajectories in simulation, and show that augmenting it with a small amount of real-world data enables successful real-world transfer for complex manipulation tasks.", "tldr": "We develop simple resetting strategies which enable off-the-shelf RL algorithms to scale to long-horizon dexterous tasks with zero task-specific engineering", "keywords": ["Robotics; sim-to-real; reinforcement learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/350b26adb10ebb8cd2c248a29e8baf960cc1a1dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on general manipulation skill learning through reinforcement learning in simulation. The authors propose a method using large-scale generation of reset states based on user-specified points of interest, eliminating the need for complex reward or curriculum design while improving training efficiency and robustness. Experiments conducted in both simulation and the real world validate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper clearly identifies a critical problem in current reinforcement learning methods for learning general manipulation skills within simulation and proposes using large-scale, valuable reset states to constrain the exploration space.\n2. The paper is clearly written, concise, and includes a comprehensive evaluation."}, "weaknesses": {"value": "1. The current tasks are not long-horizon enough. For example, real-world assembly often involves multiple-part assembly. How should the points of interest be designed in such cases? For a chair with four legs, the set of points could be very large, making sampling potentially time-consuming.\n2. The current experiments use only a robotic arm. How would the approach scale to a high-DoF dexterous hand? How can valid grasps be sampled? \n3. For manipulation tasks that involve interaction with the environment, such as using a wall to adjust an object’s pose, current method seems can not effectively restrict the exploration space.\n4. Since the points of interest may vary across objects, scaling to diverse objects might still require object-specific selection and significant user trial-and-error."}, "questions": {"value": "1. How exactly are the goal configurations defined? What happens if defining them is difficult, for example, in the case of a dexterous hand?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f1ru1LNr5k", "forum": "nAO9LcV7nE", "replyto": "nAO9LcV7nE", "signatures": ["ICLR.cc/2026/Conference/Submission22079/Reviewer_n5Rw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22079/Reviewer_n5Rw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760868599926, "cdate": 1760868599926, "tmdate": 1762942057165, "mdate": 1762942057165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to generate diverse initial states to enable more efficient reinforcement learning of long-horizon manipulation tasks. This is demonstrated on three contact-rich manipulation tasks, compared with baseline methods that utilize demonstration data. Sim-to-real transfer is achieved via distillation and cotraining with real data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed idea is simple and well executed on the proposed tasks."}, "weaknesses": {"value": "I think the paper lacks enough evaluation to turn this into a really strong paper, for example:\n\n1. The number of tasks used is quite limited. Given the rich set of tasks introduced in furniture-bench, where two of the tasks are coming from, it would be nice to evaluate the method on more tasks and report how many tasks are successful. This can give an idea of the limitations of the proposed method and what follow-up work can do to improve the results. \n\n2. Four different sources of reset distributions are proposed. However, this paper lacks ablations to verify how important those distributions are."}, "questions": {"value": "1. The details of the baseline are missing, e.g., how many demonstrations are used? \n\n2. Given the success of imitation learning using a diffusion policy with demonstration, it is not clear to me why a pure diffusion policy wouldn't work with demonstration. Can the authors provide some explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kt3678TNaX", "forum": "nAO9LcV7nE", "replyto": "nAO9LcV7nE", "signatures": ["ICLR.cc/2026/Conference/Submission22079/Reviewer_anU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22079/Reviewer_anU6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100397093, "cdate": 1761100397093, "tmdate": 1762942056846, "mdate": 1762942056846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniReset, a simple and scalable framework for training reinforcement learning policies for complex, long-horizon, and contact-rich manipulation tasks.\n\nThe framework's key insight is to cleverly bypass the difficult exploration problem in RL. It automatically generates a large-scale, diverse distribution of reset states that covers all \"reasonable\" points along the path to the goal. This allows complex, dexterous behaviors to \nemerge naturally from standard large-scale RL optimization. OmniReset is designed to operate without human demonstrations, auto-curricula, or extensive task-specific reward/hyper-parameter engineering.\n\nThe method constructs its diverse initial state distribution(beta ρ) by composing four sub-datasets: Reaching Resets (D^R), Near-Object Resets (D^NO), StableGrasp Resets (D^G), and Near-Goal Resets (D^NG). This only requires minimal high-level user specifications: a set of goal configurations (G), a workspace bounding box (W), and a bounding box for near-goal/contact-rich states (NG).\n\nThe Experimental results show that OmniReset significantly outperforms demonstration-based baselines (BC-PPO, DeepMimic, Demo Curriculum) on Easy task variants and successfully scales to the Hard variants of Screw, Drawer, and Peg tasks, where baselines struggled to make meaningful progress. Furthermore, the resulting policies exhibit superior robustness to perturbations. Finally, the paper shows that policies trained in simulation can be successfully transferred to a physical robot through vision-based distillation and co-training with a small amount of real-world data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty in Solving Exploration: The main contribution is the proposal and validation of using a diverse, minimally structured set of resets as an alternative to complex auto-curricula or reliance on expert demonstrations to solve long-horizon RL exploration challenges.\n\n2. Scalability and Performance: OmniReset successfully enables RL (PPO) to master complex, contact-rich tasks (e.g., Screw Hard) that were previously beyond the reach of existing techniques. Achieving success rates of over 97% on these benchmarks is a significant \nperformance breakthrough\n\n3. Robustness: Policies trained with OmniReset show exceptional robustness to perturbations and succeed over a much broader range of initial conditions compared to baseline methods like Demo Curriculum.\n\n4. Effective Sim-to-Real Pipeline: The work demonstrates a practical and high-value use case by distilling the learned expert policy into a visuomotor policy that achieves a 30% real-world success rate after co-training with only 100 real demonstrations, vastly outperforming a policy trained only on the real demonstrations (0% success)."}, "weaknesses": {"value": "1. Shifts Complexity from Algorithm to Task Configuration: While the paper minimizes algorithmic complexity, it shifts the burden to non-trivial task-specific configuration requirements, which are the basis of the diverse resets. This challenges the central claim of \"minimal human input”\n\nNear-Goal State Bounding Box (NG): Defining the bounding box for contact-rich states requires expert knowledge about the precise geometry and interaction points necessary to solve the task (e.g., the threads and corresponding hole for the Screw task). This step essentially encodes a critical part of the task solution as a manual configuration, introducing a significant, task-specific engineering step.\n\nPre-computed Grasps: The method relies on a pre-computed dataset of feasible grasps from a sophisticated grasp sampler. This is a powerful but unstated dependency. For novel objects or robot hands where such a tool is unavailable, generating this grasp dataset would be a substantial engineering effort in itself, acting as a form of implicit prior knowledge that is not universally available.\n\n2. Uncertain Generalizability Beyond Assembly Tasks: The effectiveness of the proposed four-part reset strategy (R,NO,G,NG) is demonstrated on three tasks that, while complex, all fall within the category of rigid object assembly and insertion. The framework's generalizability to manipulation paradigms with different structures is unproven"}, "questions": {"value": "While the authors achieve strong results on grasping and insertion, can this design generalize to other tasks, and can you quantify the labeling effort (annotation cost) required to design the task-specific annotations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QTJOvu2PvX", "forum": "nAO9LcV7nE", "replyto": "nAO9LcV7nE", "signatures": ["ICLR.cc/2026/Conference/Submission22079/Reviewer_BWSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22079/Reviewer_BWSv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762884894, "cdate": 1761762884894, "tmdate": 1762942056505, "mdate": 1762942056505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a simple method to enable RL to learn long-horizon manipulation tasks. The key idea is to initialize the scene in a diverse set of initial states relevant to the task. The authors argue that \"attempting to cover this space of behaviors may initially\nappear intractable,\" but \"the space of ‘reasonable’ manipulation behaviors is actually surprisingly small\". To this end, the authors propose resetting to reaching, near object, stable grasp, and near goal. A user aids the process by providing near goal states. The user needs to provide a set of goal configurations, a workspace for the robot, and a set of near goal states, including contact-rich and goal states.\n\nThe authors show results in 3 challenging versions of manipulation tasks - drawer, screw, and peg in simulation. Additionally, the authors show the transfer of the screw task to the real world. To achieve real robot results, a state-based policy is distilled into a vision policy. Additionally, the authors found that it's necessary to co-train this distilled policy with a small amount of real robot data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Impressive long-horizon manipulation task\n2. Empirically, significant improvement over baseline"}, "weaknesses": {"value": "1. Only one task is shown on the real robot. What was the reason for not showing real-world results on all 3 tasks?\n2. The authors claim \"OmniReset automatically generates these resets with minimal human input\". In that regard, I would expect to see results on more tasks.\n\n**Minor**\n1. Line 186: \"We will let $s ∈ S$ denote the state space\" -> \"We will let $s ∈ S$ denote the state\".\n2. \"transition dynamics of the simulator by $s′ ∼ P(·|s)$\" -> \"next state sampled from the transition dynamics of the simulator by $s′ ∼ c. P(·|s)$\"\n3. \"$a ∼ π(·|s)$ to denote control policies\" -> \"$a ∼ π(·|s)$ to denote action sampled fromm control policy\"\n4. Line 211: Suggestion $J(π) = E_{s_0∼ρ,a∼\\pi}[Σ_{t=0}^∞ γ^t r(s_t, a_t)]$ and remove \"expection is alos taken w.r.t. the actions...\"\n5. In Figure 2 caption & on line 342: space after OmniReset\n6. Line 359: Grammar \"Both setting the orientation\""}, "questions": {"value": "1. Line 372: \"standard reset distributions $ρ^S$ described above\" - what exactly is it, and which line is it defined in?\n2. What happens if we initialize around demo states with added perturbations? With enough perturbations, do you expect the state distribution to be sufficient and work similarly to the proposed method?\n3. Since the core idea is having sufficient reset coverage in relevant states, is it possible to gather them without human assistance? E.g., via disassembly and perturbations around it?\n4. How were the 100 real-world demos collected?\n5. Several related complementary works are cited in the Related Work section. What happens when Omni Reset is combined with some of them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VQEojh3dYh", "forum": "nAO9LcV7nE", "replyto": "nAO9LcV7nE", "signatures": ["ICLR.cc/2026/Conference/Submission22079/Reviewer_DGvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22079/Reviewer_DGvL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993697101, "cdate": 1761993697101, "tmdate": 1762942056134, "mdate": 1762942056134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}