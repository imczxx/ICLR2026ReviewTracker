{"id": "Rq0KarPGTB", "number": 13199, "cdate": 1758215015181, "mdate": 1759897456928, "content": {"title": "P-DROP: Poisson-Based Dropout for Graph Neural Networks", "abstract": "Stochastic processes are widely used in machine learning, yet interacting particle systems—a class of stochastic processes—have seen limited application. In this paper, we leverage an idea from classical interacting particle systems to propose a novel node selection strategy based on Poisson processes. By equipping each node with an independent Poisson clock, our method enables asynchronous and localized updates that preserve structural diversity. This approach introduces not only stochastic but also structure-aware dynamics to graph training.\n\nRecent work has introduced various drop-based techniques such as DropNode, DropEdge, and DropMessage to inject randomness and improve generalization in graph neural networks. Our Poisson-based method offers a principled alternative to these heuristics, yielding competitive or improved performance while grounding the stochasticity in a well-defined process. This work bridges probability theory and graph learning, opening a new avenue for principled stochastic design in GNNs.", "tldr": "Applying an idea from mathematical probability model \"Interacting Particle Systems\" to Graph Neural Networks.", "keywords": ["Graph Neural Networks", "Probability Theory", "Interacting Particle Systems", "Dropout", "Poisson Process"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e41f34a715e43f603387765e5edfd8a3435914b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose to activate nodes in a graph neural network using independent Poisson processes per node. It is established in the literature that allowing for asynchronous message passing can aid in mitigating effects to over-squashing or over-smoothing. The novel contribution is to use a Poisson based distribution instead of other heuristics.\n\nThe method is tested on common datasets such as Cora, CiteSeer, PubMed, and the TU Proteins dataset. No clear advantage over SOTA is demonstrated."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The general idea to model node activations using some underlying stochastic process can make sense, and is interesting.\n- The degree-aware parametrization is interesting, and investigating this further may make sense."}, "weaknesses": {"value": "- Novelty is limited: The Poisson clock is equivalent to per-node Bernoulli dropout with node-specific rates. Hence, P-DROP is close to non-uniform DropNode.\n- In all reported performance benchmarks, error estimates are missing and no advantage can be deduced (although apparently 10 runs were performed).\n- Two degree aware variants are discussed to either enhance or mitigate the influence of the graph structure on the node activation, but it does not become clear which is used, and what respective advantages/disadvantages would be. Any ablation on this is missing.\n- The algorithm may be computationally expensive, since for every node one needs to sample from the stochastic process. In addition, the resulting masks lead to a re-normalization of information propagation with slower information spread.\n- It is claimed that the approach aids in over-smoothing and over-squashing, but there is no quantitative or qualitative analysis showing this.\n- Connection to contact-processes is hinted at, but not discussed later in the manuscript.\n- Inconsistent writing: For example the acronym SGNN is not introduced and used only in two adjacent sentences.\n- Reproducibility is limited; there is no discussion of hyperparameter tuning or code availability."}, "questions": {"value": "The questions follow from the weaknesses:\n- How do degree-aware variants improve the results?\n- How similar is P-DROP to other variants of DROP.\n- Does P-DROP help with over-smoothing and over-squashing? How?\n- How big is the computational overhead of P-DROP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p1wOXxlkbs", "forum": "Rq0KarPGTB", "replyto": "Rq0KarPGTB", "signatures": ["ICLR.cc/2026/Conference/Submission13199/Reviewer_Mxim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13199/Reviewer_Mxim"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686616176, "cdate": 1760686616176, "tmdate": 1762923895741, "mdate": 1762923895741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Poisson distribution-based node selection method for GNN training. This approach probabilistically controls the propagation paths in GNNs based on local connectivity patterns. The proposed method addresses the over-smoothing problem in GNN training. Experiments confirm that in the late stages, the method achieves higher accuracy and improves training efficiency and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It fills the gap of poor effectiveness and performance of dropout methods in GNN training.\n2. The proposed node selection method fully considers the structural features of the graph, rather than relying solely on uniform sampling as in previous work.\n3. Experimental results validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The paper lacks an in-depth analysis of the proposed method and only presents the conclusion and results.\n2. Some of the experimental results in Table 1 are not optimal, and the improvements are extremely limited.\n3. The experimental section lacks certain ablation experiments, such as the impact of each node’s λ value on performance and how to determine the λ value for each node.\n4. The parameters of the baseline systems in the experiments are not mentioned, such as the dropout parameter in Table 1."}, "questions": {"value": "1. How is the λ value for each node determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dvKxZ8Zk37", "forum": "Rq0KarPGTB", "replyto": "Rq0KarPGTB", "signatures": ["ICLR.cc/2026/Conference/Submission13199/Reviewer_1tu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13199/Reviewer_1tu7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928064502, "cdate": 1761928064502, "tmdate": 1762923893426, "mdate": 1762923893426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new way to incorporate stochasticity in graph neural networks, based on Poisson processes. The nodes are updated asynchronously with messages controlled by a node-wide clock. Experiments are conducted to show the proposed method does not perform worse than existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed idea of asynchronously updating nodes is interesting."}, "weaknesses": {"value": "- The paper is haphazardly presented with countless grammatical errors, \n- There is limited technical innovation in the proposed method.\n- The mathematical reasoning is very informal.\n- The performance of the proposed method is not better.\n- Only outdated, small benchmark datasets are used."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TUKCDNhzP8", "forum": "Rq0KarPGTB", "replyto": "Rq0KarPGTB", "signatures": ["ICLR.cc/2026/Conference/Submission13199/Reviewer_yNo7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13199/Reviewer_yNo7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959767775, "cdate": 1761959767775, "tmdate": 1762923892893, "mdate": 1762923892893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces P-DROP, a structure-aware alternative to uniform drop-based regularization for GNNs. Instead of indiscriminately removing nodes or edges (as in DropNode or DropEdge), it employs an asynchronous, structure-sensitive node selection scheme driven by independent Poisson clocks. A node becomes eligible for update when its clock “rings,” introducing principled stochasticity grounded in probability theory rather than heuristic randomness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a novel, Poisson-clock–based node-selection mechanism for GNN dropout. \n- Presents a more principled alternative to uniform, heuristic drop schemes by grounding stochasticity in probability theory. \n- The asynchronous, sparse activation design is a promising direction for alleviating over-smoothing in deeper GNNs"}, "weaknesses": {"value": "The core idea is clear, but the writing and experimental evaluation need substantial improvement:\n- Underspecified math: Key parameters (λ, α) are neither clearly defined nor justified.\n- Figure 1 is there but never got mentioned.\n- Naming inconsistency: The method is referred to as both “P-DROP” and “SGNN,” causing confusion; standardize terminology.\n- Core claim untested: Over-smoothing mitigation is asserted but not evaluated on deep GNNs; include 8–32-layer studies.\n- Narrow benchmarks & weak reporting: Results are limited to a single table on Cora/CiteSeer/PubMed. Protein-graph results are brief, not tabulated, and use only GCN/GAT (no GIN).\n- No ablations: Lacks tests isolating degree sensitivity (α) from asynchronous timing; add controlled ablations and sensitivity plots."}, "questions": {"value": "Please see the weaknesses. I am also confused about the method. You mention two degree-dependent rate options (Propositions 3.1 and 3.2), which one did you use in the experiments? More details, explanations, and investigation are neede, for example, which case is better under which conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ur4rZKiUlq", "forum": "Rq0KarPGTB", "replyto": "Rq0KarPGTB", "signatures": ["ICLR.cc/2026/Conference/Submission13199/Reviewer_auD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13199/Reviewer_auD7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967187927, "cdate": 1761967187927, "tmdate": 1762923892526, "mdate": 1762923892526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}