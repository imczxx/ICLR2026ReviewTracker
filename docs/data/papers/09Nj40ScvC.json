{"id": "09Nj40ScvC", "number": 23801, "cdate": 1758348612106, "mdate": 1759896796319, "content": {"title": "Preference-Based Process Reward Model for Robust Mathematical Reasoning", "abstract": "Process reward models (PRMs) have emerged as a promising approach to guide LLMs by providing step-wise supervision, but traditional methods often rely on heuristic search strategies like Monte Carlo Tree Search (MCTS), which introduce bias and limit generalization. In this work, we propose a reinforcement learning framework guided by a   Preference-Based Process Reward Model (PPRM) , which provides step-wise supervision to refine reasoning trajectories. We first employ MCTS to estimate and select chosen and rejected rollouts, thereby constructing a high-quality step-level dataset. Our PPRM is trained on Bradley-Terry loss function, which mitigates the bias introduced by the heuristic search strategies of MCTS by leveraging preference-based learning. To enable effective RL training with PPRM, we enhance  Group Relative Policy Optimization (GRPO)  by introducing a robust advantage estimator that better captures the structure of preference-based process reward model enabling stable and efficient policy optimization. Experimental results on ProcessBench and best-of-n strategy  demonstrate that our approach achieves  $2$-$3\\%$ improvement in intermediate step accuracy compared to existing methods for complex reasoning processes, thereby improving the reasoning accuracy of the policy model across several key reasoning benchmarks.", "tldr": "", "keywords": ["Process Reward Model", "Reinforcement Learning", "Monte Carlo Tree Search"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a96cc56ba0b8dae8c095aa61356d3af1319282c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a heuristic to select preference pairs to train PRM. Meanwhile, it also modifies the advantage function of original GRPO to adapt to process reward settings."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper theoretically shows that the preference-based PRM can achieve higher expected accuracy than the PRM trained by hard labels estimated by MCTS.\n\n- They modify the GRPO advantage estimator to adapt to the preference-based RM."}, "weaknesses": {"value": "> Paradoxical motivation\n\nThe paper argues the MCTS estimation can lead to inconsistent or suboptimal results, however, they still use MCTS with a heuristic metric (Eq.1) to annotate preference.\n\n\n\n> Comparison Fairness & Comprehensiveness \n\n- Empirical aspects: **Models are trained by different datasets**, so the comparsion seems unfair. If trained by the same dataset, would the preference-based formulation still outperform baselines?\n\n- Theoretical aspect: There are many more advanced theoretical framework trying to bypass the hard label estimation of PRM [1][2][3][4], however, the theoretical part only compare with the classical PRMs.\n\n[1] Yuan, Lifan, et al. \"Free Process Rewards without Process Labels.\" Forty-second International Conference on Machine Learning.\n\n[2] Lu, Jianqiao, et al. \"Autopsv: Automated process-supervised verifier.\" Advances in Neural Information Processing Systems 2024.\n\n[3] Zhang, Zheng, et al. \"Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning.\" arXiv preprint arXiv:2509.2657.\n\n[4] Li, Wendi, and Yixuan Li. \"Process Reward Model with Q-value Rankings.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "> PRMs are also evaluated in beam-search tasks or RL tasks. Can PPRM also yield superior performance in these tasks?\n\n> The current experiments are only conducted on a single model. Can PPRM perform consistently across different backbones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rgdCO8yLHy", "forum": "09Nj40ScvC", "replyto": "09Nj40ScvC", "signatures": ["ICLR.cc/2026/Conference/Submission23801/Reviewer_meEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23801/Reviewer_meEZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583173140, "cdate": 1760583173140, "tmdate": 1762942811885, "mdate": 1762942811885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel reinforcement learning framework guided by a Preference-Based Process Reward Model (PPRM). The method first uses MCTS to select chosen and rejected rollouts. Then, Bradley-Terry loss function is used to mitigate bias in MC-value estimation by leveraging pairwise comparisons of reasoning trajectories. The method is trained using GRPO with an optimized advantage estimator to better captures the structure of preference-based process reward model. Experimental results show that the proposed PPRM improves performance on intermediate step accuracy and enhances the final policy model's performance compared to existing works, demonstrating the method's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is described in sufficient detail and appears technically sound.\n\n2. The experimental results are strong, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "### Major Concerns:\n\n1. Clarity of Motivation. The writing in the introduction and motivation sections is not sufficiently clear, which hinders the reader's understanding of the precise problem being solved.\n\n- L51-L53: The logical connection between the sentence \"Lightman et al. (Lightman et al., 2023) demonstrated the effectiveness of using human expert annotators...\" and the following phrase \"To address this...\" is abrupt and unclear. It is not evident what specific problem or limitation \"this\" refers to.\n\n- L14, L58-L61: A central motivation of the paper appears to be the mitigation of bias in MCTS. However, this bias is not clearly defined or explained at the beginning of the paper. A more comprehensive introduction to this problem is needed to properly contextualize the paper's contributions.\n\n2. Insufficient Experimental Discussion. While the experimental results are strong, the discussion and analysis are insufficient. The paper does not adequately connect the empirical gains back to the central claims made in the motivation. Specifically, the authors should provide more detailed analysis to demonstrate how the proposed method successfully alleviates the \"Limitations of PRM\" that were introduced in lines 46-63.\n\n3. The LLM Judger is not formally introduced (L190).\n\n### Minor Issues:\n\n- L42-L44, \"While the Process Reward Model (PRM) offers a promising solution by providing step-wise reinforcement learning feedback.\" is a dependent clause and grammatically incomplete.\n- L79, \"more rob-ust reasoning\"\n\n- L209-L211, there appears to be a missing equation number"}, "questions": {"value": "The core methodology and the reported results are promising, and this work appears to be a valuable contribution. However, the paper's current lack of clarity in the motivation and the insufficient experimental discussion are significant concerns. In order to maintain my rating, I would like the authors to address the major concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "imas59seJq", "forum": "09Nj40ScvC", "replyto": "09Nj40ScvC", "signatures": ["ICLR.cc/2026/Conference/Submission23801/Reviewer_N7uk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23801/Reviewer_N7uk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928729439, "cdate": 1761928729439, "tmdate": 1762942811693, "mdate": 1762942811693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced preference-based process reward model (PPRM). It leverages Bradley-Terry pairwise comparison to reduce bias in process reward modeling. PPRM combines this preference-based formulation with a modified GRPO, to use a preference-aware advantage estimator to stablize training and reduce variance. The experiments are conducted on ProcessBench and RL finetuning tasks and the results show 2-3% accuracy improvement over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper directly targets MCTS-induced heuristic bias in process supervision and this approach offers a clean conceptual and mathematical reformulation.\n2. The proposed preference-based advantage estimator fits into GRPO well and it transforms pairwise rewards into smoother, variance-reduced advantage estimates.\n3. Results span multiple reasoning datasets and evaluation setups and demonstrate consistent improvement"}, "weaknesses": {"value": "1. Although PPRM is introduced as de-biasing MCTS, it still relies on MCTS-generated trajectories to form chosen-rejected pairs. If MCTS itself samples biased reasoning paths, the BT formulation merely reweights rather than corrects them. A comparison using non-MCTS rollouts (e.g., temperature-based or random sampling) would clarify true robustness.\n2.  The reported gains likely combine effects from both (i) preference training and (ii) the new advantage estimator. A clear ablation isolating these factors — plus sensitivity analysis on α, β, and pair length penalty — is essential to interpret where the real improvements come from.\n3.  All benchmarks are math reasoning datasets. Since the claimed contribution is a general reward modeling framework, it’s unclear whether the approach generalizes to symbolic, scientific, or program synthesis reasoning tasks.\n4. Both training and evaluation involve MATH and related datasets (GSM8K variants, OlympiadBench). The authors should clarify de-duplication and overlap handling, especially since Qwen2.5-Math models have been partially trained on similar corpora.\n5. Some relevant studies are missing [1-2]\n\n[1] Entropy-Regularized Process Reward Model\n\n[2] GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning"}, "questions": {"value": "m/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jys5qVn5S", "forum": "09Nj40ScvC", "replyto": "09Nj40ScvC", "signatures": ["ICLR.cc/2026/Conference/Submission23801/Reviewer_FHML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23801/Reviewer_FHML"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936804050, "cdate": 1761936804050, "tmdate": 1762942811523, "mdate": 1762942811523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}