{"id": "2Xd5RlypLx", "number": 15222, "cdate": 1758249085423, "mdate": 1759897320576, "content": {"title": "Diverse Text Decoding via Iterative Reweighting", "abstract": "Recent advances in large language models (LLMs) have led to impressive results in text generation. However, current decoding methods still lack diversity when combined with popular sampling techniques. We propose a Reweighting-based Iterative DEcoding (OverRIDE) approach that dynamically adjusts the decoding process with history responses. Our method fine-tunes auxiliary output heads iteratively on previously generated sequences to capture and suppress semantic patterns that appear in the history responses. This inference-time training process only incurs minimal loss of efficiency. We conduct extensive experiments on various tasks, including code generation, mathematical reasoning and story generation, demonstrating that OverRIDE increases output diversity while maintaining quality. We implement OverRIDE on LLM serving systems like vLLM, achieving a 6.4\\% throughput loss for 72B models under parallel decoding.", "tldr": "We propose OverRIDE, a dynamic decoding method that improves text generation diversity.", "keywords": ["Natural Language Processing", "Large Language Models", "Text Generation", "Decoding Method"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4de416b5b0c935b498bc203826775bbeab45517.pdf", "supplementary_material": "/attachment/3cec3e748e6f47d5975436386eb19558595ec1e4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OverRIDE (Reweighting-based Iterative DEcoding), a novel decoding method that enhances diversity in LLM text generation by learning from previously generated responses. The key innovation is fine-tuning a guide model on past generations and using it to reweight the next-token distribution, thereby suppressing repetitive patterns. The authors implement a parallel version compatible with vLLM, achieving minimal throughput loss. Experiments on code generation, mathematical reasoning, and story generation demonstrate improved diversity while maintaining or improving quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Learning from generation history is an elegant solution to the diversity problem that goes beyond token-level adjustments.\n2. Strong experimental validation across multiple domains (code, math, story), model sizes (3B-72B), and sampling methods.\n3. The parallel version with vLLM integration shows the authors considered real-world deployment.\n4. The decoding dynamics analysis (Section 3.3) provides valuable insights into how the method works, showing the interplay between exploration and quality."}, "weaknesses": {"value": "1. No comparison with other diversity-enhancement methods like SimCTG, contrastive decoding variants, or other inference-time techniques. The paper only compares against standard sampling.\n2. Why is the reweighting formula in Equation 2 the right choice? No analysis of convergence properties or theoretical guarantees. Connection to exploration-exploitation tradeoffs not formalized."}, "questions": {"value": "1. How does OverRIDE compare to other diversity methods like SimCTG or contrastive decoding? A direct comparison would strengthen the claims.\n2. Can you provide theoretical analysis or intuition for why the specific reweighting formula works better than alternatives (e.g., additive vs. multiplicative)?\n3. How do you recommend selecting λ for a new model or task without extensive hyperparameter search?\n4. What is the computational breakdown between inference and adapter fine-tuning? How many gradient steps per round?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4iWEL4zCdM", "forum": "2Xd5RlypLx", "replyto": "2Xd5RlypLx", "signatures": ["ICLR.cc/2026/Conference/Submission15222/Reviewer_BHNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15222/Reviewer_BHNo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784178345, "cdate": 1761784178345, "tmdate": 1762925519560, "mdate": 1762925519560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OverRIDE, a history-aware decoding scheme to improve sample diversity without retraining the base LLM. The method keeps the original head to produce distribution, and adds a lightweight guide head (LoRA-style adapter on the output layer) that is updated online on previously generated samples to produce $q_t$. At each round, the next-token distribution used for sampling is reweighted by suppressing tokens that the guide head deems “common in prior rounds.” The paper provides an implementation aligned with parallel decoding (e.g., vLLM/SGLang-style KV reuse) and reports gains on code, math reasoning, and open-ended generation with small throughput drops that shrink for larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear, pragmatic idea:** A *history-aware* decoding objective that can be layered onto existing stacks with minimal changes (only the output head; small adapter; online updates).\n- **Engineering relevance:** Thoughtful integration with **KV-cache reuse** and round-synchronous updates; throughput overhead appears modest and decreases with model size.\n- **Quality–diversity trade-off:** Reported increases in diversity (lower candidate similarity) with **PASS@k/MAUVE** not degrading and sometimes improving.\n- **Simple hyperparameters:** Two main knobs—$\\lambda$ (suppression strength) and adapter rank $r$—make the method operationally accessible."}, "weaknesses": {"value": "1. **Temperature/top-p parity not rigorously controlled**  \n   The central claim is that OverRIDE provides more than generic flattening (i.e., increasing temperature). However, most comparisons are at author-chosen settings rather than **equal-diversity (equal entropy/KL)** conditions. Without controlled comparisons, it is hard to isolate the benefit beyond “being hotter.”\n\n2. **Stability and guardrails of ratio reweighting**  \n   The method hinges on $p_t \\propto (p/q_t)^\\lambda p\\$. If $q_t$ is small or miscalibrated on some tokens, the ratio can explode and induce overly aggressive exploration. The paper does not fully quantify **failure modes** (invalid/format-broken outputs) under large \\(\\lambda\\), nor detail **safeguards** (logit clipping, $\\epsilon$ -floors, temperature stacking).\n\n3. **Guide-head “slight drift” is asserted but under-measured**  \n   The text claims the guide model “only drifts slightly,” but lacks systematic analysis of $\\(\\mathrm{KL}(q_t\\|p)\\)$ over rounds, **adapter-norm growth**, or how drift scales with $r$ and $\\lambda$. This matters for safety and to understand when the guide becomes effectively an *anti-LM*.\n\n4. **Parallel scheduling bias**  \n   The implementation updates head $t{+}1$ immediately after sampling round $t$ and enforces a **round-ordered schedule** for KV reuse. This could introduce **order bias** across candidates. There is no study of randomized/async updates or batch-size sensitivity to rule out engineering confounds.\n\n5. **Ablations don’t fully decompose gains**  \n   Improvements in PASS@k could come from (a) genuine coverage of new solution modes vs. (b) more surface-different but semantically near misses. The paper lacks **semantic de-dup** analysis, **first-hit round distributions**, and **cost-normalized** comparisons (same steps/candidates)."}, "questions": {"value": "- **Equal-diversity controls:** Can you report results with *matched entropy* (or matched average NLL) against temperature/top-p, with significance testing?  .  \n- **Scheduling bias:** Does randomizing candidate update order or using async updates change results? Provide curves vs. batch size/KV hit rate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5nSPfkyGW5", "forum": "2Xd5RlypLx", "replyto": "2Xd5RlypLx", "signatures": ["ICLR.cc/2026/Conference/Submission15222/Reviewer_Drqn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15222/Reviewer_Drqn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959129839, "cdate": 1761959129839, "tmdate": 1762925519067, "mdate": 1762925519067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OverRIDE, an iterative reweighting decoding method designed to increase output diversity while preserving task performance. OverRIDE introduces test-time trainable output-head adapters that iteratively reweight next-token logits to suppress tokens leading to previously seen patterns, steering decoding away from already-explored distributions. The adapters are confined to the output head, updated after each round, and synchronized with sampling, which makes the method compatible with parallel inference in serving systems. Combined with standard sampling, this history-aware reweighting increases diversity while preserving quality across code, math, and open-ended generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental coverage is broad, including two LLM families (Qwen2.5 and Mistral), five parameter scales from 3B to 72B, and four standard benchmarks.\n2. OverRIDE shows consistent diversity improvements over common decoding baselines in nearly all settings, with pass@k accuracy gains, while incurring only a modest slowdown."}, "weaknesses": {"value": "1. For the CCNews open-ended long-form generation in Table 2, Distinct-n and Self-BLEU may be more appropriate diversity or similarity metrics than paragraph-level embedding cosine similarity.\n2. Comparisons focus on basic sampling schemes and min-p. How does your method perform compared with other classic baselines such as beam search and group beam search?\n3. Training and implementation details are under-specified. The core Output head adapters lack information on parameter counts, learning rates, and other settings. Section 3.4 reports only throughput in tokens per second without the hardware configuration.\n4. When restricted to a single iteration, OverRIDE has no effect and collapses to the baseline, which limits scenarios that demand single-pass decoding."}, "questions": {"value": "1. There is a discrepancy between your Table 1 and the Qwen-2.5 technical report: on HumanEval, the official PASS@1 is 52.9 for the 7B base model (Table 4 [1]) and 84.8 for the 7B Instruct model (Table 8 [1]), whereas your Table 1 lists 64.9 under “greedy.” After reviewing your released code, I see you evaluate Qwen-2.5-X-B-Instruct rather than the base Qwen-2.5-X-B. Please verify why your evaluation diverges from the official numbers, and standardize the model naming in the paper (or re-run the baselines with the base model) to avoid confusion. \n2. Additionally, I would like to know the exact evaluation setup for CCNews reported in Table 2. The relevant code does not appear to be included in your supplementary materials.\n3. In Table 3, row 315 shows Qwen-2.5-14B with a notably lower HumanEval PASS@1  (greedy)  than other sizes of the same family. Is this a typographical error?\n\n\n[1] Qwen Team. (2025). *Qwen2.5 Technical Report*. arXiv. arXiv:2412.15115."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zm3EcFWoaF", "forum": "2Xd5RlypLx", "replyto": "2Xd5RlypLx", "signatures": ["ICLR.cc/2026/Conference/Submission15222/Reviewer_wTcW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15222/Reviewer_wTcW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998718555, "cdate": 1761998718555, "tmdate": 1762925518596, "mdate": 1762925518596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel decoding method called OverRIDE, increasing semantic diversity in the generation of text by LLMs. It dynamically adjusts the probability distribution of the next word in the inference stage, encouraging the model to explore new and low-probability generation paths. The experiment proved that this method can enhance semantic diversity and achieve better results in multiple sampling processes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The homogenization of the content generated by LLMs is a widespread problem that needs to be addressed.\n2. The authors designed and implemented a parallel algorithm based on high-performance inference frameworks, which is beneficial for the use of the open-source community."}, "weaknesses": {"value": "My primary concerns relate to the comprehensiveness of the experimental evaluation. To be confident in the paper's claims, the following points need to be addressed:\n1.  Incomplete Efficiency Evaluation. The paper claims \"minimal loss of efficiency\" based solely on throughput. For a method that introduces training at inference time, this is insufficient. A more thorough analysis of computational overhead is critical. Please report Time To First Token metric for user-perceived latency, and the overall memory cost compared with normal sampling.\n2.  Limited and Potentially Dated Datasets. The experiments are conducted on relatively simple tasks. Several studies have demonstrated the crucial role of the \"critical token\" in ensuring the correctness of the model's responses. Unselectively suppressing the tokens generated in the first round may affect the model's accuracy. The benchmarks evaluated by the authors, such as MATH-500 and GSM8k, are not challenging for Qwen2.5 and might overlook this issue. It is recommended to add more cutting-edge data, at least including AIME.\n3.  Missing Comparison with key baselines. The paper compares OverRIDE with popular sampling techniques but omits a comparison with other advanced decoding methods that also aim to improve generation quality and diversity. A direct comparison with Contrastive Search is essential. Both methods modify the decoding objective to balance model likelihood with a penalty term. A head-to-head comparison on both quality and diversity metrics is necessary to properly contextualize the contributions of OverRIDE.\n4.  Concern of Hyperparameter Sensitivity Analysis. The proposed algorithm likely depends on a key hyperparameter $\\lambda$. Although the author conducted the sensitivity experiment, \\lambda may still be a challenging parameter. Users will not have fine-grained prior knowledge during the usage process. Robustness and usability of the method are supposed to be verified through a broader model family and more difficult data (as mentioned in the second concern)"}, "questions": {"value": "1. Can you add evaluations on the time to first token metric and the overall memory cost compared with normal sampling?\n2. Does OverRIDE also have considerable superiority on more challenging benchmarks? Can robustness research be conducted on more difficult data such as AIME and LiveCodeBench?\n3. The recent contrastive decoding method mentioned by the author should be compared with OverRIDE [1]. \n4. Can you provide fine-grained sensitivity analysis on the lambda performance of more model families on more challenging benchmarks?\n\n[1] Su, Yixuan, et al. \"A contrastive framework for neural text generation.\" Advances in Neural Information Processing Systems 35 (2022): 21548-21561."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2FFqRS6LJ", "forum": "2Xd5RlypLx", "replyto": "2Xd5RlypLx", "signatures": ["ICLR.cc/2026/Conference/Submission15222/Reviewer_om5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15222/Reviewer_om5c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15222/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762867847315, "cdate": 1762867847315, "tmdate": 1762925518098, "mdate": 1762925518098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}