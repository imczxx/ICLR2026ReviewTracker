{"id": "KKFv9Rl022", "number": 6090, "cdate": 1757952438834, "mdate": 1759897935944, "content": {"title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts", "abstract": "Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. \nAmong these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. \nMathematical reasoning highlights the high-level capability of VLMs to comprehend mathematical information embedded in images and to perform sophisticated reasoning processes.\nRecently, numerous visual mathematical reasoning benchmarks have been proposed to evaluate the mathematical reasoning capabilities of VLMs. \nHowever, these benchmarks suffer from several limitations: they are typically restricted to geometry problems, lack comprehensive evaluation on math word problems, and rarely assess the ability to reason across multiple images.\nTo fill this gap, we introduce \\textbf{GSM8K-V}, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is constructed by systematically mapping each sample from the widely used text-based mathematical reasoning benchmark GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate a benchmark comprising 1,319 high-quality samples.\nWe evaluate a wide range of open-source and close-source models on the proposed GSM8K-V benchmark. Our results reveal that, although existing VLMs have achieved nearly saturated performance on the text-based GSM8K, there remains substantial room for improvement on the purely visual GSM8K-V. For instance, the best-performing model, \\textit{Gemini-2.5-Pro}, attains 95.22\\% accuracy on GSM8K but only 46.93\\% on GSM8K-V.\nWe conduct a comprehensive and detailed analysis on GSM8K-V, systematically examining the limitations of existing models on this benchmark as well as potential directions for improvement. \nGSM8K-V provides a new perspective on visual mathematical reasoning and establishes a novel evaluation benchmark that can guide the research community toward developing more robust and generalizable VLMs.", "tldr": "GSM8K-V is a new multi-image visual mathematical reasoning benchmark that exposes major gaps in current vision-language models’ reasoning abilities compared to their strong performance on text-based tasks.", "keywords": ["visual mathematical reasoning", "visual benchmark", "grade school math word problem"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9784145a9be7f729008d5ef77846f57258084eb5.pdf", "supplementary_material": "/attachment/60cdd3d70a3450afe9f7e2768d2279c1551772dd.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces GSM8K-V, a large-scale benchmark that converts the classic text-based GSM8K math dataset into a purely visual version. Each original problem is decomposed into scenes and translated into multiple images (average 4 per problem), forming 1,319 visually grounded math problems. The goal is to measure how well vision-language models (VLMs) can perform visual mathematical reasoning when all information is conveyed through images instead of text. Experiments show a strong modality gap—models that perform almost perfectly on text (e.g., Gemini-2.5-Pro with 95%) drop sharply on the visual version (around 47%), while humans remain highly accurate (≈91%). The authors also perform detailed error analysis, ablations, and comparisons of perception vs reasoning failures."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Builds the first visual equivalent of GSM8K, enabling a one-to-one comparison between text and visual reasoning tasks. Uses a three-stage generation process (problem decomposition → visual scene generation → human verification) to ensure semantic fidelity between text and image.\n\n2.  Clearly exposes the visual reasoning gap of current VLMs, showing that perception and symbolic calculation remain unsolved under visual input.\n\n3. Includes ablations on question format, number of images, and visual style, providing clear insight into what factors cause model failures.\n\n4. Establishes a reliable human baseline and defines specific failure categories (e.g., perception-calculation error, instrument-reading error), offering actionable directions for model improvement."}, "weaknesses": {"value": "1. The benchmark relies on a single, uniform “cartoon-style” visual domain for all problems. This limited diversity may cause models to overfit to specific visual cues instead of learning general, transferable visual reasoning skills. As a result, it remains unclear how well the benchmark performance reflects a model’s ability to handle more realistic visual formats such as photographs, sketches, or diagrams.\n\n2. The current benchmark design mixes two different challenges — text recognition (OCR) and logical reasoning. Because many images contain numbers or symbols that must be read before reasoning, it is hard to separate perception errors from reasoning errors. A wrong answer may come from misreading a digit rather than misunderstanding the problem, making diagnosis and targeted improvement difficult.\n\n3. Each textual problem is split into multiple visual scenes, forming a multi-image narrative input. This structure increases the demand for information integration and temporal consistency, but the paper does not analyze how this decomposition affects model performance. Without a controlled comparison between single-image and multi-image formats, it is unclear how much of the observed performance drop comes from multi-image reasoning complexity rather than from the core visual reasoning challenge itself."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G8fYicMfTK", "forum": "KKFv9Rl022", "replyto": "KKFv9Rl022", "signatures": ["ICLR.cc/2026/Conference/Submission6090/Reviewer_F6kb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6090/Reviewer_F6kb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566374911, "cdate": 1761566374911, "tmdate": 1762918457313, "mdate": 1762918457313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GSM8K-V, a purely visual and multi-image benchmark for mathematical reasoning. The authors systematically transform each sample from the text-based GSM8K dataset into visual form through an automated image-generation and human-annotation pipeline, producing 1,319 high-quality samples. The work highlights the limitations of current VLMs in grounding mathematical reasoning in visual contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work addresses an important gap by extending mathematical reasoning evaluation beyond geometric diagrams to multi-image visual word problems.\n\nThe mapping from textual GSM8K samples to visual counterparts with a well-defined image-generation and validation pipeline that combines automation with human verification.\n\nThe authors benchmark a broad set of open- and closed-source VLMs."}, "weaknesses": {"value": "The paper lacks a clear rationale for converting text-based math word problems into purely visual, multi-frame representations. In current multimodal large language model design, the canonical two-stage process involves visual perception for diagram understanding followed by symbolic reasoning in the language model — leveraging both modalities. Completely removing textual input seems counterintuitive and does not necessarily yield a more realistic or informative evaluation compared to existing benchmarks such as MathVerse or MathVista.\n\nIn terms of realism perspective, short textual prompts (like fewer than 50 tokens) into several images may introduce artificial visual complexity rather than emulate real-world reasoning. If realism is the goal, video-based datasets such as VideoMathQA (VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos) provide a more natural form of multimodal reasoning involving temporal perception and sequential inference from the real world. \n\nThe benchmark closely resembles OCR-based mathematical reasoning, where models must parse textual information embedded in images. The paper does not adequately clarify how GSM8K-V differs from existing OCR or document-understanding benchmarks. Including baselines models such as MinerU or PaddleOCR would help quantify this distinction.\n\nThe contribution is largely restricted to dataset construction. The paper would benefit from demonstrating how GSM8K-V design pipeline/workflow could be used to design specific training signals or losses that enhance reasoning over visual information, and showing generalization from GSM8K-V-trained models to real-world multimodal data (e.g., whiteboard videos, scanned worksheets) would strengthen both novelty and applicability."}, "questions": {"value": "The work does not clearly articulate what unique cognitive or reasoning abilities GSM8K-V tests compared to existing benchmarks. I agree with “one picture is worth a thousand words” but this paper is inverted here: concise textual expressions are expanded into multi-image sequences. The authors may consider how to  clarify what new aspects of multimodal reasoning this transformation reveals, or what insights it offers for developing future MLLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1z7pwsGcQJ", "forum": "KKFv9Rl022", "replyto": "KKFv9Rl022", "signatures": ["ICLR.cc/2026/Conference/Submission6090/Reviewer_GZam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6090/Reviewer_GZam"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627203663, "cdate": 1761627203663, "tmdate": 1762918456977, "mdate": 1762918456977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon the well-known benchmark GSM8K (grade school math problems) and presents GSM8K-V by converting ~1.3K text-only questions to their purely-visual comic-based counterparts. The best model, Gemini pro's performance on the new benchmark drops drastically from the original performance of ~95% to ~47%, demonstrating the headroom the new benchmark introduces to the research community."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1- The paper is quite interesting and nice to read, and the idea is novel.\n\n2- The benchmark is new and challenging and has meticulous human annotations as mentioned by the paper. This is quite a contribution to the community.\n\n3- Extensive results, sensitivity/ablation/error analyses as well as detailed evaluations, make it a whole lot easy to reproduce the findings."}, "weaknesses": {"value": "1- The pipeline works extensively and heavily based on closed-source models such as GPT and hence is expensive (as mentioned in Table 7).\n\n2- Pixar and Giphli styles show minimum differences using ablation study, but it's not clear if the observations made in the paper can still be made using noisy and real-world pictures.\n\n3- Table 5 is somehow confusing, as I can't wrap my head around the logic behind the proof. For instance, GPT-4o's performance degrades badly using OCR but with caption achieves 55% which is still better than image-only ~29%. This may suggest the model is simply not good enough at perceiving images rather than being a modality issue."}, "questions": {"value": "1- What would happen to the models performances if the images are presented in real-world format rather than cartoons?\n\n2- As also mentioned in the last weakness, I still don't understand why captions are better than images? It is because the structured text in the caption helped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AYtE8mjUcW", "forum": "KKFv9Rl022", "replyto": "KKFv9Rl022", "signatures": ["ICLR.cc/2026/Conference/Submission6090/Reviewer_bjKz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6090/Reviewer_bjKz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870038780, "cdate": 1761870038780, "tmdate": 1762918456648, "mdate": 1762918456648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The GSM8K-V benchmark evaluates vision-language models handling word math problems within visual contexts. It involves converting text-based math problems into cartoon-like images through a specialized pipeline, testing the models on how they reason through the visual data. The study discusses various trials and highlights limitations in current models, underscoring the challenge of accurately combining visual reasoning with textual problem-solving."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with clear figures and detailed explanations that make the work easy to follow.\n\n2. The paper presents a systematic method for converting text-based math problems into visual formats with controlled complexity.\n\n3. The evaluation highlights a significant performance gap between visual and text-based reasoning, providing a target for visual reasoning improvements.\n\n4. The paper identifies key factors like multi-image inputs and caption structures that impact model performance, with the potential to motivate future works."}, "weaknesses": {"value": "1. While the GSM8K-V dataset focuses on educational math problems, these problems may not represent the complexity and variety of real-world mathematical reasoning tasks. The approach of converting problems that do not naturally require visual reasoning into visual formats may not align with how mathematical reasoning occurs in practice.\n\n2. The motivation of the paper remains unclear to me. For the same textual problem, visual problems can be designed with varying levels of difficulty. Introducing irrelevant or distracting information would undoubtedly decrease reasoning accuracy, and I believe this does not provide much valuable insight.\n\n3. The generation of visual scenes relies on predefined templates, which may limit the diversity of scene combinations and fail to reflect the complexity of dynamic and unstructured scenarios."}, "questions": {"value": "1. It would be valuable to categorize errors by their underlying issues, such as spatial reasoning failures, object counting mistakes, or logical inference errors.\n\n2. The authors did not provide explicit guidance on whether the model should analyze the image or solve the problem directly, which could create ambiguity even for humans. I recommend an ablation study examining how explicit instructions about the task focus affect model performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j3Pm8LiALX", "forum": "KKFv9Rl022", "replyto": "KKFv9Rl022", "signatures": ["ICLR.cc/2026/Conference/Submission6090/Reviewer_BSSw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6090/Reviewer_BSSw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995908290, "cdate": 1761995908290, "tmdate": 1762918456263, "mdate": 1762918456263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}