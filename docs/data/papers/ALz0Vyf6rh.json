{"id": "ALz0Vyf6rh", "number": 6792, "cdate": 1757995960929, "mdate": 1759897893374, "content": {"title": "IRGCL： Information Refinement Graph Contrastive Learning", "abstract": "Graph contrastive learning (GCL) has emerged as a leading paradigm in unsuper-\nvised graph representation learning (UGRL), yet existing contrastive approaches\nremain vulnerable to three persistent challenges: noisy features that distort simi-\nlarity measures, unreliable structures that contain spurious edges, and degree im-\nbalance that biases representation quality. We propose Information-Refinement\nGraph Contrastive Learning (IRGCL), a single-view contrastive learning frame-\nwork that simultaneously addresses these challenges and effectively generalizes\nacross key graph learning tasks, including node classification, clustering, and link\nprediction. IRGCL integrates three complementary components: (i) structure-\nconsistent feature selection to filter out redundant or noisy attributes; (ii) high-\nconfidence structure learning to refine graph neighborhoods; and (iii) degree-\naware focal contrastive learning to balance learning across low- and high-degree\nnodes. Extensive experiments on diverse benchmarks demonstrate that IRGCL\nconsistently outperforms state-of-the-art baselines, and ablation studies confirm\nthe distinct and complementary benefits of each component, highlighting the ne-\ncessity of jointly addressing feature quality, structural reliability, and degree im-\nbalance. Code is available at https://anonymous.4open.science/r/IRGCL-01F8.", "tldr": "", "keywords": ["feature selection", "structure learning", "degree aware", "graph contrastive learning", "single view"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e87a563d7f45411c149c30504de3e66fb44fbb7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes IRGCL, a single-view graph contrastive learning method designed to simultaneously address three common challenges in graph contrastive learning: feature noise, structural unreliability, and node degree distribution imbalance. The method integrates three synergistic modules, low-rank feature selection, confidence-guided clustering-based structure learning, and degree-aware focal contrastive loss. And IRGCL validates its performance through experiments on multiple graph learning tasks including node classification, node clustering, and link prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strength 1: The framework establishes a comprehensive architecture that jointly addresses the three key issues in GCL, feature quality, structural reliability, and degree bias, with a clear and well-structured design.\n\nStrength 2: Theoretically proves that the structure learning module monotonically decreases Dirichlet energy and enhances homophily, providing a mathematical foundation for this component."}, "weaknesses": {"value": "Weakness 1: In Module 1 (Feature Selection), the optimization requires alternating updates between the W and F matrices. This strategy cannot guarantee convergence to a global optimum and is highly likely to settle in a local optimum, making the final feature selection quality strongly dependent on initialization. Moreover, updating W involves matrix inversion, which leads to high computational complexity for high-dimensional features. Ablation studies indicate that the performance gain is relatively small compared to the substantial computational cost.\n\nWeakness 2: In implementation, Module 1 (Feature Selection) operates as an independent \"preprocessing\" step and lacks tight, end-to-end integration with the subsequent GNN encoder and contrastive learning objective. The feature selection criterion, based on reconstruction error and smoothness, may not align with the final task objective, such as discriminability in node classification.\n\nWeakness 3: Module 3 (Contrastive Learning with Neighborhood & Degree Awareness) has limitations in positive and negative sample selection for low-degree nodes. The core issue for low-degree nodes is the severe shortage of positive samples. Using only one-hop neighbors as positives lacks semantic richness, and since low-degree nodes constitute the majority of the dataset, the sampling process may result in significant loss of valuable information.\n\nError 1: There is an inaccuracy in the experimental results for the Computers dataset in Table 6."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cHPx82WJsv", "forum": "ALz0Vyf6rh", "replyto": "ALz0Vyf6rh", "signatures": ["ICLR.cc/2026/Conference/Submission6792/Reviewer_1cSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6792/Reviewer_1cSQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910614210, "cdate": 1761910614210, "tmdate": 1762919065757, "mdate": 1762919065757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a principled framework to improve the unsupervised graph contrastive learning (GCL) by replacing noisy random augmentations with structured information refinement. It introduces a three-stage approach: (1) Feature Selection via Low-Rank Approximation, which removes redundant or uninformative node features while preserving structural smoothness; (2) Structure Learning via High-Confidence Clustering, which edits graph edges based on cluster consistency, theoretically guaranteeing monotonically decreasing Dirichlet energy and increasing homophily; and (3) Contrastive Learning with Degree Awareness, which employs a degree-adaptive focal loss and JSD contrastive objective to handle unbalanced neighborhoods. Extensive experiments across benchmarks show that IRGCL achieves state-of-the-art results with strong robustness and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed low-rank feature selection effectively avoids the randomness in masking or perturbations, providing a stable approach to view enhancement.\n2. The adaptive focal weighting mechanism provides an effective solution to the problem of degree imbalance in graph structures.\n3. Extensive experiments demonstrate the good performance of IRGCL across diverse datasets."}, "weaknesses": {"value": "1. The three refinement modules lack a clearly defined systemic coupling. The overall framework appears to be a combination of several relatively independent components. Providing an unified architecture or theoretical framework would be better.\n2. The update process of $W$ lacks theoretical guarantees of convergence, which introduces uncertainty regarding the stability of optimization.\n3. The method involves a large number of hyperparameters and exhibits relatively high computational complexity, creating a gap with the authors' claim of being \"without fussy augmentations.\"\n4. The model mainly relies on local neighborhood sampling and does not explicitly capture global semantics or long-range dependencies.\n5. The clustering procedure is relatively complex and computationally demanding, which limits the applicability to relatively large-scale graphs."}, "questions": {"value": "1. The method involves a large number of hyperparameters. How are they balanced across different modules, and how is stable convergence ensured in updating $W$?\n2. How efficient is the proposed method in practice, especially when applied to large-scale graphs in terms of time and memory complexity?\n3. Since most modules are designed to enhance smoothness, how does the model prevent over-smoothing and maintain discriminative representations under this design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F16BUp6bVh", "forum": "ALz0Vyf6rh", "replyto": "ALz0Vyf6rh", "signatures": ["ICLR.cc/2026/Conference/Submission6792/Reviewer_NJ1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6792/Reviewer_NJ1V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910907925, "cdate": 1761910907925, "tmdate": 1762919065331, "mdate": 1762919065331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that existing GCLs face three major challenges: noisy feature, unreliable structures, degree imbalance. The proposed IRGCL method tackles these challenges by combining structure-aware feature selection, high-confidence structure learning and degree-aware focal contrast. The authors claim that IRGCL outperforms existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is driven by a clear motivation, which is directly addressed by the proposed method.\n2. This paper has a good theoretical analysis.\n3. The figures are well presented."}, "weaknesses": {"value": "1. The term \"spurious edges\" is mentioned in the motivation but is not explicitly defined. It remains unclear what specific types of edges fall into this category.\n2. There is an inconsistency in Section 3.1 regarding the rank of matrix W. It is initially constrained to r (rank(W) = r), but later referred to as k when discussing the number of retained features (rank(W) = k). This ambiguity needs to be resolved.\n3. Section 3.2 relies on node silhouettes to identify high-confidence clusters, yet it omits the definition and calculation of this crucial metric.\n4. The \"degree safeguard\" mentioned in Section 3.2 is a critical component for maintaining graph connectivity, but its implementation details are not provided. It is unclear how this mechanism operates to prevent nodes from becoming isolated.\n5. The compared baselines are somewhat outdated, and there is a lack of comparison with SOTA approaches. \n6. The improvements in the experiments are marginal. \n7. A fundamental weakness of this work is its lack of novel contributions, as it seems to largely repackage ideas from prior works."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SIA5d6ZoWb", "forum": "ALz0Vyf6rh", "replyto": "ALz0Vyf6rh", "signatures": ["ICLR.cc/2026/Conference/Submission6792/Reviewer_aVZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6792/Reviewer_aVZj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929718883, "cdate": 1761929718883, "tmdate": 1762919064614, "mdate": 1762919064614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of graph contrastive learning (GCL) under noisy features, unreliable graph structures, and degree imbalance. It proposes IRGCL, a single-view contrastive framework that integrates (i) Laplacian-regularized low-rank feature selection, (ii) confidence-guided clustering for structural refinement, and (iii) degree-aware focal JSD loss for balanced contrastive learning. Experiments on transductive, inductive, and clustering benchmarks demonstrate that IRGCL consistently outperforms other methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is clear, and the method flow is easy to follow.\n\n- The experiments are comprehensive, covering node classification, clustering, and link prediction tasks.\n\n- The codes are provided, enhancing reproducibility."}, "weaknesses": {"value": "- The motivation is somewhat ambiguous. Feature quality and degree imbalance are intrinsic properties of graph topology (e.g., power-law distributions), rather than specific flaws of contrastive learning. To justify their inclusion, the authors should clarify how these aspects particularly affect the contrastive objective.\n\n- Among the three stated motivations, graph reliability is mentioned but not clearly defined, especially regarding which type of \"neighborhood-based contrast\" is being referred to. Moreover, the explanation around Lines 58–63 should be elaborated in future revisions.\n\n- Experimental precision is inconsistent (mixture of one- and two-decimal reporting), which slightly reduces the rigor of the empirical section.\n\n- The baselines in Table 4 differ from those in Table 2 and lack GCL methods (e.g., PolyGCL), making it difficult to fully assess comparative effectiveness."}, "questions": {"value": "- In Figure 4, when the ratio = 0, the results on Cora and CiteSeer also appear to perform good. What accounts for this improvement?\n\n- In Tables 5 and 6, why do the accuracies on Photo and Computers remain higher than most results in Table 2 even after removing other components? Moreover, in Table 6, what causes the Acc on Computers to surge to 94.50%, significantly surpassing all baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "msVAKzORaN", "forum": "ALz0Vyf6rh", "replyto": "ALz0Vyf6rh", "signatures": ["ICLR.cc/2026/Conference/Submission6792/Reviewer_NsBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6792/Reviewer_NsBZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991865551, "cdate": 1761991865551, "tmdate": 1762919064102, "mdate": 1762919064102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}