{"id": "CikivUcvmw", "number": 13190, "cdate": 1758214890647, "mdate": 1759897457738, "content": {"title": "Towards Identifiability of Interventional Stochastic Differential Equations", "abstract": "We study identifiability of stochastic differential equations (SDE) under multiple interventions.  Our results give the first provable bounds for unique recovery of SDE parameters given samples from their stationary distributions. We give tight bounds on the number of necessary interventions for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.  We experimentally validate the recovery of true parameters in synthetic data, and motivated by our theoretical results, demonstrate the advantage of parameterizations with learnable activation functions in application to gene regulatory dynamics.", "tldr": "Proof of efficient recover of interventional SDE with low-rank constraint only from stationary distribution", "keywords": ["Stochastic differential equation", "causal disentanglement", "identifiability", "intervention"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b1e6d64eb225cf5037696ed30ec1ced999d1fdf.pdf", "supplementary_material": "/attachment/9b5184a7df273906ff6d513d838fb0135b46d9ad.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies identifiability of stochastic differential equations from stationary distributions under multiple interventions. It gives the first theoretical bounds on the number of interventions required in both linear and nonlinear (small-noise) regimes, supported by synthetic and semi-synthetic experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "•\tRigorous mathematical development with clear proofs.\n\t•\tTheoretical results are novel within the causal inference/SDE literature.\n\t•\tExperiments confirm the identifiability thresholds."}, "weaknesses": {"value": "•\tThe setting (identifiability from stationary SDEs) is quite narrow and primarily of mathematical interest.\n\t•\tThe nonlinear result holds only under restrictive assumptions (contractive drift, small noise).\n\t•\tNo real connection is made to learning algorithms or generative diffusion models, which would be essential for ICLR relevance.\n\t•\tThe applications is minimal and does not add conceptual depth."}, "questions": {"value": "•\tHow do the identifiability results for stationary interventional SDEs inform or improve learning algorithms used in practice?\n\t•\tCan the presented theory be applied to diffusion-based generative models, where SDEs define learned data distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Rxnq1C3fiM", "forum": "CikivUcvmw", "replyto": "CikivUcvmw", "signatures": ["ICLR.cc/2026/Conference/Submission13190/Reviewer_229n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13190/Reviewer_229n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714973772, "cdate": 1760714973772, "tmdate": 1762923886439, "mdate": 1762923886439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles identifiability of SDE models using only stationary snapshots under multiple interventions (no trajectories). It provides theoretical guarantees for when drift parameters are recoverable from moment information across interventions, covering both linear and nonlinear settings. It validates the ideas on synthetic and semi-synthetic GRN benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated problem.**\nThe paper addresses the challenge of recovering system dynamics from stationary, intervention-only data, a realistic and important setting for many scientific domains (e.g., biology), where collecting time-series trajectories is often infeasible.\n2. **Novel theoretical contribution.**\nThe work provides, to the best of my knowledge, the first provable identifiability guarantees for SDEs observed only through stationary interventional distributions, covering both linear and nonlinear cases."}, "weaknesses": {"value": "1.  **Theoretical presentation lacks clarity.**\nThe main theorems are difficult to follow because they do not explicitly list all required assumptions. For instance, Theorem 4.4 depends on distributional/genericity assumptions (Assumption 4.2) and a known $D$ (Assumption 4.3), yet these are not stated in the theorem itself but scattered in the text. This weakens the precision and reproducibility of the claims.\n2.  **Restrictive linear setup.** \nThe linear identifiability results rely on strong and somewhat unrealistic assumptions (e.g., requiring certain structural components or known parameters) which limit their practical applicability and make the “identifiable with $r$ interventions” message feel narrower than presented.\n3. **Theory–practice gap in the nonlinear setting.**\nThe nonlinear guarantees assume globally contractive and monotone activations, but the experiments with learnable activations (generic MLPs) do not appear to enforce these constraints. This creates a noticeable mismatch between the theoretical results and the empirical demonstrations.\n4.  **Limited regime of validity.**\nThe main results hold only in the small-noise regime. The paper itself notes that the proposed losses (e.g., KSD/Sinkhorn) become numerically unstable as noise increases, and that empirical benefits diminish in that regime, which limits the broader applicability."}, "questions": {"value": "1. **Clarify theorem statements.**\nPlease restate Theorems 4.4 and 4.8 with their assumptions explicitly enumerated (e.g., “Under Assumptions 4.2–4.3 …”; “Under Assumption 4.5, $||A\\|, ||B|| \\leq 1$, and i.i.d. interventions ...\"). This would significantly improve readability and rigor.\n2. **Activation constraints in experiments.** \nIn the nonlinear experiments with learnable activations, were monotonicity or contractivity constraints enforced (e.g., via constrained layers or regularization)? If not, how should readers reconcile the theoretical assumptions with the empirical results?\n3. **Sample complexity and scalability.**\nCould you report the approximate sample complexity (e.g., cells per intervention) needed for stable recovery in both linear and nonlinear regimes, and discuss how computational cost scales with $n$ and $r$?\n4. **Related work suggestion.** \nIn the “Dynamical System Methods” section of related work, please consider including  _Wang et al., NeurIPS 2023: \"Generator identification for linear SDEs with additive and multiplicative noise\"_, which provides identifiability results for linear SDEs based on the generator. This paper is highly relevant and would help position your contribution more clearly within the broader identifiability literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gJmq4XMhkV", "forum": "CikivUcvmw", "replyto": "CikivUcvmw", "signatures": ["ICLR.cc/2026/Conference/Submission13190/Reviewer_tcP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13190/Reviewer_tcP2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746473800, "cdate": 1761746473800, "tmdate": 1762923886111, "mdate": 1762923886111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a system with state space R^d and evolution described by a first order stochastic differential equation (SDE), with identity diffusion. The authors assume to have access to 'interventions' which means that the drift coefficient can be modified by addition of a constant term. They ask the question of identifiability of the drift from the stationary distribution of the SDE for multiple values of the intervention.\nTwo type of results are presented:\n1) Linear case. In this case the drift is linear with corresponding matrix given by the sum of a rank r term and an arbitrary known term.\nUnder a a probabilistic model for the low rank component (that in particular ensures genericity) tyhey prove that r-2 interventions are necessary and r are sufficient.\n2) Nonlinear case. The drift is assumed to be parametrized by a two-layer neural network with r hidden neurons. This case is treated in the limit in which the stochastic component of the SDE vanishes, reducing to the linear case by a perturbative argument."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In many systems that are modeled by an SDE is unrealistic to assume that we can observe trajectories, and it is instead more common to have access to the stationary distribution. The stationary measure does not uniquely identify the drift and hence the plan of studying this problem under interventions is well motivated and interesting.\nThe presentation is clear, and the results are easy to understand."}, "weaknesses": {"value": "Establishing identifiability is only the first step towards understanding estimation accuracy; optimal procedures; computational complexity and so on.\nThe identifiability result in the linear model appears a relatively direct fact of linear algebra.\nAs for the nonlinear case, the result is purely perturbative and non-quantitative. It requires the drift to be a contraction with a unique fzero, enabling perturbative argument. No quantitative estimate is given on how small \\epsilon must be for the identifiability to hold."}, "questions": {"value": "1) Assumption 4.2 is stated in a form that is not very transparent. I believe that what is required is really certain deterministic conditions on A, B, C. Instead the authors choose A,B,C to be random so that those conditions are satisfied a.s. I think it would be much better to express the conditions in deterministic form. Also \"each column is drawn iid\" probably means that the columns c_1, c_2,.. are iid.  \n\n2) Why the low-rank model, and the corresponding r hidden neurons network are good models for the problems proposed in the introduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "akLhvRuZBJ", "forum": "CikivUcvmw", "replyto": "CikivUcvmw", "signatures": ["ICLR.cc/2026/Conference/Submission13190/Reviewer_T1DS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13190/Reviewer_T1DS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058388291, "cdate": 1762058388291, "tmdate": 1762923885825, "mdate": 1762923885825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}