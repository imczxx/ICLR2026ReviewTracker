{"id": "iqAFhWistW", "number": 5454, "cdate": 1757911518458, "mdate": 1763710405508, "content": {"title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?", "abstract": "Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, which thus correspond to two core capabilities: \\textbf{\\textit{composition}} and \\textbf{\\textit{reasoning}}. Despite recent advances of T2I models in both composition and reasoning, existing benchmarks remain limited in evaluation. They not only fail to provide comprehensive coverage across and within both capabilities, but also largely restrict evaluation to low scene density and simple one-to-one reasoning. To address these limitations, we propose \\textbf{\\textsc{T2I-CoReBench}}, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (\\textit{instance}, \\textit{attribute}, and \\textit{relation}) and reasoning around the philosophical framework of inference (\\textit{deductive}, \\textit{inductive}, and \\textit{abductive}), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent real-world complexities, we curate each prompt with higher compositional density for composition and greater reasoning intensity for reasoning. To facilitate fine-grained and reliable evaluation, we also pair each evaluation prompt with a checklist that specifies individual \\textit{yes/no} questions to assess each intended element independently. In statistics, our benchmark comprises $1,080$ challenging prompts and around $13,500$ checklist questions. Experiments across 28 current T2I models reveal that their composition capability still remains limited in high compositional scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts.", "tldr": "", "keywords": ["Text-to-Image Generation", "Reasoning", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ede8fc9ad614b4ae2a7fda2df146660ece4c203.pdf", "supplementary_material": "/attachment/f8ae1c4fa747d000bd5e65aa8fb9d8181fd8bc5a.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce T2I-CoReBench, a benchmark for evaluating both composition and reasoning capabilities of text-to-image (T2I) generation models with 12 dimensions. The benchmark comprises 1,080 prompts and ~13,500 checklist visual questions, enabling fine-grained evaluation with an MLLM-based yes/no answerer. Experiments on 28 T2I models (diffusion, autoregressive, and unified architectures) reveal that while composition is steadily improving, reasoning remains the main performance bottleneck. Further, prompt rewriting shows some effectiveness in bridging this gap but remains limited in scenarios requiring deeper visual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provides the first benchmark that evaluate both composition and reasoning capabilities of T2I models.\n\n2. The main findings (reasoning lags behind composition & prompt rewriting helps but still there’s gap) is well supported with large experiments (e.g., table 3) and valuable for future research direction."}, "weaknesses": {"value": "1. **Evaluation method.** In L143-145, the authors mentioned `We propose an automatic checklist-based evaluation protocol ... individual yes/no questions ... allows fine-grained and reliable assessment' as a second contribution. However, such automatic checklist-based evaluation protocol (i.e., question generation followed by question answering) was already extensively studied in previous works [e.g., A, B, C, D], but none of them have been cited or discussed. I don't think authors can claim evaluation methodological novelty, which is separate from benchmark construction. The authors should cite these works and clarify their contributions.\n\n- [A] Hu et al., TIFA: Accurate and Interpretable Text‑to‑Image Faithfulness Evaluation with Question Answering. ICCV 2023\n- [B] Yarom et al., What You See is What You Read? Improving Text‑Image Alignment Evaluation. NeurIPS 2023.\n- [C] Cho et al., Visual Programming for Text‑to‑Image Generation and Evaluation. NeurIPS 2023\n- [D] Cho et al. Davidsonian Scene Graph: Improving Reliability in Fine‑grained Evaluation for Text‑to‑Image Generation. ICLR 2024."}, "questions": {"value": "1. This is minor point, but I feel the title is hard to understand. I didn't understand what title meant before reading the paper, and didn't expect to read text-to-image evaluation benchmark paper at all. Now I guess that authors meant composition and reasoning by 'set the stage' and 'direct the play', but still, such connection wouldn't be clear for many new readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M3aCJz2zVE", "forum": "iqAFhWistW", "replyto": "iqAFhWistW", "signatures": ["ICLR.cc/2026/Conference/Submission5454/Reviewer_DGHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5454/Reviewer_DGHz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451029994, "cdate": 1761451029994, "tmdate": 1762918071833, "mdate": 1762918071833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces T2I-CoReBench, a Composition and Reasoning Benchmark for systematic evaluation of T2I models. T2I-CoReBench addresses research gaps in comprehensiveness and complexity, evaluating both composition and reasoning within a broader taxonomy. The dataset is constructed by an automatic pipeline, using LRMs to generate data points given instructions, and further examined by humans. Each data point is accompanied by a checklist, utilizing Google Gemini Flash 2.5 as the evaluation protocol to assess checklist accomplishment and provide a quality evaluation score. Comprehensive experiments evaluate 28 T2I models with different architectures and find that most models fall short on overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Solid contribution in constructing a comprehensive and challenging benchmark on T2I. T2I-CoReBench comprehensively covers composition and reasoning tasks in a detailed taxonomy. \n2. Broad evaluation of different models offers great insights: The paper benchmarks 28 T2I models, covering open-source and closed-source models, as well as different architectures (Diffusion, Autoregressive, Unified). This makes the results comprehensive and convincing. Insights on \"Prompt Rewriting\" in Section 4.3 are intriguing.\n3. Good Presentation Quality."}, "weaknesses": {"value": "1. Evaluation relies on a single indicator, which cannot comprehensively measure the quality of T2I: There is only one indicator, the achievement rate of checklist questions, which cannot measure such as the overall rationality and stylization of the pictures. Given the examples, it also mainly examines item appearance in images, regardless of model hallucinations, etc.\n2. Both the composition and reasoning tasks have their own previous benchmarks. Merely merging the two tasks into a more comprehensive benchmark limits their necessity."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "anmuh8XhZS", "forum": "iqAFhWistW", "replyto": "iqAFhWistW", "signatures": ["ICLR.cc/2026/Conference/Submission5454/Reviewer_jqux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5454/Reviewer_jqux"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636038158, "cdate": 1761636038158, "tmdate": 1762918071554, "mdate": 1762918071554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents T2I-CoReBench, a benchmark for evaluating the compositional and reasoning abilities of text-to-image (T2I) models. The benchmark dissects model performance across multiple reasoning dimensions—logical, behavioral, hypothetical, and generalization reasoning—along with compositional skills such as multi-instance, multi-attribute, and text rendering. The authors argue that current T2I models excel in simple composition but struggle with structured reasoning that connects textual causality and visual outcomes. Comprehensive experiments compare state-of-the-art open- and closed-source models, uncovering systematic reasoning failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors categorize reasoning into fine-grained dimensions (e.g., logical, behavioral, hypothetical, abductive), offering a structured framework that is more interpretable than single-score benchmarks.\n\n2. The inclusion of over a dozen models, spanning proprietary and open-source variants, makes the findings broadly representative and informative."}, "weaknesses": {"value": "1. Some reasoning categories overlap (e.g., behavioral vs. hypothetical), raising questions about independence and orthogonality among evaluation axes."}, "questions": {"value": "1. Is there any bias in the prompt generated synthetically? \n2. Is there any distinction between the generated images of open and closed source models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u4wGoRHEPS", "forum": "iqAFhWistW", "replyto": "iqAFhWistW", "signatures": ["ICLR.cc/2026/Conference/Submission5454/Reviewer_UbDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5454/Reviewer_UbDT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967665313, "cdate": 1761967665313, "tmdate": 1762918071229, "mdate": 1762918071229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces T2I-COREBENCH, a high-density, checklist-based benchmark to jointly test composition (multi-instance, multi-attribute, multi-relation, text rendering) and reasoning (eight subtypes derived from deductive/inductive/abductive patterns) for text-to-image models. Evaluating 28 recent models, the authors find a clear gap: models can “paint” the specified elements but struggle to “think” through multi-step or implicit requirements, especially in dense scenes. Reasoning, not composition, is the current bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated split between composition vs. reasoning, with 12 concrete dimensions.\n- Dense prompts + yes/no checklists enable fine-grained, scalable evaluation.\n- Broad study on 28 models gives useful community signals about where T2I still fails."}, "weaknesses": {"value": "- Evaluator dependence and possible bias. The pipeline crucially depends on an MLLM to judge success. The paper would be stronger with a multi-evaluator or human-heavy subset to rule out evaluator-specific artifacts ."}, "questions": {"value": "- How did you de-correlate prompt/checklist style from the evaluator to avoid agreement-by-style?\n- Did you experiment with soft scores or multiple MLLM votes, and if not, why stick to strict yes/no, which can make dense prompts disproportionately harsh?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PwsL8HnnRy", "forum": "iqAFhWistW", "replyto": "iqAFhWistW", "signatures": ["ICLR.cc/2026/Conference/Submission5454/Reviewer_pbo7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5454/Reviewer_pbo7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974129389, "cdate": 1761974129389, "tmdate": 1762918070659, "mdate": 1762918070659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Paper Revision"}, "comment": {"value": "We gratefully thank all the reviewers for their valuable and constructive comments. We are encouraged that they find our motivation to split composition and reasoning clear and well-motivated (Reviewer pbo7), our taxonomy comprehensive and fine-grained (Reviewer UbDT, Reviewer jqux), and our checklist-based evaluation fine-grained and interpretable (Reviewer pbo7). We are also glad that they acknowledge our extensive experiments on 28 models provide useful community insights (Reviewer pbo7, Reviewer jqux) and that our main findings are well-supported and valuable for future research directions (Reviewer DGHz).\n\nWe address the concerns and questions in detail below. According to these comments, we have also improved our manuscript and summarized the main changes as follows:\n\n1. **Supplementary experiments with multiple evaluators (Appendix C.2 & C.3, to Reviewer pbo7):** We added evaluation results using two more open-source MLLMs (Qwen2.5-VL-72B and Qwen3-VL-30B-Thinking) and a multi-evaluator fusion strategy, demonstrating that the current evaluation trends remain robust across different evaluators.\n2. **More discussion on orthogonality over evaluation dimensions (Section 3.1 & Appendix A.4, to Reviewer UbDT):** We highlighted the orthogonality of our evaluation dimensions in the main text from both theoretical and practical perspectives.\n3. **Clarification of contributions and related work (Section 1 & 3.2, to Reviewer DGHz):** We revised the phrasing regarding the automatic checklist-based evaluation protocol to properly cite prior works (e.g., TIFA, Davidsonian Scene Graph, etc.) and clarified that our contribution focuses on the construction of our benchmark with the large-scale, human-verified checklist-based assessment."}}, "id": "dc6sEOnljv", "forum": "iqAFhWistW", "replyto": "iqAFhWistW", "signatures": ["ICLR.cc/2026/Conference/Submission5454/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5454/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission5454/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763710554630, "cdate": 1763710554630, "tmdate": 1763710554630, "mdate": 1763710554630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}