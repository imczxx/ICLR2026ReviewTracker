{"id": "46WjLHatpw", "number": 20032, "cdate": 1758301720133, "mdate": 1759897005341, "content": {"title": "Stochastic activations", "abstract": "We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n\n(1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the \\relu activation function. \n\n(2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.", "tldr": "We introduce stochastic activations, where we randomly select between several non-linear function in the feed-forward layer of a large language model.", "keywords": ["activation functions", "sparsity", "generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a7df9890c04c6c6d1fff55f1267bab94092b27a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes training LLMs with stochastic activations to strike a balance between performance sparsity/efficiency. The performance seems promising but it is unclear how much practical benefit this can have on GPUs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Ideas and experiments are clear and easy to understand.\n2. Training with stochastic activations is shown to have performance on par with SiLU activations, resolving a major weakness of ReLUs.\n3. Experiments are thorough."}, "weaknesses": {"value": "1. Practical use case for this method is unclear since GPU acceleration is not demonstrated.\n2. The generalizability is unclear. Will this setup work with pairs of activations that are less similar (e.g., ReLU + tanh)? What about non-ReLU sparse variants (e.g., soft/hard thresholding)?\n3. Lack of structured sparsity baselines."}, "questions": {"value": "1. Line 143: Unremoved comment\n2. Hard to see what is going on at the tail in figure 4. Could you shrink the y-axis range?\n3. Can you comment on when/how your method is preferred over post-training sparsity (e.g., [1]) and inference-time sparsity algorithms (e.g. [2,3])?\n\n[1] Zhang, et al., MoEfication: Transformer Feed-forward Layers are Mixtures of Experts, 2021.\n[2] Dong, et al., Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation, 2024.\n[3] Lee, et al., Cats: Contextually-aware thresholding for sparsity in large language models, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5p3U5GEPyF", "forum": "46WjLHatpw", "replyto": "46WjLHatpw", "signatures": ["ICLR.cc/2026/Conference/Submission20032/Reviewer_q6x3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20032/Reviewer_q6x3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419438427, "cdate": 1761419438427, "tmdate": 1762932931919, "mdate": 1762932931919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two distinct methods for training with multiple activation functions. The first method, Swi+FT, is a deterministic two-stage process: the model is first trained with SiLU, then the final portion of training is completed with ReLU. The goal is to achieve the high quality of a SiLU-trained model while retaining the sparsity and inference-speed benefits of ReLU, which the authors demonstrate on CPUs. The second method, StochA, trains using a stochastic activation that randomly interpolates between ReLU and SiLU (controlled by a probability p). This is proposed as a novel, inference-time tool to control output diversity, similar to temperature sampling. Both methods achieve performance similar to SiLU, with Swi+FT being 1.65x faster on CPUs and StochA providing activation swapability as a tool for increasing generation diversity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper successfully introduces and validates two distinct, novel methods for training with multiple activation functions. \n- The primary method, Swi+FT, is well-motivated and its \"best of both worlds\" claim is supported by the data. The results clearly show it produces a final ReLU model with the high test-set quality of a SiLU-trained model, while retaining ReLU's sparsity for a 1.65x speedup on CPUs.\n- The experiments are robust, validating these findings across two different model scales (1.5B and 3B) on various downstream tasks.\n- The results show a positive scaling trend: the quality benefit of Swi+FT over the ReLU baseline increases with the larger model, suggesting its value may grow with model size.\n- The second method, StochA, is also shown to be effective, achieving SiLU-level performance while providing a new, functional lever for controlling output diversity at inference time."}, "weaknesses": {"value": "- W1. The 1.65x speedup from the Swi+FT method is demonstrated exclusively on CPUs. This limits the work's impact, as large-scale models are predominantly deployed on GPUs/TPUs where the benefits of activation sparsity are often different or less pronounced. For true \"edge device\" applications, this \"decent\" speedup is likely insufficient, as activation sparsity is only one part of a much larger optimization problem.\n- W2. The diversity claim for Stocha is not convincingly supported or motivated. The diversity analysis fails to compare StochA against the most common and simple tools we already have, namely temperature and nucleus (top-p) sampling. It is unclear if StochA offers any real benefit over these established methods. \n    - The evaluation also lacks qualitative examples or application to a task where controlled diversity is critical (e.g., RL exploration), making it hard to judge the usefulness of this new diversity lever.\n     - Finally, the paper is missing comparisons to highly relevant prior work. For the StochA method, there is no comparison to other stochastic activation functions (like ASH).\n- W3. More broadly, the paper doesn't make a strong case for its novelty in a somewhat saturated field, and it's unclear if this complex approach is truly superior to simpler, established adaptive activation functions."}, "questions": {"value": "- The paper's motivation is weakened by the lack of clear justification for the complexity of its novel activation function compared to simpler stochastic regularization techniques. The authors do not discuss why this approach is necessary or superior to existing stochastic dropout methods (like Layer Dropout) which offer clearer, multi-faceted benefits in practice (e.g., dynamic model sizing or uncertainty estimation).\n- To validate the quality claims of the $\\text{Swi+FT}$ method, could the authors confirm that the total training compute (total number of iterations) was held constant or until convergence across the $\\text{SiLU}$, $\\text{ReLU}$, and $\\text{Swi+FT}$ baselines?\n- What are the benefits of $\\text{StochA}$ compared to the most widely used diversity levers (e.g., temperature sampling and top-p sampling? Evaluating on a use case where controlled diversity is critical as in RL for LLMs would be helpful. \n-  The practical benefits of the StochA method need stronger justification. The authors should explicitly discuss and quantify how the benefits of StochA (controlled diversity) compare with existing diversity methods such as temperature and top-p sampling.\n- The Related Work section contains an editorial comment that should be removed for the final version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4TTYUuTmqT", "forum": "46WjLHatpw", "replyto": "46WjLHatpw", "signatures": ["ICLR.cc/2026/Conference/Submission20032/Reviewer_rJBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20032/Reviewer_rJBi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546447612, "cdate": 1761546447612, "tmdate": 1762932930855, "mdate": 1762932930855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two new methods to leverage the expressivity of SiLU while, at the same time, the sparsity and computational efficiency of ReLU at inference time. The first method Swi+FT trains on SiLU, then adapts the network to ReLU for the last 5-10% of training. The second method StochA samples the activation as Bernoulli(p) during training or test time. The product is a ReLU-based model at inference time that outperforms ReLU-based training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "I enjoyed this paper for several reasons. The approach seems **novel** (I may be wrong, I'm not an expert in this field), yet **simple**. The paper is a largely empirical one: the authors did **extensive experiments** which validate their main thesis that SiLU's expressivity can be exploited during training to make better models; then ReLU's sparsity can be exploited at inference time to make them fast."}, "weaknesses": {"value": "The reason my score is not higher is for the following reasons. In its current state, the paper is quite difficult to read and some evaluation points not justified. If both points are resolved (should be easy to fix), I will increase my score to \"accept\". \n\n1. The main area where the paper can be improved is the presentation. Overall, there are many experimental results, which makes the takeaways get lost. For instance, Table 2 is extremely difficult to parse. Please distill Table 2 to the key results most salient for the paper, add **bolding** for the best performances, and move the rest to the appendix. In general, visually highlighting the takeaways for each paragraph of subsection is necessary given the density of the results.\n\n2. Diversity of generations ablations. Why does the F1 score signal higher diversity? I would have expected to see an actual text diversity metric, e.g., type-token ratio, entropy, etc."}, "questions": {"value": "l271 Typo: The following only applies to the StochA strategy: we evaluate the performance when if leverage the randomness at test time"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jjoTJqxc6J", "forum": "46WjLHatpw", "replyto": "46WjLHatpw", "signatures": ["ICLR.cc/2026/Conference/Submission20032/Reviewer_bZvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20032/Reviewer_bZvR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077474408, "cdate": 1762077474408, "tmdate": 1762932930573, "mdate": 1762932930573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To balance the sparsity and efficiency of activation functions during LLM training, this paper introduces stochastic activations that randomly select ReLU and SILU. Specifically, in experiments, the first method, Swi+FT, use SILU at 10% of the training steps and ReLU for the rest steps; the second method, STOCHA, randomly selects ReLU and SILU during training and uses ReLU for inference. Experimental results show the comparable performance and reduced inference times of the proposed stochastic activations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The proposed stochastic activation is simple while addressing the sparsity and computational efficiency of ReLU.\n3. The experimental results show comparable performance of stochastic activations and reduced inference time."}, "weaknesses": {"value": "1. This paper only proposes to switch activation functions between ReLU and SILU for LLM pretraining. An in-depth analysis should be conducted: why switching the activation function during pretraining and randomly selct activation functions both retain comparable performance? How models behave differently using a single or combination of these activation functions? Can we have some theoretical explanation of stochastic activations?\n2. The experiments are conducted on pretraining small-scale transformers (1.5B and 3B), which may not generalize to large-scale models. Why not apply this method to post-training?\n3. As shown in Figure 5, the performance of stochastic activations only outperform SILU on specific range of $\\alpha$ in 1.5B model and always underperforms SILU in 3B model. It seems the performance is fragile because it depends on dedicated selection of hyperparameter and behaves differently on different models."}, "questions": {"value": "See Questions mentioned in the point 1 and 2 in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4eHGTSj1Gg", "forum": "46WjLHatpw", "replyto": "46WjLHatpw", "signatures": ["ICLR.cc/2026/Conference/Submission20032/Reviewer_YY3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20032/Reviewer_YY3K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762266101475, "cdate": 1762266101475, "tmdate": 1762932929758, "mdate": 1762932929758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}