{"id": "4ZGvMY9M4l", "number": 8888, "cdate": 1758101313969, "mdate": 1763361852153, "content": {"title": "FedMH: Federated Learning with Multi-Task Head for Heterogeneous Models in Offline Signature Verification", "abstract": "Offline handwritten signature verification is a critical biometric authentication technology widely used in high-risk domains such as finance and law.\nHowever, data silos constrain models to train solely on local, limited, and imbalanced data, typically resulting in overfitting to majority classes.\nFederated learning is one of the solutions that addresses this issue by enabling collaborative training while preserving data privacy.\nHowever, existing federated learning methods face three key challenges in the scenario of offline handwritten signature verification: the scarcity of local data and class imbalance, the need for a dual-task head, and heterogeneity constraints in the model aggregation process.\nTo tackle these issues, we propose FedMH, a novel federated learning method based on a multi-task head strategy, which supports heterogeneous model environments. Specifically, FedMH comprises three core components including (1) an adaptive data augmentation strategy that enhances data diversity and improves learning in minority classes by targeting few-shot classes in local single-user genuine and forged signature subsets, (2) a dual-task head collaboration mechanism that dynamically guides parameter updates to foster synergy between tasks, and (3) a gradient optimization method employing linear probing in parameter space and Pareto improvement criteria to enable efficient knowledge aggregation across heterogeneous models. We also provide a theoretical convergence proof for FedMH to ensure its reliability and stability.\nComprehensive experiments validate the effectiveness of the proposed FedMH on three benchmark handwritten signature datasets. The proposed FedMH achieves state-of-the-art performance compared to heterogeneous federated baseline methods and solves the long-standing problem of multi-task head collaboration. At the same time, when faced with unfamiliar datasets, FedMH also demonstrates stronger generalization ability than the baseline methods.", "tldr": "", "keywords": ["Offline handwritten signature authentication", "Federated Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd3cdd6279860457536e6efd5ccf36ae7e4d90f6.pdf", "supplementary_material": "/attachment/eccabbd3f46a2944724cac27631b1b4bb1f53dd9.zip"}, "replies": [{"content": {"summary": {"value": "**FedMH** proposes a novel **heterogeneous federated learning framework** for **offline handwritten signature verification**, aiming to achieve high-accuracy authentication under privacy constraints, data imbalance, and task heterogeneity. The method introduces a **multi-task head collaboration mechanism** that jointly optimizes user identification and forgery detection, enabling effective knowledge sharing and generalization across heterogeneous model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a Pareto-based gradient optimization framework that provides a theoretically grounded means of balancing competing objectives in multi-task federated learning. This design effectively mitigates gradient conflicts and enhances optimization stability across heterogeneous clients.\n2. The authors present a relatively comprehensive non-convex convergence proof, establishing theoretical reliability under federated and heterogeneous settings. Such formal analysis is rarely provided in applied FL tasks and enhances the paper’s scientific rigor."}, "weaknesses": {"value": "1. For each client k, data from a specific user a are divided into *genuine* and *forged* signatures. As mentioned around line 170, genuine samples typically far outnumber forged ones. In the following paragraph, the minority-class enhancement is implemented via **oversampling** until the sample count matches that of the majority class. This approach may lead to **severe overfitting of the minority class**, particularly detrimental to the generalization of the downstream **forgery-detection** task.\n2. According to the authors, the number of samples may vary substantially across users (e.g., user a vs. user b). It is unclear whether this imbalance across users was explicitly considered during the data-augmentation process.\n3. The proposed data-augmentation module appears to rely on **standard transformations**—as shown in Appendix A, the augmentation functions are mostly existing operations combined into a random transformation sequence. According to the ablation results (Table 3), the DA module contributes the most to the overall performance, whereas the other two modules show **marginal improvements**.\n4. It is unclear whether the **baseline methods** in the main experiments were trained with the **same data-augmentation setup** for fair comparison.\n5. The experiments could be made more convincing by testing with a **larger number of clients** (e.g., 20 or 50) to evaluate scalability and robustness.\n6. The authors may consider discussing **related approaches that perform forgery detection prior to user identification**, and clarifying how the proposed **dual-task mechanism** offers advantages over such sequential designs."}, "questions": {"value": "I may raise the score if the authors clarify/resolve the above concerns 1, 2, 3, 4, 5, and 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uBkE93Oego", "forum": "4ZGvMY9M4l", "replyto": "4ZGvMY9M4l", "signatures": ["ICLR.cc/2026/Conference/Submission8888/Reviewer_9tRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8888/Reviewer_9tRX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707774198, "cdate": 1761707774198, "tmdate": 1762920645998, "mdate": 1762920645998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedMH, a novel heterogeneous federated learning framework tailored for offline handwritten signature verification, addressing three major challenges in this domain: (1) local data scarcity and severe class imbalance between genuine and forged signatures; (2) the necessity of handling two interdependent but distinct tasks—user classification and forgery detection; (3) the difficulty of aggregating heterogeneous models in FL environments. The main contributions of FedMH include: (1) a  Client-Adaptive Data Augmentation (DA) technique; (2) a Dual-Task Head Mechanism (DH); and (3) an Efficient Pareto Gradient Optimization (GO) method. Experiments on GPDS-10000 datasets show that FedMH achieves state-of-the-art performance, outperforming relative heterogeneous federated learning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a well-motivated multi-task head design that simultaneously handles user identification and forgery detection—two interrelated but conflicting objectives in signature verification. \n\n2.\tFedMH is explicitly designed to support heterogeneous client models. Its gradient optimization protocol allows efficient and stable aggregation across diverse architectures. \n\n3.\tThe paper provides both rigorous convergence proofs and extensive experiments on benchmark datasets, demonstrating clear superiority over several state-of-the-art baselines."}, "weaknesses": {"value": "1.\tThe oversampling-based augmentation method, presented as one of the innovations of this paper, lacks sufficient originality. In the ablation study, DA includes both oversampling and other conventional augmentation techniques, making it unclear which specific component contributes to the improvement shown in Tab. 3.\n\n2.\tDual-task learning like this is relatively common in signature verification, and gradient-based dynamic loss weighting has been widely adopted in deep learning tasks. Thus, this contribution is not sufficiently novel. Moreover, the paper lacks an ablation experiment to demonstrate how much advantage the dynamic weighting method provides compared with a static weighting strategy.\n\n3.\tIn Eq.(8), the definition of the superscript i for g is inconsistent with its earlier usage in the text.\n\n4.\tSince the gradient set is recalculated in each round, it would be helpful to present the percentage curve of how often clients select S_sync, S_boundary, or S_bias over training rounds. This visualization would help verify the claimed superiority of the proposed GO scheme.\n\n5.\tThe paper does not employ recent state-of-the-art models in offline signature verification task. (e.g., HTCSigNet[1], DetailSemNet[2]).\n\n6.\tIn the ablation study, when DA is included, the effect of DH is not evident and may even be slightly negative (see Tab. 3, rows 3 and 4: the mean improvement with DH is only around 0.0x%, while the variance increases by a similar magnitude). This result fails to convincingly demonstrate the actual contribution of DH to the overall performance of the proposed model.\n\n[1] Zheng L, et al. HTCSigNet: A Hybrid Transformer and Convolution Signature Network for offline signature verification. Pattern Recognition, 2025. \n[2] Shih M C, et al. DetailSemNet: Elevating Signature Verification Through Detail-Semantic Integration. European Conference on Computer Vision, 2024."}, "questions": {"value": "Pls see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hj4GH2huls", "forum": "4ZGvMY9M4l", "replyto": "4ZGvMY9M4l", "signatures": ["ICLR.cc/2026/Conference/Submission8888/Reviewer_Gxsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8888/Reviewer_Gxsr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866961683, "cdate": 1761866961683, "tmdate": 1762920645408, "mdate": 1762920645408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We will improve our work according to the comments as soon as possible and reply."}, "comment": {"value": "Dear reviewers and area chairs,\n\nThank you for your professional and insightful comments, which have benefited us a great deal. We will improve the content of the paper as soon as possible in accordance with your suggestions and answer your questions one by one."}}, "id": "QtwIO8gxJQ", "forum": "4ZGvMY9M4l", "replyto": "4ZGvMY9M4l", "signatures": ["ICLR.cc/2026/Conference/Submission8888/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8888/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8888/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763017886071, "cdate": 1763017886071, "tmdate": 1763017886071, "mdate": 1763017886071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedMH, a new federated learning (FL) framework designed for offline handwritten signature verification, which faces data isolation, class imbalance, and model heterogeneity across clients.\nFedMH integrates three innovations:\n\t1.\tClient-adaptive data augmentation (DA) — selectively oversamples and transforms minority (forged) samples to balance local datasets.\n\t2.\tDual-task head mechanism (DH) — models user identification and forgery detection jointly under a Pareto-optimal multi-task formulation, dynamically adjusting loss weights between the two heads.\n\t3.\tPareto gradient optimization (GO) — aggregates gradients across heterogeneous clients via a Pareto improvement criterion, ensuring stable and efficient global updates without requiring model homogeneity.\n\nThe method includes a theoretical convergence proof and extensive experiments on GPDS-10000, Bengali, and Hindi signature datasets. FedMH reportedly achieves state-of-the-art performance in both homogeneous and heterogeneous FL setups, with stronger cross-dataset generalization and lower equal error rate (EER) than baseline methods such as FedProto, FedGH, FedGen, and FedTGP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\t•\tThe idea of coupling multi-task heads (classification + verification) within federated learning using Pareto-stationary optimization is both novel and meaningful.\n\t•\tThe gradient-based Pareto aggregation bridges a gap between multi-objective optimization and heterogeneous FL, offering a creative and technically sound extension of existing frameworks.\n\t•\tThe paper reframes signature verification (traditionally single-task) into a dual-task federated optimization problem — a fresh problem formulation that broadens FL’s applicability to biometric domains.\n\nQuality\n\tThe methodology is rigorous and well-supported.\n\t•\tClear derivations for adaptive λ-weighting between task heads (Eq. 7).\n\t•\tA convergence theorem (Theorem 1) under standard smoothness and bounded gradient assumptions with a formal proof in Appendix C.\n\tThe experimental evaluation is strong:\n\t•\tCovers multiple datasets and both homogeneous and heterogeneous architectures (SigNet, OctConvNet, ViT).\n\t•\tUses standard signature metrics (AUC, FAR, FRR, EER).\n\t•\tThe reproducibility statement and open anonymous code repository further enhance credibility.\n\nClarity\n\t•\tThe mathematical sections are explicit, and appendices provide detailed derivations and algorithms.\n\t•\tThe motivation is logically presented: from data imbalance to dual-task conflict to gradient heterogeneity.\n\nSignificance\n\t•\tFederated handwritten signature verification is a practical and underexplored domain with strong privacy requirements.\n\t•\tFedMH provides a generalizable paradigm for federated multi-task learning, not limited to signatures — potentially applicable to other privacy-sensitive biometric or multi-label problems."}, "weaknesses": {"value": "Comparative novelty relative to multi-objective FL works:\n\t•\tWhile Pareto-based optimization is novel in this context, related works such as MOFL (Hu et al., 2022) and FedMOO have explored similar gradient alignment ideas.\n\t•\tThe paper could more explicitly position FedMH’s contribution relative to such multi-objective FL methods and justify why its Pareto update is distinct or superior.\n\t2.\tComputational complexity and communication overhead:\n\t•\tThe paper does not report time or communication cost, which may be substantial in large-client settings.\n\t3.\tLimited scope of datasets:\n\t•\tAlthough GPDS, Bengali, and Hindi are standard, they represent a narrow range of handwriting and cultural variability. Testing on other large-scale biometric datasets (e.g., CEDAR, MCYT) would strengthen generalization claims.\n\t\n\t4.\tInterpretability of Pareto optimization:\n\t•\tThe intuition behind Pareto-front dynamics could be more accessible to non-specialists; Figure 1d and the equations are clear but somewhat dense without geometric interpretation.\n\t5.\tMinor clarity issues:\n\t•\tThe conclusion admits the method “needs improvement in convergence speed,” but no empirical evidence of slowdown is shown."}, "questions": {"value": "1. On Pareto gradient optimization efficiency:\n\t•\tHow does the computational cost of the Pareto search (Eq. 10–13) scale with client number n?\n\t•\tCould the authors report average per-round latency or communication load compared to FedAvg or FedProto?\n\t2.\tOn λ adaptation behavior:\n\t•\tDoes λ* converge toward a stable value, oscillate, or correlate with task difficulty?\n\t•\tCould the data augmentation module generalize beyond signature verification to other image-based FL tasks?\n\t3.\tOn theoretical assumptions:\n\t•\tThe convergence proof assumes β-smoothness and bounded gradients. How realistic are these assumptions for deep CNN/Transformer backbones in practice?\n\t4.\tReproducibility details:\n\t•\tThe paper provides a link to code, but not the license or full dataset preparation scripts. For the camera-ready version, include explicit instructions and hyperparameter settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rGeLR3O6gk", "forum": "4ZGvMY9M4l", "replyto": "4ZGvMY9M4l", "signatures": ["ICLR.cc/2026/Conference/Submission8888/Reviewer_uqhD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8888/Reviewer_uqhD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970781871, "cdate": 1761970781871, "tmdate": 1762920644775, "mdate": 1762920644775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes federated learning based on a multi-task head strategy (FedMH) method for offline handwritten signature verification under limited data, class imbalance, and heterogeneous client models. It has 3 main components: (1) an adaptive data augmentation module to address intra-client class imbalance by targeting few-shot classes in local single-user genuine and forged signature subsets;  (2) a dual-task head mechanism that jointly optimizes user classification and forgery detection; and  (3) an efficient Pareto gradient optimization protocol employing linear probing in the parameter space for efficient knowledge aggregation across heterogeneous models. The proposed FedMH method is validated on GPDS, Bengali, and Hindi signature verification datasets, and results show it can outperforms SOTA heterogeneous FL baselines, e.g., FedGH, FedTGP, FedProto, in both homogeneous and heterogeneous setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Overall, the paper is clearly written, well organized, and easy to follow. FedMH formulates federated offline signature verification as an iterative process with Pareto-stationary solutions for multi-task heads.  The dual-task head is employed to update the model on local clients. It aligns task-specific gradients, and guides the model toward a Pareto stationary solution that balances the performance of both tasks.\n+ The application and challenges of federated learning in offline handwritten signature verification is well justified by the authors. \n+ A convergence proof is provided, showing that FedMH converges to a Pareto-stationary point under standard assumptions. This ensures the reliability and stability of FedMH. \n+ Experiments on GPDS, Bengali, and Hindi datasets validate the effectiveness of FedMH. It achieves SOTA performance compared to heterogeneous FL baseline methods. When faced with unfamiliar datasets, FedMH also demonstrates better performance than baseline methods. The ablation studies show the benefits from each module, although the client-adaptive minority class data augmentation (to balance/enhance local data) provides the largest improvement. \n+ The Appendix has additional information on FedMH properties, experimental settings., hyper-parameter settings, ablation studies, analysis of computational complexity, feature visualizations, description of CASE components, analysis of novelty, and experimental results that help support the paper."}, "weaknesses": {"value": "- The methodology (Section 3) is dense and difficult to follow. Although FedMH represents an effective method to addresses heterogeneity and multi-task coupling in offline signature verification, it extends on existing methods (augmentation, Pareto optimization, dual-head training).\n- The experimental validation raises some concerns. The results in Table 1 and 2 should be replicated multiple times using some cross-validation procedures.  The authors should consider using performance measures based on the precision-recall curve (as opposed to the ROC curve) to observe the impact on performance of class imbalance.\n- The choice of some loss terms and algorithmic components (e.g., thresholds and batch size) appear heuristic.  Their sensitivity should be analyzed empirically with more ablations studies.\n- Limited discussion and interpretation of experimental results in Section 4.  This paper should also contain an experimental analysis of time and memory complexity.  Computational overhead appears higher than SOTA methods yet is not analyzed, raising concerns about scalability.      \n- Their code is not made available, so there is a concern that the results in this paper would be difficult for a reader to reproduce."}, "questions": {"value": "See my comments in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BjKyvnCK2m", "forum": "4ZGvMY9M4l", "replyto": "4ZGvMY9M4l", "signatures": ["ICLR.cc/2026/Conference/Submission8888/Reviewer_vypZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8888/Reviewer_vypZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032211659, "cdate": 1762032211659, "tmdate": 1762920644292, "mdate": 1762920644292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}