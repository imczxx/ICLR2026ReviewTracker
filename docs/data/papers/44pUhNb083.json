{"id": "44pUhNb083", "number": 9023, "cdate": 1758107566312, "mdate": 1759897747710, "content": {"title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "abstract": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive capabilities but face significant limitations such as constrained exploration strategies and a severe execution bottleneck. Exploration is hindered by one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS) approaches that fail to recombine strong partial solutions. The execution bottleneck arises from lengthy code validation cycles that stifle iterative refinement. To overcome these challenges, we introduce KompeteAI, a novel AutoML framework with dynamic solution space exploration. Unlike previous MCTS methods that treat ideas in isolation, KompeteAI introduces a merging stage that composes top candidates. We further expand the hypothesis space by integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also addresses the execution bottleneck via a predictive scoring model and an accelerated debugging method, assessing solution potential using early-stage metrics to avoid costly full-code execution. This approach accelerates pipeline evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent, AIDE, and ML-Master) by an average of 3\\% on the primary AutoML benchmark, MLE-Bench. Additionally, we propose Kompete-bench to address limitations in MLE-Bench, where KompeteAI also achieves state-of-the-art results.", "tldr": "An LLM-based AutoML system that accelerates search via candidate merging, RAG, and code performance prediction. Result:  SOTA level on MLE-Bench among open source systems using a comparable LLM backend.", "keywords": ["AutoML", "LLM", "Multi-agent system", "MCTS", "Benchmark"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a399577a4fb41400a0e86f0a4ac5ca7f27cff42b.pdf", "supplementary_material": "/attachment/b49e6a92382b2364d7a0b4203dbaf7a46b3b0680.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces KompeteAI, a multi-agent AutoML system for end-to-end ML pipeline generation. It tackles two major challenges faced by LLM-based AutoML frameworks—limited exploration and slow execution. KompeteAI employs two core operators, adding and merging, to dynamically expand and recombine strong pipeline components while leveraging Retrieval-Augmented Generation (RAG) to infuse real-world knowledge from Kaggle and arXiv. To mitigate execution bottlenecks, the system integrates a predictive scoring model for early performance estimation and an accelerated debugging paradigm using simplified code. Experiments on MLE-Bench and a newly proposed Kompete-bench show consistent state-of-the-art performance, with a 6.9× speedup and 3% higher accuracy over leading baselines like RD-Agent and ML-Master. The paper also highlights deficiencies in existing benchmarks and offers Kompete-bench as a fairer alternative reflecting real-world competition dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a well-structured and technically detailed framework that integrates algorithmic tree search with multi-agent coordination. The design of adding and merging operators is conceptually sound, addressing a real gap in exploration diversity and solution recombination that prior MCTS-based systems overlook.\n- The introduction of a predictive scoring model and fast debugging loop represents a practical advance for efficiency; the ablation results clearly show how these components contribute to scaling search depth within a fixed runtime.\n- The construction of Kompete-bench is another strong contribution. By identifying concrete flaws in MLE-Bench (data partition bias, leaderboard misalignment) and providing a curated, mixed-era benchmark with real Kaggle evaluation metrics, the authors make a valuable step toward more reproducible and realistic assessment of AutoML agents."}, "weaknesses": {"value": "- Despite its solid engineering design, the overall conceptual novelty appears incremental. The core components—RAG integration, performance prediction, and tree-guided search—are thoughtful extensions of existing paradigms rather than fundamentally new algorithmic ideas. The contribution thus lies more in systematic synthesis and refinement than in conceptual breakthroughs.\n- Although the experiments are extensive, they lack deeper behavioral analysis of the system. For instance, it remains unclear how merging strategies evolve during search, or under what conditions the predictive scoring model might introduce bias or mislead exploration. The evaluation primarily reports end metrics without qualitative insights into pipeline diversity, decision dynamics, or reasoning quality.\n- The benchmark section, while valuable, does not clearly describe how the Contemporary subset was selected or how it concretely mitigates the evaluation bias inherent in MLE-Bench.\n- The writing quality could be improved, as several sections contain minor typos and stylistic inconsistencies that slightly detract from readability."}, "questions": {"value": "See #Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j8xthAW4e0", "forum": "44pUhNb083", "replyto": "44pUhNb083", "signatures": ["ICLR.cc/2026/Conference/Submission9023/Reviewer_Ahze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9023/Reviewer_Ahze"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451487921, "cdate": 1761451487921, "tmdate": 1762920746215, "mdate": 1762920746215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a system called KompeteAI, which is an autonomous multi-agent framework aimed at generating end-to-end machine-learning pipelines in a self-driving manner. The key features described include: A stage-decomposed multi-agent architecture, where agents handle tasks such as data ingestion, feature engineering, model training, hyperparameter tuning, and evaluation. A predictive scoring model that prunes weak solutions early (to reduce waste) and accelerates the pipeline generation process. An accelerated debugging paradigm, leveraging “simplified code and smaller data samples” to shorten the feedback loop during pipeline generation.\nThey also introduce a new benchmark called Kompete‑Bench to address limitations in MLE-Bench (e.g., reuse of training data for test sets)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduction of a benchmark (Kompete-Bench) is a positive step toward standardized evaluation in this space.\n2. The multi-agent decomposition is intuitive, making the system design modular and potentially extensible."}, "weaknesses": {"value": "1. Only a limited benchmark (MLE-Bench → Kompete-Bench) is used.\n2. The empirical results do not demonstrate any statistically meaningful improvement. Across benchmarks, the reported mean scores overlap with baselines once standard deviations are considered. KompeteAI is 51.5 ± 1.5 while baseline is  48.2 ± 2.5.\n3. Statistical rigor is weak. no mention of multiple random seeds, standard deviations, confidence intervals.\n4. Presentation is not good. figure readability and caption clarity suffer; some key results are aggregated rather than broken down."}, "questions": {"value": "1. What types of pipelines or datasets cause the system to fail or underperform? What are the resource costs (compute, memory) vs baseline? Without this, deployment risk is less clearly assessed.\n2. Could the authors provide an ablation study isolating the contributions of (a) stage-decomposed architecture, (b) predictive scoring model, (c) accelerated debugging loop? Which component contributes most to speed or accuracy gains?\n3. What are the resource costs (GPU hours, memory) compared to baseline methods? Is there a trade-off that practitioners should be aware of?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aIu8YlogQK", "forum": "44pUhNb083", "replyto": "44pUhNb083", "signatures": ["ICLR.cc/2026/Conference/Submission9023/Reviewer_i6sY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9023/Reviewer_i6sY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982680271, "cdate": 1761982680271, "tmdate": 1762920745512, "mdate": 1762920745512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KompeteAI, an innovative multi-agent AutoML system designed to address the exploration and execution limitations of Large Language Model (LLM)-based AutoML frameworks. In the Introduction, the authors outline challenges with current methods, including poor iterative refinement, constrained solution recombination, and slow code validation. Related Work highlights prior AutoML architectures and the need for dynamic knowledge integration and efficient scoring. KompeteAI’s architecture, detailed in Section 3, partitions the pipeline into modular stages managed by specialized agents, leverages dynamic Retrieval-Augmented Generation (RAG) from external sources, and introduces adding and merging operators for compositional exploration. A novel scoring model and accelerated debugging paradigm significantly reduce execution time. Section 4 presents Kompete-bench, a new benchmark curated to fairly evaluate multi-agent AutoML systems using recent and diverse Kaggle competitions. Experiments in Section 5 demonstrate that KompeteAI exceeds state-of-the-art results on MLE-Bench and Kompete-bench, with ablation studies confirming the critical impact of RAG, merging, and fast scoring. The Conclusion affirms KompeteAI’s advancements in automated pipeline generation, flexible knowledge integration, and robustness in real-world ML challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Multi-agent, tree-guided exploration with explicit mechanisms for “adding” and “merging” ideas/pipeline components is novel, directly addressing existing limitations in MCTS and one-shot LLM-based AutoML.\n- The use of dynamic, contextually adaptive RAG—retrieving and integrating external SOTA solutions in pipeline generation—extends current practices and enhances autonomy.\n- Extensive, comparative experiments against SOTA baselines under fixed resource constraints; ablations elucidate the contribution of each major component (RAG, merging, scoring model).\n- The Kompete-bench benchmark, curated with care to avoid data leakage and to align more honestly with actual human-level performance, deepens the impact of the empirical study.\n- Results are reported with mean ± SEM and badge/percentile-based metrics, reflecting both robustness and practical relevance.\n- Demonstrated acceleration (6.9× iteration speedup) with little to no decrease in quality.\n- The overall motivation, system overview, and empirical design are clearly articulated.\n- Improved performance (e.g., +3% average on MLE-Bench vs. SOTA) and clearer benchmarking protocols directly benefit both academic research and practical AutoML deployment."}, "weaknesses": {"value": "- More information is needed on how merging ensures improvements, how “structural and statistical traits” are reconciled, and how the merging operator avoids destructive or trivial recombinations.\n- The architecture, few-shot setup, and validation protocol of the scoring model are only briefly described. Metrics around its recall/precision in “early stopping” and error rates—critical for safe pruning—are not fully reported.\n- Sensitivity analysis of system hyperparameters (memory buffer sizes, thresholds for merging failures, RAG sample numbers) is lacking, so the method’s stability and robustness are unclear.\n- Guidance for tuning or default values for these hyperparameters is not given.\n- While mean ± SEM is reported, there is no explicit mention of statistical significance testing (e.g., paired t-test), even though some reported differences fall within the error range.\n- The number of seeds/runs and randomization details for ablation studies are unclear in the main text, making it hard to judge reliability.\n- Known failure cases (e.g., where KompeteAI fails on “Contemporary” competitions) are only implied (“far behind top leaderboard teams”) rather than explicitly analyzed or illustrated with examples.\n- Resource usage breakdown (latency, memory) and limitations (e.g., when dynamic RAG or merging could backfire, or nonscalability) are not thoroughly discussed."}, "questions": {"value": "- Can the authors clarify the architecture of the predictive scoring model and how it is trained, particularly regarding few-shot anchor selection and prevention of overfitting to anchor tasks The scoring model is pivotal in accelerating evaluation, but operational specifics—especially training data selection and validation—are deferred to brief descriptions and may affect both bias and validity.\n\n- How does the adaptive RAG component interact with the rest of the system at inference time, particularly in settings where access to external sources (like Kaggle notebooks) is unavailable or potentially restricted? The generalizability and fairness of KompeteAI rest on the ability to source knowledge consistently; if RAG behavior degrades in restricted environments, it could affect both the utility and reproducibility of the framework.\n\n- Can you provide, in the main text, pseudocode or a concise step-by-step algorithm for both the “adding” and “merging” operations, not just refer to the appendix? The current high-level description lacks concrete, reproducible detail; providing explicit algorithms would clarify the method's implementation and assist reproducibility.*\n\n- How is the scoring model validated for reliability in early-stopping poor candidates versus missing late-emerging good solutions, and what metrics define its predictive quality? The effectiveness (and risk of bias) of the scoring model could fundamentally affect search coverage; reporting quantitative metrics (e.g., error or recall rates for early pruning) is critical for evaluating practical safety/performance trade-offs.*\n\n- What hyperparameter sensitivity analyses (e.g., memory buffer sizes, thresholds for merging failures, number of RAG samples) were performed, and how stable is system performance to these changes? Stability with respect to these design choices affects both reliability and adoption; results in the main text or at least brief summary (even if details are in appendix) are necessary.\n\n- Can you clarify what statistical tests (if any) were conducted to affirm the significance of observed performance improvements (e.g., KompeteAI’s +3% over ML-Master on MLE-Bench)? The tables report mean ± SEM, but no explicit significance testing is mentioned; clarification is needed to interpret whether the reported gains are robustly significant.\n\n- Could you provide more details on the ablation study design (e.g., number of seeds/runs per system per condition, randomization protocol)? While ablation results are presented, the experimental protocol (number of repeats, control of random factors) is not fully clear, which is crucial to evaluating variance and the reliability of these component-wise claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RHMlSkCmnO", "forum": "44pUhNb083", "replyto": "44pUhNb083", "signatures": ["ICLR.cc/2026/Conference/Submission9023/Reviewer_umEn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9023/Reviewer_umEn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989965164, "cdate": 1761989965164, "tmdate": 1762920744721, "mdate": 1762920744721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce a new AutoML framework called KompeteAI. That is, a framework for automating machine learning tasks. A key idea of the approach is to decompose the machine learning workflow into separate stages, to be able to focus on specific tasks. Then, at a later stage, partial solutions are combined using an approach that involves adding and merging operations. The paper also introduces approaches to handle execution performance, as well as introduces a new benchmark called Kompete-bench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The general problem of improving AutoML \n\n- The benchmarking shows promising results compared to recent LLM-based AutoML frameworks, both when comparing with different (leading board) LLMs and with the same underlyingLLM (Figure 2)"}, "weaknesses": {"value": "- In the related work section, the competing frameworks, such as MLE-STAR and RD-AGENT, are strongly criticized, e.g., \"producing incoherent or suboptimal integrations\" and \"simply consolidates ideas without meaningfully improving solution exploration.\", but the justification for such strong statements is not that clear. \n\n- Certain parts of the paper are clearer and more precisely described than others. E.g., the section about the scoring model is fairly easy to get a high-level idea, but lacks technical depth.\n\n- The paper becomes a bit unfocused, trying to convey both a new framework for AutoML, and to provide, motivate, and describe a new benchmark. I would recommend focusing on only one of these directions in one paper."}, "questions": {"value": "- Please describe in more detail how your RAG approach (stated as dynamic in Table 1) compares to AutoML and  and RD Agent, that are stated to be in \"R&D-phase\". What is the justifications for this conclusion?\n\n- Please describe more in depth how the controlled merger of this paper differs from the recombination of the RD approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uOOzpcksyl", "forum": "44pUhNb083", "replyto": "44pUhNb083", "signatures": ["ICLR.cc/2026/Conference/Submission9023/Reviewer_753t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9023/Reviewer_753t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997740255, "cdate": 1761997740255, "tmdate": 1762920744109, "mdate": 1762920744109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}