{"id": "FwPbnIEGpk", "number": 21953, "cdate": 1758324038412, "mdate": 1759896894408, "content": {"title": "A Multi-Institutional Multimodal EEG Benchmark for Foundation Model Generalization and Early Neurological Diagnosis", "abstract": "Recent advances in deep learning have accelerated the development of foundation models (FMs) for electroencephalography (EEG), with significant efforts devoted to assembling EEG datasets and training large-scale models. However, existing EEG datasets remain highly fragmented and non-standardized, with limited regional diversity since most originate from the United States. Similarly, current EEG foundation models are trained on different datasets without consistent protocols, making it difficult to compare architectures fairly. Moreover, all existing models are trained exclusively on unimodal EEG signals, limiting their clinical utility, as many downstream diagnostic tasks, such as detecting neurodegenerative diseases, require integration of additional modalities beyond EEG.\nTo address these limitations, we introduce, for the first time VEEG, a multimodal EEG dataset comprising over $6000$ patients collected from two major hospitals outside the US. In parallel, we unify all existing public EEG datasets into a single standardized corpus, enabling the first rigorous benchmarking of state-of-the-art EEG foundation model architectures under consistent pretraining and fine-tuning pipelines. Finally, using our multimodal EEG dataset, we design and evaluate a multimodal diagnostic model, demonstrating that integrating auxiliary modalities (e.g., blood biomarkers and clinical notes) with EEG substantially improves downstream prediction accuracy, for instance, achieving a 27.64\\% gain in Alzheimer’s disease risk prediction.", "tldr": "We present VEEG, the largest non-US clinical EEG dataset with multimodal extensions, providing a diverse pretraining resource that improves foundation model generalization and early Alzheimer’s risk prediction.", "keywords": ["Clinical EEG", "Foundation models", "EEG dataset", "Regional diversity", "Alzheimer’s risk prediction", "Representation learning", "Self-supervised learning", "Multimodal learning", "Blood-based biomarkers"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32adfd862934ada57ed79db767bf64dbd1d8c6ec.pdf", "supplementary_material": "/attachment/bfb25e1a5e2a72f1a58b1bbff7b26b9b27916c39.zip"}, "replies": [{"content": {"summary": {"value": "# Summary\n\nThis paper introduces **VEEG and/or M-EEG**, addressing three key gaps in EEG foundation model research:\n\n## Key Contributions\n\n1. **Large-scale non-US dataset**: 1,170 hours of clinical EEG from 6,081 patients across two hospitals, with a multimodal subset including blood biomarkers and clinical notes\n\n2. **Unified benchmark**: First standardized corpus combining all existing public EEG datasets, enabling fair comparison of foundation model architectures under consistent training protocols\n\n3. **Multimodal diagnostic model**: Demonstrates that integrating auxiliary modalities (blood biomarkers, clinical notes) with EEG substantially improves performance.\n\n## Impact\n\nAddresses geographic bias in EEG data, enables rigorous benchmarking of foundation models, and validates multimodal approaches for early neurological diagnosis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "## Strengths\n\n1. The paper provides a thorough analysis of the research domain, clearly identifying key problems and knowledge gaps.\n\n2. It demonstrates an excellent effort in data collection and standardization.\n\n3. It introduces a novel multimodal, multiregional benchmark that enhances the field’s methodological resources.\n\n4. The **relative** improvement achieved by incorporating vital signs is substantial and noteworthy."}, "weaknesses": {"value": "## Weaknesses\n\n1.  While the identified gaps are important and relevant to the community, the key contributions and novelty appear limited and may not yet meet the standards expected at ICLR.\n\n2. The paper attempts to address too many challenges at once, which dilutes the focus and depth of the contributions.\n\n3. The work reflects a promising and commendable initiative, though the results seem preliminary and may currently be more suitable for a workshop venue. I would, however, like to reserve my final judgment until receiving clarifications to my questions in the next section, which may better reveal the novelty and impact of the work."}, "questions": {"value": "## Questions\n\n1. There are studies that have already standardized multiple datasets across regions, groups, and conditions (e.g., [1,2,3,4]). Could the authors clarify the specific added value or differentiation of their benchmark effort?\n\n2. Prior works have documented performance drops for in-region vs. out-of-region settings using the mentioned NMT and TUH datasets [4], as well as in broader distribution shift scenarios [2]. What is the novelty of the results presented in Figure 2?\n\n3. The paper states that most existing methods are unimodal and not extensible to multimodal data (Section 2.2.1). However, several studies have developed multimodal approaches integrating EEG, MEG, and fMRI, or handling multiple time series modalities with differing sampling rates [3,5]. These methods can be applied to the other time series that are mentioned in this work. Could the authors clarify how their framework goes beyond these prior efforts?\n\nOverall, while the paper demonstrates substantial effort and technical execution, much of the groundwork appears to overlap with prior research. Continuing and refining this line of work, however, could lead to valuable contributions in the near future.\n\n\n[1] Chevallier, S., Carrara, I., Aristimunha, B., Guetschel, P., Sedlar, S., Lopes, B., ... & Moreau, T. (2024). The largest EEG-based BCI reproducibility study for open science: the MOABB benchmark. arXiv preprint arXiv:2404.15319. \n\n[2] Gagnon-Audet, J. C., Ahuja, K., Darvishi-Bayazi, M. J., Mousavi, P., Dumas, G., & Rish, I. (2022). Woods: Benchmarks for out-of-distribution generalization in time series. arXiv preprint arXiv:2203.09978. \n\n[3] Charest, I., Brotherwood, P., Salvas-Hebert, M., Kay, K., & Gosselin, F. (2025). Neural activity resolved in space and time through fusion of large-scale EEG and fMRI datasets. Journal of Vision, 25(9), 2653-2653.\n\n[4] Aristimunha, B., Truong, D., Guetschel, P., Shirazi, S. Y., Guyon, I., Franco, A. R., ... & Delorme, A. (2025). EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding. arXiv preprint arXiv:2506.19141.\n\n[5] Darvishi-Bayazi, M. J., Ghaemi, M. S., Lesort, T., Arefin, M. R., Faubert, J., & Rish, I. (2024). Amplifying pathological detection in EEG signaling pathways through cross-dataset transfer learning. Computers in biology and medicine, 169, 107893. \n\n[6] Ferrante, M., Boccato, T., Rashkov, G., & Toschi, N. (2024). Towards neural foundation models for vision: Aligning eeg, meg, and fmri representations for decoding, encoding, and modality conversion. arXiv preprint arXiv:2411.09723."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper briefly addresses ethical considerations in the appendix, noting that all recordings were de-identified and institution-specific metadata anonymized to preserve privacy and clinical fidelity. However, depending on the conference’s policy regarding human subject research, additional clarification or documentation of compliance may be required. This aspect is outside my main area of expertise."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4cugYCBaFD", "forum": "FwPbnIEGpk", "replyto": "FwPbnIEGpk", "signatures": ["ICLR.cc/2026/Conference/Submission21953/Reviewer_oVMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21953/Reviewer_oVMU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419669526, "cdate": 1761419669526, "tmdate": 1762941995242, "mdate": 1762941995242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces M-EEG, a large-scale multimodal EEG dataset collected from over 6,000 patients across two hospitals outside the US. It addresses the fragmentation and lack of standardization in existing EEG datasets and models, which are mostly unimodal and US-centric. The authors also unify public EEG datasets into a standardized corpus for consistent benchmarking. Using this new dataset, they demonstrate that integrating EEG with additional modalities (like blood biomarkers and clinical notes) significantly improves diagnostic performance. For example, achieving a 27.64% gain in Alzheimer’s disease risk prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors released a new EEG benchmark, collected from two real-world hospitals and involving 6081 subjects. This benchmark demonstrates the diversity of the data across institutions and subjects, and supports multiple downstream tasks in the BCI field, addressing a key limitation in many prior single-center studies.\n- The paper includes enough baseline experiments across different modalities and tasks. These evaluations help position the dataset as a standard benchmark for future studies and demonstrate its potential impact.\n- The dataset’s scale, modality diversity, and design make it promising for transfer learning, cross-domain generalization, and multi-modal representation learning, thereby providing practical value for applied \"AI+healthcare\" works."}, "weaknesses": {"value": "- Although data collection and anonymization are mentioned in appendix, the paper lacks detailed discussion on ethical approval processes or data sharing (e.g., license type, usage restrictions). These are critical for real-world reproducibility and community adoption.\n- The paper does not analyze data distribution differences among participating institutions (e.g., demographic or equipment biases). This should be quantified or discussed more explicitly.\n- The exact release plan, access procedures, and metadata structure remain unclear. Without these, the practical usability of the dataset is reduced."}, "questions": {"value": "Please refer to the Weakness."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fKkSNwl2Kn", "forum": "FwPbnIEGpk", "replyto": "FwPbnIEGpk", "signatures": ["ICLR.cc/2026/Conference/Submission21953/Reviewer_TMat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21953/Reviewer_TMat"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552098559, "cdate": 1761552098559, "tmdate": 1762941994094, "mdate": 1762941994094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the fragmentation and US-centric bias of current EEG and the unimodal focus of existing EEG foundation models. This work introduces M-EEG, which is a large non-US clinical EEG dataset with 6081 patients and paired blood biomarkers and clinical notes and unifies major public EEG datasets into a standardized pretrianing corpus (P-EEG) + a task suite for fair downstream evaluation (T-EEG). Under identical pipelines, the authors benchmark leading EEG foundation models and show that adding regionally diverse data improves out-of-region robustness and that lightweight multimodal fusion of blood tests with EEG yields clear gains on early neurological diagnosis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I found the paper original because the authors built the largest non-US, multimodal clinical EEG set with paired blood tests and notes. It also defines a unified pretrain corpus and a clear downstream suite for fair Foundation model comparison. The quality is also fair, as the data are large, standardized, and used in controlled tests that check both in-region stability and out-of-region transfer. Also, I found the clarity good as the work opens with a concise figure-level overview of the dataset, benchmark and multimodal path, then explains P-EEG and T-EEG plainly. Finally, I think the significance is good as the authors add M-EEG keeps or improves in-region results and clearly lifts out-of-region robustness, and simple EEG+blood fusion shows large gains on Alzheimer's risk prediction."}, "weaknesses": {"value": "There are a couple of concerns that I would like to raise. \n\n(1) From my understanding, the multimodal evidence in this work is narrow, as the dataset includes both blood tests and clinical notes, yet the modelling and results use only blood, and no text modality is evaluated. I was thinking maybe providing a small text-EEG fusion baseline or even an ablation study would better support your multimodal claim. \n\n(2) Also, the linking of labs/notes to the EEG is by subject ID, and there is no stated time window around the EEG, which can mix pre-/post information. What the authors think of spelling out the windowing rule and sharing a sensitivity check with tighter and looser windows. \n\n(3) For downstream benchmarking, heterogeneous datasets are linearly mapped to a 19-channel montage. This harmonization is practical but may distort cross-dataset comparisons. Maybe a sensitivity experiment without mapping (if possible) or with alternative mappings could help, or at least, I'd like to hear the authors' thoughts on it"}, "questions": {"value": "I'd encourage the authors to check the weaknesses part first, and here, I have a couple of questions and would appreciate the authors' feedback. \n\n(1) I was wondering do the EEG+blood gains remain after controlling for lab availability and ordering patterns, for example, when you compare only cases with a common lab panel or matched draw times, or could the model be using care-path shortcuts instead of physiology? I think it'd be interesting for the community to know that. \n\n(2) I am also curious to know how you rule out site or device fingerprints as the source of out-of-region gains, given montage templates, amplifier noise, and 50 vs 60 Hz line features can differ by hospital, and would a leave-device-out or site-confusion check share a different story? It's a bit confusing. \n\n(3) My final question is that are the reported improvements are stable across random seeds and alternative splits, and statistically reliable under a patient-level bootstrap with corrections across tasks, so that the conclusion does not hinge on a single partition? \n\nI'm really looking forward to the rebuttals and appreciate the authors' hard work. I would be happy to modify the score if the answers from the authors are convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ue9FFGDxs", "forum": "FwPbnIEGpk", "replyto": "FwPbnIEGpk", "signatures": ["ICLR.cc/2026/Conference/Submission21953/Reviewer_6AaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21953/Reviewer_6AaG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968623757, "cdate": 1761968623757, "tmdate": 1762941993861, "mdate": 1762941993861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses key limitations in EEG foundation modeling: the lack of standardized benchmarks, the strong US-centric bias of existing datasets, and the unimodal (EEG-only) nature of current models.\n\nThe authors make three main contributions:\n\n1. M-EEG Dataset: They introduce M-EEG, a new, large-scale (6,081 patients, 1,170 hours) multi-institutional clinical EEG dataset collected from two hospitals outside the US. This is the largest non-US dataset by subject count and uniquely features a multimodal subset with paired EEG, blood-based biomarkers (BBB), and clinical notes.\n2. Standardized Benchmarking (P-EEG & T-EEG): They create a new benchmarking framework. This includes P-EEG, a unified pretraining corpus created by harmonizing their new M-EEG dataset with existing public datasets (TUEG and NMT Scalp), and T-EEG, a diverse benchmark of seven downstream task-oriented datasets. This framework enables the first rigorous, fair comparison of state-of-the-art EEG foundation models (CBraMOD and EEGPT).\n3. Multimodal Validation: They demonstrate the value of multimodality by designing a multimodal model that fuses EEG and blood biomarkers using cross-attention. Experiments on the PEARL dataset (using models pretrained on P-EEG) show that adding blood biomarkers significantly improves diagnostic accuracy for Alzheimer's risk prediction, achieving up to a 27.64% relative gain in balanced accuracy over an EEG-only baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Significant Data Contribution: The primary strength is the introduction of the large-scale EEG dataset. This directly addresses the critical lack of geographic and demographic diversity in existing EEG corpora (e.g., TUH, HEEDB).\n\n2. Novel Multimodality: M-EEG is uniquely multimodal, including paired EEG, blood-based biomarkers (BBB), and clinical notes for a subset of patients. This is a significant contribution that opens the door for a new class of multimodal foundation models for neurology, moving beyond EEG-only signals to incorporate other clinically vital, minimally-invasive data.\n\n3. First Standardized EEG FM Benchmark: The paper introduces a comprehensive and standardized benchmarking framework (P-EEG for pretraining, T-EEG for downstream tasks). This is a high-quality contribution that allows for a rigorous and fair comparison of SOTA EEG FMs under consistent protocols. This addresses a major limitation of prior work where fair comparison was impossible.\n\n4. Strong Empirical Evidence and Key Insights: The paper provides strong, well-supported evidence for two important claims:\n   - Regional Diversity Matters: The experiments clearly show that pretraining with the non-US M-EEG data substantially improves model generalization to out-of-region (OOD) data while maintaining performance on in-region tasks (Tables 3 & 4).\n   - Multimodality is High-Value: The paper demonstrates that integrating blood biomarkers with EEG leads to significant performance gains in a clinical task. This is a significant result for the clinical utility of FMs."}, "weaknesses": {"value": "1. Dataset Naming: The paper is severely undermined by a major, recurring inconsistency in the name of its own dataset. The title, abstract, and Section 3.1 call it \"M-EEG\". However, it is repeatedly referred to as \"VEEG\" throughout the paper, including in Section 2, Figure 1 (and its caption), Section 3.2, and Section 4.3. This is an extremely confusing and careless error that must be fixed.\n\n2. Misleading Wording in Abstract: The abstract contains two statements that are either incorrect or misleading based on the paper's own contents:\n\n   - It claims P-EEG \"unify(s) all existing public EEG datasets\". This is factually incorrect. Section 3.2.1 clearly states P-EEG consists of only three datasets (TUEG, NMT Scalp, M-EEG) and explicitly excludes major corpora like HEEDB and SHHS. This should be rephrased to \"unifies several key public datasets.\"\n   - It states, \"using our multimodal EEG dataset... achieving a 27.64% gain\". This implies the multimodal evaluation was performed on M-EEG. However, the experiment (Section 4.4, Table 5, Table 11) was conducted on the external PEARL dataset. While the model was pretrained on P-EEG (which contains M-EEG), the evaluation dataset for this specific claim was PEARL. This wording should be clarified.\n\n3. Confusing/Redundant Results Presentation: The presentation of the multimodal results in the appendix is cluttered. There are four separate tables (Tables 9, 10, 11, and 12) that show different slices of the same core experiment (original vs. P-EEG-pretrained checkpoints, concat vs. attention fusion, vs. BBB-only). This can be consolidated into one or two clear, summary tables that directly compare the most important conditions."}, "questions": {"value": "1. Please clarify the correct name of the dataset you are introducing. The paper uses \"M-EEG\" and \"VEEG\" interchangeably (e.g., Title/Abstract vs. Figure 1/Section 2). This is a major point of confusion and must be corrected.\n\n2. The abstract claims P-EEG unifies \"all\" existing datasets, but Section 3.2.1 states it's composed of TUEG, NMT-Scalp, and M-EEG, and excludes HEEDB. Can you confirm this and that you will correct the abstract to reflect the actual composition?\n\n3. The abstract's claim of a \"27.64% gain\" appears to be based on experiments on the external PEARL dataset (Table 5/11), not on a downstream task within the new M-EEG dataset. Could you please clarify this? Is there a multimodal downstream task (e.g., for AD risk) evaluated on M-EEG itself, or is M-EEG's multimodal component primarily offered as a resource for future work?\n\n4. The multimodal fusion (Section 4.4, Appendix A.3) uses blood-based biomarkers (BBB). Can you provide more detail on which specific biomarkers from the blood tests were used? Is it the full set of available lab values, or a curated subset (e.g., complete blood count, inflammatory markers) known to be relevant to neurodegeneration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The authors state that the data was \"fully de-identified before release\" and \"anonymized\", following standard procedures for handling sensitive patient data. So no ethics review is needed."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NJ5RBYX6I2", "forum": "FwPbnIEGpk", "replyto": "FwPbnIEGpk", "signatures": ["ICLR.cc/2026/Conference/Submission21953/Reviewer_9JfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21953/Reviewer_9JfV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979044332, "cdate": 1761979044332, "tmdate": 1762941993543, "mdate": 1762941993543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}