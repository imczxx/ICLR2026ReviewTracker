{"id": "4sF8dqXhOf", "number": 1219, "cdate": 1756865783268, "mdate": 1762933687410, "content": {"title": "Efficient Reasoning via Reward Model", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs).\nHowever, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step—a phenomenon known as overthinking—which substantially increases computational costs.\nPrior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance.\nTo address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that  scores the conciseness of reasoning path.\nAdditionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning.\nFrom a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties.\nBesides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method’s effectiveness and token efficiency, which achieves an 8.1\\% accuracy improvement and a 19.9\\% reduction in response token length on Qwen2.5-7B. \nFurthermore, the method generalizes well to other LLMs including Llama and Mistral.\nThe implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.", "tldr": "", "keywords": ["Large Language Model", "Large Reasoning Model", "Overthinking", "Efficient Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6507867868ecaa17e7905e3c0233bd1fa89356fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel post-training pipeline for improving reasoning efficiency in LLMs by encouraging shorter reasoning paths. Specifically, the authors introduce a Conciseness Reward Model (CRM), a compact 3B model distilled from Qwen2.5-72B-Instruct, to evaluate the brevity of generated responses. The conciseness scores are then combined with difficulty scores through an annealing schedule to produce the final reward. By integrating CRM into the RL-based post-training stage, the resulting LLM is able to generate reasoning traces that are both accurate and concise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Proposes a novel pipeline that incorporates a compact Conciseness Reward Model (CRM) to enable more efficient LLM reasoning.\n\n2. Provides comprehensive empirical evaluations across multiple LLM backbones, including Qwen, LLaMA, and Mistral."}, "weaknesses": {"value": "1. The proposed idea of distilling a compact CRM from a large and powerful LLM is intuitive but lacks rigorous analysis. It remains unclear how the large model reliably generates accurate conciseness scores for reasoning paths. Moreover, if training efficiency is not a concern, it would be valuable to justify why the authors do not directly use Qwen-72B-Instruct to compute conciseness scores.\n\n2. The baseline comparison is incomplete. Several representative post-training variants, such as DAPO, should be included to better demonstrate how much accuracy is sacrificed when pursuing reasoning efficiency.\n\n3. The reduction in the number of generated tokens achieved by CRM is relatively marginal compared to existing methods like COS and Kimi.\n\n4. The reported accuracy of the proposed CRF model is notably low, especially on challenging mathematical reasoning benchmarks such as AMC23 and AIME25. For instance, a post-trained Qwen-7B-Base can typically achieve scores of 60+ on AMC23 and 12+ on AIME25, whereas the proposed CRF only reaches 55 and 6.7, respectively, indicating substantial performance degradation. It would also strengthen the paper to include results on the widely used AIME24 benchmark."}, "questions": {"value": "Please see Weaknessnes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "z0pxpJXR6y", "forum": "4sF8dqXhOf", "replyto": "4sF8dqXhOf", "signatures": ["ICLR.cc/2026/Conference/Submission1219/Reviewer_qKNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1219/Reviewer_qKNM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570432485, "cdate": 1761570432485, "tmdate": 1762915712284, "mdate": 1762915712284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5dtowyFNrP", "forum": "4sF8dqXhOf", "replyto": "4sF8dqXhOf", "signatures": ["ICLR.cc/2026/Conference/Submission1219/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1219/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762933615193, "cdate": 1762933615193, "tmdate": 1762933615193, "mdate": 1762933615193, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of inefficient and overly verbose reasoning in large reasoning models (LRMs) trained with reinforcement learning. The authors observe two types of pathologies in prior length-penalized reward learning frameworks (e.g., RLVR, DAPO): **length collapse** (producing overly short, uninformative outputs) and **training collapse** (both reward and reasoning length drop, causing instability).\n\nTo mitigate these issues, the paper proposes an adaptive reward shaping framework based on a **Conciseness Reward Model (CRM)** and a **Conciseness Reward Function (CRF)**:\n\n1. **Conciseness Reward Model (CRM):**  \n   A 3B-parameter model trained via supervised fine-tuning on 65K augmented mathematical reasoning samples to evaluate reasoning conciseness from 0.1–1.0.  \n2. **Conciseness Reward Function (CRF):**  \n   Combines the correctness reward \\( R_i^o \\) with the conciseness score \\( c_i \\), scaled by an annealing coefficient \\( s \\) and a difficulty factor \\( d_q \\):  \n   \\[\n   \\hat{R}_i = R_i^o [1 + \\alpha \\, c_i (s + d_q)]\n   \\]\n   Conciseness is only rewarded when the output is correct, preventing reward hacking.\n\nThe authors theoretically show that CRF reduces gradient variance and improves training stability, and empirically demonstrate strong gains in both accuracy and efficiency:\n- +8.1% accuracy and −19.9% average token length compared with GRPO baseline on Qwen2.5-7B;\n- consistent performance across Llama3-8B and Mistral-7B backbones;\n- reduced reward variance and stabilized reasoning length distribution.\n\nOverall, the paper presents a principled and empirically validated method for balancing correctness and conciseness in reasoning-focused reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear problem identification.**  \n   The paper explicitly defines two major failure modes in current reward-based reasoning optimization — *length collapse* and *training collapse*. These phenomena are real, empirically verified, and practically important for RLHF/RLVR research.\n\n2. **Elegant reward formulation.**  \n   The proposed Conciseness Reward Function (CRF) only rewards brevity when the reasoning is correct, effectively preventing reward hacking and over-penalization of valid long reasoning.\n\n3. **Balanced theoretical and empirical presentation.**  \n   Theoretical derivations (variance reduction and convergence proofs) support the empirical intuition. The paper maintains a good balance between formalism and practical validation.\n\n4. **Strong practical motivation.**  \n   The method directly targets the computational inefficiency of chain-of-thought reasoning, making it relevant for scaling and deployment scenarios where token cost matters.\n\n5. **Stable optimization behavior.**  \n   Empirical curves show smoother reward and length convergence than RLVR or DAPO, suggesting real improvement in RL training dynamics."}, "weaknesses": {"value": "1. **Over-reliance on the Qwen family models.**  \n   All main experiments — including baselines, reward models, and CRM training — are built upon Qwen2.5 and Qwen-Math checkpoints.  \n   Since Qwen’s pre-training corpus contains ** similar reasoning data**, there is a strong possibility of **data contamination** or leakage.  \n   This compromises the fairness and generalizability of results, especially when all baselines share the same Qwen backbone.\n\n2. **Limited model diversity.**  \n   Although Llama-3 and Mistral results are briefly mentioned, they are neither fully trained nor ablated.  \n   The claimed “cross-model generalization” is therefore weakly supported.\n\n3. **Synthetic reward supervision.**  \n   The Conciseness Reward Model (CRM) is trained on pseudo-labels generated by another LLM (Qwen2.5), not human ratings.  \n   This introduces teacher bias — the CRM may inherit stylistic or reasoning biases from Qwen itself.\n\n4. **Domain restriction.**  \n   All experiments are in **mathematical reasoning**.  \n   No tests are provided on code, logic, or general reasoning datasets, limiting the scope of applicability.\n\n5. **Efficiency not fully quantified.**  \n   The work measures efficiency only in terms of *token length reduction*, not actual compute time or energy cost, which weakens the “efficient reasoning” claim.\n\n6. **Potential reward circularity.**  \n   Since both correctness and conciseness signals originate from similar models (Qwen-RM and Qwen-CRM), the overall reward shaping pipeline lacks independence and may reinforce model-specific stylistic tendencies rather than genuine reasoning efficiency."}, "questions": {"value": "1. **Model Dependence and Potential Data Leakage**  \n   Your experiments rely heavily on Qwen2.5, Qwen-Math, and Qwen-RM, which are known to contain large quantities of mathematical reasoning data (e.g., GSM8K, MathBench).  \n   How did you ensure that your evaluation datasets were not contaminated or overlapping with Qwen’s pretraining corpus?  \n   Would your conclusions hold if you repeated the experiments on models such as Llama-3 or Mistral that are less math-specialized?\n\n---\n\n2. **Independence of Reward Components**  \n   Both the correctness reward and the conciseness reward come from models derived from the Qwen family.  \n   Could this shared model lineage introduce circular bias into your training (i.e., the reward model reinforcing its own stylistic tendencies)?  \n   Have you tried using a different family of reward models to confirm independence?\n\n---\n\n3. **Conciseness Reward Model (CRM) Supervision Quality**  \n   The CRM is trained using pseudo-labels generated by Qwen2.5.  \n   How confident are you that these pseudo-labels reflect genuine human preferences for “conciseness” rather than model-specific stylistic bias?  \n   Did you consider human or cross-model calibration to improve robustness?\n\n---\n\n4. **Cross-domain and Cross-task Generalization**  \n   All reported results are in mathematical reasoning.  \n   Have you evaluated CoLD (or CRF) on other reasoning domains such as code generation, logic puzzles, or open-ended QA?  \n   If not, what limitations do you expect when transferring your method to such domains?\n\n---\n\n5. **Definition of Efficiency**  \n   You primarily report token-length reduction as evidence of reasoning efficiency.  \n   Have you measured real computational efficiency — e.g., wall-clock time, FLOPs, or energy savings — to validate that CRF truly improves inference efficiency rather than just shortening outputs?\n\n---\n\n6. **Training Stability and Hyperparameter Sensitivity**  \n   The annealing and difficulty coefficients (\\( s \\), \\( d_q \\), \\( \\alpha \\)) appear central to stability.  \n   How sensitive is the method to these parameters?  \n   Would the model still converge stably if they were mis-specified or fixed constants?\n\nCan you show the actual correlation between correctness and conciseness scores in your dataset?\nHow were baseline hyperparameters chosen? Can you show that other settings don't work better?\nWhat's the wall-clock overhead of calling CRM during training?\nWhy only Pass@1? Most reasoning papers report Pass@k.\nWould be useful to see human evaluation of reasoning quality and computational cost analysis of running CRM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rEUDL8rqDB", "forum": "4sF8dqXhOf", "replyto": "4sF8dqXhOf", "signatures": ["ICLR.cc/2026/Conference/Submission1219/Reviewer_gi4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1219/Reviewer_gi4v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619610535, "cdate": 1761619610535, "tmdate": 1762915712182, "mdate": 1762915712182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Conciseness Reward Model (CRM) and a Conciseness Reward Function (CRF) to improve reasoning efficiency in large reasoning models. By combining correctness-based rewards with learned conciseness signals, the method reduces redundant reasoning steps without hurting accuracy. Experiments on multiple reasoning benchmarks show that CRF achieves shorter responses and higher accuracy than prior efficient reasoning methods, effectively avoiding length and training collapse."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow and well structured\n2. The proposed and trained reward model provide a new way for concise rewarding."}, "weaknesses": {"value": "My main concerns lie in the **evaluation design**.\n\n1. The primary results focus on *Pass@K*, which is not a standard metric for evaluating current Large Reasoning Models (LRMs). While Pass@K is meaningful, a paper aiming to balance **accuracy and reasoning efficiency** should also report *Average@K* for fairer comparison with prior work.  \n2. The evaluated models are not representative of today’s **reasoning or thinking models**. The reported token reduction (from ~500 to ~300) appears modest, especially considering that modern inference engines (e.g., VLLM, SGLang) can offset such differences with optimization.  \n3. The discussion section is insightful, but the connection between those analyses and the proposed **Conciseness Reward Model (CRM)** or **Conciseness Reward Function (CRF)** remains unclear. It would be helpful to clarify how these findings directly relate to the proposed methods."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i85dmVfuPV", "forum": "4sF8dqXhOf", "replyto": "4sF8dqXhOf", "signatures": ["ICLR.cc/2026/Conference/Submission1219/Reviewer_CLdK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1219/Reviewer_CLdK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849777983, "cdate": 1761849777983, "tmdate": 1762915711981, "mdate": 1762915711981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Conciseness Reward, a new reward view to encourage concise reasoning via a learned reward model.\nThe author trained a model (CRM) to score the conciseness of an answer, and incorporated this reward into the RL algorithm's reward based on difficulty and training stage, and validated the effectiveness of the method using GRPO.\nThe author conducted sufficient ablation experiments around the proposed method.\n\nBut the main experiment for verifying its effectiveness seems to have some issues, see Weakness and Question.\nThis method introduces a reward model; it may cause new reward hacking, such as skipping steps that should not be skipped in the inference process.\nI do not recommend accepting this paper unless the author fully explains the effectiveness of their methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel perspective: optimizing with conciseness rewards.\n2. The paper is well written, with clear motivation and mathematical presentation.\n3. The experiments and ablation setting are reasonable."}, "weaknesses": {"value": "The method is reasonable and simple, so my main question is about the experimental results.\nThe issue of the reproduced baseline raises doubts about the effectiveness of the proposed method.\n\nThe reproduced baseline in Table 2 differs significantly from the cited paper, especially the paper proposed Cos conducted same experiments on Llama-3.1 8B. The number of tokens shows that the baseline reproduced by the author has a length collapse, while the original Cos paper did not. This raises doubts about the correctness of the baseline experiment of Qwen-2.5."}, "questions": {"value": "The method is simple, and the authors provide ablation experiments for reference. The main issue arises from the effectiveness of the method itself. Please refer to Weakness.\n\n1. How would the training behavior and results be if a length penalty were used in place of the score of CRM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nFWeiwh2pl", "forum": "4sF8dqXhOf", "replyto": "4sF8dqXhOf", "signatures": ["ICLR.cc/2026/Conference/Submission1219/Reviewer_9SfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1219/Reviewer_9SfF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901521433, "cdate": 1761901521433, "tmdate": 1762915711699, "mdate": 1762915711699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}