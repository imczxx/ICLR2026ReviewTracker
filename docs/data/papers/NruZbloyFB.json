{"id": "NruZbloyFB", "number": 13687, "cdate": 1758220913884, "mdate": 1763651557471, "content": {"title": "Reward Shaping Control Variates for Off-Policy Evaluation Under Sparse Rewards", "abstract": "Off-policy evaluation (OPE) is essential for deploying reinforcement learning in safety-critical settings, yet existing estimators such as importance sampling and doubly robust (DR) often exhibit prohibitively high variance when rewards are sparse. In this work, we introduce Reward-Shaping Control Variates, a new family of unbiased estimators that leverage potential-based reward shaping to construct additional zero-mean control variates. We prove that shaped estimators always yields valid variance reduction, and that combining shaping-based and Q-based control variates strictly expands the variance-reduction subspace beyond DR and its minimax variant MRDR. Empirically, we provide a systematic regime map across synthetic chains, a cancer simulator, and an ICU-sepsis benchmark showing that shaping-based OPE consistently outperforms DR in sparse-reward settings, while a hybrid estimator achieves state-of-the-art performance across sparse, noisy, and misspecified environments. Our results highlight reward shaping as a powerful and interpretable tool for robust OPE, offering both theoretical guarantees and practical improvements in domains where standard estimators fail.", "tldr": "We show how potential-based reward shaping can be leveraged as a control variate in OPE and provide theoretical and empirical performance guarantees", "keywords": ["reward shaping", "potentials", "off-policy evaluation", "bias", "variance"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/376f95c04993639161ec6d8939ba22d27d1bebc3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an unbiased low-variance off-policy evaluation operator based on reward shaping control variates. The authors leverage the property of  state shaping potential functions do not alter the optimal policy to inject knowledge to sparse reward environments, where conventional methods like DR, MRDR can fail due to extreme sparsity. The authors prove their RSCV estimator strictly reduces variance with an optimal $\\lambda^*$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper nicely combines reward shaping and off-policy evaluation and clearly leverages the idea that potential based shaping does not shift the optimality of an MDP. The paper also surveys existing approaches like DR/MRDR on this problem to better position the proposed RSCV. Presentation is clear and understandable. Math is not deep but sufficient to illustrate the advantage of RSCV. Sepsis and cancer as testbeds are standard and the results seem convincing."}, "weaknesses": {"value": "Disclaimer: I am not an expert in off-policy evaluation. \n\nMy concern is mainly technical. \n - Definition 1 defines potential based reward shaping $F(s, a, s') = \\gamma \\phi(s') - \\phi(s)$. However, no action is involved in the RHS. Do the authors suggest an extended PBRS which is based on action? But in line 244 the authors write: \n\"$\\Phi$ captures progress towards reward or risk of failure, so that the differences $\\gamma\\Phi(s_{t+1}) - \\Phi(s_t)$ absorb predictable structure in the return\". So it seems the potential is based solely on states. However, it confuses me why state-based potential could capture structure of reward that depends on actions? \n- Line 183, I don't understand why the two sides equal under $W_t = \\prod_k^t \\frac{\\pi_e(a_k|s_k)}{\\pi_b(a_k|s_k)}$? Can you show more details? \n- line 48 claims that marginalized estimators like DualDICE remain brittle due to the reward support. But isn't DICE methods estimate state marginals that do not depend on actions? \n- line 160 claims that PBRS guarantees the evaluation policy's value is estimated consistently with the reward structure. But have the authors verified that holds (approximately) true with a parametrized $\\Phi_{\\beta}$? How do you guarantee the assumption $\\mathbb{E}_{\\pi_b}[C^{\\Phi] = 0$ given a learned $\\Phi_{\\beta}$? If this holds only approximately, any insight on the difference it might bring to the result?"}, "questions": {"value": "please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JPxkAV1ygs", "forum": "NruZbloyFB", "replyto": "NruZbloyFB", "signatures": ["ICLR.cc/2026/Conference/Submission13687/Reviewer_m3u3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13687/Reviewer_m3u3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806085283, "cdate": 1761806085283, "tmdate": 1762924245494, "mdate": 1762924245494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of doing off-policy evaluation for sparse reward tasks by proposing reward-shaping control variates (RSCV). The method uses a potential-based reward shaping technique to maintain the optimal policy under the shaped MDP. It defines a learnable additional random variable which has a zero mean under the behavior-policy distribution, and learns the random variable by a potential network for variance reduction. The work is evaluated on medical treatment benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method is supported by theoretical proof, showing the modification added to the reward does not change the optimal action distribution (Theorem 1), and remains unbiased (Theorem 2). Theorem 3 and Corollary 1 show the bound of variance.\n\nThe paper discusses the difference between the proposed method and related works, DR and MRDR (Section 4.3). The discussion highlights that RSCV mainly relies on learning the potential function, while related works focus on learning the action value estimation.\n\nThe paper performs a sanity check in a clear tabular environment. A simple tabular environment is helpful in providing us with a clear demonstration of RSCV’s effectiveness with an increasing sample size. \n\nThe paper considers multiple evaluation metrics, including the bias, variance, mean-squared error, and effective sample size, supporting the theoretical result regarding the bias and variance."}, "weaknesses": {"value": "The method is limited to discrete action space tasks. For datasets with continuous action spaces, the paper discretizes the space (indicated in F.2). Discretizing values introduces information loss because different actions may be mapped to the same discrete representation. In addition, the choice of discretization parameters, such as the number of bins, can significantly affect performance and stability.\n\nFitted Q evaluation (FQE) is used as the action value estimation in DR as a baseline, however, it is not a strong baseline choice especially when the dataset coverage is limited. FQE does not properly handle the out-of-distribution action sample in bootstrapping, which can cause inaccurate value estimates even after many training iterations. \n\nThe paper discusses marginalized estimators such as DualDICE and GenDICE in the related work section, but does not include them in the empirical comparison. It may be worth checking their performance as well.\n\nExperimental validation is conducted on only three datasets, one of which serves as a sanity check. Evaluating the method on a broader range of benchmarks would make the empirical results more convincing.\n\nThe method introduces a weighting parameter ($\\lambda$) for the learnable random variable term, but lacks the sensitivity analysis on it. It would be good to discuss how the weight affects performance."}, "questions": {"value": "I have the following 2 questions:\n\n1. To optimize the model, the paper set the training process to use k-fold validation. K-fold is an effective way to prevent overfitting, but at the cost of longer computational time and larger computational resources. Could the author please further explain the computational overhead for introducing k-fold here? With a large dataset, is it recommended to further reduce the K? According to the current experiment, is the difference large when running with different K’s? \n\n2. Could the author please explain what the error bars in the figures represent? Is it the standard deviation? \n\nI would be happy to discuss further if I have misunderstood any part of the method or experimental setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AugjPEEJQr", "forum": "NruZbloyFB", "replyto": "NruZbloyFB", "signatures": ["ICLR.cc/2026/Conference/Submission13687/Reviewer_W8Fe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13687/Reviewer_W8Fe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930345790, "cdate": 1761930345790, "tmdate": 1762924245084, "mdate": 1762924245084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of doing off-policy evaluation for sparse reward tasks by proposing reward-shaping control variates (RSCV). The method uses a potential-based reward shaping technique to maintain the optimal policy under the shaped MDP. It defines a learnable additional random variable which has a zero mean under the behavior-policy distribution, and learns the random variable by a potential network for variance reduction. The work is evaluated on medical treatment benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method is supported by theoretical proof, showing the modification added to the reward does not change the optimal action distribution (Theorem 1), and remains unbiased (Theorem 2). Theorem 3 and Corollary 1 show the bound of variance.\n\nThe paper discusses the difference between the proposed method and related works, DR and MRDR (Section 4.3). The discussion highlights that RSCV mainly relies on learning the potential function, while related works focus on learning the action value estimation.\n\nThe paper performs a sanity check in a clear tabular environment. A simple tabular environment is helpful in providing us with a clear demonstration of RSCV’s effectiveness with an increasing sample size. \n\nThe paper considers multiple evaluation metrics, including the bias, variance, mean-squared error, and effective sample size, supporting the theoretical result regarding the bias and variance."}, "weaknesses": {"value": "The method is limited to discrete action space tasks. For datasets with continuous action spaces, the paper discretizes the space (indicated in F.2). Discretizing values introduces information loss because different actions may be mapped to the same discrete representation. In addition, the choice of discretization parameters, such as the number of bins, can significantly affect performance and stability.\n\nFitted Q evaluation (FQE) is used as the action value estimation in DR as a baseline, however, it is not a strong baseline choice especially when the dataset coverage is limited. FQE does not properly handle the out-of-distribution action sample in bootstrapping, which can cause inaccurate value estimates even after many training iterations. \n\nThe paper discusses marginalized estimators such as DualDICE and GenDICE in the related work section, but does not include them in the empirical comparison. It may be worth checking their performance as well.\n\nExperimental validation is conducted on only three datasets, one of which serves as a sanity check. Evaluating the method on a broader range of benchmarks would make the empirical results more convincing.\n\nThe method introduces a weighting parameter ($\\lambda$) for the learnable random variable term, but lacks the sensitivity analysis on it. It would be good to discuss how the weight affects performance."}, "questions": {"value": "I have the following 2 questions:\n\n1. To optimize the model, the paper set the training process to use k-fold validation. K-fold is an effective way to prevent overfitting, but at the cost of longer computational time and larger computational resources. Could the author please further explain the computational overhead for introducing k-fold here? With a large dataset, is it recommended to further reduce the K? According to the current experiment, is the difference large when running with different K’s? \n\n2. Could the author please explain what the error bars in the figures represent? Is it the standard deviation? \n\nI would be happy to discuss further if I have misunderstood any part of the method or experimental setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AugjPEEJQr", "forum": "NruZbloyFB", "replyto": "NruZbloyFB", "signatures": ["ICLR.cc/2026/Conference/Submission13687/Reviewer_W8Fe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13687/Reviewer_W8Fe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930345790, "cdate": 1761930345790, "tmdate": 1763693366171, "mdate": 1763693366171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical failure of standard off-policy evaluation (OPE) estimators, such as Importance Sampling (IS) and Doubly Robust (DR), which suffer from prohibitively high variance in sparse-reward environments. The authors introduce Reward-Shaping Control Variates (RSCV), a new class of unbiased estimators. The core idea is to leverage policy-invariant potential-based reward shaping (PBRS) to construct a new, provably zero-mean control variate. The authors provide a practical algorithm to learn the potential function $\\Phi$ and the optimal weight $\\lambda$ directly from data by maximizing variance reduction. Experiments on a chain MDP, a cancer simulator, and the ICU-Sepsis benchmark show that this method reduces variance and Mean Squared Error (MSE) significantly compared to standard baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method directly targets a well-known and significant limitation of OPE, high variance in sparse-reward settings, which is a common feature of real-world applications such as healthcare.\n- The experimental results demonstrate orders of magnitude improvement in variance and MSE over PDIS, DR, and MRDR in sparse-reward tasks. The method is also shown to be more robust to reward noise."}, "weaknesses": {"value": "- The method's success depends on learning a good potential function $\\Phi$. The experiments are on tabular or low-dimensional (4-47 features) state spaces. How well does the learning algorithm for $\\Phi$ (Algorithm 1) scale to high-dimensional problems (e.g., images)?\n- There is a contradiction in the ICU-Sepsis setup. Section 5.3 states $\\pi_b$ is a generated PPO policy, while Appendix F.2 says it's the empirical clinician policy (Line 895). Which was it?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nBaaEpaoxQ", "forum": "NruZbloyFB", "replyto": "NruZbloyFB", "signatures": ["ICLR.cc/2026/Conference/Submission13687/Reviewer_nuKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13687/Reviewer_nuKr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068461199, "cdate": 1762068461199, "tmdate": 1762924244330, "mdate": 1762924244330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}