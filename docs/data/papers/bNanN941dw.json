{"id": "bNanN941dw", "number": 15255, "cdate": 1758249342899, "mdate": 1759897317756, "content": {"title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data", "abstract": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C$^2$LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.", "tldr": "This paper addresses the offline clustering of bandits problem, proposing algorithms and theoretical bounds to handle various amount of data scenarios effectively.", "keywords": ["Offline Clustering of Bandits", "Offline Bandits Algorithms", "Data Insufficiency"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7ccb9d34c63b3f8e6f42501ebdcd1d873853256.pdf", "supplementary_material": "/attachment/c8389f4c01514f58fb6233c68a5e593951100417.zip"}, "replies": [{"content": {"summary": {"value": "The paper considers making an item recommendation with offline data where users belong to clusters (users in the same cluster share the same preference vector).   A test user has already made some ratings.  These ratings are used to select a cluster of similar users, the preference vector of this cluster is estimated and used to select an item to recommend to the test user.  It appears that the same item can be recommended multiple times to the same user, although the text is not completely clear on this."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, the analysis seems sound although I haven't checked all of the maths."}, "weaknesses": {"value": "The proposed algorithm seems similar to a nearest neigbours approach, which have been very well studied in recommendation systems.  I would like to see more discussion on this since it is directly relevant to the contribution.  I think I understand why the paper focuses solely on offline data, but a more useful setup would be where offline data is used to pre-train a recommender system which then carries out online learning (e.g. this is a v common setup in the recommender RL literature).  \n\nThe paper is not the first work on clustered recommendation.  See for example \"Cluster-Based Bandits: Fast Cold-Start for Recommender System New Users\", https://dl.acm.org/doi/abs/10.1145/3404835.3463033, \"Fast and Accurate User Cold-Start Learning Using Monte Carlo Tree Search\" https://dl.acm.org/doi/abs/10.1145/3523227.3546786.\n\nRegarding the theory contribution, the regret bound is a probabilistic worst-case one.  Its not clear that worst-case bounds are especially interesting or useful and the paper would be greatly strengthened by presenting more evidence on whether the worst-case analysis is much different from the observed regret."}, "questions": {"value": "See comments in weaknesses section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xakh0oJM5u", "forum": "bNanN941dw", "replyto": "bNanN941dw", "signatures": ["ICLR.cc/2026/Conference/Submission15255/Reviewer_vvYj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15255/Reviewer_vvYj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961133485, "cdate": 1760961133485, "tmdate": 1762925553729, "mdate": 1762925553729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of pooling offline data from similar users to a current user whom the learner has to make decisions for. Two algorithms are provided. The first amounts to estimating a minimum cluster distance, constructing a graph where two users are connected if their estimated parameters are closer than the minimum distance minus a confidence bound penalty, and then pooling data for users adjacent to the test user together when making decisions for the test user. The second is deferred to the appendix. Numerical experiments show outperformance relative to online algorithms, but not offline ones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors consider a salient problem of pooling data from heterogenous users together to make decisions in adapting to a new user. The paper is largely rigorously written, and the proofs appear to be sound. The experiments are involved, especially for a theory paper."}, "weaknesses": {"value": "1. The authors are very much unaware of a large body of existing literature in this field. \n- The \"clusters of bandits\" problem is essentially an offline version of the latent bandit problem. See Hong et al. (2020), who first learn clusters from offline data before taking actions online. Shi et al. (2023) discuss offline latent RL, which is a variant of the latent MDP setting of Kwon et al. (2021). \n- Rigorous guarantees for clustering tabular MDPs under the Markovian setting have been obtained by Kausik et al. (2023), with application to learning a policy from an offline dataset of clustered users in Kausik et al. (2024). The same authors deal with offline data in the linear latent contextual bandit setting in Kausik et al. (2025), adapting to a test user given data from other users as in this paper, but for online exploration. \n- Clusters of linear bandit instances, or the more broad multi-task setting, have also been addressed in the following:\n    - Yang et al. (2021) discuss a setting where the learner can play $k$ linear bandits in dimension $d$ concurrently, with a shared representation. \n    - Hu et al. (2021) solve the aforementioned problem, except with infinite actions, with a projected LinUCB-like algorithm, and extend it to reinforcement learning.\n    - Yang et al. (2022) close a gap that Yang et al. (2021) leave in the infinite action setting. \nThese are highly relevant, and should be included in the paper. This is a non-exhaustive list. It is highly alarming that the authors do not seem to be aware of this body of work. This renders some of their claims of novelty suspect. \n2. Accordingly, this influences the guarantees that the authors provide. \n- The algorithm is simple and barely involves any clustering. It amounts to connecting any two user if the estimated parameters are closer than the estimated minimum distance minus a confidence bound, and there is sufficient data for each user. Data from users connected to the test user is pooled together to form a pessimistic estimate of the test user's reward parameter, from which an action is chosen. \n- No data from non-neighbors of the test user is used, which renders the computation for those vertices moot. The algorithm would have been better presented as \"Select all users close enough to the test user, then pool their data to form a pessimistic estimate\" -- though this looks less impressive. This can be done lazily per test user, with the result cached for future use. The precomputation is not necessary. \n- Algorithm 1 peculiarly requires an estimate of the minimum gap between clusters. This is never known in practice, so it is odd that the authors consider the case where it is. The guarantees for the underestimation and overestimation policies are rather weak, and only discussed in the appendix. Other work (e.g. Kausik et al. (2023)) estimates a similar quantity from data and provides an end-to-end guarantee.\n    - Given the minimum gap, the problem is then very simple! It reduces to finding all users that are within the minimum gap minus a confidence-based penalty to be conservative -- which is exactly what Algorithm 1 is. There is no clustering that needs to be done whatsoever. \n    - A few other quantities (e.g. the smoothed regularity parameter) are also assumed to be known, which is quite a strong assumption.\n    - Other assumptions include full coverage over the action space (i.e. a full-rank covariance matrix with bounded minimum eigenvalue), and actions within the offline data that are independently drawn from some distribution. I believe that it is folklore that these may be necessary, but why exactly this is necessary is never discussed beyond appealing to prior work. \n- Algorithm 2 is named Off-CLUB but addressed as Off-C^2LUB on Line 1508. \n \n\n### References\n- Hong et al. (2020), Latent bandits revisited\n- Shi et al. (2023), Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources\n- Kwon et al. (2021), RL for latent mdps: Regret guarantees and a lower bound.\n- Kausik et al. (2023), Learning mixtures of Markov chains and MDPs\n- Kausik et al. (2024), Offline Policy Evaluation and Optimization under Confounding\n- Kausik et al. (2025), Leveraging Offline Data in Linear Latent Contextual Bandits\n- Yang et al. (2021), Impact of representation learning in linear bandits\n- Hu et al. (2021) Near-optimal representation learning for linear bandits and linear RL \n- Yang et al. (2022) Nearly minimax algorithms for linear bandits with shared representation"}, "questions": {"value": "1. How does your paper relate to the existing literature?\n2. Can Algorithm 1 be simplified as above to nearest neighbor search?\n3. Can you provide any offline bandit baselines?\n\nThis score is somewhat harsh. At the moment, I am on the fence between a 2 and a 4, and am willing to increase my score if I am proven wrong or my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F5TjgbTCPF", "forum": "bNanN941dw", "replyto": "bNanN941dw", "signatures": ["ICLR.cc/2026/Conference/Submission15255/Reviewer_dbFf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15255/Reviewer_dbFf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866128274, "cdate": 1761866128274, "tmdate": 1762925553275, "mdate": 1762925553275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies an offline, single-shot decision problem for contextual linear bandits under user heterogeneity: given historical logs across many users, first cluster users and then choose one action for a fresh test user to minimize the suboptimality gap. Two algorithms are proposed: Off-C2LUB (build edges among close users, aggregate one-hop neighbors, pessimistic decision) and Off-CLUB (start from a complete graph and remove edges, better when each user has sufficient data). Experiments on synthetic data and two public datasets suggest Off-C2LUB is robust under sparse logs while Off-CLUB improves with abundant data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The ''cluster from logs → act once'' framing is well-motivated for applications where online interaction is limited.\n\n2: The empty-graph vs complete-graph constructions align with low-data vs high-data regimes and are easy to implement.\n\n3: The analysis surfaces a bias–variance trade-off around the clustering threshold and identifies a data-sufficiency regime where the complete-graph pruning approach performs well.\n\n4: Results across synthetic and real data broadly match the narrative."}, "weaknesses": {"value": "1: Limited technical novelty. The methods largely assemble standard components—ridge regression with confidence sets, distance-threshold user graphs, and pessimistic action selection. The paper’s contribution is more in problem formulation and tidy integration than in new algorithmic primitives or estimation techniques. The one-hop aggregation choice (vs. full component) is interesting but not theoretically pinned down as a strict improvement beyond intuition and ablations.\n\n2: Strong regularity assumptions on actions. Several results assume known or well-behaved action covariances (or equivalent surrogates). In real logs, action distributions are policy-induced, user-dependent, and unknown. The paper does not provide learnable/estimable versions with concentration guarantees, leaving a gap between assumptions and practice.\n\n3: Hyperparameter tuning and validation leakage. The threshold selection policies are central, yet the tuning protocol (grids, validation splits, early stopping, cross-dataset reuse) is under-specified. It’s unclear whether any distributional information from the evaluation setup leaks into selection, and sensitivity to dimension and cluster separation is not systematically reported.\n\n4: Offline–online mismatch not considered. The paper assumes the test-time environment matches the logged data (action/context coverage, reward noise/scale), but provides no diagnostics or safeguards against distribution shift (covariate, reward, or action-set geometry) and no propensity-aware debiasing (IPS/DR) to mitigate policy-induced bias. This risks extrapolation beyond the support of the logs."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rvtHauhLtZ", "forum": "bNanN941dw", "replyto": "bNanN941dw", "signatures": ["ICLR.cc/2026/Conference/Submission15255/Reviewer_1WaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15255/Reviewer_1WaV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762419952316, "cdate": 1762419952316, "tmdate": 1762925552609, "mdate": 1762925552609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers a offline clustered linear bandit data , when there is unknown bandit parameter vector in $d$ dimensions per user and arms are sampled from a distribution whose covariance matrix has a non trivial lowest eigenvalue $\\lambda_a$. There are $U$ different users. Further, the unknown bandit parameter vectors across users are clustered, that is there are effectively only $J$ distinct parameter vectors but the membership of each cluster is unknown. There $N_u$ data points per user $u$ of rewards which is a linear dot product between unknown parameter vector for that user and the arm vector pulled with additive SubGaussian noise. \n\nThe question is when a test user from this comes at test time, and a set of arms are given to you to chose from, what is the best arm you would chose depending on your estimate for the parameter vector. Clearly, offline data per user is limited and therefore one needs to accumulate data across users in the same cluster. But there may not be enough data to cluster them accurately. \n\nPaper take the approach building a graph from an empty graph where an edge is put if the individual parameter vector estimate + confidence interval from linear gaussian concentration results over lap with another user's. There is a parameter $\\hat{\\gamma}$ that parameterizes the overlap or nearness of these intervals. The consideration is simple pariwise. For the final decision, the authors only look at 1-hop neighbors to accumulate all data and create a new estimate assuming all those come from the same parameter vector. \n\nThen based on this and some confidence estimate, the best arm is chosen. Main key ideas is that authors are careful not to introduce bias by considering heterogenous users in the same cluster causing estimate bias which is hard to overcome. The parameter $\\hat{\\gamma}$, fact of building up from the empty graph and using only one hop neighbors keep the algorithm conservative on the bias side. \n\nVery elaborate results as a function of $\\hat{\\gamma}$ is derived."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper certainly considers the offline version of the clustered linear bandit setup.\n\n2) Paper is wary of the bias due to fixed but limited data forcing heterogenous users into clusters causing issues in parameter estimates for the decision.  Thats is interesting and commendable.\n\n3) Error is decomposable as a $O(1/\\sqrt{\\lambda_a N})$ term where $N$ is the set of homogenous users identified in the test user's cluster and a bias term that depends on the inclusion of heterogenous users."}, "weaknesses": {"value": "1)  The whole machinery revolves around standard concentration of a linear gaussian model with sub gaussian noise under the case when data matrix has lowest eigenvalue bounded below.  Everything is a more detailed manipulation of the confidence estimates with a clustering routine that aggregates users with similar parameter estimates upto a confidence estimate. While the approach to be cautious with respect to clustering heterogenous users reflected in bounds and the approach, I am quite unclear about non trivial ideas in the paper. \n\n\n2) The lower bounds appears non trivial but is satisfied when there is enough data  for all users cluster correctly with high probability. What would have been useful is to have a lower bound that reflects the bias term as well.  Here again, the nontrivial nature of this bound is not clear at all.\n\n3) The assumption of arms being sampled such that the covariance is lower bounded by a some known minimum eigenvalue is also rather strong. Usually if the data is obtained from another online bandit instance, arms closer to the best arm will be sampled more giving not very non-trivial lower bounds on the lowest  eigenvalue of the Gram matrix of arms for a user. This assumptions hides such difficulty.\n\n\n4) Related works on latent bandits with cluster structure (like https://arxiv.org/abs/2301.07040, https://arxiv.org/abs/2306.13053) are missing. While these papers explore the online case, the discussion is limited to some old works and not these recent ones."}, "questions": {"value": "1) Is it possible to extend the lower bound to the case when at finite sample, the bias term is also reflected ?\n\nDepending on authors responses to my questions and my concerns expressed in the weakness section, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SPfGnb8W7F", "forum": "bNanN941dw", "replyto": "bNanN941dw", "signatures": ["ICLR.cc/2026/Conference/Submission15255/Reviewer_R7Pr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15255/Reviewer_R7Pr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762897979921, "cdate": 1762897979921, "tmdate": 1762925552036, "mdate": 1762925552036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}