{"id": "yo9WIIecAu", "number": 11743, "cdate": 1758203471130, "mdate": 1759897557599, "content": {"title": "Efficient Multi-View 3D Representation via Fusion of View-Agnostic Transformations", "abstract": "Bird's-Eye View representations are essential for 3D perception in autonomous driving, providing unified and spatially coherent scene understanding. While attention-based methods achieve strong performance through global cross-view attention, they suffer from computational inefficiencies due to redundant referencing and spatial ambiguity from ego-centric projections. To address these limitations, we introduce Mosaic View Transformation (MosaicVT), a modular framework that independently transforms multi-camera views into a unified BEV space. MosaicVT employs a camera-centric polar coordinate system, effectively resolving directional ambiguity and reducing cross-view redundancy. A novel view-agnostic positional embedding enables a single transformation module to generalize across heterogeneous camera configurations without retraining. Transformed camera-centric representations are then aligned and fused into a global BEV using a geometry-aware interpolation strategy, significantly reducing computational overhead compared to global attention mechanisms. Experimental results on the nuScenes benchmark demonstrate that MosaicVT achieves state-of-the-art performance in 3D object detection and BEV semantic segmentation while providing substantial reductions in latency and maintaining robust generalization across diverse camera setups.", "tldr": "", "keywords": ["View Transformation", "Bird's-Eye-View Representation", "Multi-View Representation", "View-Agnostic"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd162d643fd55c68e4c949f9eef708e826675306.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a novel framework for efficient multi-camera BEV representation in autonomous driving. \nThe method addresses redundancy in transformer-based view transformation by processing each camera view independently in a camera-centric polar coordinate system. \nKey innovations include View-Agnostic Positional Embedding (VAPE) for consistent 3D localization and a modular transformation-fusion pipeline that avoids cross-view attention. \nExperiments on nuScenes show that MosaicVT outperforms existing methods in 3D detection and BEV segmentation while reducing latency. \nIt also demonstrates robustness to camera perturbations and configuration changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Computational Efficiency and Reduced Redundancy. MosaicVT eliminates the computationally expensive global cross-view attention used in transformer-based methods. \n2. Novel and Robust Formulation. The introduction of the camera-centric polar coordinate system and View-Agnostic Positional Embedding (VAPE) effectively resolves spatial ambiguity inherent in image-to-BEV transformation.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The method is based on the assumption that cross-view interaction is not important for global BEV. It might be true for nuScenes with little FoV overlap between cameras. However, the paper does not take cases into consideration where there might be considerable FoV overlap between surround cameras.\n2. The paper does not compare against recent state-of-the-art methods for object detection and BEV segmentation."}, "questions": {"value": "1. As in the weakness section, could you please discuss the validity of the assumption that cross-view attention is negligible?\n2. Why do you only compare with methods that is more than one year from now?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vq9bhUuyOP", "forum": "yo9WIIecAu", "replyto": "yo9WIIecAu", "signatures": ["ICLR.cc/2026/Conference/Submission11743/Reviewer_Rq2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11743/Reviewer_Rq2n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467043865, "cdate": 1761467043865, "tmdate": 1762922775609, "mdate": 1762922775609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modular framework, MosaicVT, that generates a unified BEV representation by independently transforming multiple camera viewpoints. MosaicVT uses a central camera polar-coordinate system to address directional ambiguity and multi-view redundancy effectively. Through novel view-independent position embeddings, MosaicVT can generalize to different camera configurations without retraining. The generated camera-centered BEV representations are aligned and fused using a geometry-aware interpolation strategy, thereby significantly reducing computational overhead while maintaining high accuracy and robustness. Experimental results show that MosaicVT achieves state-of-the-art performance on 3D object detection and BEV semantic segmentation tasks on the nuScenes benchmark, while significantly reducing latency and performing robustly across different camera configurations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Different from previous methods that uniformly process the global view, we propose a method that processes each camera view independently and then aggregates them into a unified BEV space, effectively reducing cross-view interference and spatial ambiguity while avoiding the computational overhead of the global attention mechanism.\nThe method introduction is clear and logical. The authors explain the core idea through detailed formula derivation and provide complete mathematical proofs for key details.\nIn the experimental part, the effectiveness of introducing camera-centered polar coordinates as position embedding is effectively proved by detailed comparison experiments and ablation experiments"}, "weaknesses": {"value": "1. Using polar coordinates for position encoding instead of Cartesian coordinates is actually a relatively common idea, which may not be very innovative.\n2. When feature conflicts exist between different views, simple averaging and 2D convolution lack a dynamic arbitration mechanism to intelligently decide which view should be preferentially adopted, which may perform poorly when dealing with complex occlusions and view conflicts compared with more advanced fusion modules, such as attention-based fusion modules."}, "questions": {"value": "Compared with the previous unified global-view processing method, is the weighted-average fusion method and 2D convolution adopted in this paper too simple? \nAlthough this lightweight design is efficient, is its ability to aggregate information sufficient? \nAre there more advanced fusion methods (e.g., cross-view attention) that can handle information interaction between views more robustly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V8jGiBMkN7", "forum": "yo9WIIecAu", "replyto": "yo9WIIecAu", "signatures": ["ICLR.cc/2026/Conference/Submission11743/Reviewer_hBDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11743/Reviewer_hBDg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549295977, "cdate": 1761549295977, "tmdate": 1762922775039, "mdate": 1762922775039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MosaicVT, a BEV view-transformation module that (i) transforms each camera independently in a camera-centric polar frame (angles θ, radial distance r, and height h), (ii) makes the transformation view-agnostic via a positional encoding that uses relative geometric cues so one shared module can generalize across heterogeneous camera setups, and (iii) aligns & fuses the per-camera BEV “tiles” into a global BEV using geometry-aware interpolation—avoiding global cross-view attention altogether"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper pinpoints two concrete issues—redundant cross-view referencing and ego-centric ray ambiguity and ties each to a design choice (per-camera VT + camera-centric polar coordinates), giving a clean problem, method story. \n\nMosaicVT swaps only the view transformation stage inside standard pipelines (e.g., BEVDet for detection, BEVFusion for BEV segmentation), leaving heads/decoders unchanged—useful for real systems."}, "weaknesses": {"value": "1. Motivation:\n  a. The paper says prior view-transform “uses global cross-view attention,” but BEVFormer uses deformable (sparse) cross-attention from BEV queries, and LSS lifts frustums then splats to BEV—no global attention. Please clarify.\n  b. The “A core challenge of this design is ensuring that a single transformation module can operate across heterogeneous camera setups” claim lacks a baseline analysis: why can’t BEVFormer/LSS work across heterogeneous camera?\n\n2. Performance:\n  a. Temporal length/stride/cache aren’t specified, and some numbers seem below commonly reported BEVFormer-base ≈ 0.517 NDS / 0.416 mAP on nuScenes—please detail the exact setting and explain the gap.\n  b. Limited comparison coverage. Add more recent multi-view detectors (or justify exclusions) (e.g. Far3D, BEVNeXt, GeoBEV)"}, "questions": {"value": "1. In Table 9, why Camera $8\\times 22$ results in poor performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NSYwWbuVN0", "forum": "yo9WIIecAu", "replyto": "yo9WIIecAu", "signatures": ["ICLR.cc/2026/Conference/Submission11743/Reviewer_bMFc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11743/Reviewer_bMFc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665070907, "cdate": 1761665070907, "tmdate": 1762922774491, "mdate": 1762922774491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MosaicVT, a modular framework for multi-camera BEV representation learning, aiming to address the computational inefficiency from cross-view redundancy and spatial ambiguity in ego-centric projections of existing attention-based methods. Extensive experiments on the nuScenes benchmark demonstrate that MosaicVT achieves competitive performance in 3D object detection and BEV semantic segmentation, with substantially reduced latency compared to transformer-based methods. It also exhibits strong robustness to variations in camera configuration and calibration noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. MosaicVT processes each camera view independently using a camera-centric polar coordinate system, which avoids the unnecessary global cross-view attention in transformer-based methods.\n2. The proposed VAPE embeds image features using relative geometric cues, abstracting away camera-specific parameters. This enables a single transformation module to adapt to diverse camera setups without retraining.\n3. Experiments on simulated camera configuration changes and real-world calibration noise  show that MosaicVT outperforms WidthFormer and LSS in robustness."}, "weaknesses": {"value": "1. The method of converting image features into Polar BEV and then obtaining BEV features through sampling is somewhat similar to the RC-Sample proposed by GeoBEV[1]. The advantages of MosaicBEV need to be further demonstrated.\n2. MosaicBEV has not been compared with current SOTA methods, such as RayDN[2], BEVNext[2] and so on.\n3. In Table 4, MosaicBEV is only compared with WidthFormer in terms of efficiency. The efficiency comparison with LSS-based methods should also be added.\n\n[1] Zhang, Jinqing, et al. \"Geobev: Learning geometric bev representation for multi-view 3d object detection.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 9. 2025.\\\n[2] Liu, Feng, et al. \"Ray denoising: Depth-aware hard negative sampling for multi-view 3d object detection.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\\\n[3] Li, Zhenxin, et al. \"Bevnext: Reviving dense bev frameworks for 3d object detection.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "questions": {"value": "4. DFA3D also uses depth distribution to avoid the problem of depth uncertainty. What advantages does MosaicBEV have compared with DFA3D?\n5. In Figure 7, why is the impact of vertical crop on LSS greater than that of horizontal crop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7eTx6JcmGq", "forum": "yo9WIIecAu", "replyto": "yo9WIIecAu", "signatures": ["ICLR.cc/2026/Conference/Submission11743/Reviewer_5ZeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11743/Reviewer_5ZeX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919811793, "cdate": 1761919811793, "tmdate": 1762922774080, "mdate": 1762922774080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}