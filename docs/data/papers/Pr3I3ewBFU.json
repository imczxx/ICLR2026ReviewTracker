{"id": "Pr3I3ewBFU", "number": 12714, "cdate": 1758209667417, "mdate": 1763762327650, "content": {"title": "Training-Free Adaptive Frame Selection for Video-Language Understanding", "abstract": "Multimodal Large Language Models (MLLMs) have shown strong performance on image understanding tasks, but video comprehension remains a significant challenge due to the high computational cost of processing long frame sequences and the limited token capacity of underlying Large Language Models (LLMs). Prior approaches to address this often rely on uniform frame sampling, query-agnostic pruning, or require costly training of dedicated compression modules. In this work, we introduce CoSeLECT, a training-free, plug-and-play, query-guided frame selection method that intelligently subsamples video frames for efficient use in MLLMs. CoSeLECT leverages two key signals: temporal redundancy, which identifies similar frame clusters, and query relevance, which selects frames based on their semantic alignment with the input query. By combining these signals through an adaptive frame selection strategy, CoSeLECT selects frames that are both diverse and highly relevant to the query, without requiring any model-specific tuning. Our results on various base MLLMs show that CoSeLECT consistently outperforms state-of-the-art methods, including trained methods like LongVU by +3.8\\% on MVBench and +0.8\\% on VideoMME.", "tldr": "Training-Free Adaptive Frame Selection to improve efficiency and accuracy of Video Large Language Models", "keywords": ["Video Language Understanding", "Efficient Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29ee0e54f071b6ecc18984172ca37d723d279fa1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CoSeLECT, a training-free adaptive frame selection method that efficiently selects the most informative frames from a large pool by combining query relevance and temporal continuity. This approach achieves better performance than existing training-free methods on multiple video understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear and practical training-free frame selection method that combines query relevance and visual continuity in a straightforward manner. While the individual components are not novel, their combination into an adaptive, query-aware selection pipeline is sensibly designed and effectively executed. The method is well-described, easy to reproduce, and evaluated across multiple benchmarks with ablations that support key design choices. Its significance lies in offering a lightweight, plug-and-play solution that improves efficiency and performance for video understanding with MLLMs without requiring model retraining. This is useful for real-world deployment, though not theoretically groundbreaking."}, "weaknesses": {"value": "- The novelty of the paper is limited. Query-aware frame selection for Videl-LLM is not innovative, as discussed in KeyVideoLLM [1], AKS [2], and Q-Frame [3]. The paper pointed out that `these heavier methods are typically limited to sparsely pre-sampled frame pools` is interesting, but the experiment did not support the solution of this problem.\n\n- It is not clear whether the comparison of the experimental results in the paper with other training-based methods is fair.\n\n- The paper claims the proposed CoSeLECT is lightweight. However, there is a lack of systematic evaluation of latency and computing consumption, which is crucial for actual deployment.\n> but in its lightweight, principled fusion of two readily available ones—frame–text similarity for semantic relevance and inter-frame similarity for temporal continuity.\n\n- Lack of discussion of limitations.\n\n- Minor weaknesses\n  - The first equation in Section 3.4 is missing a number\n  - $\\sqrt{D_i}$ in equation (4) lacks definition\n\n[1] Liang H, Li J, Bai T, et al. Keyvideollm: Towards large-scale video keyframe selection[J]. arXiv preprint arXiv:2407.03104, 2024.\n[2] Tang X, Qiu J, Xie L, et al. Adaptive keyframe sampling for long video understanding[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 29118-29128.\n[3] Zhang S, Yang J, Yin J, et al. Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs[J]. arXiv preprint arXiv:2506.22139, 2025."}, "questions": {"value": "- I am confused about the avoidance of redundant calculations mentioned in the article in line 204. Although  LLaVA-OneVision uses SigLIP-So400M-patch14-384 as the visual encoder, it fine-tunes SigLIP during the training process, which results in them having the same structure but different parameters. So is it really possible to avoid redundant calculations? \n> Since SigLIP is also used as the vision encoder in LLaVA-OneVision (Li et al., 2024a), these embeddings can be directly reused, avoiding redundant computation.\n\n- Does pre-$\\textbf{E}_{im}$  in section 4 mean $N$ in section 3, and post-$\\textbf{E}_{im}$  means  $K$? If so, please use consistent expressions to improve reading; if not, I hope the author can further elaborate on the difference.\n\n- Lack of in-depth analysis of Table 1. With the increase of pre-$\\textbf{E}_{im}$, there is no consistent performance improvement across all benchmarks. Does this contradict the motivation of the paper?\n> Crucially, these heavier methods are typically limited to sparsely pre-sampled frame pools in order to remain computationally feasible—risking the permanent loss of “needle-in-a-haystack” moments before the selection algorithm can even evaluate them, a limitation that becomes particularly acute under resource constraints.\n\n- I am confused about the experimental results in Table 2. Is this comparison meaningful?\n  - Frame-Voyager is similar to the proposed CoSeLECT, and is also a plug-and-play model. But its LLM Size does not seem to be 7B. \n  - LongVA and VideoChat2 are Video-LLMs. How to compare them with CoSeLECT ? \n\n- Supplement CoSeLECT compares the results of the Qwen2-VL [1] experiment with AKS and Q-Frame [2], which will provide a more comprehensive evaluation. \n\n[1] Wang P, Bai S, Tan S, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution[J]. arXiv preprint arXiv:2409.12191, 2024.\n\n[2] Zhang S, Yang J, Yin J, et al. Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs[J]. arXiv preprint arXiv:2506.22139, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Nyp52AoR6W", "forum": "Pr3I3ewBFU", "replyto": "Pr3I3ewBFU", "signatures": ["ICLR.cc/2026/Conference/Submission12714/Reviewer_Lg9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12714/Reviewer_Lg9P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110132110, "cdate": 1761110132110, "tmdate": 1762923541807, "mdate": 1762923541807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Remarks by Authors"}, "comment": {"value": "We thank all reviewers for their thoughtful and positive feedback. We are particularly encouraged that they found CoSeLECT:\n\n* **Simple, training-free, and broadly applicable.** It is described as *“simple but effective”* and *“training-free”*, *“can be seamlessly integrated into existing MLLMs”*, *“model-agnostic”*, *“can be applied to any LVLMs”*, and a *“lightweight, plug-and-play solution… useful for real-world deployment”* (CaVy, vvkc, AAbb, Lg9P).\n\n* **Empirically strong and well evaluated.** Reviewers note that CoSeLECT *“achieves state-of-the-art performance across multiple video understanding benchmarks”*, *“better performance than existing training-free methods”*, is *“more flexible than AKS… competitive with training-based methods such as LongVu”*, and *“state-of-the-art… compared with both training-free and fine-tuned methods”*, with *“convincing and comprehensive”* experiments, *“extensive comparison”*, *“good ablation”*, and *“evaluation on six video understanding benchmarks”* (CaVy, vvkc, AAbb, Lg9P).\n\n* **Clear and reproducible.** The paper is called *“well-written and easy to follow”* and *“well-described, easy to reproduce”*, with *“clear visualizations that illustrate the method’s contribution”* (CaVy, vvkc, AAbb, Lg9P).\n\nThe requested changes mainly led to focused clarifications and modest additions, rather than major overhauls, which we take as a positive signal that the core approach and empirical findings were already in solid shape.\n\nBelow, we address the main concerns regarding novelty and computational overhead in detail.\n\n---\n\nWe see two main overall themes in the feedback:\n\n* **Theoretical Novelty.**  \n  We agree that using query–frame and frame–frame similarity as *signals* is not new. Our contribution is not a new similarity measure, but a **concrete, training-free allocation scheme** that specifies *how* to fuse these signals into a single-stage, query-aware budgeting mechanism over temporally coherent segments. CoSeLECT:\n  1. **Segments by visual continuity**, so selection operates over coherent temporal units instead of isolated frames;\n  2. Computes a **composite segment score** (max + mean text–frame similarity);\n  3. Uses a **duration-aware frame budget** and then **text-guided selection within segments**.\n\n  Ablations (Table 5, Appendix B) show that removing any component (only text, only continuity, no duration term, or simpler scoring) consistently harms performance, indicating that the contribution lies in the **overall allocation pipeline**, not a single heuristic.\n\n  Beyond this design, CoSeLECT is **empirically strong**, leading in 5/6 benchmarks among training-free methods and outperforming fully trained approaches such as LongVU [1] under the same backbone. It is **decoupled from any specific vision encoder or LLM**, showing consistent gains across backbones, and has a **straightforward implementation** with ablations that make its behavior and trade-offs transparent. Finally, we expose and systematically study **interpretable knobs**—the size of the pre-embedding frame pool, the number of frames fed to the LLM, and the choice of LLM / vision backbones—providing actionable guidance for practitioners (e.g., Section B.4 shows that *“more FPS is not always better”* once selection is well designed).\n\n* **Computational overhead.**  \n  While CoSeLECT does require encoding a large pool of frames, this is **in line with existing state-of-the-art methods** such as AKS [2], LongVU [1], BOLT [4], and other FPS-based approaches: any method that must *reason* about which frames to keep generally needs to encode a sizeable candidate pool. In practice, the **dominant cost** is shared with these methods: running the vision encoder and the subsequent LLM forward pass.\n\n  The *additional* cost introduced by CoSeLECT is only an $O(N)$ similarity computation over the pre-embedded frames (pairwise vision–vision and vision–text cosine similarities). As quantified in Appendix C.1, for the largest setting with pre-$E_{\\text{im}} = 1600$ frames, the similarity computation takes **1.49 ms** out of **1.87 s** total inference time (≈**0.08%**) per query. Thus, while CoSeLECT operates over a larger candidate pool, the **extra overhead due to the selection logic itself is negligible** compared to the shared cost of vision and LLM inference.\n  \n---\n\n#### References\n\n[1] Shen X, Xiong Y, Zhao C, et al. LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. *arXiv preprint arXiv:2410.17434*, 2024.  \n[2] Tang X, Qiu J, Xie L, et al. Adaptive Keyframe Sampling for Long Video Understanding. *arXiv preprint arXiv:2502.21271*, 2025.  \n[3] Zhang S, Yang J, Yin J, et al. Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs. *arXiv preprint arXiv:2506.22139*, 2025.  \n[4] Wang Y, Xu M, Gao M, et al. BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding. *arXiv preprint arXiv:2503.21483*, 2025."}}, "id": "nWCL3vhlcJ", "forum": "Pr3I3ewBFU", "replyto": "Pr3I3ewBFU", "signatures": ["ICLR.cc/2026/Conference/Submission12714/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12714/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12714/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763761538702, "cdate": 1763761538702, "tmdate": 1763762475692, "mdate": 1763762475692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free query-guided frame selection method for efficient video processing in MLLMs. It uses SigLIP cosine similarity between each frame and the given query to measure query relevance. Then it identifies scene transitions based on visual similarity. Based on these, the method adaptively allocate tokens to those scenes that is more relevant to the queries via relevance reweighting. CoSeLECT evaluates on six video understanding benchmarks and achieves state-of-the-art performance compared with both training-free and fine-tuned methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper writing is clear and easy to follow.\n+ The method is training-free and can be applied to any LVLMs.\n+ The method improves the performance on base model LLaVA-OV and Qwen2.5-VL-7B. It also outperforms other frame selection methods."}, "weaknesses": {"value": "+ The paper should compare with more video token compression or frame selection method. For example, BOLT [1] is a frame selection method.\n\n+ When retrained ratio goes down to 12.5%, CoSeLECT has several benchmarks lower than FastVID.\n\n+ Although comprehensive evaluation has been down, the paper is mainly based on empirical observation and has very limited innovation.\n\n+ The comparison does not seem entirely fair. Although 8k video tokens are finally fed into the MLLM, it still need to process additional frames during intermediate steps. Given the method involves intermediate steps and introduces computational overhead, the comparison should be made against the base model’s best performance. For example, Qwen2.5-VL got 65.1 on VideoMME.\n\n[1] BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding"}, "questions": {"value": "+ When comparing with baseline models LLaVA-OV  and Qwen2.5-VL-7B, how many frames and token per frame is used within 8k context length?\n+ Have you tried LLM’s text embedding instead of SigLIP text embedding? \n+ Some complicated question could not be used to select key frames based on embedding similarity. For example, many questions in VideoMME are like ‘which of the statement is correct?’ Therefore, query-frame embedding similarity is not a fine-grained way for frame selection.\n+ What is the computation overhead introduced and inference speed compared with base model, since the method needs to calculate similarity between consecutive frame embeddings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8XrbDLttq3", "forum": "Pr3I3ewBFU", "replyto": "Pr3I3ewBFU", "signatures": ["ICLR.cc/2026/Conference/Submission12714/Reviewer_AAbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12714/Reviewer_AAbb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427770890, "cdate": 1761427770890, "tmdate": 1762923541478, "mdate": 1762923541478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces another method to select which frames to use for video-language understanding. This is an important topic since MLLMs often have input token limitations, and having some way to prefilter the data to find the most relevant answer can often help in increasing the results. The method is training-free and can be plugged with different models. Their main competitor is AKS (Adaptive Keyframe Sampling) that also adopts a training-free paradigm. However, there are some differences with the method proposed here. Whereas AKS splits the video into equal halves, CoSeLECT segments the video into different lengths depending on intra-clip similarity. The method introduced by the authors seems a bit more flexible than AKS while providing slightly better results. The authors evaluate their method across several common benchmarks such as NextQA, MLVU, VideoMME, MVbench, and LongVideoBench. They show that their method is competitive with training-based methods such as LongVu. They also compare their method with different token reduction techniques such as uniform sampling, VisionZip, PruneVid, and others, for which they also have competitive results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A simple and yet effective training-free method for frame selection\n- Adaptative and not relying on specific video sub-clip size\n- Paper well-written\n- Extensive comparison with similar methods such as AKS\n- Extension comparison with both training-based and training-free frame selection method\n- Good ablation over the different hyper-parameters such as similarity threshold or frame pool size"}, "weaknesses": {"value": "- Some overhead introduced by the method since frames need to be processed through a SigLip encoder to compute frame similarity. Depending on the number of frames being processed, this can have an important impact even if this operation can be parallelised. \n- Lack of ablation over the vision and text encoder.\n- The paper title and abstract does not exactly match the ones in OpenReview (don't know how much that can be an issue or not)"}, "questions": {"value": "Why choosing SigLIP-ViT and not another model? Did you perform an ablation on those?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BFnBnHKWh3", "forum": "Pr3I3ewBFU", "replyto": "Pr3I3ewBFU", "signatures": ["ICLR.cc/2026/Conference/Submission12714/Reviewer_vvkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12714/Reviewer_vvkc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581290237, "cdate": 1761581290237, "tmdate": 1762923541172, "mdate": 1762923541172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method that can be seamlessly integrated into existing multimodal large language models (MLLMs). The approach jointly considers both frame-level visual diversity and overall video length, leading to more balanced and informative video representations. The method achieves state-of-the-art performance across multiple video understanding benchmarks, demonstrating both simplicity and effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is simple but effective, requiring no additional training while significantly improving performance.\n\n2. The design is model-agnostic and can be easily plugged into various MLLM architectures, indicating strong generality and practical utility.\n\n3. Experimental results are convincing and comprehensive, covering multiple datasets and metrics, with clear visualizations that illustrate the method’s contribution.\n\n4. The paper is well-written and easy to follow, making the technical insights accessible."}, "weaknesses": {"value": "1. The main concern lies in the limited novelty of the method. The use of text–visual embedding similarity as a selection strategy is not conceptually new and has been widely seen in prior works as an auxiliary component or ablation. While the empirical results are strong, the contribution is mainly engineering-level, lacking deeper methodological insight or theoretical advancement. \n\n2. In addition, the paper does not clearly explain how the method mitigates temporal information loss when modeling long videos."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hf3wrJEN0r", "forum": "Pr3I3ewBFU", "replyto": "Pr3I3ewBFU", "signatures": ["ICLR.cc/2026/Conference/Submission12714/Reviewer_CaVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12714/Reviewer_CaVy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899553114, "cdate": 1761899553114, "tmdate": 1762923540858, "mdate": 1762923540858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}