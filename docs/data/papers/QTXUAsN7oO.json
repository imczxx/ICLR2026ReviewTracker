{"id": "QTXUAsN7oO", "number": 7586, "cdate": 1758028706823, "mdate": 1759897844804, "content": {"title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation", "abstract": "Accurate and efficient Video Quality Assessment (VQA) has long been a key research challenge. Current mainstream VQA methods typically improve performance by pretraining on large-scale classification datasets (e.g., ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this strategy presents two significant challenges: (1) merely transferring semantic knowledge learned from pretraining is insufficient for VQA, as video quality depends on multiple factors (e.g., semantics, distortion, motion, aesthetics); (2) pretraining on large-scale datasets demands enormous computational resources, often dozens or even hundreds of times greater than training directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown remarkable generalization capabilities across a wide range of visual tasks, and have begun to demonstrate promising potential in quality assessment. In this work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP enhances both visual and textual representations through a Shared Cross-Modal Adapter (SCMA), which contains only a minimal number of trainable parameters and is the only component that requires training. This design significantly reduces computational cost. In addition, we introduce a set of five learnable quality-level prompts to guide the VLMs in perceiving subtle quality variations, thereby further enhancing the model’s sensitivity to video quality. Furthermore, we investigate the impact of different frame sampling strategies on VQA performance, and find that frame-difference-based sampling leads to better generalization performance across datasets. Extensive experiments demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets. Code is provided in the supplementary material.", "tldr": "", "keywords": ["Video quality assessment", "Vision-language model", "Cross-modal adapter"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4c0bb6786feee08a27349f5393f222816c160e5.pdf", "supplementary_material": "/attachment/0e7fa91f159e08e7f0fc628907f2acb2a29150bb.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript introduces Q-CLIP, a framework for VQA built on CLIP. To enhance the model's performance, the authors designed a lightweight Shared Cross-Modal Adapter (SCMA) which is the only part of the model that needs to be trained. The framework also utilizes a set of five learnable, quality-level text prompts (excellent, good, fair, poor, bad) to help the model better discern subtle differences in video quality. Additionally, the authors explore different frame-sampling strategies, finding that methods based on frame differences improve the model's generalization. Extensive experiments show that Q-CLIP achieves state-of-the-art performance on multiple VQA datasets while being computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is generally well-written, presenting the proposed methodology and results in a clear and organized structure.\n\n2. The experimental evaluation appears sufficient, providing empirical evidence to support the claims made regarding the model's performance.\n\n3. The proposed model, Q-CLIP, demonstrates improvements in VQA performance while simultaneously reducing the computational costs associated with training and inference compared to existing methods"}, "weaknesses": {"value": "The claimed novelty of the VLM-based approach is undermined by an evaluation scope that neglects key VLM-specific advantages. \n1. Unlike models such as Q-Align which show robustness with limited data, this capability isn't explored for Q-CLIP. \n2. The potential for generating reasoned quality assessments, a feature present in models like Q-insight and VisualQuality-Q1, is not investigated."}, "questions": {"value": "1. The claim that Q-CLIP is the \"first VQA model fully based on VLMs\" requires further clarification and justification. Recent methods, such as Q-Align, also appear to be constructed entirely upon VLM architectures. The authors should explicitly differentiate their framework from such existing fully VLM-based approaches to substantiate their claim of novelty.\n\n2. The experimental validation is conducted on established VQA datasets. To further demonstrate the robustness and generalizability of Q-CLIP, it would be beneficial to evaluate its performance on more recent and diverse datasets, potentially including those with specific content types or distortions, such as gaming videos (e.g., YouTube-Gaming), fine-grained quality labels (e.g., FineVQ), and short-form content (e.g., KVQ and YouTube SFV+HDR).\n\n3. The performance gains also attributable to the learnable prompt strategy and the frame-difference sampling approach. However, these components could arguably be integrated into prior learning-based or even traditional VQA architectures. To isolate the contribution of the core Q-CLIP framework, ablation studies applying the proposed prompting and sampling techniques to other existing VQA models would strengthen the paper's claims regarding the specific benefits of the proposed architecture."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pFA2waVUVU", "forum": "QTXUAsN7oO", "replyto": "QTXUAsN7oO", "signatures": ["ICLR.cc/2026/Conference/Submission7586/Reviewer_rJe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7586/Reviewer_rJe3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013066448, "cdate": 1761013066448, "tmdate": 1762919673755, "mdate": 1762919673755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Q-CLIP, a pioneering Video Quality Assessment (VQA) framework built entirely on Vision-Language Models (VLMs). It addresses two long-standing challenges in mainstream VQA methods: first, classification-based pretraining fails to capture multi-dimensional video quality factors (semantics, distortion, motion, aesthetics); second, large-scale pretraining requires excessive computational costs. Q-CLIP introduces key innovations to solve these issues. It includes a lightweight Shared Cross-Modal Adapter (SCMA) with only 0.14M trainable parameters and a set of learnable five-level quality prompts. These designs enable Q-CLIP to outperform state-of-the-art baselines across all tested datasets. Additionally, the paper explores the impact of frame-difference-based sampling on VQA performance. This pioneering exploration provides valuable insights for future research. Rigorous ablation studies, efficiency analyses, and visualizations further validate the rationality and superiority of Q-CLIP’s design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Clear writing and well-defined motivation\nThe paper follows a logical structure. Methods are clearly explained using figures and equations, and details of the model and training processes are thoroughly described, this makes reproducibility straightforward. The motivation of the work is also well-defined: it aims to solve two key problems in existing studies. One is that semantic knowledge from classification pretraining cannot capture multi-dimensional quality factors. The other is that large-scale pretraining leads to high computational costs.\n\n(2) Novel technical design\nMost previous VQA methods that use VLMs treat VLMs as auxiliary feature extractors. In contrast, Q-CLIP constructs the first VQA architecture fully based on VLMs. This design eliminates reliance on external backbone networks and fully leverages the cross-modal representation capabilities of VLMs. It is a critical breakthrough that redefines how VLMs can be applied to VQA tasks.\n\n(3) Comprehensive and convincing experiments\nThe experimental design is comprehensive and reliable. Experiments are conducted on a wide range of datasets, and results are compared with over 15 baselines (including knowledge-driven, data-driven, and VLMs-based models). This clearly demonstrates the effectiveness of Q-CLIP. Furthermore, ablation studies further confirm the rationality of Q-CLIP’s design. In addition, thorough efficiency experiments prove that Q-CLIP is highly efficient.\n\n(4) Excellent parameter optimization\nQ-CLIP’s clever and novel shared cross-modal adapter design greatly reduces the number of trainable parameters. Only 0.14M parameters need training, which highlights the high efficiency of Q-CLIP.\n(5) Pioneering exploration of frame sampling strategies\nMost previous VQA methods rely on random or uniform sampling. This work is the first to explore the impact of frame-difference-based sampling on VQA. It provides actionable insights for future research in this area."}, "weaknesses": {"value": "(1) In the comparison of fine-tuning methods, some approaches are missing. Examples include COOP and VPT. Although these methods may not perform well in VQA tasks, adding comparisons with them would make the experimental results more comprehensive.\n\n(2) Although efficiency experiments demonstrate that Q-CLIP outperforms most baseline methods, I believe there is still room for optimization in its parameter scale. Attempting to use smaller backbone networks may help achieve a better balance between performance and efficiency.\n\n(3) The paper contains a few minor writing errors. For instance, there is an extra \"s\" in line 317. In line 467, the references to \"Fig. 6(a)\" and \"Fig. 6(b)\" are inconsistent with the actual figure numbering (should correspond to Fig. 8 based on the context). These errors should be corrected to improve the paper’s accuracy."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nLxTnmIC1U", "forum": "QTXUAsN7oO", "replyto": "QTXUAsN7oO", "signatures": ["ICLR.cc/2026/Conference/Submission7586/Reviewer_HNXL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7586/Reviewer_HNXL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535715604, "cdate": 1761535715604, "tmdate": 1762919673419, "mdate": 1762919673419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Q-CLIP, a VLMs-based framework for VQA. Q-CLIP enhances both visual and textual representations through an SCMA module. This work also introduces a set of five learnable quality-level prompts to guide the VLMs in perceiving subtle quality variations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work achieves better performance with smaller parameters on VQA datasets. \n2. This submission is well-written and easy to follow."}, "weaknesses": {"value": "1. The novelty of this work is somewhat weak. This work is somewhat like combining the techniques proposed in CLIP-IQA (text-vision similarity), Q-Align (five rating levels), together with common learnable prompts. Besides, the difference-based sampling method is also simple and intuitive, which is more like a baseline method. \n2. Compared with the original CLIP, the main revision is the proposed SCMA module. However, the reason why this module works is not well analyzed or supported. \n3. In Table 1, Q-Align should also be under the type of VLMs, instead of LLMs, because Q-Align supports both vision and text inputs and could well understand vision modality. \n4. Another concern is about the encoding of temporal information. This work extracts features for different frames separately and then averages these features to obtain the video feature. However, this operation ignores the temporal information. For example, for one video, all frames are good-quality images, but there are quite large inter-frame inconsistencies; the proposed model will mistakenly assess this video as a high-quality video. \n5. This work has conducted an ablation study to show that the mean pooling is better than the transformer or mamba architecture. However, after encoding all frames into features, the temporal information (like inter-frame inconsistencies) has been almost lost. Therefore, the fusion should be performed in a less shallow position, where some pixel-level information is well kept."}, "questions": {"value": "I would like the authors to (1) clarify the novelty of this work, (2) support the proposed SCMA module, and (3) show how to assess the temporal quality of videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Qc4DTyUzc", "forum": "QTXUAsN7oO", "replyto": "QTXUAsN7oO", "signatures": ["ICLR.cc/2026/Conference/Submission7586/Reviewer_AvtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7586/Reviewer_AvtS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700873411, "cdate": 1761700873411, "tmdate": 1762919673091, "mdate": 1762919673091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces a No-Reference Video Quality Assessment framework that is built upon a VLM. To overcome the high computational cost of pretraining and the mismatch between semantic-focused pretraining and the multi-factorial nature of perceptual quality, Q-CLIP proposes a parameter-efficient adaptation strategy. The core idea is to freeze a pre-trained, video-tuned VLM backbone  and introduce two lightweight, trainable modules: Shared Cross-Modal Adapter (SCMA) and learnable five-level quality prompts.\n\nThe authors conduct extensive experiments on six VQA benchmarks (LSVQ, KoNViD-1k, LIVE-VQC, etc.), demonstrating state-of-the-art performance. The key claims are SOTA accuracy achieved with exceptionally low finetuning cost compared to both traditional VQA methods and other VLM-based adaptation techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The most significant contribution is the method's efficiency. Achieving SOTA results by only finetuning 0.14M parameters is highly compelling.\n- The method consistently achieves SOTA or competitive performance across a wide range of intra-dataset and cross-dataset benchmarks. The performance gains over other VLM-utilizing methods like CLIPVQA are notable.\n- The systematic study of frame sampling strategies is a useful, practical contribution. The finding that frame-difference-based (motion-aware) sampling improves cross-dataset generalization is insightful."}, "weaknesses": {"value": "- There are lack of explicit temporal modeling in Q-CLIP. The ablation study on frame feature fusion (Fig. 7) shows that simple mean pooling of frame features outperforms explicit temporal modeling architectures like Transformers and Mamba. It looks like to be an aggregation of frame-level quality scores. This raisessignificant doubts about its ability to assess temporal artifacts (e.g., judder, flickering, motion stutter).\n- The method's performance heavily relies on a *specific* VLM backbone (Bolya et al., 2025) that was *already* \"pre-tuned on video data\". Without an ablation using a standard *image-based* CLIP, it is impossible to know if the proposed adapter is a general-purpose VQA adapter or just a small finetuning layer for an already-powerful video model.\n- The paper's best-performing sampling strategy, \"Q-CLIP-Mixed\" (Table 1) , is never defined in the manuscript or Appendix E . This omission makes the top-reported result in Table 1 irreproducible."}, "questions": {"value": "1. How do the authors expect Q-CLIP to handle temporal-specific distortions like motion judder, flickering, or frame drops, which are critical in VQA? Does this not fundamentally limit the method's applicability to primarily spatial, frame-level artifacts?\n2. To clarify the novelty of the SCMA and prompts, could the authors please provide ablation results using a standard, *image-based* CLIP backbone (e.g., the original ViT-B/16 from Radford et al., 2021)?\n3. Appendix D.2  describes a complex, dataset-specific finetuning process. Does a single, unified strategy (e.g., training the full 0.14M SCMA) not generalize? How sensitive is the model to these specific per-dataset choices, and does this not contradict the paper's central theme of a simple, general adaptation?\n4. The \"Q-CLIP-Mixed\" sampling strategy achieves the best results in Table 1. Could the authors please provide a precise definition of this strategy? Which methods from Appendix E are combined, and in what proportions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fyMd4R69PW", "forum": "QTXUAsN7oO", "replyto": "QTXUAsN7oO", "signatures": ["ICLR.cc/2026/Conference/Submission7586/Reviewer_4auA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7586/Reviewer_4auA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762284962536, "cdate": 1762284962536, "tmdate": 1762919672781, "mdate": 1762919672781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to sincerely thank all reviewers (R1: 4auA, R2: AvtS, R3: HNXL, R4: rJe3) for their careful comments and constructive feedback. We are greatly encouraged that our approach is **efficient** (R1, R2, R3, R4), **effective** (R1, R2, R3, R4), **sound** (R3) and **well-motivated** (R3). Our sampling strategy is **practical** (R1) and **insightful** (R1, R3). Our experiments are **adequate** (R1, R3, R4) and our paper is **well-written** (R2, R3, R4).\n\n**We have carefully reviewed each comment and provided detailed point-by-point responses**, including necessary supplementary analysis, clarifications and discussions. We hope these responses address the reviewers' concerns and further clarify the contributions and significance of this study. **We would be very grateful if the reviewers could consider our answers, and we are happy to provide any further explanations if some aspects remain unclear.**"}}, "id": "qtDI9duUfC", "forum": "QTXUAsN7oO", "replyto": "QTXUAsN7oO", "signatures": ["ICLR.cc/2026/Conference/Submission7586/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7586/Authors"], "number": 19, "invitations": ["ICLR.cc/2026/Conference/Submission7586/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668716179, "cdate": 1763668716179, "tmdate": 1763668716179, "mdate": 1763668716179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}