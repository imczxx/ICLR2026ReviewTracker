{"id": "Zq3oP8F7iq", "number": 3220, "cdate": 1757379375190, "mdate": 1759898101479, "content": {"title": "Temporality as Inductive Bias: The Relational Time Graph", "abstract": "Most temporal models treat time as discrete indices or elapsed intervals, leaving key properties of temporality only implicitly captured in hidden states. We introduce the Relational Time Graph (RTG), which provides an explicit inductive bias for temporality by learning a structured field of event-to-event gaps ($\\hat{\\Delta}$). RTG integrates three temporal properties into a unified energy formulation: relativity, where experiential gaps differ across individuals and contexts; co-existence, where private timelines are softly aligned through world-synchronization (WS) edges; and shock anchoring, where gradual change is preserved except at sudden surges in activity. We evaluate RTG on two behavioral datasets in commerce and news. On supervised prediction in e-commerce, RTG improves F1 over sequential baselines, with $\\hat{\\Delta}$ carrying the main signal and TV/WS priors stabilizing field formation. Overall, RTG provides measurable advantages on supervised tasks and consistent field formation on unlabeled data, demonstrating the value of making temporality explicit through a learnable $\\hat{\\Delta}$-field.", "tldr": "We introduce RTG, a graph-based inductive bias that models time as structure rather than input, producing robust Δ̂-fields across domains.", "keywords": ["temporal representation learning", "inductive bias", "relational learning", "sequence modeling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7045602dbeda7b8d75eec283515c828758f5ed80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a representation -- Relational Time Graph (RTG) -- for a particular form of event data, which provides an explicit inductive bias for temporality. The authors claim that their method integrates relativity (heterogeneous temporal gaps across users or contexts), contemporaneity or world synchronization (alignment of private timelines through shared anchors), and shocks (abrupt regime changes and bursts) “within a single framework, treating temporality not as elapsed indices but as an explicit structural bias”. Limited experiments are performed on 2 datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper seems to try a new way to incorporate time information into models and prescribe a new graphical representation and learning approach. The claimed strength is the integration of relativity, contemporaneity, and shocks, in a way that is different from prior work."}, "weaknesses": {"value": "Frankly, I found the paper impossible to follow and therefore did not understand very much. It has been written in a way where I feel that only those familiar with the terms and the basic set-up can understand matters. Several concepts are mentioned in the paper before they have been explained or defined; sometimes, they have not been explained at all. I did not even see any formal definition of the format of the data that the model is trained on. In my view, the paper needs heavy rewriting to be ready as a conference submission.\n\nAnother weakness is the insufficient experimentation in the form of lack of sufficient baselines and inadequately addressing the motivation for why the approach and proposed graphical representation is useful."}, "questions": {"value": "Here are some additional comments and questions:\n\nI found the abstract challenging to follow – there are several terms and acronyms used that are unfamiliar, so it’s hard to understand what the work is trying to do.\n\nThe authors make a comment on page 1: “Across these approaches, time enters as input or modulation, not as a structural object.” I don’t follow. What is a structural object? And how is prior work not handling temporal structure?\n\nDelta field is mentioned and summarized as “event-to-event gaps defined on an interaction graph” on page 1, without much explanation. This was hard to understand. I’m confused about the use of the MIND-Large dataset. There are apparently no time stamps, so how is possible to learn the delta fields – which I understand to involve elapsed time intervals?\n\nIt is really hard to follow the comments near the end of page 1 and in general. The following line is illustrative: “WS edges act as anchors in the WS regularizer, softly aligning contemporaneous transitions without adding new predictive features”. So much of this is unclear and without sufficient explanation. What is TV in TV smoothing and Huber TV penalty? What is a shock, what is a WS edge?\n\nIt would have been great if Fig. 1 had been used to explain the various components. As it stands, I see the figure but cannot follow many of the panels. \n\nRe: related work – I recommend looking into and citing graphical representations of multivariate temporal point processes. From my limited understanding of the paper, it seems that these can be baselines for some tasks.\n\nThe experimental setup for next event prediction for Retail Rocket appears to be quite limited. There are a host of potential approaches for this problem. Much more experimentation is needed in general of course – more datasets (some with many more event types) and definitely more baselines. This is clearly recognized by the authors: “Our experiments were not aimed at state-of-the-art benchmarks”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SM2ZTSdOJX", "forum": "Zq3oP8F7iq", "replyto": "Zq3oP8F7iq", "signatures": ["ICLR.cc/2026/Conference/Submission3220/Reviewer_VSuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3220/Reviewer_VSuE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605565777, "cdate": 1761605565777, "tmdate": 1762916606842, "mdate": 1762916606842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Relational Time Graph (RTG), a novel framework designed to incorporate temporality into dynamic graph models by imposing an explicit, structured inductive bias based on learning event-to-event time gaps ($\\hat{\\Delta}$). This approach attempts to move beyond implicit capture of time in hidden states or reliance on simple discrete indices. The architecture is built to unify three conceptual temporal properties—relativity (individualized time gaps), co-existence (soft timeline alignment via World Synchronization), and synchronization (the capacity for behavioral transfer)—by minimizing a single unified energy function. The reported results indicate that RTG achieves competitive performance against established baselines like TGN and CTDNE on large-scale temporal datasets, including MIND-Large and Retail Rocket, while also demonstrating low training runtimes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core strength of this work is the architectural novelty and the mathematically grounded framework that treats time as an explicit, learnable relational structure. Defining and modeling the three temporal properties (relativity, co-existence, synchronization) provides a structured and interpretable mechanism for capturing complex temporal dependencies in dynamic graphs."}, "weaknesses": {"value": "The selection of an energy-based formulation introduces potential practical hurdles; while theoretically elegant for enforcing constraints, such models can often be more challenging to optimize, stabilize, and generalize than architectures relying on conventional loss functions (like margin ranking or cross-entropy), raising concerns about the model's ease of implementation and tuning in diverse, real-world scenarios. Furthermore the scope of comparative evaluation is a bit limited. The paper primarily benchmarks against older or less efficient graph-based models (TGN, CTDNE) and does not provide a comprehensive, direct comparison against highly optimized, modern memory-based methods."}, "questions": {"value": "While the overall RTG framework shows strong performance, could the authors provide a more detailed ablation study focusing specifically on the World Synchronization (WS) loss component? \n\nDoes the performance gain primarily come from the $\\hat{\\Delta}$ relativity feature, or is the explicit soft alignment of individual timelines (co-existence) the more dominant factor in mitigating the drift inherent in individual timelines, particularly in highly sparse graph regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6cJ7k9wENU", "forum": "Zq3oP8F7iq", "replyto": "Zq3oP8F7iq", "signatures": ["ICLR.cc/2026/Conference/Submission3220/Reviewer_kK37"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3220/Reviewer_kK37"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766706492, "cdate": 1761766706492, "tmdate": 1762916606377, "mdate": 1762916606377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Focusing on three properties of temporality—relativity, contemporaneity, and shocks—this work proposes to model it as an explicit structural bias. To this end, they introduce the Relational Time Graph, a framework that represents temporality as a learnable ∆-field. The validity of this model is demonstrated through a set of experiments that confirm its findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The concept of 'explicitness' in temporality modeling is interesting, and the technique is solid.\n2. This work is grounded in recommendation settings and is potentially useful."}, "weaknesses": {"value": "1. The significance and broader impact of this work are not clearly established.\n2. The manuscript's structure could be improved to enhance clarity. A specific example is the discussion of the techniques' potential applications, which appears late in the paper and would be more effective in the introduction or a dedicated section."}, "questions": {"value": "1. The concept of relativity appears to have been previously modeled in the paper 'Noether Embedding: Efficient Learning of Temporal Regularities'. How does this work differentiate its contribution from that established research ?\n2. There are minor clerical errors at line 032."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HEB4FO6rnC", "forum": "Zq3oP8F7iq", "replyto": "Zq3oP8F7iq", "signatures": ["ICLR.cc/2026/Conference/Submission3220/Reviewer_RLCj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3220/Reviewer_RLCj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974394047, "cdate": 1761974394047, "tmdate": 1762916605839, "mdate": 1762916605839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the construction of a graph to encode user actions with items, followed by learning parameters $\\hat{\\Delta}$ to represent a temporal coordinate. The learning rule consists of three terms motivated by expected properties of time in the context of recommendation and user activity data (along with a supervised loss for a task of interest, when available).\n\nExperiments examine the properties of the learned parameters $\\hat{\\Delta}$ and how they are changed with different settings of the proposed loss functions and learning procedure. They also demonstrate improved supervised performance on a forecasting task, that stems from using additional unlabelled data with the proposed unsupervised time modeling tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The empirical study is rather thorough, it is clear that the authors are well oriented in the relevant literature, and that the framework of all the proposed components has not been explored in previous work. Proper modeling of time is an important topic, and it's nice to see work on the problem.\nFinally, the demonstrated improvement on supervised learning is also encouraging."}, "weaknesses": {"value": "I found several parts of the paper a bit difficult to follow, and in general I think the writing can be significantly improved. As for the contribution, it seems like a large part of it is combining ideas that have been proposed in other works, hence the novelty may be somewhat limited. I also don't think that I fully understand the motivation for the work, and the main advantage I see is the improved forecasting performance, which is not very large. I'll expand on some of these points below:\n\n1. The introduction of the paper makes it very difficult to understand what sub-field it aims to contribute to. Much of the abstract is written in general language that talks about applications where there are time gaps between events, but in fact the method and experiments seem to be focused on a rather specific subfield of user interactions and recommendation systems. I think this limits the impact of the paper, and perhaps it is more suitable for a specialized venue on recommender systems.\n2. A lot of jargon that is rather specific to the sub-field, or to this paper, is used in the first few sections. For instance \"learnable $\\hat{\\Delta}$-field\", \"shock priors\", \"world synchronization\" and others. A simple mathematical definition of these terms along with a clear description and simple examples would be very helpful to the average reader.\n3. As the authors mention, the components of the framework are addressed in prior work (line 44), and this work unifies them to a framework. Hence the contribution sounds somewhat incremental.\n4. I assume the used baselines are common for the forecasting task in the relevant literature (I am not very familiar with the recommender systems subfield), but since the main focus is on time modeling, wouldn't it make sense to include other relevant baselines like neural temporal point processes and other methods that model time?\n5. Other than the motivation of learning a set of parameters that have behavior which seems relevant to modeling time, it is unclear what stands to be gained from the framework. The improvement on the task of interest exist, but they are modest (as the authors also state) and I was not convinced that using the additional unlabeled data in the specific way proposed in the paper, is much more helpful than other unsupervised tasks that one might define on this data.\n\nOverall, I found the paper quite difficult to follow. I think it requires a much more structured motivation, intuitive examples that make it clear how the framework can be applied in broad application areas, and a clear definition of each component of the framework (see some questions below)."}, "questions": {"value": "How should one define the graph in applications involving time gaps, but where the structure is not as clear as the user logs that motivate this work. For instance, in health records, social network data or any other field that has this type of data?\n\nSome notations seem to appear and never defined. For instance $w_e, w_{e'}$ in line 93, and $E_L$ in line 93. What are these objects? These points just make understanding the framework much more difficult."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HP5V7BE7tJ", "forum": "Zq3oP8F7iq", "replyto": "Zq3oP8F7iq", "signatures": ["ICLR.cc/2026/Conference/Submission3220/Reviewer_C85y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3220/Reviewer_C85y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230727067, "cdate": 1762230727067, "tmdate": 1762916605650, "mdate": 1762916605650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}