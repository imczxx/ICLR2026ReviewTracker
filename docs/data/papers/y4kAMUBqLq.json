{"id": "y4kAMUBqLq", "number": 19018, "cdate": 1758292750566, "mdate": 1763713602063, "content": {"title": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification", "abstract": "Despite rapid advances in code generation, current Large Language Models (LLMs) still lack an essential capability for reliable and verifiable code generation: compositional reasoning across multi-function programs. To explore this potential and important gap, we introduce DafnyCOMP, a benchmark designed to systematically evaluate LLMs on the generation of compositional specifications in Dafny. Unlike prior benchmarks that primarily target single-function annotation, DafnyCOMP focuses on programs composed of multiple interacting functions with necessary data dependencies, requiring LLMs to produce specifications that ensure correctness across component boundaries. Our benchmark comprises 300 automatically synthesized programs, each carefully constructed by combining 2–5 originally independent functions in a chain-based manner through LLM-driven synthesis. We evaluate LLMs from five leading research groups that represent the current frontier of reasoning-centric AI, including the GPT, Claude, Gemini, DeepSeek, and Qwen families. Our results reveal a striking dichotomy: while LLMs achieve both high syntax correctness (>99%) and moderate verification rates (>58%) in prior single-function benchmarks, they exhibit degraded syntax correctness (95.67%) and a catastrophic verification failure (3.69%) in DafnyCOMP's compositional tasks— a 92% performance gap. Even the most powerful LLM achieves only 7% verification at Pass@8, with most LLMs below 2%. Further analysis reveals that LLMs systematically fail at cross-functional reasoning through three primary failure modes: specification fragility (39.2%), implementation-proof misalignment (21.7%), and reasoning instability (14.1%). These failures clearly reveal the absence of compositional reasoning capabilities in current LLMs. DafnyCOMP thus establishes a diagnostic benchmark for tracking progress in verifiable code generation with LLMs, highlighting that the path from local to compositional verification remains largely uncharted.", "tldr": "We present DafnyCOMP, a benchmark showing that while LLMs excel at single-function verification, they catastrophically fail at compositional reasoning across multi-function programs, exposing a critical gap in verifiable code generation.", "keywords": ["LLM", "Formal Language", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb4f96e7f95c24c9c83b6cdcac3bdee3376cd5cd.pdf", "supplementary_material": "/attachment/7a4c2f5c2bb3f39bec5d7a0389f70d51ddcc7bae.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces DAFNYCOMP, a benchmark targeting compositional formal verification for LLMs. It assembles multi-function programs (2–5 functions). Across a series of frontier models, the paper finds a big gap between high syntax success and very low verification results on the benchmark. The authors analyze failures into three primary failure modes: specification fragility, implementation-proof misalignment, and reasoning instability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluation is thorough, covering a wide range of models\n2. The failure taxonomy is insightful, well-articulated, grounded with examples, and provided with the frequencies.\n3. The benchmark is challenging, exposing the limitations of current LLMs."}, "weaknesses": {"value": "1. The benchmark is only done on Dafny. We don't know whether results and claims will still hold for other frameworks such as Verus / Boogie.\n2. The synthesis pipeline uses the LeetCode programs. LeetCode skews toward short, single-goal algorithms, but may deviate from the real-world software use cases.\n2. There is a limited trial in trying to solve the challenges. E.g., adopting a verifier-in-the-loop repair / agentic setup would be beneficial."}, "questions": {"value": "1. Why do you choose LeetCode problems as the pool programs? Is it possible to switch to other datasets as the initial pools?\n2. Will the compositional strategy reflect the distribution of natural real-world programs?\n3. How will this methodology translate to other frameworks such as Verus? Does such a similar dataset / idea exist in the Verus or other frameworks' prior work? If not, will your work's pipeline generalize to other frameworks?\n4. How to improve the performance of your benchmark? Consider adding a baseline to improve the results"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q78fJ1NNBI", "forum": "y4kAMUBqLq", "replyto": "y4kAMUBqLq", "signatures": ["ICLR.cc/2026/Conference/Submission19018/Reviewer_mpGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19018/Reviewer_mpGN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591409275, "cdate": 1761591409275, "tmdate": 1762931063071, "mdate": 1762931063071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark set, DafnyComp, to evaluate LLMs ability to perform compositional\nreasoning. The benchmark set contains Dafny programs that are compositions of 2-5 functions. The goal\nis to prove these programs correct. To do this, one has to generate pre and postconditions (requires\nand ensures) for each function in a way that they compose to yield a proof for the main program.\nThe LLMs task is to annotate the functions enough to enable Dafny verifier to complete the proof \n(by discharging the verification conditions using an SMT solver).\n\nThe main result shows that LLMs across the board are really bad at doing this annotation task. \nThey are able to do so for single functions to some extent, but really struggle to do that for\nmultiple functions in a way that the annotations compose."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. DafnyComp is a valuable benchmark since state-of-the-art LLMs currently perform very poorly\n(about 3% for the best models).\n2. DafnyComp tests the ability to think compositionally, which is useful to evalaute.\n3. DafnyComp is also interesting because models can generate correct syntax (for annotations) with\nhigh probability, so it does demonstrate that models really struggle with compositional reasoning\n(and not getting the syntax right for a potentially low-resource language).\n4. The pipeline for constructing DafnyComp guarantees that the benchmarks are all high quality\n(i.e. they all can be formally proved correct.)"}, "weaknesses": {"value": "Weaknesses:\n\n1. The benchmarks in DafnyComp are artificial - the individual functions are natural, but the\ncompositions are contrived (authors, please correct me if I am wrong.)  So, the 92% drop from\nsingle-function to multi-function programs can also be attributed to the fact that the single-function\nprograms are natural programs that people may write and care for, and the multi-function programs\nare more arbitrary. In other words, if I were to write completely unnatural single function programs,\nwouldn't LLMs performance already deteriorate? That puts the main claim into question: maybe it is \nnot the lack of compositional reasoning, but just that the compositional programs are unnatural. \nIf the single-function programs were unnatural, or the compositional programs were natural, then \nwould we get different conclusions?\n\n2. The paper makes very strongly worded claims all through the paper, but there are a lot of \nfactors that could be responsible for the observed behavior. Many such factors are overlooked and\nnot discussed.\n\n 2.1 If the function call graph is linear (which means it can be topologically sorted), then couldn't\nwe turn the multi-function benchmarks to single-function benchmarks? If so, then would LLMs \nperform better on these derived single-function benchmarks? And would this finding impact the main\nclaims of the paper?\n\n 2.2 Is Dafny prover guaranteed to prove things if they are provable -- couldn't the SMT solver fail -- \ne.g. time out -- even when the verification condition holds true? Would this not somehow impact\nthe conclusions? For example, purely hypothetically, maybe the LLM is generating the correct \nannotations and the SMT solver is failing to prove them? I know that this is unlikely, but how \nare we guarding against that possibility?\n\n3. There is not enough explanation for how benchmarks (and their correctness assertions) are\ngenerated.\n\n 3.1 It is unclear how the assertions of the composed program are generated. Line 215 says \"We execute\neach composed program against the reference unit tests from LeetCodeDataset\" -- but my understanding\nis that you are taking individual functions from LeetCodeDataset and then creating new compositions\n(with 2-5 functions). So, LeetCodeDataset would not have reference unit tests for these composed \nprograms. There is clearly some misunderstanding here on my part.\n\n 3.2 Some of these misunderstandings could be avoided if you had a running example either in the main\npaper or at least in the Appendix. It would be good to have an example benchmark in DafnyComp. My best\nSource is Appendix G. But most of the programs there have 1 function -- clearly, they can't be part\nof DafnyComp. So, that adds to more confusion. G.4.2 is the first place where there is a program that\nuses multiple functions. It has one function, reverse_7, that is unused, that is a but confusing, but\nmaybe DafnyComp has benchmarks that contain unused functions. But what is really confusing is that\n\"nextBeautifulNumber_769\" is not a linear composition of two functions, but a loop over the \"isBeautiful\"\nfunction. \n\n 3.3 The examples in Appendix G just give some code snippet and a Dafny error. That is not helpful. \nMore explanation is needed. I could not understand most examples. For example, take G.1.1: sum >= 0\nis the only postcondition there. It is clearly implied by the loop invariant (because it contains\na conjunct sum >= 0) -- but Dafny error says \"a postcondition could not be proved...\" ?\n\nOther questions:\n\n- Contribution (ii) on Line 111 is related to a single function/procedure, right? So, it is not\nrelated to compositions, is that true?\n\n- Does DafnyComp have 300 benchmarks or 900 benchmarks (Table 2)? Or Table 2 is analyzing 3 runs for \neach of the 300 benchmarks?\n\n- Line 422-423: \"not a typo but a systematic failure to...\" -- what do you mean by \"not a typo\"?\n\n- Line 427-428: \"attention mechanisms excel at ...implementations.\" I think there is no experiment\nin the paper that provides evidence of this claim, and I am not even sure what this claim means."}, "questions": {"value": "Please refer to the Weaknesses section for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTwo6tFz4O", "forum": "y4kAMUBqLq", "replyto": "y4kAMUBqLq", "signatures": ["ICLR.cc/2026/Conference/Submission19018/Reviewer_2F9W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19018/Reviewer_2F9W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722293645, "cdate": 1761722293645, "tmdate": 1762931062541, "mdate": 1762931062541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DafnyComp, a benchmark to evalaute LLMs in . Unlike existing benchmarks, DafnyComp focuses on programs consisting of multiple functions interacting with each other. Experiments on 13 frontier LLMs show that this leads to significantly a more challenging and realistic benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The paper introduces DafnyComp, a benchmark for evaluating LLMs on the generation of compositional specifications in Dafny. Existing benchmarks for verifiable code generation only test problems with a single function. DafnyComp focuses on programs composed of multiple interacting functions with necessary data dependencies, addressing a significant gap in the real-world applications of formal verification. \n* A comprehensive set of LLMs is evaluated on DafnyComp. Results show that multi-function compositional problems present a major challenge to LLMs compared to simpler problems.\n* The authors performed a detailed and insightful analysis of LLMs' failures on DafnyComp."}, "weaknesses": {"value": "* The benchmark is relatively small (300 programs). Is it possible to scale the benchmark construction method in Sec. 3 to collect more programs?\n* The paper focuses on Dafny. It would be great to discuss potential extensions to other systems such as Lean and Verus."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SOtoaUA4Dg", "forum": "y4kAMUBqLq", "replyto": "y4kAMUBqLq", "signatures": ["ICLR.cc/2026/Conference/Submission19018/Reviewer_KdjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19018/Reviewer_KdjF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142769611, "cdate": 1762142769611, "tmdate": 1762931062205, "mdate": 1762931062205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DAFNYCOMP, a collection of 300 programs used to provide stress on the compositional generation of specifications in Dafny by chaining 2-5 functions with real dependency of data. In 13 frontier LLMs, syntax is well-behaved (96%-99% Pass at t8) but the verification is still low (3.7 percent overall; best model about 7 percent), indicating that there is a big discrepancy between syntax and semantic correctness. The failures the authors consider are three in general, namely specification fragility, implementation-proof misalignment, and reasoning instability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely evaluation of compositional verification\n- Clear, reproducible benchmark spec with Dafny ground truth.\n- Concrete numbers that expose a large syntax verification gap. \n- Insightful error analysis"}, "weaknesses": {"value": "- Data programs and specs are generated using an LLM from a single family employed in evaluation, which might be biased toward one style and negatively impact others.\n- The evaluation is limited to topologies: only chains are considered, whereas real systems include trees, DAGs, shared states, and more complex dependencies.\n- There is a lack of evaluation on more advanced test-time scaling strategies, such as multi-turn self-refinement.\n- The evaluation described in the main text does not consistently standardize or control details about compute and prompting across models (tokens, temperature, verifier retry policy)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2dlmYfu2GO", "forum": "y4kAMUBqLq", "replyto": "y4kAMUBqLq", "signatures": ["ICLR.cc/2026/Conference/Submission19018/Reviewer_M3WA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19018/Reviewer_M3WA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762250584481, "cdate": 1762250584481, "tmdate": 1762931061837, "mdate": 1762931061837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}