{"id": "KMQQzfzJdG", "number": 11586, "cdate": 1758202167080, "mdate": 1759897566096, "content": {"title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment", "abstract": "Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the model’s ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at [Anonymous URL].", "tldr": "", "keywords": ["Contrastive learning", "vision-language pre-training", "representation learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0a311e380dbf161e5e41914a3b46bd32431b79c.pdf", "supplementary_material": "/attachment/f312e88a45d89ee4e03551c93c377d6898c698a8.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a pluggable non-contrastive branch “CLIPin” that introduces an online/target branch (EMA) and prediction head without modifying the original CLIP-style dual encoder architecture. It performs dual augmentation on images and text respectively, jointly minimizing: Inter-modal regression consistency loss and intra-modal consistency loss are jointly optimized with the standard InfoNCE contrastive loss. To accommodate both contrastive and non-contrastive learning, the authors decompose the projection head into a “shared pre-projector (shared across both paradigms) + distinct sub-projectors for each,” enabling simultaneous computation of 512-dimensional contrastive features and 8192-dimensional non-contrastive features during training. Experiments demonstrate superior performance across natural domains, and medical domains. Visualizations and ablation studies are provided. Authors report AUC/mAP improvements across multiple benchmarks and demonstrate comparable or beneficial results when integrated as plugins within various frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. We provide the complete forward and target branches, EMA updates, loss formulations, and training combinations, and explicitly illustrate the data flow and module interfaces in Figure 1 for implementation and verification.\n\n2. We progressively incorporate cross-modal/intra-modal alignment and shared pre-projection, while also examining pre-projection dimensions, batch sizes, image/text augmentation activation, and removing specific alignment components. This yields a comprehensive evidence chain."}, "weaknesses": {"value": "1. The claimed effectiveness in the medical domain relies on pretraining with over 450K private retina image–report pairs, which are not publicly available even in anonymized form; this severely limits external reproducibility and independent verification of the results.\n2. The reported “universal improvement” is not consistent across frameworks—for example, in Table 2 the BLIP model on REFUGE drops from 94.47 to 92.75 AUC, indicating that the gain depends on specific architectures, datasets, and hyperparameters.\n3. Despite emphasizing multimodal semantic alignment, the paper evaluates mainly on classification metrics (AUC, mAP) and omits retrieval-based measures like Recall@K, leaving the alignment improvement only indirectly evidenced.\n4. The authors disable text augmentation in main experiments due to marginal or negative effects, which contradicts the method’s core idea of constructing two consistent views for *each* modality and may weaken the non-contrastive text branch.\n5. The optimization and stability of the learnable weights λ₍inter₎ and λ₍intra₎ are under-specified—Appendix Fig. 3 shows λ₍inter₎ increasing while λ₍intra₎ stays flat, but the paper does not clarify optimizer choice, constraints, or regularization, raising concerns of dominance or overfitting."}, "questions": {"value": "1. How are the learnable weights ( \\lambda_{\\text{inter}} ) and ( \\lambda_{\\text{intra}} ) optimized in practice (optimizer, parameterization, constraints/regularization), and how stable are they across datasets and scales (cf. Fig. A.1)? \n2. Since text augmentation is largely disabled in the main experiments (Table 6), how do you justify the non-contrastive text branch learning two “independent” views—does this weaken the claimed bidirectional alignment? \n3. Can you report retrieval metrics (e.g., Recall@K for image→text/text→image) to directly evidence cross-modal alignment improvements, in addition to AUC/mAP classification? \n4. In Table 2/8, some frameworks show degradations after adding CLIPin (e.g., BLIP on REFUGE). What factors (projector dimensions, EMA momentum, batch size, λ-schedules) drive these negative cases? \n5. How sensitive is CLIPin to the choice of projector dimensions beyond the single VOC2007 study (Table 4)? Does the 1,024-dim pre-projector generalize across domains and backbones? \n6. For medical-domain claims, given the private training corpus (Appendix A.1), can you provide a public surrogate (e.g., MIMIC-CXR, ROCO) to let the community verify your observed gains? \n7. What collapse-avoidance ablations have you tried beyond adding predictors and EMA (e.g., stop-grad placements, predictor depth), especially for heterogeneous encoders where “inter-only” failed badly (Table 7)? \n8. Your OOD-ZSC setting relies on prompt engineering (Section 4.1). How robust are the conclusions to prompt templates and label wording (especially for Chinese vs. English prompts)? \n9. Training on COCO/MUGE with ViT-B/16 (Section 4.1) is relatively small-scale for CLIP-style methods. Do the gains persist or change when scaling data/backbones (e.g., ViT-L/14, larger noisy corpora)? \n10. Could you clarify compute fairness: are all baselines retrained “from scratch” under identical augmentations, batch sizes, projector dims, and optimizer schedules—especially where weighted attention or auxiliary heads differ? \n\n**If you address my concerns, I will consider raising my score.**"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qfhr35FnEO", "forum": "KMQQzfzJdG", "replyto": "KMQQzfzJdG", "signatures": ["ICLR.cc/2026/Conference/Submission11586/Reviewer_1rpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11586/Reviewer_1rpH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390281951, "cdate": 1761390281951, "tmdate": 1762922670596, "mdate": 1762922670596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a pluggable non-contrastive branch “CLIPin” that introduces an online/target branch (EMA) and prediction head without modifying the original CLIP-style dual encoder architecture. It performs dual augmentation on images and text respectively, jointly minimizing: Inter-modal regression consistency loss and intra-modal consistency loss are jointly optimized with the standard InfoNCE contrastive loss. To accommodate both contrastive and non-contrastive learning, the authors decompose the projection head into a “shared pre-projector (shared across both paradigms) + distinct sub-projectors for each,” enabling simultaneous computation of 512-dimensional contrastive features and 8192-dimensional non-contrastive features during training. Experiments demonstrate superior performance across natural domains, and medical domains. Visualizations and ablation studies are provided. Authors report AUC/mAP improvements across multiple benchmarks and demonstrate comparable or beneficial results when integrated as plugins within various frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. We provide the complete forward and target branches, EMA updates, loss formulations, and training combinations, and explicitly illustrate the data flow and module interfaces in Figure 1 for implementation and verification.\n\n2. We progressively incorporate cross-modal/intra-modal alignment and shared pre-projection, while also examining pre-projection dimensions, batch sizes, image/text augmentation activation, and removing specific alignment components. This yields a comprehensive evidence chain."}, "weaknesses": {"value": "1. The claimed effectiveness in the medical domain relies on pretraining with over 450K private retina image–report pairs, which are not publicly available even in anonymized form; this severely limits external reproducibility and independent verification of the results.\n2. The reported universal improvement is not consistent across frameworks—for example, in Table 2 the BLIP model on REFUGE drops from 94.47 to 92.75 AUC, indicating that the gain depends on specific architectures, datasets, and hyperparameters.\n3. Despite emphasizing multimodal semantic alignment, the paper evaluates mainly on classification metrics and omits retrieval-based measures like Recall@K, leaving the alignment improvement only indirectly evidenced.\n4. The authors disable text augmentation in main experiments due to marginal or negative effects, which contradicts the method’s core idea of constructing two consistent views for *each* modality and may weaken the non-contrastive text branch.\n5. The optimization and stability of the learnable weights λ₍inter₎ and λ₍intra₎ are under-specified—*Appendix Fig. 3* shows λ₍inter₎ increasing while λ₍intra₎ stays flat, but the paper does not clarify optimizer choice, constraints, or regularization, raising concerns of dominance or overfitting."}, "questions": {"value": "### **MajorQuestions**\n\nQ1. How are the learnable weights ( \\lambda_{\\text{inter}} ) and ( \\lambda_{\\text{intra}} ) optimized in practice, and how stable are they across datasets and scales ?   \nQ2. Since text augmentation is largely disabled in the main experiments, how do you justify the non-contrastive text branch learning two “independent” views—does this weaken the claimed bidirectional alignment?   \nQ3. Can you report retrieval metrics  to directly evidence cross-modal alignment improvements, in addition to AUC/mAP classification? \nQ4. In Table 2/8, some frameworks show degradations after adding CLIPin . What factors (projector dimensions, EMA momentum, batch size, λ-schedules) drive these negative cases?   \nQ5. How sensitive is CLIPin to the choice of projector dimensions beyond the single VOC2007 study ? Does the 1,024-dim pre-projector generalize across domains and backbones?   \nQ6. For medical-domain claims, given the private training corpus , can you provide a public surrogate to let the community verify your observed gains?   \n\n### **Minor Questions**\n\nQ7. What collapse-avoidance ablations have you tried beyond adding predictors and EMA, especially for heterogeneous encoders where inter-only failed badly ?   \nQ8. Your OOD-ZSC setting relies on prompt engineering . How robust are the conclusions to prompt templates and label wording?   \nQ9. Training on COCO/MUGE with ViT-B/16 is relatively small-scale for CLIP-style methods. Do the gains persist or change when scaling data/backbones ?   \nQ10. Could you clarify compute fairness: are all baselines retrained from scratch under identical augmentations, batch sizes, projector dims, and optimizer schedules—especially where weighted attention or auxiliary heads differ?   \n\n**If you address my concerns, I will consider raising my score.**"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qfhr35FnEO", "forum": "KMQQzfzJdG", "replyto": "KMQQzfzJdG", "signatures": ["ICLR.cc/2026/Conference/Submission11586/Reviewer_1rpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11586/Reviewer_1rpH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390281951, "cdate": 1761390281951, "tmdate": 1763650581117, "mdate": 1763650581117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified plug-in which can integrate non-contrastive feature representation into CLIP-style architectures. Two shared pre-projectors for image and text modalities are designed to facilitate the integration of contrastive and non-contrastive branches. Experiments on downstream tasks demonstrate the effectiveness of proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a unified plug-in which can seamlessly integrate modular non-contrastive strategy into existing contrastive frameworks like CLIP.\n2. Experiments on downstream tasks like linear probing and prompt-based out-of-distribution zero-shot classification demonstrate that the proposed method can facilitate general and robust representation learning."}, "weaknesses": {"value": "1. Regarding Lines 40-41, the authors attribute CLIP's issues on medical datasets to \"semantically similar samples being treated as negative sample pairs.\" However, this phenomenon appears fundamentally similar to the many-to-many correspondence problem in natural datasets, where a single image/caption can be relevant to multiple batch samples. The introduction of distinct terms—\"semantic looseness\" for natural and \"semantic redundancy\" for medical datasets—for what seems to be a conceptually similar issue creates confusion and requires further clarification.\n2. Regarding Lines 169-171, the claim that the image/text predictors help prevent collapse lacks a clear mechanistic explanation. Merely stating that these components \"introduce asymmetry\" is insufficient. The authors should elaborate on the specific role this asymmetry plays to prevent the representational collapse that symmetric architectures may suffer from. Visualizations of the predictors' operations are helpful for comprehension. More importantly, ablation experiments are essential to confirm the necessity of this design.\n3. The training datasets used in this study (COCO, MUGE, and a Private Dataset) have a combined scale of no more than 1M samples. To further strengthen the validation of the proposed CLIPin's scalability, it would be beneficial to include larger-scale public datasets such as CC3M [1], CC12M [2], YFCC [3], or LAION [4]. Furthermore, the current experiments employ relatively small batch sizes (128/256/512). The work could be complemented by an investigation into the effect of larger batch sizes.\n4. The downstream task evaluation, currently restricted to linear probing and prompt-based out-of-distribution zero-shot classification, is insufficient. The benchmark suite should be extended to include other tasks like cross-modal retrieval and standard zero-shot classification, to more fully validate CLIPin's effectiveness.\n5.  How about implementing $L_{intra}$ and/or $L_{inter}$ using the constrastive loss (InfoNCE) instread of non-constrastive formulation?\n6. The paper lacks essential details regarding the image and text augmentation pipelines.\n\n[1] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed, image alt\u0002text dataset for automatic image captioning,” in ACL, 2018.\n\n[2] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts,” in CVPR, 2021.\n\n[3] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li, “Yfcc100m:\nThe new data in multimedia research,” Communications of the ACM, 2016.\n\n[4] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and\nA. Komatsuzaki, “Laion-400m: Open dataset of clip-filtered 400 million image-text pairs,” In NeurIPS Workshop, 2021."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hUUe8MUKWg", "forum": "KMQQzfzJdG", "replyto": "KMQQzfzJdG", "signatures": ["ICLR.cc/2026/Conference/Submission11586/Reviewer_2X6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11586/Reviewer_2X6B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481591223, "cdate": 1761481591223, "tmdate": 1762922669595, "mdate": 1762922669595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets a core flaw in CLIP's InfoNCE loss. The authors correctly point out that this loss function breaks down with real-world data, which is either too noisy (web-scale) or too redundant (e.g., medical), leading to false negatives and poor supervision.\n\nThe proposed solution, CLIPin, is a \"plug-and-play\" non-contrastive module that complements, rather than replaces, the standard contrastive loss. The core idea is to add a parallel, symmetric online-target network (inspired by BYOL/SimSiam) for both image and text. CLIPin introduces two new non-contrastive losses: An inter-modal loss where the online image encoder predicts the target text encoder's output (and vice-versa) and an intra-modal loss to stabilize training by matching augmented views within the same modality.\n\nTo solve the architectural mismatch between contrastive (which favors shallow projectors) and non-contrastive methods (which need deep ones), the authors use a clever shared pre-projector. This single pre-projector feeds into separate,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-grounded. It clearly identifies a practical, well-known flaw in InfoNCE—its vulnerability to noisy and redundant data—as its primary motivation .\n\nThe shared pre-projector is a clever fix for a known conflict between contrastive and non-contrastive projector designs . This design makes the \"plug-in\" claim credible and is a nice engineering contribution.\n\nThe ablations in Table 3 and Table 7 effectively show that the non-contrastive component is unstable on its own (prone to collapse) and that all parts (inter-modal, intra-modal, and contrastive) are needed for the best performance. The Grad-CAM visualizations also provide good qualitative support for the claim of improved alignment."}, "weaknesses": {"value": "The paper’s main comparison is to xCLIP. This is too narrow. It ignores other, very similar methods like Cosmos (Kim et al., 2025), which also uses cross-modality self-distillation. The novelty of this work is questionable without a more thorough discussion of these closely related non-contrastive multimodal frameworks.\n\nThe evaluation is almost entirely focused on classification (linear probe and ZSC). This is a major omission. A primary and arguably the most important use case for CLIP is cross-modal retrieval. This task relies on the InfoNCE loss to structure the entire embedding space by pushing negatives apart. The new non-contrastive loss ($\\mathcal{L}_{inter}$) only pulls positive pairs together and ignores negatives. This could easily distort the embedding space and harm retrieval performance, but the paper provides no experiments (e.g., R@K on COCO/Flickr30k/Winoground/MMVP) to confirm or deny this.\n\nThe paper's main ablations (Table 3) are \"additive,\" showing that all components together work best. But this doesn't fully isolate each part's contribution. For example, what is the effect of only adding the intra-modal loss to CLIP? Or only the inter-modal loss? The paper also introduces learnable loss weights ($\\lambda_{inter}$, $\\lambda_{intra}$) without ablating this choice against simple fixed weights, which is a significant new design element left unanalyzed.\n\n[1] Kim, Sanghwan, et al. \"Cosmos: Cross-modality self-distillation for vision language pre-training.\" Proceedings of the Computer Vision and Pattern Recognition Conference."}, "questions": {"value": "Have you evaluated this model on standard retrieval tasks? What are the R@1/R@5/R@10 metrics on the COCO or Flickr30k test sets? I am concerned the $\\mathcal{L}_{inter}$ loss may hurt retrieval performance, and the lack of these results is a major gap.\n\nHow are the learnable weights ($\\lambda_{inter}$, $\\lambda_{intra}$) implemented? Are they just standard parameters optimized via gradient descent? Why was this chosen over simpler, fixed hyperparameters, and what is its effect on stability and final performance?\n\nHow does your cross-modal regression approach differ, in practice, from the cross-modality self-distillation in Cosmos (Kim et al., 2025)?The ablation in Table 6 suggests text augmentation has little effect19. \n\nDoes this mean the non-contrastive module relies heavily on strong, natural augmentations (like COCO's multiple captions) and would be less effective in a single-caption-per-image setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tWLSk0QCk3", "forum": "KMQQzfzJdG", "replyto": "KMQQzfzJdG", "signatures": ["ICLR.cc/2026/Conference/Submission11586/Reviewer_EPmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11586/Reviewer_EPmF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971187138, "cdate": 1761971187138, "tmdate": 1762922669159, "mdate": 1762922669159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLIPin, a non-contrastive plug-in module designed to enhance multimodal semantic alignment in CLIP-style vision-language pretraining. The core idea is to complement the standard InfoNCE-based contrastive learning with instance-level non-contrastive alignment (inspired by BYOL/SimSiam), using a symmetric online-target architecture for both image and text modalities. To reconcile the architectural differences between contrastive and non-contrastive objectives, the authors introduce shared pre-projectors that feed into separate contrastive (512-dim) and non-contrastive (8192-dim) sub-projectors. CLIPin is evaluated on both natural (COCO, MUGE) and medical ([Private Dataset]) domains, showing consistent gains in linear probing and prompt-based out-of-distribution zero-shot classification across multiple downstream benchmarks. The plug-and-play nature is further validated by integrating CLIPin into several strong baselines (ALBEF, BLIP, CoCa, etc.)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- CLIPin is genuinely plug-and-play requiring no changes to base encoders and demonstrates consistent improvements when added to multiple frameworks (ALBEF, BLIP, CoCa).\n- The paper includes ablation studies, generalization tests, per-category breakdowns, and qualitative Grad-CAM visualizations, strengthening the empirical claims."}, "weaknesses": {"value": "1. The paper conflates two fundamentally distinct data issues—noisy weak supervision in natural datasets and low textual diversity in medical reports—into a single failure mode of InfoNCE. However, these problems require different mitigation strategies (e.g., robust loss vs. diversity-aware sampling). No quantitative evidence (e.g., negative sample misclassification rate, alignment entropy) is provided to justify this unified framing.\n2. The use of a non-public medical dataset ([Private Dataset]) undermines reproducibility and limits external validation. Results on public medical benchmarks (e.g., MIMIC-CXR, CheXpert) would significantly strengthen the claim.\n3. The core idea—adding BYOL-style alignment to CLIP—is a natural extension. The distinction from xCLIP (which also uses non-contrastive learning) is not sharply delineated; xCLIP focuses on distributional alignment, while CLIPin uses instance-level alignment, but this difference is not theoretically analyzed.\n4. Applying EMA to update a text encoder (Transformer) using an image encoder (ViT) as part of a shared momentum framework is nontrivial. The paper does not discuss potential instability or feature misalignment due to modality heterogeneity.\n5. The method doubles the forward pass (online + target branches) and uses high-dimensional projections (8192-dim). The paper reports 24h training on one 3090 but omits comparison to baseline CLIP’s training time or memory footprint.\n6. Table 6 shows text augmentation provides little benefit (and sometimes harms performance), yet the method description assumes two augmented text views. This raises questions about the necessity of text-side non-contrastive alignment.\n7. While Table 2 shows performance gains when integrating CLIPin into ALBEF/BLIP/CoCa, the paper omits critical details:\n- Are the original auxiliary losses (e.g., ITM, MLM, captioning) retained?\n- Is the training pipeline modified beyond adding CLIPin?\nWithout this, it is unclear whether gains stem from CLIPin’s architecture or simply from additional supervision signals."}, "questions": {"value": "My major concerns are outlined in the \"Weaknesses\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9o4PoJQns", "forum": "KMQQzfzJdG", "replyto": "KMQQzfzJdG", "signatures": ["ICLR.cc/2026/Conference/Submission11586/Reviewer_bDb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11586/Reviewer_bDb6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998366429, "cdate": 1761998366429, "tmdate": 1762922668729, "mdate": 1762922668729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}