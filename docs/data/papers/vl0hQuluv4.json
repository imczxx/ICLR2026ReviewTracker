{"id": "vl0hQuluv4", "number": 5891, "cdate": 1757943680978, "mdate": 1759897946988, "content": {"title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems", "abstract": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the ACADREASON benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. \nIt consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of ACADREASON. The code and data for the ACADREASON benchmark are available at https://anonymous.4open.science/r/Acadreason-Benchmark-1BD3/.", "tldr": "", "keywords": ["benchmark", "LLM", "Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8d5c9e455109030e3da8fe4ef00c33bd6f920e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new benchmark ACADREASON. \n\n- This benchmark is designed to be a challenging, reasoning-intensive benchmark for academic domains, including CS, Law, Econ, Math and Philosophy. Each domain contains 10 questions. \n\n- The benchmark is heavily human expert curated. The questions and answers are extracted and formulated by human experts from latest publications, along with hints and scoring checklist.\n\n- The paper also benchmarked the performance of latest Large Reasoning Model and Tool-used agents on ACADREASON. The results show the benchmark is very challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed benchmarks is a good contribution to the community. It has detailed and careful human expert annotations, which is a good effort. \n\n- The benchmark is well-positioned, as it focuses on challenging reasoning and academic domains.\n\n- The paper has benchmarked a wide range of latest LLMs and reasoning paradigms."}, "weaknesses": {"value": "- The benchmark is relatively small, 50 questions in total. Although the scoring hints can give a bit more fine-grained signals, but in general the size is limited.\n\n- It would be better to include a comparison section against other relevant benchmarks."}, "questions": {"value": "- As one aspect of the contributions is the difficulty, would it be possible to quantify the difficulty and compare with other benchmark in a more explicit way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8fp3mMUrdB", "forum": "vl0hQuluv4", "replyto": "vl0hQuluv4", "signatures": ["ICLR.cc/2026/Conference/Submission5891/Reviewer_Gkt7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5891/Reviewer_Gkt7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711946561, "cdate": 1761711946561, "tmdate": 1762918329475, "mdate": 1762918329475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ACADREASON, a new benchmark designed to evaluate academic-level reasoning of large language models (LLMs). Unlike existing reasoning benchmarks such as MMLU-Pro, GPQA, or PaperBench, which focus on either broad factual understanding or narrow scientific tasks, ACADREASON targets deep, research-oriented reasoning across five academic domains: Computer Science, Mathematics, Economics, Law, and Philosophy. The authors curate questions from over 400 research papers and design multi-step annotations. They evaluate several frontier models (GPT-5, DeepSeek-R1, Claude 3.7, Gemini 2.0, etc.) and find that all perform far below human experts, especially on methodological and conceptual reasoning. The study also analyzes the role of hints and long-form responses, showing that factual hints help more than methodological ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- High-quality benchmark design. The dataset is small but carefully curated. Questions are derived from genuine academic contexts rather than textbook or competition problems, giving ACADREASON a strong realism advantage. The multi-domain design broadens evaluation coverage beyond STEM, incorporating social science and philosophy.\n- Transparent and rigorous annotation process. The paper clearly documents every stage: data sourcing, question formulation, verification, and evaluation. The inclusion of structured checklists and fine-grained rubrics for reasoning quality (clarity, coherence, accuracy) improves reproducibility and reliability compared with prior subjective benchmarks.\n- Insightful empirical findings. The experiments reveal important trends: reasoning models still struggle on conceptual abstraction and logical grounding even when they perform well on applied math or coding tasks. The “hint effect” analysis (Table 2, Fig. 3) provides valuable insight into which types of contextual scaffolds actually help reasoning models.\n- Readable and well-structured paper. The narrative flows logically, figures are informative, and the motivation for each step is well explained. The benchmark and evaluation protocol could be easily adopted by others studying academic reasoning.\n- Contribution significance. Although not methodologically groundbreaking, ACADREASON fills a practical and conceptual gap between task-level reasoning (e.g., GSM8K, MATH) and domain-level scholarly reasoning. It contributes a valuable lens for assessing whether modern LLMs can reason like researchers rather than students."}, "weaknesses": {"value": "- Limited novelty relative to existing benchmarks. While the dataset is well executed, the idea of academic or research-style reasoning has been partially explored in GAIA, PaperBench, and DeepResearchBench. ACADREASON’s main differentiator is diversity and annotation rigor rather than a fundamentally new evaluation paradigm. A clearer comparative discussion would strengthen its originality claim.\n- Scale and statistical power. The dataset contains only about 50 finalized questions, which limits robustness and makes performance variance hard to interpret. It’s uncertain whether differences across models (often within 1–2 points) are statistically meaningful.\n- Evaluation subjectivity. Despite the structured rubric, the “LLM-as-a-judge” setup remains vulnerable to bias and consistency issues. The authors mention human spot-checks but do not quantify inter-rater agreement or cross-model evaluation consistency. A partial human-judged subset would greatly improve reliability.\n- Limited actionable insight for model design. The results primarily reaffirm known findings that reasoning LLMs remain weak in multi-step conceptual reasoning, but offer little guidance on how to improve them. The benchmark thus functions more as a diagnostic dataset than a research breakthrough.\n- Scalability and sustainability. Because the pipeline relies heavily on expert curation and manual validation, it is unclear how ACADREASON could be expanded to larger scales or adapted to new domains without significant effort."}, "questions": {"value": "- Benchmark uniqueness. How does ACADREASON differ conceptually from PaperBench and GAIA beyond domain coverage? Would the authors consider positioning it as a complement rather than a replacement benchmark?\n- Human validation. How many samples were manually verified by human experts? Is there any inter-annotator agreement score (e.g., Cohen’s κ) for the judgment process?\n- Evaluator bias. Since GPT-5-mini is used as the evaluator, did the authors test whether results are consistent when switching to another LLM judge (e.g., Claude 3 Opus)? Are ranking trends preserved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bOOkQkwy76", "forum": "vl0hQuluv4", "replyto": "vl0hQuluv4", "signatures": ["ICLR.cc/2026/Conference/Submission5891/Reviewer_RYgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5891/Reviewer_RYgK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767192218, "cdate": 1761767192218, "tmdate": 1762918328651, "mdate": 1762918328651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ACADREASON, a new benchmark designed to address shortcomings in current evaluations of Large Language Model (LLM) reasoning. The authors argue that existing benchmarks (e.g., math/code contests, general tasks) lack sufficient academic reasoning depth. To fill this gap, ACADREASON aims to evaluate the ability of LLMs and Agents to acquire and reason with specialized academic knowledge.\nThe benchmark consists of 50 expert-annotated academic problems across five high-reasoning domains: Computer Science, Economics, Law, Mathematics, and Philosophy. All questions are sourced from top-tier publications from 2023-2025 and underwent rigorous quality control to ensure they are both challenging and answerable.\nKey contributions include:\n1. The ACADREASON Benchmark: A challenging, cross-disciplinary benchmark focused on frontier academic reasoning, complete with golden answers, verifiable checklists, and three types of hints (background, definition, methodology).\n2. SOTA Model Evaluation: A systematic evaluation of over 10 state-of-the-art LLMs and Agents (e.g., GPT-5, o3, DeepSeek-R1, OAgents).\n3. Revealed Capability Gap: The results show that even the most advanced LLMs (GPT-5) score poorly (16% pass rate, 40.5% checklist score). Agents perform better (OAgents at 34% / 65.1%) but still show significant room for improvement.\n4. Hint Analysis: An ablation study demonstrating that providing hints (especially \"methodology hints\") significantly improves model performance, suggesting models struggle more with complex methods than with background knowledge."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Originality & Significance:\n  - The paper addresses a clear and important problem: how to evaluate the deep, domain-specific reasoning capabilities of LLMs.\n  - ACADREASON's uniqueness lies in its combination of breadth (spanning both STEM and humanities) and depth (focusing on recent, theoretical problems from top-tier journals). This design tests reasoning on novel knowledge, not just retrieval of pre-existing, commonly known information.\n  - The benchmark's high difficulty (evidenced by low SOTA scores) confirms its utility and significance as an evaluation tool that is not easily saturated.\n2. Quality:\n  - The benchmark's construction methodology is rigorous, involving domain experts (Master's or PhD level) for data curation and annotation.\n  - A detailed, multi-stage validation process (shown in Figure 10) was used to ensure data quality, theoretical focus, and question answerability.\n  - The evaluation framework is rich. Beyond binary pass/fail ($R_p$), the \"Checklist Score\" ($R_j$) allows for a more granular analysis of the model's reasoning process. The inclusion of three hint types is a significant strength, enabling analysis of why a model fails (e.g., lack of background knowledge vs. lack of methodological understanding).\n3. Clarity:\n  - The paper is well-organized and easy to follow. Figure 1 provides a clear overview of the benchmark construction and evaluation pipeline.\n  - Task specifications, evaluation metrics (Sec 3.4), and experimental setups (Sec 4.1) are clearly articulated.\n  - Results are presented clearly (Tables 1 & 2), and the Case Study (Fig. 4) offers a concrete, intuitive example of the task's difficulty and the difference between Agent and LLM performance."}, "weaknesses": {"value": "Based on an in-depth analysis, the paper suffers from three major and interrelated methodological flaws. These flaws severely undermine the validity of the benchmark and the reliability of its conclusions.\n1. Unverified Evaluation Reliability\nThe paper's core weakness lies in its evaluation method. The authors use GPT-5-mini as an \"LLM-as-Judge\" to automatically score model outputs.\n- Problem: This is a highly complex, expert-level reasoning task across five specialized domains (law, math, philosophy, etc.). The paper provides no evidence or validation study to prove that GPT-5-mini's scoring aligns with the judgment of human domain experts (e.g., a law professor or a mathematics PhD).\n2. Agent Data Contamination Vulnerability\nThe paper claims Agents (like OAgents) outperform LLMs, attributing this to their capabilities. However, the experimental design has a fatal \"open-book exam\" flaw.\n- Problem: 100% of the evaluation questions are sourced from publicly available, top-tier journal articles from 2023-2025. The tested Agents are permitted to use web search tools. This means an Agent can almost certainly find and \"read\" the original source paper for the question.\n- Impact: The paper claims to test the \"ability to acquire and reason over academic knowledge.\" However, this design cannot distinguish between \"logical reasoning from scratch\" and \"information retrieval + answer extraction + paraphrasing.\" The high Agent scores (up to 65.1%) are likely inflated and may test search ability, not the \"high-level reasoning\" the paper purports to measure.\n3. Flawed Construct Validity: Conflating \"Reasoning\" with \"Knowledge\"\nThis is the most fundamental issue: the benchmark likely fails to test \"general reasoning ability\" and instead tests \"memory of specific, narrow knowledge.\"\n- Problem: The experimental results show that Claude-4-sonnet, a model widely recognized for strong reasoning in other domains (like math and coding), scores a 0 on this benchmark.\n- Analysis: This contradictory finding strongly suggests that ACADREASON does not test general, transferable logical reasoning, but rather whether a model happens to \"know\" the frontier theories from these 50 specific papers.\n- Impact: The benchmark's construct validity is highly questionable.\n  - For LLMs (like GPT-5): The test is more of a \"memory test\" (i.e., were these 2023-2025 papers in its training data?).\n  - For Agents (like OAgents): The test is a \"search test\" (i.e., can it find the paper? See Flaw 2).\n\nOverall Conclusion: The benchmark fails to successfully isolate the variable of \"reasoning ability\" from \"specific knowledge-base.\" Therefore, the paper's conclusion that models like GPT-5 \"lack reasoning ability\" is unfounded."}, "questions": {"value": "Based on the methodological analysis of the paper, we kindly request clarification and supplementary evidence regarding the following two core issues:\n1. Regarding the Benchmark's Construct Validity\nThe paper claims that ACADREASON is designed to evaluate a model's \"deep reasoning ability.\" However, the experimental results (for example, Claude-4-sonnet, which is widely regarded as having strong reasoning abilities, scoring 0) strongly suggest that the benchmark may be testing memory of specific, narrow, frontier knowledge (for LLMs) or search-and-extraction capabilities (for Agents), rather than transferable, general logical reasoning.\n- Question: Can the authors provide additional evidence or control experiments to demonstrate that ACADREASON genuinely measures the core variable of \"deep reasoning\" and has effectively isolated it from potential confounding variables such as \"specific knowledge-base\" and \"information retrieval ability\"?\n2. Regarding the Evaluation's Reliability\nThe paper's conclusions (particularly the extremely low model scores) are entirely dependent on the results from using GPT-5-mini as an \"LLM-as-Judge.\" Given that these tasks span five highly specialized and complex domains (e.g., Law, Mathematics, Philosophy), the evaluation difficulty far exceeds that of common tasks.\n- Question: Can the authors provide reliability validation for GPT-5-mini's role as the judge? For example, was an Inter-Annotator Agreement (IAA) analysis conducted between GPT-5-mini's scores and the scores from human domain experts (such as law professors or mathematics PhDs)? If such evidence is lacking, how can we trust the accuracy of these automated evaluation results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1TZrWGmfz1", "forum": "vl0hQuluv4", "replyto": "vl0hQuluv4", "signatures": ["ICLR.cc/2026/Conference/Submission5891/Reviewer_Zbd9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5891/Reviewer_Zbd9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806827875, "cdate": 1761806827875, "tmdate": 1762918328174, "mdate": 1762918328174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ACADREASON, a multi‑domain benchmark intended to test high‑level academic reasoning of LLMs (and agents). It uses 50 expert‑constructed problems drawn from theoretical papers in computer science, economics, law, mathematics, and philosophy. Construction proceeds via (i) paper selection, (ii) extraction of a formal question with a golden answer, and (iii) derivation of question‑specific checklists and three types of hints (background/definitions/methodology). Evaluation adopts an LLM‑as‑Judge scheme (GPT‑5‑mini) with two metrics: Pass Rate (probability of full match to the golden answer) and Checklist Score (probability of meeting checklist criteria). Experiments show low pass rates for state‑of‑the‑art LLMs, higher but still limited scores for agents, and gains when methodology hints are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well motivated. It shows a clear gap in current reasoning benchmarks. By collecting research-level problems from recent top venues, the benchmark is both meaningful and timely.\n* The task design encourages reasoning. Each question is self-contained, with a golden answer, a short checklist, and simple hints (background, definition, methodology). This supports step-by-step solutions and makes error analysis easier.\n* The evaluation has broad coverage. It tests many state-of-the-art models and several agent systems across five domains. Domain-level results and hint ablations make the findings clear and easy to compare."}, "weaknesses": {"value": "- The dataset is small. While the examples are high quality, the limited size reduces representativeness. A semi-automatic or LLM agent system might help scale up questions.\n- The evaluation relies on a single judge (GPT-5-mini). There is no human calibration or test of consistency. Adding multiple judges (from different models, or more advanced LLM-as-Judge methods) and a small human study would make the results more reliable. \n\nOverall, the work is timely and useful as a benchmark. But it reads more like a benchmark release (more suitable for DMLR). For ICLR, I expect stronger methodological innovation—such as a calibrated multi-judge evaluation or a scalable automated pipeline—would make the contribution more suitable."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IWHezcY1xt", "forum": "vl0hQuluv4", "replyto": "vl0hQuluv4", "signatures": ["ICLR.cc/2026/Conference/Submission5891/Reviewer_k3t6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5891/Reviewer_k3t6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988807808, "cdate": 1761988807808, "tmdate": 1762918327884, "mdate": 1762918327884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}