{"id": "EhqJWMSElB", "number": 17701, "cdate": 1758279458870, "mdate": 1759897159426, "content": {"title": "Rewards Simplified: Reducing Risk in RL for Cyber Defence", "abstract": "Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse positive rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.", "tldr": "This work examines sparse and dense reward functions for autonomous cyber defence RL agents in multiple environments, introducing a ground truth score and adjusted reliability metrics for comprehensive policy evaluation.", "keywords": ["Reinforcement Learning", "Autonomous Cyber Defence", "Network Security"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c0c9155265a71f3c4b096aba89afc3f26cecd26.pdf", "supplementary_material": "/attachment/0e9c1f8abc707a310defb7eeca4a8b1223d39060.zip"}, "replies": [{"content": {"summary": {"value": "In the area of autonomous cyber defence, this paper provides an evaluation of various choice of reward functions, arguing that sparse rewards provide for improved performance and reliability. The paper conducts experiments in two established cyber-gym environments, and considers the effects of network size, etc. to the evaluation. As part of their approach, a scoring method is used that it is claimed to address some of the shortcomings of extant approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written in general. The experiments conducted are reasonably comprehensive in their motivated scope (though with shortcomings noted further down).\n\nThe arguments for the change to the ground truth scoring used and the reliability evaluation chosen are clear."}, "weaknesses": {"value": "One of my main concerns re this paper may stem from a misunderstanding. As such, I am happy to increase my score if it is established as such. But the results seem to suggest that consistently SP and SPN perform better than SN, DN and CDN. Naively to me, this indicates that postive-focused rewards functions could actually be the distinguishing beneficial feature rather than sparse rewards necessarily. Further, it seems to be that SN is worse than DN, which is not supporting of that fact that sparseness is the winning factor in the rewards formulation. \n\nIt would have been good to see results also for older metrics which the authors have built upon to come up with the metrics they used, i.e. in order to see the benefit of the improved metrics.\n\nOverall, the proposed metrics and rewards are relatively simple. The main contribution of the paper is incremental, and the evaluation of these incremental changes is potentially not comprehensive enough or conclusive."}, "questions": {"value": "I would appreciate the authors' comments on the question above as to why the results show that e.g. sparseness rather than, say, positivity is the key element of an improved rewards function. And, of course, any other responses to the potential weaknesses noted would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I6srKwYPck", "forum": "EhqJWMSElB", "replyto": "EhqJWMSElB", "signatures": ["ICLR.cc/2026/Conference/Submission17701/Reviewer_dku5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17701/Reviewer_dku5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760678237850, "cdate": 1760678237850, "tmdate": 1762927542864, "mdate": 1762927542864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a study on the importance of specific considerations for the reward function in the Yawning Titan and MiniCAGE cyber gyms. They propose a ground truth score to better capture intra-step compromise during episodes and use this to measure the effectiveness of sparse and dense reward functions. In addition to ground truth score, the authors also measure how varied reward functions affect reliability during training, and risk after training. Their experiments showed that sparse reward functions achieved better scores, better reliability, and lower risk across different environment sizes, action-spaces, and agent orderings when compared to dense reward functions."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The use of the MiniCAGE environment strengthens the argument for this method’s utility across varied environments of growing complexity. \n* Varied step order is another interesting area that the authors explore. Allowing the red agent to sometimes take 2 steps in a row is comparable to uncertain timing if agents were brought from the discrete world of simulation to a real-world system. \n* The structure of the paper and the writing is strong. All experiments are repeatable from the body of the text."}, "weaknesses": {"value": "* Using a line graph for Yawning Titan is overly simplistic. Making the line longer introduces very little additional complexity as the attack path is still the same, just longer. \n* Though the CDN function that comes with the environment may be suboptimal for agent training, it is also the metric by which we score models. In the CAGE environment, certain hosts like the OpServer are more important than others, certain actions have associated costs. And while the inclusion of Tables 18 and 19 in Appendix F help to show that SP and SPN do indeed have lower OpServer impacts, and use fewer Restore actions, we already have a “Ground truth” reward function for CAGE: the environment’s default reward function. The ground truth function should reflect the goals of the environment designers; if all hosts were weighted the same, that would be reflected in the reward function. The authors could make a much stronger argument for their method by showing that using SP or SPN during training results in better scores from the environment’s reward function directly (which appears to be the case). If this is not the case, then they are optimizing their agents for an entirely different goal; it is unclear why one should expect an agent trained to maximize “goal 1” would also maximize “goal 2” if the two goals are not strongly correlated. \n* There should be an additional policy analysis of the YT agents. It is unclear if the CDN agent is more “reluctant” to use the restore and decoy actions because they have associated costs, leading to lower ground truth scores. \n\n\nMinor issues/nitpicks: \n* I disagree with the authors’ characterization of the B-Line agent as the harder agent in Section 3.2. The leaderboard for the original CAGE-2 contest shows Meander was the most difficult agent for submissions. \n* Please bold the best values in Tables 2-6\n* The authors state on numerous occasions that “sparsely rewarded polices perform better”, but this is only the case for SP and SPN. SN is consistantly the worst policy evaluated.  \n* Sec 3.2: The “A” in CAGE stands for “autonomous” not “autonomy”."}, "questions": {"value": "* How does each reward function compare wrt the environments’ original reward functions? If they perform worse according to the default reward function, why should we say that the global reward is a better metric? If they perform better, why? \n* Are agents trained using only red-blue action ordering before being evaluated in the different environments? Would varying move order during training have a stronger effect on agent policy than the choice of reward function? \n* Is it possible to evaluate agent action order in the MiniCAGE environment as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CmubSB9SOc", "forum": "EhqJWMSElB", "replyto": "EhqJWMSElB", "signatures": ["ICLR.cc/2026/Conference/Submission17701/Reviewer_uXAQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17701/Reviewer_uXAQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761339080717, "cdate": 1761339080717, "tmdate": 1762927542494, "mdate": 1762927542494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the use of sparse and dense reward functions in two cyberattack-defense gym environments, YT and CAGE 2. The motivation is to understand whether dense rewards may introduce bias during training and whether sparse rewards can improve effectiveness and reliability while reducing risk. The authors evaluate multiple sparse and dense reward functions, including the standard dense reward functions used over multiple network sizes and different agent orders. The results show that sparse rewards lead to better performance and reliability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes an interesting observation that the dense, highly engineered rewards in cyber gym environments used for training autonomous cyber defence (ACD) agents may be misleading and limit the agents' performance. Although similar observations have been made in other RL settings, such as training board game agents, a systematic study of sparse rewards in the ACD settings was lacking. The evaluation using the YT and CAGE environments, including their standard dense rewards, is convincing. These results reveal flaws in the design of reward functions in current ACD gym environments, making a valuable contribution."}, "weaknesses": {"value": "1. While the observations are interesting, a deep analysis of the results is lacking. In particular, it would be interesting to identify which components in the dense reward functions lead to low scores and instability in YT and CAGE. For example, for YT, it seems that all dense rewards only provide negative signals, but no positive signals. I wonder if that's why the dense rewards perform worse than SPN and SP. This is also consistent with the fact that SN is often the worst. From that perspective, dense rewards actually perform better than SN by providing more detailed feedback. Hence, the conclusion that sparse rewards are always better is questionable even for the relatively simple scenarios considered in the paper. Their generalization to other settings is even less unclear.  \n2. The results were obtained assuming simple attack strategies. I wonder if similar observations hold for more sophisticated attacks, e.g., when the attacker also uses RL. \n3. The paper identifies a problem with current cyber gym environments, where agents cannot \"reliably distinguish between states in which nodes have been compromised and those in which no compromise occurred.\" I wonder if that affects the simulation results. That is, can one modify the simulators to provide the correct reward signals to the agents?"}, "questions": {"value": "Please see the discussions on weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYu2Se37tw", "forum": "EhqJWMSElB", "replyto": "EhqJWMSElB", "signatures": ["ICLR.cc/2026/Conference/Submission17701/Reviewer_xvQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17701/Reviewer_xvQ3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779466621, "cdate": 1761779466621, "tmdate": 1762927542028, "mdate": 1762927542028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}