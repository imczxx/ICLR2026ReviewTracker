{"id": "0wSlFpMsGb", "number": 25369, "cdate": 1758367238097, "mdate": 1759896723396, "content": {"title": "Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training", "abstract": "Large Language Models (LLMs) are pre-trained on large data from different sources and domains. These data most often contain trillions of tokens with large portions of copyrighted or proprietary content, which hinders the usage of such models under AI legislation. This raises the need for truly open pre-training data that is compliant with the data security regulations. In this paper, we introduce Common Corpus, the largest open dataset for LLM pre-training. The data assembled in Common Corpus are either uncopyrighted or under permissible licenses and amount to about two trillion tokens. The dataset contains a wide variety of languages, ranging from the high-resource European languages to some low-resource languages rarely represented in pre-training datasets. In addition, it includes a large portion of code data. The diversity of data sources in terms of covered domains and time periods opens up the paths for both research and entrepreneurial needs in diverse areas of knowledge. In this paper, we present the detailed provenance of data assembling and the details of dataset filtering and curation. We train two small language models on Common Corpus and find that the resulting model performs comparably to other models of their size, indicating that our dataset is suitable for multilingual pretraining. Common Corpus represents a key contribution to the ecosystem for open science research on large language models.", "tldr": "We assemble and release the largest truly open multilingual dataset for LLM pre-training consisting of 2 trillion tokens", "keywords": ["dataset", "pre-training", "large language models", "open data", "open science", "multilingual"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e141458035fcff8c02d4916469b622af70d94021.pdf", "supplementary_material": "/attachment/045fb9a31e057a27cf6dafc3e64ccda88fe88900.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces Common Corpus, a 2T-token multilingual dataset built entirely from open or permissively licensed sources (public domain, CC, open code). It aims to offer a legally clean, transparent alternative to web-scraped corpora. The authors describe the collection process, cleaning (PII removal, OCR correction, toxicity filtering), and train two small models showing comparable multilingual performance to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and relevant: strong contribution to open and compliant LLM research.\n- Impressive scale (2T tokens) and careful documentation of provenance and licenses.\n- Multilingual coverage beyond English, rare for open corpora.\n- Clear adherence to emerging best-practice frameworks (e.g., dataset documentation, PII filtering).\n- Demonstrates feasibility through working models and released tools."}, "weaknesses": {"value": "- Empirical section is limited, only small models and a few benchmarks.\n- No clear comparison to similarly “open” corpora (e.g., Dolma, KL3M) in terms of quality or coverage.\n- Curation process, though detailed, lacks quantitative measures of data quality after filtering.\n- Language balance is heavily skewed to English (~50%).\n- Evaluation of ethical filtering (toxicity, PII accuracy) could be better substantiated."}, "questions": {"value": "- How scalable is the current pipeline to truly support trillion-token multilingual expansion?\n- How do the authors ensure consistent quality across OCRed historical data?\n- Could releasing the filtering tools lead to reproducibility or bias-transfer risks?\n- Do they plan to release validation splits or subsets for standardized benchmarking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C1sxAsc5X2", "forum": "0wSlFpMsGb", "replyto": "0wSlFpMsGb", "signatures": ["ICLR.cc/2026/Conference/Submission25369/Reviewer_GQ3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25369/Reviewer_GQ3w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728757558, "cdate": 1761728757558, "tmdate": 1762943416500, "mdate": 1762943416500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment [1/2]"}, "comment": {"value": "We are grateful to all reviewers for their constructive feedback and valuable suggestions. In this general comment, we would like to present several tables to reference in our replies. \n\nFirst, we show the general comparison of Common Corpus to other popular open corpora across the dimensions of licensing, multilinguality, domain diversity (e.g., whether the corpus is only code or only governmental data vs corpora combining multiple diverse domains), and reliance on data crawled from the web. As the table shows, Common Corpus is the only open dataset combining all factors. \n\n\n| Dataset  | DCAD-2000 | KL3M | ROOTS | Common Pile | FineWeb | C4 | Dolma | Common Corpus |\n|------------------|-----------|------|-------|-------------|---------|----|-------|---------------|\n| Permissive data     |    N       | Y    | N     | Y           |    N   |   N    | N       | Y      |\n| Multilingual             | Y         | N    | Y     | N           |     Y    |    N    | N     | Y        |\n| Multidomain            | Y         | N    |   Y     | Y           |   Y      |     Y   |  Y       | Y        |\n| Beyond Web Crawl | N        |  Y    |  N     | Y           | N      | N     |    Y     | Y        |\n\n\nIn addition, to provide more details on multilinguality, we present the distribution of languages in the top-30 (top-10 are shown in Table 2 of the paper):\n\n| Language (11-20) | # Tokens | Language (21-30) | #Tokens |\n|------------------|----------|------------------|---------|\n| Dutch            | 8.1B     | Bulgarian        | 3.4B    |\n| Danish           | 6.9B     | Lithuanian       | 3.1B    |\n| Slovak           | 5.1B     | Romanian         | 2.9B    |\n| Czech            | 4.8B     | Japanese         | 2.7B    |\n| Indonesian       | 4.4B     | Arabic           | 2.7B    |\n| Estonian         | 4.4B     | Slovenian        | 2.6B    |\n| Hungarian        | 4.1B     | Latvian          | 2.6B    |\n| Swedish          | 4.0B     | Ukrainian        | 2.6B    |\n| Finnish          | 3.9B     | Croatian         | 2.4B    |\n| Maltese          | 3.6B     | Chinese          | 2.2B    |"}}, "id": "rwTFjG61ST", "forum": "0wSlFpMsGb", "replyto": "0wSlFpMsGb", "signatures": ["ICLR.cc/2026/Conference/Submission25369/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25369/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25369/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763491774903, "cdate": 1763491774903, "tmdate": 1763491774903, "mdate": 1763491774903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Common Corpus, an open dataset for pre-training LLMs. It is constituted of texts that are either uncopyrighted or under permissible licenses, for a total of two trillion tokens across a variety of languages and tasks such as coding, as well as a diversity in terms of regions and time. \n\nThe authors go into the data collection and curation process in great detail, and also train two LLMs of small size on the dataset that show that they perform on a similar level to other models of their size across a variety of datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Developing an explicitly curated dataset based on data licensing is important and a crucial contribution to the AI community.\n\nThere is an emphasis on diversity in terms of languages and regions.\n\nThe code used for creating the dataset is available, allowing others to reproduce it.\n\nThe resulting dataset can be filtered based on different criteria, including license and language, which makes it useful for developers and researchers working on specific languages or historical periods.\n\nPersonally Identifiable Information is removed with the Presidio tool, which means that there is almost no risk of data leakage from the trained models."}, "weaknesses": {"value": "- \"Ethical data\" is a very relative/hard-to-define concept -- maybe \"consensual data\" or \"legal data\" would be better alternatives?\n\n- Figure 1 is hard to read because languages such as English, Spanish or French are spoken in multiple places, so simply putting a dot on Madrid or Paris isn't representative of where the language is from \n\n- \"synthetically rewrite the document without the harmful language\" - how is this done and verified? Doesn't introducing synthetically-generated text dilute the corpus?"}, "questions": {"value": "- How are the six collections defined, what are the criteria? \n\n- How are you sure (are you sure) that none of the data is LLM-generated?\n\n- Are the audio transcripts AI- or human- generated?\n\n- How has the set of Wikidata been adapted in natural language? The example provided isn't very clear\n\n- \"Segmentext should work correctly on diverse document formats\" - did you do testing? In general, providing more information about the tools that you developed and how they work would help understand their limitations and applicabliity."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I am not a lawyer, so it would be good to have a second opinion about the types of licenses that have been kept for the corpus, and whether they are indeed permissible for training LLMs on."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6RgsXhH2eE", "forum": "0wSlFpMsGb", "replyto": "0wSlFpMsGb", "signatures": ["ICLR.cc/2026/Conference/Submission25369/Reviewer_LPfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25369/Reviewer_LPfF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857759080, "cdate": 1761857759080, "tmdate": 1762943416242, "mdate": 1762943416242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper accompanies the release of the largest open dataset for LLM pretraining: the Common Corpus. The authors fully emphasize the “open” aspect of their data, providing full transparency around the provenenance, source, and licensing of their data. They also engage in extensive data cleaning and quality improvement steps for different segments of their data. Unlike its recent open data precusors, Common Corpus also includes substantial multilingual data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The writing of this paper was very clear, and this work was done with much care, detail, and rigor. For example, the authors “involve local communities” in gathering data from diverse sources and did not machine-translate the multilingual component of their dataset. This resource is an invaluable contribution and far exceeds its predecessors in size and composition. Aside from data, this work also contributes several data cleaning tools."}, "weaknesses": {"value": "I understand that the main focus of this paper is on the data and not on model training, so it is okay that the model training results aren’t groundbreaking. One minor detail is that your choice of benchmarking results to show in Section 5 is a little strange. You focus on a few multilingual benchmarks, but compare against OLMo 1B, which may have been intended to be monolingually English. \n\nIt would have been nice to see a clear tabulation (like, a table) of how this dataset overlaps, extends, or differs from existing open datasets."}, "questions": {"value": "Data may be copied and then mislicensed. Have you checked for overlap between your dataset and data that has less permissive licenses? This might be really tricky to do, so I am just curious. \n\nIs there a way for someone to remove data about themselves or produced by themselves from this dataset? Right to be forgotten, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C4Fuk8oFGb", "forum": "0wSlFpMsGb", "replyto": "0wSlFpMsGb", "signatures": ["ICLR.cc/2026/Conference/Submission25369/Reviewer_BY7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25369/Reviewer_BY7t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995439639, "cdate": 1761995439639, "tmdate": 1762943415880, "mdate": 1762943415880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Common Corpus, a ~2T-token multilingual dataset  for LLM pre-training that contains only data that are in the public domain or released under permissive open licenses.  The authors document data sources, licensing, cleaning tools (Segmentext, OCRonos, Celadon, PII filtering), and present benchmark results for small Llama-style models trained solely on this corpus.\n\nOverall, this is a valuable contribution to open LLM research. The motivation is strong, and the authors make extensive efforts to demonstrate the usefulness of the dataset for pre-training purposes. \n\nHowever, several aspects including the very Euro-centric nature of the multilingual coverage and lack of quantitative information about ambiguous licenses need further clarification."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Timely contribution to open-data infrastructure for LLMs, especially given that many existing datasets had to be taken down due to license issues.\n\n2. Transparent and detailed documentation of dataset provenance, content, and filtering pipeline\n\n3. Releases and discusses useful tools for OCR correction and sentence segmentation\n\n4. Demonstrates that open data sourced from only permissive licenses can still yield competitive model performance."}, "weaknesses": {"value": "1. The claim of “multilingual diversity” is overstated as the dataset is heavily Euro-centric. The top ten languages are all European, and there is little inclusion of African or Asian languages.\n\n2. It is not clear how trustworthy the licensing information is. The authors do not report how many documents had ambiguous, conflicting, or missing license information.\n\n3. For certain types of data, licenses can apply inconsistently across sub-components. It is not clear how these were disambiguated.\n\n4. The data pre-processing pipeline, including Segmentext and OCRonos are underdescribed. It is not clear if any de-duplication was done or required.\n\n5. The evaluation lacks detail on per-language performance and broader benchmark coverage."}, "questions": {"value": "1. Can the authors provide a full language inventory and token count distribution, including low-resource languages?\n\n2. How are ambiguous or missing licenses handled, and what proportion of the dataset do they represent?\n\n3. What steps were taken to ensure deduplication across overlapping sources?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bkY1AsL5B0", "forum": "0wSlFpMsGb", "replyto": "0wSlFpMsGb", "signatures": ["ICLR.cc/2026/Conference/Submission25369/Reviewer_aTGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25369/Reviewer_aTGr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150353588, "cdate": 1762150353588, "tmdate": 1762943415626, "mdate": 1762943415626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}