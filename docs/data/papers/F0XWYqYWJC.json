{"id": "F0XWYqYWJC", "number": 12439, "cdate": 1758207841555, "mdate": 1763277309464, "content": {"title": "The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework", "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting _under-confidence_, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce additional calibration components, which fail to capture the intrinsic relationship between the model and the prediction confidence, resulting in limited theoretical guarantees and increased computational overhead. To address this issue, we propose a simple yet efficient graph calibration method. We establish a unified theoretical framework revealing that model confidence is jointly governed by class-centroid-level and node-level calibration at the final layer. Based on this insight, we theoretically show that reducing the weight decay of the final-layer parameters alleviates GNN under-confidence by acting on the class-centroid level, while node-level calibration acts as a finer-grained complement to class-centroid level calibration, which encourages each test node to be closer to its predicted class centroid at the final-layer representations.", "tldr": "", "keywords": ["Graph neural networks", "Confidence calibration", "Uncertainty estimation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/870dcf7da357d68f5c6e93221967f22aac33d282.pdf", "supplementary_material": "/attachment/68e5376d404995f1891bf2205ab90b904b42045d.zip"}, "replies": [{"content": {"summary": {"value": "The paper provides a theoretical and practical framework (SCAR) for calibrating GNNs by adjusting final-layer weight decay and introducing a node-level calibration step. It proves that reducing final-layer regularization enlarges class-centroid distances, alleviating under-confidence. A post-hoc correction then nudges node embeddings toward their predicted class centroids, further improving calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong theoretical grounding linking weight decay to confidence underestimation.\n2. Dual-level calibration (centroid + node) unifies model-intrinsic and post-hoc methods.\n3. Training-free node-level adjustment ensures efficiency and interpretability.\n4. Extensive experiments show lower ECE and runtime than prior calibrators"}, "weaknesses": {"value": "I haven't seen too much weakness. But I am wondering whether node-level adjustment may amplify overconfidence for misclassified nodes."}, "questions": {"value": "1. How does the method behave when the predicted class centroid is incorrect?\n2. Can the framework generalize to heterophilous or dynamic graphs? (Optional)\n3. How sensitive are results to the final-layer λ schedule?\n4. Could centroid regularization be integrated during training for joint optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Oxi79BmCi2", "forum": "F0XWYqYWJC", "replyto": "F0XWYqYWJC", "signatures": ["ICLR.cc/2026/Conference/Submission12439/Reviewer_LjB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12439/Reviewer_LjB2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914350271, "cdate": 1761914350271, "tmdate": 1762923325388, "mdate": 1762923325388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCAR, a nonparametric GNN calibration framework that analyzes miscalibration of GNNs in model perspective. SCAR theoretically shows that reducing a weight decay in the final layer leads to higher class separation, which can mitigate underconfidence of GNNs. It further performs node-level calibration to make the test node’s representation closer to the corresponding label’s centroid."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a theoretical connection between underconfidence of GNNs and final layer’s weight decay, which is valuable given the lack of theoretical analysis in GNN calibration literature.\n- The proposed method is simple yet effective, avoiding the need to train additional calibration networks as required by many existing methods.\n- Extensive experiments shows that SCAR substantially reduces ECE compared to prior baselines, as well as maintaining original classification accuracy of GNNs."}, "weaknesses": {"value": "- The proposed node-level calibration assumes that pushing test nodes toward their predicted class centroids improves confidence, which may not hold under settings such as out-of-distribution (OOD) conditions. For instance, in OOD graphs, pushing test nodes toward centroids learned from training data can degrade calibration.\n- If the original GNNs are trained with zero weight decay, the proposed method may be partially inapplicable.\n- While SCAR is efficient, it needs to search the optimal configuration over three hyperparameters. Although the authors offer practical heuristics in the appendix (e.g., $\\alpha$ should be lower than $\\beta$), it is not guaranteed that such heuristics hold universally."}, "questions": {"value": "- Could the authors show the performance of SCAR on OOD graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e4RDoXJQri", "forum": "F0XWYqYWJC", "replyto": "F0XWYqYWJC", "signatures": ["ICLR.cc/2026/Conference/Submission12439/Reviewer_nyom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12439/Reviewer_nyom"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923178007, "cdate": 1761923178007, "tmdate": 1762923325045, "mdate": 1762923325045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors conduct a comprehensive analysis of confidence calibration in Graph Neural Networks (GNNs). They first theoretically demonstrate that weight decay applied to the final-layer parameters exacerbates under-confidence by collapsing class centroids toward the origin, thereby reducing class separability. To address this, the authors propose reducing the final-layer weight decay to enhance inter-class distinction and improve confidence calibration at the class-centroid level. Additionally, they introduce a node-level calibration strategy as a fine-grained complement, which encourages each test node to move closer to its predicted class centroid while distancing itself from others in the final-layer representation space, thus improving individual calibration. Finally, they develop a unified theoretical framework that shows model confidence is jointly governed by both class-centroid-level and node-level calibration, underscoring the completeness and coherence of their approach. Extensive experiments demonstrate that the proposed method consistently outperforms state-of-the-art techniques in terms of both effectiveness and efficiency across various datasets and settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors are the first to theoretically show that final-layer weight decay aggravates GNN under-confidence, and they mitigate this by reducing the decay.\n\n2. They propose a training-free node-level calibration method as a fine-grained complement to class-centroid-level calibration.\n\n3. They develop a unified theoretical framework showing that both calibration levels jointly govern model confidence, and validate the method’s superiority across diverse settings."}, "weaknesses": {"value": "1. Missing important related work: Given that the paper focuses on confidence calibration, it is concerning that several key papers in the area of uncertainty estimation or calibration for GNNs are not cited or discussed [1-4].\n\n2. Limited baselines: The experimental comparisons would benefit from the inclusion of recent calibration methods [5]\n\n3. Restricted backbone models: The authors only evaluate their method on GCN and GAT. While these are classical models, they are no longer sufficient to represent the landscape of modern GNN architectures. Including additional backbones like GraphSAGE would strengthen the empirical claims and validate the method’s generality.\n\n[1] Uncertainty quantification over graph with conformalized graph neural networks. NeurIPS 2023\n\n[2] Energy-based Epistemic Uncertainty for Graph Neural Networks\n\n[3] Uncertainty Aware Semi-Supervised Learning on Graph Data. NeurIPS 2020\n\n[4] Calibrate Automated Graph Neural Network via Hyperparameter Uncertainty. CIKM 2022\n\n[5] GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks. ICLR 2025"}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xXI2aOwwNF", "forum": "F0XWYqYWJC", "replyto": "F0XWYqYWJC", "signatures": ["ICLR.cc/2026/Conference/Submission12439/Reviewer_kaCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12439/Reviewer_kaCQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939675321, "cdate": 1761939675321, "tmdate": 1762923324618, "mdate": 1762923324618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of confidence miscalibration in graph neural networks. The authors observe that GNN confidence is influenced by two factors in the final layer, namely Class-Centroid-Level Calibration and Node-Level Calibration. Building on this insight, they propose the SCAR framework, which unifies these two calibration components into a single theoretical framework, enabling more effective confidence calibration in GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is the paper's most significant strength. It moves beyond heuristic-based calibration by providing a rigorous theoretical analysis.\n\n2. The proposed SCAR method consistently outperforms a wide range of strong baselines across multiple datasets."}, "weaknesses": {"value": "1. The node-level calibration is refined in Eq. 10 to account for the structural bias of GNNs (nodes closer to training data get more similar representations). While this is a thoughtful addition, its evaluation is limited. An ablation study showing the performance gain of using two parameters $\\alpha$ and $\\beta$ over a single one would have strengthened this claim.\n\n2. The details of the high-order neighbors of the training node is not well specified.\n\n3. Sensitivity analysis on hyper-parameter $\\lambda^{(k)}$ is not provided."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AGCwrLyrAS", "forum": "F0XWYqYWJC", "replyto": "F0XWYqYWJC", "signatures": ["ICLR.cc/2026/Conference/Submission12439/Reviewer_BypW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12439/Reviewer_BypW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145087920, "cdate": 1762145087920, "tmdate": 1762923324148, "mdate": 1762923324148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}