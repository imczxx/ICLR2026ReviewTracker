{"id": "FI075FwAnb", "number": 15989, "cdate": 1758258158880, "mdate": 1759897268600, "content": {"title": "Geometric Constraints for Small Language Models to Understand and Expand Scientific Taxonomies", "abstract": "Recent findings reveal that token embeddings of Large Language Models (LLMs) exhibit strong hyperbolicity. This insight motivates leveraging LLMs for scientific taxonomy tasks, where maintaining and expanding hierarchical knowledge structures is critical. Although potential, generally-trained LLMs face challenges in directly handling domain-specific taxonomies, including computational cost and hallucination. Meanwhile, Small Language Models (SLMs) provide a more economical alternative if empowered with proper knowledge transfer. In this work, we introduce SS-Mono (Structure-Semantic Monotonization), a novel pipeline that combines local taxonomy augmentation from LLMs, self-supervised fine-tuning of SLMs with geometric constraints, and LLM calibration. Our approach enables efficient and accurate taxonomy expansion across root, leaf, and intermediate nodes. Extensive experiments on both leaf and non-leaf expansion benchmarks demonstrate that a fine-tuned SLM (e.g., DistilBERT-base-110M) consistently outperforms frozen LLMs (e.g., GPT-4o, Gemma-2-9B) and domain-specific baselines. These findings highlight the promise of lightweight yet effective models for structured knowledge enrichment in scientific domains.", "tldr": "", "keywords": ["Small LM", "Hyperbolic Deep Learning", "Taxonomy Structure"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c0a771b416e9a723447c004e6ea149d3a81c5996.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SS-Mono, a pipeline targeting efficient and accurate scientific taxonomy expansion by transferring hierarchical knowledge from Large Language Models to Small Language Models. SS-Mono integrates a local taxonomy augmentation phase using LLMs, a self-supervised geometric fine-tuning process with hyperbolic constraints, and LLM-based calibration for candidate ranking. The approach is evaluated on several taxonomy expansion benchmarks (SemEval-Food, WordNet-Verb, and MeSH), showing that a fine-tuned SLM outperforms both frozen LLMs and domain-specific baselines for both leaf and non-leaf node insertions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The structure of SS-Mono is modular and well-motivated: it combines structure-dominated (hyperbolic metric) and context-dominated (semantic, LLM-augmented) encoders. This dual design is well illustrated in Figure 1, which shows how LLMs are used to enrich edge semantics and how structural relations are composed.\n\nS2: Extensive experiments (see Tables 2 and 3) cover multiple datasets and include a wider range of both classical and recent baselines, including neural and LLM-based approaches. The results show consistent, substantial gains of SS-Mono, especially when augmented with LLM outputs, across MR, MRR, and Recall/Precision metrics.\n\nS3: Self-supervised training enables the model to leverage existing taxonomies without costly annotation. The mathematical formulations (see Equations 1-11 and Cone Loss in Section 3.2) are clear, concise, and consistent with state-of-the-art hyperbolic learning approaches."}, "weaknesses": {"value": "W1: While the paper claims that \"LLMs have potential but are not ready to be directly deployed,\" the LLM calibration component appears to be more of an optional enhancement than a robust part of the pipeline. Figure 2 exposes high failure rates in LLM output for reranking (frequent hallucinated edges or incomplete lists), and Section 4.3 provides only a partial mitigation of this challenge. The improvements from LLM calibration, as shown in Table 3, are not always consistent or predictable.\n\nW2: The model relies on self-supervised negative sampling and LLM-augmented context for candidate positions. The precise method for sampling hard negatives is tailored closely to the taxonomy structure as described in Section 3.4, but it is unclear whether the negative sampling strategy introduces bias or overestimates the model’s robustness to challenging/far-out queries.\n\nW3: The pipeline discusses SLM fine-tuning with geometric constraints, but does not deeply analyze (or ablate) alternative LLM fine-tuning approaches (domain adaptation, prompt-tuning, etc.). This limits direct attribution of SLM’s advantages to the geometric approach, rather than simply to the smaller/faster architecture or training regime."}, "questions": {"value": "Q1: Have the authors conducted any systematic analysis of the types or frequency of semantic drift or spurious insertions caused by LLM-augmented edge descriptions? If not, can they provide statistics or example cases showing where LLM guidance introduces incorrect or misleading hierarchy insertions?\n\nQ2: Can the negative sampling strategy in self-supervised optimization be further detailed or ablated? Is there evidence that specific hard negative choices strongly affect generalization versus random negatives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYthe4ZZ8N", "forum": "FI075FwAnb", "replyto": "FI075FwAnb", "signatures": ["ICLR.cc/2026/Conference/Submission15989/Reviewer_APGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15989/Reviewer_APGH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804289633, "cdate": 1761804289633, "tmdate": 1762926196791, "mdate": 1762926196791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores how SLMs can effectively perform scientific taxonomy expansion by leveraging geometric constraints and guidance from LLMs.  \nAuthors propose SS-MONO (Structure-Semantic Monotonization), which integrates three components: (1) local taxonomy augmentation by frozen LLMs, (2) self-supervised fine-tuning of an SLM (e.g., DistilBERT) with hyperbolic constraints to preserve hierarchical transitivity, and (3) LLM-based calibration.  \nExperiments on SemEval-Food, MeSH, and WordNet-Verb show that SS-MONO outperforms both traditional graph-based methods (e.g., TaxoExpan, TMN, QEN) and frozen LLMs like GPT-4o-mini, establishing SLMs as cost-efficient and competitive models for scientific taxonomy expansion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel perspective that connects hyperbolic geometry in LLM embeddings with taxonomy reasoning.\n- The paper is well-motivated and technically detailed.\n- The self-supervised training approach removes the need for human annotation, and the idea of “borrowing knowledge” from LLMs while retaining the efficiency of SLMs is practically appealing.  \n- The empirical results are comprehensive and support the claim of the paper."}, "weaknesses": {"value": "- The impact of geometric regularization compared to simpler fine-tuning is not explained in the main text. I think it’s worth discussing the main takeaway of the ablation studies in 4.2."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f9rUJVV242", "forum": "FI075FwAnb", "replyto": "FI075FwAnb", "signatures": ["ICLR.cc/2026/Conference/Submission15989/Reviewer_Ghyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15989/Reviewer_Ghyo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971756598, "cdate": 1761971756598, "tmdate": 1762926196420, "mdate": 1762926196420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SS-MONO (Structure-Semantic Monotonization), a novel pipeline for efficient and accurate scientific taxonomy expansion. Recognizing the strong hyperbolicity in Large Language Model (LLM) embeddings, SS-MONO addresses the high computational cost and hallucination issues associated with directly using LLMs on domain-specific taxonomies. SS-MONO leverages LLM augmentation and distills this knowledge into Small Language Models (SLMs) through self-supervised fine-tuning enforced by geometric constraints. Key modules include a structure-dominated encoder using hyperbolic representation learning to preserve hierarchy (monotonicity) and a context-dominated encoder for contextual semantics. Experiments show that a fine-tuned SLM (e.g., DistilBERT-base-110M) consistently outperforms frozen LLMs and deep learning baselines on expansion tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficiency and Cost-Effectiveness\n\nSS-MONO implements an LLM-to-SLM distillation approach, fine-tuning a Small Language Model (SLM), such as DistilBERT-base-110M. This strategy provides a practical and economical alternative to the high computational costs and difficulties associated with directly using Large Language Models (LLMs) on domain-specific taxonomies.\n\n2. Structural Integrity and Superior Overall Performance\n\nThe pipeline utilizes hyperbolic representation learning in its structure-dominated encoder to enforce geometric constraints (monotonicity), which preserves the hierarchical order. This structural awareness allows the fine-tuned SLM to consistently outperform frozen LLMs and domain-specific deep learning baselines overall.\n\n3. Self-Supervised Training\n\nThe entire training process is self-supervised, guided by the existing taxonomy's topology. This design eliminates the need for expensive human labeling efforts for expansion tasks"}, "weaknesses": {"value": "1. Performance on Large-Scale Taxonomies\n\nThe methodology demonstrates significantly lower effectiveness on the large-scale WordNet-Verb dataset (13,936 nodes, depth 12) compared to smaller datasets. SS-MONO's overall average ranking on WordNet-Verb is 1626.52, which is not SOTA and substantially less favorable than its performance on SemEval-Food (239.17) and MeSH (436.82). Does this indicate that the proposed method could have issues in scalability and can be less effective on large-scale Taxonomies?\n\n2. Non-Leaf Volatility with LLM Augmentation Observed\n\nThe performance metrics show that including LLM Augmented Descriptions (AD) does not always enhance intermediate (non-leaf) expansion. For example, on WordNet-Verb, the Non-leaf R@10 metric decreases sharply from 0.099 (SS-MONO w/o AD) to 0.035 (full SS-MONO). Other datasets have the same observation. What could lead to the performance difference between leaf nodes and non-leaf nodes? Is it because non-leaf nodes have more complicated relationship and LLM can hardly have clear annotations?"}, "questions": {"value": "1. Please update the bold format in all the experiment tables in the paper carefully. Currently many bolded numbers in the table are not exactly the one with the best value. This is very confusing and could lead to wrong conclusion.\n\n2. Please address the above two concerns I have."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rUC8YW6Yg7", "forum": "FI075FwAnb", "replyto": "FI075FwAnb", "signatures": ["ICLR.cc/2026/Conference/Submission15989/Reviewer_orUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15989/Reviewer_orUz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995246133, "cdate": 1761995246133, "tmdate": 1762926196001, "mdate": 1762926196001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of taxonomy expansion in scientific domains, specifically adding new concept nodes to an existing taxonomy (precisely in a directed acyclic hierarchy of concepts). The paper proposes a pipeline that borrows knowledge from an LLM and transfers it into SLM, which can then perform the taxonomy expansion efficiently. SS-MONO has three stages: (1) local taxonomy augmentation using an LLM - for each candidate insertion position in the taxonomy (parent node and a child node between which a new concept might be inserted) a pretained LLM is prompted to generate a textual explanation or description of local context (semantic features about the candidate position), (2) fine-tuning a SLM with geometric constraints to rank candidate insertion positions for a query concept (structure-dominated encoder that projects concept embeddings into hyperbolic space by nested entailment cones to keep hierarchical relationships, and a context-dominated encoder that embeds the textual descriptions) - here a key idea to enforce the monotonic ordering (child ≼ query ≼ parent in the embedding space) and (3) LLM-based calibrationof SLM score where to insert the new concept with use of second LLM call to re-rank the top-$k$ predicted positions. Experiments on three benchmarks demonstrate, in some cases, superiority and advantages of SS-MONO over other compared methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a creative integration of ideas from different domains: it combines hyperbolic geometry (for representing hierarchical structure) with LLM-based semantic augmentation in a small-model pipeline. While hyperbolic embeddings have been used for hierarchical tasks before and others have leveraged language models for taxonomy expansion, this provides a limited but still novel work in how it brings these together. Especially the notion of “Structure-Semantic Monotonization” is a novel formulation to ensure monotonicity in latent space (with use of nested entailment cones to enforce transitivity). \n- The technical quality of the work appears high. The paper is thorough in justifying and evaluating the approach.\n- The training scheme is cleverly self-supervised, avoiding manual annotations (removal of nodes predict their insertion)."}, "weaknesses": {"value": "- Ironically, a method motivated by avoiding LLM usage still depends on LLMs at key points. The small model alone does a lot of the work, but the pipeline requires a capable LLM to provide the augmented descriptions and to perform final calibration. In the ablation without LLM augmentations (SS-MONO w/o AD), the small model’s performance, although competitive, is not clearly superior to the best prior methods. \n- The paper does not delve deeply into what the LLM-generated “textual explanations” look like or how consistent their quality is. This is a bit of a black box in the description. If the LLM outputs poor or hallucinated explanations for some candidate positions, does that ever confuse the SLM during training? One could imagine the LLM sometimes generating a misleading context (especially if the taxonomy contains very specialized terms that the LLM isn’t familiar with). The authors did not mention any filtering or human verification of the LLM outputs. It would strengthen the work to either demonstrate that these augmented descriptions are almost always accurate, or to describe measures to handle noise in those descriptions.\n- Due to my understanding, there is no comparison to one of the baseline scenarios when LLM are similarly asked in turns to perform similar steps lika SS-MONO (pair insertion evaluairion, re-ranking)."}, "questions": {"value": "- Can the authors provide more details or examples of the prompt and output used for the LLM when generating the candidate position descriptions? This is currently abstract in the paper. For instance, if the candidate position is between parent concept P and child concept C, do you prompt the LLM with something like “Explain the relationship between P and its subcategory C” or “Give a description of where C fits under P”? And does the LLM output a few sentences describing that taxonomic context? An example would help in understanding what knowledge the LLM is injecting. Additionally, did you observe any instances of the LLM outputting incorrect information about the taxonomy? If so, how did you mitigate that (e.g., do you simply trust whatever the LLM says, or do you have a way to sanity-check it)? Clarifying this will help assess the reliability of the augmentation. And could you please evaluate LLM with the same procedure (subtasks) as SLM (inference).\n- You introduce a complex hyperbolic constraint system for the structure-dominated encoder. Did you compare or ablate this against a simpler approach (for example, a Euclidean encoder or a transformer-based graph encoder without hyperbolic projection)? In Appendix I.1 you mention investigating the role of “geometric deep learning” – can you summarize those findings? It would be insightful to know how much the hyperbolic embedding improved things. Perhaps the model could also be trained in Euclidean space with a learned ordering constraint – would that fail or perform worse? \n- In cases where a concept has multiple true parent locations (non-leaf multi-attachments), how would you operationally use SS-MONO to attach it in all the correct places?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2L0zitmfA7", "forum": "FI075FwAnb", "replyto": "FI075FwAnb", "signatures": ["ICLR.cc/2026/Conference/Submission15989/Reviewer_7uga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15989/Reviewer_7uga"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762437497103, "cdate": 1762437497103, "tmdate": 1762926195533, "mdate": 1762926195533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}