{"id": "m2nupeHqV7", "number": 6575, "cdate": 1757989413086, "mdate": 1759897907281, "content": {"title": "Layer Collaborative Low-Rank Decomposition with Automatic Rank Search for LLM Compression", "abstract": "Large Language Models (LLMs) achieve strong performance but face deployment challenges due to high storage and memory costs. Low-rank approximation via Singular Value Decomposition (SVD) offers an effective compression solution. However, existing SVD-based methods typically compress each weight matrix independently in a layer-wise manner, ignoring the cross-layer interactions within transformer blocks and causing suboptimal performance. Moreover, conventional rank allocation strategies—either greedy or based on singular value decay—are often suboptimal, overlooking the varying sensitivity of different blocks to compression. To address these issue, we propose LC-SVD, a layer collaborative SVD framework with automatic rank search that enables adaptive low-rank compres-\nsion of LLMs. Our approach includes: 1) block-wise collaborative decomposition jointly compresses all linear layers within a transformer block, preserving intra-block structural dependencies and reducing error accumulation. To improve rank allocation, we devise an error-driven rank search strategy that evaluates block sensitivity on calibration data and prioritizes capacity in more critical components via\ncandidate configuration scoring. This ensures better accuracy under fixed resource budgets. The experimental results show that LC-SVD outperforms state-of-the-art SVD-based methods, achieving lower perplexity and higher task performance.", "tldr": "We propose LC-SVD, a novel layer-collaborative SVD framework for compressing LLMs with automatic rank allocation. It jointly decomposes all layers within transformer blocks, preserving intra-block dependencies.", "keywords": ["Large Language Model", "Singular Value Decomposition", "Rank Search"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c3a11783517fa20677d2ddd2ddbf9dbabb6c31d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a post-training compression method called LC-SVD. The method augments standard per-layer SVD with a learnable whitening matrix so that the decomposition better matches the actual calibration data and reduces block-level error accumulation. It further introduces an error-driven rank search that first estimates block sensitivities from perplexity under light compression, then samples non-uniform rank configurations accordingly, and finally selects the best one using calibration perplexity. The paper also details several optimization choices, such as an adaptive stopping criterion and a numerically stable procedure for learning the whitening matrices. Empirical results and ablations show that each component contributes to the final performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The approach is described in sufficient detail, and the paper is clearly written, with experimental settings and evaluation protocols laid out in an organized way.\n* The detailed ablation study showcases the contribution from each component of the method.\n* The topic of post-training compression is practically valuable and interesting, since it could avoid the catastrophic-forgetting issues that can arise in fine-tuning–based compression methods."}, "weaknesses": {"value": "* Structure-aware (or “layer-collaborative,” as the paper calls it) compression/fine-tuning is not new in this area. Prior work such as [1] and [2] has already explored adaptive, layer-dependent low-rank allocation. The related-work section would be stronger with a more complete positioning against these lines of work.\n* Evaluating mainly at 80% parameter retention is not very compelling, since many recent pruning/quantization/compression methods operate at much lower budgets (although typically they are task-oriented, not general-purpose models as the paper investigates). The appendix shows that performance drops substantially when the budget is pushed down, which raises a natural question: in the low-budget regime, is this post-training SVD approach actually preferable to modern quantization or task-specific fine-tuning/pruning pipelines? I would like to hear the authors' thoughts on this.\n* The paper notes that previous methods “lack theoretical optimality under rank constraints,” but the proposed method itself does not provide much theoretical analysis either.\n\n[1] Zhang et al; AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.\n[2] Hua et al; Dynamic Low-rank Estimation for Transformer-based Language Models."}, "questions": {"value": "Does the choice of calibration data affect the final results? For instance, how sensitive is the method to the size of the calibration set or to shifting its source domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bpVUxoskgx", "forum": "m2nupeHqV7", "replyto": "m2nupeHqV7", "signatures": ["ICLR.cc/2026/Conference/Submission6575/Reviewer_fa67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6575/Reviewer_fa67"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791424991, "cdate": 1761791424991, "tmdate": 1762918909952, "mdate": 1762918909952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LC-SVD, a layer collaborative low-rank decomposition framework with automatic rank search for efficient compression of Large Language Models (LLMs). Unlike conventional SVD-based methods that compress each layer independently, LC-SVD jointly compresses all linear layers within a transformer block to preserve cross-layer dependencies and reduce error accumulation. It further introduces an error-driven rank search strategy that allocates ranks adaptively based on each block’s sensitivity to compression, avoiding suboptimal heuristic rank distributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work focuses on an important topic — compressing large models for deployment in resource-constrained environments.\n2. The motivations behind the two proposed improvements are clear: one aims to capture cross-layer interactions during compression, while the other addresses the optimal allocation of parameter budgets in low-rank decomposition."}, "weaknesses": {"value": "1. For “Layer Collaborative Weight Decomposition”, the idea of using gradient descent to learn a whitening matrix seems questionable. If gradient-based optimization is feasible, then directly fine-tuning the pruned low-rank weights would be more straightforward and effective. Even if global backpropagation is impractical, one could still fine-tune each block separately or employ gradient checkpointing to reduce memory overhead. Moreover, the proposed approach appears computationally expensive, since it requires performing SVD at every iteration, as the matrix $S$ changes in each step. In contrast, all compared SVD-based methods in the experiments adopt one-shot pruning without gradient descent. For a fair comparison, these baselines should also include fine-tuning to match the proposed method’s training cost and optimization benefits.\n\n2. For “Error-Driven Rank Search”, the idea of estimating each block’s sensitivity to determine non-uniform sparsity is not novel. Similar concepts have already been explored in prior works such as OWL [1] and ALS [2] for unstructured pruning, as well as ASVD [3] and WeLore [4] for SVD-based compression. To convincingly demonstrate the effectiveness of the proposed rank search, the paper should compare against these non-uniform sparsity allocation baselines rather than only using a uniform sparsity baseline (at least some of them). Moreover, the proposed approach appears also heuristic, by sampling candidate configurations under a prior and selecting the best-performing one. In addition, the improvement brought by this module is quite limited: as shown in the results, the perplexity only decreases slightly from 7.09 (with Layer Collaborative SVD) to 6.95 (full LC-SVD).\n\n3. More recent state-of-the-art SVD-based compression methods, such as Pivoting Factorization [5], are not included in the comparison, which limits the completeness of the experimental evaluation.\n\n4. Typo: Only 1) is written in abstract.\n\n\n[1] Yin, Lu, et al. \"Outlier weighed layerwise sparsity (OWL) a missing secret sauce for pruning LLMs to high sparsity.\" Proceedings of the 41st International Conference on Machine Learning. 2024.\n\n[2] Li, Wei, et al. \"Adaptive layer sparsity for large language models via activation correlation assessment.\" Advances in Neural Information Processing Systems 37 (2024): 109350-109380.\n\n[3] Yuan, Zhihang, et al. \"Asvd: Activation-aware singular value decomposition for compressing large language models.\" arXiv preprint arXiv:2312.05821 (2023).\n\n[4] JAISWAL, AJAY KUMAR, et al. \"From Low Rank Gradient Subspace Stabilization to Low-Rank Weights: Observations, Theories, and Applications.\" Forty-second International Conference on Machine Learning.\n\n[5] Zhao, Jialin, Yingtao Zhang, and Carlo Vittorio Cannistraci. \"Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "Check above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KVhkiZPfxE", "forum": "m2nupeHqV7", "replyto": "m2nupeHqV7", "signatures": ["ICLR.cc/2026/Conference/Submission6575/Reviewer_Ee2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6575/Reviewer_Ee2f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824392747, "cdate": 1761824392747, "tmdate": 1762918909485, "mdate": 1762918909485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LC-SVD for compressing LLMs via low-rank decomposition. The method consists of two components: (1) block-wise collaborative decomposition that optimizes all weight matrices within a transformer block jointly, and (2) error-driven rank search that adaptively allocates ranks based on block sensitivity.  However, the \"collaborative decomposition\" is actually standard joint optimization where each matrix still receives its own independent SVD, with only the whitening matrices being updated via a shared block-level loss. The \"error-driven rank search\" is an expensive brute-force procedures, taking one hour on GPU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Solid experimental setting: Comprehensive evaluation across 3 models, 3 compression ratios, 2 language modeling benchmarks, and 6 downstream tasks, with thorough ablation studies and reproducible implementation details."}, "weaknesses": {"value": "1. Overstated Novelty - \"Collaborative Decomposition\" is Standard Joint Optimization.  All whitening matrices are updated based on the same block reconstruction loss. Each matrix still gets its own independent SVD; the only \"joint\" aspect is that optimization uses block-level loss instead of layer-level loss in previous work. This is a few-lines code change—using block output instead of layer outputs for the loss function. \n2. \"Error-Driven Rank Search\" seems to be a brute-force hyperparameter search. Prior work already does non-uniform rank allocation; this just uses a different (expensive) search procedure. For example, previous work RankDyna (EMNLP 2023 Findings https://aclanthology.org/2023.findings-emnlp.621/), demonstrates adaptive allocation during training surpasses fixed post-hoc assignment."}, "questions": {"value": "1. How does static post-hoc search compare to RankDyna's dynamic allocation? maybe adding something like: \"Unlike methods that compress during task-specific fine-tuning (e.g., RankDyna), our approach targets post-hoc compression for general deployment without additional training. This is valuable when: (1) task data is unavailable, (2) training compute is constrained, or (3) a general-purpose compressed model is needed for multiple downstream applications without per-task adaptation.\"\n\n2. What is the total compression time (optimization + 1 hour rank search) vs. baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fpmkz2RKUR", "forum": "m2nupeHqV7", "replyto": "m2nupeHqV7", "signatures": ["ICLR.cc/2026/Conference/Submission6575/Reviewer_zuFu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6575/Reviewer_zuFu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860030482, "cdate": 1761860030482, "tmdate": 1762918909066, "mdate": 1762918909066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To mitigate reconstruction error brought by model decomposition, this paper propose propose LC-SVD, a layer collaborative SVD framework with automatic rank search that enables adaptive low-rank compression of LLMs. LC-SVD mainly includes two parts: updating whitening matrix and rank allocation. Extensive and comprehensive experiments are carried out to demonstrate the superiority of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well written and easy to follow.\n\n2. The experiments carried out in this paper are extensive and comprehensive. But it is also limited, details refer to Weaknesses below."}, "weaknesses": {"value": "1. Limited evaluation and weak baselines. SVD-LLM is outdated, and should be replaced its successor SVD-LLM V2. The evaluation lacks the comparison with other non-decomposition-based strong baseline. Additionally, experiments should also include other non-MoE models from different LLM family. As for the baseline, authors are suggested to browse the publication in recent conferences.\n\n2. Lack of novelty. The contribution of this paper is just the combination of updating whitening matrix and rank allocation, and there are so many similar works in this field. The rank search part is heuristic-based and prune to be unsound. It needs more analysis and evaluation to demonstrate its generalizability.\n\n3. Incremental contributions. At least as for myself, this paper doesn't contribute any new insight on this area."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0yqrQtp5R3", "forum": "m2nupeHqV7", "replyto": "m2nupeHqV7", "signatures": ["ICLR.cc/2026/Conference/Submission6575/Reviewer_mr2G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6575/Reviewer_mr2G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878963918, "cdate": 1761878963918, "tmdate": 1762918908196, "mdate": 1762918908196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}