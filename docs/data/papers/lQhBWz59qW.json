{"id": "lQhBWz59qW", "number": 7533, "cdate": 1758026115926, "mdate": 1759897847654, "content": {"title": "ePC: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks", "abstract": "Predictive Coding (PC) offers a biologically plausible alternative to backpropagation for neural network training, yet struggles with deeper architectures. This paper identifies the root cause and provides a principled solution. We uncover that the canonical state-based formulation of PC (sPC) is, by design, deeply inefficient on digital hardware, due to an inherent signal decay problem that scales exponentially with depth. To address this fundamental limitation, we introduce a novel reparameterization of PC, named error-based PC (ePC), which does not suffer from signal decay. By optimizing over prediction errors rather than states, ePC enables signals to reach all layers simultaneously and unattenuated, converging orders of magnitude faster than sPC. Experiments across multiple architectures and datasets demonstrate that ePC matches backpropagation's performance even for deeper models where sPC struggles. Besides practical improvements, our work provides theoretical insight into PC dynamics and establishes a foundation for scaling bio-inspired learning to deeper architectures on digital hardware and beyond.", "tldr": "We uncover that standard Predictive Coding is highly unsuitable for digital simulation and propose a fully equivalent alternative that overcomes these flaws.", "keywords": ["Predictive Coding", "Reparametrization", "Biologically plausible learning", "Energy-based models", "Computational neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61bf54ce95e35bd4325e5bc36a42a86a1a384bfa.pdf", "supplementary_material": "/attachment/a5641cb476fed21e495d8c139e7a6fce3ce7f7d3.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes an alternative gradient flow procedure for simulating the convergence to the fixed point of (deep) predictive coding (PC) networks. To do so, this work proposes to, rather than optimizing over states (with a layer-wise locally defined loss, labelled sPC) to instead carry out an optimization to the energy minimum of a the errors at every layer based upon the global loss function (ePC). Experiments and figures demonstrate that indeed ePC significantly overcomes a delay in signal propagation which is inherent to the layer-local state-based optimization which is commonly carried out (sPC), while arriving at precisely the same equilibrium point. This allows significant increases in the speed of convergence of these models to the energy minima and the training of even deeper networks than were feasible previously."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This work is very well written and laid out. The set of figures and appendices are extensive and do a very good job of providing clear exposition of the problem at hand (signal decay within sPC networks). The contribution to the niche of predictive coding based learning is clear and could allow faster and more stable simulation time across the board. In fact, it may be that only an analytic solution for the errors in PC networks would allow for a faster measurement of the equilibrium point."}, "weaknesses": {"value": "This work suffers from a few significant weaknesses in contribution and in novelty:\n1) Though this paper allows PC's equilibrium point to be simulated with greater speed and accuracy, it does so by removing one of the core and fundamental features of PC: that energies in sPC are computed at a layer-local level and do not require full backward passes to update. This work does away with that computation, in favour of a globally defined error at every layer which requires a full backpropagation operation at every timestep, and thereby almost entirely does away with the core conceptual purpose of energy in PC networks. I would therefore frame this paper as an implementational trick to reach the same fixed point as PC (assuming that ePC does not get stuck in some other local minimum or saddle point). Note that this change also means that this method can not be usefully applied for examining the dynamics of predictive coding (if one were interested in doing so). This weakness of this method and it's implications are little discussed in this work.\n\n2) Atop issue 1, it is claimed in this work (section 6.2) that \"ePC reformulates PC in a way that aligns naturally with digital processors, supporting efficient backpropagation and fast convergence across deep networks.\". This is a very misleading statement and on the whole exhibits one of the key ways that the contributions of this work are misconstrued. In fact, the ePC reformulation could not ever be more efficient than backpropagation considering that it has no local dynamics whatsoever. The fact that every timestep requires a full forward and full backward pass (i.e. a full backpropagation step) means that this method is always as expensive as backpropagation in computation (and computational machinery) and could never be more efficient.\n\n3) The two previous points also overall point to how the language used in this work is oftentimes overstated. For example, at the end of section 3.3 it is claimed that issues with the convergence time of sPC all end with \"highlighting the need for a fundamental reconsideration of PC’s standard formulation.\" And in section 3.4 \"ePC enables signals to reach all layers simultaneously without attenuation, while preserving the local weight update property that distinguishes PC from standard backpropagation.\" In effect, ePC does not appear to be a fundamental reconsideration of the principle of Predictive Coding (given that it is a trick for more rapidly reaching the energy minimum) but also the claim that it preserves local weight updating properties is rather disconcerting given that ePC carries out a full backpropagation pass at every timestep (the definition of non-local). Either ePC is an implementational trick (and therefore does not modify PC's core principles or takeaways and is a minor contribution) **OR** is a fundamental change to the proposal of PC (but therefore loses almost all relationship to PC aside from it's fixed point with much discussion necessary). It seems in this work that the authors do not choose a side and instead straddle this line claiming the benefits of both worlds."}, "questions": {"value": "If the reviewers wish to rebut the weaknesses outlined above, I would be happy to hear their opinions but am doubtful that I have misunderstood the core theoretical aspects of this work. I would be curious to hear:\n1) How do the authors see this contribution if they had to choose a side? Is it an implementational trick with little to say about predictive coding? Or do they wish to claim a significant departure from predictive coding? It would be ideal if the text was rewritten to make this clear.\n2) Given that these models can now be trained well and rather simply, how does performance scale beyond simple tasks like CIFAR and MNIST? It appears that if ePC is to have a major impact for deeper networks, the only case in which this would really matter is if it allows networks (like ResNet-18) to be trained also on more challenging tasks (such as ImageNet).\n3) Given that PC was first proposed as a biological alternative to backpropagation, how 'deep' are biological neural networks and do your results have anything to say on whether this would suggest that sPC itself is unsuitable as an alternative to biological backpropagation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EmulcDpEcx", "forum": "lQhBWz59qW", "replyto": "lQhBWz59qW", "signatures": ["ICLR.cc/2026/Conference/Submission7533/Reviewer_wrgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7533/Reviewer_wrgR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731582895, "cdate": 1761731582895, "tmdate": 1762919633452, "mdate": 1762919633452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical flaw in Predictive Coding (PC) networks: exponential signal decay during the energy minimization phase, which has historically hindered their scalability on digital hardware. The authors propose error-based PC (ePC), a novel reparameterization that optimizes over prediction errors instead of neural states. This change enables global signal propagation via a backpropagation-like mechanism, resolving the decay problem while maintaining theoretical equivalence to standard PC (sPC). Experiments confirm that ePC converges orders of magnitude faster and successfully scales to deep architectures, achieving performance comparable to backpropagation where sPC fails."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. A primary strength is the paper's insightful diagnosis of exponential signal decay as the root cause of sPC's scaling limitations. This provides a clear, principled explanation for a long-standing issue in the field, backed by both theoretical analysis and empirical evidence.\n2. The proposed ePC is an elegant reparameterization that effectively resolves the identified decay problem. By shifting the optimization from states to errors, the method preserves the core principles of PC while unlocking dramatic performance gains, demonstrating a deep understanding of the algorithm's mechanics.\n3. The paper provides compelling empirical validation for its claims. The results demonstrate a significant speedup in convergence and show that ePC successfully scales to deep networks like ResNet, matching backpropagation's performance in regimes where standard PC is unstable or fails entirely."}, "weaknesses": {"value": "1. A key appeal of PC is its local computation, which aligns with biological processes. By incorporating a global backpropagation pass for error updates, ePC sacrifices this critical feature, making the overall algorithm less biologically plausible than the original sPC formulation.\n2. The comparison is primarily against a basic sPC implementation. By not including stronger PC baselines from the literature that also aim to improve training dynamics, the paper may overstate the performance gap that ePC closes."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jRvhHmOto", "forum": "lQhBWz59qW", "replyto": "lQhBWz59qW", "signatures": ["ICLR.cc/2026/Conference/Submission7533/Reviewer_RUrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7533/Reviewer_RUrq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824325292, "cdate": 1761824325292, "tmdate": 1762919632121, "mdate": 1762919632121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces error-based predictive coding (ePC), a reparameterization of predictive coding that optimizes errors instead of states, eliminating the exponential signal decay that makes standard state-based PC (sPC) impractical on digital hardware. ePC preserves PC’s local synaptic learning rule at equilibrium while using a global backprop-style pass for inference over the error variables, enabling fast, depth-robust convergence. Experiments on MLPs, VGGs, and ResNet-18 show near-parity with backprop and large gains over sPC in both stability and speed; theory clarifies when ePC collapses to exact BP and why it otherwise differs. The work reframes PC for digital hardware but in doing so foregoes some of the proposed advantages of PC."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Pinpoints a discrete-time, depth-dependent signal decay in sPC and proposes a principled error reparameterization that removes it on digital hardware.\n\n\nShows ePC and sPC are equivalent at equilibrium yet follow different trajectories; specifies conditions where ePC=BP and warns against accidentally entering that regime.\n\n\nAcross deeper nets, ePC matches BP and far outpaces sPC in convergence (up to orders of magnitude), addressing PC’s well-known depth scaling failures."}, "weaknesses": {"value": "Weaknesses\nAdvantage over plain BP remains unclear: On standard supervised benchmarks, ePC ≈ BP in accuracy/throughput; the manuscript could better motivate when practitioners should prefer ePC to BP beyond the PC program itself.\n\nLocal/async claims need front-loaded clarity: ePC preserves local weight updates but gives up local/async inference (uses a global backprop pass). Make this trade-off explicit in the intro to avoid confusion with PC’s usual pitch. Note: this is also giving up on the advantages of PC.\n\nThe reparameterisation of Eq. 5 should not write yhat as a constant and instead make clear all the variables it’s computationally coupled to, ie yhat(x, e1, e2, …), as later stated in Section 4.2. The current writing mathematically implies that the gradients of Eq. 4-5 are the same.\n\n It is not clear whether the signal propagation analysis is as relevant after the first backward pass (or “wavefront”), since following from that the activity of any layer will be affected by both top-down and bottom-up layers. This point is supported by recent work by Ha et al. (2025). \n\nRelated, overall it remains unclear whether the documented exponential signal decay is the root cause of the untrainability of very deep PCNs, or indeed why there should be only one main cause. Indeed, Ha et al. (2025) and Innocenti et al. (2025) identify 2-3 different pathologies of training deep PCNs. As an example, a vanishing forward pass initialisation can be equally problematic, even before considering the stability of the inference dynamics. So while exponential signal decay is clearly a problem worth investigating and sharing with the community, claiming that it’s the root cause seems an overstatement.\n\nHa, M. H., Sung, Y., Jo, Y., Kim, H., & Lee, S. W. Towards Stable Learning in Predictive Coding"}, "questions": {"value": "Is this the “key assumption” stated in B.1? If so, this should be clearly stated and emphasised i the text.\n\nWhat are the FLOPs, peak activation memory, and wall-clock time-to-target for ePC vs BP on VGG-9/ResNet-18? This would clarify practical trade-offs.\n\nCan you include a small online/continual learning experiment (even toy) to showcase a concrete advantage over BP aligned with PC’s hypothesized strengths? The lietarure on this isn't convincing.\nPlease clarify in the main text (not only appendix) that ePC retains local synaptic updates but is not asynchronous/local for inference on GPUs/CPUs.\n\nThere is a rich deep learning literature about the signal propagation of the forward and backward passes of neural networks (e.g. Poole et al., 2016; Schoenholz et al., 2017). The analysis employed uses different techniques, and it would be useful if the authors clarified this while acknowledging the previously mentioned literature.\n\nIt would also be useful if the authors could discuss in more detail the relationship of their work to both Ha et al. (2025) and the ill-conditioning of the inference landscape recently reported by Innocenti et al. (2025). In particular:\nCan exponential signal decay seen as a special case of ill-conditioning, since the former it’s a statement only about the first backward pass as pointed out above, while the latter is a statement about the entire optimisation process? In particular, ill-conditioning does not make the key assumption of Section B.1 (see comment above).\n\n The authors also state that architectural tweaks do not solve this sPC-specific decay (line 221). This seems to contradict Innocenti et al. (2025), who showed that ill-conditioning is a consequence of the feedforward architecture of PCNs and that conditioning (and therefore signal propagation) would improve for densely connected architectures. The authors’ signal propagation analysis clearly relies on this architectural assumption too.\n\n\nReferences\nμPC: Scaling Predictive Coding to 100+ Layer Networks\nF Innocenti, E Mehdi Achour, C. L. Buckley\n\nHa, M. H., Sung, Y., Jo, Y., Kim, H., & Lee, S. W. Towards Stable Learning in Predictive Coding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZA6OFybehJ", "forum": "lQhBWz59qW", "replyto": "lQhBWz59qW", "signatures": ["ICLR.cc/2026/Conference/Submission7533/Reviewer_4A7U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7533/Reviewer_4A7U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845032798, "cdate": 1761845032798, "tmdate": 1762919631763, "mdate": 1762919631763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates that standard predictive coding networks exhibit depth-dependent, exponentially vanishing error propagation in discrete-time implementations and proposes an error-parameterized variant (ePC) that updates prediction errors directly, achieving fast, depth-independent convergence while retaining local weight updates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper pinpoints a very concrete, under-discussed failure mode of standard predictive coding—depth-dependent exponential attenuation due to discrete-time state updates—and reframes PC by reparameterizing over errors (ePC) to remove exactly that bottleneck."}, "weaknesses": {"value": "1. **Insufficient conceptual alignment with predictive coding (global backprop vs. PC)**\nThe proposed method explicitly constructs a global backpropagation pathway over the energy function to update the error variables, thereby propagating update signals in a backward manner. This departs from the core computational premise of predictive coding (PC), which emphasizes the propagation of prediction errors through purely local interactions between adjacent layers. Consequently, the method appears less as a genuine “reparameterization of PC” and more as “backpropagation expressed in PC notation.” The authors’ own remark further reinforces this interpretation, which suggests that the resulting structure effectively becomes a “fully connected (global) graph.”\n\n2. **Violation of locality and resulting reduction in biological plausibility**\nA significant source of credibility in the PC literature is the claim that synaptic/weight updates can, in principle, be implemented strictly locally. In contrast, the present approach requires, in its error-update phase, an explicit backward pass, along with a global control/broadcast mechanism. That is, the learning signal must be disseminated top–down in a single step; if this broadcast is impaired, the main advantage of the method—depth-independent convergence—no longer holds. This is not merely a modest deviation from the “spirit” of PC but a direct weakening of the PC narrative that learning can be supported by local signals alone, which makes it difficult to sustain strong claims of biological plausibility for the proposed variant.\n\n3. **Questionable fairness in interpreting the performance gains**\nOnce global broadcasting of errors to all layers is permitted, achieving performance comparable to backpropagation is essentially expected, since each layer now receives essentially the same global supervisory information. Nevertheless, the paper presents these results as evidence that “PC can be scaled to deep architectures,” even though such performance is obtained by relaxing precisely those constraints (locality, limited top–down communication) that distinguish PC from backpropagation. To substantiate the claim that the work overcomes an *intrinsic* limitation of PC, additional experiments under identical biological and communication constraints to standard PC would be necessary. In its current form, the contribution can be read primarily as showing that “a backpropagation-like method labeled as PC attains backpropagation-like performance.” If adopted without such qualifications, this line of work risks diverting the PC community away from genuinely local and parallel formulations and toward methods that retain the PC label but are, in substance, closer to conventional backpropagation."}, "questions": {"value": "1. In the limit where the number of inference steps (T = 1), the proposed procedure seems to reduce to **one forward pass plus one global backward pass**, which is operationally indistinguishable from standard backpropagation. Do the authors therefore regard conventional backpropagation as a special case of predictive coding under their formulation? If not, what essential property (e.g., fixed-point inference, energy minimization over latent states, or particular locality assumptions) remains that would distinguish the proposed method from ordinary BP in the (T=1) regime?\n\n2. The paper argues that introducing error variables and optimizing over them constitutes a predictive-coding-compatible reformulation. However, from an algorithmic standpoint, this appears equivalent to introducing intermediate variables solely to enable a **single global backpropagation over the inference trajectory**—i.e., to ensure that all layers receive the error signal in a single pass. In what precise sense is this still “predictive coding,” rather than simply backpropagation with an auxiliary variableization of the state? Could you please clarify the criteria by which you classify this as PC?\n\n3. The proposed algorithm requires, for its global error update, that error information be **broadcast backward across all layers** in a coordinated manner. This implies the existence of a **global control or scheduling mechanism** that supervises what is a purely layer-local, sequential message-passing process. What biological or neurophysiological evidence do the authors rely on to justify the presence of such a global, backward-directed control signal? In particular, how would such a mechanism be implemented under realistic constraints on communication bandwidth, latency, and directionality in cortical circuits, and how does this differ from the well-known objections to BP-style weight transport?\n\n4. In the special case where the number of inference steps is (T = 1), the proposed procedure can reasonably be interpreted as standard backpropagation—or, at least, as an algorithm that is functionally equivalent to backprop in terms of how error information is routed and applied. Suppose the authors nonetheless choose to classify this (T = 1) regime as a valid instance of predictive coding. Does this not amount to endorsing the view that human (or biological) learning could be implemented via a backpropagation-like mechanism? If so, could the authors point to concrete neuroscientific or computational evidence that supports the existence of such a globally coordinated, backward-directed, weight-relevant error signal in the brain? Conversely, if the authors do **not** intend to make such a commitment about biological learning, then it would be helpful to clarify the boundary conditions under which an algorithm should properly be called “predictive coding” rather than “backprop written in predictive-coding notation.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kn6n13MlfP", "forum": "lQhBWz59qW", "replyto": "lQhBWz59qW", "signatures": ["ICLR.cc/2026/Conference/Submission7533/Reviewer_QnH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7533/Reviewer_QnH3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892755753, "cdate": 1761892755753, "tmdate": 1762919631259, "mdate": 1762919631259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}