{"id": "PRR120c01e", "number": 17026, "cdate": 1758271316969, "mdate": 1759897203768, "content": {"title": "Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations", "abstract": "Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local\nVerifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful\nto decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.", "tldr": "We provide an algorithm for generating verifiable, high-level, concept-based global explanations of LLM-as-a-Judge policies.", "keywords": ["LLM-as-a-Judge", "Interpretability", "Global Explanations", "Local Explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88a606b43bd9611258173e6b1dd15933176d29e4.pdf", "supplementary_material": "/attachment/ea70a7a452becc971a1eef00b3fa7cb7b657a5bd.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the opacity and lack of interpretability in LLM-as-a-Judge systems by proposing a two-stage pipeline to extract verifiable global decision policies. It proposes a local explanation method that generates contrastive rationales in a BECAUSE-DESPITE format, and a  global algorithm that aggregates CLoVE’s local explanations into a compact policy.  The authors evaluate their method on 7 harm detection datasets (e.g., BeaverTails, HarmBench) using two LLM judges (GraniteGuardian3.2, LlamaGuard3.3), showing high fidelity to the judges’ decisions (F1 > 0.7 on most datasets) and robustness to paraphrasing/adversarial attacks. A small user study finds marginal improvements in policy understanding and significant gains in perceived usefulness over the GELPE baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of contrastive reasoning (BECAUSE–DESPITE) and verifiable concept extraction is original. The integration of CLoVE and GloVE offers a systematic pipeline for producing both local and global interpretability, which goes beyond existing rule-based or concept-bottleneck approaches.\n\n- GloVE’s use of FactReasoner to ensure that summarized concepts are probabilistically entailed by local ones adds an important layer of rigor. The inclusion of formal lemmas on graph homomorphism and entailment fidelity contributes to theoretical depth and credibility."}, "weaknesses": {"value": "- Both CLoVE and GloVE rely on multiple LLM submodules (generator, verifier, labeler). This introduces possible circular dependencies, bias propagation, and sensitivity to prompt design, which are only briefly acknowledged in the limitations section.\n\n- The evaluation focuses exclusively on harm detection. Although this is a relevant benchmark, the method’s generalizability to other LLM-as-a-Judge domains (e.g., summarization quality, factual accuracy evaluation) remains untested and should be explored in future work."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qjjdYslJ8K", "forum": "PRR120c01e", "replyto": "PRR120c01e", "signatures": ["ICLR.cc/2026/Conference/Submission17026/Reviewer_rX85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17026/Reviewer_rX85"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877433297, "cdate": 1761877433297, "tmdate": 1762927049115, "mdate": 1762927049115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study presents an LLM-based method for extracting local and potentially global rules from LLM-as-a-Judge models to interpret their opaque decision-making processes. The approach comprises three components: a concept generator, a word-level explanation module, and a verifier, each of which relies on either an LLM or other specialized NLP models. Experiments conducted across multiple datasets demonstrate the effectiveness of the proposed method in extracting rules with high fidelity and accuracy, outperforming several baseline approaches. In my view, the main contribution of this work lies in its introduction of a pipeline for empirically deriving rules from LLM-as-a-Judge models to explain their decisions. This is particularly valuable given the growing use of such models for automated evaluation in various scenarios that would otherwise require substantial human effort."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The study introduces a method that generates logical, rule-based explanations to elucidate the often opaque decision-making process of LLM-as-a-Judge models. This approach is valuable for developing more formal and rigorous explanations of LLM behaviors.\n\n2. The proposed method is presented clearly, making the overall framework easy to follow."}, "weaknesses": {"value": "1. The supplementary materials appear to be incomplete. Specifically, the document containing the proofs for the lemma introduced in the study was not provided.\n2. The application of global rules is not clearly explained, particularly in relation to the non-trivial graph-building process developed in the method.\n3. The description of the dataset usage is unclear. It is not specified which portion of the data was used for rule extraction and which portion was reserved for testing.\n4. The adversarial attacks introduced in the study are relatively trivial. It is unsurprising that minor word-level manipulations significantly degrade detection performance, and the greater impact of paragraph-level rephrasing—which alters more words—further supports this expectation. As a result, these attacks do not adequately justify the method's robustness.\n5. The ablation study indicates that the verification step does not consistently improve performance. This raises questions about its necessity and practical contribution, which should be more explicitly discussed to clarify the key takeaways regarding its importance."}, "questions": {"value": "My questions have been stated in the detailed review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DXLIkvuZo6", "forum": "PRR120c01e", "replyto": "PRR120c01e", "signatures": ["ICLR.cc/2026/Conference/Submission17026/Reviewer_kaTn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17026/Reviewer_kaTn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879894615, "cdate": 1761879894615, "tmdate": 1762927048663, "mdate": 1762927048663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the critical problem of opacity in \"LLM-as-a-Judge\" systems, whose black-box nature raises concerns about bias, reliability, and fairness. The authors argue that existing local explanations (like Chain-of-Thought) are often unreliable and fail to provide a high-level view of the judge's overall policy. They propose a two-part pipeline and evaluate it on seven harm-detection datasets, using LlamaGuard and GraniteGuardian as judges. They find the extracted GloVE policies are highly faithful to the original judge's decisions and robust to various text perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"BECAUSE... DESPITE...\" contrastive format is a significant strength. It moves beyond simple, one-sided rationales and captures the inherent ambiguity of moderation tasks, which is crucial for understanding why a judge made a specific trade-off.\n2. The pipeline's core novelty is its focus on \"verifiable\" explanations. It doesn't just trust a generated explanation, it actively verifies it. CLOVE verifies concepts against important input words (via LIME), and GloVE verifies its clustered concepts (via FactReasoner). The ablation study (Table 3) successfully demonstrates that this verification step is crucial for maintaining fidelity.\n3. The paper introduces clear, quantitative metrics for success, \"Fidelity\" and \"Performance Degradation\", to measure how well the extracted policy mimics the original judge. The GloVE pipeline shows very high fidelity (Table 1), demonstrating that the summarized policy is a very accurate \"distillation\" of the original LLM-judge.\n4. The authors go beyond simple fidelity checks to test the robustness of the GloVE-extracted rules against text paraphrasing and adversarial attacks. The finding that the GloVE rules are often more robust than the original LLM-judge is an interesting result."}, "weaknesses": {"value": "1. The primary weakness is that the proposed solution to LLM opacity is, itself, a highly complex and opaque pipeline of multiple LLMs. To explain one judge ($M$), the CLOVE algorithm uses an LLM generator ($G$), an LLM verifier ($V$), and the GloVE algorithm uses an LLM labeler and an LLM-based FactReasoner. This doesn't solve the opacity problem, it just shifts it. How do we know the verifier's policy or the FactReasoner's policy is unbiased? The final explanation is now dependent on the \"judgment\" of several other black-box models.\n2. The ultimate goal of explainability is to improve human understanding. The paper's user study fails to show this. Participants' accuracy in predicting the judge's behavior was \"marginal\" and not statistically significant (60% for GloVE vs. 58% for the baseline). This finding suggests that the \"highly faithful\" GloVE explanations do not actually help humans understand the judge's policy any better than the baseline. The paper highlights \"perceived usefulness\", but this is a much weaker claim than demonstrating improved comprehension.\n3. The full pipeline appears to be exceptionally complex and computationally expensive. Generating a single global policy requires running the base judge ($M$), plus a generator ($G$), LIME ($L$), and a verifier ($V$) for every single local explanation, followed by iterative clustering, labeling, and FactReasoning for the global policy. The paper provides no analysis of this cost, which seems prohibitive for practical use.\n4. The CLOVE algorithm's \"verifiability\" is critically dependent on a local word-based explainer like LIME. LIME and similar feature-attribution methods are known to be unstable themselves. The paper does not analyze how sensitive the final global policy is to the instability or failure of this underlying component."}, "questions": {"value": "1. The pipeline uses several LLMs (G, V, FactReasoner) to explain the target LLM ($M$). How can we trust the explanation if the components of the explainer are themselves opaque LLM-judges? Does this not simply defer the problem of bias and reliability to the explanation pipeline itself?\n2. The user study showed no significant improvement in human understanding (60% vs 58% accuracy). Given that the primary motivation of the paper is to make opaque policies \"transparent and interpretable\", how do the authors interpret this negative result?\n3. Could the authors provide an analysis of the computational cost (e.g., total LLM calls or time) required to generate one global policy? How does this cost scale with the number of local explanations?\n4. How robust is the CLOVE method to the choice of the local word-based explainer ($L$)? What happens if LIME provides incorrect or unstable word attributions, and how does that error propagate to the final global policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5602VojRgf", "forum": "PRR120c01e", "replyto": "PRR120c01e", "signatures": ["ICLR.cc/2026/Conference/Submission17026/Reviewer_prPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17026/Reviewer_prPU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882135966, "cdate": 1761882135966, "tmdate": 1762927048353, "mdate": 1762927048353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to provide an explanation of the thought process of LLM-as-a-judge in detecting harmful information by analyzing its potential decision-making process. The paper proposes a method for extracting high-level concepts from raw text, including CLoVE and GLoVE. CLoVE analyzes the positive and negative meanings of sentences to extract contrasting explanations, while GLoVE gradually aggregates local contrasting explanations to form global explanations. This helps explain the decision-making process of LLM-as-a-judge when handling complex texts, as it can be achieved through an understanding of the overall text.  \n\nThe definition of the task in this paper is somewhat confusing. The proposed method evaluates performance by comparing the analysis results of harmful texts using this method with those of other harmful content detection models, and then uses this consistency as an explanation for the interpretability of other harmful content detection models. This highly indirect approach raises doubts about the rationality of the task and experimental design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper attempts to incorporate more fine-grained analysis into the harmful detection process, including comparative explanations based on opposing perspectives and organizing local contexts into global textual explanations. This approach helps improve the performance of harmful detection and provides a certain degree of procedural transparency."}, "weaknesses": {"value": "The task definition in this paper is confusing and poorly presented. \n\nThis paper focuses on explaining and analyzing the original text in harmful detection, rather than analyzing the output results of LLM-as-a-Judge, then exhibiting their potential bias. Therefore, I do not consider this to be explanatory work on LLM-as-a-Judge, but rather more like a standalone LLM harmful detection model. \nIt may not qualify as explanatory research on defense LLMs like LlamaGuard, as the experiments in this paper compare the analysis results of GLoVE with the analysis content of other LLM-as-a-Judge models. This is quite perplexing: why should your model's analysis of the text need to correlate with the analysis of any other model?  \n\nIf the task of this paper, as mentioned in line 338—\"Our experiments focus on explaining LLM-as-a-Judge on the task of content harm detection\"—were placed earlier, such as in the introduction or abstract, it might be better. Otherwise, readers might assume you are studying the explanatory aspects of LLM-as-a-Judge for general text.  \n\nBased on my understanding of the task definition as mentioned above, the current baselines are entirely insufficient. At the very least, there should be a comparison with LlamaGuard's capability in harmful content detection.\n\nAdditionally, if my misunderstanding is due to my oversight, I apologize and would appreciate it if the authors could provide further clarification. For now, I will give a low confidence."}, "questions": {"value": "- What backbones are the Generator, Verifier, and Local word-based explainer mentioned in this paper? Has the performance of these components been individually quantified and evaluated?\n\n- How are important words defined? Has the accuracy of extracting these important words been evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RoZvyjF3sY", "forum": "PRR120c01e", "replyto": "PRR120c01e", "signatures": ["ICLR.cc/2026/Conference/Submission17026/Reviewer_ibRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17026/Reviewer_ibRx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903344087, "cdate": 1761903344087, "tmdate": 1762927048038, "mdate": 1762927048038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}