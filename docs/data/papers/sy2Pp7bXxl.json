{"id": "sy2Pp7bXxl", "number": 4457, "cdate": 1757683319091, "mdate": 1763194990422, "content": {"title": "PEAR: Pixel-aligned Expressive humAn mesh Recovery", "abstract": "Reconstructing a human mesh from a single in-the-wild image has long been a central research direction in computer vision. Existing approaches often provide only coarse reconstructions of the overall human structure, while still exhibiting noticeable misalignments in fine-grained regions such as the face and hands. Such subtle deviations may be progressively amplified in downstream tasks, leading to significant errors in the final outcomes. To address this issue, we propose PEAR—a unified framework for human mesh recovery and rendering. PEAR explicitly tackles two major limitations of current methods: inaccurate localization of fine-grained human pose details and insufficient photometric supervision for self-reconstruction.\nSpecifically, we train a transformer-based model to recover expressive 3D human geometry from a single 2D image, and integrate it with a neural renderer to jointly optimize geometry and appearance. This synergy substantially improves the accuracy of fine-grained human geometry while yielding higher-quality rendering results. In addition, we construct a large-scale dataset of images and videos with human annotations to support model training. Extensive experiments on multiple benchmark datasets demonstrate that the proposed approach achieves significant improvements in both geometric reconstruction accuracy and rendering quality.", "tldr": "We propose a human mesh recovery method that achieves pixel-level alignment.", "keywords": ["Human Mesh Recover", "Gaussian Splatting", "Rendering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc2012a153904848b33fad906128198d3ef9284f.pdf", "supplementary_material": "/attachment/c9a6393019f9555e39976162cd725b8e3f9cd155.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PEAR (Pixel-Aligned Expressive Human Mesh Recovery), a unified framework that recovers expressive 3D human meshes from a single image. Unlike prior SMPL-X based methods, PEAR integrates the FLAME head into SMPL-X (forming the Expressive Human Model, EHM), improving facial expressiveness while preserving full-body modeling. The method further introduces a two-stage training pipeline:\n\n* A ViT-based regressor predicts EHM parameters (body, face, hands, camera).\n\n* A neural renderer is used with photometric loss for pixel-level enhancement.\n\nExperiments on datasets such as UBody, 3DPW, EHF, and LSP-Extended show improvements in facial detail, hand accuracy, and overall pixel alignment. The method also demonstrates real-time avatar reconstruction (0.05s inference) and generalizes to downstream animation tasks. Ablation studies validate the benefit of two-stage training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Pixel-level photometric supervision: The second-stage neural rendering significantly improves fine-grained alignment beyond joint/parameter losses."}, "weaknesses": {"value": "1. FLAME head pose integration:\nThe paper states the proposed system estimates both SMPL-X and FLAME parameters, but it is unclear how global head orientation is consistently maintained. For example, when the body is rotated or facing away, naïvely replacing the head could cause inconsistencies between the body and face orientation. Clarification is needed on how alignment between body root pose and FLAME global pose is enforced.\n\n2. Stage-2 reliance on upper-body datasets:\nThe second-stage training leverages upper-body 3DGS datasets (e.g., UBody) for pixel-level enhancement. This raises two concerns:\n\n* Lower-body accuracy may degrade since pixel-level supervision is not applied consistently to the legs/feet.\n\n* Upper-body datasets typically lack diverse whole-body poses, potentially limiting generalization.\nRelated to this, Table 5 shows ablation across UBody, 3DPW, EHF, and UBody-intra, but it is not clearly explained how the proposed method addressed the above two concerns.\n\n3. Limited full-body evaluations:\nWhile the paper reports on 3DPW, EHF, and UBody, it lacks broader validation on challenging full-body datasets like AGORA or BEDLAM, which test both pixel alignment and kinematic reconstruction under more diverse settings. Without these, the claim of strong generalization to complex whole-body scenarios is not fully convincing."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gg8nCKdUJj", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Reviewer_Togf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Reviewer_Togf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760491816694, "cdate": 1760491816694, "tmdate": 1762917376893, "mdate": 1762917376893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Pixel-Aligned Expressive Human Mesh Recovery framework that aims to enhance fine-grained human mesh alignment and facial expressiveness from a single image. It jointly estimates SMPLX and FLAME parameters to form an Expressive Human Model (EHM), following GUAVA (ICCV 2025), addressing the common misalignment and expression limitations of existing human motion estimation works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well-written and easy to understand.\n- By combining SMPLX and FLAME within the Expressive Human Model and using photometric supervision, the framework effectively captures subtle facial expressions and hand detail, outperforming recent works on human mesh recovery."}, "weaknesses": {"value": "- The core design of this work that combines an EHM (SMPLX + FLAME) with a neural renderer for pixel-level photometric supervision,  closely follows the formulation of GUAVA (Zhang et al., ICCV 2025). While PEAR extends GUAVA’s upper-body focus to full-body reconstruction and adopts a two-stage training pipeline instead of optimization-based parameter tracking, the overall architecture and objective remain conceptually similar. The paper should clarify the key algorithmic differences or technical contributions beyond scaling the framework to full-body meshes.\n- In Eq. (6), the reconstruction parameter set $\\Phi = [\\theta_t^b, \\beta_s^b, \\theta_t^h, \\beta_s^h, \\phi_t^h, \\pi_t]$\n mixes body and facial parameters from different frames (source and target), specifically, shape parameters ($\\beta$) from the source and other parameters from the target. No ablation or comparison is presented to validate that using source $\\beta$ yields better alignment or rendering quality than other alternatives (e.g., all parameters from the target). Ablation study would be necessary to support this design.\n- Although the “Human Appearance Reconstruction” part on Related work discusses neural renderers and Gaussian-based methods, it omits a large body of mesh-based literature that directly tackles high-fidelity human surface recovery from single or multiple images. Incorporating these works would better improve the quality of this paper:\n    - [1] Saito et al., *“PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,”* ICCV 2019.\n    - [2] Shin et al., *“CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images,”* ECCV 2024.\n    - [3] Xiu et al., *“ICON: Implicit Clothed Humans Obtained from Normals,”* CVPR 2022.\n    - [4] Xiu et al., *“ECON: Explicit Clothed Humans Optimized via Normal Integration,”* CVPR 2023.\n    - [5] Liao et al., *“High-Fidelity Clothed Avatar Reconstruction from a Single Image,”* CVPR 2023.\n    - [6] Ho et al., *“SITH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion,”* CVPR 2024.\n\nI will reconsider score when all my concerns are handled well."}, "questions": {"value": "How does the model perform under challenging conditions such as occlusions, extreme lighting, or side-view inputs? Including visual examples of such cases would help readers understand the framework’s limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qKfq17FnqC", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Reviewer_7qEi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Reviewer_7qEi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495870580, "cdate": 1761495870580, "tmdate": 1762917376656, "mdate": 1762917376656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed PEAR, a framework for single-image human mesh recovery that (i) regresses an Expressive Human Model (EHM) combining SMPL-X for body and FLAME for head, and (ii) jointly trains with a neural renderer to add pixel-level photometric supervision. The proposed method reduces mesh–image misalignment in fine regions (face, hands), improves facial expressiveness, and enables fast (≈0.05 s) parameter estimation for downstream animation. Quantitatively, PEAR improves head/hand/body reconstruction accuracy across multiple benchmarks and improves rendering metrics (PSNR/SSIM/LPIPS) via joint training."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is straightforward and easy to understand; the manuscript is clearly written.\n\n- The proposed method could perform full-body 3D modeling without the need for cropping.\n\n- A large-scale human mesh dataset is annotated and slated for open release."}, "weaknesses": {"value": "- Technical novelty. The proposed pipeline closely follows GUAVA:\n(a) it adopts the enhanced human parametric model EHM (introduced by GUAVA);\n(b) in Stage-1, EHM parameters are trained using pseudo ground truth generated by GUAVA;\n(c) in Stage-2, the neural renderer reuses GUAVA’s pipeline.\nHence, compared with GUAVA, the difference is that instead of tracking-based EHM parameter estimation, this paper swaps in an HMR2-based parameter estimator and then jointly optimizes EHM estimation and the neural renderer, further fine-tuning on an extra annotated dataset. While this is potentially useful (pending stronger empirical validation that I have some concerns below), I remain concerned that the technical contribution may be insufficient for the ICLR bar.\n\n- Experiment Results. I feel the current results do not fully substantiate the claims of “a more expressive human model” and “avoiding severe misalignments commonly observed in prior methods.” See questions for my specific concerns."}, "questions": {"value": "- Table 1. What exactly are the two comparison methods? The paper never identifies them (e.g., optimization-based vs. learning-based? which body/head models are used?). Without these details, it’s difficult to draw meaningful conclusions from Table 1.\n\n- Table 3. As shown, HMR2 outperforms the proposed method on all datasets and metrics. Given HMR2 uses SMPL while the proposed method uses the enhanced EHM, and the network architecture follows HMR2’s implementation, the statement in L318 (“Our approach achieves performance comparable to specialized body pose estimation methods such as HSMR and HMR2”) is hard for me to agree. At least, the paper should discuss this discrepancy.\n\n- Table 3 is missing an important entry. As shown, on COCO and PoseTrack, among all SMPL-X methods, PyMAF-X appears to be the closest baseline to the proposed approach, yet its result on LSP is missing. Please clarify.\n\n- Table 4: As reported, 2 minutes is required for reconstruction and 0.18s for rendering in Config-A (Tracking + GUAVA Renderer which is the GUAVA baseline). These numbers differ dramatically from GUAVA’s reported performance in their paper (≈52.2 FPS and ≈0.1s reconstruction in Tables 1 and 2 of [1]). Why is there such a large discrepancy? Please explain experimental setups and metrics used in this paper.\n\n- Table 5 vs. Table 1. The MLE reported on UBody in Table 5 is inconsistent with Table 1—possibly by an order of magnitude. Please double-check and correct if needed.\n\nReferences:\n[1] Zhang, Dongbin, et al. \"GUAVA: Generalizable Upper Body 3D Gaussian Avatar.\" arXiv preprint arXiv:2505.03351 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtdKiNvFM8", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Reviewer_vhqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Reviewer_vhqT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632818810, "cdate": 1761632818810, "tmdate": 1762917376373, "mdate": 1762917376373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "PEAR and GUAVA"}, "comment": {"value": "### **Global 1: Maybe there is some misunderstanding between PEAR and GUAVA**\n\n\n**PEAR and GUAVA target entirely different tasks.** In our training pipeline, we divide the process into two stages. In the first stage, we train PEAR to estimate SMPLX and FLAME parameters using a large number of annotated SMPLX and FLAME datasets. However, we found that this alone was insufficient for PEAR to accurately capture facial details or achieve pixel-level alignment. Unlike previous methods that crop specific regions, add new modules to enhance detail capture, or increase input resolution—which significantly increases model complexity and inference time, negatively affecting downstream applications—we instead keep the model architecture unchanged and introduce GUAVA as a renderer to provide photometric loss for finetuning PEAR, enabling improved body-detail capture and pixel-level enhancement. **Experiments confirm the effectiveness of this approach, and for the first time, we achieve real-time inference of SMPLX and FLAME parameters at 100 FPS, providing fast interfaces for downstream applications.**\n\nIn summary, GUAVA is not part of PEAR. The two methods target completely different tasks: PEAR is an HMR approach, whereas GUAVA is a Gaussian avatar reconstruction method. In this sense, GUAVA can be considered a downstream application of PEAR."}}, "id": "gmy8kMymYv", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763081742962, "cdate": 1763081742962, "tmdate": 1763087619635, "mdate": 1763087619635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method aims to make human mesh recovery more expressive, faster, and better aligned with real pixels. The integration of neural rendering for supervision within a feed-forward transformer pipeline is a clear step forward in practical human modeling However, its conceptual novelty is moderate, being primarily an engineering synthesis rather than a fundamental model innovation. The dependency on pseudo labels and lack of deeper analysis (domain generalization, temporal consistency, and robustness to in-the-wild conditions) slightly weaken the scientific depth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. By jointly regressing SMPLX (body) and FLAME (head) parameters under the Expressive Human Model (EHM), the presented method unifies coarse pose estimation with fine-grained facial expressiveness, which is more practical than the SMPLX-only methods.\n2. Real-time inference (0.05 seconds per frame) from a single 256×192 image, without cropping or high-resolution input, is practically valuable for downstream animation tasks and interactive applications.\n3. The construction of a large-scale dataset with body–face–hand pseudo ground-truth (SMPLX + FLAME parameters) is a valuable community resource."}, "weaknesses": {"value": "1. From the article, especially the contribution of the introduction part, it is unclear how the method achieves the promising results. Much of the framework builds upon GUAVA and HMR2, with the main innovation being the introduction of pixel-level supervision. Despite of integrating known components, what fundamentally new representational or algorithmic insight does PEAR introduce? Is the gain mainly from adding photometric loss?\n2. The paper focuses on alignment but does not address the limitations in clothing diversity or interactions. Can PEAR handle loose clothing, hair, or accessories not modeled by EHM?\n3. The improvement from stage 2 is shown but lacks deeper analysis. How does performance vary if the renderer is frozen vs. jointly trained? Does the photometric loss risk overfitting to appearance rather than geometry?\n4. Although tested on public benchmarks, most datasets are lab-style or curated internet data. How well does PEAR generalize to truly in-the-wild inputs (e.g., occlusions, extreme expressions, motion blur)? Is there any evaluation on real-time video streams?"}, "questions": {"value": "1. It's necessary to clarify the main technical novelty of the proposed approach.\n\n2.  Can PEAR handle loose clothing, hair, or accessories not modeled by EHM?\n\n3.  How does performance vary if the renderer is frozen vs. jointly trained? Does the photometric loss risk overfitting to appearance rather than geometry?\n\n4.  How well does PEAR generalize to truly in-the-wild inputs (e.g., occlusions, extreme expressions, motion blur)? Is there any evaluation on real-time video streams?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n//a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M57YkDyZKc", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Reviewer_fZ77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Reviewer_fZ77"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726845578, "cdate": 1761726845578, "tmdate": 1762917374793, "mdate": 1762917374793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper and Supplementary Material Updates"}, "comment": {"value": "Dear Reviewers and AC,\n\nWe have updated our paper and supplementary materials, with the following key changes:\n\n1. **Code optimization**: Our method now achieves **100 FPS inference speed**, instead of the previous 20 FPS.\n2. **Additional visualizations**: We added more supplementary videos to demonstrate the effectiveness of our method, including a real-time demo showing video-stream-driven animation of digital humans, validating the real-time capability of our model.  To prevent revealing the authors’ identities and address other ethical concerns, we applied a cartoonization to the faces to ensure the IDs remain unidentifiable.\n\n3. **Paper improvements**:  We have revised the manuscript according to your suggestions, with the updated parts highlighted in orange.\n\n\n\nWe welcome your review and feedback. \n\nBest regards,\n\nThe Authors."}}, "id": "Q5VCAxgxBC", "forum": "sy2Pp7bXxl", "replyto": "sy2Pp7bXxl", "signatures": ["ICLR.cc/2026/Conference/Submission4457/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4457/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4457/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763124745728, "cdate": 1763124745728, "tmdate": 1763125457406, "mdate": 1763125457406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}