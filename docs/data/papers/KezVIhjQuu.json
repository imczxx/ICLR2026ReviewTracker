{"id": "KezVIhjQuu", "number": 8091, "cdate": 1758061067861, "mdate": 1763505695555, "content": {"title": "Approaching the Harm of Gradient Attacks While Only Flipping Labels", "abstract": "Machine learning systems deployed in distributed or federated environments are highly susceptible to adversarial manipulations, particularly availability attacks—adding imperceptible perturbations to training data, thereby rendering the trained model unavailable. Prior research in distributed machine learning has demonstrated such adversarial effects through the injection of gradients or data poisoning. In this study, we aim to enhance comprehension of the potential of weaker (action-wise) adversaries by posing the following inquiry: Can availability attacks be inflicted solely through the flipping of a subset of training labels, without altering features, and under a strict flipping budget? \n\n\nWe analyze the extent of damage caused by constrained label flipping attacks against federated learning under mean aggregation—the dominant baseline in research and production. Focusing on a distributed classification problem, (1) we propose a novel formalization of label flipping attacks on logistic regression models and derive a greedy algorithm that is provably optimal at each training step. (2) To demonstrate that availability attacks can be approached by label flipping alone, we show that a budget of only $0.1$% of labels at each training step can reduce the accuracy of the model by $6$%, and that some models can perform worse than random guessing when up to $25$% of labels are flipped. (3) We shed light on an interesting interplay between what the attacker gains from more *write-access* versus what they gain from more *flipping budget*. (4) we define and compare the power of targeted label flipping attack to that of an untargeted label flipping attack.", "tldr": "", "keywords": ["Availability attacks", "Federated Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cf2714274a29d0cd3a43c9207919508db75f0f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper demonstrates that a label-flipping adversary with a strict budget can inflict an availability attack on federated learning, even under mean aggregation. The authors propose a greedy algorithm for logistic regression that is provably optimal at each training step. Experiments show that flipping just 0.1% of labels can reduce model accuracy by 6%, and a 25% budget can force performance near random guessing. The paper emphasizes that write-access ($k$) is more valuable than a larger local flipping budget ($b$) for the attacker."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates a highly constrained threat model where the adversary can only flip a small percentage of labels and cannot alter features or overwrite gradients. This rigorous setting reveals a significant vulnerability that is difficult for typical norm-based defenses to detect, making the findings relevant for real-world federated learning deployments.\n\n2. For the binary classification setting, the authors provide a novel problem formalization and derive a greedy algorithm (Algorithm 1) that is mathematically proven to be optimal at each training epoch. This strong theoretical foundation ensures the attack strategy is maximally efficient under the given constraints.\n\n3. The analysis provides a practical trade-off, showing that wider write-access ($k$) over the dataset is more impactful for the attacker than a larger local budget ($b$). This guides future research in defense design, suggesting that limiting the number of contributing clients is paramount to robustness."}, "weaknesses": {"value": "**Major Issues:**\n\n1. The paper's theoretical framework and empirical findings are grounded in the study of logistic regression models. Despite the justification provided in the appendix regarding tractability, this model is not a mainstream choice for the current distributed and federated learning systems that the paper aims to address. This limits the immediate practical relevance of the conclusions, as the attack's effectiveness and optimality guarantees are not confirmed for the widely used deep neural networks.\n\n2. The threat model relies on the server using mean aggregation, which is non-robust. The paper does not analyze the attack's efficacy against state-of-the-art FL defenses like robust aggregators (e.g., Krum, Median). According to the gradient visualization in Figure 4, the core attack strategy is to create a large divergence between the poisoned gradient and the honest gradient. This substantial directional and magnitude shift makes the malicious client update an easily identifiable outlier, which robust defense mechanisms are specifically designed to filter.\n\n3. Another major concern lies in the assumption that the malicious worker contributes to the final batch at every training epoch. This level of guaranteed, continuous write-access is seldom maintained in a practical FL setting where clients are typically selected randomly for participation in each round. Consequently, the modeled adversary possesses an artificially strong capability that may not align with the stochastic nature of client selection in real-world distributed learning deployments.\n\n\n**Minor Issues:**\n\n1. Citations: The current citation style used throughout the paper is incorrect for ICLR submissions. It should strictly adhere to the author-year format, such as (XXX et al., 2025). All in-text citations should be revised to conform to the required ICLR template.\n\n2. References: References [31] and [32] are duplicates. Please remove one of the entries to ensure an accurate and clean bibliography.\n\n3. Abstract: The first word of the fourth point in the Abstract should be capitalized for consistency: \"(4) We define and compare...\"\n\n4. Table Formatting: The title of Table 1 (\"Notation Summary\") is incorrectly positioned on the same line as the Section 2.1 heading. The table title should be distinct and positioned above the table as per standard academic formatting rules.\"\n\n5. Equation Numbering: The core optimization formula in line 206 is incorrectly numbered as (1a). This equation should be corrected to the primary label for the problem, which is (1).\n\n6. Layout Issue: The layout on Page 4 has a major formatting error where a line of body text (line 202-203) is awkwardly inserted between Figure 3 and Figure 4. This break in the flow is confusing for the reader and needs to be resolved by correctly placing the text relative to the figures.\n\n7. Informal Algorithmic Language: The final instruction in Algorithm 1 is phrased in overly informal and explanatory language. This style deviates from the presentation expected of a formal algorithm description in a research paper.\n\n8. Undefined Notation in Figures: The meaning of $h_{\\theta}$ in Figure 1 and $I_1, I_2, I_3, I_4$ in Figure 3 are not defined or explained in the captions, main text, or the notation summary table."}, "questions": {"value": "1. If the trained model were a non-convex architecture, such as a Multi-Layer Perceptron (MLP) or a Convolutional Neural Network (CNN), would your current conclusion regarding the greedy, per-epoch optimal attack still apply?\n\n2. How is the target parameter vector ($\\alpha^{Target}$ in binary classification or $W^{Target}$ in multi-class) for the targeted attack specifically set or chosen in your experiments?\n\n3. Assuming the more realistic scenario where the attacker can only participate in and attack a single training round (epoch), what measurable impact would this single-shot attack be capable of inflicting on the final model accuracy?\n\n4. Following a single-round attack, how many subsequent honest training rounds (epochs) would be required for the model's performance to recover to its normal expected effectiveness?\n\n5. What was the distribution of data across participants (clients/workers) for the multi-class experiments? If the data were highly non-IID (severely imbalanced), how would that condition affect the severity of the attack's results?\n\n6. How is the logistic regression per-sample gradient term $\\left(\\sigma\\left(\\alpha^{\\top} x_n\\right)-y_n\\right) x_n$ in Equation (2) derived from the cross-entropy loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9381gy6yUz", "forum": "KezVIhjQuu", "replyto": "KezVIhjQuu", "signatures": ["ICLR.cc/2026/Conference/Submission8091/Reviewer_89bY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8091/Reviewer_89bY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542763242, "cdate": 1761542763242, "tmdate": 1762920077101, "mdate": 1762920077101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the potential for availability attacks in federated learning systems with mean-like aggregation. The study asks if an attacker, who is constrained to only flipping labels on existing data and cannot alter features, can still significantly degrade model performance.\n\nThe authors formalize the label-flipping attack as a per-epoch constrained optimization problem. The attacker's objective is to choose a subset of labels to flip, within a set budget, to maximally misalign the resulting gradient from a reference direction . This formulation leads to a greedy algorithm that is proven to be optimal at each training step.\n\nThe threat model assumes an attacker with limited actions (label-flipping only) but strong knowledge. The attacker is considered omniscient, with full read-access to the model's parameters and the honest gradient at each epoch, which is required to calculate the optimal flips.\n\nThe experiments, conducted on logistic regression, show that this attack can reduce model accuracy by 6% when flipping only 0.1% of labels, and can force the model to near-random performance with a 25% budget. The paper also analyzes the trade-off between an attacker's \"write-access\" (the total data fraction $k$) and their \"local budget\" (the flipping fraction $b$), concluding that having wider write-access is more impactful. The method is presented for binary classification and then extended to the multi-class setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is motivated well. Figure 1 effectively illustrates the scope of this new attack, and Figure 2 presents a clear landscape of existing attacks, serving as a good survey.\n- The paper convincingly discusses the continued relevance of mean aggregation in FL. It then presents a clean mathematical formulation for its attack, centered on the novel and significant idea of selectively flipping labels to maximize gradient misalignment.\n- The setup and the threat model are clearly described and come across as fair. The resulting attack algorithm is simple, elegant, and intuitive.\n- A major strength is that the simple, greedy algorithm is not just a heuristic; it is mathematically proven to be the optimal strategy for the attacker at each epoch, given their budget.\n- The experiments are well-planned, and the results are extremely well-presented. The thoughtful choice of visualizations, like heatmaps and bar charts, and the clear design of the plot axes, convey a large amount of useful information in a highly interpretable form.\n- The results consistently follow and confirm the intuition behind the attack. Every experimental result is interpreted with wise conclusions and insights, helping the reader gain a deeper understanding of the attack's mechanics, such as the $k$ vs $b$ trade-off.\n- The paper successfully generalizes the attack from a binary setting to the multi-class setting with a formulation that is legitimate and well-reasoned."}, "weaknesses": {"value": "- The claim that altering even a tiny fraction of labels (like 0.1% or 1%) is impactful may be overstated as a general result. This impact is highly dependent on the specific loss landscape of the data-model pair, and it should be clarified that this observation was only confirmed for the datasets and models tested.\n- The paper claims (e.g., in Line 119) that such a small alteration could bypass anomaly detection, but this is presented without evidence. A critical question remains: how does the attack have such a large impact while remaining stealthy? One would want to know why the malicious gradients are not simply diluted by the low budgets.\n- The paper is confined to logistic regression which is a good starting point and has clean math, but any results on a neural network would be more appreciated.\n- It is not clear from the main paper what dataset is used for the main experiments. The appendix mentions MNIST and CIFAR-10 but it is not clear which result figures correspond to which dataset. \n- More details may be required to describe how the target model was computed. Does the adversary assume that the entire dataset is available to find this target model?"}, "questions": {"value": "- The result in Fig 5, which appears as the primary result, showing that accuracy decreases as the global budget ($k \\times b$) increases, is somewhat obvious. What is the main motivation and takeaway from Fig 5. It may not have been referenced in the main paper. \n- How practical is the attack? Suggest scenarios where an adversary is able to flip the labels but not the features. The budget also needs justification from any practical perspective. Since only mean aggregation is targeted in this attack, one cant argue that a low budget makes the attack stealthy.\n- As the training proceeds and the loss landscape is explored, is it possible if not frequent that a sample with label X was an effective attack in a certain round, but the same sample is flipped to label Y for an effective attack on a different round?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sbJvAung6i", "forum": "KezVIhjQuu", "replyto": "KezVIhjQuu", "signatures": ["ICLR.cc/2026/Conference/Submission8091/Reviewer_VG8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8091/Reviewer_VG8F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772618596, "cdate": 1761772618596, "tmdate": 1762920076625, "mdate": 1762920076625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a formalized analysis of label-flipping attacks in federated learning, showing that even a highly constrained attacker can cause significant degradation under mean aggregation. It contributes both theoretical insight (greedy optimality per epoch) and empirical evidence (strong attack effects under minimal corruption)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. It reframes label flipping as an optimization problem with provable properties.\n2. It provides formal derivation and proof of greedy optimality."}, "weaknesses": {"value": "1. The proposed attack is limited to logistic regression under convex settings. It is unclear how the findings extend to deep or non-convex models, which are the dominant cases in modern federated learning.\n2. The assumption of an omniscient attacker contradicts the paper’s framing of a “weaker adversary.” In practice, such full access is unrealistic.\n3. The results on availability degradation (accuracy drop) are modest for small budgets and demonstrated only in simplified, synthetic setups.\n4. The work does not include a comparative evaluation against stronger existing poisoning methods (e.g., gradient matching, backdoor attacks), so its relative effectiveness remains unclear."}, "questions": {"value": "1. Why restrict the analysis to mean aggregation only? Since many federated learning systems now use robust aggregators (e.g., trimmed mean, median), would your method still apply?\n2. The theoretical analysis guarantees only per-epoch optimality. Does this guarantee extend to the entire training trajectory? Could non-greedy strategies outperform the proposed one in the long term?\n3. How does your attack compare quantitatively against existing label-flipping or gradient-matching attacks from prior work? Are there cases where those methods are strictly stronger or weaker?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GvoYn6utQs", "forum": "KezVIhjQuu", "replyto": "KezVIhjQuu", "signatures": ["ICLR.cc/2026/Conference/Submission8091/Reviewer_w5Q8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8091/Reviewer_w5Q8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948784036, "cdate": 1761948784036, "tmdate": 1762920076229, "mdate": 1762920076229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the vulnerability of distributed or federated learning environments to adversarial attacks, particularly to availability attacks. The paper focuses on a weaker attacker, that is, generating an availability attack by only flipping labels under a constrained budget. The authors formulate this problem considering logistic regression models and propose a greedy algorithm that is provably optimal at each training step. The experiments show that availability attacks generated by only flipping labels are effective in significantly degrading model accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers the weakest adversary and analyzes its effect on model robustness, which is more practical and further connects data-poisoning with gradient-based availability attacks.\n- The formulation of the label flipping attack under the budget constraint is clear.\n- The extension of the proposed algorithm to multi-class classification shows generalization.\n- Per-step optimality of the proposed greedy algorithm is analyzed."}, "weaknesses": {"value": "- A recent study [*] also particularly focuses on label flipping attacks during training and uses a similar budget-constrained optimization formulation. The paper addresses the comparison between gradient attacks and data poisoning (line 158); however, a discussion should have been included in the related work section. Since it is directly relevant, the proposed algorithm could have been compared against this label flipping attack, aside from the gradient attack comparisons presented.\n\n> [*] Bal, M. I., Cevher, V., Muehlebach, M. (2025). Adversarial Training for Defense Against Label Poisoning Attacks. International Conference on Learning Representations (ICLR).\n\n- Although per-step optimality is presented, the algorithm is locally optimal at each training iteration. That is, the temporal coupling of the model update is overlooked. A discussion on what would happen if gradient directions oscillate, and (if) greedy label flipping could cancel its own effect, should have been provided to deepen the optimality analysis.\n\n- The paper considers only the logistic regression model; hence, the derivation does not hold for deeper or non-convex models. How can the method be extended to general differentiable models, NNs? For instance, could the authors provide an empirical example of the method applied to NNs?\n\n- The authors consider a weaker adversary that can only flip labels; however, the omniscient read-access assumption is strong. In a more realistic FL setting, partial knowledge is considered. What if the adversary has no access to all parameters or access to partial data? \n\n- Although the authors discuss the choice of mean aggregation in detail, I think there should be an ablation on this design choice to analyze the effectiveness of the attack better. \n\n- The empirical evaluation includes small datasets and simple settings, i.e., no heterogeneity or non-iid partitioning typical of federated learning is tested.\n\n- The current reference style used in the paper does not comply with ICLR2026 style. \n\"Citations within the text should be based on the natbib package and include the authors’ last names and year (with the “et al.” construct for more than two authors).\" Please check the paper template and modify the manuscript for the rebuttal version.\n\n- For readability, the main text should include a summary of the experimental setup. The dataset, model and sizes should be mentioned before presenting the results to improve readability.\n\n- Minor: Line 51, Figure caption: Extra blank space before dot."}, "questions": {"value": "- How $\\alpha$_target is selected? \n- See the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ckL2NWxpSy", "forum": "KezVIhjQuu", "replyto": "KezVIhjQuu", "signatures": ["ICLR.cc/2026/Conference/Submission8091/Reviewer_VmCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8091/Reviewer_VmCo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8091/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992108507, "cdate": 1761992108507, "tmdate": 1762920075794, "mdate": 1762920075794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}