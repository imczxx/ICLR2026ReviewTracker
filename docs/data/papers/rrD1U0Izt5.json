{"id": "rrD1U0Izt5", "number": 17193, "cdate": 1758273281336, "mdate": 1759897191482, "content": {"title": "NI Sampling: Accelerating Discrete Diffusion Sampling by Token Order Optimization", "abstract": "Discrete diffusion language models (dLLMs) have recently emerged as a promising alternative to traditional autoregressive approaches, offering the flexibility to generate tokens in arbitrary orders and the potential of parallel decoding. However, existing heuristic sampling strategies remain inefficient: they choose only a small part of tokens to sample at each step, leaving substantial room for improvement. In this work, we study the problem of token sampling order optimization and demonstrate its significant potential for acceleration. Specifically, we find that fully leveraging correct predictions at each step can reduce the number of sampling iterations by an order of magnitude without compromising accuracy. Based on this, we propose Neural Indicator Sampling (NI Sampling), a general sampling order optimization framework that utilize a neural indicator to decide which tokens should be sampled at each step. We further propose a novel trajectory-preserving objective to train the indicator. Experiments on LLaDA and Dream models across multiple benchmarks show that our method achieves up to 14.3$\\times$ acceleration over full-step sampling with negligible performance drop, and consistently outperforms confidence threshold sampling in the accuracy–step trade-off.", "tldr": "We propose a general framework for sampling order optimization of discrete diffusion models by using a neural indicator.", "keywords": ["Discrete Diffusion Sampling; Neural Indicator"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/221932a0b2bc57b0b47ce9e8e151f8b88921bbc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies acceleration methods for masked diffusion sampling. While masked diffusion models accept efficient prallel sampling/unmasking of multiple tokens without significant performance drop, how much speedup we can achieve has been unknown and perople used heuristic methods to determine the sampling order and parallelization magnitude. This paper introduces Trajectory-Preserving-Order, which measures to how much extent we can reduce the redundunt function evaluations of masked diffusion without changing the one-by-one sampling trajectory. In math and programming benchmarks with the top-1 probability argmax sampler, the authors confirm that up to 24x acceleration is theoretically possible. By learning the Trajectory-Preserving-Order by a neural network (neural indicator learning), NI sampling then achievs up to 14x acceleration, outperforming the threshold-based heuristic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well written, and offers an evidence that the optimization of sampling positions in masked diffusion can greatly accelerate the sampling process, *even without changing the output/trajectory*, through the empirical studies with Trajectory-Preserving-Order. This offers a deeper understanding of masked diffusion sampling from a novel angle. The neural indicator learning also has a straightforward learning signal and succeeds in actually accelerating masked diffusions."}, "weaknesses": {"value": "- [W1] A primary weakness is that this method works only for almost deterministic sampling strategies and tasks such as argmax sampling and math/programming tasks. Thus it does not align well with the direction of reproducing randomness, such as unconditional generation and creative tasks requiring diversity.\n- [W2] As the authors admit in Secion 7, the performance of NI sampler in this paper is far from that of Trajectory-Prserving-Order and closer to the heuristic Threshold sampler without any training. I'm not sure how difficult learning the Trajectory-Preserving-Order is in reality.\n\nTypos etc:\n- L70: gsampling -> sampling\n- L106: I find many `\\cdots` for sequences (e.g., $1, \\cdots, R$), but the standard notation aligning with amsmath (when the separator symbols are low) is `\\ldots` (e.g., $1, \\ldots, R$). It is not at all essential.\n- L182: minize -> minimize\n- L209: Preversing -> Preserving (same typo at **many** other positions as well)\n- L272: Indicato -> Indicator"}, "questions": {"value": "While I lean to acceptance, I suspect that the NI sampler just conducts semi-AR sampling (that is not a major issue though). To better understand the method, I have the following question:\n- [Q1] Trajectory-Preserving-Order may contain poisitions with a random coincidence such as comma and period. However, NI sampling in my intuition, when generalized well, follows mostly the natural order of words. That can result in a gap between the two, which might be very difficult to learn. I would like to see which word is sampled in which step, in examples like Tables 9 & 10, together with the result of Trajectory-Preserving-Order."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "57eNqlPls6", "forum": "rrD1U0Izt5", "replyto": "rrD1U0Izt5", "signatures": ["ICLR.cc/2026/Conference/Submission17193/Reviewer_Duu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17193/Reviewer_Duu3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761182161480, "cdate": 1761182161480, "tmdate": 1762927168599, "mdate": 1762927168599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the potential for reducing sampling steps in discrete Large Language Models (dLLMs) by merging backward steps wherever possible. To approximate this sampling strategy, the authors propose Neural Indicator Sampling (NI Sampling), which learns to identify which positions can be unmasked at each step."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of merging multiple sampling steps into a single step is simple yet presents an interesting direction for accelerating generation.\n- The proposed merging strategy significantly speeds up sampling while maintaining performance, at least in the scenario described in Section 3."}, "weaknesses": {"value": "**Concerns about Diversity of Generated Samples**\n- The approach relies heavily on a pre-computed, deterministic trajectory. I am concerned that this may lead to reduced diversity in the generated samples, especially if the sampling algorithm is entirely deterministic (see my question below).\n- Whether generation distribution given by the merging algorithm is close to that given by the full sampling in a distribution sense is not discussed.\n- While the experiments evaluate generation performance in terms of accuracy and speed, they do not assess diversity (e.g., entropy used in LLM field), which is a crucial aspect of generation quality.\n- The generation tasks on mathematical and code datasets are insufficient for evaluating diversity.\n\n**Performance Limitations of NI Sampling**\n- It is not clear what are missing components to fill the significant performance gap between NI sampling and ideal sampling presented in Section 3. The ablation study on network size suggests that this gap is not due to the network's expressive power. \n- Section 4.3 lists the network inputs, but it remains unclear what other components might be missing to fill the performance gap. It would be appreciated if insights into it can be more elaborated.\n\n**Experiments**\n- The proposed method is only compared with full sampling and the simple threshold-based method. However, there are related works [1,2] that should be discussed at least, even if they were originally proposed for vision domains.\n- It would be beneficial to present Figure 4 and related figures as time vs. accuracy plots.\n\n[1] Jose Lezama et al., \"Improved Masked Image Generation with Token-Critic,\" ECCV 2022.  \n[2] Jose Lezama et al., \"Discrete Predictor-Corrector Diffusion Models for Image Synthesis,\" ICLR 2023.\n\n**Typos**\n- \"gsample\" in Line 70"}, "questions": {"value": "- Is the proposed method limited to deterministic reference (trajectory)?\n- Is the sampling procedure after which positions will be unmasked at each step is deterministic (based on top-1 sampling)?\n- Could you clarify the statement \"which may be due to the variance in evaluation\" in Line 376. The meaning is unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Potential ethical concerns are not discussed."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PgRaI62Kcq", "forum": "rrD1U0Izt5", "replyto": "rrD1U0Izt5", "signatures": ["ICLR.cc/2026/Conference/Submission17193/Reviewer_wmZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17193/Reviewer_wmZ5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574567157, "cdate": 1761574567157, "tmdate": 1762927168293, "mdate": 1762927168293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NI Sampling, a learnable framework to accelerate dLLMs by optimizing the token sampling order.\n\nInstead of relying on heuristic confidence-based thresholds, the method introduces a neural indicator that predicts which masked tokens can be safely revealed at each step. To train this indicator, the authors design a trajectory-preserving objective, ensuring consistency with reference sampling trajectories while achieving significant speedups.\n\nEmpirical results show up to 14× acceleration compared to full-step sampling with negligible performance drop across benchmarks such as GSM8K, MATH, HumanEval, and MBPP."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Learnable Scheduler**\n  Improving the sampling efficiency of discrete diffusion models is an important and timely research direction. The idea of approaching this problem through a *learnable* mechanism for sampling order optimization is particularly interesting and potentially impactful."}, "weaknesses": {"value": "* **Overfitting to Reference Trajectory**\n  (Please correct me if I have misunderstood this point.)\n  A major limitation of NI Sampling is that it appears to produce generations that overfit to the *reference trajectory* used during training. In this sense, it differs fundamentally from efficient sampling studies in *continuous* diffusion models, where speedup is achieved by reducing *numerical errors* of ODE solvers rather than altering the learned data distribution.\n\nWhile the paper claims acceleration in sampling speed, this efficiency primarily stems from treating tokens as *conditionally independent*—i.e., ignoring inter-token correlations. In contrast, methods that preserve token correlations (e.g., [1]) achieve efficiency without such independence assumptions. Consequently, NI Sampling resembles trajectory-conditioned approaches such as [2], which restrict the model’s output distribution to a small subset of trajectories, potentially degrading sample diversity.\n\n[1]: *Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models*, [https://openreview.net/forum?id=pD6TiCpyDR](https://openreview.net/forum?id=pD6TiCpyDR)\n[2]: *Beyond Autoregression: Fast LLMs via Self-Distillation Through Time*, [https://arxiv.org/abs/2410.21035](https://arxiv.org/abs/2410.21035)"}, "questions": {"value": "* **Diversity and Trade-off Analysis**\n  As mentioned above, the main concern is that NI Sampling might overfit to the reference trajectory, thus harming sample diversity. To quantify this, please consider reporting:\n\n  1. **Token entropy**, to measure the diversity of generated tokens.\n  2. **pass@K**, to evaluate the diversity–accuracy trade-off more explicitly.\n\nAdditionally, the paper would benefit from a **theoretical analysis** of when and why merging steps (as done in the trajectory-preserving principle) does not harm the underlying distribution. In other words, even if the merged distribution deviates from the base model’s, a formal understanding of this trade-off would allow users to make informed decisions about when NI Sampling is appropriate. Currently, the method lacks such theoretical grounding, making it unclear under what conditions the distribution shift is acceptable.\n\n[1]: *Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design*, [https://arxiv.org/abs/2402.04997](https://arxiv.org/abs/2402.04997)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d73B2dcWTw", "forum": "rrD1U0Izt5", "replyto": "rrD1U0Izt5", "signatures": ["ICLR.cc/2026/Conference/Submission17193/Reviewer_B3No"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17193/Reviewer_B3No"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997371930, "cdate": 1761997371930, "tmdate": 1762927168053, "mdate": 1762927168053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NI Sampling, an efficient decoding framework for pre-trained diffusion LLMs that optimizes token sampling order with a tiny neural indicator predicting which masked positions can be simultaneously and safely revealed at each step. Labels for the indicator are derived from a trajectory-preserving criterion that merges steps when the current predictions already match a reference trajectory, turning correct predictions into parallel. On LLaDA-8B/1.5 and Dream-7B across GSM8K, MATH, MBPP, and HumanEval, NI Sampling delivers significant speedups over full-step sampling and threshold-based sampling with negligible accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I enjoyed reading this paper. It's well-motivated. The trajectory-preserving case is great and clear, demonstrating the great potential of merging sampling steps for acceleration\n- The proposed solution is conceptually simple and practical. A lightweight MLP as the indicator brings marginal extra cost.\n- The proposed method achieves consistent and significant speedups across multiple benchmarks compared to threshold-based sampling and full-step sampling, with negligible accuracy loss.\n- Detailed description of the setup of NI, analysis, and ablation studies, in addition to the main results."}, "weaknesses": {"value": "I think this paper and its contributions could be further enhanced with more investigation into the transferability of the neural indicator:\n- I got the difference between LLaDA models and Dream, but for models from the same family (e.g., LLaDA-8B and LLaDA-1.5 used in this paper), would it be possible to use a shared pretrained neural indicator?\n- Would it be possible to use the same pretrained neural indicator for different generation window lengths?"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fGNVSdrtu2", "forum": "rrD1U0Izt5", "replyto": "rrD1U0Izt5", "signatures": ["ICLR.cc/2026/Conference/Submission17193/Reviewer_VVhC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17193/Reviewer_VVhC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997528698, "cdate": 1761997528698, "tmdate": 1762927167785, "mdate": 1762927167785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}