{"id": "CZWcDWbs0h", "number": 13506, "cdate": 1758218730949, "mdate": 1759897432226, "content": {"title": "Towards Multi-view, Explainable and Scalable Representation Learning for Spoofed Audio Detection", "abstract": "Artificial Intelligence (AI)-generated content (deepfake content) is considered to be a major threat that can lead to fraud and the spread of incorrect information. The focus of generative AI research has largely been on advancements in content generation, with limited attention given to detection of AI generated content, particularly for AI generated audio content. Although foundation models offer powerful representations for detecting spoofed audio, they suffer from limitations in explainability and slow extraction speeds, which affect scalability. Prior research has integrated sociolinguistic expertise to identify phonetic and phonological cues in spoken English for spoofed audio detection. This approach, while successful, was limited in scale as it relied on the labeling of linguistic features by domain experts. In this paper we propose a novel model to auto-label expert-informed phonetic and phonological cues through deep-learning based representations fine tuned with domain expert input. As such, using the fine-tuning method with expert-informed features, we scale this interdisciplinary approach and demonstrate its benefits in enhancing explainability and optimizing resource utilization (consumed time) for utilizing foundation models in large-scale applications. For example, when considering XLSR-Wav2vec-ResNet18 as one of the most recent baselines, findings indicate that our method has decreased the Equal Error Rate of the baseline model in audio deepfake detection with at least 7\\% (effectiveness) across a subset of ASVspoof5 dataset.  In our proposed cost efficient ensemble setup, we have 31\\% time reduction in audio deepfake detection (scalability). Additionally, the algorithmically encoded linguistic features enhance the explainability via reverse engineering (explainability). Our proposed method is a multi-view approach as it takes advantage of not only deep representations, but also human expert-informed phonetic and phonological aspects of natural speech.", "tldr": "Linguistic module to add explainability and speed to audio deepfake detectors at scale", "keywords": ["audio deepfake", "spoofed audio detection", "Artificial Intelligence", "linguistics", "sociolinguistics", "linguistic perception"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/751f60feb4b6cdf27ed8114eabd87c3df197989d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ALIRAS, a method for spoofed audio detection that aims to improve explainability, scalability, and effectiveness. The core idea is to augment deep learning features from foundation models (e.g., Wav2Vec-XLSR, HuBERT) with expert-informed linguistic cues (anomalous breath, pitch, and audio quality). To make this scalable, the authors train a lightweight VGGish-based model to auto-label these linguistic features on a large dataset after being trained on a small, manually-labeled set. The ALIRAS features are then combined with foundation model features through ensemble methods, including a cost-efficient cascade model to reduce computation time. The authors use SHAP to provide explanations for the model's predictions based on the linguistic features."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The problem being addressed, creating explainable, scalable, and effective audio deepfake detectors, is timely and important for the AI community.\n- The concept of a multi-view approach, combining low-level deep representations with high-level, human-interpretable linguistic features, is a conceptually interesting direction for improving model robustness and trustworthiness."}, "weaknesses": {"value": "- Lack of Novelty: The proposed method is a straightforward combination of existing techniques. The use of expert-informed linguistic features for spoof detection builds directly on prior work cited by the authors (Zahra Khanjani, 2023). The \"auto-labeling\" component is a standard application of transfer learning or pseudo-labeling, and the ensemble techniques (weighted averaging and a cascaded filter) are rudimentary. The paper does not introduce any novel methodology to the field.\n- Misleading Scalability Claims: The paper claims a 31% reduction in processing time as a scalability improvement. This is not a true algorithmic enhancement; it is a simple engineering trade-off where a faster, less accurate model is used to filter data for a slower, more powerful one. The comparison in Table 4, which pits the inference time of a tiny pre-trained model against the full feature extraction of a massive one, is an unfair comparison that exaggerates the benefit.\n- Insufficient Data and Lack of Experimental Rigor: The study is not \"large-scale\" as claimed. The core ALIRAS model is fine-tuned on an \"Expert-labeled Dataset\" containing only 840 audio samples or an \"Large Scale Dataset\" containing only 7000 training samples, a size insufficient to guarantee a generalizable or robust model. The paper also omits crucial experimental details like data splits, training protocols, and hyperparameter tuning, which makes the results irreproducible.\n- Limited Baseline Comparisons: The paper's evaluation is narrow, comparing only against three foundation model backbones (Wav2Vec-XLSR, HuBERT, and WavLM). It fails to benchmark against a wider array of state-of-the-art spoof detection systems, making it difficult to properly contextualize the performance and effectiveness of the proposed method.\n- Poor Presentation Quality: The paper's presentation is below the expected standard for a top-tier conference. The manuscript suffers from unprofessional formatting and structural issues that impede clarity and credibility, making it difficult for readers to follow the methodology and results."}, "questions": {"value": "Given the fundamental issues detailed in the Weaknesses section, I will forego listing minor clarification questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKLdrmtbpY", "forum": "CZWcDWbs0h", "replyto": "CZWcDWbs0h", "signatures": ["ICLR.cc/2026/Conference/Submission13506/Reviewer_ptqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13506/Reviewer_ptqV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772069368, "cdate": 1760772069368, "tmdate": 1762924119349, "mdate": 1762924119349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a multi-view representation learning method, ALIRAS, for spoofed audio detection. The method aims to enhance explainability and scalability by incorporating expert-defined linguistic features (respiration, pitch, audio quality) into deep learning models. The core of the approach involves fine-tuning a VGGish model on an expert-labeled dataset to create an auto-labeling system. This ALIRAS model is then integrated with larger foundation models (XLSR, HuBERT, WavLM) using ensemble strategies, with the authors claiming improvements in processing time and explainability while maintaining or enhancing detection performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the critical and interconnected limitations of **explainability** and **scalability** in modern audio deepfake detection, which are major hurdles for deploying large foundation models.\n\n- The concept of a **cost-efficient ensemble**—using a lightweight, expert-informed model (ALIRAS) as a pre-filter to reduce the computational load on heavier models—is a practical and novel architectural approach to improving scalability.\n\n- The method offers a clear pathway to **explainability** by mapping model decisions back to tangible, human-understandable linguistic features (breath, pitch, quality) via SHAP analysis."}, "weaknesses": {"value": "- **Questionable Effectiveness of the ALIRAS Method**: The paper's primary contribution, the ALIRAS module, shows questionable efficacy. As per Table 5, integrating ALIRAS with HuBERT-ResNet18 results in **no change** to the Equal Error Rate (EER) (0.171 vs 0.171). Similarly, the integration with VGGish-MLP shows only a negligible change (0.302 vs 0.300). This strongly suggests that the ALIRAS module provides little to no additive detection performance, directly undermining the paper's claims of effectiveness.\n- **Insufficient Model Training Data**: The validity of the ALIRAS auto-labeling model, which is central to the entire method, is not convincing. As shown in Table 2, this model is fine-tuned on an Expert-labeled Dataset containing **only 840 samples**. This data volume is exceptionally small and very likely insufficient to train a model to reliably capture complex and subtle acoustic anomalies like \"anomalous pitch\" or \"audio quality.\" The paper does not provide sufficient evidence that the fine-tuned VGGish model has truly learned these features rather than just overfitting to this tiny dataset.\n- **Limited Benchmarking**: The evaluation presented in Table 5 is inadequate for a modern anti-spoofing paper. The authors did not compare their results against any current state-of-the-art (SOTA) countermeasures, such as **AASIST**, **XLSR_Mamba**. \n- **Incomplete Dataset Evaluation**: The experimental setup on the primary dataset (ASVspoof 2021 DF, mentioned in Table 1) is not general. The authors only use a subset (7,000 samples) rather than the full, standard evaluation set. To make any claims of robustness, the method should have been tested on more diverse, \"in-the-wild\" datasets that include real-world distortions, such as **CodecFake** or the **In-The-Wild** dataset.\n- **Presentation Errors**: The paper contains confusing errors. In Figure 1, there is a stray arrow originating from \"FC(3)\" that appears to be an artifact. In Table 5, the model \"AliRAS-MLP-VGGish-MLP\" uses a hyphen (-) which is inconsistent with the + notation used for all other ensembles (e.g., AliRAS-MLP + XLSR-ResNet18) and the methodology described in the text."}, "questions": {"value": "- Given the extremely small Expert-labeled Dataset (840 samples), how can the authors be confident that the VGGish model (Table 2) learned generalizable representations of the three linguistic features? Was any k-fold cross-validation performed on this small dataset? What were the precision, recall, and F1-scores for detecting each of the three features individually?\n- To experimentally verify the contribution of each linguistic feature in the ALIRAS method, have the authors conducted an ablation study? For instance, what is the performance (EER) of the ALIRAS-MLP model when trained and tested using only the 'breath' feature, only the 'pitch' feature, or only the 'audio quality' feature, respectively? Providing this experimental data would clearly demonstrate the necessity of all three features for the final model's performance.\n- The performance of the \"cost-efficient\" ensemble is critically dependent on the 0.55 decision threshold selected for the ALIRAS-MLP model. How was this \"optimal\" threshold determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F344Cete5S", "forum": "CZWcDWbs0h", "replyto": "CZWcDWbs0h", "signatures": ["ICLR.cc/2026/Conference/Submission13506/Reviewer_vRjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13506/Reviewer_vRjh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542207527, "cdate": 1761542207527, "tmdate": 1762924118989, "mdate": 1762924118989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles audio deepfake detection—classifying whether an audio sample is real or synthesized.\nThe main idea is to fuse two classifiers: (1) a classifier that predicts through three interpretable features (presence of breath, pitch anomaly, audio quality anomaly), and (2) a classifier based on self-supervised speech features.\nThe first classifier has the advantage of being more efficient (uses small networks to predict features) and more interpretable (predictions can be explained through these intermediary features).\nThe two classifiers are combined using an asymmetric rule: whenever the first classifier predicts \"fake\", the final prediction is \"fake\"; otherwise, the prediction of the second classifier is returned.\nThis approach is evaluated on 14,000 samples from ASVspoof 2019 and ASVspoof 2021."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses two important goals for practical deepfake detection: explainability and scalability.\n- The simplicity of the approach is appreciated and makes it potentially practical to deploy."}, "weaknesses": {"value": "- The evaluation is carried on a single dataset and there are no comparisons to prior work. For example, Martín Doñas et al. (2024) and Wang and Yamagishi (2024), amongst others, evaluate in the ASV19 → ASV21 setup. They obtain errors well as low as 1.16% EER, which are substantially better than the 17.1% EER reported in the paper.\n- Given that the auto-labeling model is able to predict the interpretable features with only 71% AUC (Table 2), how can we trust it for the next stage (deepfake detection)? Note that this performance is also an upper-bound: on out-of-domain data (ASV19 or ASV21), I expect this value to be even lower.\n- The point above raises concerns about the paper claim of explainability. How can we trust explanations based on \"anomalous breath\" or \"pitch anomaly\" when these features themselves are only ~70% accurate?\n- The presentation of the paper could be improved. For example: parts of the methodological section includes ample discussion on prior work, which makes the paper hard to follow; the naming of the method is not always consistent, e.g., Table 5 explicitly names the \"cost efficient\" combinations, while in Table 3 this is left implicit; in Table 4 it's not immediately clear where the difference in time appears between VGGish and ALiRAS, which also relies on VGGish; the authors refer to the three features (breath, pitch, audio quality) as \"linguistic\", but these are rather phonetic or phonological features.\n- The combinations of the two models biases the predictions towards the \"fake\" class (Eq. 3). Why not use the model's confidence as the gating criterion?\n\nReferences:\n- Martín Doñas, Juan Manuel, et al. \"Exploring self-supervised embeddings and synthetic data augmentation for robust audio deepfake detection.\" *Interspeech*, 2024.\n- Wang, Xin and Junichi Yamagishi, “Can large-scale vocoded spoofed data improve speech spoofing countermeasure with a self-supervised front end?” _ICASSP_, 2024."}, "questions": {"value": "Questions:\n- See weakness.\n- How were the sizes of the auto-labeling networks been selected (L214–215)? Did you try training a similar sized VGGish network for the task of deepfake detection?\n- What is the annotator inter-agreement of the \"linguistic\" annotations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "meF4TLzfXh", "forum": "CZWcDWbs0h", "replyto": "CZWcDWbs0h", "signatures": ["ICLR.cc/2026/Conference/Submission13506/Reviewer_ZPBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13506/Reviewer_ZPBx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670550610, "cdate": 1761670550610, "tmdate": 1762924118647, "mdate": 1762924118647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an Auto-labeled Linguistic Representations for Audio Spoofing Detection system, which is a multi-view, expert-in-the-loop framework for detecting AI-generated audio. The method combines foundation model features (e.g., Wav2Vec-XLSR, HuBERT, WavLM) with auto-labeled phonetic and phonological cues derived from expert linguistic annotations. Experiments show that ALiRAS achieves a 7% improvement in Equal Error Rate over XLSR-ResNet18 and a 31% reduction in processing time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper bridges sociolinguistics and deep learning, introducing an automatic labeling process that scales previous human-expert-based approaches.\n\n2. The authors build upon and extend the previous expert-in-the-loop framework, showing continuity and advancement in explainable ADD design."}, "weaknesses": {"value": "1. **The paper’s organization could be significantly improved.** In particular, the Introduction section contains an extensive discussion of prior work that would be more appropriately placed in a Related Work section. Additionally, the citation style and formatting are inconsistent and do not adhere to standard conventions, which disrupts the reading flow. The roadmap paragraph also lacks clear section references (e.g., section numbers), making it harder for readers to follow the paper’s structure. Overall, these issues reduce the clarity and readability of the paper.\n\n2. While the integration of linguistic auto-labeling is novel, the underlying architecture (ensemble of MLP + ResNet18 with existing embeddings) may be seen as incremental rather than fundamentally novel.\n\n3. The linguistic features (breath, pitch, audio quality) are few and coarse-grained; it remains unclear whether such limited cues can generalize to diverse languages, speakers, or unseen generation methods.\n\n4. All linguistic labeling and evaluation are English-based, using ASVspoof subsets. No cross-lingual or real-world noisy evaluation is provided to support the “scalable” or “generalizable” claims."}, "questions": {"value": "1. How well do the auto-labeled linguistic features generalize to unseen datasets or non-English speech without expert retraining?\n\n2. Did you perform any human evaluation (e.g., linguist or user study) to verify that SHAP explanations correspond to meaningful phonetic phenomena?\n\n3. Have the authors conducted ablation studies to investigate how much each component (auto-labeling, cost-efficient ensemble, linguistic feature type) contributes to the overall performance gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UJ7fqYZvn6", "forum": "CZWcDWbs0h", "replyto": "CZWcDWbs0h", "signatures": ["ICLR.cc/2026/Conference/Submission13506/Reviewer_ASCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13506/Reviewer_ASCz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909586552, "cdate": 1761909586552, "tmdate": 1762924118330, "mdate": 1762924118330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}