{"id": "mXPeXZ1KWT", "number": 5125, "cdate": 1757851862906, "mdate": 1759897993169, "content": {"title": "Event-T2M: Event-level Conditioning for Complex Text-to-Motion Synthesis", "abstract": "Text-to-motion generation has advanced with diffusion models, yet existing systems often collapse complex multi-action prompts into a single embedding, leading to omissions, reordering, or unnatural transitions. In this work, we shift perspective by introducing a principled definition of an event as the smallest semantically self-contained action or state change in a text prompt that can be temporally aligned with a motion segment. Building on this definition, we pro- pose Event-T2M, a diffusion-based framework that decomposes prompts into events, encodes each with a motion-aware retrieval model, and integrates them through event-based cross-attention in Conformer blocks. Existing benchmarks mix simple and multi-event prompts, making it unclear whether models that succeed on single actions generalize to multi-action cases. To address this, we con- struct HumanML3D-E, the first benchmark stratified by event count. Experiments on HumanML3D, KIT-ML, and HumanML3D-E show that Event-T2M matches state-of-the-art baselines on standard tests while outperforming them as event complexity increases. Human studies validate the plausibility of our event definition, the reliability of HumanML3D-E, and the superiority of Event-T2M in generating multi-event motions that preserve order and naturalness close to ground- truth. These results establish event-level conditioning as a generalizable principle for advancing text-to-motion generation beyond single-action prompts.", "tldr": "", "keywords": ["text-to-motion generation", "event-level conditioning", "event decomposiiton"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f758370d3eb9fc3684fb06062a0d19ccc59282f.pdf", "supplementary_material": "/attachment/7ffea1983e8341383ceacf35d3f74591149a898d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Event-T2M—a diffusion framework that decomposes prompts into multi-events, encodes them with a TMR encoder, and integrates them via event-based cross-attention in Conformer blocks. The authors also build HumanML3D-E, a benchmark stratified by event count. Experiments show Event-T2M matches SOTA on standard tests (HumanML3D, KIT-ML) and outperforms baselines as event complexity rises."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper solves multi-action prompt mishandling via a principled \"event\" definition and Event-T2M (with event decomposition, TMR encoding, and ECA module), avoiding action issues like omissions. Authors also build HumanML3D-E, the first event-count-stratified benchmark, fixing existing benchmark gaps. This paper also provides solid experiments (matching SOTA on HumanML3D/KIT-ML, outperforming baselines on complex HumanML3D-E) and user studies validating event definition, benchmark reliability, and motion quality."}, "weaknesses": {"value": "The ablation analysis in this paper is limited in scope. Although multiple new modules are proposed (e.g., LIMM, ATII, Conformer, ECA), the experiments focus solely on the ECA module and the text encoder, failing to assess the necessity and individual impact of the other introduced components."}, "questions": {"value": "1. In Equations (3) and (7), a coefficient of 0.5 is applied to the residual term. It remains unclear why this specific coefficient was chosen instead of 1, and the rationale behind this design choice warrants further explanation.\n2. The efficiency analysis appears to overlook the computational overhead introduced by the Large Language Model (LLM). While the baseline model (e.g., Momask) does not employ an LLM for text segmentation, the proposed Event-T2M model utilizes an LLM to partition the input text into events. The time cost associated with this LLM processing stage should be accounted for in the overall efficiency evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mG1KqYyoHb", "forum": "mXPeXZ1KWT", "replyto": "mXPeXZ1KWT", "signatures": ["ICLR.cc/2026/Conference/Submission5125/Reviewer_mvTh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5125/Reviewer_mvTh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760950191685, "cdate": 1760950191685, "tmdate": 1762917895731, "mdate": 1762917895731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful evaluation. We are actively addressing all concerns raised by the reviewers. In this rebuttal,\n\n&nbsp;&nbsp;&nbsp;&nbsp;➡ We first outline the key issues identified by each reviewer and clearly describe the directions and experiments we are currently pursuing.\n\n&nbsp;&nbsp;&nbsp;&nbsp;➡ As these experiments finish, we will update the rebuttal with the corresponding results as promptly as possible.\n\nSome items, such as experiments on additional datasets and evaluations requiring human annotation, naturally require more time to complete. We kindly ask for the reviewers’ understanding as we prioritize delivering the most critical results as early as possible.\n\nWe appreciate the following strengths highlighted across the reviews:\n\n&nbsp;&nbsp;&nbsp;&nbsp;● HumanML3D-E was recognized as a valuable benchmark for analyzing motion generation by event complexity and as the first event-level stratified evaluation (hnW1, 5wEr, Kztd, mvTh).\n\n&nbsp;&nbsp;&nbsp;&nbsp;● Reviewers noted that modeling complexity through the number of events is intuitive and that our event definition and Event-T2M framework provide a principled way to address long and complex prompts (hnW1, Kztd, mvTh).\n\n&nbsp;&nbsp;&nbsp;&nbsp;● The event-based design, especially the ECA module, was viewed as reasonable and meaningful (hnW1, mvTh).\n\n&nbsp;&nbsp;&nbsp;&nbsp;● Our strong results on HumanML3D/KIT-ML and improvements on HumanML3D-E, supported by human studies validating event definitions and motion quality, were positively received, as was the clarity of the writing (hnW1, mvTh).\n\nMultiple reviewers raised concerns regarding “fairness and completeness of our experimental evaluation”.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **hnW1**: requested human validation of LLM-generated event labels, comparison with stronger baselines (MoGenTS, MARDM, LaMP), and extending the benchmark to other datasets (KIT-ML, Motion-X) for better generalizability.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **Kztd**: asked for evaluation on a human-segmented complex-motion test set independent of the LLM pipeline, and for testing baselines with TMR replacing CLIP (including TMR word-level K/V) to assess fairness.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **5wEr**: questioned whether event-driven methods outperform action-driven or hybrid approaches, and whether our method is superior to retrieval-enhanced T2M pipelines.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **mvTh**: noted that ablations only cover ECA and the text encoder, leaving other modules (LIMM, ATII, Conformer) insufficiently analyzed, and pointed out that LLM preprocessing costs should be included in the efficiency comparison.\n\nWe will address these concerns through the following additions:\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ **Broader benchmark coverage**: event-level evaluation on KIT-ML and Motion-X, plus comparisons with stronger baselines (MoGenTS, MARDM, LaMP).\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ **Human-refined evaluation**: testing on manually segmented complex-motion subsets and manually checking LLM event split accuracy.\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ **Alternative paradigms**: comparing event-, action-, and hybrid-driven variants, retrieval-based methods, and baselines with CLIP → TMR replacements.\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ **LLM cost analysis**: reporting computation/latency costs of LLM-based event decomposition and incorporating them into overall efficiency comparisons.\n\nSeveral reviewers raised concerns regarding “contribution clarity”.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **Kztd**: highlighted missing discussion of prior LLM-based methods (ATOM, InstructMotion) and requested clearer distinctions from Event-T2M.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **5wEr**: argued that TMR may be only an input change and that LIMM, ATII, and ECA resemble common patterns.\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ We will address these by clearly positioning Event-T2M relative to ATOM and InstructMotion, explaining conceptual differences, and adding ablations that isolate the impact of LIMM, ATII, and ECA to demonstrate their technical contributions.\n\nLastly, other reviewers raised concerns regarding “additional methodological analysis”.\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **hnW1**: requested failure cases on complex/multi-event prompts (e.g., ≥4 events).\n\n&nbsp;&nbsp;&nbsp;&nbsp;● **mvTh**: asked for justification of the 0.5 residual coefficient and comparison with a weight of 1.\n\n&nbsp;&nbsp;&nbsp;&nbsp;➤ We will include qualitative failure-case examples for both our model and baselines, analyze their causes, and provide rationale and sensitivity results for the residual weight choice.\n\nWe address all remaining reviewer-specific issues in the individual responses below."}}, "id": "EkBQxEfPZr", "forum": "mXPeXZ1KWT", "replyto": "mXPeXZ1KWT", "signatures": ["ICLR.cc/2026/Conference/Submission5125/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5125/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5125/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763660736745, "cdate": 1763660736745, "tmdate": 1763660736745, "mdate": 1763660736745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Event-T2M, a diffusion-based framework that decomposes complex text prompts into semantically self-contained events and generates motion through event-based cross-attention. It also builds HumanML3D-E, the first benchmark stratified by event count, and demonstrates that Event-T2M maintains state-of-the-art performance while significantly improving motion coherence and naturalness for multi-event prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Proposes an event-based paradigm for motion generation.\n\n2.Constructs the first event-level motion generation dataset."}, "weaknesses": {"value": "1.Does event-driven motion generation offer advantages over action-driven or hybrid (action + event) methods?\n2.Does the proposed method outperform approaches that enhance motion quality through motion retrieval?\n3.In TMR, innovation based solely on input differences does not constitute true novelty.\n4.LIMM, ATII, and ECA follow common module design patterns and lack sufficient originality."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sKYwdnQ0oN", "forum": "mXPeXZ1KWT", "replyto": "mXPeXZ1KWT", "signatures": ["ICLR.cc/2026/Conference/Submission5125/Reviewer_5wEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5125/Reviewer_5wEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705380930, "cdate": 1761705380930, "tmdate": 1762917895346, "mdate": 1762917895346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key challenge in complex Text-to-Motion generation: the difficulty of existing models in handling prompts with multiple sub-motions, which often leads to omissions, merging, or reordering of motions. To solve this, the authors propose Event-T2M, a diffusion-based framework. The core idea is to first introduce a definition of an \"event\" as the smallest, semantically self-contained action unit in a text. They then use a Large Language Model (LLM) to decompose the input text into a sequence of these \"event\" clauses. These clauses are subsequently encoded by a TMR encoder (trained for motion-text alignment) into \"event tokens.\" Finally, these event tokens are injected into a Conformer-based diffusion model via a novel \"Event-based Cross-attention\" (ECA) module to guide the generation of the motion sequence. Furthermore, to specifically evaluate the model's ability to handle complex prompts, the authors construct a new benchmark, HumanML3D-E, which stratifies the HumanML3D test set by the number of events in the text. Experimental results show that Event-T2M achieves comparable performance to SOTA on standard benchmarks (HumanML3D, KIT-ML) but significantly outperforms baselines on the new HumanML3D-E benchmark, especially as event complexity increases."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The problem significance is huge. Generating complex and consistent human motions is an unsolved challenge in the T2M field.\n- This paper proposes a novel benchmark called HumanML3D-E. This is the first benchmark stratified by the \"event complexity\" of the prompts. It provides a very valuable evaluation tool for future research on long and complex T2M generation field.\n- The idea of decompose the complex motions is very intuitive and logical."}, "weaknesses": {"value": "- **Unfair Comparison**: The authors' new benchmark, HumanML3D-E, is constructed using an LLM and a specific \"event-aware prompt.\" However, the proposed model, Event-T2M, **also relies on the exact same LLM and the exact same prompt** in its data preprocessing stage. Event-T2M is evaluated on a test set that is perfectly aligned with its own training and inference pipeline. In contrast, all baseline models are evaluated without using this LLM-based event decomposition preprocessing. This constitutes an extremely unfair comparison. The poor performance of the baselines on HumanML3D-E is likely just an artifact of their input representation (e.g., CLIP-based word tokens) being mismatched with the benchmark's construction (LLM-based clauses), not because their architectures inherently fail at complexity.\n\n- **Limited Technical Novelty**: I must point out that there are already some methods trying to solve the generation of the long motions using LLM. For instance, the recent ATOM[1] framework uses GPT-4 to construct event-level prompts and GPT-4V as an AI reward model to fine-tune a generator, specifically targeting event-level alignment (integrity, temporal order, and frequency). Additionally, InstructMotion[2] explicitly uses an LLM to generate long prompts, subsequently using Reinforcement Learning (RL) to fine-tune an autoregressive motion generator. **Worryingly, these highly relevant prior works, which also tackle event-level or complex alignment using LLMs, are not cited or discussed in the paper's Related Works.**\n\n[1] Han H, Wu X, Liao H, et al. Atom: Aligning text-to-motion model at event-level with gpt-4vision reward[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 22746-22755.\n[2] Mao, Yunyao, et al. \"Learning generalizable human motion generator with reinforcement learning.\" arXiv preprint arXiv:2405.15541 (2024)."}, "questions": {"value": "- Can you evaluate your model on a test set that was not constructed using your LLM pipeline? On a complex motion test set where events were manually segmented and temporally aligned by human annotators, would Event-T2M still show an advantage over baselines?\n- For a fair comparison, if you were to replace the CLIP encoder in the baseline models (like AttT2M or MoMask) with the TMR encoder (and use TMR's word-level tokens as K/V), how much would their performance on HumanML3D-E improve? As I know, replacing the CLIP encoder with TMR encoder will significantly improve the performance.\n- What are the differences between Event-T2M and other motion generators using LLM for decomposition?\n\n**I will raise the score to a positive mark if you can address my concerns regarding the \"unfair comparison\"** (mainly concerning the experimental setup for the TMR encoder and the aspects mentioned in the Weaknesses section)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v03iM4QyWB", "forum": "mXPeXZ1KWT", "replyto": "mXPeXZ1KWT", "signatures": ["ICLR.cc/2026/Conference/Submission5125/Reviewer_Kztd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5125/Reviewer_Kztd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929850488, "cdate": 1761929850488, "tmdate": 1762917894811, "mdate": 1762917894811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an event-level text-to-motion generation benchmark and a stronger baseline for the task. To explicitly model the complexity of a target motion, the paper proposed to utilize an LLM to split the motion text prompts into event-level actions, where a larger number of events indicates harder cases. For a stronger baseline of the proposed task, an event-based cross-attention module is injected into the diffusion-based motion generation framework to improve the performance. Experimental results on the benchmark dataset validate the effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The point of modeling the motion complexity by the number of events is straightforward and reasonable. The proposed HumanML3D-E benchmark will be beneficial to the community, which can evaluate motion generation frameworks on more detailed levels of complexity.\n- The experimental analysis of different methods on different event counts supports the motivation of the proposed event-based benchmark. \n- The design of the event-based cross-attention module is reasonable and validated by ablation studies.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The events of a motion are divided by an LLM with text input only. The label may contain errors. Manually validating the labels or sampling cases to check the accuracy rate of the LLM labels will be beneficial.\n- The paper misses some comparisons with some recent stronger baselines, e.g., MoGenTS (NeurIPS 2024), MARDM (CVPR 2025), and LAMP (ICLR 2025). \n- The event-based benchmark only contains one dataset, HumanML3D. It's better to add more datasets, e.g., KIT-ML, Motion-X, to better validate the generalizability of different methods.\n- Providing some failure cases of the proposed framework and previous work on complex scenarios, e.g., 4 events, will be beneficial."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DyNzWBJI8i", "forum": "mXPeXZ1KWT", "replyto": "mXPeXZ1KWT", "signatures": ["ICLR.cc/2026/Conference/Submission5125/Reviewer_hnW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5125/Reviewer_hnW1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283294987, "cdate": 1762283294987, "tmdate": 1762917894451, "mdate": 1762917894451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}