{"id": "DuDFytFC5Z", "number": 11764, "cdate": 1758203615120, "mdate": 1763748362492, "content": {"title": "Rethinking Causal Mask Attention for Vision-Language Inference", "abstract": "Causal attention has become a foundational mechanism in autoregressive Vision-Language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model’s ability to leverage future context that often contains essential semantic cues for accurate inference.\nIn this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting.\nWe first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model’s capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies.\nWe evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.", "tldr": "We challenge the traditional causal mask design and comprehensively investigate its role in vision-language inference.", "keywords": ["Vision-Language Inference", "Casual Mask Attention"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1406464ded377c5ea4b527a7bf479fd00c1aec31.pdf", "supplementary_material": "/attachment/94cf4b4d58c8643f2814d61d595fb97ea1c567d9.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the default use of standard causal attention masks in VLMs. The paper empirically demonstrates that relaxing causal constraints for visual tokens can improve performance, whereas relaxing them for textual tokens severely degrades it. The authors then systematically investigate a family of \"future-aware\" causal masks: \n1. $M^{f}$ (Full Future): Allows visual tokens to see all future visual and textual tokens.\n2. $M^{v2v}$ (Visual-to-Visual): Allows visual tokens to see future visual tokens only.\n3. $M^{v2t}$ (Visual-to-Textual): Allows visual tokens to see future textual tokens only.\n\nTheir analysis reveals that these relaxed masks provide task-specific benefits: $M^{f}$ excels at temporal multi-image tasks, $M^{v2v}$ at visual relation inference, and $M^{v2t}$ at text-rich image QA. Recognizing that these masks introduce significant computational overhead during decoding, the authors propose a \"Light Future Aware Attention\" mechanism. This method computes the future-aware attention only during the prefill stage, then uses kernel pooling to compress this future semantic information and \"merge\" it into the initial past token representations. This approach allows the model to benefit from future context while retaining the standard, efficient causal mask during the autoregressive decoding phase. Experiments across 15+ benchmarks show this lightweight method achieves performance on par with or even exceeding the full future-aware masks, but with a 2-3x speedup in decoding latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a simple, intuitive, yet largely overlooked problem: the fundamental mismatch between the strictly sequential causal mask and the non-sequential nature of visual data.\n2. The paper provides a thorough and systematic breakdown of the problem. The graphs and tables are clear."}, "weaknesses": {"value": "1. The proposed method focuses on processing input tokens during the prefill stage. It's unclear how, or if, this concept could be extended to multi-turn dialogue where the future is truly unknown."}, "questions": {"value": "1. How do you train your VLMs? Do all settings follow LLaVA's 1.5 training process?\n2. Why don't you use an additional token, similar to an attention sink?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBlb9AzeOb", "forum": "DuDFytFC5Z", "replyto": "DuDFytFC5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11764/Reviewer_11Kz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11764/Reviewer_11Kz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552852440, "cdate": 1761552852440, "tmdate": 1762922789598, "mdate": 1762922789598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the suitability of standard causal attention masks, inherited from text-only LLMs, for autoregressive Vision-Language Models (VLMs). The authors argue that the strict left-to-right masking applied to visual tokens is an overly rigid constraint that prevents the model from leveraging valuable future context essential for visual reasoning. Through empirical analysis, the paper demonstrates that relaxing these causal constraints for visual queries can surprisingly improve performance. Based on this, the authors propose a family of future-aware causal masks that selectively allow visual tokens to access future context. To mitigate the computational cost of this relaxation, they also introduce a lightweight pooling mechanism to compress and merge future semantic information into past representations during the prefill stage, preserving the efficient autoregressive structure for decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper compellingly identifies a fundamental misalignment between the sequential, autoregressive nature of LLM-native causal masks and the more holistic, non-sequential nature of visual information processing. I like the motivation.\n- The authors systematically evaluate different masking strategies ($M^f$, $M^{v2v}$, $M^{v2t}$) and connect their benefits to specific categories of vision-language tasks (e.g., temporal reasoning, visual relation, text-rich QA), providing a nuanced understanding of when and why future context is beneficial.\n- The findings are well-supported by experiments across several benchmarks, demonstrating consistent performance gains from the proposed future-aware masks and their lightweight variants."}, "weaknesses": {"value": "- Lack the Ethics statement and Reproducibility statement in the main text.\n- The paper demonstrates that different future-aware masks (e.g., $M^{v2v}$ vs. $M^{v2t}$) are optimal for different tasks. This raises a practical question: how would a single general model choose the correct mask without a priori knowledge of the downstream task? The paper does not propose a dynamic or learned mechanism for this selection.\n- The experiments are conducted by modifying the LLaVA. It doesn't explore how these masking strategies might affect the model, such as Qwen-VL series or VILA.\n- The details of the ID kernel pooling and its integration could be explained in greater detail to ensure the reproducibility.\n- The paper's core idea of rethinking causal masks for specific modalities shares a conceptual similarity with other recent work CoT-VLA [1], which adapts attention for action tokens. This general direction of moving beyond rigid, text-native causal masking is necessary and very meaningful for the development of MLLMs. I am open for rasing my score if the authors can address my questions.\n\n[1] Zhao, Qingqing, et al. \"Cot-vla: Visual chain-of-thought reasoning for vision-language-action models.\" CVPR 2025."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SFzs1avM6j", "forum": "DuDFytFC5Z", "replyto": "DuDFytFC5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11764/Reviewer_3gm7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11764/Reviewer_3gm7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943880405, "cdate": 1761943880405, "tmdate": 1762922789070, "mdate": 1762922789070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the role of causal masking in autoregressive Vision-Language Models (VLMs). While causal attention ensures sequential generation for text, the authors argue that its direct adoption for visual tokens is suboptimal because visual information is inherently non-sequential. Through systematic analysis, they demonstrate that relaxing causal masks for vision tokens—allowing selective access to future context—can improve performance in temporal reasoning, relational understanding, and text-rich visual tasks. They propose three future-aware masking variants (visual-to-visual, visual-to-textual, and full), along with a lightweight merging mechanism that compresses future information into past tokens to preserve autoregressive efficiency. Extensive experiments on 15 multimodal benchmarks show consistent gains across tasks, with improved reasoning accuracy and minimal latency overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a fresh and well-motivated perspective on how causal masking—originally designed for textual decoding—may be suboptimal for vision tokens. This conceptual rethinking addresses a fundamental assumption in current VLMs and opens up a new line of research on modality-aware causality.\n\n2. The proposed light future-aware attention introduces future context compression without retraining or architectural changes, adding negligible latency while delivering consistent gains.\n\n3. Experimental results show consistent accuracy boosts across 15+ benchmarks."}, "weaknesses": {"value": "Actually I really like the insight this paper focuses on — questioning how VLMs can break free from the traditional causal attention inherited from language models. The idea is intuitively sound and genuinely interesting. However, given the paper’s current state, I cannot yet recommend acceptance. **I strongly encourage the authors to carefully revise the work, as it has great potential**. My main concerns are as follows:\n\n1. The paper currently provides an investigation and an inference-only solution, which reveals some of the limitations of causal masking but does not fundamentally resolve them. To convincingly verify the effectiveness of a non-causal design, the model should be adjusted (or at least fine-tuned) from the training stage, not just modified at inference.\n\n2. The results are not yet compelling, even though I believe the proposed approach can work. For instance, in Table 4, the overall performance of different LLaVA-7B and LLaVA-13B variants is quite close — no single attention structure consistently outperforms all others, which makes it difficult to draw firm conclusions.\n\n3. The idea of expanding receptive fields by merging future information into early tokens is not entirely novel; similar strategies have appeared in recent pure vision models [1].\n\n4. The paper should also discuss the connection between attention sinks [2] and the proposed approach. Prior studies have shown that language models naturally allocate disproportionate attention to early tokens. Could the proposed merging mechanism be implicitly benefiting from this effect? A more explicit analysis of this interaction would greatly strengthen the work.\n\n[1] Wang F, Yang T, Yu Y, et al. Causal image modeling for efficient visual understanding. 2024.\n\n[2] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with\nattention sinks,” arXiv preprint arXiv:2309.17453, 2023."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ainx50znmj", "forum": "DuDFytFC5Z", "replyto": "DuDFytFC5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11764/Reviewer_DpXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11764/Reviewer_DpXd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972838678, "cdate": 1761972838678, "tmdate": 1762922788593, "mdate": 1762922788593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic investigation into the role of causal attention in VLM. The authors identify a key mismatch: the strict left-to-right causal masking inherited from text-only models constrains visual tokens, which naturally lack sequential order. To address this, they introduce a family of future-aware causal masks that selectively relax causal constraints, enabling visual queries to attend to future visual (M^{v2v}), textual (M^{v2t}), or both types of tokens (M^f). The paper further proposes a lightweight variant that encodes future-attention information into a prefix during prefill, balancing performance gains with efficient autoregressive decoding. Extensive experiments on 15 multimodal benchmarks show consistent, task-dependent improvements, providing new insights into the design of modality-aware attention mechanisms for VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a fundamental problem that the misalignment between text-oriented causal attention and the non-sequential nature of visual processing in VLMs. \n2.  The authors conduct a large-scale, systematic analysis of multiple future-aware causal masking strategies (M^f, M^{v2v}, M^{v2t}) across diverse multimodal benchmarks. The clear, task-dependent findings (e.g., M^{v2v} for visual reasoning, M^{v2t} for text-rich QA) provide concrete and interpretable insights.\n3. The paper is well-written and logically structured, with precise definitions, clear experimental setups, and insightful discussions that effectively connect results back to the core research question."}, "weaknesses": {"value": "1. It would be helpful for the authors to clarify how the proposed future-aware masking strategy differs from existing approaches that also leverage bidirectional attention or cross-attention in vision-language or multimodal models. Specifically, how does this method compare conceptually and practically to (1) prior works that implement fully bidirectional attention over visual tokens[1], or (2) models that achieve modality alignment primarily through cross-attention mechanisms[2]? A discussion or empirical comparison would strengthen the contribution and highlight the unique aspects of the proposed approach.\n\n2. The proposed method primarily evaluates on interleaved text-image input sequences. However, in practice, many multimodal inputs follow different ordering patterns, such as text → image → text, or multiple consecutive text tokens followed by an image. It would be valuable for the authors to discuss how sensitive the future-aware masking strategies are to such variations in input order, and whether the same masking mechanisms can generalize effectively to these alternative sequence structures.\n\n3. The proposed \"light\" method uses kernel pooling to compress future attention and merges it into the first token based on the \"attention sink\" phenomenon. This mechanism is somewhat heuristic. The paper could be strengthened by exploring more sophisticated or learnable compression techniques to dynamically determine what context is most important to merge and where to merge it.\n\n\n[1] Kuo, Chia-Wen, et al. \"D-Attn: Decomposed Attention for Large Vision-and-Language Model.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n[2] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" International conference on machine learning. PMLR, 2023."}, "questions": {"value": "1. For the first weakness, could the authors further clarify the main novelty of this proposed work compared with other existing studies? \n2. In Table 4, the M^f variant, which combines both M^{v2v} and M^{v2t}, does not consistently achieve the best performance. Could the authors elaborate on why integrating both future visual and textual signals does not yield additive benefits? Some discussion on possible interference or redundancy effects would be valuable.\n3. Following Question 2, if M^f or M^f_merge does not consistently achieve the best performance across all tasks, how should one decide which variant to use during inference when the task type is unknown? Is there a possibility for the model to automatically adapt or select the most suitable masking strategy based on the input modality or context?\n4. It would be interesting to understand how the proposed future-aware masking mechanism generalizes to video-like or multi-frame visual inputs, where temporal order becomes more prominent. Would the same masking strategies still apply, or would additional temporal constraints need to be considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjjc1mLjSE", "forum": "DuDFytFC5Z", "replyto": "DuDFytFC5Z", "signatures": ["ICLR.cc/2026/Conference/Submission11764/Reviewer_TbBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11764/Reviewer_TbBr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104101353, "cdate": 1762104101353, "tmdate": 1762922788247, "mdate": 1762922788247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}