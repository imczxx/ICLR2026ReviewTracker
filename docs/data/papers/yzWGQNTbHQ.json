{"id": "yzWGQNTbHQ", "number": 11735, "cdate": 1758203382259, "mdate": 1759897557893, "content": {"title": "Next-Token Gradient Sensitivity Probing for LLM Hallucination Detection", "abstract": "Large language models (LLMs) demonstrate remarkable text generation capabilities but often produce hallucinations, e.g., factually inaccurate or unfaithful content, posing significant deployment risks. Detecting such hallucinations remains challenging due to their commonly subtle and localized nature. In this paper, we propose a gradient-based paradigm for hallucination detection by probing next-token prediction sensitivity. Specifically, we introduce a statistic called Next-token Gradient Sensitivity (NGS), which quantifies the first-order gradient of the maximum log-probability of the next token w.r.t. the current token's layer embedding, measuring local prediction fragility. We prove that its norm bounds the prediction confidence changes under small perturbations. Building on NGS, we develop NGS-based Hallucination Detection (NGS-HD), a method that reframes detection as a token-level distribution comparison task. NGS-HD computes the Maximum Mean Discrepancy (MMD) between each NGS of test tokens and NGS distributions from referenced truthful and hallucinated tokens, aggregating these MMDs into a global truthfulness score for detection. We further derive finite-sample separation bounds for this score, providing theoretical guarantees for its reliability. Extensive experiments demonstrate that NGS-HD outperforms baseline methods, offering a reliable and interpretable solution for detecting LLM hallucinations.", "tldr": "", "keywords": ["Large Language Models; Hallucination Detection; Maximum Mean Discrepancy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a1a3045b46c405c401a5ea6cda6862de9402e76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of hallucinations in content generated by large language models and proposes a novel detection method called NGS-HD, based on next-token gradient sensitivity. The core idea is that hallucinated tokens exhibit significant instability in next-token predictions under small perturbations in their embedding representations. The authors introduce the NGS metric, which measures the gradient of the maximum log-probability of the next token with respect to the current token embedding, thereby capturing the local sensitivity of the model’s predictions. Building on this, NGS-HD compares each test token against pre-constructed real/hallucinated reference distributions using MMD, and aggregates the results into a global authenticity score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is model-agnostic.\n\n2. The problem addressed is important.\n\n3. The paper is well-written and easy to read."}, "weaknesses": {"value": "1. Dependence on the reference set: NGS-HD requires pre-constructed NGS reference distributions for real and hallucinated tokens, which may be difficult to obtain in certain domains or low-resource settings. It is recommended to explore unsupervised or semi-supervised methods for constructing the reference set.\n\n2. Stability of gradient computation: Gradients can be affected by factors such as model training state and layer selection. Although the experiments show that middle-to-high layers perform better, the impact of gradient noise on detection stability is not thoroughly discussed.\n\n3. Limited capability for long-text processing: While token-level comparisons preserve fine-grained information, the method’s scalability to very long texts (e.g., document-level generation) has not been adequately validated. It is suggested to test on long-text benchmarks such as NarrativeQA.\n\n4. No distinction between hallucination types: The method does not explicitly differentiate between factual hallucinations and logical hallucinations, nor does it discuss differences in detection performance across these types.\n\n5. Insufficient visualization and interpretability: Although token-level score visualizations are provided, there is a lack of in-depth analysis explaining why certain tokens exhibit high sensitivity. Combining with attention maps or representation space analysis could enhance interpretability."}, "questions": {"value": "1. Dependence on the reference set: NGS-HD requires pre-constructed NGS reference distributions for real and hallucinated tokens, which may be difficult to obtain in certain domains or low-resource settings. It is recommended to explore unsupervised or semi-supervised methods for constructing the reference set.\n\n2. Stability of gradient computation: Gradients can be affected by factors such as model training state and layer selection. Although the experiments show that middle-to-high layers perform better, the impact of gradient noise on detection stability is not thoroughly discussed.\n\n3. Limited capability for long-text processing: While token-level comparisons preserve fine-grained information, the method’s scalability to very long texts (e.g., document-level generation) has not been adequately validated. It is suggested to test on long-text benchmarks such as NarrativeQA.\n\n4. No distinction between hallucination types: The method does not explicitly differentiate between factual hallucinations and logical hallucinations, nor does it discuss differences in detection performance across these types.\n\n5. Insufficient visualization and interpretability: Although token-level score visualizations are provided, there is a lack of in-depth analysis explaining why certain tokens exhibit high sensitivity. Combining with attention maps or representation space analysis could enhance interpretability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fDnhnuftS6", "forum": "yzWGQNTbHQ", "replyto": "yzWGQNTbHQ", "signatures": ["ICLR.cc/2026/Conference/Submission11735/Reviewer_UFQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11735/Reviewer_UFQk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760508963407, "cdate": 1760508963407, "tmdate": 1762922771100, "mdate": 1762922771100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NGS-HD, a gradient-based hallucination detection method that measures the local instability of LLM predictions by computing the gradient of the next-token's maximum log-probability with respect to the current token's layer embedding. The approach is motivated by empirical observations that hallucinated tokens show higher sensitivity to small embedding perturbations compared to truthful tokens. Instead of aggregating token representations into a single sequence-level feature, NGS-HD preserves fine-grained information by comparing the distribution of test token NGSs against pre-collected reference distributions of truthful and hallucinated tokens using Maximum Mean Discrepancy (MMD). The method achieves strong results, outperforming baselines, while requiring only a single backward pass and providing theoretical guarantees through finite-sample separation bounds."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- **(S1)** Empirical observation that hallucinated tokens show higher sensitivity to perturbations provides strong intuitive motivation for using NGS for hallucination detection.\n- **(S2)** The method requires only a single backward pass to compute all NGS vectors, making it practical to use and efficient.\n- **(S3)** The use of MMD to measure the discrepancy between hallucinated and truthful distributions is novel and interesting; it is a natural, neat way to circumvent the need to aggregate token representations into a single sequence-level feature, a common practice in other hallucination detection methods.\n- **(S3)** The method shows strong empirical performance and has a thorough ablation study. \n- **(S4)** The paper presents theoretical guarantees and analysis of the method."}, "weaknesses": {"value": "## Weaknesses\n- **(W1)** The authors describe the difference in discrepency between hallucinated and truthful tokens as \"significant\": \n  > As shown in Figures 2(a)–(b), both metrics are significantly larger for hallucinated tokens across noise levels, indicating higher sensitivity.\n\n  However, it seems that the difference is not statistically significant in Figure 2.\n- **(W2)** Limited evaluation scope - only QA tasks and small datasets. Additionally, the `3:1` train/test split is skewed.\n- **(W3)** The methods depend on labelled data to build reference sets. Training the MMD kernel on the reference sets makes this slightly more concerning, as we might risk overfitting. The paper does not provide cross-dataset generalization results.\n- **(W4)** The method averages per-token scores across all tokens (Eq. 4), which would dilute hallucination signals in long sequences. This is not tested in the paper.\n- **(W5), minor** The order of the arguments in the MMD function seems to change between Eq. 3 and Eq. 4. Consider changing “Remark” on line 302 to “Remarks” as this paragraph consists of a list of remarks."}, "questions": {"value": "The following questions detail weaknesses/potential misunderstandings I had while reading the paper. Given strong answers to the these questions, I'll be open to increasing my score.\n\n---\n\n- **(Q1)** It seems that the difference in discrepency between hallucinated and truthful tokens is not statistically significant in Figure 2. What is the reason for this?\n- **(Q2)** In Theorem 1, the authors assume that $\\nabla_{\\mathbf{e}} \\mathbf{s}(\\mathbf{e})$ is $L$-Lipschitz. Does this generally hold? For what $L$?\n- **(Q3)** The reference NGS distributions (for truthful and hallucinated tokens) depend on a choice of a truthful and hallucinated reference set. How sensitive is the method to the choice of these references? Does it generalize to domains other than the ones used in the reference sets? This is slightly more concerning as the MMD kernel is also trained, and we might risk overfitting.\n- **(Q4)** As far as I understand, the paper treats the NGS gradients $\\{\\mathbf{g}_t\\}_{t=1}^N$ as independent, e.g., for generating $P_\\mathrm{tru}$ we take gradients at different token positions. Is my understanding correct? If so, is this assumption justified? Can we model the correlation between gradients?\n- **(Q5)** In Theorem 2, what do the consntants in the $\\mathcal{O}$ notation depend on? Do they depend on the RKHS kernel?\n- **(Q6)** The paper claims the method can detect \"localized hallucinations within longer texts\" (lines 80-81). However, the final detection score averages per-token MMD values across all tokens. For long sequences with only a small subset of hallucinated tokens, this averaging would dilute the hallucination signal among many truthful tokens, potentially causing misclassification. How does the averaging-based aggregation remain robust to such “sparse” hallucinations? If possible, please provide empirical results on how the answer length impacts the performance of the method.\n- **(Q7)** The method requires computing NGS for each token and then calculating per-token MMD against all reference vectors. How does the method scale with answer length, particularly for long-context scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M6uGxjhmZb", "forum": "yzWGQNTbHQ", "replyto": "yzWGQNTbHQ", "signatures": ["ICLR.cc/2026/Conference/Submission11735/Reviewer_bbKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11735/Reviewer_bbKK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914379457, "cdate": 1761914379457, "tmdate": 1762922770581, "mdate": 1762922770581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a gradient-based sensitivity probing method for hallucination detection. It introduce a statistic named Next-token Gradient Sensitivity (NGS) that captures the sensitivity of the next-token prediction to minor perturbations in contextual representations of LLMs. By comparing the NGS distribution of test tokens and truthful/hallucinated tokens, the proposed method can finally produce a truthfulness score for hallucination detection. The experimental results demonstrate that the proposed method outperforms baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The concept of Next-token Gradient Sensitivity and the NGS-based hallucination detection approach are innovative. The theoretical derivations are rigorous and sound.\n\n2. Experiments on relevant benchmarks consistently demonstrate the superiority of the proposed method. Subsequent analyses further validate the soundness of this work."}, "weaknesses": {"value": "1. In practical deployment scenarios, hallucination detection methods do not have access to the referenced truthful and hallucinated tokens. How does the proposed method work under such scenario?\n\n2. Missing strong baseline methods for comparison, for example, \"Enhancing uncertainty-based hallucination detection with stronger focus\" by Zhang et al. and \"Knowledge-centric hallucination detection\" by Hu et al."}, "questions": {"value": "1. How about the evaluation resource cost of the proposed approach? A comparison between this work and other related works could improve the soundness of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tromWN3Gcg", "forum": "yzWGQNTbHQ", "replyto": "yzWGQNTbHQ", "signatures": ["ICLR.cc/2026/Conference/Submission11735/Reviewer_8sjc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11735/Reviewer_8sjc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935515577, "cdate": 1761935515577, "tmdate": 1762922770126, "mdate": 1762922770126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new gradient-sensitivity-based method — Next-token Gradient Sensitivity (NGS) — for detecting hallucinations in large language model (LLM) generations. The authors quantify the local instability of model predictions by computing the gradient of the maximum log-probability of the next token with respect to the current token’s representation. Based on this, they introduce NGS-HD, which detects hallucinations by comparing the distribution of NGS with reference distributions of truthful and hallucinated tokens using the Maximum Mean Discrepancy (MMD) metric."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tUnlike previous methods based on whole-sequence representations or feature aggregation, NGS-HD performs distributional comparisons at the token level.\n2.\tThe proposed NGS metric is centered on gradient sensitivity, fundamentally revealing the relationship between LLM hallucinations and prediction instability, and further providing theoretical upper bounds (Theorems 1 and 2)."}, "weaknesses": {"value": "1.\tThe validity of the methodological assumptions requires further discussion: The computation of NGS relies on the assumption of the “maximum log-probability token” (argmax token). However, in diverse generation settings (e.g., sampling or temperature-controlled decoding), non-maximum tokens may also play an important role. The applicability of this assumption in open-ended generation scenarios should be further discussed.\n2.\tLack of robustness analysis regarding gradient noise sensitivity: Although the authors emphasize the efficiency of single backward propagation, gradient computation can be affected by floating-point errors and noise. It would be valuable to include robustness analyses under varying noise levels or batch sizes.\n3.\tExperimental scope could be further expanded: Incorporating gray-box experiments on closed-source models such as GPT-4 or Gemma-2 would help verify the model-agnostic nature of the method. Since current experiments are limited to QA datasets, it is recommended to test the method on open-ended text generation tasks (e.g., summarization or dialogue) to assess its generalizability.\n4.\tThe MMD design is somewhat complex and relies on reference datasets: While the theoretical formulation of the method is elegant, its practical deployment requires pre-collected reference distributions of truthful and hallucinated tokens, which may limit usability. It is suggested to explore adaptive or unsupervised strategies for constructing reference sets.\n5.\tLack of error visualization and case analysis in comparisons: Although the quantitative results are comprehensive, the paper lacks qualitative visualization of hallucination cases (e.g., illustrating the correspondence between gradient norm maps and hallucinated regions), which would enhance interpretability and insight."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZJ2kvFBoLh", "forum": "yzWGQNTbHQ", "replyto": "yzWGQNTbHQ", "signatures": ["ICLR.cc/2026/Conference/Submission11735/Reviewer_U7zH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11735/Reviewer_U7zH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991158169, "cdate": 1761991158169, "tmdate": 1762922769705, "mdate": 1762922769705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}