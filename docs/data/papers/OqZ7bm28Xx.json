{"id": "OqZ7bm28Xx", "number": 3999, "cdate": 1757581716968, "mdate": 1759898059094, "content": {"title": "SpatialViz-Bench: A Cognitively-Grounded Benchmark for Diagnosing Spatial Visualization in MLLMs", "abstract": "Humans can imagine and manipulate visual images mentally, a capability known as \\textit{spatial visualization}. \nWhile many multi-modal benchmarks assess reasoning on visible visual information, the ability to infer unseen relationships through spatial visualization remains insufficiently evaluated as a spatial skill. \nThis reliance on publicly sourced problems from IQ tests or math competitions risks data contamination and compromises assessment reliability.\nTo this end, we introduce \\textbf{\\textit{SpatialViz-Bench}}, a comprehensive multi-modal benchmark for \\textit{spatial visualization} with \\emph{12} tasks across \\emph{4} sub-abilities, comprising \\emph{1,180} programmatically generated problems, a scalable framework that allows for expansion to ensure fair and continuously reliable evaluations. \nOur evaluation of \\emph{27} Multi-modal Large Language Models (MLLMs) reveals wide performance variations, demonstrates the benchmark's strong discriminative power, and uncovers counter-intuitive findings: Chain-of-Thought (CoT) prompting paradoxically degrades accuracy on open-source models.\nThrough statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs exhibit deficiencies in \\textit{spatial visualization} tasks, thereby addressing a significant lacuna in the field.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Spatial Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc1e82e835608c8a82b802a162724701330dab69.pdf", "supplementary_material": "/attachment/aa666c32e28e3fe1bb3eb097fb25d75813ae4a4d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SpatialViz-Bench, a new benchmark for diagnosing spatial visualization abilities in MLLMs. The authors address limitations and potential data contamination in existing benchmarks by creating a scalable framework that programmatically generates 1,180 novel problems across 12 distinct tasks. Their evaluation of 27 MLLMs reveals wide performance variations and shows that even state-of-the-art models struggle with these tasks. The diagnostic analysis suggests that these failures stem from fundamental deficits in perception and spatial transformation rather than high-level reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of a synthetic dataset is a significant strength, as it effectively mitigates the risk of data contamination and ensures comprehensive coverage across a wide range of tasks.\n2. The benchmark demonstrates strong discriminative power, effectively highlighting performance differences across a broad spectrum of MLLMs.\n3. The statistical error analysis, which breaks down failures into fundamental perceptual and spatial transformation deficits, provides valuable insights into the core limitations of current models."}, "weaknesses": {"value": "1. Figure 1 is dense and difficult to parse due to information overload. The upper-right legend, in particular, is unclear: the meaning of the \"error mark\" and the reference to \"existing evaluation\" are not immediately evident.\n2. The Related Work section requires revision. It currently reads more like an extension of the Introduction rather than an objective survey of the field. The tone is heavily opinion-based, with most existing work being cited primarily to emphasize their limitations. Furthermore, the section appears to neglect several of the most recent or concurrent benchmarks focused on spatial reasoning.[1-6]. \n\n\nReferences:\n\n[1] Ramakrishnan, Santhosh Kumar, et al. \"Does Spatial Cognition Emerge in Frontier Models?.\" arXiv preprint arXiv:2410.06468 (2024).\n\n[2] Stogiannidis, Ilias, Steven McDonagh, and Sotirios A. Tsaftaris. \"Mind the gap: Benchmarking spatial reasoning in vision-language models.\" arXiv preprint arXiv:2503.19707 (2025).\n\n[3] Xu, Wenrui, et al. \"Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics.\" arXiv preprint arXiv:2502.11859 (2025).\n\n[4] Li, Chengzu, et al. \"11plus-bench: Demystifying multimodal llm spatial reasoning with cognitive-inspired analysis.\" arXiv preprint arXiv:2508.20068 (2025).\n\n[5] Tang, Kexian, et al. \"LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?.\" arXiv preprint arXiv:2503.19990 (2025).\n\n[6] Yin, Baiqiao, et al. \"Spatial Mental Modeling from Limited Views.\" arXiv preprint arXiv:2506.21458 (2025)."}, "questions": {"value": "1. The methodology for labeling error categories is unclear. Are these categories mutually exclusive (uni-label), or can a single failure case be assigned multiple labels (multi-label)? If it is a uni-label system, clarification is needed on how the \"critical error\" is determined or prioritized over other potential errors in a single instance.\n\n2. Regarding the counter-intuitive finding that CoT prompting degrades performance on open-source models: have the authors investigated whether this could be attributed to issues such as parsing failures in the CoT responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iBErmrzXIz", "forum": "OqZ7bm28Xx", "replyto": "OqZ7bm28Xx", "signatures": ["ICLR.cc/2026/Conference/Submission3999/Reviewer_nVip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3999/Reviewer_nVip"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396166500, "cdate": 1761396166500, "tmdate": 1762917130019, "mdate": 1762917130019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SpatialViz-Bench, a cognitively-grounded, programmatically generated benchmark designed to evaluate spatial visualization abilities in Multi-modal Large Language Models (MLLMs). Covering 12 tasks across four sub-abilities (mental rotation, mental folding, visual penetration, and mental animation), the benchmark comprises 1,180 problems and enables scalable, contamination-resistant evaluation. Experiments on 27 MLLMs reveal significant performance gaps with humans and highlight that current models struggle primarily with perceptual and spatial transformation tasks, not high-level reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty & Relevance: Proposes a comprehensive, cognitively-motivated benchmark specifically targeting spatial visualization in MLLMs, addressing a clear gap in current evaluation practices. The \"gap\" identified here - whether LLMs can infer unseen relationships through\nspatial visualization - is a novel and crucial problem, not intensively researched\n\n- Methodological Rigor: The eye for detail in the error analysis as well as ensuring scalability are good grounds to cover in a benchmark paper. The idea of randomizing generations and dynamically updating the test bank is a valid thought for benchmark's robustness.\n\n- Insightful Analysis: Provides detailed error analysis and ablation studies, revealing that model failures are rooted in perceptual and transformation deficits rather than logic. The fact that CoT degrades model performance for open-source artefacts is an interesting call out and calls for further investigation"}, "weaknesses": {"value": "- CoT Degradation Analysis can be fleshed out more : \n   - This is a crucial and rather counter-intuitive observation made by the paper. It would be good to provide more information on the details of the framework for this specific study to establish trust on these results. \n   - The ablation study is limited to accuracy comparisons with and without CoT, and the explanation for the degradation is largely speculative (e.g., CoT text may distract models not fine-tuned for long-form reasoning). There is a need for more in-depth, systematic root cause analysis—such as controlled experiments isolating prompt length, reasoning style, or model architecture effects.\n   - The study does not explore whether prompt engineering, alternative CoT formats, or model-specific tuning could mitigate the observed degradation. Need to ablate on the CoT settings to ensure this is not a framework bias\n   - The paper does not provide error breakdowns or qualitative examples specifically illustrating how CoT leads to errors in open-source models.\n\n- Data Contamination solutions needs more validation : the idea of dynamically updating the test bank with randomized generations (while maintaining standardized templates for fair comparisons) is a sound hypothesis to mitigate data contamination issues, but quantification/testing to confirm the validity of this hypothesis is lacking\n    - There is a need for basic adversarial or retrieval-based analysis to test if models are “memorizing” or “recalling” similar problems from pretraining.\n    - The effectiveness of dynamic test bank updates is asserted, but not validated with concrete evidence or contamination metrics. There is no empirical study or audit quantifying the risk or presence of contamination in current models."}, "questions": {"value": "As covered in the weaknesses, there is need for some validations on the claims and observations made"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C7bnzXVB2l", "forum": "OqZ7bm28Xx", "replyto": "OqZ7bm28Xx", "signatures": ["ICLR.cc/2026/Conference/Submission3999/Reviewer_9vwG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3999/Reviewer_9vwG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505976498, "cdate": 1761505976498, "tmdate": 1762917129779, "mdate": 1762917129779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpatialViz-Bench, a new benchmark designed to evaluate the critical but under-tested skill of spatial visualization in Multi-modal Large Language Models (MLLMs). The authors argue that existing benchmarks fail to adequately assess an MLLM's ability to mentally manipulate unseen objects (e.g., mentally folding a net into a cube) and often rely on publicly sourced problems, risking data contamination.\nTo address this, SpatialViz-Bench offers a comprehensive suite of 1,180 problems across 12 tasks, which are programmatically generated to ensure fairness, scalability, and resistance to data leakage. The tasks are grounded in cognitive science, covering four core abilities: mental rotation, mental folding, visual penetration, and mental animation. Through an extensive evaluation of 27 MLLMs, the paper reveals several key findings: 1. A significant performance gap exists between all current models, including state-of-the-art systems like Gemini-2.5-pro, and a human baseline, highlighting the difficulty of the tasks. 2. In a counter-intuitive discovery, Chain-of-Thought (CoT) prompting paradoxically degrades the accuracy of many open-source models. 3. Error analysis indicates that model failures stem primarily from fundamental deficits in visual perception and spatial transformation, rather than high-level logical reasoning.\nUltimately, SpatialViz-Bench provides a robust, diagnostic tool for the research community, identifying a crucial weakness in modern MLLMs and guiding future efforts toward improving their core visuospatial intelligence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Addresses a Critical and Under-Evaluated Research Gap in MLLMs\nThe paper's strength is its focus on **spatial visualization**, a specific and advanced cognitive skill that has been largely overlooked by existing MLLM benchmarks. While many benchmarks test reasoning on *visible* information (e.g., \"What color is the car on the left?\"), SpatialViz-Bench is one of the first to systematically evaluate the ability to reason about *unseen* spatial relationships through mental manipulation. This is a crucial capability for real-world applications in fields like engineering (CAD models), medicine (interpreting 3D scans), and robotics (planning object manipulation), making this benchmark both timely and highly relevant.\n\n\n### 2. Diagnostic Analysis Beyond Simple Accuracy Metrics\nThe statistical error analysis (Figure 4) and the qualitative case studies (Figure 5) provide deeper insights into the models' failure modes. The conclusion that **\"Perceptual and Spatial Transformation errors dominate failures\"** is an important finding. It suggests that the primary bottleneck for MLLMs is not high-level logic or instruction following, but rather fundamental deficits in visual perception and mental manipulation. This provides a clear and actionable direction for future research and model development.\n\n\n\n\n### 3. Methodologically Robust and Trustworthy Benchmark Construction\nThe authors have designed a benchmark that directly confronts the most pressing challenges in modern AI evaluation, particularly data contamination and reliability. The core strength here is the **programmatic generation** of 11 out of the 12 tasks. This approach provides several key advantages:\n*   **Mitigates Data Contamination:** By generating novel, synthetic problems, the benchmark ensures that models are tested on their reasoning abilities rather than their ability to recall answers to similar problems scraped from the internet (e.g., from public IQ tests or math competitions).\n*   **Ensures Scalability and Longevity:** The programmatic framework allows for the continuous generation of new test cases. This creates a \"dynamic test bank\" that can be updated over time, making the benchmark a sustainable and long-lasting resource that is resilient to future training data leakage.\n*   **Enables Controlled Difficulty and Systematic Analysis:** The generation process allows for precise control over task difficulty (e.g., by increasing the number of folds or the complexity of an assembly). It also enables the systematic creation of distractor options based on common error types, which makes the evaluation more diagnostic."}, "weaknesses": {"value": "**1. The \"Cognitively-Grounded\" Claim is Overstated**\n\nThe paper's implementation of its central claim to be \"cognitively-grounded\" is shallow. True cognitive grounding goes beyond simply adopting high-level task categories from historical psychology literature; it requires integrating principles of human cognition into task design, particularly concerning difficulty and error analysis.\n\n*   **Absence of Parametric Difficulty Scaling:** The paper's approach to scaling difficulty, as outlined in **Table 1 (lines 218-236)**, relies on intuitive and qualitative heuristics such as \"**Larger assemblies,**\" \"**More folds,**\" or \"**More system modules.**\" This approach fails to incorporate research on spatial reasoning. For example, a deeply grounded mental rotation task would parametrically vary the angle of rotation, as human reaction time and error rates are known to increase linearly with this variable. Similarly, a paper folding task's difficulty could be systematically controlled by the number of folds or the complexity of the resulting hole symmetry. By not grounding its difficulty levels in these quantifiable cognitive-load metrics, the benchmark cannot guarantee that \"Level 1\" is consistently and measurably more difficult than \"Level 0\" in a way that reflects cognitive processes.\n\n*   **Implication:** This lack of parametric control weakens the benchmark's diagnostic power. We cannot be certain that a model's performance drop from Level 0 to Level 1 is due to a failure in a specific cognitive step or simply an inability to handle more visual \"clutter.\" The benchmark measures performance on harder problems but doesn't provide the fine-grained control needed to diagnose *why* they are harder from a cognitive standpoint.\n\n\n\n**2. Issue with manually designed \"Mechanical System\" task**\n\nOne of the paper's arguments is its use of programmatic generation to create a scalable, dynamic, and contamination-resistant benchmark.  However, this rigor is abandoned for one of the 12 tasks, creating inconsistency.\n\n*   **Contradiction of Core Philosophy:** The authors state in **Section 3.3 (lines 265-267)** that the \"**Mechanical System task was manually designed**\" using \"**representative public simulations.**\" This contradicts their own critique of other benchmarks that \"**rely heavily on web-sourced problems, risking data leakage**\" (**line 096**). This task is now vulnerable to the exact problems the rest of the benchmark was designed to solve. The \"public simulations\" may very well be part of the pre-training corpora of the models being evaluated, turning a test of reasoning into a test of memorization.\n\n*   **Implication:** This inconsistency compromises the internal validity of the benchmark's results. Performance on the Mechanical System task cannot be reliably compared to the other 11 tasks. As seen in **Table 2 (line 328)**, many open-source models score surprisingly high on this task relative to their poor performance on other 3D tasks like Cube Unfolding or 3D Rotation. Is this because they possess a distinct \"intuitive physics\" capability, as the authors suggest (**lines 2534-2537**), or is it because they have been exposed to the source material during training? The manual design of this task makes it difficult to disentangle these possibilities, undermining the clarity of the evaluation.\n\n\n\n\n**3. The \"Chain-of-Thought Paradox\"**\n\nThe paper's finding that CoT prompting degrades performance on open-source models is a thought provoking conclusion. However, the analysis supporting this claim is not robust enough to rule out simpler, confounding explanations.\n\n*   **The Flaw of a One-Size-Fits-All Prompt:** The entire CoT analysis rests on a single, standardized prompt template applied to all models, detailed in **Appendix E.2 (lines 2347-2351)**. This is a methodological flaw. Different MLLMs are instruction-tuned on vast and varied datasets, leading to high sensitivity to prompt phrasing, structure, and keywords. A prompt that works well for one model family may be confusing or suboptimal for another. The observed performance degradation may not reflect a fundamental cognitive \"interference\" but rather a failure of prompt engineering—the models may simply be failing to adhere to an unfamiliar instruction format. A robust study should  include a prompt sensitivity analysis, testing several CoT variations to ensure the effect was not an artifact of their chosen template.\n\n*   **Selection Bias in the Comparison Group:** The paper introduces a selection bias that skews the comparison. The authors explicitly state they \"**excluded models... unable to adhere to the format (e.g., InternVL3-2B)**\" from the non-CoT evaluation (**lines 368-370**). This means the group of open-source models in the non-CoT test is a pre-filtered subset of the most capable or compliant models. The weaker models that fail even simple formatting are removed. This inflates the baseline non-CoT performance for the open-source category, making the subsequent drop when CoT is enabled appear larger than it may actually be.\n\n*   **Speculative Explanation:** The authors hypothesize that CoT \"**interferes with their intrinsic visual judgment**\" (**line 395**). The paper provides no qualitative analysis of the generated CoT content itself. A rigorous analysis would involve manually inspecting the reasoning steps to see *how* they fail. Do the models make perceptual errors in their text (e.g., \"the orange face is next to the green face\" when it is not)? Or are the errors logical, showing a flawed transformation process? The error analysis in **Figure 4 (line 390)** categorizes final outcomes but does not correlate specific error types with the use (or non-use) of CoT. Without this evidence, alternative hypotheses—such as the models simply generating irrelevant, distracting text—are equally plausible.\n\n\n\n**4. A Foundational Claim Lacking Empirical Verification: \"Vision-Dependence\"**\n\nThe paper rightly positions its benchmark as highly \"vision-dependent,\" stating that \"**textual input alone is insufficient**\" for problem-solving (**line 309**). While intuitively true, this is a foundational, empirical claim that is asserted rather than proven. The necessity of the visual input could have been easily and powerfully demonstrated with a simple control experiment. By feeding the text-only components of the prompts to a powerful text-only LLM (like GPT-4) and recording its performance, the authors could have provided definitive quantitative evidence for their claim. A resulting accuracy at or near the random-chance baseline (**25.08%**, noted in **Table 2**) would have formally established the visual modality as indispensable for this benchmark, thereby strengthening the paper's core motivation and contribution."}, "questions": {"value": "*   **The CoT Prompt:** The paper makes a strong claim about CoT, but the specifics of the prompt used are relegated to an appendix.\n    *   Can you give examples of the CoT prompt?\n    *   Why was a single, standardized prompt chosen for all models? It is well-known that prompt effectiveness is highly model-dependent. The observed negative effect might be an artifact of a suboptimal prompt for certain models, not an inherent issue with CoT itself.\n\n\n*   **Use of External Datasets:** For the \"Three-View Projection\" task, Table 1 mentions \"Real engineering parts (DeepCAD).\" It's unclear if DeepCAD is another procedurally generated source or a fixed dataset. If it's a fixed dataset, how does this align with the goal of creating a dynamically updatable benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXBEJQ364J", "forum": "OqZ7bm28Xx", "replyto": "OqZ7bm28Xx", "signatures": ["ICLR.cc/2026/Conference/Submission3999/Reviewer_GoVB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3999/Reviewer_GoVB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917633749, "cdate": 1761917633749, "tmdate": 1762917129473, "mdate": 1762917129473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpatialViz-Bench, a benchmark that isolates and evaluates spatial visualization in multimodal LLMs across 4 sub-abilities mental rotation, mental folding, visual penetration and mental animation. This is introduced using 12 tasks and 1,180 programmatically generated problems with controllable difficulty. Eleven tasks are created with a Python + FreeCAD pipeline to enable scalable expansion and minimize contamination; one task (mechanical systems) is manually curated. The authors evaluate 27 MLLMs (8 closed-source, 19 open-source) in zero-shot settings with both Direct Answer and CoT prompting. Results show a large gap to humans and reveal an interesting finding that CoT hurts many open-source models, widening the closed vs. open-source gap. The diagnostic error analysis finds failures dominated by Perceptual and Spatial Transformation errors, suggesting limits in visuo-spatial processing rather than high-level logic. The benchmark aims to provide a stable, expandable, and contamination-resistant testbed to improve progress on visuo-spatial capabilties in MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Centers spatial visualization (not just perception or generic “spatial reasoning”) and grounds it in cognitive science, decomposing ability into 4 sub-abilities with 12 targeted tasks.\n* Programmatic generation (Python + FreeCAD) for 11/12 tasks yields controllable difficulty, systematic distractors, and an expandable test bank to mitigate contamination; standardized templates reduce instruction confounds.\n* Broad coverage (rotation, folding, penetration, animation) plus error taxonomy (perceptual, transformation, memorization, instruction following, methodological, calculation/reasoning) surfaces where models fail rather than just how much.\n* Clear hierarchy among models with large human–model gap and near-random performance on core 3D tasks for many models; CoT ablation highlights that enforced reasoning text can degrade open-source model accuracy."}, "weaknesses": {"value": "* Lacks uncertainty estimates (CIs/SEs) and significance testing across models/tasks.\n* The “CoT hurts” result may be prompt and format-sensitive. More prompts, temperature sweeps, and output-length controls are needed to ensure the effect isn’t an artifact of prompt adherence or extraction errors.\n* Pure multiple-choice framing may enable elimination strategies viz. no free-form diagram/pose prediction, 3D pose estimation, or procedural rollouts that might better stress genuine mental simulation.\n* There is limited evidence that gains on SpatialViz-Bench transfer to robotics, CAD, surgical planning, or navigation\n* The error analysis is “primarily manual” with model assistance; inter-annotator agreement (e.g., Cohen’s κ) is not reported."}, "questions": {"value": "1. How sensitive are CoT results to prompt variants, answer extraction rules, and output-length limits? Reporting accuracy across a prompt suite and per-model variance bars would provide more insights.\n2. Is possible to also publish 95% CIs per task/model and paired significance tests across prompts (CoT vs. Direct), and across difficulty levels to substantiate the key conclusions.\n3. Do improvements on SpatialViz-Bench correlate with performance on downstream visuospatial tasks or questions, VQA on CAD/DeepCAD?\n4. Was there any ablations on image resolution, number of views, and rendering style (lighting, line thickness) to determine sensitivity to low-level vision vs. spatial reasoning.\n5. Was there an analysis on controlled difficulty and its relation to performance of the models to identify where does it break?\n6. For the six error categories, is it possible to report inter-annotator agreement (κ)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MpeWjGTC6b", "forum": "OqZ7bm28Xx", "replyto": "OqZ7bm28Xx", "signatures": ["ICLR.cc/2026/Conference/Submission3999/Reviewer_XXzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3999/Reviewer_XXzx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972415686, "cdate": 1761972415686, "tmdate": 1762917129265, "mdate": 1762917129265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}