{"id": "hIBMCDOwMI", "number": 13215, "cdate": 1758215155996, "mdate": 1763524249956, "content": {"title": "Sample-Efficient Distributionally Robust Multi-Agent Reinforcement Learning via Online Interaction", "abstract": "Well-trained multi-agent systems can fail when deployed in real-world environments due to model mismatches between the training and deployment environments, caused by environment uncertainties including noise or adversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance system resilience by optimizing for worst-case performance over a defined set of environmental uncertainties. However, current methods are limited by their dependence on simulators or large offline datasets, which are often unavailable. This paper pioneers the study of online learning in DRMGs, where agents learn directly from environmental interactions without prior data. We introduce the Multiplayer Optimistic Robust Nash Value Iteration (MORNAVI) algorithm and provide the first provable guarantees for this setting. Our theoretical analysis demonstrates that the algorithm achieves low regret and efficiently finds the optimal robust policy for uncertainty sets measured by Total Variation divergence and Kullback-Leibler divergence. These results establish a new, practical path toward developing truly robust multi-agent systems.", "tldr": "", "keywords": ["distributionally robust", "multi-agent", "markov game"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f4af01dd46ae7ed1af55b3c9b342a697602544f.pdf", "supplementary_material": "/attachment/7807e09b134e13d646a8f1bd5d864c5da1be5a47.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose a systematic and principled approach for online learning in distributionally robust Markov Games with environment uncertainty. This is done by first revealing the hardness in online DRMGs,  then proposing an online robust MARL algorithm, Multiplayer Optimistic Robust Nash Value Iteration (MORNAVI), which offers the first provable robust guarantee in the setting. The theoretical analysis demonstrates MORNAVI achieves low regret, finds the optimal robust policy and achieves high sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall the paper is well-written, a little hard to understand due to the nature of a theoretical paper but clear enough to understand given the context. The theories given are rigorious and the threat model, algorithm and proofs seems correct, but I didn't check all the proofs.\n\n1. The hardness of online DRMGs provides a clear motivation of the problem, and the contructed examples gives an intuitive thought experiment for future researchers to design their algorithms.\n\n2. The derivations on sample complexity provides a rigorious theoretical foundation for establishing the regret of existing algorithms, and shows the result achieves complexity comparable with existing works, despite operating in online settings instead of easier generative and offline settings. The complexity bound is quite tight."}, "weaknesses": {"value": "1. The main weakness I belive is that current online MARL parameterized by neural networks still do not have high enough sample efficiency to support this work that learns purely online, without any offline datasets available. However, accepting this paper would greatly benefit future research on this topic.\n\n2. The paper is purely theoretical and might be designed for tabular case, instead of existing MARL using function approximations. While this is beneficial for theoretical analysis, it remains unknown how to adapt this algorithm to modern environments, such as MPE, SMAC and Multi-Agent Mujoco. This is clearly reflected in stage 1, Nominal Transition Estimation and stage 2, EQUILIBRIUM subroutine. Estimating the distribution of future states can be inaccurate in modern environments with large state space using simple empirical average, and computing the equilibrium such as CE or CCE can be hard in tasks such as SMAC, or continuous control.\n\n3. What is the algorithmic differencce between MORNAVI and existing distributional robust approach? I am not very familiar with distributionally robust MARL literature, so I assume the main difference lies in Eqn. 5 and 6, since other parts of the algorithm is not too different from existing robust MARL approach.\n\nTo solve these problems, can you provide suggestions on empirical algorithms that leverage the advantage of MORNAVI, but use  function approximations? this would greatly strengthen this paper. However, I give a rating of 6 based on the current version."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SYkGimvMf", "forum": "hIBMCDOwMI", "replyto": "hIBMCDOwMI", "signatures": ["ICLR.cc/2026/Conference/Submission13215/Reviewer_bkNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13215/Reviewer_bkNi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600491507, "cdate": 1760600491507, "tmdate": 1762923906578, "mdate": 1762923906578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a systematic and principled approach for online learning in distributionally robust Markov Games with environment uncertainty. This is done by first revealing the hardness in online DRMGs,  then proposing an online robust MARL algorithm, Multiplayer Optimistic Robust Nash Value Iteration (MORNAVI), which offers the first provable robust guarantee in the setting. The theoretical analysis demonstrates MORNAVI achieves low regret, finds the optimal robust policy and achieves high sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall the paper is well-written, a little hard to understand due to the nature of a theoretical paper but clear enough to understand given the context. The theories given are rigorious and the threat model, algorithm and proofs seems correct, but I didn't check all the proofs.\n\n1. The hardness of online DRMGs provides a clear motivation of the problem, and the contructed examples gives an intuitive thought experiment for future researchers to design their algorithms.\n\n2. The derivations on sample complexity provides a rigorious theoretical foundation for establishing the regret of existing algorithms, and shows the result achieves complexity comparable with existing works, despite operating in online settings instead of easier generative and offline settings. The complexity bound is quite tight."}, "weaknesses": {"value": "1. The main weakness I belive is that current online MARL parameterized by neural networks still do not have high enough sample efficiency to support this work that learns purely online, without any offline datasets available. However, accepting this paper would greatly benefit future research on this topic.\n\n2. The paper is purely theoretical and might be designed for tabular case, instead of existing MARL using function approximations. While this is beneficial for theoretical analysis, it remains unknown how to adapt this algorithm to modern environments, such as MPE, SMAC and Multi-Agent Mujoco. This is clearly reflected in stage 1, Nominal Transition Estimation and stage 2, EQUILIBRIUM subroutine. Estimating the distribution of future states can be inaccurate in modern environments with large state space using simple empirical average, and computing the equilibrium such as CE or CCE can be hard in tasks such as SMAC, or continuous control.\n\n3. What is the algorithmic differencce between MORNAVI and existing distributional robust approach? I am not very familiar with distributionally robust MARL literature, so I assume the main difference lies in Eqn. 5 and 6, since other parts of the algorithm is not too different from existing robust MARL approach.\n\nTo solve these problems, can you provide suggestions on empirical algorithms that leverage the advantage of MORNAVI, but use  function approximations? this would greatly strengthen this paper. However, I give a rating of 6 based on the current version."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SYkGimvMf", "forum": "hIBMCDOwMI", "replyto": "hIBMCDOwMI", "signatures": ["ICLR.cc/2026/Conference/Submission13215/Reviewer_bkNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13215/Reviewer_bkNi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600491507, "cdate": 1760600491507, "tmdate": 1763531284265, "mdate": 1763531284265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles online learning in Distributionally Robust Markov Games (DRMGs) and proposes f-MORNAVI‚Äîa model-based algorithm that learns empirical transition models, constructs optimistic and pessimistic value estimates under f-divergence uncertainty, and computes equilibria (Nash/CCE/CE) at each step. The authors prove hardness results, showing that support-shift uncertainty (e.g., TV sets) yields linear regret. They also derive upper bounds for total variation and KL uncertainty, establishing near-matching sample-efficient guarantees. The work provides the first theoretical framework for online DRMGs with rigorous proofs and detailed analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper supplies both lower bounds (separations) and matching upper bounds (for TV and KL) together. The proof structure is standard but carefully adapted to the robust multi-agent setting. \n- The paper identifies and formalizes the online DRMG problem (vs. prior offline/generative-model work) and isolates two distinct hardness phenomena (support shift and curse-of-multi-agency)."}, "weaknesses": {"value": "- All upper bounds and the lower bounds include the product of agent action counts. This is a severe scalability concern (exponential in number of agents if each has many actions). The paper acknowledges this as an open question but does not give practical guidance or alleviate it. This limits real-world applicability.\n- The algorithm requires solving an equilibrium (Nash/CE/CCE) in the stagewise matrix game for each state and timestep. In practice large action space and many states make these subroutines expensive. The paper needs to discuss practical computational approaches and complexity per episode.\n- The KL regret/sample complexity contains an $exp(O(H^2))$ term. For long horizons this is prohibitive; more discussion of whether this dependence is inherent (and how it scales in practice) is needed."}, "questions": {"value": "- Lack of empirical validation despite practical motivation: The paper is motivated by bridging the sim-to-real gap in MARL, yet includes no experiments or case studies. This omission significantly weakens the claim that the approach improves practical robustness or connects theory and real-world performance. Would you be able to empirically show your proposed method compared with previous work in several simulation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "udfwFz6E82", "forum": "hIBMCDOwMI", "replyto": "hIBMCDOwMI", "signatures": ["ICLR.cc/2026/Conference/Submission13215/Reviewer_cbht"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13215/Reviewer_cbht"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525942172, "cdate": 1761525942172, "tmdate": 1762923906075, "mdate": 1762923906075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces f-MORNAVI, a model-based online algorithm for distributionally robust Markov games (DRMGs). The method estimates the dynamics from interaction with the environment and performs planning under uncertainty sets defined by f-divergences. It incorporates an equilibrium solver and provides regret bounds for online DRMGs, along with lower bounds that highlight inherent hardness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The theoretical development is clear, with high-probability regret bounds for TV and KL uncertainty sets and sample-complexity corollaries to equilibrium under the NE, CE, and CCE.\n\n+ The algorithmic design well-designed and motivated. It separates model estimation, robust optimistic planning with divergence-aware bonuses, and an equilibrium, and the mathematical treatment of support shift is interesting and well-written."}, "weaknesses": {"value": "-- The paper lacks empirical validation. Although the theoretical results looks sound, there is a lack of experimental evidence that the proposed online method outperforms prior approaches or that the constants or overheads are practical. \n\n-- The practical comparison to generative or offline baselines and to out-of-distribution scenarios is unclear. It would help to quantify how the robust online procedure fares against strong non-robust or offline/generative methods on OOD tasks.\n\n-- While the theory derives sample-complexity corollaries for reaching approximate equilibrium, the lack of experiments makes it hard to assess the real-world gap between these bounds."}, "questions": {"value": "see the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5KmtQfk4uF", "forum": "hIBMCDOwMI", "replyto": "hIBMCDOwMI", "signatures": ["ICLR.cc/2026/Conference/Submission13215/Reviewer_axWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13215/Reviewer_axWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857614368, "cdate": 1761857614368, "tmdate": 1762923904886, "mdate": 1762923904886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles online distributionally robust Markov games (DRMGs) without a simulator or offline dataset and proposes f-MORNAVI, an optimistic-robust, model-based meta-algorithm for general f-divergence uncertainty sets (with concrete TV and KL instantiations). It proves (i) hardness results‚Äîlinear regret with support shift and ‚àöK regret without shift but still scaling with the joint action size‚Äîand (ii) first regret bounds for online DRMGs in the TV/KL cases, plus corresponding sample-complexity bounds. Algorithmically, f-MORNAVI estimates a nominal kernel online, plans via robust Bellman operators augmented with UCB-style bonuses tailored to the uncertainty geometry, and computes an equilibrium (NE/CE/CCE) at each step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n\nThe paper addresses a relatively unexplored problem: online distributionally robust Markov games (DRMGs) without access to simulators or offline data. While the formulation itself extends concepts familiar from single-agent robust RL and generative/offline DRMG studies, applying them to the online multi-agent regime is a natural but nontrivial step. The proposed MORNAVI framework‚Äîintegrating optimism for exploration with robustness against uncertainty‚Äîis conceptually consistent with prior work on optimistic robust RL, though its multi-agent adaptation and generalization to \nùëì\nf-divergence sets give it modest originality. The contribution is incremental rather than groundbreaking but helps close an existing theoretical gap.\n\nQuality\n\nThe theoretical development is careful and technically competent. The paper provides hardness results, upper bounds, and sample-complexity analysis that align with known results in related literature. The regret guarantees for both Total Variation and KL uncertainty sets are plausible extensions of existing robust RL theory. However, the analysis follows established proof techniques (empirical Bernstein inequalities, Bellman contraction arguments, etc.) rather than introducing fundamentally new analytical tools. Some aspects‚Äîsuch as the correctness of the correlated equilibrium definitions and the reliance on specific assumptions like rectangularity and failure states‚Äîshould be clarified or corrected to fully validate the results. Overall, the quality is solid but not exceptional.\n\nClarity\n\nThe paper‚Äôs structure is standard and logical, with a clear flow from problem motivation to algorithm design and theoretical results. Nevertheless, the writing is mathematically dense, and several sections would benefit from higher-level intuition to guide readers through technical derivations. Certain notational inconsistencies (e.g., repeated theorem numbering, unclear equilibrium notation) reduce readability. While the main ideas can be followed by experts in the field, the exposition is unlikely to be easily accessible to a broader ICLR audience without additional clarifications or illustrative examples.\n\nSignificance\n\nThe significance of the paper lies primarily in its problem setting rather than in the methodological innovation. Establishing regret bounds for online DRMGs is useful for the theory of robust multi-agent learning, but practical impact remains limited given the heavy dependence on joint action space size and the lack of empirical validation. The work reinforces the difficulty of scaling robustness in multi-agent systems but does not yet provide clear strategies to mitigate these challenges. The theoretical results are incremental but may serve as a foundation for future improvements in scalability or algorithmic design."}, "weaknesses": {"value": "1. Limited Conceptual Novelty Beyond Extension\nWhile the paper presents a rigorous treatment of online Distributionally Robust Markov Games (DRMGs), its conceptual novelty is limited. The proposed MORNAVI algorithm largely repackages existing principles‚Äînamely optimism in exploration and robust Bellman operators‚Äîpreviously developed in single-agent robust RL (e.g., Wang & Zou, NeurIPS 2021; Dong et al., ICML 2022; Panaganti & Kalathil, ICML 2022). Extending these to the multi-agent setting is a logical next step but not a fundamentally new paradigm. The paper would benefit from a clearer articulation of what new technical difficulties arise in the multi-agent online case (beyond joint-action explosion) and how MORNAVI specifically overcomes them.\n\n2. Overstated Claims of Firstness and Theoretical Gap\nThe paper repeatedly claims to be the first to address online DRMGs with provable guarantees, yet concurrent and closely related works‚Äîsuch as RONAVI (Farhat et al., 2025, arXiv)‚Äîalready study similar formulations with optimism‚Äìrobustness integration and provide comparable regret bounds. Moreover, previous generative or oracle-based DRMG works (Shi et al., 2024; Ma et al., 2023) have already laid much of the theoretical groundwork. The authors should moderate their 'first provable guarantee' claim and explicitly position their contribution in relation to these concurrent developments.\n\n3. Incomplete or Inaccurate Definitions\nThere are important definitional inaccuracies that need correction before the theory can be considered reliable:\n- The robust coarse correlated equilibrium (CCE) definition in Section 2 is identical to the robust Nash equilibrium definition, which is incorrect.\n- The paper assumes existence of robust NE without proof. For general-sum DRMGs, NE existence is nontrivial; only CE/CCE are guaranteed.\nCorrecting these definitions and clarifying the associated assumptions would strengthen the theoretical soundness and interpretability of the results.\n\n4. Dependence on Restrictive Assumptions\nSeveral assumptions used to make the analysis tractable are strong and may limit practical relevance:\n- The rectangular uncertainty set assumption eliminates coupling across states and agents, simplifying proofs but overlooking realistic uncertainty.\n- The failure-states assumption for TV uncertainty ensures that unseen transitions are learnable, which may not hold in online exploration.\n- The algorithm presupposes centralized model updates and full observability.\nA section explicitly acknowledging these assumptions‚Äô implications‚Äîand discussing potential relaxation strategies‚Äîwould improve credibility.\n\n5. Lack of Empirical or Illustrative Validation\nAlthough the paper is theoretical, it would benefit from a minimal empirical illustration or simulation. A simple 2-player gridworld or coordination game with environmental noise could demonstrate how MORNAVI behaves in practice, whether the derived regret bounds are observable, and how robustness manifests under distribution shifts.\n\n6. Unresolved Scalability Challenge\nWhile the authors discuss the curse of multi-agency (joint action dependence), the paper stops short of offering even partial mitigation strategies. Theoretical exploration of structured policies (mean-field approximations, factored models, or correlated policies) could point toward reducing the exponential dependence on ‚àèAi.\n\n7. Exposition and Structural Issues\nThe exposition can be improved in several ways:\n- Duplicate theorem numbering causes confusion.\n- Several notations (e.g., œÉ_ùí´[V], œÅ_min, P_min) are undefined when first introduced.\n- The confidence interval construction is only in the appendix and should be summarized in the main text.\nClarifying these would improve readability and allow reviewers to verify correctness more easily.\n\nSummary of Actionable Suggestions\n1. Correct the CCE/CE definitions and restate all regret results accordingly.\n2. Clearly differentiate the paper from concurrent work like RONAVI and moderate novelty claims.\n3. Discuss the impact and realism of rectangular and failure-state assumptions.\n4. Include a small synthetic experiment to illustrate algorithm behavior.\n5. Explore scalability strategies to reduce dependence on joint action size.\n6. Fix theorem numbering, ensure consistent notation, and summarize key proof steps in the main text."}, "questions": {"value": "1. Clarification of the Equilibrium Definitions\nThe definition of robust coarse correlated equilibrium (CCE) in Section 2 appears identical to the Nash equilibrium condition. Could the authors clarify whether this was intentional, and if not, provide the correct formulation of the CCE obedience constraints? If the CCE definition is corrected, would this change any of the stated regret bounds or equilibrium existence claims? A clarification of whether the theoretical guarantees hold for all equilibrium notions (NE, CE, CCE) under the same assumptions would be very helpful.\n\n2. Existence of Robust NE and Practical Computability\nThe paper defines a robust NE as a product policy, but general-sum robust games may not guarantee existence. Are there known conditions under which the robust NE considered in this paper is guaranteed to exist (e.g., convex‚Äìconcave payoff structures or zero-sum cases)? How is the equilibrium computed in practice within the MORNAVI framework‚Äîvia an exact solver or approximate methods? Including an explanation of computational feasibility would make the algorithmic contribution clearer.\n\n3. Clarification of the Failure-State Assumption\nThe regret bound under TV divergence relies on the failure-state assumption, which seems to restrict uncertainty to transitions that are still reachable through exploration. Could the authors formalize this assumption more explicitly and discuss its implications? What happens if this assumption is violated‚Äîdoes the regret bound degrade gracefully, or does the algorithm fail entirely? A sensitivity analysis or a theoretical relaxation would strengthen the argument.\n\n4. Scope of the Rectangular Uncertainty Set\nThe analysis assumes rectangular (decoupled) uncertainty sets across states and agents, which simplifies the dynamic programming recursion but limits expressiveness. Could the authors comment on whether their approach could handle non-rectangular (coupled) uncertainty sets, perhaps through approximate decomposition? Would any part of the regret proof break down under correlated uncertainties across agents?\n\n5. Regret Bound Tightness and Scaling with Joint Actions\nThe regret bounds in Theorems 2 and 3 scale with the product of action space sizes (‚àèAi), which makes them impractical for even moderate numbers of agents. Could the authors clarify whether this dependence is inherent or a proof artifact? Are there potential structural assumptions (e.g., mean-field or factored game structures) that could reduce this dependence while maintaining robustness?\n\n6. Comparisons with Concurrent Work\nThe authors position MORNAVI as the first to provide online DRMG guarantees, but RONAVI (Farhat et al., 2025) and related works appear to address similar settings. Could the authors provide a more explicit comparison in terms of assumptions (e.g., oracle access, divergence type), theoretical guarantees, and computational complexity? If RONAVI uses similar optimism‚Äìrobustness design principles, what distinguishes MORNAVI‚Äôs theoretical contribution?\n\n7. Confidence Interval Construction and Proof Transparency\nThe proof of optimism for the robust Q-value relies on confidence intervals that bound the uncertainty-adjusted Bellman operator. Could the authors sketch the key steps or inequalities (e.g., dual form of œÉùí´[V]) in the main text to make the logic more transparent? How do the bonus terms differ in structure between the TV and KL cases, and what intuition explains these differences?\n\n8. Empirical or Illustrative Demonstration\nEven a small-scale experiment could provide insight into how the algorithm performs under model mismatch. Could the authors include or discuss results on a simple two-player coordination or adversarial environment? Observing whether the empirical regret trend aligns with the theoretical rate would help substantiate the practical value of the theoretical development.\n\n9. Theoretical Open Questions\nThe paper concludes by raising the question of whether online DRMG algorithms can overcome the curse of multi-agency. Could the authors elaborate on potential directions‚Äîsuch as hierarchical decomposition, correlated equilibrium relaxation, or partial coordination‚Äîthat might reduce this scaling? Are there theoretical obstacles (e.g., impossibility results) suggesting that sublinear regret without ‚àèAi dependence might be unattainable?\n\n10. Expository and Structural Improvements\nThere are a few presentation issues that would benefit from revision:\n- Duplicate numbering of Theorem 1 for hardness and upper-bound results.\n- Undefined symbols (e.g., œÉùí´[V], œÅmin, Pmin) at their first appearance.\n- Several long equations could use short textual interpretation lines to help readers follow the logic.\nAddressing these would significantly improve readability and make the theoretical arguments easier to follow.\n\nSummary\nThe main clarifications that could substantially change my evaluation are:\n- Correcting and explaining the CCE/CE definitions and their impact on results.\n- Providing clearer justification for key assumptions (failure states, rectangularity).\n- Offering an explicit comparison with concurrent works.\n- Demonstrating even minimal empirical validation or illustrating scalability considerations.\nThese improvements would make the contribution more transparent, the assumptions more credible, and the theoretical results easier to interpret and verify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5i4xxKRfld", "forum": "hIBMCDOwMI", "replyto": "hIBMCDOwMI", "signatures": ["ICLR.cc/2026/Conference/Submission13215/Reviewer_cwx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13215/Reviewer_cwx1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975401983, "cdate": 1761975401983, "tmdate": 1762923904239, "mdate": 1762923904239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}