{"id": "rxQnMSNCUs", "number": 4377, "cdate": 1757669074677, "mdate": 1759898036032, "content": {"title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "abstract": "Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "tldr": "Dataset and SFT+RL training framework to enhance response content and timing in proactive interaction for Video MLLMs", "keywords": ["Proactive Interaction", "Video Dialogue", "Video MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66d6c7d32b3309331b3040bac67e80e861358a5f.pdf", "supplementary_material": "/attachment/d131870948b189c7ffc87be9e535e2f518557ba7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel video multimodal large language model (Video MLLM) called MMDuet2, designed to enhance the model's proactive interaction capabilities—that is, the ability to autonomously decide when and how to respond while watching streaming videos. To address the challenges of existing methods requiring manual threshold adjustment and precise timestamp annotation of responses, the authors propose an innovative multi-round reinforcement learning (RL) training method. This method encourages the model to make timely and accurate responses at the right time through a carefully designed reward mechanism, without the need for precise timestamp annotation. Furthermore, the authors constructed a large-scale proactive dialogue dataset containing 52k videos for training. Experimental results show that MMDuet2 achieves state-of-the-art performance on benchmarks such as ProactiveVideoQA, significantly outperforming existing models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper innovatively introduces multi-round reinforcement learning, using a reward mechanism to teach the model to find the optimal response time, cleverly circumventing the challenge of precise time labeling.\n2. The authors constructed a large-scale dataset containing 52k videos, providing a solid data foundation for training more robust active models.\n3. Experimental results show that the MMDuet2 model trained with SFT+RL outperforms previous state-of-the-art models and our own model trained solely with SFT on authoritative active interaction benchmarks such as ProactiveVideoQA. This demonstrates the effectiveness of reinforcement learning methods."}, "weaknesses": {"value": "1. Using the \"NO REPLY\" text token is a concise and universal approach, but it also means that if the model chooses not to respond, a complete generation process (generating both tokens) is still required, which limits its inference efficiency.\n2. The study on the reward component is insufficient, and related ablation experiments are lacking. The total reward is a weighted sum of four components, but the paper only mentions \"a certain hyperparameter search\" providing a set of weights without conducting ablation studies. This fails to clearly explain the specific contribution of each reward or penalty to the model's performance.\n3. The task is limited to question-and-answer type tasks, and from data construction to model evaluation, it relies heavily on the question-and-answer paradigm. Therefore, the model's generalization ability for non-question-and-answer type proactive response tasks, such as caption tasks, needs to be tested.\n4. There is insufficient comparative testing with other active response models. Related work mentions some active response models, such as Dispider [2] and TimeChat-Online[3]. However, the main experiments did not directly compare these models.\n\n[1] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 18407–18418, 2024a.\n\n[2] Rui Qian, Shuangrui Ding, Xiao wen Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. ArXiv, abs/2501.03218, 2025.\n\n[3] Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, and Xu Sun. Timechatonline: 80% visual tokens are naturally redundant in streaming videos. ArXiv, abs/2504.17343, 2025."}, "questions": {"value": "1. The dialogue template shown in Figure 2 of the paper appears to be standard practice for frame-by-frame streaming video understanding models. Furthermore, the core mechanism for generating \"NO REPLY\" seems functionally equivalent to predicting a specific EOS token (VideoLLM-online [1]). Therefore, aside from the specific implementation, what is the fundamental innovation of this \"text-to-text approach\"?\n2. To better contextualize the performance of MMDuet2, would the authors consider providing direct empirical comparisons against other relevant models, such as Dispider [2] and TimeChat-Online [3]? At the same time, could you please provide an ablation study analyzing the individual impact of the four reward components ($r_{\\text{PAUC}}$, $r_{\\text{rep}}$, $r_{\\text{inspan}}$, $r_{\\text{pfx}}$) on the model's behavior?\n3. The authors employed a strategy of placing the model's response at the end of its response time period during the SFT phase to avoid the model developing illusions before seeing the relevant event. However, the goal of the RL phase is to encourage the model to make the correct response as early as possible. Are these two goals contradictory during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2T6456bVhO", "forum": "rxQnMSNCUs", "replyto": "rxQnMSNCUs", "signatures": ["ICLR.cc/2026/Conference/Submission4377/Reviewer_3Vfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4377/Reviewer_3Vfh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828099103, "cdate": 1761828099103, "tmdate": 1762917324996, "mdate": 1762917324996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMDuet2, aiming to enhance the proactive interaction capabilities of Video Multimodal Large Language Models (Video MLLMs) through multi-turn Reinforcement Learning (RL), enabling the model to autonomously decide when to respond during video playback. The authors construct a new dataset of 52k videos and present a text-to-text RL approach with a reward mechanism, including PAUC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper addresses proactive interaction, which is an important and challenging promblem for making Video MLLMs more natural and practical in real-time applications.\n\n2.  The use of RL to overcome the difficulty of precise reply time annotation is a promising avenue, and the reward mechanism design  theoretically considers timeliness, accuracy, and redundancy.\n\n3. The creation of a large-scale new dataset (52k videos with two dialogue types) provides a valuable resource for research in this field."}, "weaknesses": {"value": "1. The central contribution of this paper lies in rl , which is explicitly designed to improve proactive interaction timing. However, the paper reports that during training on complex ego-centric video tasks, the model exhibited reward hacking behavior—generating large amounts of repetitive content. Although this issue is solved by early stopping, such manual action may show a instability in the reward design and optimization process.\n\n2. The occurrence of reward hacking indicates that the learned policy may not genuinely capture proactive interaction behavior but instead exploits the reward function. The paper does not provide ablations or diagnostic analyses to clarify why this failure occurs, nor does it offer evidence that the method can generalize to more complex or long-duration scenarios without collapsing."}, "questions": {"value": "1. Regarding the observed reward hacking issue, was there a deeper analysis of its root causes beyond \"early stopping\"? Were more sophisticated reward shaping, curiosity mechanisms, or RL algorithms attempted to enhance the model's robustness on complex tasks?\n\n2. Please elaborate on how the largely \"offline\" QA and captioning datasets in the SFT stage were processed and transformed to effectively support RL training for proactive interaction, rather than merely enhancing general understanding?\n\n3. The paper states that reducing the frame interval from 2 seconds to 1 second during inference significantly improves performance, even if the RL phase used a 2-second interval. Does this suggest that the model's decision-making process is highly sensitive to the temporal granularity of the input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GO0LfkRWnA", "forum": "rxQnMSNCUs", "replyto": "rxQnMSNCUs", "signatures": ["ICLR.cc/2026/Conference/Submission4377/Reviewer_rgJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4377/Reviewer_rgJo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970544958, "cdate": 1761970544958, "tmdate": 1762917324749, "mdate": 1762917324749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the reward-based RL in online video LLM settings and propose MMDuet2 to autonomously determine whether to respond as soon as possible or remain silent in a proactive manner by RL training. This paper starts from curating online video llm training data, which is designed for multiturn proactivate dialogue, then design the specialized chat template for it and use SFT and RL to train the model in the off-policy and on-policy way for proactivate capabilities. Experimental results show that the proposed MMDuet2 can proactively answer user's queries in the online video streaming setting while maintain the ability to answer in offline video setting."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) This paper is one of the first batch to investigate RL training (esp. GRPO) for proactively answering in online streaming video settings, which is of substantial novelty. Also, the paper investigate the reward, the key component of GRPO, and model it specifically for online video settings (PAUC).\n\n2) The authors design a training dataset especially for online video streaming setting and corresponding chat template. Looking forward to the dataset open sourcing.\n\n3) The MMDuet2 trained by SFT and GRPO outperform previous online methods while maintaining offline video understanding capabilities"}, "weaknesses": {"value": "There are no major technical concerns about this paper, but I want to address some minor points as follows:\n\n1) As the key component to apply GRPO to online video settings, the ablations on rewards should be more addressed. Did authors try other rewards than PAUC? Please compare several reward formulations and discuss why PAUC is preferred.\n\n2) The author is encouraged to report the actual inference speed and latency of the MMDuet2 to see if it is realtime in practical scenes.\n\nAlso, the organization of this paper needs to be improved further since the current version seems too flattened with every details being straight written without a main storyline."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "suAR1ljGEa", "forum": "rxQnMSNCUs", "replyto": "rxQnMSNCUs", "signatures": ["ICLR.cc/2026/Conference/Submission4377/Reviewer_Rz9J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4377/Reviewer_Rz9J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997305471, "cdate": 1761997305471, "tmdate": 1762917324333, "mdate": 1762917324333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}