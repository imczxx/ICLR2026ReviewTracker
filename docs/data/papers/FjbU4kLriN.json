{"id": "FjbU4kLriN", "number": 20454, "cdate": 1758306338457, "mdate": 1763736222347, "content": {"title": "Unique between the lines: benchmarking re-identification risk for text anonymization", "abstract": "Data containing sensitive personal information is increasingly used to train, finetune, or query Large Language Models (LLMs), raising the risk such data may be inadvertently leaked. Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft’s Presidio or Anthropic’s PII purifier. These tools are generally evaluated based on their ability to remove manually annotated identifiers (e.g., names), yet their effectiveness at preventing reidentification remains unclear. We introduce RAT-Bench, which is to the best of our knowledge, the first modern, synthetic benchmark for measuring the effectiveness of text anonymization tools at preventing re-identification. We use U.S. demographic statistics to generate synthetic, yet realistic text, that contains various direct and indirect identifiers across diverse domains and levels of difficulty. We apply a range of NER- and LLM-based text anonymization tools on our benchmark and, based on the attributes an LLM-based attacker is able to infer correctly from the anonymized text, we report the risk that any individual will be correctly re-identified in the U.S. population. Our results show that existing tools still often miss direct identifiers or leave enough indirect information for successful re-identification. Indeed, even for the widely used anonymizer Azure and state-of-the-art GPT-4 anonymizer instantiated with the Rescriber prompt, the success rate remains at 29% and 27%, respectively. We conduct ablations for number and type of attribute, and also study the utility and cost of anonymization. We find that NER-based methods can reduce re-identification risk substantially, albeit sometimes at a strong cost in utility. LLM-based tools remove identifiable information more precisely, yet require a higher computational cost. We will release the benchmark and encourage community efforts to expand it, so it remains a robust test as tools become better in the future.", "tldr": "We introduce a benchmark that evaluates text anonymizers by re-identification risk, using synthetic but realistic texts seeded with U.S. demographics.", "keywords": ["anonymization", "re-identification", "privacy", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19c968f8f4c4a048dd3cc8d15ae0e426ce4d1774.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a benchmark on individual reidentification risks from natural texts, aimed at evaluating anonymization methods in the face of an LLM adversary. The benchmark is synthetically constructed by sampling US demographic tabular data and synthesizing text around it using an LLM. The synthesized text may contain both indirect identifiers (e.g., sex) and direct identifiers (e.g., social security number). There are three difficulty levels defined, easy, medium, and hard, depending on the types of identifiers that are contained and ont their difficulty of extraction. Using their benchmark, the authors evaluate several anonymization tools, and conclude that they all still pose a large risk of reidentification post-anonymization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Important problem.\n- Linking the inferred attributes to the concrete reidentification risk is a useful metric."}, "weaknesses": {"value": "Overall, the paper lacks novelty.\n- The introduced benchmark does not offer sufficient addition over existing datasets for this purpose, e.g., SynthPAI (Yukhymenko et al. 2024). Also, it is unclear why a reconstruction of such a dataset is even needed, why could one not just extend SynthPAI to include direct identifiers.\n- It is clear that direct identifiers, if left in the text, will lead to reidentification. As such, *any* prior benchmark of classical anonymization, NER, etc. has already measured this risk. This is not a novel contribution.\n- Also, the reidentification risk from direct identifiers, is simply a bijective and monotonic function of the inferred attributes. As such, already the results of prior work on LLM inference against anonymized text (e.g., Staab et al. 2024 & 2025) already carry exactly the same conclusions. Simply a conversion can be applied to the attribute inference accuracy metrics. And this conversion is not a contribution of this paper either, as it was introduced, according to the authors' citations as well, by Rocher et al. (2019).\n- The paper lacks novel experiments and conclusions. The difficulty level-based separations in observed inference patterns have already been observed by Staab et al. (2024) & (2025). The poor utility of NER-based classical anonymmization tools has also been already observed by Staab et al. 2025. The fact that NER-based anonymizers do better with direct identifiers/direct inclusions of the indirect identifiers and struggle with contextual clues has been observed, once again, by Staab et al. (2024) & (2025). The cost of LLM-anonymization has also already been analyzed by Staab et al. (2025). The sole truly novel experiment is included in Figure 3, showing a growing reidentification risk with an increased number of attributes in the text. Note however that this experiment's results are unfortunately not surprising, given the overall monotonicity of reidentification.\n\nFurther, the paper is not state-of-the-art.\n- The LLM adversary is a weak local model; understimating the privacy risks. Other works on LLM-based inference risks argue for strong adversaries for this reason (Staab et al. 2025).\n- The LLM-based anonymization method used in the evaluation is, in the face of existing literature to this end, ad-hoc. The authors should have evaluated the LLM-based anonymization methods that they cite themselves as well in Section 2.\n- The introduced benchmark dataset is not as realistic, robust, and versatile as datasets used in other works. Unclear why these were not employed, or why their synthetic construction quality criteria were not observed (e.g., Yukhymenko et al. 2024).\n\nMinor.\n- The figures on page 8 are rather small and difficult to read.\n- The x-axis in Figure 1 (a) is unclear. How can one have a monotonous function (ordered) over these categories (independent)?\n\n**References**\n\nL Rocher et al. Estimating the success of re-identifications in incomplete datasets using generative models. Nature Communications 2019.\n\nR Staab et al. Beyond memorization: Violating privacy via inference with large language models. ICLR 2024.\n\nH Yukhymenko et al. A Synthetic Dataset for Personal Attribute Inference. NeurIPS 2024.\n\nR Staab et al. Large language models are advanced anonymizers. ICLR 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rIJyzODHBs", "forum": "FjbU4kLriN", "replyto": "FjbU4kLriN", "signatures": ["ICLR.cc/2026/Conference/Submission20454/Reviewer_8xbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20454/Reviewer_8xbg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761078717719, "cdate": 1761078717719, "tmdate": 1762933898387, "mdate": 1762933898387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first modern synthetic benchmark for evaluating text anonymization tools by measuring their effectiveness at preventing re-identification of individuals in the U.S. population. The authors generate realistic text grounded in demographic data containing both direct identifiers (names, SSNs) and indirect identifiers (occupation, state), apply seven anonymization tools (NER-based and LLM-based), and use an adversarial LLM to infer attributes from the anonymized text."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper introduces the first benchmark that measures anonymization effectiveness through actual re-identification risk rather than just entity recall, combining both direct and indirect identifiers with a rigorous methodology grounded in real U.S. demographic data (PUMS) and the established re-identification framework from Rocher et al. (2019).\n2) The benchmark design is thorough, incorporating controlled difficulty levels (easy/medium/hard), diverse scenarios (medical transcripts, chatbot interactions)"}, "weaknesses": {"value": "* The paper misses several important strong text-to-text privatization baselines [1,2,3]. Listing them here, would highly recommend discussing and if possible adding them as baselines to properly support the claim \"The best anonymizer leaves a significant re-identification risk of 36% in our setup\"\n\n\n* Did the authors try to consider multiple threat models, such as the Static Attacker and Adaptive Attacker in [4]?\n\n\nRefs\n\n[1] Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations .WSDM 2020\n\n[2] TEM: High Utility Metric Differential Privacy on Text, SIAM, 2023\n\n[3] Locally differentially private document generation using zero shot prompting, EMNLP 2023\n\n[4] The Limits of Word Level Differential Privacy, EMNLP 2022."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mCf7NE5BmV", "forum": "FjbU4kLriN", "replyto": "FjbU4kLriN", "signatures": ["ICLR.cc/2026/Conference/Submission20454/Reviewer_bkRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20454/Reviewer_bkRV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814503772, "cdate": 1761814503772, "tmdate": 1762933897336, "mdate": 1762933897336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new dataset and benchmark for evaluating how well current PII anonymizers and text sanitizers perform at actually removing direct and indirect attributes from written texts. In particular, the presented dataset of 600 samples (across three hardness levels) consists of two main settings: (1) fictional doctor-patient conversations and (2) fictional AI chatbot interactions. The authors measure how well an adversary (instantiated via an LLM) can recover these direct and indirect identifiers both before and after anonymization. For indirect identifiers, the likelihood of unique identifiability (based on the US Census) is reported. On their dataset, they benchmark five NER-based anonymizers and one version of LLM-based anonymization (in two model variants), finding that overall Azure performs best across both settings and hardness levels. Typical trends across hardness levels are also ablated, and overall, in any level or setting, the final re-identification success rate remains very high ($\\geq 24\\%$). While generally faster and more thorough than the tested LLM versions, NER anonymizers have a higher impact on the utility of the resulting texts (as reported via BLEU and ROUGE)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall issue of existing anonymizers falling short of anonymizing texts is an actual problem with real-world implications for people's privacy. Any effort in this direction is worth it in the reviewer's opinion. Existing industry-level anonymizers should not miss \"an average of 4.8%\" of direct identifiers.\n- The general split of hardness levels is nice and reveals intuitive patterns. Overall, having datasets to evaluate the performance of methods on is important for the field. Further, the two scenarios are generally not unrealistic.\n- The inclusion of the re-identification risk via indirect attributes seems a good way of contextualizing non-direct privacy leakage in this setting.\n- The failure cases found this way (App. E) can (hopefully) help to improve the underlying anonymizers themselves."}, "weaknesses": {"value": "- The individual samples are, in parts, quite unrealistic (in particular, level 2). This by itself is not an issue (for PII removal, it should work statically on any data source); however, it is problematic when we try to draw conclusions about the real-world impact of the lack of proper anonymization. With this, the dataset has more relevance as an individual \"how well are removal engines in these synthetic scenarios\" benchmark.\n- This goes hand in hand with these samples being generated by a quite weak model (2.5-flash) and not having undergone any human validation (at least not reported). While the samples I saw were generally fine (although quite artificial), prior work in this area either used real-world data [1,3] or released human-verified samples alongside [4,5].\n- The ground-truth adversary was chosen as an 8B LLM. This is particularly problematic, as prior work has shown that attribute or identifier inference correlates strongly with model capabilities. As such, the currently reported numbers serve merely as a rough lower bound, which likely could be considerably higher. This is in part even acknowledged by \"but rather because the attacker LLM itself struggles more to correctly infer attributes even without anonymization,\" which generally holds but much more so with weaker adversaries.\n- The instantiated LLM anonymizers based on the Anthropic PII perform worse than Azure. At the same time, prior work (e.g., [4]) has already shown that stronger anonymization via LLMs is possible, raising the question of why such stronger methods were not evaluated (in particular, many of the missed instances in Table 6 seem like they should be detectable there). Similarly, the current LLM anonymizers were instantiated with comparatively weak models, which can make a big difference in anonymization.\n- The \"being first to do it\" angle seems a bit oversold to the reviewer given that prior work already has generated at least equally realistic synthetic benchmarks for anonymization [5] that could be directly adapted to the re-identification setting by applying the corresponding definition from"}, "questions": {"value": "Besides the above, I have the following questions:\n\n- Not many dataset statistics are given in the work. What is the distribution of these identifiers for the numbers you report? How are they distributed over hardness levels? How are they co-distributed in texts and potentially correlated? So far, I can only roughly estimate some marginals from the recall numbers in Figure 5 and Figure 6.\n- Do you have general insights on which types of failures (FP and FN) we observe per model type?\n- How do you interpret your current results with respect to prior work that shows LLMs clearly outperforming methods like Azure or Presidio [2,4,5]? Which conclusions should practitioners, users, and anonymization system developers draw when they see both that Azure is insufficient across much prior work and the statement that they \"provide computationally efficient protection\" (on synthetic data)?\n- Smaller: Matching based on Jaro-Winkler for free-text attributes is an improvement over direct matching. Did you manually verify that this threshold holds here? From personal human post-labeling efforts, this can still be partly off.\n\nSources\n\n[1] Pilán, Ildikó, et al. \"The text anonymization benchmark (tab): A dedicated corpus and evaluation framework for text anonymization.\" _Computational Linguistics_ 48.4 (2022): 1053-1101.\\\n[2] Bubeck, Sébastien, et al. \"Sparks of artificial general intelligence: Early experiments with gpt-4.\" _arXiv preprint arXiv:2303.12712_ (2023).\\\n[3] Staab, Robin, et al. \"Beyond memorization: Violating privacy via inference with large language models.\" _arXiv preprint arXiv:2310.07298_ (2023).\\\n[4] Staab, Robin, et al. \"Large language models are advanced anonymizers.\" _arXiv preprint arXiv:2402.13846_ (2024).\\\n[5] Yukhymenko, Hanna, et al. \"A synthetic dataset for personal attribute inference.\" _Advances in Neural Information Processing Systems_ 37 (2024): 120735-120779.\\\n[6] Luc Rocher, Julien M Hendrickx, and Yves-Alexandre De Montjoye. Estimating the success of\nre-identifications in incomplete datasets using generative models. Nature communications, 10(1):\n3069, 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GS4yCpF0eA", "forum": "FjbU4kLriN", "replyto": "FjbU4kLriN", "signatures": ["ICLR.cc/2026/Conference/Submission20454/Reviewer_uffD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20454/Reviewer_uffD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840940417, "cdate": 1761840940417, "tmdate": 1762933896356, "mdate": 1762933896356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses the topic of PII removal and identity re-identification in the context of LLMs. In particular, it presents a novel synthetic benchmark aimed at measuring re-identification rates with contemporary anomymization approaches. The paper generates the benchmark synthetically based on entity types sampled from the 5% Public Use Microdata Sample (PMUS). This data is then anonymized using NER- and LLM-based approaches. The benchmark itself then measures re-identification risk against an attack. The authors focus on medical appointment transcripts and AI chatbot interactions as scenarios for the benchmark. Gemini 2.5 Flash is used as the model to generate texts from given attributes. The benchmark is evaluated on several anonymization tools: Azure Language Studio, Presidio, Scrubudub, GliNER, UniNER  (NER-based approaches) as well as Anthropic’s PII purifier prompt used with Gemini 2.5 Flash and Llama-3.1-8B. As the attacker, the authors use Llama-3.1-8B-Instruct in conjunction with an existing attack prompt. The main findings show re-identification rates above 35% across all tools, indicating that the investigated anonymization methods anonymize texts in a far from optimal way. The paper provides additional results on varying difficulty levels for the anonymization as well as varying the number of direct or indirect identifiers. Finally, the paper assesses the utility loss incurred from anonymization and reports that tools that anonymize texts more aggressively yield the highest utility losses, and that LLM-based methods preserve utility better than NER-based ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides an interesting benchmark to measure the success rates of anonymization tools in NLP. It shows that recent anonymization tools are still far from perfect, an insight which is likely going to be interesting to the wider research community focussing on privacy in NLP. The paper presents extensive experiments, including algorithm ablations (e.g., with respect to the number of direct and indirect identifiers) and is overall well-written and well-structured."}, "weaknesses": {"value": "* The paper makes repeated use of Llama models for both anonymizing the data, and evaluating the anonymizers on the benchmark. I’m concerned that using the same model family across attack and evaluation can lead to unintended biases. I would encourage the authors to at least discuss this potential issue.\n* Related to the above, it would be interesting to see additional models employed for generating benchmark data, to observe how different models affect anonymization performance. \n* The paper does not seem to be using any form of human validation of the generated benchmark. It would be interesting to see small-scale human annotation studies verifying that the generated benchmark data satisfies the intended shape of the task (esp. w.r.t. the difficulty levels).\n* It would be great if the benchmark could be given a more unique name (for easier references)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G1Jkh6II7Z", "forum": "FjbU4kLriN", "replyto": "FjbU4kLriN", "signatures": ["ICLR.cc/2026/Conference/Submission20454/Reviewer_RT7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20454/Reviewer_RT7r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114173956, "cdate": 1762114173956, "tmdate": 1762933895732, "mdate": 1762933895732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}