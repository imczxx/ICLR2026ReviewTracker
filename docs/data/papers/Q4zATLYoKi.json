{"id": "Q4zATLYoKi", "number": 8838, "cdate": 1758099650041, "mdate": 1763740739203, "content": {"title": "From Inpainting to Editing: A Self-Bootstrapping Paradigm for Context-Rich Visual Dubbing", "abstract": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning (i.e., masked video frames and misaligned static references) forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping paradigm that reframes visual dubbing from an under-specified inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer (DiT), first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based editor model is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This frame-aligned video conditioning forms a rich \"context\" for the editor, providing it with complete identity cues, scene interactions (e.g., lighting and occlusions), and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy that aligns diffusion stages with visual hierarchies, significantly enhancing contextual learning and dubbing quality. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios. Our visualizations are available at the anonymous page x-dub-lab.github.io, and code will be released to benefit the community.", "tldr": "", "keywords": ["Visual dubbing", "Diffusion Transformers", "Contextual learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f49ae3b3097dcf96596173e3b193adf11cda57c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Based on the paper, a self-bootstrapping dubbing paradigm is proposed that leverages Diffusion Transformers (DiT) as both a generator of context-rich paired data and a video editor trained on them. This approach transforms dubbing from an under-specified inpainting task into a well-conditioned video-to-video editing problem. A timestep-adaptive multi-phase learning strategy is introduced to disentangle visual information learning across diffusion timesteps, facilitating more effective contextual learning and yielding enhanced lip-sync quality and visual coherence. Additionally, a new benchmark is proposed for evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents several notable strengths: it introduces a valuable benchmark for evaluation, features clear and well-structured writing, and demonstrates strong experimental design through comprehensive quantitative comparisons with solid evaluation metrics."}, "weaknesses": {"value": "1. Limited Innovation: The core contribution appears to primarily reside in the benchmark development, while the video editing component relies mainly on some training strategies rather than substantial methodological breakthroughs.\n2. Insufficient Supplementary Materials: The absence of supplementary video results restricts evaluation to single-frame qualitative analysis, which fails to adequately demonstrate the method's effectiveness, particularly as the presented results lack compelling visual evidence."}, "questions": {"value": "1. What is the intrinsic relationship between the two different DiTs in relation to the task focused on in the paper?\n2. Inference speed and resource requirements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't have any concern."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6B6SgFaDe", "forum": "Q4zATLYoKi", "replyto": "Q4zATLYoKi", "signatures": ["ICLR.cc/2026/Conference/Submission8838/Reviewer_oymE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8838/Reviewer_oymE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644732011, "cdate": 1761644732011, "tmdate": 1762920607461, "mdate": 1762920607461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes X-Dub, a self-bootstrapping diffusion framework that reframes visual dubbing from mask-based inpainting to context-rich video-to-video editing. A DiT-based generator first synthesizes paired videos with consistent context but varied lip motion, which are then used to train an editor for precise, audio-driven dubbing. The method achieves strong lip-sync accuracy, identity preservation, and robustness on both standard and challenging benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel paradigm that redefines visual dubbing as context-rich editing rather than inpainting, addressing the long-standing issue of incomplete contextual data.\n\n2. The self-bootstrapping design elegantly generates synthetic paired data with high contextual consistency, enabling effective training without real-world paired supervision.\n\n3. The proposed timestep-adaptive multi-phase strategy effectively disentangles global, lip, and texture information, leading to improved visual coherence and lip-sync precision.\n\n4. The method demonstrates comprehensive and consistent performance gains."}, "weaknesses": {"value": "1. The model’s dependence on self-generated training pairs may introduce domain bias and accumulate artifacts, limiting generalization to real-world data.\n\n2. If the Generator produces mouth jitter or unnatural expressions, the Editor may learn to correct these artifacts rather than the true audio-driven dubbing mechanism.\n\n3. Despite short-segment generation, long videos may still suffer from color or expression drift.\n\n4. The Editor is trained on stable, noise-free, and pose-aligned pairs, which may reduce robustness under real-world conditions with occlusion, desynchronization, or compression noise.\n\n5. The overall training pipeline is complicated, with high computational cost in multi-phase LoRA tuning and limited end-to-end optimization."}, "questions": {"value": "1. How do the authors prevent artifacts or domain bias from self-generated training pairs from misleading the Editor?\n\n2. Is the two-stage, multi-phase pipeline scalable, or could joint optimization simplify training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DBY88VcJdc", "forum": "Q4zATLYoKi", "replyto": "Q4zATLYoKi", "signatures": ["ICLR.cc/2026/Conference/Submission8838/Reviewer_h2PZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8838/Reviewer_h2PZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818100707, "cdate": 1761818100707, "tmdate": 1762920606853, "mdate": 1762920606853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes X-Dub, a self-bootstrapping framework for audio-driven visual dubbing that shifts from traditional mask-based inpainting to context-rich video-to-video editing using Diffusion Transformers (DiTs). A DiT generator creates lip-altered companion videos to form paired training data with originals, enabling a DiT editor to focus on precise lip synchronization while preserving identity and handling challenges like occlusions and lighting variations. A timestep-adaptive multi-phase learning strategy aligns diffusion stages with global structure, lip shapes, and textures for enhanced quality. The work introduces ContextDubBench, a robust benchmark combining real and generated content. Experiments demonstrate state-of-the-art lip sync, identity fidelity, and robustness over baselines like Wav2Lip, Diff2Lip, and MuseTalk."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is clearly defined — the paper nicely distinguishes visual dubbing from generic talking-head or animation generation, and provides a systematic analysis of why existing self-reconstruction based methods fail in this setting.\n2. The paper is well written and easy to follow. The motivation, pipeline, and experiments are clearly presented, making it accessible even to non-specialists in video synthesis.\n3. The visual results are impressive and effectively demonstrate the model’s advantages. The qualitative comparisons clearly highlight improvements in lip-sync accuracy and identity consistency."}, "weaknesses": {"value": "1. The diffusion denoising process is hierarchical: early timesteps produce very coarse, blurry structure while fine details only emerge later. Lip-sync and identity losses, however, demand semantic-level precision. Applying those high-level losses too early can inject mismatched gradient signals, disturbing the coarse-stage optimization and causing blurriness, slow convergence, or instability.\n2. The framework is two-stage (generator → editor), but it's unclear what exactly is contained in the promised “context.” It reads like the second stage simply fine-tunes or borrows features produced by the first stage — we need a clearer, concrete definition of what information the context carries and why that specific context is better than, say, stronger reference frames or alternative conditioning.\n3. Using diffusion twice (generate contextual pair then edit) substantially increases compute. Compared to single-stage, end-to-end video generation/editing methods, the proposed pipeline may be heavier but the paper does not convincingly show a runtime or efficiency advantage — so the trade-off between quality and cost is unclear."}, "questions": {"value": "1. Can you provide a more rigorous analysis of how lip-sync and identity losses interact with the diffusion timestep schedule? Right now the timestep choices look manual, please justify them theoretically or empirically, and show that these losses do not interfere destructively across timesteps.\n2. Please provide concrete compute and latency numbers (FLOPs, GPU-hours, or wall-clock inference time) and clarify whether the method can meet real-time or near-real-time constraints. If not real-time, what are the practical deployment limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j678sTcseu", "forum": "Q4zATLYoKi", "replyto": "Q4zATLYoKi", "signatures": ["ICLR.cc/2026/Conference/Submission8838/Reviewer_Ji3y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8838/Reviewer_Ji3y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896285637, "cdate": 1761896285637, "tmdate": 1762920606512, "mdate": 1762920606512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a self-bootstrapping framework for talking-head generation, where a diffusion-based generator constructs synthetic contextual pairs to train an editor that performs high-quality dubbing without explicit masks. A timestep-adaptive multi-phase learning strategy is proposed to balance reconstruction, identity, and lip-sync objectives across different noise levels. The method shows strong results on standard benchmarks, demonstrating improved identity preservation and lip-sync accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents strong quantitative results and competitive performance.\n- It is clearly written and easy to follow."}, "weaknesses": {"value": "1. The timestep-adaptive multi-phase learning strategy is central to the paper but not fully explained. It would be helpful to clarify how the phase ranges and α thresholds were chosen, and how different timesteps were selected for applying losses such as identity or lip-sync  loss. Additional quantitative or sensitivity analysis, or references supporting these design choices, would strengthen the methodology.\n2. It is unclear how much of the improvement comes from the constructed paired data versus the timestep-adaptive multi-phase learning. The relative contribution of each factor is not explicitly analyzed, making it difficult to interpret the source of performance gains."}, "questions": {"value": "1. Could the authors clarify how the phase boundaries and α thresholds were determined—for instance, whether they were selected through validation experiments or set heuristically?\n2. Would applying the timestep-adaptive multi-phase learning to the generator* alone lead to comparable results? This could help clarify whether the performance gains mainly come from the timestep-adaptive learning or the constructed paired data.\n3. It would be helpful if the authors could provide qualitative video examples on the HDTF benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "si4xOIpazu", "forum": "Q4zATLYoKi", "replyto": "Q4zATLYoKi", "signatures": ["ICLR.cc/2026/Conference/Submission8838/Reviewer_LNYW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8838/Reviewer_LNYW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987956507, "cdate": 1761987956507, "tmdate": 1762920606117, "mdate": 1762920606117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}