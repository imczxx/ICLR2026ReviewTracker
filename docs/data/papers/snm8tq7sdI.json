{"id": "snm8tq7sdI", "number": 3082, "cdate": 1757329597585, "mdate": 1763746186730, "content": {"title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding responses with retrieved information. As an emerging paradigm, Agentic RAG further enhances this process by introducing autonomous LLM agents into the information seeking process. However, existing benchmarks fall short in evaluating such systems, as they are confined to a static retrieval environment with a fixed, limited corpus and simple queries that fail to elicit agentic behavior. Moreover, their evaluation protocols assess information seeking effectiveness by pre-defined gold sets of documents, making them unsuitable for the open-ended and dynamic nature of real-world web environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with challenging questions designed for assessing agentic information seeking in real-world, dynamic web environments. We propose a systematic methodology for constructing challenging queries satisfying the criteria of determinacy, difficulty, and diversity. Based on this, we develop the first evaluation framework tailored to dynamic agentic information seeking, including fine-grained metrics about the accuracy, utility, and compactness of information seeking outcomes. Through extensive experiments across LLMs, search engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and offers actionable insights for future research", "tldr": "We introduce InfoDeepSeek, a new benchmark with challenging questions and fine-grained evaluation designed for assessing agentic information seeking in real-world, dynamic web environments.", "keywords": ["Search Agent", "Large Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff9ab890884f04a4657bccc19d078e4987004270.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces InfoDeepSeek, a benchmark designed to test how well AI agents actually perform complex information-seeking tasks, arguing that existing benchmarks are far too simple. Using a challenging new dataset, the authors demonstrate that even the most advanced agents still struggle significantly with multi-step reasoning and noisy information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work articulates the limitations of existing RAG benchmarks, noting that simple queries are insufficient for evaluating the complex behaviors of modern agentic systems in dynamic web environments.\n\n2. The paper introduces a high-quality, challenging dataset with a robust construction methodology. \n\n3. he authors propose fine-grained metrics (like Information Accuracy, IA@k) tailored for a dynamic setting."}, "weaknesses": {"value": "1. The paper fails to include any non-agentic baselines (e.g., standard RAG). Without this comparison, the utility of the agentic framework is unsubstantiated. The poor results (17.73% ACC) could be an artifact of the framework's own inefficiency (as hinted by \"Retrieval Interference\") rather than just task difficulty.\n\n2. The benchmark narrowly evaluates \"information seeking\" by decoupling it from \"generation.\" This is a critical flaw, as the core challenge of an agentic system is not just finding information but synthesizing, reasoning over, and filtering the noisy, contradictory results—a process that happens at the generation step."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m6qzATA2lu", "forum": "snm8tq7sdI", "replyto": "snm8tq7sdI", "signatures": ["ICLR.cc/2026/Conference/Submission3082/Reviewer_5hy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3082/Reviewer_5hy5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678543902, "cdate": 1761678543902, "tmdate": 1762916543560, "mdate": 1762916543560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper introduces InfoDeepSeek, a benchmark designed to evaluate agentic information-seeking behaviors of RAG models, aiming to address the limitations of existing benchmarks that operate in static retrieval environments and lack question complexity.\n\n2. During dataset construction, InfoDeepSeek considers the characteristics of determinacy, difficulty, and diversity, and employs a multi-stage human verification process, ultimately collecting 1,032 validated data entries.\n\n3. The experiments evaluate a range of retrieval-augmented LLMs across different real-world search engines, yielding valuable insights into model capabilities and the impact of search engine quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is solid, which clearly points out the drawbacks of existing agentic search benchmarks that rely on static corpus evaluation.\n\n2. The data construction is relatively comprehensive, covering key aspects relevant to agentic information-seeking evaluation, such as domain diversity, question difficulty, and question types.\n\n3. Experiments include multiple LLMs, search engines, and ablations (test-time scaling, retrieval interference), giving the work empirical depth."}, "weaknesses": {"value": "1. The involvement of LLMs/agents in data filtering and construction may introduce bias. It is necessary to first examine the consistency between LLMs/agents and human annotators to ensure reliability, as well as to conduct multiple evaluation runs to verify reproducibility.\n\n2. The paper did not report confidence intervals or variance analyses for the metric EEU, making its stability questionable, since a decrease in ACC could paradoxically lead to an increase in EEU.\n\n3. The IC metric relies on human-annotated S_q, which affects both the automation of the evaluation process and its consistency, as differences in annotation standards across annotators may impact reliability.\n\n4. The lack of ground truth forces the evaluation of search results to depend heavily on LLM-as-Judge (e.g., IA@k), which raises concerns about evaluation cost, especially for multi-turn agentic search scenarios."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BpAoQTSb29", "forum": "snm8tq7sdI", "replyto": "snm8tq7sdI", "signatures": ["ICLR.cc/2026/Conference/Submission3082/Reviewer_29uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3082/Reviewer_29uc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794835806, "cdate": 1761794835806, "tmdate": 1762916543406, "mdate": 1762916543406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark (InfoDeepSeek) for agentic information seeking. The dataset contains questions that meet three criteria: determinacy (clear, unique answer), difficulty, and diversity. Some of the questions are manually curated and some are collected using LLM agents, with multiple validation steps.   \nThey propose multiple new metrics for evaluating retrieval results on top of answer accuracy. They evaluate search agent systems with different LLMs and different search agents, and provide some analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea for the paper is easy to understand. Notations are pretty clear in general. \n2. The dataset may be useful for evaluating agentic search systems. \n3. Some of the newly proposed metrics could be useful for evaluating agentic information retrieval."}, "weaknesses": {"value": "1. The dataset does not create new characteristics that are lacking in current benchmarks. For example, FRAMES (Krishna et al., 2025) and BrowseComp (Wei et al., 2025) are designed to be difficult and require multi-turn retrieval. I think the paper should focus more on what past datasets lack and what are the new contributions. \n2. Definition of the metrics may not be very useful. For example, information accuracy (IA) does not actually entail “information accuracy”, because inaccurate information could lead to correct answers and vice versa given the definition. A better way might be using a LLM judge if gold documents are not provided. Effective Evidence Utilization (EEU) also does not seem very useful, given the metric depends largely on the quality of the initial set. If the quality of the initial set is low (like the case of DuckDuckGo), the metric might not be informative. I leave the comment for information compactness (IC) to the “Questions” section.\n3. The description of the data construction process is very vague. The exact procedure for identifying the anchor knowledge (or the definition of “anchor knowledge”) is unclear. There is also no rule for diversification, making the distribution of topics and languages seem arbitrary. \n4. The paper would benefit from some error analysis. It would be helpful to show how agentic systems fail on each type of the questions (e.g. multi-hop, long-tail, etc.), and it would make clear how the data contributes to the community. \n5. The difficulty filtering may not be very useful, as some LMs can still directly answer the questions. (Section 6.3) This contradicts the claims of the authors, where LMs should not be able to answer these questions directly. Also it seems plausible to discard the question if one out of the two LMs (GPT-4o and DeepSeek-R1) can answer correctly. Why discard only when both answers correctly?\n\nReference: \nKrishna, Satyapriya, et al. \"Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation.\" Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2025.\nWei, Jason, et al. \"Browsecomp: A simple yet challenging benchmark for browsing agents.\" arXiv preprint arXiv:2504.12516 (2025)."}, "questions": {"value": "1. Could you describe the data construction process clearer?  \na. How are anchor knowledge selected? The description in the main text is vague, and even when I read the appendix is not very clear to me. For example in L908-909, what does “candidate fact” mean? Also “anchor knowledge” is not rigorously defined.  \nb. What’s the instruction for drafting the questions? Do all annotators see the same instructions? What are the prompts for the LM to draft the questions?  \nc. How do you do the diversification of questions? Based on the descriptions it seems very arbitrary.  \nd. How many examples are checked by humans and how many are checked by agents? What’s the exact process? The description in L264-L269 is very vague.  \ne. For checking the difficulty of questions, why not also prompt LMs with just their parametric knowledge? Some questions might be solvable with only parametric knowledge, as described in Section 6.3.  \n2. What’s the main reason for stopping at 5 iterations? \n3. For the predominant language prompt, how often do they actually switch to that language? \n4. For the retrieval interference experiments, what’s the percentage of examples where LM can answer correctly using parametric knowledge? \n5. What’s the use of EEU? It seems higher the better, but you also mention that for DuckDuckGo it's an artifact of poor retrieval quality? How good should the retrieval quality be in order for the metric to be useful? \n6. Why not discuss IC at all in Section 6.2? What’s the purpose of having it then? Also GPT-4o has very low IC, why?\n7. Is $n_q$ always 5 for IC? If not, why not use $k$ instead of $n_q$, if agent could answer correctly with only $k$ documents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkgnOmeoAl", "forum": "snm8tq7sdI", "replyto": "snm8tq7sdI", "signatures": ["ICLR.cc/2026/Conference/Submission3082/Reviewer_Ys9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3082/Reviewer_Ys9X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859341755, "cdate": 1761859341755, "tmdate": 1762916543179, "mdate": 1762916543179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes InfoDeepSeek, a benchmark for evaluating the information‑seeking capability of agentic RAG systems in realistic, dynamic web environments with complex queries, addressing limitations of static‑corpus evaluations with simple queries. It contributes 1032 diverse, deterministic, high-difficulty questions spanning multiple languages and challenging attributes. The authors describe a three‑stage construction pipeline (drafting, filtering, multi‑stage validation) that is run manually but can be automated with LLM agents. They further propose a dynamic evaluation protocol with fine‑grained metrics for benchmarking answer/context accuracy, evidence utility, and informational compactness. They further propose a dynamic evaluation protocol with fine‑grained metrics (with low ACC), and analyses highlight test‑time scaling gains from allowing more retrieval steps, widespread retrieval interference, and benefits of language‑aware retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluating agentic RAG systems on the live web is novel and addresses a critical gap in prior static‑corpus evaluation pipelines; the curated benchmark is valuable to the community.\n2. The proposed metrics—IA@k, EEU, and IC—are specifically designed to assess information‑seeking and evidence‑utilization capabilities of agentic RAG systems.\n3. Thorough experiment on different aspects including various LLMs, search engines, number of searching steps during inference, and predominant languages.The finding of retrieval interference is insightful and likely to motivate further research."}, "weaknesses": {"value": "1. **Reproducibility challenges in a dynamic web environment**: As the authors note, the real‑world web involves _`\"massive document volume, content drift, URL decay\"`_, which motivates InfoDeepSeek. However, this also means that search results and webpage contents can change over time, and thus two researchers running the same agent at different times may obtain different outcomes if sources change or vanish, making results hard to reproduce and compare across time.\n2. **LLM evaluation**: The current accuracy metrics relies on LLM evaluators, which may be fallible or biased, as already seen in the case of false premise queries. This challenge is likely to grow as the benchmark scales to new question types with harder attributes—harder questions are also harder to evaluate for correctness.\n3. **Design of the IC metric**: The IC metric requires human annotated golden set of supportive source webpages $S_q$. What if there are alternative valid sources / webpages, e.g., multiple valid paths to the correct answers? Following W.2, what if some of the sources and webpages are invalid or being modified so that it becomes incorrect over time?\n4. **Subset‑only analyses**: Due to resource constraints, experiments other than the main results reported in Table 3 are conducted on a subset of 245 queries which is understandable. However, this can raise concerns whether the observed trends (e.g., model gaps across attributes or differences among search engines) persist on the full 1032 questions. It would be helpful if the authors can report distributional similarity between the subset and the full set (e.g., question attributes, predominant language, and size of the $S_q$ ).\n5. **All results are reported in a single run**: As the LLMs are implemented with temperature equals to 1 (Sec.E.1), a single run can yield a noisy estimate. Multiple seeds/runs or confidence intervals would better characterize variance.\n6.  **Issue with the reported statistics**:\n    - In Table 3, the IC value for GPT-4o is 0.91 which is significantly different from other LLM's performance and even smaller than 1 with a very low ACC, I suspect this is a typo here.\n    - In Sec. C.4, 1023 high-quality queries -> 1032 high-quality queries.\n    - Table 5, Gemini-2.5-Flash -> Gemini-2.0-Flash."}, "questions": {"value": "Please see the weakness section. In addition, I have the following questions:\n\n1. **Dataset composition (manual vs. agentic)**: Sec. C.1 and Sec. C.2 introduced the manual and agent-assisted pipeline for dataset construction, respectively. However, Sec. C.2 claims that 245 questions are generated by this agentic pipeline. How were the 1032 questions used in the main experiments produced—what fraction came from each pipeline, or does the 245 questions generated by the agentic pipeline are exactly the 245 questions used in the experiment analyses? Are there any quality difference between the dataset generated using these 2 methods?\n2. **Human effort for evaluation on new models**: When evaluating a new LLM on the full benchmark, what fraction of items require human arbitration (e.g., due to LLM‑judge disagreement)? Please report absolute counts and percentages, and—if available—breakdowns by attribute (e.g., false premise, multi‑hop) and by language.\n3. **Interference rate**: When reporting the interference rates in Table 10 and Table 11, is that possible to also include the percentage of the questions that can be directly answer without retrieval (i.e., the denominator of interference rate)? This will be helpful to understand how often this interference phenomenon will appear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8tCvKMYP5k", "forum": "snm8tq7sdI", "replyto": "snm8tq7sdI", "signatures": ["ICLR.cc/2026/Conference/Submission3082/Reviewer_rBcv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3082/Reviewer_rBcv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129031931, "cdate": 1762129031931, "tmdate": 1762916542979, "mdate": 1762916542979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}