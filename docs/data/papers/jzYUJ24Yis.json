{"id": "jzYUJ24Yis", "number": 21021, "cdate": 1758312907577, "mdate": 1763592442985, "content": {"title": "The Role of Stochastic Environments in Enabling Adam", "abstract": "Adaptive optimizers, most notably Adam (and AdamW), are ubiquitous in large-scale first-order training. Yet many theoretical treatments either omit or distort the very features that drive Adam's empirical success, such as momentum and bias correction. Building on recent online-to-nonconvex reductions, we develop a refined discounted-to-nonconvex analysis that respects these features and yields guarantees under a statistically grounded setting. Our key technical contributions are twofold. First, we formalize an online learning with shifting stochastic environments framework that aligns with non-smooth, non-convex stochastic optimization and sharpens how discounted regret translates to optimality conditions. Second, we introduce Adam-FTRL, an online algorithm that exactly matches the plain Adam update in vector form, and prove competitive discounted regret bounds without clipping or unrealistic parameter couplings. Via our conversion, these bounds imply robust convergence of Adam-FTRL to $(\\lambda,\\rho)$-stationary points, achieving the optimal iteration complexity under favorable environmental stochasticity and shift complexity. The analysis further highlights two environment measures: a normalized signal-to-noise ratio (NSNR) and a discounted shift complexity, which govern convergence behavior and help explain the conditions under which Adam attains its theoretical guarantees.", "tldr": "", "keywords": ["Online learning", "regret", "Adam", "nonconvex optimization", "changing environments"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d54c1817dabdce8a73f2df03f3d5230e5721d283.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the discounted-to-nonconvex framework under shifting stochastic environments. Furthermore, it introduces an online learning algorithm, Adam-FTRL, which incorporates both momentum and bias correction mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a detailed theoretical analysis of the proposed methods, particularly the Adam-FTRL algorithm, which integrates both momentum and bias correction mechanisms.\n\n2. In addition, the authors demonstrate optimal iteration complexity for nonconvex optimization."}, "weaknesses": {"value": "Although the paper provides sufficient theoretical analysis and introduces new methods, it lacks numerical experiments to validate their performance.\n\nCompared with some existing works (Li et al., 2024; Wang et al., 2024), the G-Lipschitz assumption may be too strong, and the final results rely on the constants  $c_0$ and $c_1$."}, "questions": {"value": "Despite its strengths, I have some concerns about the novelty of the work. The discounted-to-nonconvex conversion framework has already been extensively studied in prior research (Zhang & Cutkosky, 2024). In this paper, the conversion algorithm closely follows that framework, with the main modification being the reformulation of the online algorithm to recover the original Adam. Furthermore, many of the analytical techniques rely heavily on lemmas from existing works.\n\nCould the authors further clarify their contributions regarding the shifting stochastic environment and the robust convergence of Adam-FTRL? In Definition 4.1, if u_t is set as the average of gradients and v_t as the stochastic estimate, the subsequent assumptions appear fairly standard in stochastic optimization. Am I missing something here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "r0Bq9RWzBL", "forum": "jzYUJ24Yis", "replyto": "jzYUJ24Yis", "signatures": ["ICLR.cc/2026/Conference/Submission21021/Reviewer_ZT1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21021/Reviewer_ZT1z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458781687, "cdate": 1761458781687, "tmdate": 1762940606575, "mdate": 1762940606575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a refined discounted-to-nonconvex analysis for Adam in the context of non-smooth and non-convex optimization. To derive a better regret bound for Adam, the authors propose an online learning with shifting stochastic environments framework. Then, they introduce Adam-FTRL, an online algorithm variant of Adam, which can transfer the non-smooth/non-convex problem to an online learning problem. They further prove the competitive discounted regret bounds without clipping or unrealistic parameter couplings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The major novelty of this paper lies in introducing the shifting stochastic environments framework with some specific structures, and proposing Adam-FTRL without clipping. The former one may help to derive a better regret bound for Adam. The latter one is a more realistic form for Adam.\n\n- The authors further provide some theoretical regret bounds to illustrate the effectiveness of Adam-FTRL."}, "weaknesses": {"value": "I have some concerns as follows.\n\n- The environmental constants $c_0$ and $c_1$ look very strange, without any obvious explanation or intuition. In my view, $c_1$ is the ratio between the mean and covariance of the cost vector ${\\bm v}_t$, while $c_0$, is only a formula dependent on $\\beta_1$, which emerges during the regret analysis. The authors claim that \"The environmental constants $c_0$ and $c_1$ provide valuable theoretical\ninsight into the role of stochasticity and shifting complexity.\", without convincing evidence.\n\n- The whole paper is very similar to [1], particularly the regret analysis. The only major difference, in my view, is to drop the clipping of Adam. However, I do not see any obvious advantages brought by the Adam-FTRL without clipping. In addition, I think dropping the clipping would not essentially affect the regret analysis. If the analysis is essentially different, I suggest that the authors provide a detailed discussion.\n\n- There are also some presentation problems. For example, Lemma 4.5 is established based on using $g_t$, the stochastic gradient, as the cost vector, which is not clearly presented.  \n \n[1] Kwangjun Ahn and Ashok Cutkosky. Adam with model exponential moving average is effective for nonconvex optimization. arXiv preprint arXiv:2405.18199, 2024."}, "questions": {"value": "- What's the major difference between this paper to [1]? If the major difference lies in incorporating the corrective term and dropping the clipping, it would be better to explain the additional challenge brought by these changes.\n\n- Are there any intuition for $c_0$ and $c_1$? In my view, the two terms are just the mathematical formulas. I suggest the authors to provide more explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mM3l1CuIAA", "forum": "jzYUJ24Yis", "replyto": "jzYUJ24Yis", "signatures": ["ICLR.cc/2026/Conference/Submission21021/Reviewer_G6VC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21021/Reviewer_G6VC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653091490, "cdate": 1761653091490, "tmdate": 1762940605825, "mdate": 1762940605825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an analysis of an adam-like algorithm using the online-to-nonconvex framework, providing a guarantee for identifying a certain notion of stationary point. The key contribution appears to be removing a clipping operation present in some algorithms using this framework, as well as allowing for a more practically-relevant choice for beta2 parameter."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "While there are clipping-free methods already using this framework, achieving a good understanding of why beta2 should be set much larger than beta1 is an interesting open question."}, "weaknesses": {"value": "The “piecewise constant stochastic environments” presented here confuse me. Why should I expect this to hold? The authors mention something about a connection to batch size, but I don’t follow this connection at all. I can imagine a scenario in which one might try to assume a slowly varying stochastic environment, but then one is back in the smooth optimization regime we are trying to avoid. \nCan the authors provide a strong justification for why this assumption should be a good model?\nFurther, why should I expect the beta1 value to be such that the convergence rate is actually good rather than large enough to cause a poor convergence rate?\n\nFinally, I do not understand how the beta2 improvement comes about. It’s suspicious to me that beta2’s value does not appear at all in the final convergence guarantee. What’s going on here? Is it really the case as suggested by Lemma 5.1 that any beta2 in the range beta1 to sqrt(beta1) is equivalent? That seems highly unlikely.\n\nIn general, I feel also that this paper presents no concrete *advantage* for beta2 different from beta1^2: at best we seem to be seeing something like things might not be too bad if the environment is sufficiently cooperative."}, "questions": {"value": "see weaknesses. Happy to revise my opinion if they can be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6rVY8mYdjO", "forum": "jzYUJ24Yis", "replyto": "jzYUJ24Yis", "signatures": ["ICLR.cc/2026/Conference/Submission21021/Reviewer_xTgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21021/Reviewer_xTgn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942591106, "cdate": 1761942591106, "tmdate": 1762940604317, "mdate": 1762940604317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a discounted–to–nonconvex analysis for Adam via an online-learning view in shifting stochastic environments. The authors (i) define two environment measures, NSNR and discounted shift complexity, to control noise level and drift, (ii) propose Adam-FTRL, an online learner whose vector update matches Adam with momentum and bias-correction , and (iii) prove discounted-regret bounds that, after conversion, yield convergence to $(\\lambda, \\rho)$-stationary points with the optimal $O(\\epsilon^{-7/2})$ iteration complexity when the environment is favorable (small $c_0, c_1$). They emphasize avoiding clipping or contrived couplings found in some prior analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear to follow, and the shifting-environment setup and the discounted conversion theorem are precisely stated.\n\n- Adam-FTRL’s update matches Adam (vector form) with momentum and bias correction, and the regret bound avoids explicit clipping.\n\n- The discussion effectively highlights and explains the roles of stochasticity (NSNR) and drift (shift complexity)."}, "weaknesses": {"value": "- The hyperparameter constraint is unrealistic and not practical. The key bounded-increment lemma (Lemma 5.1) assumes $\\beta_2\\le \\sqrt{\\beta_1}$. This excludes standard $(\\beta_1, \\beta_2) = (0.99, 0.999)$ and effectively re-introduces the kind of coupling the paper claims to avoid. The regret bound and conversion then inherit this hidden assumption.\n\n- Real Adam (i.e., Adam used in practice) is coordinate-wise, the paper only analyzes the norm version and requires extra assumptions but defers them, limiting external validity of the claims. In addition, a highly related prior work actually provides convergence guarantee for the coordinate-wise (clipped-)Adam [1].\n\n- The paper criticizes prior clipping/couplings, yet replaces them with a different coupling $\\beta_2\\le \\sqrt{\\beta_1}$ to obtain Lemma 5.1. This should be called out as an assumption, not presented as \"no compromises\".\n\n\n[1] Ahn, Kwangjun, and Ashok Cutkosky. \"Adam with model exponential moving average is effective for nonconvex optimization.\" Advances in Neural Information Processing Systems 37 (2024): 94909-94933."}, "questions": {"value": "- Is it possible to remove the $\\beta_2\\le \\sqrt{\\beta_1}$ assumption, or can the bounded-increment argument be redone for standard $(\\beta_1, \\beta_2) = (0.99, 0.999)$? If not, could you quantify how the regret/conversion bound degrades when $\\beta_2$ is larger?\n\n- For extending the current result to coordinate-wise Adam, which exact technical assumptions would be enough to obtain the same regret bound? A brief appendix sketch (or a counterexample) would be very helpful.\n\n- A small experiment with controlled segment lengths and variance (varying $c_0, c_1$) would substantiate the theory’s qualitative predictions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ENQs3UBHOX", "forum": "jzYUJ24Yis", "replyto": "jzYUJ24Yis", "signatures": ["ICLR.cc/2026/Conference/Submission21021/Reviewer_DGUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21021/Reviewer_DGUz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945806943, "cdate": 1761945806943, "tmdate": 1762940603670, "mdate": 1762940603670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}