{"id": "biegtqdqmg", "number": 12442, "cdate": 1758207875157, "mdate": 1759897509665, "content": {"title": "TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction", "abstract": "Historically, neuroscience has progressed by fragmenting into specialized domains, each focusing on isolated modalities, tasks, or brain regions. While fruitful, this approach hinders the development of a unified model of cognition. Here, we introduce TRIBE, the first deep neural network trained to predict brain responses to stimuli across multiple modalities, cortical areas and individuals. By combining the pretrained representations of text, audio and video foundational models and handling their time-evolving nature with a transformer, our model can precisely model the spatial and temporal fMRI responses to videos, achieving the first place in the Algonauts 2025 brain encoding competition with a significant margin over competitors. Ablations show that while unimodal models can reliably predict their corresponding cortical networks (e.g. visual or auditory networks), they are systematically outperformed by our multimodal model in high-level associative cortices. Currently applied to perception and comprehension, our approach paves the way towards building an integrative model of representations in the human brain. Our code is available at \\url{https://anonymous.4open.science/r/algonauts-2025-C63E}.", "tldr": "We introduce a deep learning model to predict brain responses to videos", "keywords": ["brain", "encoding", "multimodal"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/859ddc8036c2cc09785fece1f6e3b6a5b7a9f12d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "TRIBE presents and develops a neural network to predict whole-brain fMRI responses to naturalistic videos through integrated text, audio, and visual processing. The work addresses three critical limitations of prior encoding models: their reliance on linear mappings between AI and brain representations, subject-specific training that cannot leverage cross-individual similarities, and restriction to single modalities. The architecture extracts features from pretrained foundation models (Llama-3.2-3B for text, Wav2Vec-BERT-2.0 for audio, V-JEPA-2 for video), synchronizes them at 2 Hz, compresses layerwise representations, projects to a shared 1024-dimensional space, and processes concatenated multimodal embeddings through an 8-layer transformer with learnable subject and positional embeddings before mapping to 1000 cortical parcels.\n\nTraining on over 80 hours of fMRI per subject from the Courtois NeuroMod dataset, TRIBE achieves first place among 263 teams in the Algonauts 2025 competition, achieving a substantial margin over competitors. All 1000 parcels show significant prediction accuracy, and the model captures 54% of explainable variance on average, exceeding 80% in auditory and language cortices. Performance remains robust on highly out-of-distribution stimuli including animations, nature documentaries, and silent films. Ablations demonstrate that multimodality provides greatest benefits in associative cortices (up to 30% improvement over best unimodal baseline) while vision-only features slightly outperform in primary visual cortex. Both the transformer architecture and multi-subject training prove essential, and scaling analyses show continued improvement with more data and longer linguistic context without saturation.\n\nthe spatial averaging required for computational tractability likely discards retinotopic information critical for low-level visual encoding. Nevertheless, the work represents significant methodological progress by demonstrating that nonlinear integration of foundation model representations within a unified subject-general architecture can effectively predict whole-brain responses to complex naturalistic stimuli, providing a foundation for more comprehensive models of human cognition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The model’s architecture is not a brute-force fusion but a theoretically motivated hierarchy: frozen modality experts (V-JEPA2, Wav2Vec2-BERT, LLaMA3.2) adapted into a shared latent space, followed by a transformer capturing temporal and intersubject alignment. The multisubject encoder and modality dropout mechanisms seem particularly well thought out, improving both biological plausibility and statistical efficiency\n\n2) The multimodal integration analysis is both methodologically clear and scientifically meaningful: ablations show monotonic gains from unimodal to bimodal to trimodal configurations (0.22 → 0.31 Pearson), empirically supporting the hypothesis that integrated sensory embeddings yield richer cortical alignment.\n\n3) TRIBE achieves a normalized Pearson correlation of ~0.54 ± 0.1 across 1,000 parcels, explaining more than half the variance in fMRI responses. The consistency across subjects and cortical regions is noteworthy,"}, "weaknesses": {"value": "1) V-JEPA activations are averaged over patches to manage compute, and the authors themselves expect degradation in low-level, retinotopic cortex. That choice complicates interpreting modality-dominance maps in early vision. It would make the paper stronger if the authors can provide some clarity here.\n\n2) The normalized Pearson metric (Eq. 1, claiming TRIBE captures \"54% of explainable variance\") relies on test-retest reliability (ρself) computed from only two repeated movies: Hidden Figures and Life. While I acknowledge that this follows standard practice in the field, Table 2 reveals substantial performance heterogeneity across genres: raw Pearson correlations span nearly 2× from Friends (0.32) to Charlie Chaplin (0.17). Given that different stimulus classes likely have different signal-to-noise characteristics (e.g., silent films vs. dialogue-heavy sitcoms, etc.), the reliability ceiling may also vary systematically across genres. Without demonstrating that Hidden Figures and Life yield consistent ρself estimates, or that their ceiling generalizes to the broader stimulus distribution, the \"54%\" figure may not accurately represent explainable variance for all content types in the test set. This perhaps limits the interpretation of normalized scores, particularly for out-of-distribution evaluation. A simple robustness check comparing ρself between the two repeated movies, or per-parcel ceiling variability, would  help to strengthen confidence in this metric.\n\n3)  The pipeline downsamples all modalities to 2 Hz, processes via transformer, and pools to TR (1.49s), but nowhere mentions hemodynamic response function (HRF) convolution. The HRF introduces ~5-6 second lag and temporal smoothing between neural activity and BOLD. While the transformer's large receptive field may implicitly learn such dynamics, three concerns arise: (1) downsampling audio from 50 Hz → 2 Hz may discard rapid transients (phonemes, prosody) that could improve prediction when properly lagged/smoothed to TR resolution, (2) interpretability suffers i.e. is the model learning neural dynamics requiring HRF or fitting BOLD directly?, and (3) no ablation (Table 3) tests whether explicit HRF convolution would improve performance. Given that competitors likely vary in temporal modeling choices, this could represent either a hidden performance limiter or an architectural shortcut that the transformer adequately compensates for but the paper provides no evidence to distinguish these possibilities."}, "questions": {"value": "1) Section 2.3 describes layer normalization after projecting each modality to 1024-D. For replicability, please clarify: (1) Are raw embeddings from each foundation model normalized (e.g., z-scored per modality over time) before the linear projection, or do they enter the projection layer unnormalized? Given different embedding dimensions and scales across models, this choice affects how the network learns to balance modalities. \n\n2) The paper tests multiple layer-grouping strategies (Table 3) but does not report which configurations perform best or where. Even a summary-level analysis would be valuable: Do models extracting early vs. late layers from each backbone receive differential ensemble weights in sensory vs. associative regions? I recognize that the 1000-model ensemble with confounded hyperparameters complicates attribution, but even coarse patterns (e.g., \"models grouping early V-JEPA layers achieved higher weights in V1\") would provide interpretability consistent with standard brain encoding practice. If full layer-wise decomposition is infeasible, reporting which layer-grouping strategies from Table 3 achieved highest overall performance can help inform architectural choices for future works.\n\n3) Table 2 shows substantial performance variation across stimulus types (0.32 for Friends vs. 0.17 for Chaplin). While Figure 4a provides modality ablations averaged across validation data, exploring whether multimodal benefits vary by stimulus characteristics could offer additional insight, e.g., whether audio contributes less for silent films. I recognize this may require re-running unimodal models on each test movie and that the limited sample (n=1 per genre) constrains statistical conclusions, but even illustrative case comparisons could be informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oXEBS16pYN", "forum": "biegtqdqmg", "replyto": "biegtqdqmg", "signatures": ["ICLR.cc/2026/Conference/Submission12442/Reviewer_wQpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12442/Reviewer_wQpp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608712862, "cdate": 1761608712862, "tmdate": 1762923327450, "mdate": 1762923327450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRIBE, a deep learning model that non-linearly integrates text, audio, and video features using a transformer to predict whole-brain fMRI responses to naturalistic videos. Its primary strength is its state-of-the-art performance, demonstrated by winning the Algonauts 2025 competition. The work is significant as it simultaneously addresses some key limitations of previous encoding models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this article is very good, as it analyzes the entire brain information from a multimodal perspective. It is of great practical significance and better aligns with the data processing procedures in the era of large models. Therefore, it is also very beneficial for researching more general brain foundation data.\n\n2. The experiments are well-conducted, and the performance is also good. Its 1st-place ranking out of 263 teams in a competitive benchmark is the strongest and most objective evidence for its effectiveness.\n\n3. Besides the model, this paper provides valuable neuroscientific insights, showing that multimodality provides the largest gains in associative cortices and reveals expected unimodal/bimodal dominance in different brain regions."}, "weaknesses": {"value": "1. The article does not disclose or discuss the complexity of the model. To the best of my knowledge, many previous brain decoding projects employed relatively small models. However, the current method employs multiple pre-trained large models, and it should provide the overall size of the model so that others can evaluate and use it.\n\n2. Starting from line 48, the first two motivations actually involve many models that no longer use simple regression. Moreover, numerous recent studies have focused on multi-subject scenarios (e.g.,[1][2][3][4]), and it is essential to accurately describe the current research status .\n\n(In fact, the main weakness has been honestly stated by the authors in the \"limitations\" section.)\n\n[1] Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data\n\n[2] Umbrae: Unified multimodal brain decoding\n\n[3] CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding\n\n[4] Toward Generalizing Visual Brain Decoding to Unseen Subjects"}, "questions": {"value": "1. The article mentions in the limitations section that the number of participants is currently small. Another concern I have is whether the model can be generalized to new subjects as discussed in [1]. If the brain decoding model is used in practice, it is unrealistic for each user to collect a large amount of their own data. With more modalities and increased data volume, can the model demonstrate its generalization ability across different subjects? (For example, a model trained on subjects 1-3 should also work for subject 4.)\n\n2. I also have doubts about the accuracy because this task involves predicting brain activity in response to different stimuli. However, even when the same subjects receive identical stimuli, their brain activity often varies, especially with temporal stimuli like audio and video. Since I lack the relevant background knowledge in neuroscience, I'm unsure if this mapping is reasonable. Nevertheless, this issue is more related to the competition's design. The authors can provide an explanation based on the specific circumstances.\n\n[1] Toward Generalizing Visual Brain Decoding to Unseen Subjects"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0rHpZFhlkE", "forum": "biegtqdqmg", "replyto": "biegtqdqmg", "signatures": ["ICLR.cc/2026/Conference/Submission12442/Reviewer_dPCk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12442/Reviewer_dPCk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758753220, "cdate": 1761758753220, "tmdate": 1762923327116, "mdate": 1762923327116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TRIBE, a model that predicts brain activity while people watch movies, using features from large pretrained models for text, audio, and video. By combining these three types of information and processing them with a transformer, the model can capture how different parts of the brain respond to complex, real-world stimuli. TRIBE achieves first place in a public competition, showing strong results across the whole brain, especially in regions that integrate multiple senses. The work suggests that using multimodal data and deep learning can help build more general models of brain function."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Combines text, audio, and video features for brain prediction, which covers more brain regions than single-modality models.\n2. Achieves clear state-of-the-art results on a public benchmark, outperforming all other competitors by a noticeable margin.\n3. The model design is practical and scalable, using existing large models and a transformer to handle real-world, naturalistic data."}, "weaknesses": {"value": "1. The model relies on combining a large number of models (ensemble), so it’s unclear how well a single model works in practice.\n2. Some details in the paper are inconsistent, for example the feature dimensions in Figure 2 don’t match the numbers in the methods section, and the number of teams is sometimes 262 and sometimes 263."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RLFRzXO0Qp", "forum": "biegtqdqmg", "replyto": "biegtqdqmg", "signatures": ["ICLR.cc/2026/Conference/Submission12442/Reviewer_DyxP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12442/Reviewer_DyxP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762331670133, "cdate": 1762331670133, "tmdate": 1762923326691, "mdate": 1762923326691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}