{"id": "Ly6NP07Rw5", "number": 13429, "cdate": 1758217742724, "mdate": 1759897438085, "content": {"title": "Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model", "abstract": "Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply $v$ loss as the prediction objective on MDM ($v$MDM), where $v$ is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also retarget a large motion dataset to the SMPL skeleton for evaluation. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.", "tldr": "", "keywords": ["motion generation", "diffusion model", "motion representation", "biped animation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed146c1d2d923ea0359f003b53a90d96abead392.pdf", "supplementary_material": "/attachment/3601a5ce82cd43c649058aa2ef05e96734c635fe.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the impact of motion representations and loss functions on human motion generation using diffusion models. It proposes vMDM, an extended version of the Motion Diffusion Model (MDM) that adopts the v loss (a weighted combination of motion data and noise) as the prediction objective, supplemented with geometric losses (position, velocity, foot contact loss). The authors systematically compare six common motion representations (one position-based: JP; five rotation-based: RP6JR, RPQJR, RPAJR, RPEJR, RPMJR) on two datasets (HumanAct12 and retargeted 100STYLE to SMPL skeleton). Key results show that the position-based JP representation outperforms rotation-based ones in vMDM in terms of diversity, fidelity, and training efficiency, while rotation-based representations (e.g., RP6JR) excel in motion stability. Additionally, the integration of v loss balances generation performance and training speed, and temporal smoothing with a Gaussian filter mitigates jitter issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Focuses on fundamental yet under-explored questions (motion representations and loss design) in diffusion-based human motion generation, filling the gap of inconsistent motion representation standards in existing work.\n\n2. Rigorously evaluates six motion representations across quantitative (FID, KID, precision, recall, diversity, smoothness) and qualitative metrics, with sufficient ablation studies on loss functions and temporal smoothing.\n\n3. The proposed vMDM (v loss + geometric loss) achieves better performance than baseline methods (ACTOR, MoDi, MDM) on key metrics, and JP representation reduces training time by avoiding complex forward kinematics computations.\n\n4. Retargets the large-scale 100STYLE dataset to the SMPL skeleton, verifying the framework’s effectiveness across datasets with limited and extensive motion ranges."}, "weaknesses": {"value": "1. vMDM performs poorly with rotation-based representations without additional data cleaning, and the paper fails to propose targeted improvements for this limitation.\n\n2.  Position-based JP representation suffers from keyframe popping, while rotation-based ones face freezing/drifting; temporal smoothing (Gaussian filter) is a post-processing patch rather than a fundamental solution.\n\n3. Only compares with three baseline methods, missing recent state-of-the-art diffusion-based motion generation models. Evaluation metrics lack assessments of motion dynamics and physical plausibility (e.g., balance, momentum conservation).\n\n4. The weight setting of the v loss (ω(t)=ᾱₜ) and geometric loss components lacks detailed justification; the impact of hyperparameters (e.g., diffusion time steps, Transformer structure) on different motion representations is not discussed."}, "questions": {"value": "1. What is the theoretical or empirical basis for choosing ω(t)=ᾱₜ as the weight of the v loss? Have you tested other weight functions (e.g., adaptive weights based on motion types) and their impacts?\n\n2. Why do rotation-based representations introduce more noisy points (as shown in feature heatmaps)? Is it related to data preprocessing, representation inherent properties, or model architecture?\n\n3. How does vMDM perform on datasets with more complex motions (e.g., acrobatics, interactive motions) beyond locomotion and simple actions? Can JP representation maintain its advantages in such scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tbIw7gGQNr", "forum": "Ly6NP07Rw5", "replyto": "Ly6NP07Rw5", "signatures": ["ICLR.cc/2026/Conference/Submission13429/Reviewer_Dy62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13429/Reviewer_Dy62"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914389092, "cdate": 1761914389092, "tmdate": 1762924055901, "mdate": 1762924055901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors revisit core design choices for diffusion-based human motion generation, examining how motion representation and objective choice affect quality, diversity, and efficiency. Building on MDM, they adopt a v-parameterized objective within a controlled architecture and systematically compare six representations—joint positions and five rotation parameterizations—under a unified training/evaluation setup on HumanAct12 and a 100STYLE-to-SMPL benchmark. The main result is that JP with vMDM achieves the best overall FID/KID and recall, while rotation features yield smoother transitions but more artifacts. The study further includes ablations on representation choice, loss design, and compute, showing notable training-time gains for vMDM and additional speedups with JP by avoiding forward kinematics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript tackles the impact of motion representation on diffusion-based motion generation—a meaningful question that can inform and inspire subsequent research.\n2. The manuscript provides thorough quantitative and qualitative analyses across multiple motion representations, offering clear empirical evidence to support the study’s conclusions."}, "weaknesses": {"value": "1. **Limited methodological breadth**. Experiments are confined to MDM, without evaluating other motion-generation methods (e.g., VAE- or autoregressive-based models, or more recent architectures). This narrow scope limits the generality and external validity of the conclusions.\n2. **No study of combined representations**. Each representation is trained and tested in isolation. As noted by the authors, prior work and datasets (e.g., HumanML3D [1]) often combine permutations of joint positions and joint rotations to exploit complementary cues. Without exploring combined representations, the study cannot assess potential synergies and its practical meaning is reduced.\n3. **Dataset scope is narrow**. Results are reported only on 100STYLE and HumanAct12, omitting broader and widely used benchmarks such as AMASS [2] and the HumanML3D [1] used by MDM. This limits coverage, motion diversity, comparability to prior work, and cross-dataset generalization, weakening the strength of the empirical claims.\n\n> [1] Guo, Chuan, et al. \"Generating diverse and natural 3d human motions from text.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n> [2] Mahmood, Naureen, et al. \"AMASS: Archive of motion capture as surface shapes.\" *Proceedings of the IEEE/CVF international conference on computer vision*. 2019."}, "questions": {"value": "The results in this manuscript report that joint positions produce more coherent and natural motion. However, many downstream applications require SMPL parameters, and converting JP to SMPL adds a post-processing/IK step that is time-consuming and can introduce rotation errors. Prior work (e.g., MotionStreamer [3]) therefore combines joint positions and rotations. How do you view this trade-off in practice when choosing motion representations.\n\n> [3] Xiao, Lixing, et al. \"MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space.\" *arXiv preprint arXiv:2503.15451*(2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M5p5MqZQor", "forum": "Ly6NP07Rw5", "replyto": "Ly6NP07Rw5", "signatures": ["ICLR.cc/2026/Conference/Submission13429/Reviewer_j864"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13429/Reviewer_j864"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931699336, "cdate": 1761931699336, "tmdate": 1762924055240, "mdate": 1762924055240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fundamental in human motion generation, the representation itself. The authors take the MDM architecture and train various alternatives with different representations and with x0 and v prediction. In specific, they compare six motion representations — joint positions,6D rotations + root, quaternion + root, axis-angle + root, euler + root, rotation matrices + root. Experiments are conducted on two different datasets, HumanACT12 and 100STYLE, using quantitative metrics common in the motion generation field (FID, R-precision, diversity, etc). Overall, the authors argue that representation JP and v-loss surpass other qualities, stability, and efficiency in motion diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Modern advancements in the motion generation field are mostly at the architectural level; however, representation itself is the fundamental problem as well as the diffusion objective. As a researcher in this field, I agree with the author's motivation for the paper.\n2. Various comparisons across six representations are conducted on a controlled architecture, and the results follow intuition and motivation."}, "weaknesses": {"value": "1. Though I admire the motivation and respect the authors’ effort to explore representation-level questions in motion generation. As a researcher in this field, **I sincerely believe it is also important to acknowledge prior work that has already addressed many of these issues**. Several earlier papers have studied these choices and reported similar or stronger findings before this submission.\n\nMLD, MotionLCM, and MotionStreamer have shown that latent spaces are a much better representation to model than raw motion representations (for both x0 and noise prediction).\n\nSALAD, MARDM, and  ACMDM have shown that v prediction is better than x0 prediction and noise prediction in both latent and raw motion data.\n\nACMDM shows that absolute joint positions are better than other representations for various motion generation tasks, even with the simplest architecture.\n\nSALAD and ACMDM show that condition injection is also an important factor in the generation quality.\n\nAnd above method also conducts experiments in the standard benchmark in HumanML3D and KIT-ML datasets.\n\n2. There are now many larger, more diverse benchmarks for human motion generation, and HumanML3D in particular has long became the default dataset for evaluating text-conditioned motion models in recent work. Without results on a standard benchmark like HumanML3D, the general validity of the experimental conclusions in this paper is difficult to assess. Especially as MDM is considered a quite old method, with many more methods that surpass MDM.\n\n3. The paper claims to retarget 100STYLE for evaluation; however, prior work (e.g., SMooDi) has already performed 100STYLE retargeting to common skeleton formats more than a year ago.\n\n4. While representation choice and the use of v-prediction are important, motion generation quality is also strongly affected by how conditioning is injected. This is left out in the paper, while previous works such as SALAD, ACMDM have proven that cross attention and AdaLN are much better than in-context learning used in MDM.\n\n5. The visual results of the overall best method: v prediction and JP still exhibit heavy floating and jittering, shown in the supplemental video.\n\n[HumanML3D]Guo, Chuan, et al. \"Generating Diverse and Natural 3D Human Motions from Text.\" CVPR 2022\n\n[KIT-ML]Plappert, Matthias, et al. \"The kit motion-language dataset.\" Big data 2016.\n\n[MLD]Chen, Xin, et al. \"Executing your commands via motion diffusion in latent space.\" CVPR 2023.\n\n[MotionLCM]Dai, Wenxun, et al. \"Motionlcm: Real-time controllable motion generation via latent consistency model.\" ECCV 2024\n\n[MotionStreamer]Xiao, Lixing, et al. \"MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space.\" ICCV 2025.\n\n[SALAD]Hong, Seokhyeon, et al. \"SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing.\" CVPR 2025\n\n[MARDM]Meng, Zichong, et al. \"Rethinking Diffusion for Text-Driven Human Motion Generation.\" CVPR 2025.\n\n[ACMDM]Meng, Zichong, et al. \"Absolute Coordinates Make Motion Generation Easy.\" ArXiv 2025\n\n[SmooDi]Zhong, Lei, et al. \"Smoodi: Stylized motion diffusion model.\" ECCV 2024"}, "questions": {"value": "1. Why are FIDs so high in all tables in the experiment section? May I ask what the ground truth FID is? The high level of FID seems unreasonable and may lose the meaning of comparison.\n2. Many work (MLD, MotionLCM, MotionLCM v2, MARDM, MotionStreamer, ACMDM) show that latent space modeling may potentially have different results than raw motion modeling, and is quickly becoming the default choice. Would the author also include a latent space comparison to make the experiment complete?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lFYlDTrBzR", "forum": "Ly6NP07Rw5", "replyto": "Ly6NP07Rw5", "signatures": ["ICLR.cc/2026/Conference/Submission13429/Reviewer_Dxar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13429/Reviewer_Dxar"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934022209, "cdate": 1761934022209, "tmdate": 1762924054747, "mdate": 1762924054747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits motion representation for diffusion-based motion generation: starting from MDM/vMDM, they plug in six encodings (JP, RP6JR, RPQJR, RPAJR, RPEJR, RPMJR) and run a series of comparisons on HumanAct12 and 100STYLE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clean ablation of six motion representations within one diffusion backbone (MDM/vMDM) with standard metrics (FID/KID/precision/recall/diversity).\n\n2. Clear empirical takeaway in their setup: under vMDM, JP outperforms rotation-based reps and is faster to train.\n\n3. Practical notes on training/inference that practitioners can immediately try."}, "weaknesses": {"value": "1. The general question “which motion representation is easier to learn for diffusion models” is not new. For example, MARDM [1] discusses redundant motion representations for training VQ-based vs. diffusion-based models. MotionStreamer [2] proposes a 272-D motion representation to remove post-processing that is required for animation. InterGen [3] introduces a representation tailored for two-person interactions. ACMDM [4] shows that absolute/global joint coordinates improve motion fidelity and text alignment, which is basically the same finding here (JP > others in diversity/fidelity/efficiency). Compared to this thread, the paper is incremental, it should explicitly acknowledge these works and clarify how its findings differ or inspire new directions.\n\n2. All experiments are on HumanAct12 and 100STYLE. These are small-scale. I don’t see why the authors didn’t evaluate on widely used benchmarks like HumanML3D or KIT-ML, or newer/larger datasets such as SnapMoGen. Relying on small datasets makes the analysis less convincing.\n\n3. No statistical variation (mean ± std across seeds) is reported.\n\n4. As a benchmark/analysis paper, even though results are given, I still don’t know why one representation is better than another. The paper should add more experiments and analysis that can explain the reasons.\n\nReferences:\n\n[1] Rethinking Diffusion for Text-Driven Human Motion Generation\n\n[2] MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space\n\n[3] InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions\n\n[4] Absolute Coordinates Make Motion Generation Easy"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LL9CUgzQqO", "forum": "Ly6NP07Rw5", "replyto": "Ly6NP07Rw5", "signatures": ["ICLR.cc/2026/Conference/Submission13429/Reviewer_PvFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13429/Reviewer_PvFL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974956073, "cdate": 1761974956073, "tmdate": 1762924052612, "mdate": 1762924052612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}