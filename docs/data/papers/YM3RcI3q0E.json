{"id": "YM3RcI3q0E", "number": 10630, "cdate": 1758178126132, "mdate": 1759897639368, "content": {"title": "Hallucination Reduction with CASAL:  Contrastive Activation Steering for Amortized Learning", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by $\\sim30\\%$-$40 \\%$ across multiple short-form QA benchmarks. CASAL is $\\sim$30x more compute-efficient and $\\sim$20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.", "tldr": "Amortized activation steering effectively reduce hallucination in text-o multi-modal and Mixture-of-Expert models.", "keywords": ["hallucination", "representation learning", "interpretability", "finetuning", "steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2cb5f362a971db80966f78ba12bc503ef606fc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model’s weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Introducing a training method inspired by interpretability findings and amortized optimization. \n2. Efficiency Gains: CASAL’s objective function enables local and lightweight parameter updates.\n3. Robust Generalization: The trained model retains its general capabilities while avoiding excessive refusals.\n4. Versatility: CASAL training is modality-agnostic, effectively mitigating hallucination in both text-only and multimodal models.\n5. Broad Applicability: present the first ever steering-based training framework with general applicability to both dense and Mixture-of-Experts (MoE) models."}, "weaknesses": {"value": "1. It's great that you've shown CASAL doesn't hurt scores on benchmarks like MMLU. I'm wondering if making it more truthful also makes it more conservative or robotic in its answers. Do you have any examples that show what its answering style is like after applying CASAL?\n2. In long-form content like stories, reports, or code, hallucinations can be sneakier—like mixing a few wrong facts into a long, mostly correct text. How well does CASAL handle these kinds of subtle, long-form hallucinations? \n3. What makes CASAL better than other methods for stopping hallucinations? Also, can it be used together with other popular techniques?"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z3SE2sD8Vk", "forum": "YM3RcI3q0E", "replyto": "YM3RcI3q0E", "signatures": ["ICLR.cc/2026/Conference/Submission10630/Reviewer_Th4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10630/Reviewer_Th4B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640686270, "cdate": 1761640686270, "tmdate": 1762921887769, "mdate": 1762921887769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CASAL (Contrastive Activation Steering for Amortized Learning), a novel training framework that integrates interpretability and amortized optimization to mitigate hallucinations in large language models (LLMs). Unlike prior steering-based methods that require inference-time interventions, CASAL embeds “knowledge boundary” awareness directly into model weights by amortizing the activation-steering process. The method fine-tunes a lightweight submodule within a single transformer layer using a local representation loss derived from residual stream activations. CASAL achieves a 30–40% hallucination reduction, is 30× more compute-efficient and 20× more data-efficient than strong baselines such as LoRA-based SFT and DPO, and generalizes across dense, Mixture-of-Experts (MoE), and vision-language models"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual novelty. CASAL introduces the first amortized activation-steering framework, combining interpretability insights with end-to-end optimization. It moves beyond inference-time steering toward embedding factual awareness into model parameters.\n2. Efficiency and simplicity. The approach modifies only a small submodule within one layer and requires minimal additional training cost, offering a practical balance between effectiveness and efficiency.\n3. Strong empirical results. Experiments on multiple QA benchmarks demonstrate substantial reductions in hallucination without significant performance drops. The improvements are consistent across architectures and modalities.\n4. Theoretical and practical value. By grounding activation-space interpretability in a training objective, CASAL provides a fresh paradigm for factual reliability and confidence calibration in LLMs."}, "weaknesses": {"value": "1. Narrow evaluation scope. The experiments are primarily on short-form QA tasks. The method’s performance in long-form generation or reasoning-heavy domains remains untested.\n2. Limited ablation on architectural choices. The effect of which layer or module is selected for amortization is not deeply explored, leaving open questions about optimal layer placement.\n3. Unclear stability at large scales. It is not evident whether the method remains stable and effective for models beyond 70B parameters or in multi-turn conversational settings."}, "questions": {"value": "1. How sensitive is CASAL to the selection of the target layer and the steering coefficient?\n2. Does repeated amortization across layers risk reducing factual calibration?\n3. Can the amortized module be transferred between models of different sizes?\n4. How does CASAL behave in multi-turn conversations or retrieval-augmented contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1l9WB9QqB", "forum": "YM3RcI3q0E", "replyto": "YM3RcI3q0E", "signatures": ["ICLR.cc/2026/Conference/Submission10630/Reviewer_FqvX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10630/Reviewer_FqvX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710763470, "cdate": 1761710763470, "tmdate": 1762921887437, "mdate": 1762921887437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a resource efficient fine tuning technique to mitigate hallucinations in LLMs. The main idea of the paper is to steer the activations directly though the model weights to avoid hallucinations. This is achieved by labeling the questions as known and unknown and using these labels to estimate the steering vector and target activations. Finally, a submodule of LLM is finetuned by minimizing the MSE loss between current and desired activations. The proposed method achieves 30-40% reduction in hallucination and also has been show to 30X more compute efficient and 20X data efficient than LORA and DPO based techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is easy to read and well presented. The evaluation of the propoped method is exhaustive.\n- The idea of amortized activation steering is interesting and enables model alignment via scalable finetuning pipeline.\n- The computational and data efficiency of the proposed method is of significant value.\n- The method has also been further evaluated on MoEs and VLMs, noting its broad applicability."}, "weaknesses": {"value": "- The current evaluation is mainly focused on the short form QA and does not show impact on the complex reasoning tasks and open ended generations.\n- Its not very clear how the steering vector magnitude impacts the generalization capabilities while allowing activation steering mechanism. \n- The current method does not explicitly ensure that learned knowledge boundary does not degrade non-knowledge functionality of the submodule of MLP layer.\n- The steering vectors are estimated from models internal knowledge base. It might reinforce the biases inherent in internal knowledge via activation steering."}, "questions": {"value": "- How is the impact of threshold (chosen as 7) in probing labels known and unknown and ultimately steering vectors?\n- Can the authors apply the method on reasoning tasks or long form QA? This would make the paper stronger.\n- How is the rank of update matrix in submodule chosen?\n- Did the authors explore any other losses for representation alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jHInzbEN1m", "forum": "YM3RcI3q0E", "replyto": "YM3RcI3q0E", "signatures": ["ICLR.cc/2026/Conference/Submission10630/Reviewer_rR6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10630/Reviewer_rR6p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899422808, "cdate": 1761899422808, "tmdate": 1762921887104, "mdate": 1762921887104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Contrastive Activation Steering for Amortized Learning (CASAL), a method that enables large language models to answer questions they know while abstaining from those they do not. The proposed approach requires lightweight training, is claimed to be both efficient and effective, and is evaluated on language-only and vision-language benchmarks to reduce hallucination rates without significantly increasing unnecessary refusals. The authors also claim that this is the first steering-based training method shown to be effective for both dense and MoE/sparse setups."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and easy to follow. It is also detailed and comprehensive.\n\n* The experimental setup is fair and addresses several practical aspects of the solution, such as efficiency and preserving the original model’s capabilities on general tasks while reducing hallucinations. I particularly appreciate that the results are supported with confidence intervals.\n\n* To assess the preservation of the original model’s general capabilities, the authors evaluated it on various setups, including general knowledge, reasoning, and coherence in multi-turn conversations. The results indicate that the models maintain overall performance comparable to the baseline.\n\n* The proposed approach is evaluated in both the language and the vision-language domains, and architecture-wise covers LLMs, multimodal LLMs, and MoEs."}, "weaknesses": {"value": "* It would be interesting to also compare against online RL algorithms in this setup, e.g., PPO or GRPO.\n\n* In Tables 1 and 5, could you add reference performances from models like GPT-5 and Gemini 2 to provide a sense of the SOTA performance? Otherwise, the improvements are marginal in many cases with quite high standard deviations, so it is difficult to gauge if the improvements are significant.\n\n* In fact, you can do a significance test to confirm whether the improvements are statistically significant or not, using the standard error at a 95% confidence level.\n\n* It would be nice to evaluate the proposed method on at least two more LLMs of different sizes, such as Qwen 3, GPT-OSS, of varying sizes. Evaluating only on LLaMA 3.1 8B makes the setup weaker.\n\n* For vision-language tasks, it would be nice to see how other baselines perform, e.g., DPO, SFT, or GRPO.\n\n* WorldCuisines-VQA is not a hallucination-related dataset. Could you please discuss how you are testing hallucination with it? \n\n* l.252: *“We compute Silhouette score as a measure of cluster separation.”* — Is this a standard technique in the related literature to understand the decision boundaries between known and unknown queries? If so, could you please share some references? Also, how does the decision boundary compare between DPO/SFT and CASAL?\n\n* Based on Fig. 2, adding more data beyond 1280 does not lower the hallucination rate. Does this mean the method does not scale with more data? Is this because CASAL trains a lightweight one-layer model, which may saturate quickly?\n\n* How are the standard deviations calculated in the tables? \n\n* Also, when you are making the results bold, you should bold those numbers that are the best, considering the base model. If the base model is the best, then you should bold that instead of the applied methods. Otherwise, it is misleading the readers, see Table 1.\n\n* In Table 1, there is a sharp drop (6%) in performance on POPQA -- can you explain why there is such a drop when applying CASAL? DPO seems to be performing better. In fact, you are deteriorating in both refusal rate and accuracy on POPQA.\n\n* It seems the refusal rate increased and accuracy dropped after CASAL in OOD generalization, as per Tables 3 and 4. Can you comment on that? Also, it would be great to have some reference points here with DPO, GRPO, SFT, etc.\n\n* What does “Before CASAL” refer to? Is it simply the base model? In tables where only “Before CASAL” and “After CASAL” are reported, I strongly recommend including additional reference points such as DPO, SFT, and GRPO. Otherwise, it is difficult to interpret the results. For example, in Table 5, training with WorldCuisine-QA using a specific loss compared to the base Qwen 2.5-VL model may show some improvement, but it does not convey much without comparisons to other loss functions.\n\n* The appendix formatting seems to be problematic in some places. Please check p.46 (blank page) and p.47 (overflow)."}, "questions": {"value": "Please see weaknesses above; I remain open to adjusting my scores pending a convincing rebuttal addressing the identified weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7vyy2ctzow", "forum": "YM3RcI3q0E", "replyto": "YM3RcI3q0E", "signatures": ["ICLR.cc/2026/Conference/Submission10630/Reviewer_CQ6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10630/Reviewer_CQ6A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029933817, "cdate": 1762029933817, "tmdate": 1762921886738, "mdate": 1762921886738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}