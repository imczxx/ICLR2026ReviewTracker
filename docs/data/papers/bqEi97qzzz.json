{"id": "bqEi97qzzz", "number": 16547, "cdate": 1758265886452, "mdate": 1759897233861, "content": {"title": "RepSpec: Structural Re-parameterized Draft Model Training for Speculative Decoding", "abstract": "As the parameter size of large language models (LLMs) continues to grow, the latency of autoregressive inference increases due to memory-bound computational inefficiency. To address this, speculative decoding has been proposed, where a large target model verifies multiple tokens generated in parallel by a smaller draft model. However, the performance of speculative decoding is fundamentally limited by the draft model’s capacity, which stems from the parameter gap between the two models. To overcome this limitation, we propose RepSpec, which combines structural re-parameterization with draft model training. During training, redundant linear structures are introduced and later merged into the backbone network during inference, thus enhancing the draft model’s training effectiveness without increasing inference cost. By applying our method to improve the current state-of-the-art approach, EAGLE, we achieve a significant improvement in accepted sequence length. Furthermore, considering the specific characteristics of the speculative decoding scenario, we explore a hybrid training strategy that combines linear and nonlinear structures, which yields a further improvement in acceptance length.", "tldr": "We introduce RepSpec, a training method for speculative decoding that uses structural re-parameterization to temporarily expand the draft model’s capacity during training—without adding inference cost.", "keywords": ["Reparameterization", "Speculative Decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3c44b0cc1f2e503502304ade05138570c0d4764.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper adapts structural re-parameterization to draft model training for speculative decoding. While the idea is technically sound, the work suffers from marginal gains, limited novelty, and insufficient practical impact. Below are the key criticisms supporting rejection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 The paper presents a well-motivated adaptation of structural re-parameterization techniques to the emerging domain of speculative decoding. While this technique has been widely used in convolutional networks, its application to draft model training in autoregressive decoding is timely and relevant. \n\n2 The method effectively decouples training-time complexity from inference-time efficiency, maintaining the lightweight nature of draft models while enhancing their capacity during training.t"}, "weaknesses": {"value": "1 The work directly adapts structural re-parameterization—a well-established technique in convolutional networks—to draft model training without conceptual innovation. The hybrid variant introduces non-mergeable nonlinear components but fails to justify the increased inference costs, contradicting the low-latency objective of speculative decoding.\n\n2 Performance improvements are modest: pure linear re-parameterization improves acceptance length by only 7–10% on LLaMA-8B, while the hybrid method incurs additional latency.\n\n3 Experiments are confined to small models (≤13B) and academic benchmarks. The paper lacks comparisons to larger models."}, "questions": {"value": "The paper focuses on comparing with EAGLE, Medusa, and Hydra, but how does RepSpec perform against other draft model optimization strategies such as knowledge distillation or dynamic architecture methods? Are there scenarios where training-free approaches (e.g., self-speculative decoding) might be more practical despite potentially shorter acceptance lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YxxxZxfl7k", "forum": "bqEi97qzzz", "replyto": "bqEi97qzzz", "signatures": ["ICLR.cc/2026/Conference/Submission16547/Reviewer_15fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16547/Reviewer_15fy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761143549019, "cdate": 1761143549019, "tmdate": 1762926627664, "mdate": 1762926627664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structured re-parameterization method, REPSpec, which enhances the training of draft models by introducing additional layers during training and merging them during inference. Extensive experimental results demonstrate that the proposed method significantly improves the performance of existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The application of structured re-parameterization to the field of speculative decoding aligns well with the specific requirements of draft models.\n\n2. Extensive experiments have been conducted to explore the effectiveness of different architectural designs."}, "weaknesses": {"value": "1. The proposed method introduces a significant increase in training overhead.\n\n2. Although it improves the acceptance rate, the hybrid approach also incurs additional inference costs, resulting in limited overall end-to-end acceleration."}, "questions": {"value": "1. Although Appendix A provides some implementation details, the specific placement and strategy for incorporating nonlinear factors remain somewhat unclear.\n\n2. If my understanding is correct, is there a fundamental difference between introducing unmergeable nonlinear factors and directly increasing the size of the draft model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBiljL0mCA", "forum": "bqEi97qzzz", "replyto": "bqEi97qzzz", "signatures": ["ICLR.cc/2026/Conference/Submission16547/Reviewer_XtQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16547/Reviewer_XtQb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909506575, "cdate": 1761909506575, "tmdate": 1762926627287, "mdate": 1762926627287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes RepSpec, a method of training draft models in speculative decoding using structural re-parameterization. This method enhances the draft model's capacity during training without increasing its inference cost. The core idea is to augment the draft model's linear layers with redundant, mergeable branches (Pre, Post, Bypass) during training, which are then fused into a single layer for inference. Furthermore, the authors introduce a hybrid method that incorporates a minimal, non-mergeable nonlinear component, justified by the fact that the draft model's inference time is a small fraction of the total SD latency. Experiments on various SD methods (EAGLE, Medusa, Hydra) and LLMs (LLaMA-3.1-8B, LLaMA-2-13B, Vicuna-7B) demonstrate that RepSpec consistently improves the accepted sequence length and end-to-end decoding speed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The article applies the structural re-parameterization techniques previously used in convolutional neural networks to the training of the draft model for speculative decoding, perfectly adapting to the characteristics of the draft model that are insensitive to training costs and sensitive to inference costs.\n2. The experimental results fully demonstrate the effectiveness of its method, including end-to-end acceleration and draft acceptance rate.\n3. This method has certain value in practical applications."}, "weaknesses": {"value": "1. Limited Theoretical Insight: The paper provides a solid empirical foundation but offers limited theoretical analysis of why the re-parameterization helps in this specific context (beyond general optimization benefits). The discussion in Appendix E is a good start but could be more integrated.\n2. Training cost (minor): Although the draft model is not very sensitive to training costs (as mentioned above), it still presents certain challenges in resource limited scenarios, especially when the base model is large."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eVFdfA6F0E", "forum": "bqEi97qzzz", "replyto": "bqEi97qzzz", "signatures": ["ICLR.cc/2026/Conference/Submission16547/Reviewer_Vyrc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16547/Reviewer_Vyrc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916687592, "cdate": 1761916687592, "tmdate": 1762926626853, "mdate": 1762926626853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RepSpec, a training framework to fix the key bottleneck in speculative decoding: the draft model's weak capacity. The core idea is structural re-parameterization. During training, authors expand the draft model by adding mergeable linear structures (like Pre and Bypass layers) to boost its capacity. At inference time, these structures are merged back into the original layers, resulting in zero additional inference cost. This ``train-large, infer-small\" method improves the draft model's effectiveness, leading to better acceptance lengths and overall speedup. A Hybrid version also adds non-linear blocks for a minimal cost, which pays off on larger target models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of merging the linear part of the model architecture during inference is novel and motivated. \n\n- The results shows speedups compared to the SOTA EAGLE. For example, there is the 7.3% improvement over EAGLE1 on LLaMA-3.1 8B (Table 1, T=0). The ablation studies are comprehensive."}, "weaknesses": {"value": "- Pure linear method gives limited gains. Though the bypass path may make the training more effective, there is a performance ceiling for that. The ceiling can be related to the model size. The paper's own results show that while this method works well on the 8B model, the \"Hybrid\" method outperforms it on the larger 13B model. This implies that the zero-cost benefit comes with a performance ceiling that the authors themselves had to address with the costlier hybrid variant.\n\n- The benefits are not as simple as adding more layers. Will adding too much linear re-parameterization can actually degrade training performance? This brings up the question that whether the re-parameterization structure is a sensitive hyperparameter that must be carefully tuned, rather than a simple, scalable fix.\n\n- The training overhead is also a concern. The required training GPU memory increases and the training speed is also reduced.\n\n- How about larger models?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E4hqJygoIx", "forum": "bqEi97qzzz", "replyto": "bqEi97qzzz", "signatures": ["ICLR.cc/2026/Conference/Submission16547/Reviewer_cX35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16547/Reviewer_cX35"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975929671, "cdate": 1761975929671, "tmdate": 1762926626389, "mdate": 1762926626389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}