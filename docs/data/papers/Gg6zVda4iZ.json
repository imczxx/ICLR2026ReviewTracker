{"id": "Gg6zVda4iZ", "number": 20740, "cdate": 1758309536671, "mdate": 1759896960944, "content": {"title": "Enabling Tool Use of Reasoning Models Without Verifiable Reward via SFT-RL Loop", "abstract": "Large reasoning models have shown remarkable capabilities, but their internal knowledge is limited, restricting their ability to solve complex tasks. An attractive solution is to integrate external tools—such as Python for math reasoning or search engines for knowledge-intensive queries. Yet, teaching models to use tools effectively remains a significant challenge. Existing approaches often depend on reinforcement learning (RL) with accuracy-based verifiable rewards or cold-start pipelines that perform supervised fine-tuning (SFT) followed by an RL stage. These methods are shown to be notoriously unstable, prone to entropy collapse or convergence to suboptimal behaviors. The problem is compounded in real-world tool-use scenarios where accuracy signals are either unavailable or unverifiable. To address this, we propose $\\texttt{SR-Loop}$, a general training framework that alternates between SFT and RL phases without relying on accuracy-based rewards in the RL stage. The SFT phase preserves output structure and constrains harmful exploration by imitating expert demonstrations, while the RL phase encourages discovery of new behaviors and improvements beyond the initial policy. By repeatedly cycling between these phases, $\\texttt{SR-Loop}$ achieves stable learning and progressively enhances tool-use capabilities using only structural and execution-based rewards. Experiments show that $\\texttt{SR-Loop}$ not only prevents training collapse but also delivers competitive performance on complex tool-use reasoning tasks—without requiring explicit accuracy supervision during RL. Moreover, the framework generalizes beyond tool use, proving effective for training general reasoning models even in settings without external tools.", "tldr": "", "keywords": ["Large Reasoning Models", "Tool Use", "Reinforcement Learning", "Post-Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ca67ceed76207273bb57d9dc64f0ce06c209123.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the severe instability that occurs when training large reasoning models to use tools, particularly during reinforcement learning (RL). The authors propose an alternating training approach between supervised SFT and RL. SFT stabilizes learning, preserves output structure, and constrains harmful exploration. RL encourages exploration while ensuring improvement over the fine-tuned policy. During RL, the authors train the models' tool use capabilities using Format Reward and Tool Use Reward rather than ground-truth Accuracy reward. They believe this allows the model to autonomously explore and discover improved strategies while maintaining valid and structured outputs. Their experimental results show that relying solely on auxiliary reward signals during RL can bring substantial accuracy improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors attempt to combine SFT and RL, leveraging the strengths of both to address the instability problem that exists in tool RL training, which is an exploration of a new approach."}, "weaknesses": {"value": "1. In Section 3, the baselines used for comparison do not seem entirely fair — most of them do not use tools, such as LUFFY. Moreover, the paper does not compare against other works related to Tool-Integrated Reasoning, such as SimpleTIR [1] or ZeroTIR [2].\n\n2. From the experimental results (e.g., Table 1), it appears that adding an accuracy reward constraint yields more stable results than omitting it. Therefore, the authors’ advocacy for training without ground-truth accuracy rewards does not seem strongly justified.\n\n3. Although the authors treat SFT and RL as one complete cycle of alternating SFT and RL phases within an epoch, it is unclear how much each contributes to the overall accuracy. For instance, in Figure 4, if both phases take up half of the training steps, the RL phase seems to contribute very little to accuracy improvement.\n\n4. Again, from Figure 4, we can see that SR-Loop has relatively few training steps overall, and accuracy gradually converges toward the end. This makes it difficult to determine whether, in long-term training, omitting supervision from ground-truth accuracy rewards might lead to “hacking” behavior.\n\n[1] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning\n[2] Agent RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving"}, "questions": {"value": "1. Could the authors include additional Tool-Integrated Reasoning–related baselines in the experiments, such as SimpleTIR?\n\n2. It would be helpful to see how SFT and RL each contribute to the model’s performance. This could support the authors’ claim that “SFT serves to stabilize learning, preserve output structure, and constrain harmful exploration, while RL encourages exploration that improves upon the fine-tuned policy.”\n\n3. Could the authors observe the changes in reward and accuracy of SR-Loop training over a longer training period?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jDqyvPjDKo", "forum": "Gg6zVda4iZ", "replyto": "Gg6zVda4iZ", "signatures": ["ICLR.cc/2026/Conference/Submission20740/Reviewer_cUV8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20740/Reviewer_cUV8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839126089, "cdate": 1761839126089, "tmdate": 1762934162143, "mdate": 1762934162143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SR-Loop, an iterative SFT→RL training scheme for tool-augmented reasoning that do not need to use accuracy-based rewards during RL."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Achieves competitive results without explicit accuracy rewards."}, "weaknesses": {"value": "1. The method extends two-stage SFT+RL with multi iteration training, but do not discuss about the training stability on the loop schedule.\n2. The only tool evaluated is code, breadth beyond code execution is unclear.\n3. The reward design is under-specified, details for terms such as the reasoning–tool balance are missing."}, "questions": {"value": "1. How are the tool-use reward and the reasoning–tool balance computed?\n2. Does the loop expose the policy to more on-policy data, and how do you separate method gains from extra data?\n3. What is the rationale for the chosen reward weights, and can you provide ablations to verify each reward’s effectiveness and show which one contribute most?\n4. With training/eval confined to a code sandbox, do the results generalize to multi-tool scenarios?\n5. Adding an explicit accuracy reward (SRL w/ acc) improves performance, where does it help most, and at what weight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iquMwsCSvp", "forum": "Gg6zVda4iZ", "replyto": "Gg6zVda4iZ", "signatures": ["ICLR.cc/2026/Conference/Submission20740/Reviewer_ynSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20740/Reviewer_ynSk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933322560, "cdate": 1761933322560, "tmdate": 1762934161183, "mdate": 1762934161183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes SR‑Loop, an alternating Supervised Fine‑Tuning (SFT) and Reinforcement Learning (RL) framework to train tool use without accuracy-based verifiable rewards during RL. The RL uses structured proxy rewards (format, tool invocation success, and reasoning–tool balance), while SFT periodically re‑aligns outputs to expert demonstrations to avoid collapse and drift \n\n- Demonstrates improvements on math tool‑use tasks (Python execution) across Skywork‑OR1‑Math‑7B and Qwen2.5 variants, showing higher pass@1 than SFT+RL and other baselines; adding an accuracy reward on top of SR‑Loop further boosts performance \n\n- Extends to tool‑free reasoning: using only format rewards, SR‑Loop improves in‑domain math and generalizes to out‑of‑distribution benchmarks (ARC‑C, GPQA‑Diamond, MMLU‑Pro)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Quality\n- Comprehensive math evaluation across multiple models/benchmarks; includes ablations (SFT‑only vs. SFT→GRPO, SR‑Loop with/without accuracy reward) and reward–accuracy correlation analysis, supporting claims of stability and gains \n\nClarity\n- The training loop (Algorithm 1), reward composition, and dataset setup are explained clearly; figures illustrate failure modes of pure RL and the corrective role of SFT in the loop \n\nSignificance \n- Addresses common instability of tool‑use RL (entropy collapse, drift) and scarce verifiable rewards in real tasks; results suggest a robust recipe applicable beyond tool use"}, "weaknesses": {"value": "Dependence on off‑policy expert data\n- SR‑Loop requires a capable off‑policy model and human‑verified SFT traces; feasibility in domains without high‑quality demonstrations remains uncertain (noted in Limitations) \n\nProxy reward fidelity and gaming\n- Format/tool/balance rewards are weakly aligned with task success; more evidence is needed on robustness to reward hacking and failure cases, especially in multi‑turn settings \n\nEvaluation scope\n- Focused on math and Python tool execution; lacks results for heterogeneous, non‑deterministic tools (web search, DBs), multi‑turn agents, or mixed simulated/live environments \n\nNovelty \n\n- Alternating SFT and RL with periodic re-alignment to expert traces while using non‑accuracy proxy rewards (format, tool‑use, balance) is closely related to prior alternating or cooperative SFT↔RL recipes, off‑policy–guided rollouts, and DAgger‑style loops, as well as recent tool‑use RL that relies on structural/execution rewards rather than verifiable accuracy. Unless the authors more sharply differentiate SR‑Loop (e.g., with a distinct learning principle, theoretical guarantees, or convergence properties) and run stronger, controlled head‑to‑head comparisons against cooperative SFT+RL, DAgger‑like alternation, and tool‑reward baselines, the contribution risks being seen as incremental packaging of known ingredients rather than a fundamentally new algorithm. Concretely, the paper would benefit from ablations that (i) hold total SFT budget constant while varying loop cadence, (ii) swap in alternative proxy‑reward shapes to show the loop—not the reward—drives gains, and (iii) extend beyond math/Python to heterogeneous, non‑deterministic tools to demonstrate broader novelty and impact"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i8Oy22vs3o", "forum": "Gg6zVda4iZ", "replyto": "Gg6zVda4iZ", "signatures": ["ICLR.cc/2026/Conference/Submission20740/Reviewer_13nM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20740/Reviewer_13nM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934139982, "cdate": 1761934139982, "tmdate": 1762934160138, "mdate": 1762934160138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the training of large reasoning models with tool utilization, and proposes a novel framework named SR-Loop, which is equipped with two alternating phases. The first one is SFT phase, in which the model is trained using SFT data to acquire tool-use capabilities. The second one is RL phase, in which the model is encouraged to explore improved strategies. Several experimental studies are conducted to verified its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is logically well structured and easy to follow.\n- The authors provide detailed experimental analyses demonstrating the superiority of **SR-Loop**."}, "weaknesses": {"value": "- In **Line 143**, it is unclear how the computational complexity $O(T^2 \\epsilon)$ is derived. The authors are encouraged to provide a high-level explanation of this result.\n- It remains unclear why a **distributional shift** problem arises after the SFT stage. The reviewer suggests adding an experiment to verify this claim.\n- The computation of the **Reasoning–Tool Balance Reward** is not clearly described.\n- In **(3)**, how to determine the weight of each term is not explained. An ablation study on the weight settings is necessary.\n- The paper includes too few baselines and lacks comparisons with tool-related baselines such as **Search-R1** [1].\n\n---\n[1] Search-R1: Training llms to reason and leverage search engines with reinforcement learning. 2025."}, "questions": {"value": "See them in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5fa1pnxhm", "forum": "Gg6zVda4iZ", "replyto": "Gg6zVda4iZ", "signatures": ["ICLR.cc/2026/Conference/Submission20740/Reviewer_XKve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20740/Reviewer_XKve"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012689216, "cdate": 1762012689216, "tmdate": 1762934158993, "mdate": 1762934158993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}