{"id": "3NbMnPh7vM", "number": 3333, "cdate": 1757404273049, "mdate": 1759898095318, "content": {"title": "Mastering Domain Shift Image Enhancement Via Differentiable Physics", "abstract": "Visual perception in the wild have demonstrated transformative potential across a wide range of applications, spanning from planetary exploration to deep-sea monitoring missions. However, a fundamental challenge remains in enabling visual perception enhancement that can explicitly extract rules and support interactive, precise manipulation in unknown, dynamic environments—particularly under conditions of large scale data absence, heterogeneous data distribution, and without the supervision of annotated images. Our approach introduces a differentiable physics framework that unifies the camera response model (CRM) with deep learning to achieve visual perception enhancement under multiple degradation conditions. Specifically, grounded in fundamental principles of radiation physics, we formulate the camera response function (CRF) calibration as a constrained optimization problem. Then we reconstruct the brightness transformation function (BTF) in traditional CRM as a multi-scale generative network, completely decoupling it from the CRF. Meanwhile, we design a dual-branch contrastive encoder that enables the BTF to regulate the irradiance enhancement process through multi-scale exposure distributions learned from guide images. This offers a flexible BTF interface supporting stable and controllable domain generalization for image enhancement. Through comprehensive experiments, our method significantly advances domain generalization capabilities in adaptive image enhancement, outperforming specialized counterparts by margins of +1.226 (UIQM) averaged across challenging unseen underwater domains.", "tldr": "", "keywords": ["camera response model", "camera response function", "brightness transformation function", "dual branch contrastive encoder"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7cea113123fbe116d77c58030c3261472509adb.pdf", "supplementary_material": "/attachment/6456b0f0737451d4ae74c3481b5acd8e81598c83.pdf"}, "replies": [{"content": {"summary": {"value": "This paper tackles image enhancement generalization across domains that exhibit diverse and coupled degradations. The authors decouple CRF and BTF, and leverage contrastive learning together with a generative network. First, CRF calibration is performed by modeling camera nonlinearity with DoRF-based bases. Next, representations are learned in irradiance space via contrastive learning between a distorted domain and a normal-exposure domain. Finally, an AFDM module is trained using a value-magnitude–based sort-matching algorithm, enabling robust enhancement on unseen domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Presents a framework capable of handling complex and unseen degradations that prior LLIE/UIE approaches typically struggle with.\n2. Clearly articulates that the conventional CRM coupling between CRF and BTF can hinder enhancement under domain shift, and proposes a principled decoupling.\n3. Utilizes dual-branch contrastive learning to capture domain representations in irradiance space.\n4. The paper offers dense analyses and thorough empirical comparisons, supporting the validity of the proposed methodology and aiding readability."}, "weaknesses": {"value": "1. The paper would benefit from an explicit assessment of CRF optimization quality and a sensitivity analysis quantifying the impact of CRF estimation errors on enhancement outcomes.\n2. Implementation details are limited (e.g., network specifications, hyperparameters, and experimental settings), which may impede reproducibility."}, "questions": {"value": "1. In the absence of any camera configuration information during training, is CRF calibration still feasible?\n2. When constructing distorted vs. normal-exposure domains, must the datasets be captured with the same camera and/or contain similar content (e.g., underwater imagery vs. everyday scenes), or is cross-camera/cross-content pairing acceptable?\n3. In AFDM, what is the precise rationale for value-based sorting? Given that deeper layers encode different semantics, why is a uniform sorting mechanism appropriate across layers?\n4. Do you anticipate the framework to remain effective when the distorted/normal domain pair extends beyond low-light scenarios?\n5. Were hyperparameter studies conducted for the loss weights composing L_BTF? If so, please summarize the settings and findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5CpfipqQJf", "forum": "3NbMnPh7vM", "replyto": "3NbMnPh7vM", "signatures": ["ICLR.cc/2026/Conference/Submission3333/Reviewer_vfWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3333/Reviewer_vfWa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814754098, "cdate": 1761814754098, "tmdate": 1762916676349, "mdate": 1762916676349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Summary\nThis paper proposes a physics-guided image enhancement framework integrating a differentiable camera response model (CRM) with deep generative learning. It decouples the camera response function (CRF) and brightness transformation function (BTF), introduces a dual-branch contrastive autoencoder (CAE) for domain-invariant feature extraction, and an adaptive feature distribution matching (AFDM) module for differentiable eCDF alignment. Experiments on underwater and low-light datasets show improved generalization and real-world robotic deployment.\n\n### Strengths\n- Novel integration of physical modeling (CRM) and deep networks.  \n- Dual-branch contrastive learning enhances robustness to domain shifts.  \n- AFDM offers a differentiable alternative to AdaIN/histogram matching.  \n- Comprehensive comparisons with strong baselines.\n\n### Weaknesses\n- Conceptual novelty is limited; CRF–BTF modeling and ADMM optimization follow prior work (e.g., LECARM).  \n- The “differentiable physics” part is loosely coupled with learning; no end-to-end integration.  \n- Writing and figures are dense and hard to follow.  \n- Evaluations rely mainly on no-reference metrics without perceptual or user studies.  \n- Focused on applications rather than learning theory—less aligned with ICLR scope."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Physics-inspired framework:**\nThe idea of combining radiometric camera modeling with deep generative learning is meaningful and provides some interpretability rarely seen in image enhancement research.\n\n2. **Dual-branch contrastive learning:**\nThe proposed CAE design with two symmetrical branches improves robustness to domain shifts and is theoretically analyzed (Eq. 8–9) for stability.\n\n3. **Differentiable eCDF alignment:**\nThe AFDM module introduces an elegant differentiable alternative to AdaIN or histogram matching, aligning distributions across domains.\n\n4. **Comprehensive experiments:**\nThe paper presents extensive visual and quantitative results on multiple domains (underwater, low-light, haze), and real-world robotic deployment adds practical value."}, "weaknesses": {"value": "1. **Limited novelty:**  \n   The contributions appear incremental. The CRF–BTF decoupling and ADMM-based CRF estimation follow previous works such as LECARM (Ren et al., TCSVT’18). Dual-branch contrastive learning and feature matching extend standard paradigms rather than introduce fundamentally new learning concepts.\n\n2. **Weak physics–learning integration:**  \n   Although termed “differentiable physics,” the physical CRF calibration is treated as an offline optimization rather than an end-to-end differentiable module. The connection between physical modeling and learned BTF remains loosely coupled.\n\n3. **Clarity and presentation issues:**  \n   The paper is dense, with unclear notation (e.g., reusing *f, g, E, P*), verbose equations, and missing intuition. The forward pipeline is not clearly explained—particularly how CRF calibration influences BTF training and inference. Figures are complex but not explanatory.\n\n4. **Empirical rigor:**  \n   Evaluation relies mainly on no-reference metrics (UIQM, UCIQE, CCF), which are noisy and limited in reflecting perceptual quality. No LPIPS/PSNR results, runtime comparison, or user study are provided. Statistical significance of the reported +1.226 UIQM improvement is unclear.\n\n5. **Language quality:**  \n   The writing contains numerous grammatical errors and awkward phrasing, making it difficult to follow in several sections (especially §2.2–2.3). The overall readability is below the ICLR standard.\n\n6. **Reproducibility concerns:**  No code or config reference is provided."}, "questions": {"value": "1. How is the CRF calibration integrated during training or inference? Is it pre-computed or updated jointly with BTF?  \n2. What is the nature of the “guide image” used for AFDM—does it come from another domain, or is it sampled within the same dataset?  \n3. Can the authors report perceptual metrics (e.g., LPIPS, PSNR) or user study results to strengthen claims about visual quality?  \n4. How does the method generalize across *unseen* domains (e.g., training on low-light and testing on haze)?  \n5. What is the computational complexity compared to strong baselines such as Diff-Retinex++ or WFI2-Net?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HsGXG5KXUO", "forum": "3NbMnPh7vM", "replyto": "3NbMnPh7vM", "signatures": ["ICLR.cc/2026/Conference/Submission3333/Reviewer_v49C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3333/Reviewer_v49C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851326876, "cdate": 1761851326876, "tmdate": 1762916675961, "mdate": 1762916675961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a differentiable physics framework that combines camera response modeling (CRM) and deep learning to achieve domain-generalized image enhancement under multiple degradation conditions. The traditional Camera Response Model (CRM) couples the Camera Response Function (CRF) and the Brightness Transformation Function (BTF), limiting adaptability. This work replaces the hand-crafted BTF with a generative network, enabling flexible brightness transformation independent of CRF."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed CRF calibration is formulated as a constrained optimization problem, solved using an EMA-ADMM algorithm, ensuring monotonicity and stability of the response curve. The authors develop a dual-branch contrastive learning strategy extracts discriminative latent features from distorted and guide images, enhancing cross-domain generalization."}, "weaknesses": {"value": "The paper is dense and notation-heavy, with long equations and overlapping terminology (CRF/BTF/CAE/AFDM) that could overwhelm readers. Some figures (e.g., Fig. 1–2) are small and lack explanatory captions for non-specialists. Despite the “domain shift” claim, experiments are primarily underwater-focused; cross-scene validation (e.g., haze, night, thermal) is limited. The model’s adaptability to other physics domains is implied but not empirically shown."}, "questions": {"value": "1. How does the model ensure physical consistency between the CRF and the learned BTF network during training? Is there a regularization term enforcing the CRM equation g(f(E), k) = f(kE)?\n\n2. The Sort-Matching alignment is claimed to be differentiable — could the authors clarify how gradients are propagated through the sort indices without introducing instability?\n\n3. What is the runtime overhead compared to diffusion-based models like Diff-Retinex++ or Retinexformer? Is real-time operation feasible on embedded platforms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NoSAKELpcC", "forum": "3NbMnPh7vM", "replyto": "3NbMnPh7vM", "signatures": ["ICLR.cc/2026/Conference/Submission3333/Reviewer_twVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3333/Reviewer_twVV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922193800, "cdate": 1761922193800, "tmdate": 1762916675654, "mdate": 1762916675654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Phoenix, a mask refinement framework that generates semantic-aware adversarial perturbations and employs a tri-directional contrastive loss between ground-truth, noisy, and refined masks. Experiments on three datasets show moderate improvements over recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is technically consistent and well-written, though dense in mathematical notation.  \nThe paper is well-organized with clear figures and ablations.  \nThe method is fully implemented and experimentally evaluated on multiple datasets."}, "weaknesses": {"value": "1. The central idea of using adversarial perturbations to represent semantic uncertainty is not convincingly supported. The paper provides intuitive motivation but no solid theoretical or empirical evidence demonstrating that embedding-space perturbations truly correspond to realistic segmentation errors.\n2. The methodological gap from SegRefiner or SAMRefiner is incremental rather than fundamental. AMP closely resembles existing adversarial augmentation or consistency-regularization approaches, while the CMRL mainly extends standard contrastive or triplet learning.\n3. Performance improvements are relatively small, sometimes within 1–2% over recent baselines, without statistical significance testing."}, "questions": {"value": "1. How is the perturbation generated in “semantic space” rather than feature space—any quantitative or visual validation?\n2. Could the authors provide a stability curve showing performance vs. perturbation strength?\n3. Is there any correlation analysis between AMP perturbation regions and true model error maps?\n4. Have the authors analyzed whether stronger adversarial perturbations might degrade mask quality or lead to unstable training? Figure 11 qualitatively illustrates several failure cases. A quantitative failure breakdown could make the claims more credible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2xRdoPKAIe", "forum": "3NbMnPh7vM", "replyto": "3NbMnPh7vM", "signatures": ["ICLR.cc/2026/Conference/Submission3333/Reviewer_8dut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3333/Reviewer_8dut"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926924979, "cdate": 1761926924979, "tmdate": 1762916675465, "mdate": 1762916675465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a camera response modeling (CRM) framework for illumination enhancement under severe domain shifts, particularly targeting underwater robotic vision. The overall idea of incorporating CRF calibration, dual-branch contrastive learning, and adaptive feature distribution matching into a unified BTF-based enhancement framework is interesting and technically non-trivial."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper brings together physical camera modeling and representation learning in a way that is fresh and useful. \n2. The paper is well-organized.  The dual-branch contrastive setup is a simple but clever idea for handling the two exposure domains, and the AFDM module offers a lightweight way to align feature distributions.\n3.  Experiments on real underwater robotic data consistently show clearer structure and better color recovery compared to existing low-light and underwater enhancement methods. \n4. The work is technically solid and has practical relevance."}, "weaknesses": {"value": "1. Although the abstract highlights CRF calibration via constrained optimization and a generative formulation of BTF, it does not clearly articulate the novelty compared to existing CRM-based enhancement methods (e.g., LECARM [1]).\n2. The central claim of “CRF–BTF decoupling” is questionable.  Since BTF is reparameterized as a generative network, it is unclear whether the resulting formulation remains physically valid or merely acts as a learned post-processing module.  \n3. There is no ablation comparing dual-branch InfoNCE vs. a standard single-branch InfoNCE setup. It remains unclear whether the gains come from the two-branch structure itself, or simply from stronger augmentations or more negatives.\n4. The performance claim appears narrow relative to the general domain-generalization claim, and it is uncertain whether the proposed method generalizes beyond underwater enhancement.\n5. Baseline comparisons may not be entirely fair. Competing methods are sequential LLE→UIE pipelines, whereas the proposed approach is a unified end-to-end model. The evaluation would be stronger if it included contemporary end-to-end enhancement models that jointly address exposure and color correction.\n6. Although the paper positions itself as addressing general domain shift in image enhancement, all experiments focus exclusively on underwater imagery, relying on a single sensor (IMX335) and a specific robotic platform. This makes it difficult to assess whether the proposed CRM framework truly handles broader forms of domain shift (e.g., cross-camera, cross-ISP, low-light photography, outdoor illumination changes).\n\n\n[1] LECARM: Low-Light Image Enhancement using Camera Response Model."}, "questions": {"value": "1. Could you clarify what aspects of your formulation go beyond existing CRM pipelines, e.g., in terms of modeling assumptions, optimization strategy, or the role of contrastive learning?\n2. Can the authors provide evidence,qualitative or quantitative, that the method generalizes beyond the underwater setting? Even a small cross-domain test (e.g., different enhancement tasks or other camera datasets) would help clarify whether the method is intended as a general framework or is primarily specialized for underwater enhancement.\n3. If a conventional InfoNCE with mixed-domain negatives works similarly, why introduce a more complex dual-branch formulation?\n4. Although the physical motivation is reasonable, the experiments do not include a “w/o CRF” variant to show its actual impact.\n5. All results are based on a single IMX335 camera, so it is unclear whether the method generalizes to unseen cameras, different ISP pipelines, or other underwater conditions. Given that the paper emphasizes domain shift, the lack of broader evaluation is noticeable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2xRdoPKAIe", "forum": "3NbMnPh7vM", "replyto": "3NbMnPh7vM", "signatures": ["ICLR.cc/2026/Conference/Submission3333/Reviewer_8dut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3333/Reviewer_8dut"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926924979, "cdate": 1761926924979, "tmdate": 1763282385845, "mdate": 1763282385845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}