{"id": "GIoNMdPLLO", "number": 10975, "cdate": 1758185969151, "mdate": 1759897617104, "content": {"title": "Align and Adapt: Enhancing LLM Format Alignment and Knowledge Adaptation via Reverse Constraints Generation", "abstract": "Building effective LLM agents requires strong instruction-following capability in addition to domain knowledge. While human-annotated long-form QA (LFQA) datasets contain rich factual content, we find that directly fine-tuning on them degrades instruction-following performance, making it impractical to create domain-specific agents. Recent research on instruction-tuning has focused on augmenting existing instruction-tuning or conversational datasets to create complex instruction-tuning dataset, enabling LLMs to better handle fine-grained and nuanced instructions. While effective, these augmentation approaches risk distorting semantic meaning of the long-form QA datasets. We propose REFER (REstructure, Feature Extract, Reverse constraint generation), a framework that transforms human-annotated long-form QA datasets into high-quality instruction-tuning datasets focused on verifiable constraints. REFER preserves the original semantics while integrates fine-grained format constraints into the dataset, enabling LLMs to improve instruction-following capability without sacrificing domain knowledge. Extensive evaluations on instruction-following benchmarks show that LLaMA-2-7B models fine-tuned with REFER exhibit stronger generalization in complex and multi-turn instruction following compared to both standard instruction-tuning and direct LFQA fine-tuning. REFER also emphasizes security and efficient where all the data augmentation is performed without external APIs, and supervised fine-tuning uses lightweight, reproducible LoRA adapters. Our results demonstrate that REFER enables the practical creation of domain-specific LLM agents with enhanced instruction-following capability which is something unattainable with naive LFQA fine-tuning.", "tldr": "", "keywords": ["Instruction Tuning", "Complex Instruction Following", "Long-form QA", "Format Alignment", "Back Translation", "Data Augmentation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00e53fd8f899ee4d08ac80baf7521739bd29f081.pdf", "supplementary_material": "/attachment/f04a5a738f3f19817c1a49f8fa395143b75e5936.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, a data construction pipeline name REFER is proposed for transforming long-form QA datasets into high-quality instruction-tuning datasets with verifiable constraints. Experiments across multiple instruction-following benchmarks show that the resulting dataset improves instruction-following performance without compromising domain knowledge adaptation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "To asses the instruction-following capability of the model, evaluations are conducted on 3 different relevant datasets."}, "weaknesses": {"value": "1. According to the paper, the proposed data construction method is primarily an engineering implementation, which offers limited insights to the research community.\n2. The paper suffers from major writing issues. The motivation is not adequately articulated in the introduction and is instead deferred to Section 3, which is atypical and makes it difficult for readers to quickly grasp the rationale behind the approach. The method description is unclear and hard to follow; for example, at line 212 the meanings of features H and S are not explained. There are also formatting and presentation problems: Figures 1 and 2 are not cited in the text, leaving their relevance unclear. Additional errors include the notation of sequence C at line 219, the first word at line 249, and a capitalization mistake at line 311. The authors should carefully proofread the paper before submission.\n3. The experiments are conducted only on an older, small-scale model (LLaMA-2-7B, released in July 2023), which is insufficient to substantiate the paper’s conclusions. Moreover, the necessity of the paper’s initial motivation—training both domain knowledge and instruction-following capabilities using a single dataset—remains unclear. The experimental section does not address this concern; see the questions below."}, "questions": {"value": "I am skeptical of the paper's motivation. Why convert LFQA data into an instruction-following format? During post-training, mixing domain-specific LFQA with general instruction-following data should enable the model to acquire domain knowledge while preserving instruction-following capabilities. What advantages does building a single, more complex training set provide, and is there empirical evidence that it outperforms mixed training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YhFI9j44a1", "forum": "GIoNMdPLLO", "replyto": "GIoNMdPLLO", "signatures": ["ICLR.cc/2026/Conference/Submission10975/Reviewer_8P91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10975/Reviewer_8P91"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760670362173, "cdate": 1760670362173, "tmdate": 1762922167616, "mdate": 1762922167616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to convert a Long Answer QA dataset into a multi-turn dialogue dataset that can be used to train a model and inject both instruction-following capabilities and domain specific knowledge."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The motivation and the related works are clearly written."}, "weaknesses": {"value": "# Weaknesses\n- The paper uses Llama-2-7B-Chat as the baseline model and the Natural Questions dataset which unfortunately are both a little outdated. Unless a degradation in instruction-following capabilities is observed when training a more recent model (that has undergone a better post-training process) on a more recent dataset, the motivation for this project does not hold. \n- The paper misses many key details about the experimental setup, which makes it hard to evaluate the correctness (See Questions 1-8, 10-12). \n- The paper presents results that are questionable (See Questions 14-15). Unless these concerns are properly addressed and explained by the authors, I will maintain a strong reject.\n- The captions for figures are too short and do not contain enough information. Please explain the key details and define any keywords that are relevant to understand the figures.\n\n# Nit-picky\n1. ​​ShareGPT missing citation (line 56), DeepSpeed missing citation (line 262), ROUGE missing citation (line 305), GPT-4o missing citation (line 313)\n2. Minor grammatical errors (e.g., “is not align” in line 95)\n3. Minor typos (e.g., Natural Question -> Natural Questions in lines 235-236)\n4. Captions for tables should be above the tables; captions for figures should be below the figures."}, "questions": {"value": "1. Which LLM did the authors use in “Structural Rewrite” (line 208)? What exactly is the list of possible rewritten structures? Is each answer rewritten into all possible options? Or is one option randomly chosen for each answer? Why is it a good idea to rewrite the answer into a different form when the question isn’t rewritten to specifically ask for that format? In NQ, a question looks something like “when did richmond last play in a preliminary final.” How do you answer this question in a JSONL or Markdown?\n2. Can the authors clarify which version they used when they say Qwen-32B-Instruct (line 210)? The Qwen3 technical report is cited, but Qwen3 does not have an “Instruct” version. It only has “Qwen3-32B” as the only version with thinking mode and non-thinking model fused. \n3. What is the definition of “Hard constraint features” and “Soft constraint features” (Figure 4)?\n4. How do you combine LLM, Spacy, and Python rule-based methods (lines 210-211)? What do you use each component for? What are the prompts used for the LLM? What are the features extracted with SpaCy?\n5. In line 213, the same n is used in both h_n and s_n. Does this mean the same number of hard constraints and soft constraints was generated for all questions? \n6. In Figure 4, how was Constraints Pool constructed? What is the definition of “seed constraints”? The notation suggests only 3 constraints are selected for each question. How is this seed constraint different from the “constraints” that are generated after reverse constraint generation? The notation in Figure 4 suggests that only 3 constraints are generated. However, in the main text (line 218), the authors suggest that n constraints are generated. Is this correct? What are constraints? How are they different from “constraint features”?\n7. Figure 4 also shows a “final answer $$a_f$$” which isn’t explained in the main body. How is this different from “original answer a”? \n8. The main text mentions “lightweight structural edits” (line 222-223). Does this affect any of the constraints that were imposed on the answer? What is the exact rule for applying the structural edits? Why is this not mentioned in Figure 4?\n9. Thakur et al. (2023) that is cited does not contain any reference to Natural Questions. **Why was this paper cited?** What is the correct filtered dataset that the authors use? \n10. Could the authors provide more details of the process in line 238-239? How was the subset of 40k data points selected? Was there a bucket for answer length? What was the set of buckets? How many examples existed per bucket pre-filtering? How many examples were selected per bucket? \n11. In lines 240-245, the authors mention “for our evaluation.” Just to clarify, the authors mean “training dataset” correct? These datasets are not used for evaluation. **Why do the authors mention 2 different training datasets?** Tables 2-6 do not mention which version of the training dataset the results correspond to. But in the main text, the authors keep on referring to “both of our REFER models” (line 336). Why do the authors include some subsets of the LFRQA into the training data? This was not mentioned anywhere before Section 5.1. Why did the authors select the Finance and Lifestyle subsets?\n12. Why do the authors select the examples from LFRQA dataset as evaluation prompts (line 311)? The authors report that the same dataset was used for training data. Isn’t this test data leakage? \n13. Can the authors clarify the model in line 401? LLaMA-3.1-75B does not exist.\n14. **Are the authors confident about the results in Table 5?** The LiveBench paper (He et al., 2024) reports GPT-4o number to be around 50% and Llama-3-8B-Instruct to be around 25%, Mistral-7B-Instruct-v0.2 to be around 18%. The reported number of 64% for GPT-4o does not match the original paper. The reported number of 36% for Llama-2-7B-Chat is higher than Llama-3-8B-Instruct? That does not make sense.\n15. **Are the authors confident about the results in Table 7, 9 (duplicate)?** Assuming “Vanilla” in Table 9 means the Llama-2-7B-Chat model (w/o finetuning), the answers generated by the model should be semantically different from the ground-truth results in the LFRQA dataset, which contain highly specific examples or keywords. The F1 score of >= 0.8 for the “Vanilla” model seems too high. Also, the test data was chosen to be a part of the training data. The model that was trained on the test data should almost memorize the specific parts of the QA dataset. How can the win-rate be only 57% against the base model that has never seen the ground-truth response?  \n\n[1] Thakur et al., “Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks,”  NAACL 2021.\n\n[2] He et al., “LiveBench: A Challenging, Contamination-Free LLM Benchmark,” preprint, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1jV8U5FB26", "forum": "GIoNMdPLLO", "replyto": "GIoNMdPLLO", "signatures": ["ICLR.cc/2026/Conference/Submission10975/Reviewer_HDNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10975/Reviewer_HDNy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761339371411, "cdate": 1761339371411, "tmdate": 1762922166971, "mdate": 1762922166971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the key problem that **fine-tuning LLMs on domain-specific long-form QA datasets often harms their instruction-following ability**, as such data lack structural diversity and alignment between questions and answers. To address this, it proposes **REFER** — a framework that augments human-annotated QA datasets by adding **verifiable format constraints** without altering factual meaning. Through question rewriting, structural answer reformats, and reverse constraint generation, REFER produces instruction-tuning data that preserve knowledge while enhancing adherence to complex instructions. Experiments on **LLaMA-2-7B** show significant improvements over baselines like Conifer and UltraIF on **IFEval**, **Multi-IF**, and **LiveBench**, demonstrating better generalization and domain adaptation. Overall, as a reviewer, the work effectively addresses a real pain point in domain fine-tuning, offering a **novel, practical, and reproducible** solution for building instruction-aligned LLM agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths:**\n\n1. **Clear motivation:** Clearly identifies that fine-tuning on domain-specific QA datasets often degrades instruction-following ability, establishing a strong and relevant research problem.\n2. **Simple yet effective method:** Introduces a straightforward but impactful framework (REFER) that restructures existing QA data through reverse constraint generation, requiring minimal additional resources.\n3. **Sound results:** Achieves consistent performance gains across major benchmarks — e.g., improvements on **IFEval (+3–5%)**, **Multi-IF (+2–4%)**, and **LiveBench (+2%)** over baselines like Conifer and UltraIF.\n4. **Clarity and reproducibility:** Provides transparent, step-by-step descriptions of data processing, feature extraction, constraint generation, and fine-tuning using open-source tools (Qwen, SpaCy, LoRA), ensuring easy replication.\n5. **Practical impact:** Effectively bridges factual QA data and instruction-tuning, offering a scalable, interpretable approach to enhance both **alignment** and **domain adaptation** in LLMs."}, "weaknesses": {"value": "**Weaknesses**\n\n1. Limited model evaluation: The method is only tested on LLaMA-2-7B, leaving uncertainty about its generalizability to larger or different architectures (e.g., Mis(x)tral, Qwen).\n2. Narrow domain coverage: Experiments focus on limited domains (Finance and Lifestyle), which may not fully demonstrate REFER’s adaptability to more specialized fields like legal or biomedical data."}, "questions": {"value": "1. why we need the reverse constraint generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Wb9qSkGACf", "forum": "GIoNMdPLLO", "replyto": "GIoNMdPLLO", "signatures": ["ICLR.cc/2026/Conference/Submission10975/Reviewer_iMmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10975/Reviewer_iMmT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032547035, "cdate": 1762032547035, "tmdate": 1762922166302, "mdate": 1762922166302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes REFER, a data-centric pipeline that converts human-annotated long-form QA (LFQA) into multi-turn instruction-tuning data with verifiable format constraints. \n\nThe pipeline consists of 4 steps:  \n1) question rewrites to diversify prompts, \n2) structural rewrites of answers, \n3) feature extraction from answers using Qwen-32B + SpaCy + rules, and \n4) reverse constraint generation that injects format constraints without altering semantics. \n\nThe method aims to improve instruction-following while retaining domain knowledge, addressing the finding that naive LFQA fine-tuning harms instruction following. Experiments on IFEval, Multi-IF, and LiveBench, plus a domain-adaptation study on LFRQA (Finance, Lifestyle), show Llama-2-7B fine-tuned with REFER outperforms LFQA fine-tuning and several data-augmentating baselines on constraint following and maintains competitive performance elsewhere. The paper provides prompt templates, constraint examples, training settings, and authors promised to open-source the framework and constraint pool."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is clearly defined, well motivated and consist of concrete prompt templates and examples.\n- The authors demonstrate the relevance of the problem, showing that naive LFQA SFT hurts instruction following, while their method REFER mitigates it. They present significant gains against compared baselines.\n- Reproducibility details are provided: dataset composition, LoRA settings, training schedule, and explicit benchmark protocols are listed; prompts for augmentation are provided in the appendix. The authors promised to open-source the framework. \n- Method is practical and shows effectiveness using open-source models/tools (Qwen-32B, SpaCy), training with LoRA, not needing external APIs / proprietary models for augmentation."}, "weaknesses": {"value": "- **Limited ablations on REFER design:** It is hard to attribute where gains primarily come from. We don’t see isolation of:\n  - impact of question rewrite vs structural rewrite vs reverse constraints;\n  - impact of multi-turn formatting;\n  - a broad discussion on sensitivity to constraint categories (length/case/punctuation/etc.,).\n- **Novelty:** I am not fully convinced about the novelty of the proposed solution. There are already several lines of work that (i) turn long or structured text into instructions [1], (ii) build constraint-rich instruction-following datasets from existing content [2,3,4], (iii) fine-tune models on domain data while preserving knowledge [5], and (iv) decompose and verify multi-constraint instructions in an agent-like way at inference time [6,7]. The paper should explain more clearly what is actually new beyond combining these ideas, and whether the contribution is more than smart engineering or pipeline design.\n\nReferences: \n\n[1] Köksal, Abdullatif, et al. \"LongForm: Effective Instruction Tuning with Reverse Instructions.\" EMNLP 2024\n\n[2] Sun, Haoran, et al. \"Conifer: Improving complex constrained instruction-following ability of large language models.\" arXiv preprint arXiv:2404.02823 (2024).\n\n[3] An, Kaikai, et al. \"UltraIF: Advancing Instruction Following from the Wild.\" arXiv preprint arXiv:2502.04153 (2025)\n\n[4] He, Qianyu, et al. \"From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models.\" EMNLP 2024.\n\n[5] Li, Haiyun, et al. \"KEFT: Knowledge-Enhanced Fine-Tuning for Large Language Models in Domain-Specific Question Answering.\" Transactions of the Association for Computational Linguistics (2025). \n\n[6] Ferraz, Thomas, et al. \"LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints.\" EMNLP 2024.\n\n[7] Zhang, Xianren, et al. \"Divide-Verify-Refine: Aligning LLM Responses with Complex Instructions.\" arXiv:2410.12207 (2024)"}, "questions": {"value": "1) Why Table misses 2 “Llama-2-7B (Base)” line? \n\n2) The authors presented the code in the supplemental material. Do the authors plan to release code? In the conclusion you promised to release the framework, but what exactly will be included in this release?\n\n3) Can you discuss more the degradation that happens on LiveBench? How can we avoid it? Which elements of the method make it present a smaller degradation? \n\n4) Typos: \n- On line 249 “he” instead of “The”.\n- Duplicate “answer answer” (line 123)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qbRLo6le0i", "forum": "GIoNMdPLLO", "replyto": "GIoNMdPLLO", "signatures": ["ICLR.cc/2026/Conference/Submission10975/Reviewer_WMcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10975/Reviewer_WMcq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116208280, "cdate": 1762116208280, "tmdate": 1762922165747, "mdate": 1762922165747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}