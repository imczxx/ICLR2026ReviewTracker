{"id": "CgQvm7TDz5", "number": 6999, "cdate": 1758004503298, "mdate": 1759897878668, "content": {"title": "This State Looks Like That: Self-Interpretable Reinforcement Learning Agents using Prototype Soft Actor-Critic", "abstract": "Reinforcement learning (RL) has achieved remarkable success across complex decision-making tasks, especially with the advent of deep neural networks. However, the resulting models are often opaque, making their deployment in safety-critical domains challenging. Explainable AI aims to address this issue, but most specific efforts for deep RL remain limited either to post-hoc explanation methods or to imitation learning and distillation procedures. These latter approaches rely on pre-trained black-box agents and are typically restricted to environments with discrete action spaces, limiting their scalability and interpretability. In this paper, we introduce ProtoSAC, a novel deep RL architecture that integrates a prototype-based actor into the Soft Actor-Critic (SAC) algorithm, enabling intrinsic interpretability in continuous action spaces. Our method learns a set of prototypes that represent interpretable state clusters, each associated with a Gaussian action distribution. Actions are generated as a similarity-weighted mixture over these prototypes, providing transparent decision-making without sacrificing performance. We evaluate ProtoSAC on continuous action-space environments and show that it matches the performance of the original SAC while offering enhanced interpretability.", "tldr": "", "keywords": ["explainable ai", "reinforcement learning", "interpretability", "self explainable", "soft actor critic"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a71c7b974bb2ff832cdd27ef59db33bf227c4459.pdf", "supplementary_material": "/attachment/a7f7a9d654f773cbc8c39f9007bd4763d21a95f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ProtoSAC, a novel deep reinforcement learning architecture for continuous control tasks that is intrinsically interpretable. It integrates a prototype-based actor into the Soft Actor-Critic (SAC) framework. The agent learns a set of representative \"prototype\" states, each associated with an action distribution. Actions are then generated as a similarity-weighted mixture of these prototype policies, making the decision-making process transparent. The authors demonstrate that ProtoSAC matches the performance of the original SAC on several benchmark environments while providing clear explainability, a significant step beyond post-hoc methods or approaches limited to discrete action spaces."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Impressively matches the performance of a strong baseline (SAC) without a significant trade-off, directly addressing a key challenge in XAI.\n2.   The \"this state looks like that\" reasoning framework is highly intuitive. Visualized prototypes and their weights offer clear, actionable insights into the agent's policy."}, "weaknesses": {"value": "1.  The method introduces computational overhead compared to standard SAC due to similarity calculations, extra loss terms, and prototype management. The paper could benefit from a more detailed analysis of this overhead and its scalability.\n2.  The model introduces several new hyperparameters (e.g., number of prototypes `K`, update frequency `M`, regularization coefficients `γ`). A sensitivity analysis or ablation study on these would strengthen the paper's claims and improve reproducibility.\n3.  The authors note high variance in early training, which may be tied to prototype initialization. The work could be improved by exploring more robust initialization strategies to increase stability."}, "questions": {"value": "1.  Have you investigated the trade-off between the number of prototypes `K`, model performance, and the granularity of the explanations? Could `K` be adapted dynamically based on environment complexity?\n2. The current \"hard replacement\" strategy for updating prototypes might risk forgetting rare but critical edge cases. Have you considered \"soft\" update mechanisms, such as slowly updating a prototype's embedding with new, similar state features?\n3. What is your perspective on scaling ProtoSAC to high-dimensional state spaces, such as image-based inputs (e.g., Atari)? What would be the main challenges for the encoder design and the similarity metric in such scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wwb9NsyG9H", "forum": "CgQvm7TDz5", "replyto": "CgQvm7TDz5", "signatures": ["ICLR.cc/2026/Conference/Submission6999/Reviewer_pKoK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6999/Reviewer_pKoK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761151394136, "cdate": 1761151394136, "tmdate": 1762919215524, "mdate": 1762919215524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ProtoSAC, a variant of Soft Actor-Critic which integrates a prototype framework during training time rather than posthoc. ProtoSAC enables yielding intrinsic, case-based explanations by factorizing the action space as a similarity-weighted mixture of per-prototype Gaussians. Experiments on four continuous action space environments shows the method largely matches SAC performance while providing the above mentioned interpretability benefits."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Prototype based SAC is both intuitive and enables drop in modification.\n* For the environments tested, performance appears to hold versus the black box, with performance generally higher then the Shared-PW-Net baseline.\n* The paper is generally well written and easy to understand."}, "weaknesses": {"value": "* **W1** The novelty of the word is quite limited, appearing to be a straight forward application of prototype based frameworks to SAC. As the authors acknowledge, their work is very related work PW-Net and Shared PW-Net, the functional difference rl training versus distillation / imitation learning.\n* **W2** The authors argue that distilling from a black box \"limits the learning capabilities to what the black-box model has already learned,\" which is true, but no experiments in this paper convincingly show that directly applying PW nets during RL training is more scalable then distillation.\n* **W3** The experimental results in the paper are extremely limited. Pendulum, Lunar Lander, Mountain Car, and Inverted Pendulum are broadly considered toy environments. This directly ties into W2: if the novelty of the approach rests on reinforcement learning being more scalable than imitation learning /distillation for learning prototype nets, experiments on more complicated environments should validate this. I would expect at the minimum experiments on MuJoCo.\n\n* **W4** The work would benefit from a user study considering the primary objective of introducing interpretability to SAC. Anecodes in the form of Figures 3/4 are helpful but not enough to validate the utility of the learned prototypes.\n\n* **W5** The ablation study as shown in Figure 5 is not convincing. Removing orthogonal loss (blue) or the entropy loss (green) does not seem to have an impact on final performance. The claim that, \"These findings suggest that the orthogonal loss and the negative entropy loss work in a complementary way: the orthogonal loss promotes diversity among prototypes, ensuring better coverage of the\nstate space, while the negative entropy loss encourages the model to rely on more than one prototype\nwith high similarity. Together, they help achieve more robust and generalized policies.\" Its unclear how Figure 5 shows either of these claims."}, "questions": {"value": "1. W2/3 would be cleared if the authors are able to demonstrate the method outperforms distillation / imitation learning on more complex environments, such as MuJoCo HalfCheetah-v4 or Procgen CoinRun. \n2. Can the authors clarify what evidence supports their claims on line 645? It is unclear to me how variance in return during training implies better diversey among prototypes and better state space coverage.  \n3. How does the perceived interpretability of ablation variants compare to the proposed method? A user study here appears to be necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VFUO8c5hYl", "forum": "CgQvm7TDz5", "replyto": "CgQvm7TDz5", "signatures": ["ICLR.cc/2026/Conference/Submission6999/Reviewer_V6re"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6999/Reviewer_V6re"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796226748, "cdate": 1761796226748, "tmdate": 1762919214948, "mdate": 1762919214948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProtoSAC, a novel self-interpretable reinforcement learning framework that integrates a prototype-based interpretation mechanism directly into the Soft Actor-Critic (SAC) framework for continuous control tasks. Its primary contribution is an intrinsically interpretable architecture where the actor's policy is defined by a similarity-weighted combination of learned prototypes, each representing an interpretable state cluster with an associated Gaussian action distribution. This design provides transparent, case-based decision-making without relying on post-hoc explanations or imitation learning from a black-box agent. The authors demonstrate that ProtoSAC achieves performance competitive with the standard SAC baseline and outperforms existing self-explainable methods like Shared-PW-Net across several benchmark environments, while also offering a prototype update mechanism and regularization losses to enhance interpretability and stability during training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents an application of a prototype-based explanation framework (referred to as \"ProtoSAC\") to deep Reinforcement Learning, specifically within the Soft Actor-Critic (SAC) algorithm. The following is a broad assessment of its strengths across key dimensions.\n\n2. Originality: The originality of the paper is moderate. The originality of this work is low. The paper simply combines two well-established techniques: the Soft Actor-Critic (SAC) algorithm and prototype-based explanation methods. Neither component is significantly modified or improved upon. The integration of these elements is straightforward and does not represent a novel algorithmic or theoretical contribution. The approach applies an existing interpretability paradigm to a new domain but fails to demonstrate substantive innovation in either the RL algorithm or the explanation framework itself.\n\n3. Quality: The technical quality of the work is adequate but could be strengthened. The experimental validation, while demonstrating the method's basic functionality, is somewhat limited in scope, relying on a narrow set of environments. A more comprehensive evaluation, including comparisons to other explanation baselines and a deeper, more quantitative analysis of the prototype fidelity, would be necessary to robustly validate the framework's effectiveness and general applicability.\n\n4. Clarity: The paper is generally clearly written, effectively motivating the need for explainability in RL and providing a coherent high-level overview of the ProtoSAC framework. However, the clarity is occasionally hindered, particularly by a central framework diagram that is not fully intuitive. The flow of how prototypes are generated and then utilized for explanation could be described with more precise, step-by-step detail to improve reader comprehension.\n\n5. Significance: The significance of the work is limited. While applying prototype-based explanations to deep RL is a novel combination, the insights gained are not particularly surprising or impactful. We already have a strong understanding from other fields that prototypes can cluster behaviors and identify decision patterns. Applying this method to an RL agent merely confirms these known properties in a new context, without yielding any novel or counter-intuitive findings about the agent's learning process. The results are confirmatory rather than groundbreaking, making the overall contribution feel incremental."}, "weaknesses": {"value": "1. The experimental environments used are overly limited and simplistic. The chosen benchmarks—Pendulum, MountainCar, InvertedPendulum, and LunarLander—are relatively simple and classic in contemporary deep reinforcement learning (DRL) research. These environments feature low-dimensional state vectors rather than high-dimensional pixel inputs, and the agent operates under clear dynamical models with intuitively understandable optimal policies.\n\n2. ProtoSAC exhibits limited scalability and robustness. Even in these simple settings, ProtoSAC shows a slight performance degradation compared to the standard SAC baseline, as evidenced in Figure 2 and Table 2. The policies required in these environments are inherently straightforward—primarily involving swinging, balancing, or landing—and yet the model already relies on 30 to 60 prototypes. In more complex tasks that involve long-term planning, hierarchical decision-making, or hidden variables, a significantly larger number of prototypes would likely be needed to adequately cover the state space. This raises serious concerns regarding the method's scalability and general robustness.\n\n3. ProtoSAC is narrowly built upon specific assumptions of the SAC algorithm. The method integrates prototypes only with SAC and cannot be directly extended to other actor-critic algorithms, such as deterministic policy-based methods like DDPG. Furthermore, it does not readily adapt to environments with non-continuous action spaces or those requiring alternative distribution representations—for instance, bounded continuous actions modeled using Beta distributions.\n\n4. Insufficient analysis of experimental results. The interpretability claims of ProtoSAC are not convincingly supported by Figures 3 and 4 or their accompanying analysis. Although the captions state that “Each prototype is represented by its associated state and the action Gaussian distribution,” Figure 3 only visualizes the Gaussian distributions of actions (a, b, c, d), without illustrating the corresponding states associated with the four prototypes. This omission makes it unclear which specific states these prototypes represent. While the figures show how the model makes decisions—by blending prototypes—they fail to illustrate why those decisions are made, since the reader cannot see which representative states the agent considers similar to the current observation.\n\n5. The absence of XAI/Explainable RL comparative experiments. As one of the main contributions of this article, the interpretability of this method also requires corresponding comparative experiments as support. However, although this article made a performance comparison with the explainable Shared-PW-Net, no comparison was made in terms of explainability."}, "questions": {"value": "1. Expand the experimental validation to more complex environments. To rigorously assess the robustness and scalability of the proposed method, it is crucial to test it on benchmarks with high-dimensional state spaces, such as those requiring processing of image or video inputs. Demonstrating competitive performance and meaningful interpretability in these challenging settings would significantly strengthen the paper's contributions.\n\n2. Conduct a more comprehensive evaluation of interpretability. The current qualitative analysis should be supplemented with a comparative study against other explainable DRL methods, particularly post-hoc approaches like ProtoX. A quantitative and/or human-study-based comparison would more convincingly demonstrate the advantages and unique value of the intrinsic interpretability provided by ProtoSAC.\n\n3. Enhance the ablation studies. The paper would benefit from an additional ablation experiment that investigates the impact of the prototype update mechanism. By presenting results from a model variant where this update process is disabled, the authors could quantitatively validate its necessity for maintaining performance and prototype quality throughout training.\n\n4. Revise the visualizations in Figures 3 and 4. To fully deliver on the promise of prototype-based explanation, these figures must be modified to visually show the actual states associated with each prototype. The current figures only display the action distributions, which makes it impossible for a reader to understand why a specific action is chosen. Illustrating the prototype states is fundamental for validating the \"this state looks like that\" reasoning.\n\n5. I suggest that the author conduct a more in-depth analysis of the relationship among state, prototype and action distribution. In fact, it is very important to understand which observed variables in state/prototype affect the generation of actions.\n\n6. The chart placement of this article needs further adjustment to make it easier to read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndtLSmWrrW", "forum": "CgQvm7TDz5", "replyto": "CgQvm7TDz5", "signatures": ["ICLR.cc/2026/Conference/Submission6999/Reviewer_i8Go"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6999/Reviewer_i8Go"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904970573, "cdate": 1761904970573, "tmdate": 1762919214536, "mdate": 1762919214536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an extension to current interpretable by design deep reinforcement learning algorithms by working prototypes into the training of soft actor critic. The method essential works by incorporating prototypes into the actor during training with the relevant gaussian processes. Comparisons are made against PW-Net variants and SAC vanilla algorithms, showing better performance than baseline interpretable by design methods, and almost equal performance to SAC."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper (as far as I know) is the first to propose a method to train interpretable by design deep RL agents from the ground up, which represents a significant contribution.\n\nAppropriate baselines are chosen, and the environments tested sufficient I believe.\n\nThe ability to work in continuous action spaces is a big plus, as this represents most real-world deployment tasks of RL agents such as co-pilots etc."}, "weaknesses": {"value": "I miss some qualitative analysis of the prototypes themselves, it is difficult to say how useful the final system would be for an end user, although to be fair this is a byproduct of most interpretability papers in the ML conferences.\n\nThe paper doesn’t consider pixel state spaces as far as I understand, which is a significant limitation. Would this mean the method is not applicable to deep learning problems? Or did you use pixel state spaces for these problems? If it is purely symbolic, I don't think it's fair to call this an interpretable by design deep RL algorithm, but I'm willing to hear a counter argument."}, "questions": {"value": "I might have missed it in the manuscript, but did you use the symbolic or pixel state spaces for these problems?\n\nDo you have any idea as to the qualitative properties of the prototypes? How easy would it be for a user to gain some kind of understanding of the model globally?\n\nIn the forward pass are you taking the state encoding’s similarity to all prototypes? I am wondering if the final output is a calculation traced back to all prototypes, which could be difficult to interpret."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qsor9WZaPz", "forum": "CgQvm7TDz5", "replyto": "CgQvm7TDz5", "signatures": ["ICLR.cc/2026/Conference/Submission6999/Reviewer_3FXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6999/Reviewer_3FXf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947466583, "cdate": 1761947466583, "tmdate": 1762919214124, "mdate": 1762919214124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}