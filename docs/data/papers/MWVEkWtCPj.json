{"id": "MWVEkWtCPj", "number": 8746, "cdate": 1758096812381, "mdate": 1759897766030, "content": {"title": "FREQMIXATTNET: CONTRASTIVELY SUPERVISED FREQUENCY-MIXING ATTENTION FOR TIME-SERIES FORECASTING", "abstract": "Time series forecasting has gained significant attention due to its wide applicability in domains such as traffic prediction and weather monitoring. However, it remains a challenging task because of complex temporal patterns, such as multi-scale periodicities and dynamic fluctuations. Existing methods often focus on either time-domain decomposition or frequency-domain analysis, but rarely integrate both effectively.In this paper, we propose FreqMixAttNet, a novel cross-domain forecasting framework that mixes time and frequency representations via a cross-domain attention mechanism. We first introduce an adaptive convolutional wavelet decomposition to model and separate trend and seasonal components more efficiently. The seasonal part is dual-encoded in both time and frequency domains, which are treated as distinct modalities and fused through a cross-transform attention module. Meanwhile, the trend component is captured by a simple multi-scale MLP in the time domain.To further enhance robustness without pretraining, we incorporate a contrastive auxiliary loss. The combination of adaptive convolution, cross-domain mixing attention, and contrastive learning contributes to the superior performance of our method. Extensive experiments on multiple real-world benchmarks show that FreqMixAttNet consistently outperforms prior state-of-the-art methods, demonstrating the effectiveness of our unified cross-domain design.", "tldr": "FreqMixAttNet: Cross-domain time-frequency fusion framework using adaptive convolution and contrastive auxiliary loss for superior time series  forecast", "keywords": ["Time series", "Frequency-domain", "Attention", "contrastive learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a4f21d66bd992d591cddf27dc7404a517102891.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FREQMIXATTNET, a frequency-mixing attention network that combines multi-frequency representation with contrastive learning to improve cross-domain image classification. The framework appears reasonable. However, several key aspects need clarification and stronger experimental evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel combination of frequency mixing and attention mechanisms for cross-domain feature learning.\n\n2. The use of contrastive loss to enhance domain-invariant representation is well motivated.\n\n3. The proposed model shows consistent improvement on several benchmark datasets.\n\n4. The structure of the paper is mostly clear, and the technical formulation is generally easy to follow."}, "weaknesses": {"value": "1. Lack of theoretical justification:\nThe frequency-mixing operation is described empirically, but the paper does not clearly explain why combining frequency bands enhances domain generalization. A mathematical or intuitive explanation of the mechanism would strengthen the contribution.\n\n2. Ablation analysis is insufficient:\nThere is no clear separation of the contributions from the frequency-mixing module, attention mechanism, and contrastive loss. An ablation study quantifying the improvement of each component is essential.\n\n3. Limited comparison to recent baselines:\nThe paper mainly compares with classic methods but omits some strong recent works (e.g., transformers-based DG models, diffusion-based domain adaptation). Including these would make the evaluation more convincing.\n\n4. Experimental details missing:\nKey training details (e.g., batch size, optimizer, learning rate schedule, number of epochs) are not fully provided, making reproducibility difficult.\n\n5. Visualization and qualitative results:\nThe paper would benefit from t-SNE plots or attention heatmaps to demonstrate that the model learns domain-invariant features.\n\n6. Contrastive loss formulation:\nThe contrastive learning section lacks details about positive/negative pair sampling, temperature parameter, and how it interacts with cross-domain samples.\n\n7. English writing and presentation:\nSeveral grammatical and formatting issues exist (e.g., inconsistent figure captions, equation numbering). The introduction and conclusion sections could be refined for clarity and conciseness."}, "questions": {"value": "1. Provide a deeper theoretical or intuitive analysis of frequency mixing and its link to domain invariance.\n\n2. Add a comprehensive ablation study (baseline vs. +FreqMix, +Attention, +CL).\n\n3. Include more recent comparison methods and report statistical significance.\n\n4. Provide visual evidence of learned representations (feature maps, t-SNE, Grad-CAM).\n\n5. Refine language and structure — a professional proofreading pass is recommended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5819H6xdg", "forum": "MWVEkWtCPj", "replyto": "MWVEkWtCPj", "signatures": ["ICLR.cc/2026/Conference/Submission8746/Reviewer_GbPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8746/Reviewer_GbPX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760620478043, "cdate": 1760620478043, "tmdate": 1762920536324, "mdate": 1762920536324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a forecasting framework that fuses time-domain and frequency-domain representations via cross-attention, with an adaptive convolutional wavelet decomposition and contrastive auxiliary loss. Experiments across several standard long-horizon benchmarks and comprehensive ablations show the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework design is clear and well validated by extensive ablation studies, such as removing adaptive conv, removing wavelet decomposition, disabling cross-domain attention, dropping the contrastive loss, etc. It's also new to use wavelet decomposition for trend and seasonality decomposition. \n\n2. Extensive experiments validate the effectiveness of the proposed approach. The paper also provides detailed results on sensitivity analysis which improves the transparency."}, "weaknesses": {"value": "1. The novelty is limited as there are many existing works on combining time domain and frequency domain analysis with decomposition [1]. The paper should more precisely explain what is fundamentally new about the proposed method compared to existing time and frequency domain approaches and why/how such design helps.\n\n2. It is already known that different baselines perform the best under different lookback windows [2], so it is a bit unfair to compare with a unified lookback window for all baselines.\n\n3. The writings look repetitive. For example, Table 2 vs Table 6, Table 3 vs Table 7, Table 4 vs Table 8/9/10 are very redundant. The paper would benefit from merging redundant tables and making the writings more concise.\n\n[1] First De-Trend then Attend: Rethinking Attention for Time-Series Forecasting\n\n[2] Scaling Law for Time Series Forecasting"}, "questions": {"value": "1. What is the definition of forecastability in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ObjtgUQdRo", "forum": "MWVEkWtCPj", "replyto": "MWVEkWtCPj", "signatures": ["ICLR.cc/2026/Conference/Submission8746/Reviewer_g8k2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8746/Reviewer_g8k2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723159600, "cdate": 1761723159600, "tmdate": 1762920535870, "mdate": 1762920535870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors aim to improve time-series forecasting by jointly modeling temporal and frequency representations. To achieve this goal, they propose FreqMixAttNet, a unified cross-domain framework that integrates information from both the time and frequency domains. Specifically, the model consists of three key components: (1) a Patch-Contextual Adaptive Convolution module for context-aware feature extraction, (2) a Cross-Domain Mixing Attention mechanism to enable interaction between time- and frequency-domain features, and (3) a Contrastive Auxiliary Learning strategy to enhance robustness and generalization. In the experiments, the authors evaluate the proposed method on six benchmark datasets and compare it with several state-of-the-art baselines, demonstrating consistent improvements in forecasting accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors proposed a cross-domain module to integrate time and frequency domain representations.\n2. The organization and writing are easy to follow."}, "weaknesses": {"value": "1. The paper makes an abrupt transition from discussing how previous methods separately handle time and frequency domains to emphasizing robustness improvement. The connection between these two aspects is unclear, and this logical discontinuity weakens the overall coherence and persuasiveness of the paper’s argumentation.\n2. The authors do not provide a clear explanation of why the proposed cross-domain attention mechanism can improve forecasting accuracy and lacks interpretability analysis to support this claim. It is also worth questioning whether the interaction between the two modalities could introduce redundant or interfering information that might negatively affect prediction performance.\n3. The paper lacks an analysis of computational complexity, including comparisons of training time, inference efficiency, and parameter counts with baseline models.\n4. In the paper, there are several hyperparameters are introduced, but only the impact of $$\\beta_1$$is analyzed. The paper does not examine how different loss weights influence the results, nor does it clarify how the weights of multiple loss terms are designed. And another question is whether they sum to a fixed value or are tuned independently. A more  discussion of these aspects is needed.\n5. There are some small typos and inconsistencies. For example, there are missing spaces between some sentences.  Besides,“hyperparamter” should be “hyperparameter”, and “analyst” should be “analyze”, among similar minor spelling errors."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8znWDI0ENZ", "forum": "MWVEkWtCPj", "replyto": "MWVEkWtCPj", "signatures": ["ICLR.cc/2026/Conference/Submission8746/Reviewer_Ta1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8746/Reviewer_Ta1Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961455677, "cdate": 1761961455677, "tmdate": 1762920535542, "mdate": 1762920535542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}