{"id": "oGPeI321sI", "number": 2959, "cdate": 1757307336914, "mdate": 1759898117212, "content": {"title": "Max-Speedup Speculative Sampling: A Generic Tree Construction Principle", "abstract": "Speculative sampling has emerged as a promising approach for accelerating large language models (LLMs) inference by leveraging a lightweight draft model to propose multiple candidate tokens, which are then verified in parallel by a target model. \nRecent methods enhance this process by structuring candidate sequences into a token tree for more efficient verification. \nHowever, existing tree construction methods overly rely on acceptance length as a proxy for speedup.\nSuch an indirect pursuit renders it challenging to achieve the optimal tree structure for maximum speedup.\nIn this paper, we first revisit prior approaches and find they suffer from two key limitations: analytical intractability and the assumption of node independence.\nWe then redefine the costs and benefits of each tree node, derive a function that characterizes the relationship between time reduction and draft length, and prove its convexity. \nFinally, we extend this analytical framework to tree structures and propose a general principle for tree construction aimed at maximizing speedup.\nApplying this principle to state-of-the-art tree-based speculative sampling methods consistently delivers significant gains, improving overall performance by 4\\% to 14\\% and achieving end-to-end speedup of 1.97× to 2.68×.\nThe implementation is publicly available at: \n% The code is available at:\n\\url{https://anonymous.4open.science/r/GTCP-CC76/README.md}.", "tldr": "", "keywords": ["Large Language Models", "Lossless Acceleration", "Speculative Sampling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7567700db1b12ad9412e70565ef74a7e7009ad59.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates speculative sampling for accelerating large language model inference.\nThe authors propose a Generic Tree Construction Principle to decide which candidate nodes should be added during tree-based speculative decoding. They provide a convex-optimization-style analysis of expected time vs. acceptance rate and claim that the derived principle can yield maximum speedups under a given cost budget."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The attempt to formalize benefit-cost trade-offs in tree-based speculative decoding is interesting."}, "weaknesses": {"value": "1 The proposed “generic principle” essentially reformulates existing tree-based speculative sampling as an optimization problem with pruning rules. It does not introduce a fundamentally new mechanism or algorithmic insight beyond prior work (e.g., Sequoia, EAGLE-2, SpecInfer). The contribution is more of an incremental improvement rather than a conceptual breakthrough.\n2 The analysis relies on assuming that each node’s acceptance probability can be estimated accurately and independently. In real LLM decoding, acceptance rates depend on context, sampling temperature, and model randomness—these assumptions rarely hold.\nThe convexity proof therefore has limited practical significance.\n3 The paper’s tone is conversational (“we first revisit…”, “we then extend…”) and the exposition lacks precision.\nSome equations and definitions are introduced loosely, and related work is not carefully positioned against the most recent 2024–2025 literature on LLM inference acceleration."}, "questions": {"value": "The paper does not address when the principle might fail (e.g., under high temperature, small batch size, or hardware constraints).\nPractical deployment concerns (GPU memory, caching, latency) are absent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eVR5zLywSz", "forum": "oGPeI321sI", "replyto": "oGPeI321sI", "signatures": ["ICLR.cc/2026/Conference/Submission2959/Reviewer_PxyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2959/Reviewer_PxyG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761095263596, "cdate": 1761095263596, "tmdate": 1762916462873, "mdate": 1762916462873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generic tree construction principle (GT principle) for speculative sampling that directly maximizes inference speedup—rather than optimizing the proxy metric of average acceptance length. By redefining per-node cost and benefit and proving the convexity of time reduction with respect to tree size, the method selects only nodes with positive net benefit. Applied to both static (Sequoia) and dynamic (EAGLE-2) tree methods, it achieves consistent end-to-end speedups of 1.97×–2.68× across model sizes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Principled formulation: The paper provides a clear, theoretically motivated criterion for tree construction, moving beyond heuristic design. It correctly identifies the core limitation of prior work—optimizing for average acceptance length as a proxy—which can be misaligned with the true objective of minimizing total inference time.\n2. Strong empirical validation: Results across model sizes (7B–33B), tasks, temperatures, and batch sizes demonstrate robustness. The paper goes beyond simple benchmarking by including ablation studies on memory efficiency, sensitivity to hyperparameters (like the cost threshold `c`), and generalization across different LLM architectures (Llama-2/3, Qwen2, Vicuna).\n3. Plug-and-play applicability: GT integrates seamlessly with both static (Sequoia) and dynamic (EAGLE-2) methods, showing broad utility. The simplicity of the principle—select nodes where estimated benefit outweighs cost—makes it highly practical and easy to adopt by the community."}, "weaknesses": {"value": "1. The GT principle evaluates nodes based on individual net benefit, assuming independence. However, in speculative decoding trees, the utility of a node may stem from its role in enabling high-value subtrees—such as acting as a necessary prefix for multiple high-probability continuations—making isolated evaluation potentially suboptimal.\n1. In the integration with EAGLE-2, the adaptive tree depth is governed solely by the acceptance probability along the leftmost path. This choice lacks theoretical justification and may overlook more informative aggregation strategies that better reflect overall tree quality at each depth.\n1. The GT principle relies on accurate estimates of token acceptance probabilities to compute node benefit. Since the true acceptance rate ε is unobservable during inference, any systematic bias or variance in its proxy could degrade the effectiveness of node selection."}, "questions": {"value": "1. Could the GT principle miss structurally important “bridge” nodes—those with low immediate benefit but essential for reaching high-benefit descendants—due to its greedy, node-wise selection criterion? If so, how might one extend the framework to account for subtree-level value?\n2. Why is the leftmost path used as the sole indicator for adaptive depth in the EAGLE-2 integration? Have alternatives—such as using the maximum, average, or entropy-weighted acceptance probability across all nodes at a given depth—been considered, and how do they affect speedup and stability?\n3. In real-world deployment, the acceptance probability must be approximated. How robust is the GT principle to inaccuracies in this estimation? Are there practical strategies (e.g., calibration) to mitigate performance degradation under estimation noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "egy1Q4HOEb", "forum": "oGPeI321sI", "replyto": "oGPeI321sI", "signatures": ["ICLR.cc/2026/Conference/Submission2959/Reviewer_Wruo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2959/Reviewer_Wruo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904033623, "cdate": 1761904033623, "tmdate": 1762916462522, "mdate": 1762916462522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper proposes an early stopping algorithm for tree-based speculative decoding to reduce extra draft runs, aiming to improve speed-up with only a slight reduction in acceptance.\n- Claims that expected speed-up is a convex function over draft length—but this seems like a known property already."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a practical idea to reduce unnecessary draft generation, which can save compute during inference.\n- Focuses on speed-up optimization, which is critical for real-world deployment of speculative decoding."}, "weaknesses": {"value": "- No comparison with adaptive draft length methods, despite the proposed approach being conceptually similar (stopping draft generation dynamically).\n- Eq. (4) is missing $M_T$​ on $\\epsilon$; text below Eq. (4) should include $M_T \\epsilon$ since they come together.\n- Full form of GT is unclear—should clarify if it means General Tree.\n- Criticism of Sequoia in the static tree section seems invalid:\n  - Sequoia is offline, so its cost is a one-time calibration, which is standard and not a runtime overhead unless extremely large.\n  - The claim that Sequoia fails to find an optimal speed-up tree needs evidence.\n- Eq. (10) and Eq. (11) lack intuition—currently appear arbitrary.\n- Improvement in speed for static tree in Table 1 is marginal (second decimal point).\n- Fig. 3 legend says “GE,” likely should be “GT.”\n- Table 4 does not specify the evaluation task.\n- Overall, the paper does not convincingly show significant gains compared to existing methods."}, "questions": {"value": "- Please check weaknesses\n- Can the authors include SpecBench comparisons using EAGLE2/3 with LLaMA3 family, I'm curious to see performance on models with  large vocabulary models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TGb1D53ej5", "forum": "oGPeI321sI", "replyto": "oGPeI321sI", "signatures": ["ICLR.cc/2026/Conference/Submission2959/Reviewer_AyBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2959/Reviewer_AyBk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157909132, "cdate": 1762157909132, "tmdate": 1762916462330, "mdate": 1762916462330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a \"Generic Tree Construction Principle\" (GT principle) for tree-based speculative sampling in large language models (LLMs), aiming to maximize inference speedup by selecting nodes with positive net benefits based on redefined costs and benefits. The authors derive a convex function for time reduction, extend it to tree structures, and integrate it into existing methods like Sequoia and EAGLE-2, claiming 4-14% performance gains and speedups up to 2.68×. Experiments are conducted on benchmarks like Spec-Bench with models from 7B to 33B parameters."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has a reasonable structure and clear writing. The authors elaborate on the research methods in detail, making it highly readable.\n2. The proposed method can be easily integrated with existing speculative decoding algorithms to achieve performance improvements.\n3. The experiments are comprehensive, covering multiple model scales (Vicuna with 7B–33B parameters), tasks (machine translation, transformation, summarization, question answering, multi-turn dialogue, and retrieval-augmented generation from Spec-Bench), and baseline methods (Sequoia, EAGLE-2, with additional methods including EAGLE, EAGLE-3, Medusa, and Hydra in the appendix)."}, "weaknesses": {"value": "1. Although the method proposed by the authors achieves a 4%-14% performance improvement compared to the baselines, the improvement margin is relatively small. The end-to-end speedup is largely contributed by the baseline methods, and the enhancement brought by the authors' method to the baselines is quite limited. Additionally, the improvement in the R value (memory-to-speedup ratio) is negligible.\n2. Code generation tasks (such as the HumanEval benchmark) are not included, and there are few experimental results on larger models (e.g., 70B models)."}, "questions": {"value": "1. Could you provide the quality metrics for each benchmark in Spec-Bench (such as BLEU for the WMT task, accuracy for the GSM8K task, etc.) to demonstrate that the GT principle does not reduce the utility of the model?\n2. The authors claim that \"all experiments were repeated 5 times under the same hyperparameters, and the final results are presented as averages\". Could you please provide the variance of the 5 experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "a757yHsMmt", "forum": "oGPeI321sI", "replyto": "oGPeI321sI", "signatures": ["ICLR.cc/2026/Conference/Submission2959/Reviewer_SzNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2959/Reviewer_SzNe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179426016, "cdate": 1762179426016, "tmdate": 1762916461972, "mdate": 1762916461972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}