{"id": "FxGCfnJswW", "number": 16510, "cdate": 1758265384791, "mdate": 1759897236138, "content": {"title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, with Reinforcement Learning (RL) playing a key role in adapting them to specific applications. In mathematical problem solving, however, the reliance on ground truth answers poses significant challenges due to their high collection cost and limited availability.  \nThis work explores the use of simple surrogate signals, format and length, to guide RL training. We find that early training is dominated by format learning, where structural feedback alone accounts for most performance gains. Incorporating length-based rewards further refines outputs by discouraging overly long or short responses, enabling a GRPO approach with format-length signals to match, and in some cases surpass, ground-truth-based optimization. For example, our method achieves 40.0\\% accuracy on AIME2024 with a 7B base model, and generalizes across different model sizes and series.  \nBeyond practical efficiency, these findings provide an inspirational perspective on RL: rather than imparting new knowledge, RL primarily activates reasoning capabilities already embedded in pre-trained models. This insight suggests that lightweight, label-efficient strategies can complement pre-training to unlock LLMs’ latent potential in reasoning-intensive tasks.", "tldr": "", "keywords": ["Reinforcement Learning from Verifiable Rewards", "Large Language Models", "Reinforcement Learning", "GRPO", "without ground truth", "label free"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9eac3036ee78bec0ae05d2e991b16e4216e2b11b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies math reasoning and uses two proxy signals (i.e., format and length), to replace answer-based rewards for label-free RL. These signals are added via replacing GRPO algorithm's GT reward to the combination of these two proxy rewards.\n1. The format reward R_f is a yes/no check: did the model follow the required template (e.g., did it include a valid boxed final answer).\n2. The length reward R_l is a piecewise “rise-then-fall” function that encourages a moderate chain of though (not too long nor too short).\n\nExperiments show that early gains mostly come from learning the right format; within about 15 steps the model gets most of the accuracy improvement. After that, adding the length signal gives extra gains. Across several base models and datasets, this approach matches or slightly beats answer-based GRPO. The authors argue that RL here mainly activates reasoning ability already present from pretraining, rather than emerging new knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a concrete, clear, reproducible reward design.\n2. The paper empirically shows a two-phase pattern: (1) early gains from learning the output format (2) followed by further gains from length control\n3. The paper is in general lean presented, very easy-to-follow, (e.g., Qwen-Math prompt format, format checker, GRPO setup) and the formulas are also well-organized."}, "weaknesses": {"value": "1. Current math-QA benchmarks already provide relatively rich verifiable signals; harder settings (e.g., proof generation, open-ended reasoning) are underexplored so far. So I do hope the authors can extend the experiments to these tasks.\n\n2. Results look Qwen-centric with unclear generalization to Qwen3/Llama/Mistral. In fact, there are several papers showing that qwen2.5 series of models can be successfully \"RL\"ed even with spurious rewards. I really want to see results on more models.\n\n3. Will these reward function design also have potential for over-reflection, repetition, and mechanical length inflation? i.e., How to de-risk these factors."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FMiyxZRTF9", "forum": "FxGCfnJswW", "replyto": "FxGCfnJswW", "signatures": ["ICLR.cc/2026/Conference/Submission16510/Reviewer_6Aps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16510/Reviewer_6Aps"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837280282, "cdate": 1761837280282, "tmdate": 1762926602594, "mdate": 1762926602594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using simpler losses that only account for the length and formatting of the answers, as opposed to (just) the correctness, in order to boost RL performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of this paper:\n- the losses proposed by the authors do not require ground truth labels and are simple and interpretable\n- the proposed method achieves a large boost quite quickly in training, showing that a lot of performance can be extracted by supervising the formatting and length"}, "weaknesses": {"value": "The main weaknesses of this paper:\n- In general, I am not sure supervising just for formatting and length would be enough on more complicated math proofs/code, and more importantly, much longer traces\n- I am not sure exactly what the main message of this paper is meant to be. The authors' experiments show that rewarding for correctness achieves at least the same performance as rewarding for formatting and length on these tasks (for example, format-only and format-length in Table 2 are much worse than correctness for the Llama model). I am not sure whether the authors are suggesting that correctness rewards should be replaced with rewards for formatting/length, or that both should be used concurrently\n- The authors claim that format length can surpass correctness check (in the abstract), and while this is true in Table 2, gaps in post training of roughly 2-3% are not clearly statistically significant given the very noisy nature of RL\n- How does one tune the parameter p controlling the length of the chain of thought?"}, "questions": {"value": "What is meant by online SFT? Does it mean that the model is SFTd on its own generations, similar to Expert Iteration (EI)?\nHave you swept the hyperparameters for GRPO and SFT?\nAre the models that get low reward actually not solving the problems, or is it the case that simply they are not wrapping their answer in \\boxed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "46xHsjFN4z", "forum": "FxGCfnJswW", "replyto": "FxGCfnJswW", "signatures": ["ICLR.cc/2026/Conference/Submission16510/Reviewer_XzHt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16510/Reviewer_XzHt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979153907, "cdate": 1761979153907, "tmdate": 1762926602245, "mdate": 1762926602245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies training LLMs with surrogate rewards on reasoning tasks —a format check and a piecewise length reward— arguing these signals can stand in for ground-truth correctness. The method targets the early regime where “format learning” dominates, and claims that adding a length prior prevents saturation and nudges the model toward more effective reasoning. On Qwen2.5-Math and OctoThinker (based on Llama3.1-8B), the authors report that format-only quickly matches correctness-based GRPO in the first ~15 steps but plateaus, while format+length (their main method) achieves competitive or occasionally better accuracy than correctness-based GRPO. They also include experiments on uncontaminated 2025 test sets and analyze response-length dynamics and reflective-word usage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written and experimental details are clear.\n- Results are across multiple datasets with care taken regarding decontamination and across model sizes and families.\n- Authors provide additional qualitative analysis (response-length trends, reflective-word frequencies, etc) and ablations (eg. RL vs SFT for format learning) that align with their findings."}, "weaknesses": {"value": "- My main concern is the novelty of the work; the author’s central claim– that simple structure-based signals (format, length) can substitute correctness in GRPO– are preceded by several prior works which study RLVR without external supervision rewards [1,2] as well as study whether RL is eliciting capabilities beyond what can be achieved by the base model [3,4,5], which are not cited in this work (the above examples I gave is not exhaustive– the authors do cite [6]).\n- In particular, [2] shows very similar results in the random reward setting, and it seems that the results found by this work are by virtue of the choice of Qwen 2.5 Math base model and bias introduced by things like clipping with the GRPO algorithm - indeed, using the correctness reward is better for OctoThinker as we see in Table 2. \n- Since the efficacy of the methods in this work seem to be highly model-dependent (as the authors themselves note: “If the base model is not powerful, we expect this approach to yield limited gains.”), I think the work would be much stronger if the authors did further ablations regarding the design of their length-based reward, verified against a correctness + length baseline, and when one can expect their surrogate signal to capture most of the gains from a correctness-based reward given a choice of base model (for instance, relating pass @ 64 performance or ability to benefit from additional response tokens). Please see ‘Questions’ for more specific details in this regard.\n\nMinor: \n- Some areas in the manuscript need `\\citep` (eg. citations in the Introduction).\n- Wording in sentence from lines 214-215\n- Line 375: thes -> these\n- Line 729: A.5 Introducion -> Introduction\n\n[1] Agarwal, Shivam, et al. \"The unreasonable effectiveness of entropy minimization in llm reasoning.\" arXiv preprint arXiv:2505.15134 (2025).\n[2] Shao, Rulin, et al. \"Spurious rewards: Rethinking training signals in rlvr.\" arXiv preprint arXiv:2506.10947 (2025).\n[3] Liu, Mingjie, et al. \"Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models.\" arXiv preprint arXiv:2505.24864 (2025).\n[4] Wu, Fang, et al. \"The Invisible Leash: Why RLVR May or May Not Escape Its Origin.\" arXiv preprint arXiv:2507.14843 (2025).\n[5] Zhao, Rosie, et al. \"Echo chamber: Rl post-training amplifies behaviors learned in pretraining.\" arXiv preprint arXiv:2504.07912 (2025).\n[6] Yue, Yang, et al. \"Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?.\" arXiv preprint arXiv:2504.13837 (2025)."}, "questions": {"value": "- Did the authors observe any instances where their format+length reward did not capture most of the performance gains from a correctness-based reward? Negative results in their setting are still informative.\n- Could the authors include a correctness + length-based reward baseline (weighted average, or add length reward only on correct responses)?\n- In Shao et al. they observe that their performance gains from random rewards may be due to bias in the GRPO formulation, particularly clipping focuses the model on its existing reasoning pattern distribution. If the authors remove clipping effects (either by removing clipping entirely, or making the rollout policy identical to the updating policy) do they observe the same effects?\n- How sensitive are the authors findings to their choice of p in their length reward?\n- What are the pass @ 64 performance metrics for the other models (I only saw numbers for Qwen 2.5 7B in Table 4, I apologize if I missed the others)? Does this correlate with OctoThinker’s comparatively worse relative gains from the format+length reward? More broadly, what are the authors’ thoughts on a necessary quantitative condition for the base model to fulfill for their proposed method to work effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8DOeuSsCc", "forum": "FxGCfnJswW", "replyto": "FxGCfnJswW", "signatures": ["ICLR.cc/2026/Conference/Submission16510/Reviewer_CwVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16510/Reviewer_CwVY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152446160, "cdate": 1762152446160, "tmdate": 1762926601800, "mdate": 1762926601800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}