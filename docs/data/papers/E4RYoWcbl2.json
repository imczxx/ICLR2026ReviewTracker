{"id": "E4RYoWcbl2", "number": 9647, "cdate": 1758132504072, "mdate": 1759897706825, "content": {"title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization", "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.", "tldr": "We propose a max-reward reinforcement learning framework for code optimization with large language models (LLMs).", "keywords": ["Large Language Model", "Code Optimization", "Inference-Time Techniques of Large Language Model", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f21bf03faa4279cf3f4a6ea260643f019c56b4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MaxCode, a framework that casts LLM-based code optimization as a max-reward reinforcement learning problem, focusing on finding the single best-performing solution rather than maximizing cumulative rewards. They enhanced the observation space by using an LLM-based \"critique\" model to translate raw execution feedback (e.g., timing, errors) into actionable, natural language insights, and collect the best-seen reward so far. The framework demonstrates significant performance improvements on the KernelBench (CUDA) and PIE (C++) optimization benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Actionable Feedback Loop:** Raw execution feedback (e.g., \"20% slower than the baseline\") is often unhelpful. Translating this into a diagnostic natural language critique (e.g., \"probable memory bandwidth bottleneck, consider fusing operations\") provides a much richer and more actionable signal for the generator LLM's iterative refinement.\n- **Strong Empirical Results:** The method achieves significant relative speedup improvements over strong baselines. The ablation studies validated that the critique and trajectory components are the primary drivers of this success."}, "weaknesses": {"value": "- **Critique Model as a Black Box:** The critique model is central to the paper's positive results, but it is treated as a given. There is no analysis of the quality of the critiques, common failure modes (e.g., does it ever \"hallucinate\" a bottleneck?), or whether a much smaller, fine-tuned, or distilled model could serve this purpose at a fraction of the cost.\n- **Unanalyzed Computational Cost:** The framework introduces significant computational overhead. At each step, it requires an inference pass from a powerful generator LLM and a separate critique LLM. This effectively at least doubles the LLM-related inference cost, in addition to the already expensive code execution step. This cost-benefit trade-off is not discussed.\n- **Clarity and Presentation:** The paper's clarity could be improved, making it difficult to follow. The introduction uses several non-standard terms (e.g., \"max-reward inference operator,\" \"best-discounted reward,\" \"categorical Value model\") without immediate, clear explanations. While the meaning can be inferred, a brief definition would be benficial to the readability.\n- **Minor:** The presentation of the figures needs improvement. The text in Figure 1 is too small to be legible, making its purpose unclear. Similarly, Figure 2 is titled \"MaxCode search method,\" but the diagram is a high-level data-flow chart, and it isn't immediately obvious how it represents a search algorithm."}, "questions": {"value": "- The critique model demonstrates a strong capability for generating high-quality, actionable suggestions. Have the authors considered using this model to directly generate code refinements, rather than only using its output as an observation for the separate policy model? This could potentially simplify the overall framework.\n- Could you elaborate on the training details for the reward-to-go model? The paper states it was trained on trajectory prefixes of length less than 2. For such short sequences, the max-reward value function would seem to be heavily influenced by the immediate reward rather than longer-term potential. What was the rationale for this choice, and how are the target values computed for these short prefixes when they are sampled from longer rollouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8eOe7A8DjL", "forum": "E4RYoWcbl2", "replyto": "E4RYoWcbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9647/Reviewer_qxWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9647/Reviewer_qxWk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504422572, "cdate": 1761504422572, "tmdate": 1762921175952, "mdate": 1762921175952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MaxCode, a max-reward RL formulation for inference-time code optimization. The method models iterative refinement as an MDP whose state augments initial/current code with execution feedback and natural-language critiques. It defines a max-reward return and trains a categorical reward-to-go/value model by discretizing speedup into bins; the learned value is then used to guide candidate expansion/selection at search time. Experiments on KernelBench (L1/L2) and PIE (subset) integrate MaxCode into Effi-Learner and CUDA-LLM, reporting gains in median max-speedup and average rank, with larger margins when combined with CUDA-LLM. Overall, the paper provides a unifying inference-time optimization view with modest empirical improvements."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Problem relevance: Inference-time optimization for LLM-generated code is practical and timely; even small speedups can be impactful in deployment.\n- Conceptual unification: Recasting existing refinement methods under a max-reward RL framework offers a common lens and a value-guided expansion heuristic that can plug into multiple search variants.\n- Empirical signal: On KernelBench/PIE, integrating MaxCode yields consistent but incremental improvements overall, especially with CUDA-LLM."}, "weaknesses": {"value": "- The framework mainly adapts existing iteration-based methods and execution-feedback-based prompt strategies; the max-reward RL formulation appears conceptual rather than introducing new algorithms.\n- Several core elements (e.g., critique usage, value estimation mechanism, Q-function applicability) are only loosely described, limiting reproducibility and technical insight.\n- Performance gains are small and inconsistent, baselines lack diversity beyond execution-feedback approaches, and ablations do not sufficiently justify which components drive improvements.\n- I find the paper difficult to follow. Important technical components are underspecified (e.g., how critique feedback is incorporated into the model state, how the Q-function is estimated in practice, and the exact architecture/training details for the reward-to-go model), making the approach challenging to reproduce or fully assess."}, "questions": {"value": "- Could you better justify how the max-reward formulation changes the decision process compared to existing iterative refinement? Currently, the policy remains fixed, so what specific improvement does RL provide beyond a unifying view?\n- Could you explicitly describe how natural-language critique influences candidate generation? Is critique concatenated in prompts only, or parsed into structured signals?\n- The paper lacks architecture and training details (prompt format, token budget, sampling strategy, stopping criteria). Could you elaborate to ensure reproducibility?\n- Since Effi-Learner was originally evaluated on Effi-Bench, why is Effi-Bench not included here? Would broader and more standard evaluation benchmarks affect conclusions?\n- Search-based optimization is stochastic. Could you report variance/error bars and test significance to confirm improvements are reliable?\n- The proposed method increases search iterations and critique calls. What is the runtime overhead relative to the speedup gains? Does the improvement justify the added compute cost?·"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zeLO9f2re6", "forum": "E4RYoWcbl2", "replyto": "E4RYoWcbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9647/Reviewer_cyNi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9647/Reviewer_cyNi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915787564, "cdate": 1761915787564, "tmdate": 1762921175463, "mdate": 1762921175463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MaxCode, a framework that formulates LLM code optimization as a max-reward RL problem. Its core idea is to augment the observation space using two key components: the best-so-far reward and a natural language critique generated by a separate LLM. This critique model translates raw execution feedback into actionable, diagnostic insights. Experiments show that the MaxCode framework significantly improves the optimization performance of existing search methods on the KernelBench and PIE benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper tackles optimizing code efficiency via LLMs problem by reframing inference-time search under a unified max-reward RL perspective.\n\n-\tThe formulation is modular and general, making it easy to plug into existing search pipelines such as CUDA-LLM or Effi-Learner, with consistent empirical gains across both CUDA and C++ domains.\n\n-\tThe critique-augmented observation design is intuitive yet effective, improving exploration quality without modifying base model weights.\n\n-\tThe experimental section covers multiple realistic benchmarks (KernelBench, PIE) and provides reasonably detailed ablations that support the main claims."}, "weaknesses": {"value": "-\tThe proposed max-reward RL formulation mainly reinterprets existing search heuristics under a unified lens. While clean and modular, it does not introduce a fundamentally new learning algorithm or search operator beyond the combination of best-so-far reward and critique-based observation.\n\n-\tThe section describing the generative value/reward-to-go model contradicts its reported results: the text claims underperformance on KernelBench-L1, yet Table 2 shows a clear gain. This discrepancy weakens confidence in the analysis and leaves the effectiveness of the reward model unclear.\n\n-\tThe “distribution-shift” explanation for the reward model’s instability is plausible but untested. No evidence (e.g., off-policy evaluation or correlation between predicted and actual rewards) is provided to substantiate this claim.\n\n-\tFigures are visually cluttered, lack clear labels or legends,  and do not convey the intended insights."}, "questions": {"value": "-\tCan you clarify the contradiction between the textual description of RQ4 and Table 2? Was this a labeling or analysis error?\n\n-\tDoes the critique model remain effective if used with open-source policies (e.g., CodeLlama, Qwen2-Coder), or is it tightly coupled to Claude-Sonnet?\n\n\n-\tHave you quantified how well the reward-to-go predictions correlate with actual speedup or final reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FFEGDuKaJa", "forum": "E4RYoWcbl2", "replyto": "E4RYoWcbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9647/Reviewer_drvW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9647/Reviewer_drvW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149178817, "cdate": 1762149178817, "tmdate": 1762921175058, "mdate": 1762921175058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors point out two challenging when optimizing code, including the complexity of code writing and the interpretation of performance metrics. To solve them, they explore inference-time search algorithms and propose the MaxCode framework, which unifies existing search methods under a max-reward reinforcement learning. The experiments on CUDA and C++ optimization benchmarks demonstrate the efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic and direction are practical and promising. Code optimization is important for CUDA and C++.\n2. The paper is well-written, and the motivation is clear."}, "weaknesses": {"value": "1. The presentation needs to be improved, especially Figure 1. The code in the figure is not clear.\n2. Commas and periods are missing in the formulas, e.g., Eq. (3), (4). In Lines 272-282, there is no Eq number. \n3. The evaluation data is limited. The results are evaluated on only two benchmarks. \n4. Figures 3 and 4 are not clear.\n5. The proposed method is not novel. It seems to combine the search algorithm and RL.\n6. The baselines are limited. Please compare with both open-source and closed-source SOTA LLMs, e.g., gpt5, Claude Sonnet 4.5, or Qwen, DeepSeek, etc. \n7. In Table 2, the improvement of +reward is not significant. \n8. The source code and data are missing."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UnVAQ2OWb6", "forum": "E4RYoWcbl2", "replyto": "E4RYoWcbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9647/Reviewer_2hbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9647/Reviewer_2hbN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762259271771, "cdate": 1762259271771, "tmdate": 1762921174688, "mdate": 1762921174688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}