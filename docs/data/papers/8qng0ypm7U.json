{"id": "8qng0ypm7U", "number": 7991, "cdate": 1758049715771, "mdate": 1759897816973, "content": {"title": "Confidence Calibration in Source-Free Domain Adaptation based on Pseudo-Labels", "abstract": "In this study, we explore the setting of source-free domain adaptation where access to labeled data from the source domain is restricted.\n We address the challenges associated with calibrating the prediction uncertainty of the adapted network solely based on unlabeled data from the target domain.    To approximate the unknown true labels, we use pseudo-labels generated by the source model.    Despite the high noise level in pseudo-labels, our empirical analysis reveals that the network’s accuracy computed using them closely matches the accuracy obtained with the true labels. Based on this observation, we propose a strategy for source-free confidence calibration. Our method is evaluated on standard domain adaptation benchmarks and achieves performance comparable to, or even better than, methods that require access to source data. Moreover, we significantly improve upon the state-of-the-art in source-free confidence calibration methods.", "tldr": "We suggest a confident calibration method for the source-free domain adaptation setup which is based on pseudo-labels.", "keywords": ["confidence calibration", "domain adaptation", "pseudo-labels"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e3be0fc09645a4c890b3fa1d9e6f645b06d5dde.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of confidence calibration in Source-Free Domain Adaptation (SFDA), a setting where pre-trained source models are adapted to unlabeled target domains without access to source data (due to privacy, legal, or proprietary constraints). A key limitation of existing work is that most calibration methods rely on labeled source data or labeled target data, i.e., assumptions incompatible with SFDA. The paper’s core empirical insight is that noisy pseudo-labels (generated from the source model) can reliably approximate true labels for accuracy estimation: even with significant noise, the accuracy computed using pseudo-labels closely matches that of true labels. Building on this, the authors propose Pseudo-Label Confidence Calibration (PLCC), a lightweight algorithm that: (1) generates Enhanced Pseudo-Labels (EPL) using a pre-trained feature extractor (Swin-B) and class centroids (via cosine distance), (2) estimates bin-wise accuracy using EPL, and (3) applies temperature scaling to minimize Adaptive Expected Calibration Error (adaECE) on the target domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. SFDA is critical for real-world deployments (e.g., medical imaging, autonomous driving) where source data access is restricted. The paper fills a key gap by focusing on confidence calibration.\n2. The core insight (pseudo-labels approximate true labels for accuracy estimation) is rigorously validated.\n3. Tests on 4 standard benchmarks and 3 distinct SFDA methods (DCPL, SHOT, AaD) ensure generalizability."}, "weaknesses": {"value": "1. The paper’s core insight (pseudo-label accuracy approximates true accuracy) is supported empirically but lacks theoretical grounding. For example, no formal proof or bounds on the error between pseudo-label-based accuracy and true accuracy.\n2. Minor typos (e.g., \"AAD\" instead of \"AaD\" in Tables 21–23) distract from readability.\n3. No comparison to recent SFDA calibration methods (e.g., post-2023 work beyond PseudoCal) to contextualize PLCC’s state-of-the-art claims."}, "questions": {"value": "1. How centroids are initialized and updated during adaptation (e.g., are centroids fixed after initial computation, or refined iteratively?).\n2. Why Swin-B is chosen over other feature extractors (e.g., ViT, ResNet), no ablation on feature extractor impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8vHhFrUD9j", "forum": "8qng0ypm7U", "replyto": "8qng0ypm7U", "signatures": ["ICLR.cc/2026/Conference/Submission7991/Reviewer_oQVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7991/Reviewer_oQVS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479117893, "cdate": 1761479117893, "tmdate": 1762919996719, "mdate": 1762919996719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PLCC, a method for calibrating models in source-free domain adaptation using pseudo labels instead of true labels. It assumes that the agreement between model predictions and pseudo labels approximates their agreement with ground truth, enabling label-free calibration estimation. The authors enhance pseudo labels through an EPL process that refines class assignments using pre-trained features and class centroids. Experiments on several benchmarks show that PLCC achieves lower calibration errors compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and practical problem, i.e., calibration for source-free domain adaptation, which has received limited attention.\n2. The proposed PLCC method is conceptually simple and easy to implement.\n3. The authors conduct extensive experiments on multiple benchmarks, showing consistent improvements in calibration metrics.\n4. The paper is well organized and reproducible."}, "weaknesses": {"value": "1. I think the main weakness of this paper is that it approaches the problem based on confidence calibration theory, but the theoretical depth is far from enough. This needs to be strengthened. Specifically, the conditions and implications of equations (3) and (4) are worth calibrating and considering. In addition, the paper relies on a marginal approximation rather than a conditionally valid equality, which the authors describe as a “very mild assumption.” However, the manuscript does not specify under what distributions or scenarios this assumption holds or provide any theoretical limits on when it fails. Moreover, while Equation (5) approximates overall accuracy using pseudo labels, there is no analysis of the estimation error’s bias, variance, or sample complexity, nor any bound under varying noise rates. This weakens the theoretical soundness of the proposed approach.\n2. No ablation on pseudo-label quality. The method assumes EPL substantially improves label reliability, but there is no quantitative study of pseudo-label noise rate before and after EPL.\n3. The approach is designed specifically for closed-set classification; it is unclear whether PLCC works for open-set or multi-label tasks common in SFDA research.\n4. Lack of UDA task-specific qualitative analysis."}, "questions": {"value": "The methodology in this paper appears relatively simple, suggesting that the author could strengthen it with more in-depth theoretical foundations and sound analytical support to demonstrate its effectiveness. I believe the current version of the paper is somewhat superficial and could benefit from greater theoretical depth. In addition, the experimental design may warrant further consideration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6upTq4bFUC", "forum": "8qng0ypm7U", "replyto": "8qng0ypm7U", "signatures": ["ICLR.cc/2026/Conference/Submission7991/Reviewer_K3qN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7991/Reviewer_K3qN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571873096, "cdate": 1761571873096, "tmdate": 1762919996304, "mdate": 1762919996304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a source-free confidence calibration method. Through experimental observations, the authors find that noisy pseudo-labels can serve as effective surrogates for true labels in source-free domain adaptation calibration, even without access to target ground-truth labels. Building on this insight, they introduce PLCC, a prototype- and enhanced pseudo-label–based confidence calibration approach that derives a final temperature scale for model calibration. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem studied in this paper is meaningful and challenging.\n    \n- The proposed method has been validated on standard SFDA benchmarks and shows effectiveness."}, "weaknesses": {"value": "- One major concern lies in the strong assumption behind Equation (3) in the main paper. Is this equality a general phenomenon? Since it serves as the key motivation for algorithm development, clarifying this assumption is crucial to ensuring the soundness of the paper.\n    \n- If noisy pseudo-labels can already be regarded as true labels for SFDA calibration, it remains unclear why the paper introduces the enhanced pseudo-label strategy.\n    \n- It appears that most of the performance gains stem from the enhanced pseudo-labels, which, however, are not thoroughly discussed. As this enhancement depends on an external pre-trained model, have the authors explored alternative pseudo-label enhancement strategies?\n    \n- When discussing computational complexity, is the use of the external pre-trained model taken into account?\n    \n- The overall presentation could be further improved. Please refer to the detailed comments below:\n    \n    - In the abstract, it is unclear what accuracy refers to — is it a pure performance metric or a calibration-related measure?\n        \n    - The methodology section could highlight the key points more clearly. If the main technical novelty lies primarily in temperature scaling, it may be considered somewhat limited.\n        \n    - Minor suggestion: Since the method is developed based on empirical observations, Figure 4 would be more appropriately placed earlier in the paper (e.g., in the Introduction) to better motivate the approach.\n\n- There is a lack of the LLM usage declaration in the manuscript."}, "questions": {"value": "I have listed all my questions and concerns in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4qjfXUuVnM", "forum": "8qng0ypm7U", "replyto": "8qng0ypm7U", "signatures": ["ICLR.cc/2026/Conference/Submission7991/Reviewer_toFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7991/Reviewer_toFH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656299161, "cdate": 1761656299161, "tmdate": 1762919995945, "mdate": 1762919995945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}