{"id": "8mal7Q96Fc", "number": 11698, "cdate": 1758203157135, "mdate": 1763035950834, "content": {"title": "Mitigating Classifier Dimensional Collapse via Random Masking in Federated Linear Probing", "abstract": "Integrating a pre-trained model into federated learning (FL) is an emergent direction to facilitate the industrial deployment. Federated linear probing (FLP) is a practical paradigm that gains the communication efficiency from FL and generalization from pre-trained model, while still suffering from dimensional collapse that undermines its effectiveness. Dimensional collapse, originating from data heterogeneity, challenges the embedding space construction in FL, leading to suboptimal convergence and generalization. With a generalized frozen embedding extractor, FLP seems to be robust against dimensional collapse. However, in this paper, we emphasize that the dimensional collapse can also be represented in classifier construction, affecting the performance of the model. We propose FedRM to solve this problem, which randomly masks the dimension of the embedding and the classifier during the training to enforce the classifier to focus fairly on each dimension, guaranteeing diversity during decision generation. The simplicity of the method retains the communication efficiency of the FLP. We conduct empirical experiments to comprehensively evaluate the performance of FedRM. The results show FedRM achieves an overwhelming trade-off between efficiency and utility.", "tldr": "", "keywords": ["federated learning", "data heterogeneity", "dimensional collapse", "pre-trained model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7e5ee132030c7e5f5ddc7bf41e3ff0f7eca439d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies dimensional collapse in federated linear probing (FLP) scenarios and proposes FedRM, a method that applies multi-ratio random masking to both the embedding and the classifier during training. By enforcing attention to diverse subsets of feature dimensions, FedRM aims to avoid over-reliance on a few discriminative directions, promoting more robust classifier generalization under data heterogeneity. The method is lightweight and communication-efficient. Extensive experiments are conducted on various image classification benchmarks under heterogeneous federated settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1ÔºâThe paper identifies an overlooked problem in federated learning, dimensional collapse at the classifier level, even when embeddings from frozen pre-trained backbones are robust, highlighting an important gap in the current literature.\n\n2ÔºâFedRM is straightforward to implement and computationally lightweight, and can be seamlessly integrated into existing methods. Furthermore, the overhead introduced by this method is minimal, making it highly efficient.\n\n3ÔºâThe paper is clearly written and easy to follow. Additionally, Figure 2 provides an intuitive overview of FedRM, helping readers grasp the workflow and the masking scheme."}, "weaknesses": {"value": "1ÔºâThe paper's core premise focuses on FLP, which is an increasingly outdated approach. The current state-of-the-art in federated fine-tuning predominantly leverages Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA or Adapters, which adapt the model's internal representations rather than just the final layer. It is widely acknowledged that adapting only the final output layer often yields suboptimal performance, thus limiting the practical utility of the proposed method.\n\n2ÔºâThe paper's central claim‚Äî\"dimensional collapse in the classifier\" (Section 4.1)‚Äîis poorly substantiated. The analysis is superficial, relying on brief textual descriptions rather than rigorous theoretical or experimental evidence. Despite motivating the entire work from this perspective, the paper lacks deep theoretical insights. No formal definitions or quantification (e.g., metrics reflecting the effective dimensionality or a direct analysis of the classifier weights' singular value distribution) are provided to empirically demonstrate the existence or severity of this challenge.\n\n3Ôºâ The proposed FedRM method offers limited novelty, as it closely resembles a standard regularization technique (e.g., Dropout) applied to the embeddings and classifier weights. Additionally, the experimental evaluation is confined to traditional classification tasks. This overlooks the current, dominant trend of applying pre-trained models (such as LLMs and MLLMs) to more complex and relevant generative and understanding tasks.\n\n4ÔºâThe choice of baselines is weak and lacks diversity. The paper fails to benchmark against modern, relevant state-of-the-art methods in federated PEFT (e.g., FLoRA[1]). Furthermore, there is no direct benchmarking or discussion of other highly relevant recent approaches that also employ masking, random noise, or collapse mitigation strategies in federated learning.\n\n\n\n[1] FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TD7yx4YHdv", "forum": "8mal7Q96Fc", "replyto": "8mal7Q96Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11698/Reviewer_7jxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11698/Reviewer_7jxB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724415961, "cdate": 1761724415961, "tmdate": 1762922747215, "mdate": 1762922747215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Q4Nkm2eLI9", "forum": "8mal7Q96Fc", "replyto": "8mal7Q96Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11698/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11698/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763035949880, "cdate": 1763035949880, "tmdate": 1763035949880, "mdate": 1763035949880, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the dimensional collapse presented in the classifier due to data heterogeneity. The proposed method, FedRM, introduces random masks during training to enforce the classifier to focus fairly on each dimension. Extensive experiments under various models and datasets demonstrate that the proposed method achieves an overwhelming trade-off between efficiency and utility. However, the score of this paper tends toward rejection because: (1) the core claim regarding classifier dimensional collapse is not well justified both theoretically and empirically, and (2) the experimental claims do not fully support the contribution of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper addresses the data heterogeneity problem in federated linear probing through simple yet effective modifications. Extensive experiments across various models and datasets demonstrate that the proposed method achieves an excellent trade-off between efficiency and utility."}, "weaknesses": {"value": "(1) The core claim regarding classifier dimensional collapse is not well justified both theoretically and empirically. To address this concern, please clarify and provide detailed explanations for the following:\n\n(1-a) A critical gap in the analysis is how FedAvg's aggregation mechanism interacts with local dimensional collapse. When multiple clients with different biased classifiers aggregate their parameters, the averaging process naturally generalizes dimension-specific biases across clients. The paper claims that ‚Äúthe global classifier inevitably inherits the dimensional collapse‚Äù (lines 280‚Äì282) without addressing why FedAvg's generalization does not mitigate this issue. Please clarify: What specifically differentiates the collapse mitigation achieved by FedRM from the natural mitigation effect of FedAvg aggregation? Can you provide a comparative analysis regarding this concern? For example, showing dimension utilization patterns for (i) local classifiers before aggregation, (ii) the global classifier after FedAvg, and (iii) the global classifier with FedRM. This is particularly important since the Personalized FL literature suggests that simple aggregation like FedAvg can diminish individual client characteristics under data heterogeneity, which would naturally counteract dimension-specific biases.\n\n(1-b) The paper lacks a theoretical analysis explaining why uniform dimension utilization is beneficial. Why is forcing the classifier to attend to all dimensions uniformly better than allowing it to focus on task-relevant dimensions? Could enforcing uniform utilization harm performance when certain dimensions are genuinely more informative for the task? Please provide a clear and intuitive explanation, preferably with empirical support.\n\n(1-c) The paper claims that ‚Äúwe investigate dimensional collapse occurring in the classifier in FLP, where data heterogeneity causes the classifier to rely on only a few dimensions rather than attending uniformly to all dimensions.‚Äù (lines 125‚Äì127), yet provides no empirical evidence, such as a visualization of the parameters of the classifier.\n\n(2) The experimental claims do not fully support the contribution of the proposed method. To address this concern, please clarify and provide detailed explanations for the following:\n\n(2-a) All results appear to be from single runs without confidence intervals. For rigorous evaluation, please provide mean ¬± standard deviation over multiple random seeds.\n\n(2-b) With M masks, the proposed method performs M forward passes per training step. How much does the computational cost increase as the number of masks increases, and how many masks are used as the default?\n\n(2-c) Figure 3 only removes masks incrementally, but critical ablations need to be provided to verify the robustness of the proposed method, such as the effect of the regularization weight $Œ±_{RM}$.\n\n(2- d) Performance comparison with zero-shot methods does not necessarily support the superiority of the proposed method, since those methods are training-free and have fundamentally different computational profiles. If the authors aim to present the superiority of the FLP-like method, a full fine-tuning‚Äìbased method needs to be added to the comparison.\n\nAlso, there are several minor issues to improve the paper:\n\n(3) For reproducibility, it would be better to provide complete algorithm pseudocode showing the full training loop, including mask generation, application, and aggregation.\n\n(4) Figure 2 has multiple issues: the server‚Äôs mask section shows two arrows with unclear meaning; the client‚Äôs embedding is depicted as a matrix, but Equation (8) (line 248) defines $ùëÖ_{ùëò,ùëñ} ‚àà ùëÖ^{1√óùëë}$ as a vector. If the matrix represents batched embeddings (batch_size √ó d), this should be explicitly stated, and the notation should be adjusted throughout."}, "questions": {"value": "Please refer to the Weakness section for detailed comments. In particular, I would appreciate clarification on the questions raised for each weakness. I will reconsider my evaluation after reviewing the authors‚Äô rebuttal to these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HBWYxkQ6w9", "forum": "8mal7Q96Fc", "replyto": "8mal7Q96Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11698/Reviewer_7ofs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11698/Reviewer_7ofs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817620062, "cdate": 1761817620062, "tmdate": 1762922746816, "mdate": 1762922746816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that under Non-IID data, dimensional collapse can occur at the classifier layer even if the backbone is robust. The authors propose FedRM, a multi-ratio random masking regularizer applied to both embeddings and the classifier during local training, which forces the head to exploit diverse feature dimensions rather than a few dominant directions. Experiments across several vision datasets and two backbones show consistent gains over FedAvg and several PEFT/zero-shot baselines, and the gains grow with dataset difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper articulates a specific and important problem within FLP. The concept of shifting \"dimensional collapse\" from the representation level to the classifier level is an interesting and plausible argument in the context of data heterogeneity.\n\n2. The proposed FedRM method is conceptually clear and simple to implement.\n\n3. The ablation study clearly demonstrates the benefit of using multi-ratio masks."}, "weaknesses": {"value": "1. The paper's core premise is the existence of \"classifier dimensional collapse\". However, the evidence provided is indirect. The paper lacks a direct empirical analysis of this phenomenon, e.g., an analysis of the singular value spectrum or effective rank of the global classifier's weight matrix in the baseline (FedAvg FLP), which could visually demonstrate the \"collapse\" it claims to solve.\n\n2. The idea of random masking as a regularizer has strong conceptual links to techniques like Dropout and DropConnect. The paper fails to compare or contrast FedRM with these classic regularization techniques.\n\n3. The results in Appendix Table 5 show that FedRM's performance on the PathMNIST dataset (51.66%) is dramatically worse than the FedAvg baseline (65.49%). This \"performance degradation\"  directly contradicts the method's core claim of improving performance under heterogeneity. The authors note this in the text but provide no explanation."}, "questions": {"value": "1. Can you provide direct evidence that \"classifier dimensional collapse\" is happening in the FedAvg (FLP) baseline and that FedRM is mitigating it?\n\n2. Why does FedRM perform so much worse than the FedAvg baseline on PathMNIST (Table 5)? Does this imply that the regularization method can be detrimental for certain data distributions or tasks?\n\n3. Regarding the ViT-Base results (Table 4), given that the Fed3R initialization is so effective, did you attempt to combine Fed3R initialization with your FedRM regularization? The two methods seem orthogonal and might yield the best performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ryzrU5jPTT", "forum": "8mal7Q96Fc", "replyto": "8mal7Q96Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11698/Reviewer_kqJq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11698/Reviewer_kqJq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827177919, "cdate": 1761827177919, "tmdate": 1762922746394, "mdate": 1762922746394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper developed an effective framework, namely FedRM, to mitigate classifier dimensional collapse under data heterogeneity. The core idea of FedRM is to employ multiple random masks with varying ratios applied to the embedding and the corresponding mask regularizations to jointly guide the optimization process. Extensive experimental results show the superiority of FedRM compared with existing baselines. However, there exist some concerns as follows: 1) The idea of this paper is too simple and why multiple random masks can address the issue of dimensional collapse is not clear; 2) A lot of federated pre-trained model frameworks were proposed recently, but the baselines used in the experiments are not state-of-art methods; 3) The theoretical analysis and experimental evaluation for why multiple random masks can address the issue of dimensional collapse are missing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper developed an effective framework, namely FedRM, to mitigate classifier dimensional collapse under data heterogeneity. The core idea of FedRM is to employ multiple random masks with varying ratios applied to the embedding and the corresponding mask regularizations to jointly guide the optimization process. Extensive experimental results show the superiority of FedRM compared with existing baselines. The performance of FedRM becoming increasingly significant as the complexity of the dataset grows."}, "weaknesses": {"value": "I have some concerns as follows: \n1) The idea of this paper is too simple and why multiple random masks can address the issue of dimensional collapse is not clear; \n2) A lot of federated pre-trained model frameworks were proposed recently, but the baselines used in the experiments are not state-of-art methods; \n3) The theoretical analysis and experimental evaluation for why multiple random masks can address the issue of dimensional collapse are missing."}, "questions": {"value": "I have some questions as follows:\n1) The authors should add the theoretical analysis and experimental evaluation for why multiple random masks can address the issue of dimensional collapse are missing. The idea of this paper is too simple and why multiple random masks can address the issue of dimensional collapse is not clear.\n2) A lot of federated pre-trained model frameworks were proposed recently. The authors should add some state-of-art methods in the experiments.\n3) In some experimental results, the performance of FedRM looks poor, such as Tables 4 and 5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wM0VqkJzvI", "forum": "8mal7Q96Fc", "replyto": "8mal7Q96Fc", "signatures": ["ICLR.cc/2026/Conference/Submission11698/Reviewer_TLQW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11698/Reviewer_TLQW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982757214, "cdate": 1761982757214, "tmdate": 1762922745581, "mdate": 1762922745581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}