{"id": "tXEA2cwHum", "number": 22380, "cdate": 1758330312275, "mdate": 1759896869478, "content": {"title": "Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization", "abstract": "Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta weights between the customized LLM and the corresponding base model. However, they exhibit inadequate performance at high compression ratios due to their empirical nature. In this work, we introduce DeltaMix, an adaptive mixed-precision delta-compression framework designed to minimize quantization error in the singular value decomposition (SVD) space without imposing additional assumptions. DeltaMix provides a theoretical justification for the necessity of mixed-precision compression and presents a practical quantization solution that involves solving a 0/1 linear integer programming problem alongside a reconstruction target correction method. Experimental results across multiple models and benchmarks illustrate that DeltaMix consistently outperforms all baseline methods. Notably, on tasks such as AIME2024 and GQA, DeltaMix exceeds the performance of the best baseline, Delta-CoMe, by 22.3\\% and 6.1\\% for 7B parameter models, respectively.", "tldr": "", "keywords": ["delta-compression", "quantization", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30f2dad99c341359f8761848d828e457a6970b94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes an SVD-guided approach for low-bit mixed-precision compression of weight deltas (differences between original and fine-tuned models). The authors first provide a theoretical derivation showing that mixed-precision quantization is beneficial for the V matrix but less useful for the U matrix in SVD decomposition. To compensate for the error incurred by quantization of the V matrix during the U quantization step, the authors introduce Reconstruction Target Correction, which shifts U to a new optimal value. The analysis is accompanied by empirical evidence. Based on this foundation, the authors formulate the optimal compression as an integer programming problem and leverage a dedicated solver. The proposed method is validated on several large language and vision models and compared with prior work on delta compression."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The SVD-guided search for mixed-precision quantization appears to be novel in the context of model compression.\n* Delta-Mix noticeably outperforms baselines in terms of final accuracy for the same compression target."}, "weaknesses": {"value": "* While theoretically and practically sound, the proposed method—formulated as compression of a model delta of the same size as the original model—still seems less appealing than established PEFT techniques [1, 2, 3, 4]. LoRA adapters can be one or two orders of magnitude smaller than the total number of model parameters, yet remain competitive with full fine-tuning when properly tuned [5]. The learning rate adopted in the experiments (4e-5) may not be optimal for LoRA.\n\n---\nReferences\n\n[1] Hu, Edward J., et al. \"Lora: Low-rank adaptation of large language models.\" ICLR 1.2 (2022): 3.\n\n[2] Liu, Shih-Yang, et al. \"Dora: Weight-decomposed low-rank adaptation.\" Forty-first International Conference on Machine Learning. 2024.\n\n[3] Zhang, Qingru, et al. \"Adalora: Adaptive budget allocation for parameter-efficient fine-tuning.\" arXiv preprint arXiv:2303.10512 (2023).\n\n[4] Kopiczko, Dawid J., Tijmen Blankevoort, and Yuki M. Asano. \"Vera: Vector-based random matrix adaptation.\" arXiv preprint \narXiv:2310.11454 (2023).\n\n[5] https://thinkingmachines.ai/blog/lora/"}, "questions": {"value": "* Can the proposed method be applied to LoRA adapters? This would potentially enable even higher compression rates and the possibility of serving a large number of fine-tuned versions of a given model simultaneously.\n\n\n* The proposed method seems to be quantized representation-agnostic. Can Delta-Mix be combined with a vector quantization scheme [1, 2, 3] to achieve even higher compression rates with minimal performance degradation?\n\n\n* LoRA may sometimes lack sufficient expressiveness to fully capture the difference between two models when the difference is substantial. How well do sparse + low-rank adapters [4] perform in this context?\n\n---\nReferences\n\n[1] Van Baalen, Mart, et al. \"Gptvq: The blessing of dimensionality for llm quantization.\" arXiv preprint arXiv:2402.15319 (2024).\n\n[2] Egiazarian, Vage, et al. \"Extreme compression of large language models via additive quantization.\" arXiv preprint arXiv:2401.06118 (2024).\n\n[3] Chee, Jerry, et al. \"Quip: 2-bit quantization of large language models with guarantees.\" Advances in Neural Information Processing Systems 36 (2023): 4396-4429.\n\n[4] Nikdan, Mahdi, et al. \"Rosa: Accurate parameter-efficient fine-tuning via robust adaptation.\" arXiv preprint arXiv:2401.04679 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PchJAbJYLz", "forum": "tXEA2cwHum", "replyto": "tXEA2cwHum", "signatures": ["ICLR.cc/2026/Conference/Submission22380/Reviewer_g1TA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22380/Reviewer_g1TA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381696403, "cdate": 1761381696403, "tmdate": 1762942192019, "mdate": 1762942192019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DeltaMix, an adaptive mixed-precision delta-compression framework for fine-tuned large language models.\nThe method decomposes delta weights using SVD and formulates quantization as minimizing layer-wise reconstruction error.\nUnlike prior empirical approaches, DeltaMix derives a mathematically grounded formulation showing why mixed-precision is necessary and models bit allocation as a 0/1 integer-linear program with an additional RTC step.\nExtensive experiments on reasoning, math, code, and multimodal benchmarks across diverse models demonstrate consistent performance gains, while also reducing GPU memory usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Strong theoretical foundation.** The work formalizes SVD-based delta-compression as an explicit quantization-error-minimization problem and proves the necessity of mixed-precision allocation, advancing the theoretical rigor of delta-compression research.\n\n**2. Comprehensive empirical validation.** Evaluations on 7B and 14B LLMs across four domains (reasoning, math, code, vision-language) show clear and reproducible gains over Delta-CoMe, BitDelta, and low-rank baselines.\n\n**3. Practical deployment benefits with thorough system analysis.** The paper provides valuable end-to-end evaluation showing 6× memory savings and superior scaling properties, enabling deployment of different models for uncompressed approaches. \nThe analysis of prefill time, generation speed, and varying arrival rates demonstrates real-world applicability beyond just accuracy metrics."}, "weaknesses": {"value": "**1. Limited scalability analysis.** While integer-linear optimization is solved once per model, reported solving times (≈ 30 min for 7B) may become impractical for larger or frequent model updates. \nDiscussion on scaling to 70B+ models is missing.\n\n**2. Ablation study.** Although four task types are covered, the paper lacks ablation on calibration-set size, bit-budget sensitivity, or robustness under distribution shift, which are important for real-world deployment.\n\n**3. Computational overhead.** Table 10 shows DeltaMix requires 3× more time than Delta-CoMe (per block), totaling 1-3 hours for full models. \nWhile the paper dismisses this as \"acceptable since quantization is performed only once,\" this represents significant overhead for practitioners, especially for larger models or when iterating on model development."}, "questions": {"value": "**1. Complexity and scalability.** How does the integer-program’s solving time and memory footprint scale with layer size and number of candidate bit-widths? \nCould approximate or heuristic solvers yield near-optimal results faster?\n\n**2. Sensitivity towards calibration data.** How robust is the bit-allocation when the calibration set is small or domain-mismatched?\nDoes performance degrade significantly with limited calibration data, and how does this compare to baselines that may be less calibration-dependent?\n\n**3. Generalization to other compression forms.** Could the same error-minimization principle be adapted for pruning or hybrid pruning-quantization pipelines?\n\n**Justification for Rating.**\n\nThe paper presents a novel and theoretically motivated approach to delta compression.\nHowever, the experimental section lacks sufficient analysis of scalability, sensitivity, and efficiency trade-offs, which limits the practical completeness of the proposed framework.\nI am open to raising the score if these concerns are adequately addressed in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RUKTeNIG9t", "forum": "tXEA2cwHum", "replyto": "tXEA2cwHum", "signatures": ["ICLR.cc/2026/Conference/Submission22380/Reviewer_mFqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22380/Reviewer_mFqR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923089545, "cdate": 1761923089545, "tmdate": 1762942191796, "mdate": 1762942191796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DELTAMIX, a delta-compression framework that works in the SVD space of the fine-tuned-minus-base weight matrix $(W = U\\Sigma V)$. The key analytical step decomposes the per-row quantization error for (V) into a fixed “scaling” term $(\\Sigma_{ii}^2)$ and a data-dependent “difference” term $(\\Delta V_i X X^\\top \\Delta V_i^\\top)$. This yields a rationale for row-wise mixed precision on (V) under a global bit-budget. The bit allocation is cast as a 0/1 integer linear program, and the method introduces a Reconstruction Target Correction (RTC) to reduce bias when later quantizing (U). Experiments across reasoning, math, code, and multimodal tasks claim consistent gains over SVD low-rank, BitDelta, and Delta-CoMe at (\\alpha=1/16), including large margins on AIME2024 (e.g., +22.3% over Delta-CoMe for 7B) and improved memory/speed scaling when hosting many deltas. Reported quantization overhead is higher than Delta-CoMe but presented as a one-time cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Principled objective: Explicitly minimizes a reconstruction-error surrogate in SVD space, yielding a clear justification for row-wise mixed precision of (V) under a bit budget. The $(\\Sigma_{ii}^2)$ scaling vs. difference decomposition is intuitive and actionable. \n* Concrete optimization: Bit allocation via 0/1 ILP provides a crisp mechanism to trade off error and storage, with constraints for budget and a cap $(f_{\\max})$ on distinct bitwidths. \n* RTC mechanism: The Reconstruction Target Correction before quantizing (U) reduces deviation induced by using $(\\hat{V})$ as the target, with measurable gains in harder regimes. \n* Empirical coverage: Multi-task evaluation (math/reasoning/code/VLM) across 7B and 13–14B backbones; large improvements are shown where $(\\lVert \\Delta W \\rVert)$ is big (e.g., AIME2024, some multimodal). \n* Serving relevance: Memory and latency scaling when hosting many fine-tuned variants is compelling; DELTAMIX supports more concurrent models than baselines in the reported setup."}, "weaknesses": {"value": "1. Inconsistency with “no singular-value assumptions.” The method claims to avoid empirical reliance on singular values, yet Section D.1 discards the last (k) ranks by singular-value magnitude to accelerate quantization, explicitly invoking the “larger singular values are more important” heuristic that the paper earlier critiques. This weakens the methodological positioning and may bias comparisons. \n2. Fair-budget accounting is under-specified. Results are reported at $(\\alpha = 1/16)$, but the paper does not precisely tabulate end-to-end storage (including $(U, \\Sigma, V)$, any indices/masks, solver-driven zero-bit ranks, and calibration metadata) vs. baselines. Without an apples-to-apples byte breakdown, it’s hard to assess dominance beyond accuracy. \n3. Selective gains; some regressions. While DELTAMIX shines on AIME2024 and certain VLM settings, elsewhere it’s only on par or slightly worse (e.g., 13–14B Math500 in Table 2). The average gains (~2–3%) are modest and may not outweigh extra complexity in production settings. \n4. Calibration sensitivity not analyzed. The difference term depends on calibration activations (X). The paper doesn’t study how sample size, domain shift, or layer-wise weighting affect EV estimates and allocations, nor robustness across seeds. \n5. Optimization overhead and practicality. The ILP solve is reported as ~29.4 minutes for a 7B variant; quantization takes ~1.2 h for 7B and ~2.4 h for 14B, versus ~0.4 h / 0.8 h for Delta-CoMe, all on a single GPU. This overhead may be non-trivial at scale, especially if per-task calibrations are required. \n6. Baselines and scope. Comparisons omit some strong PTQ/structured baselines relevant to error control (e.g., SPQR/SPQR-like sparse-quant, SVD-LLM variants in comparable regimes) or server-side delta systems beyond Delta-CoMe. This leaves open whether the observed gains are specific to the chosen set. \n7. Missing systems details. The paper does not clearly state how $(\\Sigma)$ is stored/quantized, nor the runtime cost of reconstructing (W) vs. baseline delta formats. The serving experiment is helpful but still abstracts away some operator-level costs. \n8. Ablations are narrow. The $(f_{\\max})$ study covers one model/task slice; RTC ablation is limited in breadth. A per-layer bit allocation analysis vs. error/outliers is mentioned, but stronger causal links to downstream accuracy would help."}, "questions": {"value": "1. Budget parity: Provide a byte-accurate storage table for every method and model (including $(\\Sigma)$, indices, zero-bit ranks, any metadata). Confirm that $(\\alpha=1/16)$ implies comparable on-disk and in-memory footprints across methods. \n2. Singular-value reliance: Reconcile the claim to “eschew reliance on singular values” with D.1 rank truncation by $(\\sigma)$. Can you replicate results without this heuristic, or with a heuristic-free pruning guided solely by EV? \n3. Calibration robustness: How many calibration samples are used per layer, how are they selected, and what is the variance of EV and the ILP allocation across seeds/domains? Show accuracy vs. calibration-set size curves. \n4. RTC cost/benefit: Quantify the computational overhead of RTC and analyze when it helps most. Could a joint optimization of $(U,V)$ under the same objective remove the need for RTC? \n5. $\\Sigma$ handling: Are singular values stored in full precision? If quantized, to what precision, and how does that trade off with accuracy vs. bits? \n6. Operators/runtime: Provide end-to-end operator-level latency of reconstructing (W) or applying $(U\\Sigma V)$ directly vs. baselines for single- and multi-tenant serving, including prefill and decode breakdowns. \n7. Broader baselines: Add comparisons to SPQR-style sparse-quant and SVD-LLM truncation-aware variants under matching storage, plus recent delta/tuning hybrids, to strengthen the empirical case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pPcaHZZyCX", "forum": "tXEA2cwHum", "replyto": "tXEA2cwHum", "signatures": ["ICLR.cc/2026/Conference/Submission22380/Reviewer_jUCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22380/Reviewer_jUCn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924872322, "cdate": 1761924872322, "tmdate": 1762942191502, "mdate": 1762942191502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper proposes EDC (Enhancing Delta Compression), a framework for improving delta-based model compression of large language models. It focuses on compressing fine-tuned deltas between a base model and its adapted version (e.g., instruction-tuned or domain-specific variants). The core idea is to enhance representational compactness by combining adaptive low-rank decomposition, residual quantization, and layer-wise scaling reweighting of delta tensors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a practical problem in model distribution and storage: delta checkpoint compression for multi-task or multi-domain fine-tuned models."}, "weaknesses": {"value": "- The core idea—combining low-rank and quantized residual compression, is well explored in prior works such as QLoRA, AdaLoRA, and CompAdapter. The proposed “layer-wise scaling reweighting” is a small variant of norm-based importance metrics used in parameter-efficient tuning.\n\n- The method is entirely empirical. The paper lacks mathematical justification or analysis on how the scaling or residual quantization improves representational fidelity beyond heuristic intuition.\n\n- Experiments are restricted to a few fine-tuning tasks and medium-size models (≤13B). No results are provided for large instruction-tuned LLMs (>70B) or multi-domain deltas where compression instability typically arises.\n\n- The paper reports storage reduction but not latency, energy, or end-to-end loading improvements. For real-world LLM deployment, I/O and kernel fusion dominate runtime, which EDC does not address."}, "questions": {"value": "- How does EDC interact with prefix caching or parameter sharing in serving systems? Can deltas be applied incrementally without full reconstruction?\n\n- What is the compression–accuracy trade-off compared to existing adapter compression frameworks like CompAdapter or LoRA-Prune under identical settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3wnKwa2aw", "forum": "tXEA2cwHum", "replyto": "tXEA2cwHum", "signatures": ["ICLR.cc/2026/Conference/Submission22380/Reviewer_yi6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22380/Reviewer_yi6X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976423321, "cdate": 1761976423321, "tmdate": 1762942191250, "mdate": 1762942191250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}