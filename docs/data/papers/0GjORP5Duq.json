{"id": "0GjORP5Duq", "number": 24104, "cdate": 1758352745950, "mdate": 1759896781548, "content": {"title": "RACA-CLIP: Relation-Aware Compositional Alignment for CLIP", "abstract": "Vision-Language Models (VLMs) such as CLIP excel at broad multimodal tasks, yet struggle with compositional reasoning. Despite capturing coarse correlations, they often act like “bags-of-words” missing fine-grained structures such as object–attribute bindings and inter-object relations. We attribute this to: (i) limited compositional diversity in large-scale image–text data, and (ii) contrastive objectives that emphasize global alignment over grounded structure. To address this, we propose a hierarchical fine-grained alignment framework that explicitly bridges visual and textual components at the object, attribute, and relation levels. Unlike prior work relying on parsers, we leverage scene graph annotated datasets for structured supervision, requiring no extra labeling. We introduce a hierarchical fine-grained loss to complement standard contrastive learning by grounding entities and relations across modalities. Experiments on compositional benchmarks SugarCrepe, What’sUp, and Cola show large gains in capturing nuanced structure, while preserving performance on standard vision-language tasks. RACA CLIP method improves compositional reasoning accuracy by +24.86% on SugarCrepe, +5.7% on What’sUp, and +4.76 on Cola, offering a simple yet effective path toward stronger, human-like compositional understanding.", "tldr": "Building compositionality robust CLIP model by region aware training objectives, pushing them towards better reasoning.", "keywords": ["Explainablity", "Vision-Language Models", "Compositionality"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bd2b282be01ace8b9e99b998f90c76c516b8eed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the persistent challenge of compositional reasoning in vision–language models such as CLIP. \nIt proposes RACA-CLIP, a structured contrastive learning framework that integrates scene-graph supervision to align visual and textual representations at the object, attribute, and relation levels.\nThe method achieves consistent gains on compositional reasoning benchmarks and maintains competitive zero-shot performance on ImageNet and retrieval tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper propose a structured contrastive framework that integrates scene-graph representations into CLIP models, which aligns image regions with corresponding text descriptions via IoU-based multi-positive matching.\n2. Comprehensive experiments on compositional benchmarks demonstrate the effectiveness with substantial gains and analysis. The inclusion of weight interpolation analysis and representation-level statistics strengthens interpretability."}, "weaknesses": {"value": "1. Although RACA-CLIP claims improved compositional reasoning, the evaluation relies mainly on accuracy gains on compositional benchmarks.  There is no causal or probing analysis that clearly isolates whether improvements come from structure-aware learning or simply data augmentation with GBC scene graphs.\n2. The paper uses ViT-B and LoRA fine-tuning, but doesn’t examine whether the approach scales gracefully to larger models (e.g. ViT-L).\n3. Qualitative analysis would help verify whether the claimed improvements correspond to better interpretability rather than just numeric gains."}, "questions": {"value": "How sensitive is RACA-CLIP to the quality and noise of scene graph annotations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lq50BSn8wy", "forum": "0GjORP5Duq", "replyto": "0GjORP5Duq", "signatures": ["ICLR.cc/2026/Conference/Submission24104/Reviewer_NBkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24104/Reviewer_NBkA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465822517, "cdate": 1761465822517, "tmdate": 1762942939222, "mdate": 1762942939222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RACA-CLIP, a structured contrastive learning framework designed to improve compositional reasoning in vision-language models (VLMs), specifically focusing on relation-aware and attribute-grounded alignment. The method introduces region-level contrastive learning with IoU-weighted alignment between detected objects and caption spans, as well as a triplet supervision mechanism over structured ⟨subject, relation, object⟩ units, leveraging scene-graph annotations to provide fine-grained grounding signals. The approach preserves the dual-encoder architecture of CLIP while injecting relational inductive bias into the learned embeddings. Experiments across five compositional benchmarks show consistent improvements over CLIP and other enhanced baselines, including large gains in SugarCrepe’s Add and Swap settings (+16.24 and +24.86), while retaining — and occasionally improving — zero-shot recognition and retrieval performance. Ablation studies and controlled analyses suggest that the performance gains stem from improved binding between objects, attributes, and relations, rather than from memorization or dataset artifacts. Overall, the paper contributes an impactful improvement to a key weakness of modern contrastive VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies fundamental limitations of contrastive VLMs such as CLIP, citing the lack of structural inductive bias and over-reliance on global alignment that ignores object–attribute bindings and inter-object relations. This motivation is compelling and well-supported by prior analyses.\n\nThe proposed hierarchical alignment introduces region-level IoU-weighted contrastive learning and relation-aware triplet supervision (⟨s, r, o⟩), explicitly modeling the compositional structure of images and captions in a way that complements standard CLIP training.\n\nThe method leverages scene-graph annotated datasets to enable alignment supervision without requiring additional labeling efforts, improving practicality and efficiency."}, "weaknesses": {"value": "The approach requires accurate object, attribute, and relational supervision. The authors acknowledge that performance may degrade with lower graph fidelity, but no robustness evaluation is provided.\n\nThe method introduces computational overhead (region features + triplet losses), yet there is no cost or latency analysis of fine-tuning or inference, which affects real-world use cases.\n\nGraphs primarily capture physical/spatial relations; more abstract or high-level reasoning (e.g., intent, affordance, temporal actions) is not evaluated. Compositional generalization outside benchmarks like SugarCrepe and What’sUp remains unclear."}, "questions": {"value": "Do the improvements correlate with real-world compositional benchmarks outside synthetic evaluations (SugarCrepe-like)?\n\nCan the method be applied to generative multimodal models (e.g., aligning decoder-token grounding)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QkqygkmdWh", "forum": "0GjORP5Duq", "replyto": "0GjORP5Duq", "signatures": ["ICLR.cc/2026/Conference/Submission24104/Reviewer_ftKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24104/Reviewer_ftKs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876440275, "cdate": 1761876440275, "tmdate": 1762942938919, "mdate": 1762942938919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use scene-graph as a way to augment training in CLIP for enahnced compositional understanding capacity. The method uses existing scene-graph dataset to supervise region-aware contrastive learning, with improvement shown in the downstream benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and easy to follow. The proposed method is well explained, particularly around lines 264 and 298. On the selected benchmark, it demonstrates improved performance compared to the baseline."}, "weaknesses": {"value": "1. Limited novelty:\nThe idea of using scene graphs as external supervision has already been shown to improve performance [1,2,3]. Compared to these works, this paper introduces very limited novelty. The claim that “scene graph annotated datasets are leveraged for structured supervision without additional labeling” is misleading, as these datasets are still manually annotated—only pre-processed differently. Using such well-annotated data for contrastive supervision is not a particularly novel or interesting contribution.\n\n2. Incomplete evaluation:\nThe evaluation is insufficient. While the paper reports results on SugarCrepe, it does not evaluate on other established benchmarks such as Winoground or MMVP, which would better demonstrate generalization and fine-grained reasoning improvements.\n\n3. Insufficient related work review:\nPrior studies [1,2,3] have already explored using scene graphs to enhance fine-grained understanding through contrastive learning, yet this paper provides very limited discussion of them. Additionally, works like [1,4] also achieve strong results on SugarCrepe and should be included in the comparison table for completeness.\n\n\n\n[1] Huang, Yufeng, et al. \"Structure-clip: Towards scene graph knowledge to enhance multi-modal structured representations.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 3. 2024.\n[2] Herzig, Roei, et al. \"Incorporating structured representations into pretrained vision & language models using scene graphs.\" arXiv preprint arXiv:2305.06343 (2023).\n[3] Li, Liunian Harold, et al. \"Grounded language-image pre-training.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n[4] Zhang, Le, Rabiul Awal, and Aishwarya Agrawal. \"Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "How do authors justify the novelty of the method compare to [1,2,3]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lr6LD66aqI", "forum": "0GjORP5Duq", "replyto": "0GjORP5Duq", "signatures": ["ICLR.cc/2026/Conference/Submission24104/Reviewer_Upu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24104/Reviewer_Upu5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918502470, "cdate": 1761918502470, "tmdate": 1762942938648, "mdate": 1762942938648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RACA-CLIP, a structured contrastive framework that enhances CLIP’s compositional reasoning by integrating scene-graph-based supervision. This paper introduces region-level IoU-weighted alignment and relation-aware triplet losses to better capture object–attribute bindings and inter-object relations. Trained on the graph-based captioning dataset, RACA-CLIP achieves large improvements on compositional benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The model demonstrates robust performance on several compositional benchmarks, outperforming CLIP, NegCLIP, and TripletCLIP while maintaining strong zero-shot and retrieval accuracy.\n\nThis paper clearly explains the global-only limitation of CLIP and motivates structured alignment with strong intuition and references."}, "weaknesses": {"value": "The method heavily depends on scene-graph annotations and LLM-based triplet extraction, which may introduce noise or inconsistencies and limit scalability to unstructured web data.\n\nThe computational cost of extracting scene graphs and aligning multiple fine-grained objectives isn’t discussed. Could you please discuss it?\n\nIf the impact on broader downstream tasks, such as visual question answering or image generation, this paper will be more comprehensive."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YNlayCypFs", "forum": "0GjORP5Duq", "replyto": "0GjORP5Duq", "signatures": ["ICLR.cc/2026/Conference/Submission24104/Reviewer_vCJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24104/Reviewer_vCJz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986149661, "cdate": 1761986149661, "tmdate": 1762942938368, "mdate": 1762942938368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}