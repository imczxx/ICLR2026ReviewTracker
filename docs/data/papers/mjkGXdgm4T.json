{"id": "mjkGXdgm4T", "number": 280, "cdate": 1756733356938, "mdate": 1759898269377, "content": {"title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies", "abstract": "Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of \\emph{choice-only shortcuts} in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that  minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1–54.2\\%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants. We provide the source code and dataset viewer in the appendix, and will release the full source code, dataset, and annotation tool publicly upon acceptance.", "tldr": "PRISMM-Bench is the first benchmark of real reviewer-flagged multimodal inconsistencies in scientific papers, revealing that even state-of-the-art LMMs struggle to detect and resolve them.", "keywords": ["Large Multimodal Models", "Scentific document understanding", "evaluation benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e09dc7f5f8a79f50a03b6b386ed260c7fbc3b808.pdf", "supplementary_material": "/attachment/074b8a128dea2117569f8c9b2d024e0b5f521fff.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents PRISMM-Bench, a new benchmark designed to evaluate large multimodal models (LMMs) on their capacity to detect, remedy, and reason about inconsistencies between multimodal content (text, figures, tables, and equations) in scientific papers. The authors construct the benchmark by mining real reviewer-flagged inconsistencies from ICLR submissions, refining this through LLM-based filtering and human annotation, resulting in 262 validated inconsistency cases across 242 papers. The benchmark includes three multiple-choice tasks (inconsistency identification, remedy, and pair match) and introduces a structured JSON-based answer format to minimize shortcut exploitation in model evaluation. Results from evaluating 21 LMMs demonstrate that the tasks remain highly challenging, even for state-of-the-art proprietary models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Authentic Benchmark Construction: A key strength is the grounding of all inconsistency cases in real reviewer feedback from ICLR 2025 submissions, ensuring ecological validity. The paper moves away from synthetic or artificially injected errors, avoiding pitfalls of prior work.\n2. Thorough Pipeline and Documentation: The multi-stage pipeline involving LLM filtering, human verification, and metadata annotation is robust and clearly described, supporting reproducibility.\n3. Bias Mitigation: The use of structured JSON answer formats is a methodologically novel move to counteract answer-choice linguistic shortcuts.\n4. Comprehensive Empirical Analysis: The experimental results cover 21 models, consider various model sizes and architectures, different input granularities, and ablation on answer format and context."}, "weaknesses": {"value": "1. Limited Benchmark Scale and Generalizability: Despite claims of diversity, the dataset is relatively small (262 inconsistencies), and coverage is restricted to rejected or withdrawn ICLR 2025 submissions, all presumably from the AI/ML domain.\n2. Potential for Shortcut Leakage Remains: The proposed JSON-based format mitigates, but does not fully obviate, the risk of answer-pattern exploitation. As seen in Table 2/3, model accuracy without context remains above chance. More analysis on possible residual shortcuts and the robustness of the JSON representation to adversarial answer construction would strengthen the remedy."}, "questions": {"value": "1. Dataset Scale/Future Expansion: Are there specific plans or pilots for expanding PRISMM-Bench to domains beyond machine learning (e.g., other scientific fields) to address the current limitations in scope and domain diversity?\n2. Inconsistency Existence Judgment: Most scientific papers may have no detectable inconsistencies, yet models first need to judge inconsistency existence before analysis. Currently, PRISMM-Bench only uses samples from papers with reviewer-flagged inconsistencies, and all three tasks’ answer options assume \"inconsistencies definitely exist\". Has the team considered supplementing it with samples from inconsistency-free papers and adding a \"This section has no inconsistencies\" option to multiple-choice tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xDbIv9L2iU", "forum": "mjkGXdgm4T", "replyto": "mjkGXdgm4T", "signatures": ["ICLR.cc/2026/Conference/Submission280/Reviewer_gBzJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission280/Reviewer_gBzJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288556974, "cdate": 1761288556974, "tmdate": 1762915484156, "mdate": 1762915484156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISMM-Bench, a benchmark for evaluating LMMs on multimodal inconsistencies in scientific papers. Its primary contribution is sourcing 262 inconsistencies from *real ICLR 2025 peer reviews* rather than synthetic data. The benchmark includes three tasks (Identification, Remedy, Pair Match) and proposes a novel JSON-based answer format to mitigate linguistic bias in MCQ evaluation. Experiments demonstrate that even state-of-the-art LMMs (e.g., Gemini 2.5 Pro) struggle (max 54.2% accuracy), proving the benchmark's difficulty."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* **High Originality and Authenticity:** The paper addresses the critical problem of multimodal inconsistency using real-world data sourced directly from expert peer reviews. This is a significant improvement over benchmarks based on synthetic errors.\n* **Novel Debiasing Method:** The structured JSON answer format is an innovative and effective method to combat linguistic shortcuts in MCQ evaluations. The user study provides strong evidence for its necessity and utility."}, "weaknesses": {"value": "* **Limited Scale and Domain:** The benchmark's primary flaw is its small scale (262 instances) and narrow scope (only AI papers from ICLR 2025). This severely limits its statistical power and generality as a benchmark.\n* **Methodological Gaps:** The paper fails to report Inter-Annotator Agreement (IAA) for its human-verified dataset. This is a crucial omission that makes it difficult to assess the objectivity and quality of the annotations."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uguD7fpdg3", "forum": "mjkGXdgm4T", "replyto": "mjkGXdgm4T", "signatures": ["ICLR.cc/2026/Conference/Submission280/Reviewer_KPqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission280/Reviewer_KPqE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804087342, "cdate": 1761804087342, "tmdate": 1762915483962, "mdate": 1762915483962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PRISMM-Bench, a benchmark for evaluating large multimodal models (LMMs) on identifying and fixing inconsistencies in research papers. The inconsistencies are sourced from those explicitly mentioned by reviewers in OpenReview reviews of ICLR 2025 papers. The benchmark consists of 3 tasks in multiple-choice format for 1) identifying the inconsistency (Ident),  proposing solution to fix inconsistency (Remedy), and matching elements of inconsistency (Match). Observing that LMMs tends to leverage text priors as shortcuts that heavily biased the results, the authors propose a debiasing method that converts the natural language options into JSON format which has shown to be effective. Experiment results of 21 LMMs on the benchmark reveals that even the best performing models struggles in these tasks especially in more realisitic long-context setting, showcasing the challenges of applying these LMMs as scholar assistants for tasks like identifying and fixing inconsistency with fine-grained grounding in multimodal context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presented task is both novel and of potential practical value. Testing LMMs capability in identifying and fixing inconsistency in papers could be a good indicator of their capabilities in fine-grained multimodal grounding and understanding, while also foster the development of tools based on these models to help authors check their papers.\n2. The observation & analysis of choice-only shortcuts is insightful, and the proposed JSON-based debiasing method largely increase the utility of the benchmark and also provide a potentially useful debiasing approach for future MCQ-based benchmark construction."}, "weaknesses": {"value": "1. For both the identification & remedy tasks, I am not fully convinced that the MCQ setup is the optimal way of presenting this task. While the MCQ task does help probe LMMs capabilities, they are both indirect and less realistic - in real world scenarios we would expect the LMMs to identify and remedy these inconsistencies from scratch instead of selecting from a set of options. Plus all the shortcut biases the authors have observed for MCQ. So I would consider formulate them as generation tasks that directly ask LMMs to output what are the inconsistencies & solutions. It seems to me the evaluation should also be easy and less biased - I would expect LLM-as-a-judge can determine if the generated inconsistency & solution are the same with human annotated ground-truth at a pretty high accuracy.\n2. The subset results for the user study might not faithfully reflects the human-LMMs gap. In table 2, the JSON performance of InternVL3.5 38B R & Qwen 2.5 VL 72B on both focused & Document are significantly higher than their corresponding performance on the full set as shown in table 1."}, "questions": {"value": "1. In addition to the weaknesses mentioned above, can the authors explain why the yield rate in terms of  # of human validated inconsistencies / # LLM filtered inconsistencies is so low? Judging from the numbers in the paper, only ~5% of the LLM recognized potential inconsistencies are validated, which significantly constraint the overall size of the benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qq6rO6LUhp", "forum": "mjkGXdgm4T", "replyto": "mjkGXdgm4T", "signatures": ["ICLR.cc/2026/Conference/Submission280/Reviewer_CmSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission280/Reviewer_CmSk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869921586, "cdate": 1761869921586, "tmdate": 1762915483787, "mdate": 1762915483787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISMM-Bench, the first benchmark derived from real peer-reviewed inconsistencies in multimodal scientific papers. It systematically mines OpenReview comments to construct tasks that assess LMMs on detecting and correcting inconsistencies across text, figures, tables, and equations. The work fills a meaningful gap in multimodal reasoning evaluation in terms of inconsistency detection in scientific papers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of instructing LMMs to discover the inconsistencies is interesting. This is a very difficult task, even for reasoning models, and it evaluates various abilities, including reasoning, scientific graph understanding, and long multimodal context understanding.\n\n2. The idea of using reviews from the OpenReview website is clever, which reflects the real-world scenarios and leverages the implicit human-labeled data.\n\n3. The data collection pipeline is reasonable, and the authors have done extensive experiments to provide insightful conclusions."}, "weaknesses": {"value": "1. The questions are all in multiple-choice form. Although this is easy to evaluate, it is not representative of how LMMs will be used in real-world scenarios, which may weaken the results and conclusions of the paper. \n\n2. The scope of the benchmark is another concern. The benchmark only contains 262 inconsistencies, which may be too small to derive a solid conclusion. Besides, the paper is only from ICLR 2025, which mainly consists of AI fields. The LMMs' ability to discern inconsistencies in other fields has not been explored."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IzOnSfKVXz", "forum": "mjkGXdgm4T", "replyto": "mjkGXdgm4T", "signatures": ["ICLR.cc/2026/Conference/Submission280/Reviewer_stE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission280/Reviewer_stE4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974493354, "cdate": 1761974493354, "tmdate": 1762915483652, "mdate": 1762915483652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}