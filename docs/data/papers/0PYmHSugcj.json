{"id": "0PYmHSugcj", "number": 8583, "cdate": 1758091705698, "mdate": 1759897775069, "content": {"title": "MemOracle: Symbolic Reasoning with Associative Retrieval for Harmful Content Detection", "abstract": "Memes, as a prevalent form of online communication, often combine text and imagery to convey complex and sometimes harmful messages. Detecting hateful content in memes poses significant challenges due to their multimodal nature and the need for contextual reasoning. we propose a novel framework built upon vision-language models, empowered by multimodal retrieval and symbolic reasoning, to assess the harmfulness of memes. Specifically, Our system first parses the input meme using a vision-language model to extract image-text elements and semantic descriptions. These are embedded into a joint representation and stored in a vector database. For any given query meme, we retrieves similar examples from the database. A large language model is then employed to reason over the query meme based on the retrieved examples, guided by a predefined definition of hateful memes and a symbolic Chain-of-Thought prompt. The reasoning proceeds in three stages: translation, planning, and solver, producing both a decision and an explanatory rationale. Our approach enables a more transparent and context-aware assessment of online multimodal content. Comprehensive experiments on public FHM, HarM and MultiOff datasets outperforms state-of-the-art hateful meme detection frameworks on acc, balanced acc and mcc, demonstrate the effectiveness of MemOracle in interpreting and identifying harmful content. Comprehensive experiments on the public FHM, HarM, and MultiOff datasets demonstrate that MemOracle consistently surpasses state-of-the-art hateful meme detection models across accuracy, balanced accuracy, and MCC, highlighting its effectiveness in interpreting and identifying harmful content.", "tldr": "We introduce MemOracle, a retrieval-augmented symbolic reasoning system for multimodal hateful meme detection with improved interpretability and accuracy.", "keywords": ["Symbolic Reasoning", "Neuro-Symbolic AI", "Multimodal Representation Learning", "Harmful Content Detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00a473a056a90cf9913ccdb421b81f4848ca1783.pdf", "supplementary_material": "/attachment/edea9135e32aa3940ee17ca281068726f54cd5bb.pdf"}, "replies": [{"content": {"summary": {"value": "This paper present MEMORACLE to tackle meme classification through retrieval and reasoning. MEMORACLE is a framework combining the extraction of textual features through VLM, performing similarity-based dense vector retrieval through CLIP, and then performing symbol-based Chain-of-Thought prompting through translator, planner, and solver.  The CoT reasoning is based on the retrieved similar examples, acting as demonstrating associative examples. The purposed method is evaluated on various hateful meme datasets, demonstrating the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The symbolic CoT reasoning pipeline is novel for hateful meme detection and reasoning. \n- The system produce interpretable reasoning and detection results."}, "weaknesses": {"value": "- The authors claim that their method achieves state-of-the-art performance. However, this assertion is not supported by the reported results, which lag behind several existing approaches. For instance, PromptHate[1] reports accuracies of 72.98% on FHM and 84.47% on HarM-c, compared to MEMORACLE’s 66.10% and 70.34%. Similarly, RGCL[2] achieves 78.8% and 87.0% on the same datasets, respectively. This is almost 20% of accuracy difference. I would argue such binary classification system with sub 70% accuracy would be unusable. These methods also rely on much smaller backbones such as CLIP, rather than expensive reasoning-based inference systems with much larger LLM. Although those models are fine-tuned, the authors’ claim of state-of-the-art performance remains overstated. If the authors intended to refer to the zero/few-shot setting, that should be clearly specified. Even then, several recent methods perform comparably or better under such conditions. For example, LOREHM[3] achieves 70.2% and 74.6% on FHM and HarM-c, respectively, using a similar retrieval-plus-reasoning pipeline. More recent works, such as RA-HMD and U-CoT+, further improve upon these results (though these may be considered concurrent).\n\n- Furthermore, BridgeMod[4] demonstrates that even a simple BM25-based retrieval (for few-shot demonstrating examples retrieval, similar as the associative retrieval proposed) combined with naive LLM classification, without explicit symbolic reasoning, can achieve comparable results (66.0% with Mistral-7B and 65.4% with Qwen2-7B on FHM). Together with the stronger results of LOREHM, which also integrates CLIP-based retrieval and text-based chain-of-thought reasoning, this raises questions about the added value of MEMORACLE’s symbolic reasoning pipeline. \n\n- The authors state that the framework “produces judgments that are both logically consistent and faithful to the original multimodal content.” I would recommend evaluating this claim using an external LLM-as-a-judge approach to objectively assess logical consistency and faithfulness. Furthermore, since the paper emphasizes enhancing the interpretability of hateful meme detection, it would be valuable to include quantitative evaluations that support this claim (e.g., measuring explanation quality, agreement with human rationale, or other interpretability metrics).\n\n- The paper includes only one and a half pages of citations, which seems insufficient for the scope of the work. The authors should consider adding several relevant and recent references, particularly those referred above. Incorporating these works would improve the paper’s positioning and strengthen its connection to prior research. I would also suggest improving the related work sections.\n\n\n\n\n[1]Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Jing Jiang. 2022. Prompting for Multimodal Hateful Meme Classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 321–332, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n[2] Jingbiao Mei, Jinghong Chen, Weizhe Lin, Bill Byrne, and Marcus Tomalin. 2024. Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5333–5347, Bangkok, Thailand. Association for Computational Linguistics.\n\n[3] Jianzhao Huang, Hongzhan Lin, Liu Ziyan, Ziyang Luo, Guang Chen, and Jing Ma. 2024. Towards Low-Resource Harmful Meme Detection with LMM Agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2269–2293, Miami, Florida, USA. Association for Computational Linguistics.\n\n[4] Ming Shan Hee, Aditi Kumaresan, and Roy Ka-Wei Lee. 2024. Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 7785–7799, Miami, Florida, USA. Association for Computational Linguistics."}, "questions": {"value": "- Regarding inference cost, the system seems to combine multiple stages using LMM and LLM, have you measured the inference cost? It would be helpful to understand the system’s computational efficiency relative to existing baselines.\n\n\n- In Appendix D, the authors compare performance across different language models. I am curious whether the overall performance bottleneck could stem from the limited capacity of the LMMs used in the visual encoding stage. Since all visual information is condensed into textual descriptions, the richness of these representations might depend heavily on the vision model’s capability and the image-to-text prompting strategy. Have the authors explored using stronger visual encoders or refining the image-to-text prompts to improve this step? This seems plausible given that closed-source systems like GPT-4o achieve higher zero-shot accuracy on datasets such as FHM (over 70%) without explicit reasoning, suggesting that the visual understanding and grounding ability of the more capable model could play a key role."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNqjXZ3Kqd", "forum": "0PYmHSugcj", "replyto": "0PYmHSugcj", "signatures": ["ICLR.cc/2026/Conference/Submission8583/Reviewer_tgpK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8583/Reviewer_tgpK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760786729860, "cdate": 1760786729860, "tmdate": 1762920431682, "mdate": 1762920431682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MemOracle, a framework for hateful meme detection that integrates multimodal retrieval with a symbolic reasoning pipeline (Translator, Planner, Solver). The core idea of enhancing a VLM with structured, logical reasoning is novel and has merit."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addressing a critical and timely challenge in online content moderation by tackling the nuanced problem of hateful meme detection through multimodal reasoning. Introducing a novel framework that effectively integrates structured symbolic reasoning with associative retrieval, enhancing both detection performance and model interpretability."}, "weaknesses": {"value": "1. The manuscript does not adhere to standard academic formatting conventions, which significantly detracts from its professionalism and readability.\n- As noted, table captions must be placed above the table, not below. All figures and tables must have descriptive captions.\n- The stray hyphen (`-`) before the abstract is unacceptable.\nInconsistent and enlarged fonts in tables and figures suggest a lack of care in preparation.\n- The authors must meticulously proofread the entire document and strictly conform to the conference's style and formatting guidelines before any re-submission.\n\n2. The literature review fails to properly contextualize this work. The review largely ignores relevant work from the last 3-4 years. The field of explainable and reasoning-based meme detection is more mature than portrayed. The paper claims a key contribution in \"interpretability\" through symbolic reasoning. However, this claim is not substantiated against existing work that also provides explanations. The authors must thoroughly survey and differentiate their approach from these existing methods for providing explanations and logical reasoning in multimodal hate speech detection.\n\n3. The benchmarks primarily compare against smaller open-source LMMs (up to 13B parameters). This is insufficient. To establish a strong baseline, the authors should include comparisons with: Models like GPT-4V, GPT-4o, and Gemini 1.5/2.0 Pro, which are known to possess strong multimodal reasoning capabilities. Models such as LLaVA-1.5-34B, or other VLMs in the 30B+ parameter range."}, "questions": {"value": "1. As above\n2. Is there any justification for the selection of backbone models, such as Qwen2.5 and CLIP-ViT-B/32 (associative retrieval), in the methods used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b2ePxUTfwf", "forum": "0PYmHSugcj", "replyto": "0PYmHSugcj", "signatures": ["ICLR.cc/2026/Conference/Submission8583/Reviewer_rFu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8583/Reviewer_rFu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725620587, "cdate": 1761725620587, "tmdate": 1762920431320, "mdate": 1762920431320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a coherent, timely pipeline for multimodal hateful meme detection: it parses the meme, retrieves semantically related “premises,” and reasons over them using a three-stage LLM module. This is a conceptual step beyond treating memes as single-instance classification, and the authors show that both retrieval and the symbolic planner actually matter through ablations. However, unresolved weaknesses remain. First, results are all single-number reports with no statistical significance or variance, so the claimed gains over MinD/Evolver are not yet defensible. Second, the retrieval story is shown only within the same meme pools used for evaluation, so the claimed “associative retrieval” has not yet been demonstrated to generalize to out-of-pool or cross-domain memes. On top of that, the pipeline is heavy, multiple large models plus a vector DB, with no latency/cost section, which weakens the case for deployability in real moderation workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: Retrieval-augmented, modular architecture that unifies VLM perception with LLM-based symbolic reasoning for multimodal hateful meme detection, enabling more context-aware decisions.\n\nS2: Consistent, strong results across four public meme/harm datasets (FHM, MultiOff, HarM-c, HarM-p) against strong VLM and recent reasoning baselines, demonstrating breadth of applicability within this subset.\n\nS3: Ablations that remove individual reasoning stages and show clear performance drops, supporting the claim that symbolic planning is an essential part of the gains.\n\nS4: Qualitative examples that illustrate how retrieval + symbolic reasoning can correct VLM errors and provide human-interpretable rationales, which is valuable for moderation workflows."}, "weaknesses": {"value": "W1: Reported improvements are single-run numbers; despite having multiple datasets and tasks, the absence of statistical significance or variance analysis makes the gains less conclusive.\n\nW2: The pipeline depends on several large components (VLM, LLM, CLIP, vector DB); although the modularity is attractive (see S1), the paper does not provide an efficiency, cost, or latency profile, so deployability in real-time moderation remains unclear.\n\nW3: Retrieval is built from the same meme datasets used for evaluation; while the current retrieval is shown to matter, this leaves open whether the “associative retrieval” idea holds on out-of-pool or more diverse/multilingual memes.\n\nW4: Symbolic reasoning is defined through LLM prompting, but there is no robustness study to prompt changes or noisy parses, so reliability in realistic, noisy inputs is not established.\n\nW5: The approach explicitly assumes access to historical or related content and even notes possible bias and the need for a buffer period, but there is no reported performance for “no/low-neighbor” settings.\n\nW6: Even though the authors provide ablations over k and over reasoning stages, the paper does not fully disentangle how much of the gain comes from retrieval quality vs. symbolic template/planner quality, limiting attribution.\n\nW7: The method relies on accurate in-meme text capture, but there is no measurement of degradation under poor OCR, stylized fonts, or low resolution."}, "questions": {"value": "Q1: Would the authors consider testing and benchmarking against newer and bigger VL models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rpC2G1zEhb", "forum": "0PYmHSugcj", "replyto": "0PYmHSugcj", "signatures": ["ICLR.cc/2026/Conference/Submission8583/Reviewer_KTB5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8583/Reviewer_KTB5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813226619, "cdate": 1761813226619, "tmdate": 1762920430839, "mdate": 1762920430839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MEMORACLE, a retrieval-augmented multimodal framework that combines a vision–language parser, associative retrieval (Milvus + CLIP embeddings), and a three-stage symbolic reasoning pipeline to detect hateful/harmful memes. The system translates multimodal content into first-order logic (FOL) style predicates, retrieves K nearest “associative” memes from a vector memory, and uses an LLM/VLM combination to produce step-by-step rationales and a final binary/multi-label decision. Experiments report improvements over recent reasoning baselines (MinD, Evolver) and several open LMM baselines on FHM, MultiOff and HarM variants. Key implementation details and ablations (retrieval size k, component removal, modality weighting) are provided in the Appendix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tAddresses an important and practical problem (hateful/harmful meme detection) where multimodal, contextual reasoning is genuinely needed. \n2.\tThe pipeline is comprehensive and evaluated on several standard datasets (FHM, MultiOff, HarM variants), with multiple metrics (ACC, BACC, MCC, Macro-F1) reported. \n3.\tAblation experiments (retrieval size k, removal of Translator/Planner/Solver stages, modality weighting) are included and help identify which modules contribute most to performance."}, "weaknesses": {"value": "1. The Translator claims to output FOL expressions, but the paper lacks a formal grammar, typing rules, or a description of how ambiguous natural language/image inputs map to logical atoms, predicates, and quantifiers. This gap prevents independent verification and may hide brittle heuristics used in practice. Provide a formal specification (or pseudo-code) of the translation algorithm and discuss failure cases.  \n\n2. The Planner and Solver are described at a high level (step sequences, Modus Ponens usage), but the system’s treatment of contradictions, uncertain evidence, or conflicting retrieved premises is unspecified. How are contradictory retrieved memes reconciled? Is there weighting or belief tracking? These are crucial for symbolic reasoning claims. \n\n3. Report standard deviations across multiple runs, provide random seeds, and include statistical tests to support claims like “outperforms by up to 9%”. Without confidence intervals or p-values the numerical claims are weak.\n\n4. Ablations show the Planner is important, but do not isolate whether gains come from (a) better parse quality, (b) retrieval content, or (c) LLM prompt phrasing. I recommend: (i) ablate quality of Translator outputs (oracle symbolic inputs vs. automatic), (ii) evaluate retrieval relevance quantitatively (precision@k) and (iii) ablate prompt templates and LLM temperature/decoding. \n\n5. The paper claims interpretable step-by-step rationales. Provide a human evaluation (annotator agreement / faithfulness / helpfulness) to substantiate interpretability claims—automatic metrics are insufficient. \n\n6. Table 1/2 placement, spacing inconsistencies (`first-order logic(FOL)` vs `first-order logic (FOL)`), and figure alignment problems reduce readability. Fix figure subpanel alignment, ensure consistent typography, and place tables adjacent to first references."}, "questions": {"value": "1. Please provide the formal specification (grammar or schema) used by the Translator to produce FOL atoms and the mapping rules for common image/text constructs (e.g., panels, dialogues, sarcasm markers). If this uses learned prompts, provide the exact prompt templates and examples (oracle vs automatic). \n\n2. How does the Planner/Solver handle contradictory retrieved premises? Is there a ranking/weighting or conflict-resolution mechanism? Please describe the concrete merge (⊕) operator and provide pseudocode for the Planner/Solver execution. \n\n3. Provide variance estimates (std. dev) for the main reported metrics and indicate number of random seeds / runs used. Were the retrieval indices or LLMs deterministic across runs? \n\n4. Can you show examples where MEMORACLE produces an incorrect rationale but still the correct final label (and vice versa)? Such failure analyses would clarify whether the rationales are faithful or post-hoc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xVrFePDjX5", "forum": "0PYmHSugcj", "replyto": "0PYmHSugcj", "signatures": ["ICLR.cc/2026/Conference/Submission8583/Reviewer_ceAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8583/Reviewer_ceAb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977692441, "cdate": 1761977692441, "tmdate": 1762920430480, "mdate": 1762920430480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}