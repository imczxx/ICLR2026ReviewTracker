{"id": "Lkndkxeemx", "number": 5653, "cdate": 1757925604485, "mdate": 1759897962617, "content": {"title": "A Near-Optimal Best-of-Both-Worlds Algorithm for Federated Bandits", "abstract": "This paper studies federated multi-armed bandit (MAB) problems where multiple agents working together to solve a common MAB problem through a communication network. We focus on the heterogeneous setting in which no single agent can identify the global best arm using only local biased observations. In this setting, different agents may select the same arm at the same time step but receive varying rewards. We propose a novel algorithm called \\textsc{FedFTRL} for this problem, which is the first work to achieve near-optimal regret guarantees in both stochastic and adversarial environments. Notably, in the adversarial regime, our algorithm achieves $O(\\sqrt{T})$ regret which is a significant improvement over the state-of-the-art regret of $O(T^{\\frac{2}{3}})$ \\citep{yi2023doubly}. We also provide numerical evaluations comparing our algorithm with baseline methods, demonstrating the effectiveness of our approach on both synthetic and real-world datasets.", "tldr": "", "keywords": ["Federated Bandits", "Mutli-armed Bandits", "Best-of-both-worlds"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7955cb419d3acf737a2d673587e679732113fbe.pdf", "supplementary_material": "/attachment/e7d7bed5aad320e5950fdbd3dcf92e4dba99bc0e.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies distributed multi-armed bandits with heterogeneous losses under both stochastic and adversarial regimes. It introduces FEDFTRL, a novel algorithm that is the first to achieve near-optimal regret in both settings. Comprehensive experiments are provided to validate the theoretical guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Contributions\n1. First establish an $O(\\sqrt{T})$ regret bound in the adversarial regime.\n2. Prove a best-of-both-worlds guarantee\n3. Paper is well written."}, "weaknesses": {"value": "1. Requires $O(K + VD)$ bits of communication per round; the protocol differs substantially from prior work (previous only required $O(K)$ ), so direct comparisons are not straightforward.\n2. With an additional $O(VD)$ budget per round, the problem essentially reduces to a standard multi-armed bandit with delayed feedback."}, "questions": {"value": "Please explain weaknesses, and I may significantly revise my review comments accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oP2ngNrNua", "forum": "Lkndkxeemx", "replyto": "Lkndkxeemx", "signatures": ["ICLR.cc/2026/Conference/Submission5653/Reviewer_ouUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5653/Reviewer_ouUu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824422548, "cdate": 1761824422548, "tmdate": 1762918179009, "mdate": 1762918179009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the federated multi armed bandit problem and focuses on the heterogenous setting where agents cannot determine the globally optimal arm using their local biased observations. The major contribution is the proposed Best-of-Both-Worlds algorithm that performs robustly in both stochastic and adversarial environments. The proposed algorithm adapts the Follow-the-Regularized-Leader framework and incorporate a hybrid regularizer typically used for bandits with delayed feedback. The authors view this as analogous to the latency caused by decentralized communication. Novelty includes the use of a communication scheme that tracks ddeviation records and a truncated loss estiator to keep agent action probabilities nearly aligned despite the heterogeneous feedback. They demonstrate that the FEDFTRL algorithm achieves near optimal regret bounds."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core contribution of the paper is the first Best-of-Both-Worlds regret guarantee for the heterogeneou federated bandit setting.\n\n2. Achieving $O(\\sqrt{T})$ individual regret in adversarial setting is a clear improvement over previous results.\n\n3. The theoretical analysis seems deep and the derived regret bounds match the known lower bounds.\n\n4. Experiments are comprehensive."}, "weaknesses": {"value": "1. The major limitation is the communication complexity which is explicitly mentioned by the authors as well. Each agent requires communicating $O(K+VD)$ bits of information every round. This is a potential practical bottleneck for large scale federated systems with many agents or high network diameter.\n\n2. The algorithm relies on special hybrid regularizers and fine tuned time varying learning rates that are defined based on network characteristics. Even though they are necessary for theoretical guarantees, this complexity may hinder deployment and tuning in practice.\n\n3. While the adaptation is novel for federated setting, the theoretical framework relies heavily on importing and combining existing BOBW literature."}, "questions": {"value": "1. Given that the communication cost is $O(K + V D)$ bits per round for each agent, can youprovide a more detailed discussion on the practical implications of this dependency on the number of agents $V$ and the network diameter $D$ for typical federated systems? Quantifying how $V$ and $D$ affect runtime in the experiments would be beneficial.\n\n2. The parameter $C_t^P$ in Eq. (2) quantifies the delay caused by decentralized communication and $C_T^P$ captures the dependence on network topology. Can the authors further clarify the intuitive meaning of how these complexity measures dictate the regret?\n\n3. The truncated loss estimator $\\tilde{\\ell}_{v,t}$ is crucial for stabilizing action probabilities. Can the authors comment on the practical robustness of the truncation threshold $(12V C_t^P \\gamma_t)$ of the denominator. Especially, regarding its potential sensitivity to misspecified initial parameters or dynamic changes in the network topology captured by $C_t^P$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xC1Lsg0Wb3", "forum": "Lkndkxeemx", "replyto": "Lkndkxeemx", "signatures": ["ICLR.cc/2026/Conference/Submission5653/Reviewer_yECc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5653/Reviewer_yECc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941761568, "cdate": 1761941761568, "tmdate": 1762918178628, "mdate": 1762918178628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper study the Best-of-Both-Worlds (BoBW) problem in decentralized federated bandit setting, where it follows the FTRL algorithm’s idea but handle delay and client heterogeneity in federated settings via a modified learning-rate schedule and truncated loss estimators. Theoretical results are provided for the proposed algorithm and the regret almost matches the lower bound in this setting. Numerical experiments validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Theoretical guarantee for the proposed method is strong which almost matches the lower bound in this setting."}, "weaknesses": {"value": "We need to know the topology and $D$ beforehand."}, "questions": {"value": "**Learning rates:** How are learning rates in (201) compared with that in the delayed feedback (Masoudian et al., 2022) ?\n\n**Communication cost:** How is $x_{v,t}(k)$ compared to  $12VC_t^{P}\\gamma_t$ in (4)? In practice, how many rounds do we need to truncate the loss and broadcast? \n\n**equation (5):** It seems V multiplies to the loss after communication as well? In this case given $P$ is doubly stochastic, (when the loss estimates of the neighbors are closed), loss estimator seems to have exponential growth as time $t$ increases. Could the author elaborate this more?\n\n**Clarification on matching bounds:** If the graph $G$ is known, could we always construct a doubly stochastic matrix $P$ as in Remark 1 to achieve the nearly matching bounds? \n\nTypos: \nline 155: (u,v)\\not\\in E"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Nq2DEw6wf", "forum": "Lkndkxeemx", "replyto": "Lkndkxeemx", "signatures": ["ICLR.cc/2026/Conference/Submission5653/Reviewer_cGPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5653/Reviewer_cGPZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969928492, "cdate": 1761969928492, "tmdate": 1762918178307, "mdate": 1762918178307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a multi-agent multi-armed bandit problem, where $V$ agents each face an identical copy of a set of $K$ arms. At each round, agents can share information with their neighbors according to a communication graph. Due to privacy constraints, agents are only allowed to share statistics of the losses rather than the raw losses themselves. The authors design a best-of-both-worlds (BOBW) algorithm that achieves near-optimal regrets in both stochastic and adversarial regimes. The paper also includes numerical evaluations of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper establishes new state-of-the-art BOBW regrets for the multi-agent privacy-preserving bandit setting.\n- The work introduces a truncated loss estimator, which ensures that individual regrets across agents remain similar (Lemma 1). This is a notable contribution, as prior multi-agent bandit works typically require agents to be fully homogeneous to derive individual regret guarantees."}, "weaknesses": {"value": "- Main concern: The definition of the feedback  $\\ell_{v, t}(k_{v, t})$ is unclear. The paper highlights that it is biased, but does not clearly describe the nature or extent of the bias. Moreover, in Section 6.1, the feedback does not appear to show any bias. Could the authors clarify this point?\n- It is unclear whether the goal of each agent is to minimize its own individual regret or a global regret across all agents. While the abstract emphasizes that no single agent can identify the globally best arm, this distinction is not explicitly modeled in the problem formulation. Additionally, if each agent is minimizing individual regret, identifying the global best arm may not be necessary. Clarification here would be helpful.\n- The use of the term \"federated\" may be misleading, as federated learning typically involves a central server, whereas the setup in this work appears to be fully distributed."}, "questions": {"value": "- Is the proof of Lemma 2 in the appendix actually meant to support Lemma 1? Please add clear references in the main text to help readers locate the corresponding proofs.\n- Why was IND-FTRL not included in the evaluation shown in Figure 2?\n- This work achieves a significant improvement in the regret bound for the adversarial regime. Could the authors elaborate on which algorithmic components, analysis techniques, or assumptions are responsible for this improvement compared to (Yi & Vojnović, 2023)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eeh1peCnoi", "forum": "Lkndkxeemx", "replyto": "Lkndkxeemx", "signatures": ["ICLR.cc/2026/Conference/Submission5653/Reviewer_39L1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5653/Reviewer_39L1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193532310, "cdate": 1762193532310, "tmdate": 1762918177910, "mdate": 1762918177910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}