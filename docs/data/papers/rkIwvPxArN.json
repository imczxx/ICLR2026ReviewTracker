{"id": "rkIwvPxArN", "number": 6251, "cdate": 1757962181565, "mdate": 1759897926861, "content": {"title": "Verification Limits Code LLM Training", "abstract": "Large language models for code generation increasingly rely on synthetic data, where both problem solutions and verification tests are generated by models. While this enables scalable data creation, it introduces a previously unexplored bottleneck: the verification ceiling, in which the quality and diversity of training data are fundamentally constrained by the capabilities of synthetic verifiers. In this work, we systematically study how verification design and strategies influence model performance. \nWe investigate (i) \\textbf{what} we verify by analyzing the impact of test complexity and quantity: richer test suites improve code generation capabilities (on average +3 pass@1), while quantity alone yields diminishing returns, (ii) \\textbf{how} we verify by exploring relaxed pass thresholds: rigid 100\\% pass criteria can be overly restrictive. By allowing for relaxed thresholds or incorporating LLM-based soft verification, we can recover valuable training data, leading to a 2–4 point improvement in pass@1 performance. However, this benefit is contingent upon the strength and diversity of the test cases used, and (iii) \\textbf{why} verification remains necessary through controlled comparisons of formally correct versus incorrect solutions and human evaluation: retaining diverse correct solutions per problem yields consistent generalization gains. \nOur results show that Verification as currently practiced is too rigid, filtering out valuable diversity. But it cannot be discarded, only recalibrated. By combining calibrated verification with diverse, challenging problem–solution pairs, we outline a path to break the verification ceiling and unlock stronger code generation models.", "tldr": "We show that verification is both the bottleneck and the key in synthetic code training, and propose ways to break the verification ceiling for better generalization.", "keywords": ["LLM", "code", "verification", "data optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec8d72748445a237c44ae3a359a623bf02e0a62a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the verification limits in training code LLMs with automatically generated unit tests. The authors argue that applying stricter verification, such as requiring more unit-test passes, may unintentionally remove difficult or creative code samples, leading to a bias toward simpler problems. They empirically analyze how test complexity, test quantity, and threshold settings affect model performance across several code benchmarks. Beyond unit tests, the paper explores direct LLM-based verification, where LLMs act as judges to assess the correctness of generated code. Experiments show that this soft verification approach recovers complementary high-quality data and achieves performance comparable to structured test-based filtering."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an intuitively important but under-explored question in code-LLM training, namely how the design of verification procedures (number of tests, threshold τ, or use of LLM judges) affects data quality and downstream model performance."}, "weaknesses": {"value": "1. The overall experimental design appears poorly motivated and insufficiently validated. The authors start by setting the verification threshold τ = 1, which produces questionable conclusions (for example, the reported drop in performance after more than two test cases can be meaningless). An overly strict threshold would incorrectly filter out many valid solutions due to imperfect test cases, while a looser threshold can actually mitigate this issue. Such an extreme setting should have first been examined on a verified dataset to calibrate the effect of τ before using it in data filtering and model training.\n\n2. The experimental treatment of τ is inconsistent and lacks justification. The authors later switch to τ = 0.6 without systematic reasoning or reference to prior literature on verification thresholds. Since model capabilities and problem difficulties vary widely, a global τ averaged across all samples is inherently ill-defined.\n\n3. The analysis overlooks alternative explanations for the observed trends. For instance, improvements after changing τ may simply result from selecting a threshold that happens to retain a larger proportion of correct solutions, rather than from the claimed increase in diversity.\n\n4. The analysis of unit tests is also rather trivial, as it only considers the number of tests. Some test cases may belong to the same equivalence class and therefore contribute little additional coverage. A more meaningful analysis should consider functional or branch coverage instead of simple test counts.\n\n5. The reported performance gains are small and may not be statistically significant. For example, in Figure 2, the base model improves from 28 to around 34–36 using 70K samples, which is a limited gain. The differences between test generation strategies are also very small and could easily fall within normal variance depending on checkpoint selection or random seeds.\n\n6. Several figures (for example, Figure 7) are blurry. The authors are recommended to regenerate all figures using vector graphics formats for professional presentation. In addition, in Figure 3 (left), two points are both labeled as 0.29 yet appear at different heights, suggesting inconsistent plotting precision or rounding issues that should be clarified and reported with higher numerical accuracy."}, "questions": {"value": "I suggest conducting an intermediate analysis of the verification threshold τ on datasets with known ground-truth solutions, rather than applying the filtering process directly to training data. Understanding how τ influences the correctness distribution and the proportion of partially correct samples before model training would make the end-to-end results more interpretable and convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "c6yR1XWcKe", "forum": "rkIwvPxArN", "replyto": "rkIwvPxArN", "signatures": ["ICLR.cc/2026/Conference/Submission6251/Reviewer_uxox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6251/Reviewer_uxox"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554701812, "cdate": 1761554701812, "tmdate": 1762918572408, "mdate": 1762918572408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the verification ceiling problem for synthetic code generation defined as code verification systems being the limiting factor of data quality for LLMs. Through SFT experiments they show that 1) structured and contrastive prompting lead to better training signals, 2) more tests isn’t necessarily a better for downstream training in that the higher bar leads towards a bias of selecting easier problems, 3) performance is increased when you allow hard problems that don’t pass all tests vs disallowing them, and 4) a human study reveals that many synthetic test suites are incomplete. In short, in verification we should calibrate rather than discard problems that don’t pass all tests."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* There are some nontrivial findings in this paper namely that training on hard problems with incomplete passing tests is better than discarding those problems since they “failed” and that prompting LLMs to use structured prompts (ask to target edge cases and under-tested logic) and contrastive prompting (ask to generate new tests such that one of the example solutions provided fails) leads to better performance as well. \n* The authors conduct a human study giving insight into the benefits of junior vs senior coding experts."}, "weaknesses": {"value": "* 2025 has been the year of RLVR yet the related works section fails to acknowledge the rich literature of that area yet alone all the other kinds of work that has gone into code verification, RL, and LLM training. \n* Overall the paper reads as a project report rather than a controlled scientific study. There are several things missing formal definitions including complexity, correctness and accuracy. In the human study, there is no rubric defined to guide the participants in “correctness and completeness”. Are we talking about semantic correctness, passing tests correctness, lines of code completeness, or tests touching all parameters completeness, etc. \n* There is a lack of qualitative analysis in the results. At best, there is just a superficial discussion around the results of the charts but no forward thinking or in depth reasoning. For example, the authors have the insight, “human evaluation reveals that many discarded solutions are in fact valid,” yet fail to provide a taxonomy on the kinds of failures in the test suites that lead these false rejections."}, "questions": {"value": "None at this time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N1h0g88Gjl", "forum": "rkIwvPxArN", "replyto": "rkIwvPxArN", "signatures": ["ICLR.cc/2026/Conference/Submission6251/Reviewer_dQMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6251/Reviewer_dQMt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860399835, "cdate": 1761860399835, "tmdate": 1762918571819, "mdate": 1762918571819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to reason about the effect of verification in synthetic code training dataset generation. The authors argue that stricter verification leads to filtering out harder/diverse problems that can be valuable to include in the training dataset. This is demonstrated through a series of experiments where the authors changed the test generation pipeline and strictness of the verification and showed the results on common code benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Studying an interesting and timely problem that has impact across multiple different training paradigm\n- The paper is easy to understand/read and presents the idea well"}, "weaknesses": {"value": "- One major issue I had with the paper is the phrasing and position that stricter test generation \n\t- the author claims that \"While stricter verification does reduce incorrect solutions, it also disproportionately filters out hard coding problems\" \n\t- this is definitely true based on the results, however I think the authors failed to acknowledge why.\n\t- the reason is that generating correct tests are extremely difficult, as such, generating more tests (with strict test passing rule) will ultimately lead to filtering out many correct solutions with incorrect tests\n\t- the authors in the paper seems to mainly suggest stricter test passing rule is not optimal as it removes more difficult problems (or decrease the variety of problems)\n\t- it is more like the optimal test passing rule is a function of how good the test generation teacher llm is (for example if we have a perfect test oracle for any arbitrary problems, this strict passing rule would not be a problem)\n\t- and if we had a horrible test generation teacher llm, we would not be using verification at all\n\t- i think the authors failed to acknowledge this important understanding.\n- Lack of diversity in studied LLMs:\n\t- the authors only evaluate using one base model of Command 7B and DeepSeek-v3 as the teacher model\n\t- i think it would be interesting and improve the work if more models were studied (especially teacher) to showcase similar findings\n- Training dataset composition:\n\t- given the work is on studying the affect of verification on synthetic data, the authors included quite a bit of non-synthetic data (in section 3)\n\t- why that particular data mixture is not well explained\n\nOverall, I think this paper asks some interesting questions with some detailed analysis, I am inclined to increase my score if the authors can address the concerns I had and perhaps rephrase some of the findings in the paper.\n\nminor issues:\n- In abstract and other places in the paper: \"on average +3 pass@1\" +3 itself is extremely unclear what it means (is it precent points, is it relative increase?)"}, "questions": {"value": "1. why did the author include non-synthetic training samples in the training dataset?\n2. please comment on the main concern on why strict verification can be both good and bad (see weakness for more detail)\n3. why did the author only report aggregated results on the multiple evaluation benchmarks? Are there differences between individual benchmark performance compared with the aggregate in certain cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGxdBBwjli", "forum": "rkIwvPxArN", "replyto": "rkIwvPxArN", "signatures": ["ICLR.cc/2026/Conference/Submission6251/Reviewer_JiP4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6251/Reviewer_JiP4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970218906, "cdate": 1761970218906, "tmdate": 1762918571398, "mdate": 1762918571398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to reason about the effect of verification in synthetic code training dataset generation. The authors argue that stricter verification leads to filtering out harder/diverse problems that can be valuable to include in the training dataset. This is demonstrated through a series of experiments where the authors changed the test generation pipeline and strictness of the verification and showed the results on common code benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Studying an interesting and timely problem that has impact across multiple different training paradigm\n- The paper is easy to understand/read and presents the idea well"}, "weaknesses": {"value": "- One major issue I had with the paper is the phrasing and position that stricter test generation \n\t- the author claims that \"While stricter verification does reduce incorrect solutions, it also disproportionately filters out hard coding problems\" \n\t- this is definitely true based on the results, however I think the authors failed to acknowledge why.\n\t- the reason is that generating correct tests are extremely difficult, as such, generating more tests (with strict test passing rule) will ultimately lead to filtering out many correct solutions with incorrect tests\n\t- the authors in the paper seems to mainly suggest stricter test passing rule is not optimal as it removes more difficult problems (or decrease the variety of problems)\n\t- it is more like the optimal test passing rule is a function of how good the test generation teacher llm is (for example if we have a perfect test oracle for any arbitrary problems, this strict passing rule would not be a problem)\n\t- and if we had a horrible test generation teacher llm, we would not be using verification at all\n\t- i think the authors failed to acknowledge this important understanding.\n- Lack of diversity in studied LLMs:\n\t- the authors only evaluate using one base model of Command 7B and DeepSeek-v3 as the teacher model\n\t- i think it would be interesting and improve the work if more models were studied (especially teacher) to showcase similar findings\n- Training dataset composition:\n\t- given the work is on studying the affect of verification on synthetic data, the authors included quite a bit of non-synthetic data (in section 3)\n\t- why that particular data mixture is not well explained\n\nOverall, I think this paper asks some interesting questions with some detailed analysis, I am inclined to increase my score if the authors can address the concerns I had and perhaps rephrase some of the findings in the paper.\n\nminor issues:\n- In abstract and other places in the paper: \"on average +3 pass@1\" +3 itself is extremely unclear what it means (is it precent points, is it relative increase?)"}, "questions": {"value": "1. why did the author include non-synthetic training samples in the training dataset?\n2. please comment on the main concern on why strict verification can be both good and bad (see weakness for more detail)\n3. why did the author only report aggregated results on the multiple evaluation benchmarks? Are there differences between individual benchmark performance compared with the aggregate in certain cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGxdBBwjli", "forum": "rkIwvPxArN", "replyto": "rkIwvPxArN", "signatures": ["ICLR.cc/2026/Conference/Submission6251/Reviewer_JiP4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6251/Reviewer_JiP4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970218906, "cdate": 1761970218906, "tmdate": 1763475042031, "mdate": 1763475042031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and systematically investigates the verification ceiling problem: a previously underexplored bottleneck in synthetic code generation pipelines where training data quality is fundamentally constrained by verifier capabilities. Through controlled experiments across multiple benchmarks and languages, the authors demonstrate that current verification practices are too rigid, filtering out valuable diversity while admitting noisy solutions. By recalibrating verification strategies—improving test complexity, relaxing strict thresholds, and incorporating LLM-based soft verification—the paper achieves 2–4 point pass@1 improvements while maintaining accuracy-efficiency trade-offs. The core insight: verification is necessary but must be adapted to the problem domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper investigates an important problem in code generation: how to provide an effective and efficient verification for training. It answers the three research questions: what to verify, how to verify, and why to verify. These findings provide good guidance for training LLMs for code\n2. It provides comprehensive studies for each aspect. For example, it investigates the relationship between the problem difficulty and the unit test quantity, and further explores the relationship between the training performance and the problem difficulty.\n3. It also includes the human evaluation of the synthetic unit tests to demonstrate the limitations of current synthesized code solutions."}, "weaknesses": {"value": "1. While this paper investigates the verification in training, it mainly focuses on supervised finetuning: the verification only provides 0/1 signal to indicate whether a candidate should be filtered out when creating training data. This leads to one problem: with weak verification, a low-quality solution is indistinguishable from a high-quality one, because both of them can pass the verification and be used for training. This will actually introduce noise into the training, because the model will be optimized towards both directions. \n2. It is unclear how many test cases are used per sample for verification. In Section 4.2, it evaluates {1,2,3,4} test cases per sample. However, such a small number of unit tests may be insufficient to verify the functionality correctness.\n3. The experiments in Section 4.2 show that with a stricter threshold, fewer difficult problems will be kept. This indicates that actually none of the candidate solutions can correctly solve the problem. With a weaker threshold, these solutions can be kept, but they are not correct. I think in such a scenario, the better way is to enlarge the candidate sets to find a correct solution, instead of lowering the threshold to keep the incorrect ones, which may provide the incorrect training signals. \n4. All experiments are conducted with a single base model Command 7B base model. It is unclear whether the conclusion is universal and also holds for other models. \n5. Figures and Tables:\n\t1. More explanation should be provided for Figure 1. What does the x-axis refer to? What are the meanings of the three test suite types? Which model is evaluated here?\n\t2. It is better to use the histogram instead of the plot in Figure 2, since there are no dependencies between x values (different methods). \n\t3. The figures do not meet the ICLR format requirements. \n\t4. In Figure 3 (right), why are there no problems with medium (score=3) level difficulty?\n\t5. In Figures 2, 3, 4, which test set is used?\n\t6. What is the default number of test cases in the experiments? For tau=0.6, does it mean that each sample should have at least 5 test cases and pass 3 of them to be kept as the training data?"}, "questions": {"value": "1. With weak verification, some low-quality (even incorrect) solutions may be introduced to the training. Do you think such noise will have a negative impact on the training?\n2. In all training, the same number of problems (70K) is used. How do you sample these problems when using different thresholds? Does it mean that the diversity and difficulty of the coding problems will also be affected due to the sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g3cHxhsQei", "forum": "rkIwvPxArN", "replyto": "rkIwvPxArN", "signatures": ["ICLR.cc/2026/Conference/Submission6251/Reviewer_9ASf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6251/Reviewer_9ASf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6251/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240754164, "cdate": 1762240754164, "tmdate": 1762918570807, "mdate": 1762918570807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}