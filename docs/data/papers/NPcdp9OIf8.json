{"id": "NPcdp9OIf8", "number": 7173, "cdate": 1758010346949, "mdate": 1763739701579, "content": {"title": "InstructX: Towards Unified Visual Editing with MLLM Guidance", "abstract": "With recent advances in Multimodal Large Language Models (MLLM) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in‑depth analysis of MLLM design choice. Moreover, the integration of MLLM and diffusion models remains an open challenge in some difficult tasks, \\textit{e.g.}, video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLM and diffusion model for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. \\textit{(1)} We show that training on image data can emerge video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. \\textit{(2)} By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieve state-of-the-art performance.", "tldr": "", "keywords": ["Multimodal Generation", "Image Editing", "Video Editing", "Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a026e5c6c964a0d59a906eec9f38eb4a4a5efbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose InstructX which integrates MLLMs with diffusion models to perform instruction-driven editing tasks across both images and videos within a single model and it explores optimal design choices for combining MLLMs and diffusion models, emphasizing that MLLMs should actively participate in the editing process rather than being treated as mere feature extractors.\n\nThe framework leverages large-scale image editing data to train video editing capabilities, addressing the scarcity of high-quality video editing datasets.\n\n The paper introduces VIE-Bench, a high-quality benchmark for instruction-based video editing, comprising diverse tasks such as object addition, removal, style change, and reference-based edits.\n\nExtensive experiments demonstrate that InstructX outperforms open-source methods and competes with closed-source solutions in both image and video editing tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It critically evaluates different architectural approaches for combining MLLMs and diffusion models (fig3), offering evidence-based recommendations for optimal integration. ​\n\nIt proposes a novel approach to address the lack of high-quality video editing datasets by leveraging image data to train video editing models. ​\n\nThe introduction of VIE-Bench provides a new standard for evaluating instruction-based video editing methods, which can be used for future research comparisons. ​"}, "weaknesses": {"value": "The main weakness of this paper is that its overall takeaway message feels rather weak. While the architectural design presented is a valid and potentially useful contribution from which readers can learn, it is unclear what additional insights or lessons the audience can gain beyond this. The paper reads more like a technical report than a research study. I encourage the authors to provide deeper insights and discussions that can inspire readers, rather than merely presenting results"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fGP8SIbWUd", "forum": "NPcdp9OIf8", "replyto": "NPcdp9OIf8", "signatures": ["ICLR.cc/2026/Conference/Submission7173/Reviewer_6Kao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7173/Reviewer_6Kao"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851756748, "cdate": 1761851756748, "tmdate": 1762919334292, "mdate": 1762919334292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InstructX, a unified framework for instruction-guided image and video editing, which leveraging multimodal large language models (MLLMs) to improve diffusion-based editing quality. The core idea is to use a MLLM to extract related text prompt feature to replace the original text input of the diffusion model. The MLLM processes the instruction and source visual content, producing learning meta-queries that guide a DiT to perform the edit. During training, three training stages are performed, including feature alignment, joint training, and quality fine-tuning. The author claimed that training on image data can emerge video editing capabilities without explicit supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tA unified design that seamlessly handles both image and video editing in a single model.\n\n2.\tIntroduces VIE-bench, a high-quality, instruction-based video editing benchmark with 140 examples across 8 categories.\n\n3.\tThe method outperforms many open-source methods and competes with closed-source models."}, "weaknesses": {"value": "1.\tMy biggest concern is the novelty of the proposed method. I think the core idea of InstructX is very similar to metaQuery, including the core technique used. Compared to metaQuery, the differences lie in 1) InstructX uses Lora in MLLM while metaQuery simple freezes the MLLM, 2) InstructX uses lightweight MLP as the connector, while metaQuery use a relatively large transformer. There two modifications are good, and make the model well, but I don’t think the novelty is sufficient.\n\n2.\tWhile the author said the training on image data can emerge video editing capabilities, the method still needs video editing training data. Is it possible to completely get rid of video data?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yIRdah0qLs", "forum": "NPcdp9OIf8", "replyto": "NPcdp9OIf8", "signatures": ["ICLR.cc/2026/Conference/Submission7173/Reviewer_PmDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7173/Reviewer_PmDh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902944828, "cdate": 1761902944828, "tmdate": 1762919333499, "mdate": 1762919333499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "InstructX presents an end-to-end framework that unifies image and video instruction editing. A LoRA-tuned Qwen-VL-3B serves as the understanding module; 256 learnable queries are appended for images and 512 for videos, and only the hidden states of these queries are retained as “editing features.” A lightweight two-layer MLP replaces the large Transformer connector used in prior MetaQuery-style systems, mapping the query features to the text channel of a Wan 2.1 DiT decoder. Training is carried out in three stages: (1) image-only feature alignment, (2) mixed image–video training that injects the source VAE latent into the noise to boost fidelity and elicits zero-shot video capability, and (3) a small high-quality finetuning stage. Evaluated on ImgEdit-Bench and GEdit-Bench for images and the authors’ VIE-Bench for videos, InstructX attains better overall scores among open-source methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well structured and easy to follow. \n\n2.\tThe “small Query + LoRA + lightweight MLP” design matches the performance of large Transformer connectors while lowering memory and compute cost. \n\n3.\tThe evaluation is comprehensive, spanning multiple public benchmarks for both image and video editing."}, "weaknesses": {"value": "1.\tLimited novelty: the approach mainly extends the MetaQuery “query + connector” paradigm, replacing the large Transformer with a small MLP and adding extra VLM fine-tuning; the claimed unified image/video capability mainly relies on the pre-trained Wan 2.1 backbone rather than a new algorithmic contribution. \n\n2.\tThe video branch employs a fixed set of 512 queries; whether this is sufficient for long or highly complex videos—and thus able to capture long-range temporal semantics—remains unverified. \n\n3.\tThe paper lacks experiments or discussion on high-resolution video editing, leaving scalability to larger resolutions unexplored."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OHmVml4VTG", "forum": "NPcdp9OIf8", "replyto": "NPcdp9OIf8", "signatures": ["ICLR.cc/2026/Conference/Submission7173/Reviewer_jWAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7173/Reviewer_jWAu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970256913, "cdate": 1761970256913, "tmdate": 1762919333007, "mdate": 1762919333007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose InstructX, a unified framework for image and video editing that integrates MLLM with diffusion models. They adopt Separate Learnable Queries for image and video, finetune the MLLM using LoRA, and introduce a lightweight MLP connector to effectively inject the understanding capabilities of MLLM into the diffusion model. Extensive experiments on image and video editing tasks demonstrate the superior performance of InstructX. Additionally, the authors introduce VIE-Bench, a benchmark specifically designed to evaluate instruction-based video editing capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-designed architecture. InstructX leverages Learnable Queries (similar to MetaQuery) to enable effective modality interaction. Rather than treating MLLM as feature extractors that freeze the MLLM and train a large connector, the framework finetunes the MLLM with LoRA and uses a lightweight MLP connector, enabling faster convergence and improved performance. \n\n2. Well-designed data curation and training strategy. The authors create a large-scale training dataset, and their training strategy is well-motivated.\n3. Comprehensive evaluation. Comprehensive experiments on ImgEdit-Bench, GEdit-Bench, and proposed VIE-Bench show high performance on instruction-based image and video editing tasks."}, "weaknesses": {"value": "Interestingly, InstructX exhibits video segmentation and style transfer abilities, which are absent from the video training data but present in the image data. It would strengthen the paper to provide quantitative results comparing model performance when trained with only video data versus both image and video data, particularly on segmentation and style transfer tasks."}, "questions": {"value": "1. How to balance the training of image data and video data or different stages? Any ablations?\n2. Where can we find the model size of this method, and how can we compare it with other methods?\n3. How to handle the longer video editing in this framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JfQzPmVsjv", "forum": "NPcdp9OIf8", "replyto": "NPcdp9OIf8", "signatures": ["ICLR.cc/2026/Conference/Submission7173/Reviewer_7XS6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7173/Reviewer_7XS6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975033178, "cdate": 1761975033178, "tmdate": 1762919332572, "mdate": 1762919332572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "We thank all the reviewers for their efforts in improving our paper to a higher standard. We have addressed all your concerns and update the manuscript accordingly. If any further questions, please let us know—we welcome any discussion."}}, "id": "6gODBAMoRy", "forum": "NPcdp9OIf8", "replyto": "NPcdp9OIf8", "signatures": ["ICLR.cc/2026/Conference/Submission7173/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7173/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7173/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763740502905, "cdate": 1763740502905, "tmdate": 1763740502905, "mdate": 1763740502905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}