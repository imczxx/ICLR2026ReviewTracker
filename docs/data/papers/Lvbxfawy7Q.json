{"id": "Lvbxfawy7Q", "number": 12561, "cdate": 1758208573955, "mdate": 1759897501609, "content": {"title": "InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation", "abstract": "Large Language Models (LLMs) are increasingly capable of generating complete applications from natural language instructions, creating new opportunities in science and education. In these domains, interactive scientific demonstrations are particularly valuable for explaining concepts, supporting new teaching methods, and presenting research findings. Generating such demonstrations requires models to combine accurate scientific knowledge with the ability to implement interactive front-end code that behaves correctly and responds to user actions. This capability goes beyond the scope of existing benchmarks, which typically evaluate either knowledge question answering without grounding in code or static web code generation without scientific interactivity. To evaluate this integrated ability, we design a hybrid framework that combines programmatic functional testing to rigorously verify interaction logic with visually-grounded qualitative testing to assess rendered outputs against reference snapshots. Building on this framework, we present InteractScience, a benchmark consisting of a substantial set of carefully designed questions across five scientific domains, each paired with unit tests, reference snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs and report results that highlight ongoing weaknesses in integrating domain knowledge with interactive front-end coding. Our work positions InteractScience as the first benchmark to automatically measure this combined capability with realistic interactive operations, providing a foundation for advancing reliable and educationally useful scientific demonstration code generation.", "tldr": "", "keywords": ["Benchmark", "Large Language Models", "Scientific Demonstration", "Code Generation", "Interactive Applications"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/337364e0e50cdda7ccb8e14f96a086be5972528e.pdf", "supplementary_material": "/attachment/a1f2efdc0dc3477698a24397058a3717affc9231.zip"}, "replies": [{"content": {"summary": {"value": "This work describes a benchmark project of using large language models (LLMs) for creating scientific demos, spanning over five domains on 150 tasks. The authors employed 30 models for this task, and evaluated them by the accuracy of the content through test cases (LLM-generated) and the interactive performances (CLIP and VLM-Judge). This work shows how LLMs currently perform in the domain of scientific demo generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The work is easy to follow. Overall, the communication of the work is smooth and clear.\n+ The motivation of the work is well-established. It has great potential to be a pivotal work in the domain of scientific demo generation."}, "weaknesses": {"value": "- One major issue is the contribution of the work. While having these numbers showing how good each model is could be interesting for many model builders, it is not discussed how this work can be helpful in the future -- how would the authors position themselves in making the actual contribution towards these models? What do these results mean for the domain? These are way more important questions that the paper has a chance to discuss, but fails to do.\n- Another major limitation of the work is the missed details on many implementation processes. The details are below in questions, and some of them can be quite important validation questions."}, "questions": {"value": "- Lines 28-31: How exactly does the work help the domain despite sharing a bunch of numbers showing the model is not working very well now? This is not clear.\n- Line 47: Note: This is a fairly wide range scope claimed when talking about scientific demonstration code generation.\n- Line 66 Figure 1: three types of tasks. Do you have a reference for previous work categorizing tasks this way? This may (optionally) require some taxonomy prior work.\n- Line 91 VLM as judge: How accurate is this? The targeted audiences are also important. For example, how students perceive this will be different from how the general public does. Student levels are also important (elementary or high school?)\n- Related work: As commented, the scope of scientific visualization is too broad. Until now, we still don't know what exact domain/context we are trying to create interactive demos for. This is unclear to audiences.\n- Line 249: The exact selection criteria are needed.\n- Line 252: About the difficulty of the demos: How did you define the difficulty of problems/demos?\n- Line 257: Did you validate these generated tests to see if they work well?\n- Conclusion section: Results are shared and described in detail. However, there's no discussion on what this means in the main context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fta3827JUh", "forum": "Lvbxfawy7Q", "replyto": "Lvbxfawy7Q", "signatures": ["ICLR.cc/2026/Conference/Submission12561/Reviewer_ei4T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12561/Reviewer_ei4T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517707667, "cdate": 1761517707667, "tmdate": 1762923417810, "mdate": 1762923417810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents INTERACTSCIENCE, a new benchmark for evaluating LLMs on the task of scientific demonstration code generation. In this setting, the input is an implementation plan, while the output is a self-contained HTML file that renders an interactive scientific demonstration. The paper collects 150 examples from the Wolfram Demonstrations Project across five domains. For evaluation on these 150 examples, the authors propose Programmatic Functional Testing (PFT) -- a unit test for interaction logic -- and Visually-Grounded Qualitative Testing (VQT) -- a visual comparison using CLIP and a VLM-as-Judge. They evaluate 30 LLMs and provide quantitative results and analyses across difficulty levels and disciplines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- S1. The paper explores a useful domain -- scientific demonstration synthesis -- that goes beyond static visualization or plain code generation. This domain is particularly relevant for education, research demonstrations, etc.\n* S2. The authors introduce a systematic evaluation method combining unit tests, visual similarity measures, and checklist-based assessments. This evaluation suite can comprehensively assess the implementation quality of interactive demonstrations.\n* S3. The benchmark covers multiple disciplines and difficulty levels and provides a comprehensive comparison across many models."}, "weaknesses": {"value": "* W1. Limited novelty in the evaluation framework. The idea of designing unit tests and using a VLM-as-a-judge is not particularly new. While the framework is practical, it lacks strong originality.\n- W2. VLM-as-Judge reliability remains questionable. The \"Comparison of VLM-as-Judge Configurations\" (lines 429–462) only investigates which input combination yields the highest correlation with human scores. This justifies the prompt design but does not establish how reliable the VLM judge is compared to human evaluation. Therefore, the overall reliability of this automatic judging process remains unaddressed. I suggest that the authors also report the reliability of the VLM-as-Judge compared with human judgments.\n- W3. Potential evaluation issue. The visual evaluation assumes a single \"correct\" snapshot. However, many scientific demonstrations can have multiple equally valid visualizations (e.g., different color schemes, scaling, or styles). Such variability could lead to unfair penalization of valid outputs when using CLIP- or VLM-based scoring. How does the evaluation framework address this issue, or is this an inherent limitation of the proposed evaluation method?\n* W4. Lack of failure analysis. The analyses focus mainly on validating design choices (e.g., whether to include reference snapshots or checklists) rather than uncovering deeper insights or failure patterns of existing models. No detailed failure analysis is provided to explain why models fail, what error types dominate, or what insights could guide domain practitioners, which weakens the paper's analytical depth.\n* W5. Writing clarity and notation. Section 3.3 is hard to follow. Many notations (e.g., (t_{vqt}=(A,i_{ref},L))) are introduced only once and never reused, adding cognitive load without improving clarity. Simplifying descriptions and adding concrete examples would make the evaluation process easier to understand."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psZt6LVfZ3", "forum": "Lvbxfawy7Q", "replyto": "Lvbxfawy7Q", "signatures": ["ICLR.cc/2026/Conference/Submission12561/Reviewer_kGTP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12561/Reviewer_kGTP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601100599, "cdate": 1761601100599, "tmdate": 1762923417493, "mdate": 1762923417493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a comprehensive benchmark called INTERACTSCIENCE designed to evaluate large language models (LLMs)’ ability to generate interactive scientific demonstrations that combine scientific reasoning, front-end logic, and visual rendering. The study develops a hybrid evaluation framework combining Programmatic Functional Testing (PFT), which verifies user interaction logic through deterministic unit tests, and Visually-Grounded Qualitative Testing (VQT), which measures visual and semantic correctness using CLIP similarity and Vision-Language Model scoring. The resulting INTERACTSCIENCE benchmark contains 150 curated tasks from five scientific domains, each equipped with structured implementation plans, unit tests, and reference snapshots sourced from the Wolfram Demonstrations Project. Evaluations on 30 open- and closed-source models reveal that most models can produce interactive interfaces but often fail to maintain correct scientific logic, exposing a gap between functional and conceptual accuracy. Closed-source models like GPT-5 perform best overall, while large open-source models show scaling improvements. The benchmark offers the first automated, reproducible framework to jointly assess reasoning, visual fidelity, and interactivity in scientific code generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines and formalizes Scientific Demonstration Code Generation, a previously underexplored intersection of scientific reasoning, interactivity, and visualization\n2. Combining deterministic testing (PFT) and perceptual/semantic evaluation (VQT) is a strong methodological innovation that goes beyond subjective visual checks.\n3. The benchmark spans five domains and three difficulty levels, ensuring balanced diversity and challenging coverage.\n4. Evaluating 30 models across both open and closed-source LLMs."}, "weaknesses": {"value": "1. While correlation of VLM and human scores is measured, human involvement in data verification and semantic correctness remains minimal. The validation of synthesized test cases is important and should be included in the main content.\n2. In line 259, the author mentioned “manual inspection and rule-based validation,” but did not provide sufficient details on how the inspection was conducted or what specific rules were used for validation. In Appendix B, they stated that only 10 synthesized instances were randomly sampled for manual inspection, which makes this evaluation rather limited and less convincing.\n3. In figure 3, the results show better Overall Pass Rate (OPR) on hard question, which is a little bit counter-intuitive. From the analysis, the author did not provide a good explanation of this."}, "questions": {"value": "1. How did the authors verify the correctness of the synthesized test cases?\n2. For VLM-as-Judge, how were the checklists designed? How was the manual inspection conducted, and how many evaluators were involved? Additionally, how were conflicts between human evaluators resolved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9WZcHbiecD", "forum": "Lvbxfawy7Q", "replyto": "Lvbxfawy7Q", "signatures": ["ICLR.cc/2026/Conference/Submission12561/Reviewer_df5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12561/Reviewer_df5Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681241554, "cdate": 1761681241554, "tmdate": 1762923417203, "mdate": 1762923417203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new benchmark for language models that evaluates their ability to create interactive simulations of various mathematical, statistical, or scientific concepts by following the instructions laid out in a plan. The artifacts produced by the models are evaluated against reference demonstrations from the Wolfram Demonstrations Project, measuring fidelity in terms of both action-assertion tuples and the visual similarity of the outputs. The authors evaluate a large number of open- and closed-source LLMs on their benchmarks and provide a descriptive analysis of their results, arguing that the benchmark serves as an unsolved challenge which could be useful for developing more reliable AI assistants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this paper is its clarity: the motivation is well-supported, the technical aspects are well-explained (for the most part -- see below), and the conclusions generally are well-supported by the data. I’m also pleased with the wide range of models that were examined and the additional sub-experiments performed to validate the various metrics used for evaluation (though some details are missing -- again see below). I think that the topic of the paper is both timely and relevant."}, "weaknesses": {"value": "While the overall flow of the paper is clear, there are a few points in which important details are omitted or left vague which make the finer points more difficult to follow. For instance, the “Action Success Rate” is described as “the percentage of cases where the expected visual state appears after the specified action sequence” but “expected” is not clearly defined in either the main text or appendix. A reader might assume this refers to a scientifically correct output, but a later reference to ASR makes it seem as though it accounts for only the ability to, for instance, click a button or move a slider. In addition, unlike the other VQT measure, ASR is defined as an accuracy rate. If it’s based on the output of a VLM, how is the judge’s confidence incorporated into the measure?\n\nIn addition, I was not able to find any details about the human study used to validate the VLM-as-judge. How many participants were there, and how were they recruited / compensated? Again, I appreciate the addition of this experiment but it is difficult to assess its implications without more details.\n\nFinally, I think the paper would benefit from an explicit example of the entire evaluation process (i.e. a sequence of screenshots showing the output after each action along with the corresponding image from the reference demonstration), which would help clarify some of the points raised above."}, "questions": {"value": "- How is the Action Success Rate defined and computed?\n- What are the full details for the human study used to validate the VLM-as-judge?\n- Are there are common threads between the scientific concepts that models appear to struggle with (i.e. across disciplines)?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors include a human user study, but I wasn't able to find any details about the participants (i.e. number, recruitment process, compensation, or IRB approval)."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lIoWfcoj3s", "forum": "Lvbxfawy7Q", "replyto": "Lvbxfawy7Q", "signatures": ["ICLR.cc/2026/Conference/Submission12561/Reviewer_Y5KH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12561/Reviewer_Y5KH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770312514, "cdate": 1761770312514, "tmdate": 1762923416622, "mdate": 1762923416622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents scientific demonstration code generation by large language models. Towards that end, the authors adapted VLM-as-judge to evaluate the tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely topic focusing on the LLM and code generation\n- Broader impact towards the research community"}, "weaknesses": {"value": "1. The human evaluation of VLM-as-Judge is absent. I recommend the authors to consult the following paper [1], specifically Table G1 and G2 of Appendix with human judgment and LLM-as-Judge correlation and Cohen’s Kappa between multiple raters on a small subset. This step is crucial, as LLMs demonstrate hallucinations and prompting language also determines LLM evaluation quality.\n\n2. The recent relevant literature is missing. In fact, SWE-bench Multimodal already have multimodal programming questions answered [2].\n\n\n\nReference\n\n1. Zhou, Xuhui, Hyunwoo Kim, Faeze Brahman, Liwei Jiang, Hao Zhu, Ximing Lu, Frank Xu et al. \"Haicosystem: An ecosystem for sandboxing safety risks in human-ai interactions.\" COLM 2025\nhttps://arxiv.org/pdf/2409.16427\n2. Yang, J., Jimenez, C.E., Zhang, A.L., Lieret, K., Yang, J., Wu, X., Press, O., Muennighoff, N., Synnaeve, G., Narasimhan, K.R. and Yang, D., SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?. In The Thirteenth International Conference on Learning Representations. https://www.swebench.com/multimodal.html"}, "questions": {"value": "How does the proposed benchmark perform compared to the SWE-bench Multimodal?\nLink:  https://www.swebench.com/multimodal.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RnufwGBtGA", "forum": "Lvbxfawy7Q", "replyto": "Lvbxfawy7Q", "signatures": ["ICLR.cc/2026/Conference/Submission12561/Reviewer_uoa9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12561/Reviewer_uoa9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12561/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934202935, "cdate": 1761934202935, "tmdate": 1762923416086, "mdate": 1762923416086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}