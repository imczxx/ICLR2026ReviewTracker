{"id": "sx48LQhCqF", "number": 8375, "cdate": 1758080571119, "mdate": 1763716002487, "content": {"title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View", "abstract": "The ongoing debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks the reasoning capabilities of large language models (LLMs) is still not fully resolved. Some studies contend that RLVR mainly improves sampling efficiency but at the expense of diversity and exploratory capacity, resulting in capability boundary shrinkage. In contrast, others demonstrate that prolonged training can lead to the emergence of novel reasoning strategies, suggesting capability boundary expansion. To reconcile these contradictory findings, we theoretically and empirically show that both perspectives are partially valid—each aligning with a separate phase in an inherent two-stage probability mass dynamic: (1) Exploitation stage: initially, the model primarily samples explored high-reward and low-reward tokens, while rarely selecting the potentially optimal token. Positive advantage estimates increase the probability of high-reward tokens and decrease those of low-reward tokens, yet the optimal token’s probability remains largely unchanged during this stage. (2) Exploration stage: as training advances, the growth rate of previously acquired high-reward tokens slows as their probabilities approach saturation. When a potentially optimal token—now receiving positive advantage estimates—is occasionally sampled, its probability increases, while those of the originally high-reward tokens decrease. This dynamic suggests that over-exploitation during the exploitation stage may lead to capability boundary shrinkage, whereas prolonged training into the exploration stage can promote an expansion of the reasoning capability boundary. Building upon our insights, we revisit the potential of only using relative negative gradients for prolonging training, providing a theoretical and empirical foundation for the development of more advanced reasoning capabilities.", "tldr": "Based on a two-stage dynamic view of probability mass allocation, this study theoretically and empirically addresses the ongoing debate on whether RLVR shrinks or expands LLM reasoning capabilities.", "keywords": ["Reinforcement Learning with Verifiable Rewards", "Reasoning Capability Boundary", "A Two-Stage Dynamic"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0833cae669c8ee1bf015db83fb4037842e04d3ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates a central controversy in RLVR: whether RLVR shrinks or expands the reasoning capability boundary of LLMs. By analyzing the dynamical evolution of the change of logits in the gradient space, the authors propose a two-stage dynamic framework of probability mass evolution:\n\n1. Exploitation stage: the model mainly reinforces high-reward tokens, causing capability boundary shrinkage.\n2. Exploration stage: once high-probability tokens saturate, relative negative gradients redirect updates to low-probability yet potentially optimal tokens, enabling boundary expansion.\n\nThey derive the expected logit update rule (Theorem 1), validate the two-stage behavior through a toy example, and then empirically evaluate GRPO-N and GSPO-N (variants using only relative negative gradients) on MATH, AIME, AMC, ARC-c, and MMLU-Pro benchmarks, showing improved exploration and stable Pass@k performance compared to GRPO/GSPO.\n\nThe topic is timely and relevant, the proposed two-stage framework is inspiring and well supported by toy example and theoretical analysis. However, I have the following two main concerns. First is the potential gap between theoretical analysis and (see weakness 1) the practical GRPO methods we use. Second is the novelty of the proposed method (see weakness 2). So, although I indeed enjoy reading this paper and believe that the paper has great potential, I could only give a weak reject at this stage. I am looking forward to see the improvement of the paper and would be happy to increase my evalution during the discussion stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Timely and relevant topic.**\nThe “RLVR reasoning capability debate” is currently a major research question (e.g., DeepSeek-R1, O1/O3). The paper addresses it with both theoretical and empirical clarity.\n\n2. **Elegant theoretical framing and well-designed toy example**\nThe derivation of *Lemma 1 + Theorem 1* connects token-level probability dynamics to capability boundary movement, presenting a physically interpretable “force-like” view similar to learning-dynamics literature. The intuition that action a1 saturates and then the confidence of action a2 gradually increases is very novel and inspiring. (However, this also leads to my main concern in weakness 1). \n\n3. **The paper is well written and easy to follow.**"}, "weaknesses": {"value": "1. If I understand correctly, the two-stage dynamics unfold as follows, following the notations from the toy example. At the beginning of training, action a1 (with reward r1=0.8) and action a3 (with reward r3=0) have relatively high confidence and are therefore more likely to be sampled than action a2 (with reward r2=1.0). During this early stage, since the confidence on a1 has not yet saturated, even though the model occasionally samples a2 by chance, the increase in the probability of a1 dominates the updates. In other words, the probability mass reduced from penalizing a3 is primarily reallocated to a1.\n    \n    As training proceeds and the probability of a1 gradually saturates while the probability of a3 continues to decrease, the probability mass released from a3 is then absorbed by a2. This marks the beginning of the exploration stage described in the paper, where the pair of a1 and a2 may be sampled within the same group. In this phase, since r1<r2, a1 effectively becomes the relative negative example, and the probability mass shifts from a1 to a2. This process can, in principle, continue if there exist additional hidden actions, for example, a4 with reward r4>r2>r1. (I.e., a4 will dominate as the training goes on.)\n    \n    **A key assumption in the above reasoning is that r1<r2.** If r1=r2, the gradients applied to a1 and a2 would always be identical, preventing a2 from absorbing probability mass from a1. However, in most practical implementations of GRPO and its variants, the reward is 1/0. This creates a conceptual gap between the theoretical framework and real training settings.\n    That said, I do not view this gap as a bad thing. Rather, it highlights an important research direction: incorporating more fine-grained reward structures into RLVR training. From this perspective, I find the framework proposed in this paper highly promising. If the authors could explicitly discuss this assumption and consider RLVR variants with continuous or graded rewards, the paper would be significantly strengthened.\n\n2. The proposed method itself is not sufficiently novel. GRPO-N and GSPO-N simply remove the positive-gradient component, which is conceptually similar to the NSR configuration discussed in [1]. I understand that the main contribution of this paper lies in introducing the two-stage framework to interpret the dynamics of reasoning capability in RLVR, rather than proposing a new algorithm. However, incorporating an algorithmic idea directly inspired by this framework would make the paper much stronger. For instance, one could design a quantitative metric to detect the end of the exploitation stage during training, and subsequently increase the influence of negative examples once the model transitions into the exploration stage.\n\n[1] Zhu, Xinyu, et al. \"The surprising effectiveness of negative reinforcement in LLM reasoning.\" arXiv preprint arXiv:2506.01347 (2025)."}, "questions": {"value": "1. It would be nice to add some titles to each subfigure in Figure 1. Or at least highlight their different settings in the caption. It took me quite a while to find that the rewards of different settings (in small legends) are different.\n\n2. The second panel in Figure 2 shows that GSPO can mitigate the entropy decay compared with GRPO. The gap after ablating the positive gradients of GSPO is also smaller. Will the theory proposed in this paper explain this phenomenon?\n\n3. I am not quite sure what we could conclude from Figure 3. Any discussions about that?\n\n4. How sensitive is the two-stage transition point to group size G?\n\n5. (Not so important) Is it possible to have a demonstration in the real setting, like Figure 1 in the toy example? Figure 2 might be a good starting point. Maybe directly observing the confidence of the chosen token and other candidate tokens? Not quite sure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0hS79AdLYE", "forum": "sx48LQhCqF", "replyto": "sx48LQhCqF", "signatures": ["ICLR.cc/2026/Conference/Submission8375/Reviewer_PFHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8375/Reviewer_PFHy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760768537201, "cdate": 1760768537201, "tmdate": 1762920282803, "mdate": 1762920282803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether RL with verifiable rewards (RLVR) causes reasoning shrinkage or expansion in large language models. It proposes a theoretical analysis of Softmax logit and what to link to the “two-stage dynamics”:\n(1) an exploitation stage where high-reward tokens are reinforced and probability mass concentrates, causing capability shrinkage, followed by\n(2) an exploration stage where prolonged training redistributes probability mass toward previously low-probability optimal actions, enabling capability expansion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Discussing a real and important empirical debate in RLVR reasoning.\n2. The toy helps intuition\n3. Experiments cover multiple baselines (GRPO, GSPO, ±N variants) and several reasoning benchmarks."}, "weaknesses": {"value": "The central “two-stage dynamics” claim lacks rigorous justification:\n- The proposed two-phase interpretation remains largely speculative and is not formally or mathematically derived.\n- The analysis does not identify when—or under what precise conditions—a transition between “shrinkage” and “expansion” should occur, nor does it guarantee that such a transition will happen in practice.\n- In particular, if the model is already sufficiently exploitative, it may rarely sample low-confidence alternative solutions, making the proposed expansion phase unlikely and potentially leading instead to losing exploration ability.\n\nGRPO-N lacks novelty and is equivalent to prior NSR: The paper’s main algorithmic contribution, GRPO-N, is effectively identical to the previously proposed Negative Sample Reinforcement (NSR) method [1]. It follows the same underlying idea and exhibits the same empirical behavior. This undermines the claimed novelty of the submission.\n\n[1] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gnpV7RkAUs", "forum": "sx48LQhCqF", "replyto": "sx48LQhCqF", "signatures": ["ICLR.cc/2026/Conference/Submission8375/Reviewer_xPjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8375/Reviewer_xPjS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716550213, "cdate": 1761716550213, "tmdate": 1762920282443, "mdate": 1762920282443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an ongoing controversy in reinforcement learning with verifiable rewards of whether RLVR shrinks or expands reasoning capability boundaries. The authors investigate probability mass dynamics, identifying two phases of training: An exploitation stage (associated with boundary shrinkage) and an exploration stage (associated with expansion). Based on their analysis, the paper proposes modified algorithms that use only relative negative gradients. Results show improved reasoning diversity and competitive Pass@k scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed two-stage dynamic provides an interesting and compelling conceptual synthesis: early exploitation leads to capability concentration, followed by later exploration leading to expansion.\n- The paper is well motivated, written, and easy to follow. \n- The experimental section is comprehensive, spanning multiple datasets with in- and out-of-domain evaluations.\n- Experiments with Qwen2.5-Math-7B show improvements over the base model for large k."}, "weaknesses": {"value": "- The theoretical analysis approximates importance ratios as ~1 and omits regularizers (KL, clipping). While this is justified for simplicity, it could overlook how they affect the two-stage transition in real training.\n- While the authors' variant with Llama-3.2-3B-Instruct improves upon GRPO, the method still suffers from capability shrinkage, as indicated by the lower Pass@k metrics compared to the base model (Appendix C.3). The paper may benefit from more depth in the discussion of the comparison between Qwen and Llama. \n- The experiments appear to run for fewer than 50 training steps (Fig. 2). Given that the paper’s central argument hinges on dynamics observable during prolonged training, this experimental horizon seems too short to observe the dynamics fully."}, "questions": {"value": "- How do clipping and KL-regularization alter the two-stage dynamics?\n- Why is the performance of Llama-3.2-3B-Instruct compared to the base model worse than the metrics of Qwen2.5-Math-7B?\n- Why does GSPO-N lower test-entropy compared to GSPO?\n- When using only negative gradients, what is the impact on training stability?\n- Why did you only train for fewer than 50 training steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1YYFxJAVEz", "forum": "sx48LQhCqF", "replyto": "sx48LQhCqF", "signatures": ["ICLR.cc/2026/Conference/Submission8375/Reviewer_rxbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8375/Reviewer_rxbi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998381503, "cdate": 1761998381503, "tmdate": 1762920281913, "mdate": 1762920281913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the ongoing discussion on RLVR by making the proposition that RLVR can both shrink and expand LLM reasoning capability, in two phases. Early exploitation stage sharpens distribution and narrows support; the later exploration stage redistributes mass to newly sampled, high-reward regions.\n\nTheoretically, it provides a mathematical formalization of this two-stage process. Methodologically, based on this analysis, the paper proposes GRPO-N and GSPO-N, variants of group policy optimization algorithms. These methods operate by using \"exclusively relative negative gradients\".1 The authors claim this approach mitigates entropy collapse and enables the stable training required to reach exploration stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a timely topic, so it is very interesting to read this paper.\n\n- The theoretical contribution us simple derivations that connect relative advantages to probability mass reallocation.\n\n- The empirical observation on entropy collapse and rebound aligns with other findings that RLVR often collapses entropy early without intervention."}, "weaknesses": {"value": "- The underlying problem of understanding RLVR dynamics is both valid and important. However, the paper’s framing of it as an unresolved paradox (“shrinkage, expansion, or both?”) is not well supported. Cui et al. clearly show that entropy drops sharply during the early training stage, with most performance gains and the associated entropy collapse occurring early on (see figure 2 in their work). Liu et al. demonstrate that “sustained gains” require extended training, using stability mechanisms such as policy resets to expand reasoning boundaries. The literature does not present a real debate or contradiction—rather, it reflects observations of different phenomena occurring at different time scales.\n \n\n- The paper's core method, GRPO-N that uses only negative-advantage gradients appears to be a direct rediscovery of NSR (fZhu et al.) The authors cite this work but fail to compare against it or differentiate their contribution."}, "questions": {"value": "- The paper claims that GRPO-N's entropy \"significantly surpasses that of the base model\". Given GRPO-N only punishes bad actions and never reinforces good ones, isn't this \"high entropy\" simply the policy flattening by learning what not to do, without ever converging on what to do, as explained by Lemma 1? How do you explain this \"high entropy\" with GRPO-N's Pass@1 performance in Table 1?\n\n- GRPO-N method discards all positive-gradient samples. This seems sample-inefficient. Have you measured the wall-clock time and number of samples GRPO-N requires to converge, compared to standard GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tTyBqJ0egM", "forum": "sx48LQhCqF", "replyto": "sx48LQhCqF", "signatures": ["ICLR.cc/2026/Conference/Submission8375/Reviewer_RBrT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8375/Reviewer_RBrT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052533274, "cdate": 1762052533274, "tmdate": 1762920281287, "mdate": 1762920281287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}