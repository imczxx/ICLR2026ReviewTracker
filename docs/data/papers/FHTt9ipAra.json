{"id": "FHTt9ipAra", "number": 7218, "cdate": 1758011993202, "mdate": 1763744336821, "content": {"title": "Mitigating Hallucination in Vision-Language Model with Depth and Spatial-aware Key-Value Refinement", "abstract": "Large vision–language models (VLMs) deliver state-of-the-art results on a wide range of multimodal tasks, yet they remain prone to visual hallucinations, producing content that is not grounded in the input image. \nDespite progress with visual supervision, reinforcement learning, and post-hoc attention reshaping, the representational origins of hallucinations remain unclear.\nOur study reveals that successful grounding emerges when adjacent visual tokens exhibit coherent alignment, while hallucinations arise when key vectors scatter isotropically, weakening cross-modal attention and blurring object boundaries. \nBuilding on this insight, we propose Depth and Spatial aware Cache Refinement (DSCR), a lightweight and training-free method that augments the Transformer's key-value (KV) cache with depth cues and 2D spatial proximity. \nDSCR clusters vectors within objects and separates those across surfaces, guiding attention toward relevant regions without any fine-tuning.\nComprehensive evaluations show that DSCR consistently reduces hallucinations, delivering up to 23\\% accuracy gains across MME, POPE, RePOPE, CHAIR, and a new depth-sensitive benchmark. \nOur findings highlight KV-coherence as a core factor behind hallucinations and demonstrate a practical, model-agnostic solution for enhancing VLM reliability.", "tldr": "", "keywords": ["Hallucination in Vision Language Model", "Depth and Spatial-aware key value Cache Refinement", "Key-Value Cache Manipulation", "Multi Modal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e011727f28a1945e2feefea2c50e040a5a469e0e.pdf", "supplementary_material": "/attachment/37bea7438f954086d06a693b90355fa4415bf7ab.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free and lightweight method to mitigate LVLM hallucinations by encoding spatial and depth priors. The core idea is to re-weight the key–value pairs of visual tokens in the cache to enhance grounding, without additional training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated and supported by insightful visualizations. The method is novel and achieves consistent improvements on both standard object hallucination benchmarks and attribute/spatial hallucination benchmarks.\n2. The paper is overall well written and easy to follow."}, "weaknesses": {"value": "1. Since both MME and POPE are yes/no questions, it would strengthen the evaluation to extend Table 3 to include detailed CHAIR scores across LVLMs and baselines.\n2. The paper provides spatial and depth priors but does not provide enough discussion of related work on grounding visual information.\n\n**Clarification**\n1. (Major) In Table 1, Qwen-VL Count row: your method is not the best (+155) but is still bolded. Similarly, for Qwen-VL Poster row, your method (165.99) is not the best. Please correct this.\n2. In Figure 1, the Non-Hallucination subfigure is a bit confusing. why are there two “A. Yes” labels?"}, "questions": {"value": "1. I saw the ablation results in Tables 10–14. Could you provide more intuition on why key-only reweighting performs better, and why selecting layers 10–39 (deep layers) works best?\n2. To confirm: my understanding is that you apply re-weighting only to the visual token cache, not the text tokens. If that’s correct, it would help to make this explicit throughout sec 2.3.\n3. A broader question: do you envision future LVLMs incorporating 3D vision encoders directly to enhance spatial reasoning?\n\nwould be happy to raise my score if my concerns are addressed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bpfPU8Qw4e", "forum": "FHTt9ipAra", "replyto": "FHTt9ipAra", "signatures": ["ICLR.cc/2026/Conference/Submission7218/Reviewer_gacf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7218/Reviewer_gacf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760712741121, "cdate": 1760712741121, "tmdate": 1762919364689, "mdate": 1762919364689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "COMMON RESPONSE"}, "comment": {"value": "**COMMON RESPONSE**\n\nWe sincerely thank Reviewers nKMD, qhDk, UhZY, and gacf for their careful evaluation and constructive feedback. We are encouraged that they highlighted the following strengths of our work:\n\n- **New perspective on hallucination mechanism:** Reviewers appreciated that our analysis links hallucinations to the coherence of neighboring key vectors and to object boundary patterns in Transformer representations, which provides a clear and novel direction to study the representational origin of hallucinations in VLMs. (Reviewer nKMD; Reviewer qhDk; Reviewer UhZY)\n\n- **Training free, model agnostic, lightweight design:** DSCR was recognized as a simple and efficient method that refines the KV cache at inference without additional training, can be applied to diverse LVLM backbones, and can be combined with many existing hallucination mitigation methods. (Reviewer nKMD; Reviewer qhDk; Reviewer UhZY; Reviewer gacf)\n\n- **Strong and consistent empirical gains:** Reviewers noted that DSCR achieves excellent performance and consistent improvements across multiple hallucination benchmarks and architectures, covering both standard object hallucination and attribute or spatial hallucination settings. Reviewer qhDk in particular highlighted that our experiments report accuracy gains of up to 23%. (Reviewer nKMD; Reviewer qhDk; Reviewer UhZY; Reviewer gacf)\n\n- **Comprehensive analysis and ablations:** The paper was recognized for extensive analyses such as attention score comparisons and PCA visualizations of key vector distributions with and without DSCR, as well as ablation studies on depth only, spatial only, combined depth plus spatial weighting, and layer ranges. These results support the current design and also motivated several useful suggestions for additional quantitative metrics and comparisons, which we address in our response. (Reviewer nKMD; Reviewer UhZY; Reviewer gacf)\n\n- **Clear and accessible presentation:** Multiple reviewers commented that the paper is well written, easy to follow, and supported by effective figures and visualizations that clarify both the method and its empirical effects. (Reviewer nKMD; Reviewer qhDk; Reviewer UhZY; Reviewer gacf)\n\nWe are grateful for these positive assessments and for the detailed suggestions in the weaknesses and questions. In the remainder of our response, we address each concern in turn and provide additional experiments and analyses to further support the effectiveness and robustness of DSCR.\n\n**All modifications in the main paper are highlighted in blue.**"}}, "id": "hcGLwQ6hdn", "forum": "FHTt9ipAra", "replyto": "FHTt9ipAra", "signatures": ["ICLR.cc/2026/Conference/Submission7218/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7218/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7218/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763740563965, "cdate": 1763740563965, "tmdate": 1763740577519, "mdate": 1763740577519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the representational origin of visual hallucinations in vision–language models (VLMs) and proposes a training-free method, Depth and Spatial-aware Cache Refinement (DSCR), that modifies the visual key–value (KV) cache before decoding. The central empirical observation is that successful grounding correlates with coherent alignment of neighboring key vectors, while hallucination correlates with isotropically scattered keys that blur object boundaries. DSCR injects depth cues and 2D spatial proximity into the KV cache by forming a proximity-weighted refinement of keys (optionally values), which clusters vectors within objects and separates vectors across surfaces. The method is model- and query-agnostic, adds negligible overhead, and shows gains on multiple hallucination benchmarks. The paper also introduces a hallucination benchmark to occlusions and similar-depth confounders."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear mechanistic story grounded in the model’s internal representations. The paper ties hallucination to a loss of neighboring-key coherence, and supports this with PCA-based visualizations across layers and attention diagnostics that show increased attention to image tokens when DSCR is applied.\n- Simple, training-free intervention with broad applicability. DSCR only modifies the KV cache at inference, without changing weights. The reported overhead is small and the method improves several VLM backbones.\n- Consistent gains on hallucination evaluations. Across MME, POPE/RePOPE, CHAIR, and AMBER, the method shows improvements; the depth hallucination mini-benchmark highlights gains in occluded and similar-depth cases."}, "weaknesses": {"value": "- The link between “neighboring-key coherence” and hallucination is mainly supported by PCA visualizations and attention trends. The paper describes that, in hallucination cases, keys scatter and object boundaries blur, but it does not define a quantitative measure of “neighboring-key similarity” or “key-vector dispersion,” nor does it report large-scale correlations with hallucination/error rates. To strengthen the claim, it is benificial to introduce simple, layer-wise metrics (for example, average cosine similarity with spatial neighbors, or a local PCA explained-variance ratio) and report correlation/predictive power with confidence intervals.\n\n- Comparisons to alternative geometry-injection routes are missing. Since DSCR effectively injects geometric priors, it would be informative to compare against baselines that (i) use a stronger vision encoder that already has geometric priors or (ii) concat depth features with visual tokens as inputs for VLM without KV rewriting. This would clarify the unique benefits of operating in KV cache.\n\n- Evidence for depth–spatial complementarity is limited. In Table 13, depth-only and spatial-only achieve the same total score (645), while the combined setting is only modestly higher (650), with improvements concentrated in one submetric. Figure 4(c,d) validates each component on its targeted subset but does not establish that combining both consistently outperforms either alone across the same benchmarks. More side-by-side results with multiple runs, error bars, and paired tests, and stratified by scene attributes (e.g., occlusion density, depth discontinuities, similar-depth distractors), are helpful to show that the combination truly helps.\n\n- Breadth beyond hallucination-centric tasks is limited. The positive COCO captioning result is helpful, but a broader suite (e.g., additional VQA settings) would better establish that there is no negative transfer to general VL capabilities."}, "questions": {"value": "- Since DSCR leaves VLM weights frozen, could the lack of adaptation to DSCR-refined keys limit the attainable gains? Have you tried enabling fine-tuning with DSCR active, and if so, did you observe larger improvements or any shift in preferred settings (e.g., Key-only vs. Key+Value, layer ranges)?\n- DSCR modifies only the visual branch of VLMs. Can it mitigate hallucinations that arise when the language model misinterprets visual tokens (i.e., unsatisfied vision-language alignment)? Do you have controlled analyses or case studies indicating whether DSCR helps in such language model-driven error modes?\n- Semantic richness vs. key scattering. The paper describes hallucination cases where keys scatter and object boundaries blur. Could similar patterns also appear in images with genuinely rich, heterogeneous semantics? After applying DSCR, is there any measurable loss of semantic diversity that could harm performance on VL tasks requiring fine-grained distinctions or rare attributes? \n- Minor presentation. In Figure 1(a), the non-hallucination examples show “X” and “O” even though there is no wrong answer, which is confusing. Also, the “A.” prefix for answers can be misread as an option label. Clarifying the notation would improve readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Z3EEy0fJn", "forum": "FHTt9ipAra", "replyto": "FHTt9ipAra", "signatures": ["ICLR.cc/2026/Conference/Submission7218/Reviewer_UhZY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7218/Reviewer_UhZY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794153771, "cdate": 1761794153771, "tmdate": 1762919364234, "mdate": 1762919364234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Depth and Spatial-aware Cache Refinement (DSCR) — a lightweight, training-free method to suppress visual hallucinations in LVLMs. The authors identify that hallucinations stem from incoherent alignment among key vectors (KVs) within the Transformer’s attention mechanism, which disrupts the cross-modal grounding between visual and textual inputs. DSCR addresses this by refining the Transformer’s KV cache using geometric and spatial priors derived from monocular depth estimation. Through integrating 3D depth cues and 2D spatial proximity, DSCR enforces coherence among tokens representing the same object and separates tokens across different surfaces, thereby improving visual grounding and reducing false object generation. Extensive experiments over multiple benchmarks demonstrate up to 23% accuracy improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a novel and elegant perspective on the origin of hallucinations by analyzing the internal coherence of key vectors within multimodal Transformers.\n\n2. The DSCR method is training-free, model-agnostic, and computationally efficient.\n\n3. The experimental evaluation is comprehensive and rigorous, and the writing is clear."}, "weaknesses": {"value": "1. As shown on the right side of Figure 5, while DSCR leverages pre-computed depth maps, its performance inevitably depends on the quality of depth estimation, which may introduce inaccuracies in complex lighting or occlusion scenarios.\n\n2. Why would the misalignment of key vectors weaken the model’s ability to correctly interpret visual inputs? I believe there is no essential connection. I hope the authors can provide further clarification.\n\n3. This paper focuses on optimizing the static reasoning phase, without exploring joint training or adaptive tuning of deep cues. I believe that the misalignment between visual and textual modality features is also an important factor contributing to hallucination. Should additional training be incorporated into DSCR to enhance alignment with textual vectors?"}, "questions": {"value": "1. Does DSCR take the user’s input query into account? If it only considers image features while ignoring textual features, will this lead to performance degradation in handling more complex queries?\n\n2. Why does an isotropic distribution of key vectors tend to produce hallucinations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gc8PhvJ5TW", "forum": "FHTt9ipAra", "replyto": "FHTt9ipAra", "signatures": ["ICLR.cc/2026/Conference/Submission7218/Reviewer_qhDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7218/Reviewer_qhDk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808726892, "cdate": 1761808726892, "tmdate": 1762919363792, "mdate": 1762919363792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to mitigate hallucination in VLMs by first studying the key vector distribution in model's transformer layers. The authors discovered that key vector distributions exhibit distinct object boundary patterns when models are faithful, while showing blurred object borders when hallucinating. This phenomena motivated the authors to propose DSCR, a training-free method that corrects key and value vectors using depth and spatial similarity maps as guidance. Under DSCR, a separate depth estimator model is used to generate a depth map. Coupled with spatial proximity, weightage is calculated to rebalance the key and value vectors during inference. Experimental results show that DSCR achieves high performance on various hallucination benchmarks. In addition, DSCR is also complementary with other hallucination mitigation methods, further boosting the evaluation performance on multiple open source VLM models. \n\nThe paper contributes to the research community by introducing a new correlation between key vector similarity and hallucination. The training free method can be applied on top of many existing works to further reduce VLM hallucination and improve model performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Overall, the paper is well written and the concepts are easy to follow. The authors also offer a new direction to study the underlying cause of hallucination. The proposed DSCR method shows good performance and generalisability. It is an efficient design that is complementary with many existing hallucination mitigation methods. The validity of DSCR design is sufficiently supported by many experiments. The author conducted extensive analysis experiments including attention score value comparison and key vector distribution visualisation for models with and without DSCR correction. The authors also conducted comprehensive ablation experiments such as depth only, spatial only and depth with spatial weightage calculation as shown in appendix."}, "weaknesses": {"value": "It is insightful for the authors to reveal the different key vector PCA visualisations for hallucinating and non-hallucinating VLM inference scenarios. However, this qualitative analysis can be sensitive to different factors such as object size, object position, difficulty in text query, etc. This slightly undermines the reliability of this discovery and thus the motivation of the method design. The authors could provide more evidence, such as quantitative experimental results, to show that the phenomena is universal, that blurring of object boundaries key vectors is common for different images and query types."}, "questions": {"value": "1. In figure 3 different shades represent different values. However, why does the distribution pattern look like this? Which 2 image patches are compared?  \n2. Based on the ablation experiment table 14, it shows that the best performance is achieved when DSCR is applied to layer 10–39. Why does the final method apply to all layers instead of following the finding from this ablation study?  \n3. For the key vector PCA visualisation in figure 1, does the pattern stay the same across different layers? Or is the distinct object boundary only specific to certain attention layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GRJL0nus6F", "forum": "FHTt9ipAra", "replyto": "FHTt9ipAra", "signatures": ["ICLR.cc/2026/Conference/Submission7218/Reviewer_nKMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7218/Reviewer_nKMD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882292009, "cdate": 1761882292009, "tmdate": 1762919363464, "mdate": 1762919363464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}