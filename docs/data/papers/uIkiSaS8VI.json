{"id": "uIkiSaS8VI", "number": 278, "cdate": 1756733308980, "mdate": 1759898269448, "content": {"title": "Reinforcing Agentic Search Via Reward Density Optimization", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low **Reward Density** in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the **Reward Density Optimization** problem, which aims to improve the reward obtained per unit of exploration cost. We introduce **InfoFlow**, a systematic framework that tackles this problem from three aspects. 1) **Subproblem decomposition**: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) **Failure-guided hints**: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) **Dual-agent refinement**: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.", "tldr": "", "keywords": ["tool-integrated reasoning", "large language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccfd883066855e71544165c361178990721faff9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a dual-agent mechanism for Deep Search Question Answering (DSQA) using LLMs. The method makes three main contributions: (1) a dual-agent framework with a researcher agent that plans and takes actions and a refiner agent that summarizes gathered information; (2) the introduction of subgoals to guide the search; and (3) off-policy hints to improve agent performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1- Comparison to strong recent baselines.\n2- A wide range of datasets. \n3- Outperforming all baselines in most of the datasets."}, "weaknesses": {"value": "1- The presentation should be much clearer. For example, the process for generating hints needs to be described in sufficient detail in the main paper, with a pointer to the appendix for further specifics. Currently, the main paper provides no explanation of how the off-policy hints are produced.\n\n2- Although the proposed method clearly outperforms the baselines, the performance gains seem trivial. If we effectively double the resources by employing two agents simultaneously and further add subgoals and hints, it is expected that the method would surpass agents that lack these tools. Moreover, the hints are generated by a separate LLM (Gemini 2.5 Pro), indicating that the approach relies on even more resources."}, "questions": {"value": "1- How are the off-policy hints generated? Are they constructed manually?\n\n2- The paper states that each node in the reasoning tree is a subgoal to achieve. This would include nodes that are not on the final path. Is treating all such nodes as subgoals/milestones the intended and correct approach?\n\n3- Could you clarify the statement: “Following the official implementation, accuracy is judged by an LLM.” What exactly does “judged by an LLM” entail?\n\n4- In Table 2, when comparing models of the same size, your method employs two LLM agents. Does this effectively mean your approach uses roughly twice the parameters/resources of the other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TOcaVI6TLL", "forum": "uIkiSaS8VI", "replyto": "uIkiSaS8VI", "signatures": ["ICLR.cc/2026/Conference/Submission278/Reviewer_a4QG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission278/Reviewer_a4QG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761438346622, "cdate": 1761438346622, "tmdate": 1762915483790, "mdate": 1762915483790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfoFlow, a dual-agent framework designed to address the problem of low reward density when training language model agents for agentic deep search tasks. The framework tackles this by combining three techniques: (1) decomposing tasks into sub-problems and shaping rewards at intermediate stages, (2) using adaptive off-policy hints to guide exploration and rescue failed rollouts, and (3) introducing a division of labor between a “researcher” agent (responsible for high-level planning) and a “refiner” agent (summarizing retrieved evidence to reduce the trajectory context burden). InfoFlow is empirically evaluated on a series of benchmarks for deep question answering and agentic search."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.   The paper addresses a well-motivated and topical challenge in RL for information-seeking agents: the problem of sparse and low-density rewards in multi-step reasoning and deep search. The reward shaping method, which leverages explicit subgoal decomposition with grounded, weighted milestones, is conceptually clear and technically well-integrated.\n2.   InfoFlow’s architecture is thoughtfully designed: separating researcher and refiner agents is a practical step toward reducing context length and making deeper reasoning more feasible. Figure 2 compellingly visualizes this effect, showing accuracy gains along with drastic context length reduction, which directly supports the claim about memory bottlenecks in monolithic LLM agents.\n3.   The use of adaptive off-policy hints is well-motivated and addresses a real limitation in on-policy RL for sparse, long-horizon tasks."}, "weaknesses": {"value": "1.   The method’s tight coupling to the InfoSeek dataset (with its tree-structured subgoal annotation and LLM-generated hints) limits broader impact. While the technique is shown to outperform prior work on standard QA benchmarks, the core innovation (dense, process-based guidance) seems most advantageous only when such decomposition is available.\n2.   There are no experiments analyzing the quality, coverage, or utilization/impact of hints injected during RL—an important aspect given the claimed centrality of off-policy hints to exploration. Similarly, the comparative breakdown of successful interventions via hint-injection (at various values of $K_h$ or types of hints) is missing."}, "questions": {"value": "1.   Could the impact of increased computation or inference cost in the dual-agent framework be quantified relative to conventional approaches?\n2.   In Table 1, the “solve none” rate remains elevated (>45%) even after co-training, indicating that difficult instances remain largely unsolved. What are the key failure modes, and do they cluster by reasoning depth or other properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PdlPehR0dH", "forum": "uIkiSaS8VI", "replyto": "uIkiSaS8VI", "signatures": ["ICLR.cc/2026/Conference/Submission278/Reviewer_hDmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission278/Reviewer_hDmp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896791607, "cdate": 1761896791607, "tmdate": 1762915483632, "mdate": 1762915483632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets deep-search agent tasks and proposes a pipeline named InfoFlow to tackle the sparse-reward problem in RLVR settings. The method introduces sub-goal–based intermediate rewards, injects hints into partially failed trajectories to increase success probability, and employs a refiner agent to summarize long texts and generate concise representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and common challenge in deep-search agent tasks.\n2. The writing is clear and easy to follow.\n3. The use of an auxiliary refiner agent to summarize and compress task information is interesting and potentially generalizable."}, "weaknesses": {"value": "1. The main innovations claimed by this paper: sub-goal–based dense rewards and failure-guided hints are primarily data-driven enhancements rather than methodological advances. Both rely on stronger external LLM APIs for sub-goal generation and hint generation through specifically designed prompts. This makes the approach highly engineered and dataset-specific rather than an automated, generalizable \"strategy\". The contribution is therefore weakened, as the benefit largely stems from using denser supervision from stronger models. In contrast, many prior works have proposed principled, model-independent methods for reward densification, from general reward shaping in RL to process reward models in LLM training. Thus, relying on the LLM API to provide a sub-goal reward applicable only to customized datasets seems highly intensive and lacks innovation, and should certainly not be considered the main contribution of the paper.\n2. Since the sub-goal decomposition, sub-goal reward, and trajectory hints are all derived from external APIs, the method distills additional knowledge into the training data. Comparing this enhanced dataset against baselines trained on unaugmented data is unfair and does not demonstrate algorithmic superiority, only data quality differences.\n3. In Section 3.2.1, the use of the term *rejection sampling* is inaccurate, as the described process is a deterministic data filtering procedure rather than probabilistic rejection sampling in the Monte Carlo sense. I suggest the authors revise this terminology to avoid misunderstanding.\n4. The paper provided an anonymous code link, but the repository is empty (at least at the time of review)."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U0L1H7yhsF", "forum": "uIkiSaS8VI", "replyto": "uIkiSaS8VI", "signatures": ["ICLR.cc/2026/Conference/Submission278/Reviewer_weiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission278/Reviewer_weiU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914026312, "cdate": 1761914026312, "tmdate": 1762915483455, "mdate": 1762915483455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InfoFlow, a reinforcement-learning framework for agentic deep search that tackles sparse rewards by optimizing reward density through three ingredients: (i) sub-goal reward shaping from a research-tree decomposition with importance-weighted sub-goals, (ii) adaptive off-policy hints that inject high-leverage guiding queries when rollouts stall, and (iii) a dual-agent architecture where a Refiner condenses retrieved evidence for a Researcher to stabilize and shorten reasoning–search loops; training is bootstrapped via rejection-sampling fine-tuning on ~3.4k high-quality trajectories from InfoSeek before on-policy RL (GRPO). Empirically, InfoFlow (3B/7B) outperforms strong open agents, even larger 32B ones, and is competitive with proprietary models on BrowseComp-Plus and general QA; ablations show the biggest drop without dual-agent RFT, with further declines when removing sub-goal shaping and hints, underscoring each component’s contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly identifies and tackles the core bottleneck of deep, search-integrated reasoning—low reward density—and designs the system around increasing accuracy with less context, laying a principled target for optimization.\n\n- The dual-agent architecture (Researcher + Refiner) is well-motivated and empirically effective: adding a 3B Refiner boosts success while cutting the Researcher’s context by ~45%, freeing capacity for higher-level planning."}, "weaknesses": {"value": "1/ Despite gains, absolute accuracy on the hardest setting remains low (e.g., BrowseComp-Plus tops out around ~23% for 7B), suggesting substantial headroom and possible brittleness on deep-search tasks. Why do trajectories still fail most of the time, and what failure modes dominate (retrieval, planning, or grounding)?\n\n2/ Evaluation relies on a single retriever (BM25) \"for fair comparison.\" This controls a variable but also risks a retriever bottleneck driving the results. Would neural or hybrid retrieval change the ranking or reduce the need for hints/shaping? Please provide sensitivity to the retriever choice.\n\n3/ The method’s strongest ablation effect is from the dual-agent RFT cold-start; without it, performance collapses (e.g., 23.2->10.2 on BrowseComp-Plus). This raises concern that the approach heavily depends on curated, high-quality trajectories; how robust is it when RFT data are noisy, scarce, or domain-shifted? \n\n4/ Sub-goal shaping and off-policy hints densify rewards but may risk reward hacking or overfitting to annotation style. How are sub-goals validated, and can the policy succeed when hints are withheld at test time or when the sub-goal generator is imperfect?"}, "questions": {"value": "Please refer to the questions in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8jP8W8lft3", "forum": "uIkiSaS8VI", "replyto": "uIkiSaS8VI", "signatures": ["ICLR.cc/2026/Conference/Submission278/Reviewer_GJJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission278/Reviewer_GJJo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942270660, "cdate": 1761942270660, "tmdate": 1762915483259, "mdate": 1762915483259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}