{"id": "QP7WX3XmEy", "number": 10036, "cdate": 1758157546045, "mdate": 1759897679482, "content": {"title": "SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models", "abstract": "Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces **SafeProtein**, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated **SafeProtein-Bench**, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available.", "tldr": "", "keywords": ["Biosafety", "Protein Red-teaming", "Protein Jailbreak"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0b1dc42259daedd56a8fcdf55bcc7004b9235e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SafeProtein, a red-teaming framework, and an accompanying benchmark SafeProtein-Bench, designed to evaluate the biosecurity risks of protein foundation models. Experiments show that under multiple masking and generation strategies, ESM3 and DPLM2 can be induced to recover potentially harmful proteins."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper focuses on the critical issue at the intersection of AI safety and biosecurity, providing a systematic framework and benchmark for this problem.\n- The authors propose a variety of generation strategies, including multiple masking strategies and five generation paradigms (including Foldseek structural prompts and score-guided beam/diffusion guidance)."}, "weaknesses": {"value": "Major\n- The authors used ESMFold to predict structures of generated sequences, which is fast but not very accurate, and ESM-2 is used as the backbone network in ESMFold. Since the training sets of ESM2 and ESM-3 may have an overlap, the authors should further validate this potential bias.\n- The highest success rates reported (e.g., for Strategies 2, 4, and 5 in Figure 3 and Table 4) all rely on providing the \"Native Backbone Structure\" as a prompt. This is an extremely strong assumption that weakens the \"jailbreak\" claim. \n\nMinor:\n- Fig 1A: Maked -> Masked"}, "questions": {"value": "- What is the pLLDT distribution of the predicted structures? Low-quality structures should be rejected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4ZfANX4Bbq", "forum": "QP7WX3XmEy", "replyto": "QP7WX3XmEy", "signatures": ["ICLR.cc/2026/Conference/Submission10036/Reviewer_MKFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10036/Reviewer_MKFR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539492286, "cdate": 1761539492286, "tmdate": 1762921441362, "mdate": 1762921441362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SafeProtein, a systematic red-teaming framework for protein foundation models, together with a benchmark, SafeProtein-Bench. The core task is masked recovery on real harmful proteins (toxins and viral proteins): the authors mask sequence token, then probe whether models can reconstruct both sequence and structure under five generation conditions; and a score-guided diffusion scheme. Evaluation is intentionally conservative: joint thresholds on sequence identity and structural RMSD (tighter cutoffs at lower mask ratios) to reduce false positives. Experiments on ESM3-open and DPLM2-650M show notably high “jailbreak” success, especially for ESM3 under score-guided decoding. The authors argue this indicates latent dual-use/biosecurity risks in current Protein FMs. They also acknowledge limits (no wet-lab validation; larger closed models not tested) and sketch future alignment/mitigation directions. This is an interesting approach, but my major concern is about whether the conclusion is solid enough."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work targets a real governance gap: standardized, model-facing tests for dual-use risks in Protein LMs. Translating LLM red-teaming into a protein co-design setting, and operationalizing “jailbreak” as conservation-aware masked recovery, provides a concrete, reproducible way to probe risk.\n\n- The framework covers multimodal prompting (sequence + structure), multiple decoding strategies (including heuristic beam and diffusion guidance), and a clear evaluation protocol. The components form a closed loop that other groups could adopt or extend.\n\n- The dataset focuses on experimentally resolved harmful proteins (toxins/regulated viruses), enforces length filters, and ships conservation profiles—making the masking task biologically meaningful (i.e., hitting likely functional residues rather than random spans).\n\n\n- Results are broken down by model, masking strategy, mask ratio, and generation strategy; the added Strategy 4/5 experiments for ESM3 make the security story clearer (structure prompts and score-guided decoding substantially raise success rates).\n- The overall presentation of the paper is excellent."}, "weaknesses": {"value": "- The main claim relies on joint identity + RMSD as a proxy for “harmful capability.” That’s a reasonable first pass, but structural proximity does not guarantee functional equivalence. Without even lightweight functional proxies, risk remains “potential.” Adding small-scale functional surrogates (active-site recovery accuracy; catalytic residue recovery; binding/docking scores; interface geometry preservation) would strengthen the causal link from “recovery” to “operational risk.”\n\n\n- For scale reasons the paper uses ESMfold to predict structures of generated sequences. If RMSD/ptm disagreements exist versus AF3 (or similar models), some success/failure calls and even the relative ordering of strategies might change at the margin. As I know, protein structure prediction models at current stage have significant variances(like Proteinix, AF3, ESMFold).\n\n\n- Score-guided decoding uses sequence identity with a ptm penalty, which is close to the evaluation signal. The consistently high success rates across masks/ratios might partly reflect alignment between the guidance objective and the benchmark metric. A sensitivity study replacing the guidance score with more “functional” criteria (e.g., pocket geometry, interface complementarity, docking energy, active-site recovery) would clarify whether the effect persists under metric shifts.\n\n\n- The Foldseek pipeline filters candidates by UniProt annotations to avoid harmful templates, but annotation coverage is imperfect. Structural near-neighbors could still encode homologous functional scaffolds. More detail on blacklists/thresholds/human review, plus error analysis of false admits/false rejects, would help assess whether Strategy 3 is genuinely “benign-guided” rather than “near-homolog-guided.”\n\n\n\n- The paper notes protein sequences are not human-readable; consequently, “success” is defined operationally via recovery metrics. A short appendix that explicitly maps red-team success to real-world misuse preconditions would limit over-interpretation and guide policymakers."}, "questions": {"value": "Could the authors check a representative subset and report agreement with ESMfold on RMSD/ptm (e.g., mean absolute differences, rank correlations for success/failure)? If discrepancies are sizable, how do conclusions change (e.g., which strategies look most risky)?\n\n\nAny plans for evaluations on larger ESM3 variants? If direct access is infeasible, could the authors demonstrate transferability: learn attack strategies on open models and apply them, via constrained prompts/decoding knobs, to closed APIs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper showed the risk of generating bad proteins, but I think it is still far from actual concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fwsZLxgxZO", "forum": "QP7WX3XmEy", "replyto": "QP7WX3XmEy", "signatures": ["ICLR.cc/2026/Conference/Submission10036/Reviewer_KK47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10036/Reviewer_KK47"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797134798, "cdate": 1761797134798, "tmdate": 1762921441116, "mdate": 1762921441116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission introduces ​​SafeProtein​​, the first systematic red-teaming framework designed to evaluate and expose potential biosafety risks in protein foundation models (e.g., ESM3, DPLM2). The authors also present ​​SafeProtein-Bench​​, a curated benchmark dataset of harmful proteins (toxins, viral proteins) alongside a comprehensive evaluation protocol. By combining multimodal prompt engineering and heuristic beam search, SafeProtein successfully jailbreaks the protein foundation models, showing that protein foundation models present potential biosafety risks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel Problem Formulation:** This submission introduces a new research direction by systematically studying the red-teaming of protein foundation models, a topic that has not been previously explored in the literature.\n\n- **Comprehensive Methodological Exploration:** Given the absence of prior work, the authors propose multiple red-teaming strategies and conduct extensive experiments to compare their effectiveness, providing valuable insights into the relative strengths of different approaches.\n\n- **Well-Constructed Benchmark:** The paper presents SafeProtein-Bench, a carefully curated benchmark for evaluating the dual-use potential of protein language models. The benchmark includes a manually verified dataset of 429 harmful proteins with experimentally determined structures, along with detailed dataset construction procedures and a rigorous evaluation protocol."}, "weaknesses": {"value": "- **Limited Conceptual Foundation**: The fundamental premise of applying LLM-style \"jailbreaking\" to protein models appears to have limited scientific value. The work primarily combines concepts from two distinct domains without demonstrating significant depth in machine learning methodology or practical applicability. The approach lacks substantive technical innovation beyond this conceptual fusion.\n\n- **Narrow Model Evaluation**: The study's scope is constrained to only two protein foundation models (ESM3 and DPLM2), with no comparative analysis of their respective results. This limited selection prevents meaningful generalizations about the broader landscape of protein models and their vulnerabilities.\n\n- **Restricted Benchmark Scope**: As a benchmark, SafeProtein-Bench suffers from limited coverage, focusing exclusively on diffusion-based language models while neglecting other important architectural paradigms such as autoregressive models for protein sequences or diffusion approaches for backbone structure generation.\n\n- **Insufficient Failure Analysis**: The paper lacks thorough investigation of failure cases, missing opportunities to provide insights into why certain attack strategies succeed while others fail. This omission limits the work's utility for developing effective defensive strategies against potential misuse."}, "questions": {"value": "See the weakness above. \n\nThe paper's fundamental premise raises an important question: **Why study the safety of protein foundation models, and is there significant evidence for its necessity?**\n\nThe work would benefit from stronger justification connecting the demonstrated adversarial vulnerabilities to plausible threat scenarios that justify the research investment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper only involves in silico evaluation. Thus, there are no ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWnFGwFd0c", "forum": "QP7WX3XmEy", "replyto": "QP7WX3XmEy", "signatures": ["ICLR.cc/2026/Conference/Submission10036/Reviewer_b3zG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10036/Reviewer_b3zG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813054346, "cdate": 1761813054346, "tmdate": 1762921440840, "mdate": 1762921440840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies **red-teaming for protein language models (PLMs)** — a scenario in which an adversarial user attempts to use PLMs to discover harmful protein sequences. The authors construct a dataset of harmful proteins (toxins and viruses) and investigate whether a **generative model** can recover these harmful sequences when they are masked."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Given the task of recovering sequences from curated databases, the authors do a good job of considering multiple sampling techniques.  \n- The topic of **safety** and **dual-use** concerns in generative models for biology is both timely and important."}, "weaknesses": {"value": "- I am not entirely convinced of the **relevance** of this specific type of red-teaming for producing harmful proteins. The idea that harmful proteins could be generated through *zero-shot prompting* of PLMs seems unrealistic, to the best of my understanding.  \n  Can the authors justify why they believe harmful proteins would realistically be generated this way?  \n- The paper does not sufficiently discuss prior research on the **generation of harmful proteins**, which is central to the stated goal of this work.  \n  It would strengthen the paper if the authors devoted a substantial portion of the introduction — or even an entire section — to known or plausible **use cases of foundation models for harmful purposes**, and then clearly justified how their evaluation setup directly relates to these use cases."}, "questions": {"value": "1. Can the authors provide a **comprehensive literature review** on potential use cases of foundation models for designing harmful proteins?  \n2. Can the authors clearly explain the **role of foundation models** in these use cases and how their proposed evaluation helps **mitigate such risks**?  \n3. Are the proteins in the curated harmful-protein database **part of the ESM-3 training set**?  \n   If so, how does that affect the validity of the results?  \n   More broadly, for the task of discovering **new viruses or toxins**, shouldn’t we primarily care about sequences **outside** the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VTzyRcUFU4", "forum": "QP7WX3XmEy", "replyto": "QP7WX3XmEy", "signatures": ["ICLR.cc/2026/Conference/Submission10036/Reviewer_vm5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10036/Reviewer_vm5Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960707569, "cdate": 1761960707569, "tmdate": 1762921439798, "mdate": 1762921439798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}