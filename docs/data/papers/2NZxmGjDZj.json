{"id": "2NZxmGjDZj", "number": 21302, "cdate": 1758316035773, "mdate": 1759896929836, "content": {"title": "Learning the energy relaxation manifold from unrelaxed structures with RelaxNet", "abstract": "In efforts to bypass computationally-expensive density functional theory (DFT) calculations for energy minimization and structure relaxation, rapid progress in the development of machine learning force fields (MLFF) and more robust models that adhere to quantum chemistry/physical paradigms and constraints have been realized. However, most research to date involves energy predictions in a static frame only (i.e., given a specific atomic configuration, predict the energy of the current or final instance), which neglects intermediary physical insight-providing contexts. In this work, we developed RelaxNet, a dynamics-aware, equivariant deep learning model that leverages neural ordinary differential equations (neural ODEs) and message passing neural networks (MPNNs) for predicting the energy relaxation landscape between the initial unrelaxed structure and final relaxed structure for the first time. From just the initial structure, which is often the configuration that is fed into DFT simulations, we can accurately recover the energy/forces for the entire trajectory at a competitive prediction accuracy. We further provide extensive insights on the use of implicit vs. explicit latent embedding evolution schemes to offer perspectives on optimal methods for future works integrating expensive graph-based neural networks and neural ODEs.", "tldr": "We developed RelaxNet, a dynamics-aware, equivariant model that leverages neural ordinary differential equations and message passing neural networks for predicting energy relaxation landscapes between initial unrelaxed and final relaxed structures.", "keywords": ["neural ODEs", "energy minimization", "trajectory", "relaxation", "forcefield", "optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a894cda6e864d7e3ad8d348c45670ab74d7b0ac0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors tackle the task of predicting the per-structure energy and per-atom forces for a relaxation trajectory, given only the initial positions.\n\nThey propose to train a NeuralODE wrapped around a MPNN.\nThe ODE state consists of positions x(0) and learned “velocity” that is updated via the predicted force (obtained as gradient of the predicted energy).\nThe authors ablate two design choices to update the latent embeddings h, that are produced by the encoder block.\nImplicit latent: h is not an ODE state. It’s recomputed from the current positions x(t) via the encoder at every pseudo-time step and then used to predict energy. \n\nExplicit latent: h(0) is initialized and included as an ODE state variable, then updated each step by an MLP (along with the position and velocity)\nThey test their approach on crystal structure trajectories from the JARVIS database."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Moves beyond single-frame energy prediction to trajectory learning between initial and relaxed states\n- On JARVIS, authors report 17.06 meV/atom (final frame) vs. ~27–33 meV/atom for baselines, albeit with caveats"}, "weaknesses": {"value": "- It is unclear to me when one would be interested in predicting a relaxation trajectory compared to a MLIP that can do relaxation and any other dynamics (like MD)\n- The approach limits data to sequential frames of fixed length\n- The experiments only cover one not-so-common dataset with only 30k datapoints"}, "questions": {"value": "- I think the real benchmark is running relaxations with MLIPs. Can you compare the trajectory and final geometry error between relaxations with MLIPs, RelaxNet, and ground truth trajectories?\n- I don’t quite understand how table 3 is computed. As I understand, RelaxNet will arrive at a different final structure than the data. Are the reported MAE_E for the baseline at the final RelaxNet-generated-structure or at the dataset-structure? For the ground truth, do you run DFT on the RelaxNet-generated-structure?\n- You report only the MAE of the Energies in table 3, can you also include the Force MAE?\n- It would be helpful to highlight (bold) the best numbers in the tables\n- Can you measure the difference in inference time cost between running relaxations with an MLIP to using RelaxNet?\n- What does this sentence in line 107 mean? \"these equivariant models can be extended to periodicity-dependent applications, like molecular modeling and DFT”\n- Does the use of a Neural ODE increase the cost of training compared to an MLIP by a lot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UYLF22Nurz", "forum": "2NZxmGjDZj", "replyto": "2NZxmGjDZj", "signatures": ["ICLR.cc/2026/Conference/Submission21302/Reviewer_J9gF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21302/Reviewer_J9gF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687249775, "cdate": 1761687249775, "tmdate": 1762941681170, "mdate": 1762941681170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RelaxNet, a neural ODE-based model that predicts the entire DFT energy relaxation trajectory from just the initial unrelaxed structure."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is a novel problem with a well-proposed method. Predicting the entire relaxation trajectory is both novel and of high practical value. \n\n- provides good starting points for DFT simulations, reducing convergence time\n\n- offers a clear comparison of implicit vs. explicit latent evolution across different trajectory lengths\n\n- physics-based model"}, "weaknesses": {"value": "- Expensive training: 133-311 min/epoch for implicit method is prohibitive; even explicit (17-137 min) is slow. How scalable is this model\n- No analysis of trajectory smoothness, physical plausibility, or whether intermediate states are actually useful\n- limited dataset coverage"}, "questions": {"value": "- Beyond energy and force MAE, how well does RelaxNet reproduce the actual geometric pathway of the relaxation? For example, what is the average RMSD between the predicted and true final structures?\n\n- The model is trained on DFT relaxation trajectories. If you initialized it with a perturbed structure that lies off the training trajectories (but within the distribution of atomic configurations), would it reliably find a path to a reasonable local minimum, or is it primarily learning to interpolate between seen initial and final states?\n\n- How sensitive are the results to the choice of the ODE solver (RK4) and its hyperparameters (tolerances, step size)? Was any exploration done with adaptive solvers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6PvIqhUz2K", "forum": "2NZxmGjDZj", "replyto": "2NZxmGjDZj", "signatures": ["ICLR.cc/2026/Conference/Submission21302/Reviewer_HMBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21302/Reviewer_HMBn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870145363, "cdate": 1761870145363, "tmdate": 1762941680829, "mdate": 1762941680829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RelaxNet, a dynamics-aware and equivariant deep learning model for structure relaxation. To address the limitation of prior works that primarily predict energy for static structures while overlooking intermediate physical insights, RelaxNet leverages neural ordinary differential equations (neural ODEs) combined with message-passing neural networks (MPNNs) to model the energy relaxation landscape between initial and final structural states. Empirical results and extensive ablations are provided to support the rationale of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of using neural ODEs to model relaxation trajectories is novel and thoughtfully implemented, offering a promising alternative formulation for this task.\n* RelaxNet achieves competitive accuracy compared to state-of-the-art methods on standard benchmarks."}, "weaknesses": {"value": "* While modeling intermediate states is intuitively appealing, it remains unclear why this should necessarily improve the prediction accuracy of the final relaxed state. As shown in Table 3, the performance gain over strong baselines is marginal. The authors should provide stronger intuition—or theoretical justification—for why this intermediate modeling leads to better final-state predictions.\n* Insufficient experimental analysis:\n  * There is no clearly defined, chemically meaningful accuracy threshold for evaluating relaxation performance, making it difficult to assess the practical utility of the reported results.\n  * A comparison of training cost (e.g., wall-clock time, memory usage) between RelaxNet and baselines is missing. Given the modest performance improvement, such analysis is essential to evaluate the method's efficiency.\n  * The requirement that training systems must contain more than $n$ frames (due to the neural ODE formulation) raises concerns about generalizability, since this limitation may restrict the applicability of RelaxNet to systems where long relaxation trajectories are unavailable."}, "questions": {"value": "* In Equation (2b), the notation $G$ is used without a clear definition. Could the authors clarify its meaning?\n* Could the authors provide more detailed explanations of the data processing steps mentioned in line 181?\n* The second term in Equation (7), $\\hat{E}_{t, s+1} - \\hat{E}_{t,s}$, is difficult to interpret. Could the authors explain it more clearly?\n* How exactly is the energy MAE computed for RelaxNet for evaluation? \n* For systems with varying numbers of frames, the authors sample $n$ equidistant points. Does this result in different effective time intervals between samples? If so, could this introduce ambiguity in the learned dynamics or physical consistency of the gradient field?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns observed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LA8jQ98B7N", "forum": "2NZxmGjDZj", "replyto": "2NZxmGjDZj", "signatures": ["ICLR.cc/2026/Conference/Submission21302/Reviewer_w4Rk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21302/Reviewer_w4Rk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106711970, "cdate": 1762106711970, "tmdate": 1762941680426, "mdate": 1762941680426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}