{"id": "OGDIXDfaN4", "number": 4320, "cdate": 1757662707453, "mdate": 1759898039784, "content": {"title": "COMI: Coarse-to-fine Context Compression via Marginal Information Gain", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.", "tldr": "Coarse-to-fine Context Compression via Marginal Information Gain", "keywords": ["Large Language Model", "Long Context"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cef54c3c6dc3042fd25fcb251187dcbf77580d54.pdf", "supplementary_material": "/attachment/35d2983b31ae8fbad7e192f4dfcc0e595a2eb190.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the computational inefficiency and semantic redundancy of long-context LLMs, arguing that existing methods overlook redundancy among retained tokens. The proposed COMI framework introduces Marginal Information Gain (MIG)—a metric that balances query relevance with low redundancy—to guide its two-stage compression process. This process includes Coarse-Grained Group Reallocation for adaptive budget distribution across context segments and Fine-Grained Token Merging for a weighted combination of unique tokens within those segments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main idea of establishing a compression criterion based on Marginal Information Gain (MIG), which explicitly rewards relevance and penalizes redundancy, is a valuable contribution to the field. This addresses a key weakness of previous work that depends only on relevance.\n\nThe paper validates COMI across a robust set of benchmarks spanning different task types, including single-hop QA, multi-hop QA, QA on long narratives, and summarization, using two modern LLM backbones (LLaMA-2-7B and Qwen2-7B), demonstrating its generalizability and robustness."}, "weaknesses": {"value": "**1) Lack of Direct MIG Metric Validation**\n>The paper's entire foundation rests on the Marginal Information Gain (MIG) metric, which influences two key, complex decisions: group reallocation and token weighting. The authors rely on circular reasoning, assuming that the improvement in end-to-end performance (EM/F1 scores) implicitly proves that MIG is effective.\n>However, they only provide theoretical justification (Appendix A) and indirect, final results. They neglect to conduct an essential diagnostic experiment to verify that the MIG score actually indicates higher informative value and lower redundancy in a measurable, ground-truth context. Specifically, they do not demonstrate whether a token's MIG level correlates better with its true importance (such as being part of the ground-truth answer) than a simple relevance score.\n>To improve this, the authors need to perform an independent, diagnostic validation of MIG prior to embedding it into the complex COMI framework. This should include a statistical comparison (like AUC) showing MIG's ability to predict critical tokens better than mere relevance.\n\n**2) Missing Diagnostic Evidence for Core Claims (Retention and Elimination)**\n>The paper's two main objectives are to successfully retain query-relevant information and remove semantic redundancy. However, the current evaluation is inadequate because it only reports final LLM outputs (EM, F1), which serve as indirect indicators of compression efficiency.\n\n> **Evidence for Retention**: The paper never reports on the efficiency of its selection process. It is unknown what percentage of the actual critical tokens (those required for the ground-truth answer) are physically retained in the compressed context compared to baseline methods. The high final EM score is thus only an indirect indicator, not a definitive measure of success in retention.\n\n> **Evidence for Redundancy Elimination**: The most novel part of the MIG metric is the redundancy penalty, yet the authors provide no internal analysis to demonstrate that the final compressed context is actually less redundant or more diverse than contexts generated by relevance-only methods. Without a quantitative measure of diversity within the compressed context itself, the claim that MIG effectively \"eliminates semantic redundancy\" remains unsubstantiated."}, "questions": {"value": "1. Validation of the Novel Metric (MIG)\n>The effectiveness of the core technical contribution, the Marginal Information Gain (MIG) metric, is currently inferred from final performance scores. We consider the direct empirical validation of this novel metric to be paramount for the rigor of the submission.\n>Could the authors provide a dedicated diagnostic analysis to verify that the computed MIG level correlates better with a token's true importance (e.g., its presence in the ground-truth answer) than a pure relevance score does?\n\n2. Missing Diagnostic Evidence for Core Claims\n>The paper claims success in retaining query-relevant information and removing semantic redundancy.\n>To support these crucial claims, could the authors provide additional quantitative analyses demonstrating the effectiveness of the proposed methodology in both areas? This evidence should move beyond EM/F1 scores to examine the characteristics of the compressed context itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g5NXF8Jo3h", "forum": "OGDIXDfaN4", "replyto": "OGDIXDfaN4", "signatures": ["ICLR.cc/2026/Conference/Submission4320/Reviewer_RNvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4320/Reviewer_RNvR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761195708955, "cdate": 1761195708955, "tmdate": 1762917294811, "mdate": 1762917294811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMI, a context compression framework that tackles inefficiency and redundancy in long-context LLMs. Its core contribution is the Marginal Information Gain (MIG) metric, which guides compression by balancing query relevance against information redundancy.\n\nThe MIG metric prioritizes information that is both salient (relevant) and unique (non-redundant).\n\nCOMI operates in a coarse-to-fine process:\n\nCoarse-Grained Group Reallocation: Dynamically allocates the compression budget across large semantic groups based on their inter-group MIG.\n\nFine-Grained Token Merging: Uses intra-group token-level MIG as SoftMax weights to merge tokens, preserving key information within each group.\n\nKey Results:\nCOMI was evaluated on QA and summarization tasks (e.g., NaturalQuestions, MultiNews) at high compression rates (up to 32x). Using a Qwen2-7B backbone at 32x compression, it achieved an improvement of approximately 25 points in Exact Match (EM) on NaturalQuestions, significantly outperforming baselines. Ablation studies also confirmed the effectiveness of both stages and the redundancy penalty."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clear Motivation: The paper's motivation is clear. It identifies that LLMs fail to address the high semantic redundancy found among query-relevant tokens, leading to suboptimal attention allocation. The pilot experiment provides empirical evidence for this claim. The redundancy and semantic overlapping in the compression units have not been covered mainly in this domain. \n\nStrong Empirical Results: The method demonstrates significant performance improvements over existing baselines, especially at high compression rates.\n\nEfficiency: The paper reports reasonable latency, suggesting the method is practical and achieves its strong performance."}, "weaknesses": {"value": "### Applicability to Stronger Long-Context Models: \n\nWhile COMI effectively boosts the base models' ability to digest context with reasonable latency, it notably outperforms even the original prompt baseline. This suggests that the chosen base models (LLaMA-2-7B-chat, Qwen2-7B-instruct) have weak innate long-context understanding (also widely known as the \"lost-in-the-middle\" problem).\nTherefore, I want to see whether COMI can still provides benefits when applied to current models (e.g. Qwen3-4B-Instruct, ...) that are already equipped with long-context capabilities. For such models, simply using the full, uncompressed prompt might be a more direct and effective solution for these QA tasks. While this might be a bit rigorous standard for a compression paper, demonstrating gains on these current models would be necessary to significantly support and expand the paper's claims.\n\n### Performance at Scale (Context Length): \n\nIt seems that the experiments are capped at no longer than 32k context. This is relatively limited given that modern models are pushing to 1M+ tokens. How does the method scale to contexts significantly longer than 32k? A stress test on much larger inputs is needed to verify its scalability.\n\n### Risk of Single-Token Representation for Budgeting\n\nThe coarse-grained reallocation stage determines a segment's entire compression budget based on the MIG score of a single representative token (the one with max query relevance). I am concerned about the validity of this proxy. If other, non-representative tokens within that same segment also contain information crucial to the segment's overall value (e.g., less relevant but also far less redundant info), their contribution is ignored during this critical budgeting step. This could lead to an inaccurate estimation of the segment's true aggregate MIG (relevance and redundancy) and result in a suboptimal budget allocation. I wonder if this simplification is problematic."}, "questions": {"value": "Details of Figure 1: Could you provide more detail of the Figure 1? Also, I think It would be better to measure the redundancy of compression within the existing compression methods rather than LLMs attention, which can more clearly illuminate the limitations of current methods.\n\nImpact of segment size: What is the size (e.g., number of tokens) of the \"equal-length\" segments used for coarse-grained reallocation? How was this size chosen, and how sensitive is the model's performance to this value?\n\nTraining Fairness: Were all baseline methods compared under identical training data and compute budgets to ensure a fair comparison?\n\nCompression Rate Flexibility: The method appears to be trained for and evaluated at fixed compression rates (e.g., 16x, 32x). Is it possible for COMI to dynamically determine an optimal compression rate based on the query or content complexity, rather than relying on a fixed target?\n\nQualitative Analysis and Interpretability: Can qualitative examples or case studies (including failure cases) be provided? It might be insightful to approximate the results of compression into interpretable forms (e.g. natural language) and compare with other baselines.\nThis would help demonstrate how COMI retains important information and removes redundancy in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SdHvht0IPE", "forum": "OGDIXDfaN4", "replyto": "OGDIXDfaN4", "signatures": ["ICLR.cc/2026/Conference/Submission4320/Reviewer_Nnax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4320/Reviewer_Nnax"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579286231, "cdate": 1761579286231, "tmdate": 1762917294325, "mdate": 1762917294325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMI, which applies Marginal Information Gain (MIG) to compress long contexts. It first reallocates compression budgets across segments via inter-group MIG, then merges tokens within each segment using intra-group MIG weights. Experiments are conducted to demonstrate the effectiveness of this method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methods part of the paper is written clearly.\n2. The structure of the paper is well organized."}, "weaknesses": {"value": "1. The proposed method seems to work only for the encoder-decoder architecture, but this is not the mainstream architecture nowadays. There is no motivation discussion to clarify why the authors would like to design a compression method that does not work for the most popular decoder-only structure.\n2. There is no analysis of the compression rate pattern. The compression rate is determined dynamically via MIG; however, the paper does not provide any analysis to illustrate the pattern of the compression rate.\n3. There is no comparison of the compression rate between the proposed method and other methods. I think this is important to eliminate the concern that performance improvement may come from the relatively low compression rate compared to other methods.\n4. The efficiency analysis needs more details. The proposed method separates the compression and generation stages, but there is no information on which stages are used in Table 3.\n5. There is no limitation part to discuss the potential improvements of the paper."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmovsJIFLj", "forum": "OGDIXDfaN4", "replyto": "OGDIXDfaN4", "signatures": ["ICLR.cc/2026/Conference/Submission4320/Reviewer_fQJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4320/Reviewer_fQJU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736509011, "cdate": 1761736509011, "tmdate": 1762917293678, "mdate": 1762917293678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issues of slow inference, high computational cost, and information redundancy in large models under long-context scenarios by proposing COMI: a coarse-to-fine compression framework based on Marginal Information Gain (MIG). Building upon GMSA, the method introduces two key improvements:\n\n1.Adaptive compression rate allocation for different segments using MIG as a metric.\n\n2.Weighted token merge within groups based on MIG, suppressing redundancy while preserving relevance.Experiments show that COMI significantly outperforms existing methods even at a 32× compression rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Addresses redundancy among relevant tokens by incorporating a penalty score for related but redundant tokens, effectively reducing token redundancy.\n\n2.Adaptive compression rate allocation across groups avoids applying the same pruning rate to both high-relevance and low-relevance groups, making the method more reasonable.\n\n3.Weighted token merging within groups based on MIG scores reduces redundancy while ensuring relevance and preserving semantic diversity.\n\n4.Experiments demonstrate the effectiveness of COMI across different models and task types."}, "weaknesses": {"value": "1.Insufficient clarity in figures: The layout of II/I/III in Figure 3 may cause reading difficulties. It is recommended to improve or enhance the labeling.\n\n2.Insufficient comparative experiments under low compression rates: Comparative experiments under low compression rates are only conducted with the Activation Beacon method. It is recommended to include more methods in the tests.\n\n3.Lack of controlled experiments under the same FLOPs: Although the authors compare accuracy under the same compression rates, they do not compare the accuracy of different methods under the same FLOPs. Table 5.4 shows that COMI's FLOPs at 32×compression are higher than those of GMSA. It is recommended to supplement performance comparisons under the same FLOPs to more fairly reflect the efficiency-effectiveness trade-off."}, "questions": {"value": "1.The article compares methods under the same compression rate, but Section 5.4 shows that COMI's FLOPs at 32× compression are higher than those of GMSA. Have the authors attempted to compare the accuracy of different methods under the same FLOPs?\n\n2.In the experimental Section 5, Figure 4 only shows the performance of the Activation Beacon method and COMI at 2×, 4×, and 8× compression rates. How do other methods perform under these conditions, and does COMI still achieve the best performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4nJb0gcz2v", "forum": "OGDIXDfaN4", "replyto": "OGDIXDfaN4", "signatures": ["ICLR.cc/2026/Conference/Submission4320/Reviewer_Locn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4320/Reviewer_Locn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830644312, "cdate": 1761830644312, "tmdate": 1762917293217, "mdate": 1762917293217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}