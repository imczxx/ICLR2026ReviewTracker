{"id": "ZOaXR5CXGu", "number": 10089, "cdate": 1758160368290, "mdate": 1763570919598, "content": {"title": "LLMs Are Too Smart to Be Average: Controlling LLM Proficiency via Guided Decoding", "abstract": "Large Language Models (LLMs) are increasingly  being used to simulate human behavior in applications such as educational technology, user modeling, and human-AI interaction. However, LLMs often default to expert-level reasoning, even when prompted to simulate individuals with limited or average proficiency. This misalignment limits their ability to realistically simulate users with diverse proficiency. In this work, we propose \\textbf{Guided Decoding}, a decoding-time method for controlling the reasoning proficiency of LLMs during inference. Our approach fuses token-level logits from two sources: a reference prompt that elicits expert-level reasoning, and one or more guidance prompts that induce suboptimal reasoning patterns. By adjusting the contribution of these signals, our Guided Decoding enables fine-grained control over the model’s reasoning behavior. Experiments on multiple question answering benchmarks demonstrate that our method not only modulates accuracy, but also influences the reasoning process, enabling more faithful simulation of users across a spectrum of proficiency levels.", "tldr": "", "keywords": ["simulation", "guided decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff63d0c6ea8ac2ccb3838d902a1a2d56504c0dc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method for controlling the proficiency level of a LLM when answering questions. By providing reference and guidance prompts, the authors instruct the LLM to generate both professional and intentionally flawed responses. During decoding, they interpolate between the logits produced under these two prompting conditions to control the proficiency level of the final answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Does not require fine-tuning.\n* Model agnostic approach.\n* Can be applied in a plug and play manner."}, "weaknesses": {"value": "* The paper is difficult to read and lacks clarity in presentation.\n* It does not include a clear measurement or evaluation of reasoning proficiency levels when controlling the proficiency level by their method.\n* There is insufficient evidence to support the claimed bias in proficiency levels (only one example is shown).\n* Contains several verbal errors."}, "questions": {"value": "* How do you conclude that \"the model continues to apply high-proficiency reasoning regardless of the intended role\"? In particular, you state \"Even the D-score response converges on the correct answer of $10, revealing that the model continues to apply high-proficiency reasoning regardless of the intended role\" Please justify this conclusion.\n\n* How did you observe an \"expert default bias\"? Is this claim based on a single example, or on multiple cases? Please provide quantitative evidence (counts, proportions, or statistical tests) to support the existence of this bias.\n\n* Are the statements in the last paragraphs of Section 4.2 (the text after Eq. 8) correct? Specifically:\n  * \"This similarity-based fusion ensures that guidance prompts more aligned with the expert reasoning path are weighted more heavily\"\n  * \"A lower lvalue corresponds to higher proficiency. In particular, l= N leads to minimal influence from guidance and simulates a high-performing user.\"\n  * \"As λ increases, the model becomes increasingly susceptible to distractive cues encoded in the guidance prompts.\"\n\n\n* Do you have quantitative results showing that controlling the proficiency level of answers also affects reasoning quality? In the paper you report only final answer quality. Please report metrics that capture intermediate reasoning quality (e.g., chain of thought correctness or other reasoning diagnostics) when proficiency is varied.\n\n* On the StrategyQA benchmark, you report that with a smaller Llama variant your method outperforms alternatives, while with a larger variant few-shot prompting is consistently better. Do you have an intuition for why model size reverses the relative performance? Can you run experiments with larger versions of the other two LLMs to test whether this effect is due to model size rather than model family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8vSXCMmgmK", "forum": "ZOaXR5CXGu", "replyto": "ZOaXR5CXGu", "signatures": ["ICLR.cc/2026/Conference/Submission10089/Reviewer_oYbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10089/Reviewer_oYbY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760568972919, "cdate": 1760568972919, "tmdate": 1762921474290, "mdate": 1762921474290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"LLMs Are Too Smart to Be Average: Controlling LLM Proficiency via Guided Decoding\" proposes Guided Decoding, a novel approach to modulate the proficiency level of answers generated by large language models (LLMs). The method first uses GPT-4o to produce a reference prompt and several guidance prompts for a given question, then extracts logit vectors from a white-box LLM. Through a specially designed fusion mechanism, the model can generate responses at different proficiency levels, simulating answers from students with varying capabilities. Experiments show that Guided Decoding achieves the best or near-best scores on both SDS and PDS metrics across multiple QA tasks compared to all baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes an innovative token-level decoding framework that effectively simulates student responses at different proficiency levels. This has meaningful implications for AI-driven education and adaptive tutoring systems.\n- The paper is well-written and easy to follow. The experimental results are convincing within the presented scope and demonstrate clear improvements over prior work."}, "weaknesses": {"value": "- There are a few minor typos — for example: Line 82: missing a period. Line 262: incorrect notation for the reference prompt.\n\n- The experiments are somewhat limited. While the paper reports SDS and PDS scores and includes a basic ablation study, it lacks further empirical analysis. For instance, the paper mentions that using 3 guidance prompts yields the best performance, but no experiments on more guidance prompts to support this. Furthermore, the computation cost and inference time needs to be discussed.\n\n- Some notation and consistency issues affect readability. In Section 3.2, $L$ is introduced as the proficiency level, whereas in Section 4.2, $l$ is used with an opposite interpretation (ascending vs. descending proficiency). Section 5.3 then extends the latter usage. It would be clearer to unify the notation and interpretation throughout the paper."}, "questions": {"value": "1. The effectiveness of Guided Decoding may depend on the capability of the base LLM. For example, if the base model performs poorly on a given dataset, Guided Decoding might not be able to produce a full range of responses (e.g., A+ to F) but only within a narrower band (e.g., around C). How do you plan to address this limitation or adapt the method to weaker base models?\n\n2. In Section 3.2, Property 3 appears to be somewhat subjective. Could the authors clarify how this property could be quantitatively evaluated or verified?\n\n3. The method operates via token-level sequential modifications. However, token-level differences do not always correspond to meaningful semantic differences at the sentence or paragraph level. For example, two answers may differ only in the order of explanation (reason-first vs. answer-first) but convey equivalent understanding. How does the proposed method handle or account for this?\n\n4. In line 264, the paper states that $\\beta$ reduces the model’s inherent tendency to default toward correct answers. Could the authors explain why this adjustment is necessary, given that $\\lambda$ in Equation (9) already controls the bias toward higher proficiency responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JdELDFvSAc", "forum": "ZOaXR5CXGu", "replyto": "ZOaXR5CXGu", "signatures": ["ICLR.cc/2026/Conference/Submission10089/Reviewer_wkof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10089/Reviewer_wkof"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657482611, "cdate": 1761657482611, "tmdate": 1762921473949, "mdate": 1762921473949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the \"expert default bias\" of LLMs, where models tend to produce high-proficiency answers even when prompted to simulate average or novice users. The authors formulate the problem of \"controllable proficiency simulation\" and propose Guided Decoding, a decoding-time method that controls reasoning proficiency. The method works by fusing token-level logits from a high-proficiency \"reference prompt\" with one or more low-proficiency \"guidance prompts\" that emulate flawed reasoning. The authors introduce two metrics, Proficiency Deviation Score and Slope Deviation Score, to evaluate the alignment and consistency of simulated proficiency. Experiments across four datasets and several models show the method's ability to modulate performance more smoothly than prompting-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies and formulates a relevant and timely problem: the \"expert default bias\" of LLMs and the need for \"controllable proficiency simulation\". This is significant for applications in education, user modeling, and human-AI interaction.\n2.The introduction of the PDS and SDS metrics provides a quantitative framework for evaluating this new problem, moving beyond simple accuracy to measure global alignment and local consistency."}, "weaknesses": {"value": "1. The method's effectiveness hinges on the availability of high-quality \"guidance prompts\" that capture \"realistic reasoning flaws\". The paper suggests using GPT to generate these, but this process is not deeply explored. This reliance on generated or manually-crafted flawed examples seems difficult to scale and may introduce its own biases. The ablation study (Table 4) confirms that performance is highly dependent on the number of guidance prompts.\n2. The core evaluation metrics (PDS and SDS) are predicated on an assumption that task performance increases linearly with latent ability. This assumption, while motivated by psychometric theory, is a strong and potentially inaccurate simplification. The validity of the primary quantitative results rests heavily on this untested assumption of linearity.\n3. The core mechanism of interpolating logits from an \"expert\" and an \"amateur\" stream is a variation of existing techniques like contrastive decoding, which the paper cites. While the application to proficiency control is new, the technical novelty of the decoding strategy itself is limited."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yN5hkdOTE5", "forum": "ZOaXR5CXGu", "replyto": "ZOaXR5CXGu", "signatures": ["ICLR.cc/2026/Conference/Submission10089/Reviewer_CaA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10089/Reviewer_CaA5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980903848, "cdate": 1761980903848, "tmdate": 1762921473535, "mdate": 1762921473535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}