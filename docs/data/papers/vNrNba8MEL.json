{"id": "vNrNba8MEL", "number": 8949, "cdate": 1758103652944, "mdate": 1759897752250, "content": {"title": "Circumventing Safety Alignment in Large Language Models via Embedding Space Toxicity Attenuation", "abstract": "Large Language Models (LLMs), especially open-source LLMs, have achieved remarkable success across various critical domains. However, their open nature also inadvertently introduces significant security risks, particularly through embedding space poisoning. While previous research has investigated universal perturbation methods, the dynamics of LLM safety alignment at the embedding level remain insufficiently understood despite their potential severity. \n\nWe propose **ETTA (Embedding Transformation Toxicity Attenuation)**, a novel framework that identifies and attenuates toxicity-sensitive dimensions in embedding space via linear transformations. ETTA bypasses model refusal behaviors while preserving linguistic coherence, without requiring model fine-tuning or access to training data. Evaluated on five representative open-source LLMs, ETTA achieves a high average attack success rate of **88.61\\%**, outperforming the best baseline by **11.34\\%**, and generalizes to safety-enhanced models (e.g., 77.39\\% ASR on instruction-tuned defenses). These results highlight a critical vulnerability in current alignment strategies and the need for embedding-aware defenses.", "tldr": "", "keywords": ["Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e9da4b5d44ad19127d5ce166d76e033fe5c736c.pdf", "supplementary_material": "/attachment/a21fe8715b705d513d142b492f453b7a3814085a.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a method to bypass the safety guardrail of LLMs by manipulating the embedding space. The experiments on one harmful dataset show that the proposed method can achieve good attack performance on white-box models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is novel to my knowledge.\n\n2. The attack performance is high.\n\n3. The paper writing is good.\n\n4. Many failure cases are provided."}, "weaknesses": {"value": "1. If my understanding is correct, the proposed method is designed for white-box attack, which is a relatively easy task. Many simple but effective methods have been proposed for white-box attack, such as harmful finetuning (Fine-tuning aligned language models compromises safety, even when users do not intend to!). What is the advantage of the proposed method compared with these existing white-box attack methods?\n\n2. Manipulating the embedding space for jailbreaking has been explored in existing studies, such as the SCAV method (Uncovering Safety Risks of Large Language Models through Concept Activation Vector). The difference lies in the specific embedding space to manipulate. In SCAV, they manipulate the embedding space in hidden Transformer layers while in this paper the authors select the token embedding space. Then a natural question is, what is the advantage of manipulating the token embedding space?\n\n3. Only one harmful dataset is used for evaluation, which is not sufficient.\n\n4. It is not clear what is the quality of the harmful response generated by the proposed method, since the proposed method leads to obvious quality decline in normal tasks.\n\n5. More defense methods should be used in experiments to show that the proposed method can bypass existing defense methods.\n\n6. More attack methods for white-box scenarios should be used for comparison. In this paper, most of the methods for comparison are for black-box setting, which is not fair."}, "questions": {"value": "see my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BneMmWxJdw", "forum": "vNrNba8MEL", "replyto": "vNrNba8MEL", "signatures": ["ICLR.cc/2026/Conference/Submission8949/Reviewer_h6QN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8949/Reviewer_h6QN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296820179, "cdate": 1761296820179, "tmdate": 1762920689367, "mdate": 1762920689367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ETTA, an LLM attack framework. It can accurately identify and weaken the dimensions related to toxicity in the embedding vectors, and bring them below the model's safety detection threshold. ETTA does not require retraining the model and can deceive the model into executing malicious instructions while maintaining semantic coherence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper clearly empirically demonstrates and utilizes the linear separability of toxicity in the LLM embedding space and the threshold effect. \n- The ASR achieved by ETTA is high, realistically illustrating the potential risks of open-source LLMs.\n- The experiments in this paper are comprehensive, taking into account the scenarios of attacks (black-box and white-box)."}, "weaknesses": {"value": "- **This paper would significantly benefit from a more thorough comparison with related work**. The claim around L44 is unconvincing. Since 2023, several studies [1,2,3] have explored how internal representations affect LLM safety. While the proposed discovery and manipulation of toxicity at the embedding layer is relatively novel, the paper lacks strong empirical evidence demonstrating why attacks at the embedding level are more advantageous than those proposed by related work.\n- **The assumptions regarding the attacker’s capacity are not well grounded**. Such assumptions should reflect realistic closed-source attack scenarios. In open-source settings, there is no reason to limit the attacker’s access, whereas in closed-source models, attackers are typically constrained to prompt manipulation rather than direct embedding modification.\n- **ETTA relies on LLMs to detect semantic drift, which is cost expensive and inelegant**. While efficiency may not be the top priority for attacks, this approach introduces an unnecessary trade-off between quality and cost.\n- **Sec 4.6 presents encouraging results on black-box attacks but lacks sufficient detail to be persuasive**. It remains unclear whether the embeddings used by OpenAI models directly correspond to their published embedding models, and how the proposed method extends to other closed-source models with undisclosed embedding models.\n\n**Minor:**\n\nThe paper incorrectly applies citation formatting, where parenthetical references are incorrectly not bracketed. The authors should review the ICLR guidelines for proper citation usage.\n\n[1] Representation Engineering: A Top-Down Approach to AI Transparency, https://arxiv.org/abs/2310.01405\n\n[2] Uncovering Safety Risks of Large Language Models through Concept Activation Vector, https://arxiv.org/abs/2404.12038\n\n[3] Refusal in Language Models Is Mediated by a Single Direction, https://arxiv.org/abs/2406.11717"}, "questions": {"value": "- The gray region in Fig. 2(b) contains several cases. Could authors characterize these boundary points? Do they reflect inherently ambiguous instances, or do they share properties with clearly classified cases but are affected by suboptimal boundary extraction?\n- The threshold used in Fig. 2(b) appears loosely defined, as it depends on model-specific factors such as representation dimensionality. Could authors comment on whether this definition is meaningful and comparable across models?\n- The claim that word embedding layer exhibits linear separability seems to conflict with prior work. For instance, [2] in weakness section reports that safety-related concepts typically emerge in middle layers, whereas early layers, especially layer 0, show negligible linear separability. I invite the authors to comment on this apparent discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v0pppPhfOr", "forum": "vNrNba8MEL", "replyto": "vNrNba8MEL", "signatures": ["ICLR.cc/2026/Conference/Submission8949/Reviewer_ANm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8949/Reviewer_ANm1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485760799, "cdate": 1761485760799, "tmdate": 1762920688947, "mdate": 1762920688947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ETTA, a framework that circumvents LLM safety alignment by manipulating embedding representations. The method is based on the finding that toxicity is linearly separable in the embedding space, allowing the authors to isolate and \"attenuate\" toxicity-related features via a linear transformation. This technique, guided by a classifier LLM, effectively bypasses model refusal mechanisms while maintaining model performance on standard benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work is grounded in an empirical analysis that demonstrates a clear, linear separability between toxic and benign word embeddings, which forms the basis for the entire attack.\n- The evaluation is comprehensive, testing the attack's effectiveness against five different open-source LLMs, its impact on general capabilities, and its resilience against several existing defense mechanisms"}, "weaknesses": {"value": "- The threat model assumes a high level of access, requiring the attacker to inject malicious code into the model's embedding pipeline to manipulate tensors at runtime. This is a more demanding prerequisite than many black-box or data-poisoning scenarios.\n- The method's effectiveness and efficiency are heavily dependent on using a powerful, external classifier LLM (GPT-4o) to guide the attenuation factor search. The paper's own ablation study shows that substituting this with a smaller open-source model drastically reduces the attack success rate.\n- The paper notes that the training of the linear transformation matrix is sensitive to random initialization, leading to a 12.4% variance in Attack Success Rate across different seeds. This suggests a degree of instability in reproducing the attack's optimal performance.\n- What is the surprise part of this paper if we consider [1]?\n\n[1] Representation Engineering: A Top-Down Approach to AI Transparency"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AOfkntwQ0t", "forum": "vNrNba8MEL", "replyto": "vNrNba8MEL", "signatures": ["ICLR.cc/2026/Conference/Submission8949/Reviewer_xjhK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8949/Reviewer_xjhK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924211102, "cdate": 1761924211102, "tmdate": 1762920688174, "mdate": 1762920688174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates embedding-space poisoning as a security threat to open-source LLMs and finds that toxic and benign prompts populate linearly separable regions of the embedding space. This reveals a geometric threshold that effectively governs refusal versus compliance. Leveraging this insight, the paper introduces ETTA, an embedding-level attack that first identifies toxicity-sensitive dimensions via a learned hyperplane, and then applies targeted linear transformations to attenuate those signals. ETTA bypasses refusals without fine-tuning and without access to training data, while preserving linguistic semantics.\n\nOn AdvBench and five popular open-source LLMs, ETTA achieves an average 88.61% attack success rate (+11.34 pp over the best baseline), generalizes to safety-enhanced and randomized-smoothing defenses (77.39% and 60.15% ASR, respectively), and induces only small capability drops on TruthfulQA and MMLU.\n\nContributions are: (i) empirically revealing a geometric vulnerability in current LLM safety alignment; (ii) proposing a practical embedding transformation that exploits this vulnerability to preserve coherence while reliably bypassing refusals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work reframes jailbreaks through the lens of embedding-space geometry: jailbreak triggers occupy linearly separable regions defined by toxicity-sensitive dimensions, which can be attenuated via simple linear transformations. Conceptually, this is an elegant synthesis of classical linear separability reasoning with modern LLM safety concerns, while practically removing major limitations of prior perturbation-based methods (e.g., optimization overheads and semantic drift).\n\n2. The evaluation is thorough and well-designed: it spans five diverse open-source LLMs, employs a standard safety benchmark (AdvBench), reports concrete efficiency metrics (≈1.92 minutes per attack), and achieves state-of-the-art performance with an +11.34 percentage point ASR improvement over the best baseline. In addition, it includes stress tests against both instruction-tuned models and randomized smoothing defenses, and conducts careful utility assessments on TruthfulQA and MMLU, showing only modest degradation."}, "weaknesses": {"value": "1. The paper does not engage with several state-of-the-art attack methods [1–3]. Given that ETTA is conceptually close to lines of work such as [2–4], a direct comparison is necessary to establish incremental value. Please add head-to-head results and a discussion clarifying what ETTA contributes beyond these approaches.\n\n2. Because the study targets open-source LLMs, a straightforward “unlearning/guardrail removal” or lightweight model-editing baseline could provide a simpler pathway to bypass refusals. In addition, ETTA uses word-level replacements as part of its perturbation strategy for close models, which can induce semantic shifts and degrade relevance. Please quantify semantic drift (e.g., BERTScore/SimCSE to the original intent, factual consistency, human adequacy ratings) and report ASR without using the proposed method.\n\n3. Commercial deployments typically combine model-internal guardrails with external moderation filters (e.g., [input/output classifiers](https://platform.openai.com/docs/guides/moderation)). The current evaluation focuses on model refusal but does not test end-to-end pipelines that include moderation. Please evaluate ETTA with realistic pre-/post-generation filters enabled, and report both (a) pre-moderation pass rates and (b) post-generation block rates. If ETTA’s gains diminish under these settings, discuss how to adapt the method (or why the failure is fundamental).\n\n4. Because the proposed method bypasses guardrails via perturbations, please evaluate whether robustness/adversarial training (e.g., [5]) mitigates ETTA. Concretely, report (i) ASR against models trained with [5] under the same threat model (embedding-level perturbations), (ii) transferability of ETTA from standard to robustly trained models.\n\n\nReferences:\n\n[1] Andriushchenko et al. 2024. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks\n\n[2] Arditi et al. 2024. Refusal in Language Models Is Mediated by a Single Direction\n\n[3] Wollschlager et al. 2025. The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence\n\n[4]. Bayat et al. 205. Steering Large Language Model Activations in Sparse Spaces\n\n[5]. Yu.  et al. 2024, Advanced defense: Robust llm safeguarding via refusal feature adversarial training"}, "questions": {"value": "Regarding the SVM training, did you train an individual SVM for each LLM?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o1ALTz95vF", "forum": "vNrNba8MEL", "replyto": "vNrNba8MEL", "signatures": ["ICLR.cc/2026/Conference/Submission8949/Reviewer_tudL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8949/Reviewer_tudL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127973035, "cdate": 1762127973035, "tmdate": 1762920687631, "mdate": 1762920687631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}