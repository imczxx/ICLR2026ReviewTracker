{"id": "Ma0LTdFVeL", "number": 19413, "cdate": 1758296037997, "mdate": 1759897040240, "content": {"title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment", "abstract": "Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways that lead to consistent outcomes under exploratory sampling. To address this, we formalize self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6\\% on GSM8K), single-agent reasoning (+23.7\\% on MATH), sampling-based inference (+22.4\\% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7\\% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3\\% on GPQA, +11.6\\% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.", "tldr": "Language models learn to maintain consistent answers across diverse reasoning paths and ground arguments in peer reasoning by reinforcing their own debate consensus, driving reasoning self-improvement.", "keywords": ["language model", "reasoning", "self-consistency", "multi-agent debate", "multi-agent reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f66785396df03272c0d97985e884aaa28c7d71a.pdf", "supplementary_material": "/attachment/dfa968df06ca013ec91899fcfcc1cb05abac5a8e.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the author argues that self-consistency should be a desired property of well-aligned reasoning models, and they address this by introducing MACA, a reinforcement learning framework that post-trains models to prefer consensus-supporting trajectories using majority/minority outcomes from multi-agent debate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed MACA framework can relax the need for ground truth labels and uses majority outcomes as the learning signal."}, "weaknesses": {"value": "1. Learning from multi-agent debate trajectories is explored in prior work [1], as well as using the majority answer as the learning signal [2, 3]. In [1], the author also shows different levels of learning from the consensus-supporting and dissenting trajectories. This limits the novelty of this work.\n2. Post-training methods like GRPO naturally sharpen the distribution and improve pass@1 performance substantially. This is similar to the motivation of this work about internalizing consistency and has been proven to be very effective. The author should compare with ScPO [2] and TTRL [3] to show how many improvements are coming from multi-agent debate, or the proposed method even outperforms these baselines. \n3. The training is conducted on the base model instead of the instruction-tuned version. Comparing with instruction-tuned models is necessary, and it will be more convincing if the model is trained from instruction-tuned checkpoints.\n4. Limiting responses to 256 tokens does not make sense to me. This budget is not sufficient for tasks like MATH and GPQA. Although in the appendix, this budget increases to 512, it is still too small.\n5. The fact that \"Unsupervised majority-vote signal is comparable to ground-truth\" as shown in the ablation study is established in [2, 3], again limiting the novelty of this work. \n6. The improvements are overclaiming since it is comparing the trained performance with the base model. The improvements should be compared with external baselines such as [2] and [3].\n7. On the note of \"how many improvements are coming from multi-agent debate\", it is also important to compare with [1].\n\n\n[1] https://arxiv.org/abs/2402.01620\n[2] https://arxiv.org/abs/2411.04109\n[3] https://arxiv.org/abs/2504.16084"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BY0vAY7AQl", "forum": "Ma0LTdFVeL", "replyto": "Ma0LTdFVeL", "signatures": ["ICLR.cc/2026/Conference/Submission19413/Reviewer_H9sy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19413/Reviewer_H9sy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407433444, "cdate": 1761407433444, "tmdate": 1762931329864, "mdate": 1762931329864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MACA, a self-supervised, reinforcement learning framework for post-training LLMs to enhance self-consistency in reasoning. The MACA approach formalizes self-consistency as an intrinsic property of LLMs and utilizes multi-agent debate, where multiple clones of an LLM generate, critique, and revise reasoning trajectories. Th framework generates training signals from these deliberative exchanges and optimizes LLMs using several objectives to internalize consensus rather than simple aggregation. Substantial empirical improvements are demonstrated over baseline and strong post-training baselines on mathematical, science, and commonsense reasoning benchmarks, with gains in both accuracy and self-consistency, generalization to unseen tasks, and efficiency in inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The MACA framework is not limited to GRPO; it can also be integrated with multi-agent RL and preference-based alignment objectives, suggesting broader applicability to LLM self-alignment settings.\n\n2. The improvements in self-consistency observed on mathematical reasoning tasks transfer to science and commonsense benchmarks, indicating that the method generalizes beyond a single domain."}, "weaknesses": {"value": "1. The core learning loop is closely aligned with recent test-time reinforcement learning approaches [1], in which multiple sampled reasoning trajectories are compared and the consensus outcome is used as a self-supervised preference signal to update the model. The main distinction in this paper is that the consensus signal is generated via multi-agent debate rather than independent sampling. However, this conceptual similarity should be made more explicit, and a direct comparison to test-time RL methods is necessary to clarify what is genuinely novel in the proposed contribution.\n\n2. While the improvements over SFT baselines and previous post-training paradigms are clear, the paper would benefit from direct, quantitative comparison to more diverse non-MACA multi-agent aggregation schemes. It is unclear how much gain stems from the debate protocol itself versus just using more samples at training.\n\n[1] Zuo et al., \"TTRL: Test-time Reinforcement learning.\" 2025"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P29e9u6ljY", "forum": "Ma0LTdFVeL", "replyto": "Ma0LTdFVeL", "signatures": ["ICLR.cc/2026/Conference/Submission19413/Reviewer_HLw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19413/Reviewer_HLw1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798993197, "cdate": 1761798993197, "tmdate": 1762931329366, "mdate": 1762931329366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Language Models’ reasoning inconsistencies stemming from probabilistic sampling by introducing a reinforcement learning framework MACA. MACA utilizes multi-agent debate to self-generate preference data, designating trajectories aligned with the majority consensus as 'preferred' and dissenting trajectories as 'not preferred'. It is then optimized on these self-supervised signals, using methods like DPO, to favor consensus-forming reasoning paths. It yields improvements in self-consistency and single-agent reasoning, and demonstrates generalization to unseen domains."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper tries to address an important problem: the reasoning inconsistency of LMs.\n\n2.The MACA framework is conceptually simple.\n\n3.The paper provides a very detailed appendix."}, "weaknesses": {"value": "1.The core mechanism fails in face of “correct but minority” reasoning and actively rewards incorrect consensus.\n\n2.The evidence for the central novelty claim is weak. As shown in Table 6, gains are marginal in 5 of 8 cases when compared fairly MV(C) with DMV(C).\n\n3.The method essentially trains the model to agree more strongly with what it already agrees on, creating a self-reinforcing echo chamber that may amplify a model's inherent biases."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OUSa21WX6n", "forum": "Ma0LTdFVeL", "replyto": "Ma0LTdFVeL", "signatures": ["ICLR.cc/2026/Conference/Submission19413/Reviewer_Jz54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19413/Reviewer_Jz54"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329946465, "cdate": 1762329946465, "tmdate": 1762931328823, "mdate": 1762931328823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MACA (Multi-Agent Consensus Alignment), a post-training framework that “internalizes” self-consistency by using multi-agent debate to generate majority (consensus) and minority (dissent) trajectories, then optimizing the model with MV-DPO/MV-KTO/MV-GRPO or MV-SFT on those signals. The paper formalizes self-consistency via majority probability over sampled reasoning paths and measures agreement in multi-agent settings. Experiments on small LLMs (2B–8B) across math-heavy benchmarks report sizeable gains in sampling consistency and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explicitly defines single-agent sampling consistency and multi-agent agreement, making the target capability measurable, and uses them consistently in analysis.\n\n2. MACA reuses standard post-training objectives (DPO/KTO/GRPO/SFT) with debate-derived preference signals, and shows that MV-DPO/KTO tend to outperform SFT and scalar-reward baselines across several model/dataset pairs.\n\n3. The paper compares debate-majority supervision to ground-truth labels and finds similar performance, and also tests training with/without peer context during debate—both helpful for understanding what drives gains."}, "weaknesses": {"value": "1. Since self-consistency prompting (Wang et al., 2022) is a main comparator, I expected a training-time baseline that (i) uses self-consistency/majority vote to curate a dataset (e.g., majority-consistent rollouts), then (ii) finetunes/SFTs on that curated set—without multi-agent debate. This would test whether debate-derived signals truly add value over classical self-consistency data augmentation.\n\n2. The training/inference cost versus gains isn’t quantified (GPU hours, wall-clock, debate throughput).\n\n3. Most training is math-centric; generalization to GPQA/CSQA is interesting but still limited in breadth. Also, the “formalization of self-consistency” largely re-casts majority probability/consensus ideas already known from self-consistency and majority-vote literature; the novelty is primarily in using debate-derived preference pairs, which would be stronger if contrasted directly against the SC-curation+FT baseline noted above."}, "questions": {"value": "As in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DpVALhGgts", "forum": "Ma0LTdFVeL", "replyto": "Ma0LTdFVeL", "signatures": ["ICLR.cc/2026/Conference/Submission19413/Reviewer_d4Qb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19413/Reviewer_d4Qb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762450848730, "cdate": 1762450848730, "tmdate": 1762931328372, "mdate": 1762931328372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MACA (multi-agent consensus alignment), a reinforcement learning framework that trains LLMs to be more self-consistent. MACA uses debate-derived consensus trajectories and trains LLMs via preference learning (DPO/KTO) or other objectives (GRPO, SFT).\n\nExperiments show significant performance gains on math reasoning datasets and strong generalization to unseen reasoning domains, validating that MACA can improve self-alignment and elevate reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work is well-motivated, addressing LLMs' inconsistency when sampled multiple times. The proposed method is simple yet effective, using majority voting from multi-agent debates as a weak supervision signal to construct preference pairs for training.\n- Thorough experiments and ablations yield valuable insights, such as \"Multi-agent debate produces more informative training signals than single-round majority voting\" and \"Addressing consensus alignment through preference learning improves over GRPO and SFT\"."}, "weaknesses": {"value": "No major weaknesses. A few questions are listed below."}, "questions": {"value": "- Regarding the scaling of debate settings, will **more agents** or **more rounds** or **heterogeneous agents** yield better consensus? Would scaling these dimensions lead to higher-quality pairwise training data?\n\n- Cross-model transfer: If debate trajectories generated by a more capable LLM (e.g., Llama-8B) are used to train a smaller model (e.g., Llama-3B), how would this impact the smaller model’s **self-consistency** and accuracy?\n\n- Presentation: In Figure 2, which post-training methods (DPO, KTO, or SFT) are being illustrated? In Table 4, what does “Debate” refer to in the single-agent setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hxprYLq7QK", "forum": "Ma0LTdFVeL", "replyto": "Ma0LTdFVeL", "signatures": ["ICLR.cc/2026/Conference/Submission19413/Reviewer_eWWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19413/Reviewer_eWWi"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762460772989, "cdate": 1762460772989, "tmdate": 1762931327951, "mdate": 1762931327951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}