{"id": "AqCG1RUbyO", "number": 6351, "cdate": 1757972071517, "mdate": 1763089877526, "content": {"title": "Posterior-Guided Visual Token Pruning in Vision–Language Models", "abstract": "Vision-language models (VLMs) with dynamic resolution vision encoders achieve strong performance, but face significant efficiency challenges due to long input sequences. A common approach is to assess the importance of tokens and prune those that are less informative. Recent methods utilizing a small VLM to provide the importance map of visual tokens have outperformed existing rule-based and similarity-driven pruning approaches, particularly under high pruning ratios. However, directly using the small VLM remains unreliable, as it aggregates cross-attention weights between all the generated answer tokens of the small VLM and the visual inputs to form an importance map, which can lead to noisy guidance if the generated answer is incorrect.\nTo address this, we invert the approach by having it detect non-informative visual tokens according to the user's input query. By adding a learnable information bottleneck in the small VLM, we can approximate the posterior distribution of non-important visual tokens. This enables the small model to highlight broad informative regions, allowing the large VLM to retain its reasoning capacity with improved efficiency.\nExtensive experiments on eight benchmarks demonstrate the effectiveness of our approach. With only 5\\% of visual tokens retained, the large VLM preserves 95\\% of its original performance, outperforming the state of the art by 8\\%.", "tldr": "", "keywords": ["efficient vision-language models", "variational inference", "token pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1f97778e6d77a4719a68878a02baa6419cc744bf.pdf", "supplementary_material": "/attachment/80901fe5f0fd4313ca4837f8b680fb06ec1068bd.zip"}, "replies": [{"content": {"summary": {"value": "Vision-language models (VLMs) with dynamic-resolution encoders suffer from inefficiency due to long visual token sequences. This paper proposes Posterior-Guided Pruning (PGP), which introduces a learnable information bottleneck to model the posterior of non-informative tokens conditioned on the query. Tokens with large KL divergence from a learnable prior are retained. PGP produces reliable, query-aware pruning in a single forward pass, preserving broad task-relevant regions. On eight benchmarks, keeping only 5% of visual tokens retains 95% of performance and surpasses prior state-of-the-art by 8%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel training-based method that is different from attention-based heuristics with a principled variational information bottleneck formulation.\n\n2. The experiments demonstrate an excellent accuracy–ratio trade-offs. With only 5% of visual tokens retained, the large VLM still preserves 95% of its original performance.\n\n3. The paper is very well written and easy to read, with a clear logical flow."}, "weaknesses": {"value": "1. Generalization yet to be verified: The paper lacks experiments on different models; they only conduct experiments primarily on the InternVL family; effectiveness on other architectures (e.g., LLaVA-OV[1], InstructBLIP[2]) remains to be validated.\n\n2. Limited applicability: Assumes the availability of uncompressed visual token sequences, such as QwenVL, and a smaller model of the same architecture; not directly applicable to VLMs with built-in token merging or without a matching small version.\n\n3. Baseline selection is not accurate: The comparison with existing methods is not entirely fair or accurate, as some baselines are not aligned in settings or optimization conditions. For example, the baselines are all training-free methods, which are substantially different from the training setting in this paper. Therefore, more methods should be compared, such as PDrop[3], M3[4], FastVLM[5], and so on.\n\n\n[1] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv, 2024a.\n\n[2] Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. ArXiv, 2023a.\n\n[3] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. CVPR, 2025.\n\n[4] Cai, Mu and Yang, Jianwei and Gao, Jianfeng and Lee, Yong Jae. M3: Matryoshka Multimodal Models. ICLR, 2025.\n\n[5] Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari. FastVLM: Efficient Vision Encoding for Vision Language Models. CVPR, 2025."}, "questions": {"value": "1. Although the paper shows a decrease in TFLOPs, the real runtime benefit is not evident. Could the authors report the actual CUDA time or wall-clock inference latency, considering the extra small-model pass? How much practical speedup is observed?\n\n2. Attention can be noisy due to sink/anchor tokens. I’m curious whether your KL-based posterior scores are more robust than attention-based saliency, or whether similar noise could still leak in via the small VLM. Do you have analyses (e.g., perturbation or correlation tests) to illustrate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZdiT975iVq", "forum": "AqCG1RUbyO", "replyto": "AqCG1RUbyO", "signatures": ["ICLR.cc/2026/Conference/Submission6351/Reviewer_HTDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6351/Reviewer_HTDS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473343214, "cdate": 1761473343214, "tmdate": 1762918642464, "mdate": 1762918642464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you to all the reviewers for your constructive feedback and the time you dedicated to evaluating our submission. After careful consideration, we have decided to formally withdraw the paper. The reviewers’ comments have been highly valuable, and we will use them to substantially strengthen the manuscript for future submission."}}, "id": "mZ0G0UyCqV", "forum": "AqCG1RUbyO", "replyto": "AqCG1RUbyO", "signatures": ["ICLR.cc/2026/Conference/Submission6351/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6351/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763089876762, "cdate": 1763089876762, "tmdate": 1763089876762, "mdate": 1763089876762, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient visual token pruning method for vision–language models (VLMs). By introducing a learnable information bottleneck in a small VLM, the model identifies non-important visual tokens rather than directly selecting important ones, producing more reliable visual masks. This approach enables the large VLM to retain most of its reasoning capacity while using only a small fraction of tokens. Experiments on eight benchmarks show significant improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is coherent and well-motivated, effectively demonstrating that identifying non-informative visual tokens via a small VLM can guide efficient pruning. Based on this insight, the authors propose a query-guided, bottleneck-based approach that preserves reasoning performance while substantially reducing the number of visual tokens.\n\n2. The experiments are comprehensive, with the proposed algorithm being validated across multiple benchmarks, demonstrating its feasibility."}, "weaknesses": {"value": "1. I am somewhat skeptical about the practicality of relying on a small VLM in the efficiency domain. The paper only reports the FLOPs reduction of the main model itself, but I would like to see whether the overall system—**including the small model**—**actually achieves a reduction in latency**.\n\n2. As noted by the authors in the Limitations section, PGP cannot currently be adapted to QwenVL, which is a drawback. However, I suggest that the authors try adapting it to models such as LLaVA-Next or LLaVA-OneVision to demonstrate the compatibility of PGP.\n\n3. It would be helpful to include comparisons with methods such as VisionZip and SparseVLM in the table 1."}, "questions": {"value": "1. The comparison in Table 1 seems unfair, as the FastV paper itself recommends pruning in the first two layers. Why, then, is FastV evaluated with pruning at the 9th layer in this comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0N2680goE6", "forum": "AqCG1RUbyO", "replyto": "AqCG1RUbyO", "signatures": ["ICLR.cc/2026/Conference/Submission6351/Reviewer_fZ2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6351/Reviewer_fZ2F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551400553, "cdate": 1761551400553, "tmdate": 1762918641692, "mdate": 1762918641692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PGP, a posterior-guided visual token pruning framework for vision-language models. Instead of relying on attention or answer-driven heuristics, the method trains a small VLM with a variational information bottleneck to estimate non-informative visual tokens via KL divergence. High-KL tokens are retained for the large model. \n\nExperiments across eight benchmarks show that retaining only 5% of tokens preserves ~95% accuracy, outperforming prior work. The method is efficient, architecture-compatible, and robust on complex queries."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well motivated paper: from attention-driven to posterior-driven pruning.\n\n2. Good empirical performance, especially at extreme sparsity.\n\n3. Works with existing architectures and optimized attention kernels."}, "weaknesses": {"value": "1. Primarily evaluated on InternVL; transfer to other VLMs (e.g., Qwen) unclear.\n\n\n2. Requires training a small VLM module -- not plug-and-play (existing literature).\n\n\n3.Baseline Coverage is limited."}, "questions": {"value": "1. Generalization Beyond InternVL: applying PGP to models like Qwen-VL or LLaVA-OneVision that compress tokens earlier?\n\n2. What is the additional compute required to train the small VLM bottleneck module? How does this compare to plug-and-play pruning methods in real deployment scenarios?\n\n3. How sensitive is the method to the choice of learnable prior? \n\n4. Several competitive recent pruning methods such as SparseVLM, PyramidDrop, VisionZip, and VScan are not included in your comparison. Could you include these baselines or explain why they were excluded?\n\n5. Can you share examples where PGP fails or under-performs? For example, tasks requiring very localized information or OCR-dense scenes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uHMB4FUtbA", "forum": "AqCG1RUbyO", "replyto": "AqCG1RUbyO", "signatures": ["ICLR.cc/2026/Conference/Submission6351/Reviewer_kXKx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6351/Reviewer_kXKx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208590397, "cdate": 1762208590397, "tmdate": 1762918641319, "mdate": 1762918641319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work concentrates on the inference acceleration of vLLMs, especially when the input sequence is too long. In particular, basd on existing works of small VLM oriented token pruning, this work detects non-informative visual tokens according to the user’s input query.  By adding a learnable information bottleneck in the small VLM, the method can approximate the posterior distribution of non-important visual tokens, which enables the small model to highlight broad informative regions. Experiments are done on several benchmarks and the results show good performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths are as follows:  \n1.Good technical view.  This work formulates token importance estimation as amortized variational inference, providing a sound theoretical basis for pruning guidance.  \n2.Experiments are done on eight data benchmarks. The results in Sec 4 show promising performance and emperical analysis helps method understanding."}, "weaknesses": {"value": "The weakness are listed as follows:  \n1.Complexity of the probabilistic model. Since the amortized variational inference formulation may increase model complexity, what is the complexity burden of obtaining this pruning policy? Will the complexity is large than the saved complexity of pruning?  \n2.Dependence on small-VLM training quality. The proposed PGP is based on small VLMs. So it is important to evaluate the model performance on different small VLM models.  \n3.Missing related works. There are some othe important works[1,2,3,4] are missing. These works should also be compared and discussed.  \n4.The experiments are done on specific InternVL model. It is important to verify the method on different architectures such as Qwen, LLaVa.  \n5.Interpretability of KL-divergence based pruning decisions. Although  KL-divergence-based scoring is principled, there is limited qualitative analysis on what visual patterns are pruned or retained.  \n\n[1] Boosting multimodal large language models with visual tokens withdrawal for rapid inference  \n[2] Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification.  \n[3] Visionzip: Longer is better but not necessary in vision language models  \n[4] Folder: Accelerating multi-modal large language models with enhanced performance"}, "questions": {"value": "Se above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xZ6VCmSSf5", "forum": "AqCG1RUbyO", "replyto": "AqCG1RUbyO", "signatures": ["ICLR.cc/2026/Conference/Submission6351/Reviewer_fH3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6351/Reviewer_fH3E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762418388546, "cdate": 1762418388546, "tmdate": 1762918640736, "mdate": 1762918640736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}