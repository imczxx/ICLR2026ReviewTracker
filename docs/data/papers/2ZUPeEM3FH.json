{"id": "2ZUPeEM3FH", "number": 10807, "cdate": 1758182301917, "mdate": 1759897627526, "content": {"title": "Towards Prompt-Robust Machine-Generated Text Detection", "abstract": "Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their robustness in settings where prompts used to generate text are unobserved. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. We conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves average improvements of 45.3% to 62.5% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini), with gains reaching up to 100% in some cases.", "tldr": "", "keywords": ["LLM detection", "Rewrite-based detection", "Learning distance", "Prompt robust"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48da8286f4dced35acf0a2274ffbaf8453776a92.pdf", "supplementary_material": "/attachment/b088b867f6cb3f15a87a50f826047584328a004d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new rewrite-based detection algorithm for the detection problem of LLM generated text. The author first analyzes the principle and robustness of rewriting detection algorithms from a geometric perspective, and then proposes a new method for adaptive learning text rewriting distance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear theoretical insight:\nThe geometric analysis (Propositions 1 & 2) provides an explanation for why rewrite-based approaches remain robust in unseen prompt scenarios.\n\n2. Extensive experimentation:\nThe experiments are comprehensive — 27 settings across 3 datasets, 3 prompt types, and multiple target models (Claude-3.5, GPT-4o, Gemini-2.5) — showing consistent improvements.\n\n3. Adversarial robustness:\nThe evaluation under paraphrasing and decoherence attacks demonstrates the proposed method’s strong robustness."}, "weaknesses": {"value": "1. Questionable novelty of “Prompt-robust detection”:\nThe notion of prompt-robustness largely overlaps with prior work on generalization detection, where detectors are expected to perform under unknown prompts or semantic variations. Many existing zero-shot methods (e.g., DetectGPT, Fast-DetectGPT) are already prompt-agnostic, so it’s unclear whether the proposed notion is truly new.\n\n2. Fairness of comparison (Figure 2):\nThe proposed method requires training, whereas Fast-DetectGPT is a zero-shot detector. Comparing them directly may be unfair since the proposed approach has access to additional supervision and compute.\n\n3. Lack of implementation details:\nThe paper is vague about key training aspects, such as the architecture of the distance module, learning rate, optimization steps, dataset scale, or fine-tuning hyperparameters. This limits reproducibility.\n\n4. High computational cost:\nThe method involves multiple LLM queries — one for rewriting and another for distance computation — which can be prohibitively expensive in large-scale or real-time scenarios.\n\n5. Misleading improvement reporting:\nThe “relative gain” metric (e.g., “average improvements of 45.3%–62.5%”) can exaggerate the results. For example, when AUROC improves from 0.951 to 0.987, the absolute gain is minor (~0.036), yet the table reports a 72.9% “relative gain.” Reporting absolute AUC improvements would be more transparent and standard."}, "questions": {"value": "1. What are the detailed training settings (loss function, optimizer, learning rate, batch size, training steps)?\n\n2. Does Figure 2 involve an unfair comparison between a trained detector and zero-shot baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u8Jhd3n4Qu", "forum": "2ZUPeEM3FH", "replyto": "2ZUPeEM3FH", "signatures": ["ICLR.cc/2026/Conference/Submission10807/Reviewer_J3u5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10807/Reviewer_J3u5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579943079, "cdate": 1761579943079, "tmdate": 1762922017046, "mdate": 1762922017046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt-robust detection framework for LLM-generated text based on a geometric understanding of rewrite-based methods. The authors show that human-written texts exhibit larger reconstruction errors than LLM-generated ones, and that this difference remains stable even under unseen prompts. Building on this insight, they introduce a method that learns an adaptive distance function between an input text and its rewritten version using a fine-tuned language model, rather than relying on fixed distances such as BLEU or BERTScore. Experiments across many datasets and several LLMs demonstrate improvement over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow\n2. Important and timely topic\n3. prompt robustness is very important for these trained llm-text classifiers."}, "weaknesses": {"value": "1. My major concern lies in the geometric assumptions underpinning the theory, which are elegant but often unrealistic in practice. The framework assumes that LLM-generated text is a linear projection of human-written text onto an “LLM subspace” (Assumption 2) and that rewriting behaves equivalently on human and LLM-like inputs (Assumption 3). However, real-world text generation is highly nonlinear and context-dependent, and rewriting can amplify stylistic or semantic differences depending on prompts, temperature, and decoding randomness. The additive noise model  R(x) = \\Pi_M(x) + e  also oversimplifies rewriting dynamics, as  e may not be small or confined to the same subspace. Consequently, the theoretical claims (Propositions 1–2) serve more as conceptual heuristics than as formal guarantees of robustness.\n\n2. From a practical standpoint, the proposed approach faces several deployment challenges. First, it requires direct access to the target LLM for rewriting, which may not always be feasible in real-world . This dependency makes the method less suitable for general-purpose detection across unknown or evolving models. Second, the approach involves fine-tuning a distance model to adapt to each target LLM’s text distribution. This fine-tuning step also risks overfitting when the available training data are limited in domain diversity or prompt variety, potentially reducing generalization to unseen contexts. Finally, the paper offers little analysis on scalability and runtime efficiency, especially for long documents or large-scale batch detection scenarios where repeated rewriting and distance computation could become prohibitively expensive. These constraints limit the method’s immediate practicality despite its strong empirical performance.\n\n3. In my understanding, your method requires fine-tuning the detector for each target LLM. Given this, is it fair to directly compare your approach with previous rewrite-based or other detection methods that operate in a zero-shot or non–fine-tuned setting? How do you ensure that the comparison across methods remains fair and consistent?\n\nIn general, I would consider this a borderline paper."}, "questions": {"value": "1. The theoretical framework relies on strong geometric assumptions (e.g., LLM text as a projection of human text). Have you empirically verified or tested how well these assumptions hold in practice?\n\n2. How well does the learned distance function transfer to new or unseen LLMs without re–fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H1K3e3bGA9", "forum": "2ZUPeEM3FH", "replyto": "2ZUPeEM3FH", "signatures": ["ICLR.cc/2026/Conference/Submission10807/Reviewer_pN4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10807/Reviewer_pN4E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700326127, "cdate": 1761700326127, "tmdate": 1762922016569, "mdate": 1762922016569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the prompt-robust detection of LLM-generated text, a realistic and challenging setting where the prompts used to produce the text are unobserved. The authors propose a geometric interpretation of rewrite-based detection methods, proving why reconstruction errors differ between human and machine text (Proposition 1) and remain robust to prompt-induced distribution shifts (Proposition 2). Building on this, they introduce a machine-learning-based rewrite detector that learns a distance function via fine-tuning a language model, rather than relying on fixed metrics like BERTScore or Levenshtein distance. Extensive experiments across 24 datasets, 7 LLMs, and 3 prompt types show relative AUC improvements of 45–62% over the strongest baselines, with strong resistance to adversarial paraphrasing and decoherence attacks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Introduces a clear geometric interpretation of rewrite-based detectors, offering theoretical insight that previous empirical works lacked.\n- The learned-distance formulation bridges theory and implementation elegantly, and the optimization is compatible with LoRA-style fine-tuning for scalability.\n- Evaluated on > 100 settings (24 datasets, 7 LLMs, 3 prompt types, 2 attacks) with consistent superiority over 11 baselines.\n- Addresses the realistic \"unseen-prompt\" condition that undermines most prior detectors, and demonstrates resilience under paraphrasing/decoherence."}, "weaknesses": {"value": "- The Hilbert-space and projection assumptions (Assumptions 1–3) are strong, but empirical verification of these geometric hypotheses is limited.\n- The authors mention small declines on certain datasets but do not analyze why the learned distance struggles there.\n- Because the detector is trained using LLM-generated corpora, it may implicitly learn stylistic or semantic regularities specific to those generation distributions. Discussing how well the method generalizes beyond the seven tested generators (e.g., to unseen LLM families) would strengthen the paper.\n- Fine-tuning a surrogate model to learn the distance is non-trivial; runtime and parameter-efficiency trade-offs are not quantified."}, "questions": {"value": "Although this is a well-motivated paper that advances in prompt-robust LLM detection, I still have a few questions that I would truly appreciate if the authors could kindly address during the rebuttal:\n- The experiments convincingly demonstrate robustness to paraphrasing and decoherence attacks. Could the authors kindly comment on how the method might behave under stronger semantic-preserving perturbations (e.g., back-translation, style transfer), or whether they foresee any limitations under such conditions?\n- While the average AUC gains are impressive, the tables do not include variance or confidence intervals. Would it be possible for the authors to share whether these results were averaged over multiple random seeds, and how stable the improvements are across runs?\n- Finally, since the approach requires multiple rewritings and fine-tuning, could the authors give a sense of the computational cost? For example, how much GPU time is typically needed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LlYV2XTOCa", "forum": "2ZUPeEM3FH", "replyto": "2ZUPeEM3FH", "signatures": ["ICLR.cc/2026/Conference/Submission10807/Reviewer_oNqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10807/Reviewer_oNqV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816456546, "cdate": 1761816456546, "tmdate": 1762922016215, "mdate": 1762922016215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-tuning approach for pre-trained LLMs that aims to maximize the divergence in reconstruction errors between human-rewritten and LLM-rewritten text. The method is grounded in the assumption that LLM-generated texts form a subset of human-written texts, and that this relationship persists in their semantic projections—thus justifying the adopted distance-learning strategy. Evaluated through extensive experiments, the method is shown to outperform existing baselines. The primary contribution of this work is a novel perspective on modeling the divergence between LLM-generated and human-written text, coupled with comprehensive empirical validation of the method's effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a well-structured categorization of existing methods for detecting human-written and LLM-generated text, help building a clear roadmap and contextualizing its own research direction.\n\n2. It introduces a novel perspective centered on maximizing the discrepancy between human-written and machine-generated text, which is supported by well-motivated theoretical assumptions.\n\n3. The method's effectiveness is validated through extensive experiments, which include comparisons against a wide range of baselines across multiple datasets."}, "weaknesses": {"value": "1. The paper mentions that \"adaptively learn a distance function\" to enhance detection performance, but the implementation of this adaptive learning process is not clearly described. \n\n2. The pre-trained LLMs used in the experiments are not explicitly identified, making it difficult to assess the experimental setup with confidence. Given the importance of model selection in evaluating the contribution of the work, such implementation details should be unambiguously stated.\n\n3. There is an apparent discrepancy between the reported \"relative gain\" (e.g., claims of 100% improvement) and the modest absolute improvements observed in the AUC scores. Although the calculation method is provided in the appendix, the presentation in the main text can be misleading. Highlighting large relative gains derived from small absolute baselines may overstate the practical advancement and requires clearer contextualization to avoid confusion."}, "questions": {"value": "My questions and comments have been stated in the detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oKAiCYLTap", "forum": "2ZUPeEM3FH", "replyto": "2ZUPeEM3FH", "signatures": ["ICLR.cc/2026/Conference/Submission10807/Reviewer_7oJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10807/Reviewer_7oJT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895182965, "cdate": 1761895182965, "tmdate": 1762922015768, "mdate": 1762922015768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear reviewers,\n\nWe sincerely appreciate all reviewers' thoughtful comments. In response to these comments, we have conducted various experiments whose results are detailed in **Table A** to **Table K**, and revised our theories under less restrictive assumptions.\n\nWe noticed that similar comments were raised by different reviewers. For your convenience and easy navigation, we have bolded all key information (e.g., **Table B**, **Response C3**). You may use your browser's search function to quickly locate them.\n\nWe hope our responses address your concerns. Please feel free to ask any further questions.\n\nBest regards,"}}, "id": "aMD4DxXpSe", "forum": "2ZUPeEM3FH", "replyto": "2ZUPeEM3FH", "signatures": ["ICLR.cc/2026/Conference/Submission10807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10807/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission10807/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763734834095, "cdate": 1763734834095, "tmdate": 1763734834095, "mdate": 1763734834095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}