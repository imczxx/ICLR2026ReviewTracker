{"id": "OaWiP9VTgO", "number": 6357, "cdate": 1757972843770, "mdate": 1763748850831, "content": {"title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression", "abstract": "Neural network compression techniques typically require expensive fine-tuning or search procedures, rendering them impractical on commodity hardware. Inspired by recent LLM compression research, we present a general activation-aware factorization framework that can be applied to a broad range of layers. Moreover, we introduce a scalable budgeted rank allocator that allows flexible control over compression targets (e.g., retaining 50\\% of parameters) with no overhead. Together, these components form BALF, an efficient pipeline for compressing models without fine-tuning. We demonstrate its effectiveness across multiple scales and architectures, from ResNet-20 on CIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it achieves excellent results in the fine-tuning-free regime. For instance, BALF reduces FLOPs on ResNeXt-101 by 45\\% with only a 1-percentage-point top-1 accuracy drop.", "tldr": "We propose a principled fine-tuning-free low-rank factorization framework that works on a broad class of architectures.", "keywords": ["model compression", "factorization", "decomposition", "activation-aware", "SVD", "low-rank"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eef8b3b9ce4785466dd7b7de7706be28f2dcd49a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BALF, a fine-tuning-free compression pipeline that unifies activation-aware low-rank factorization across fully connected, convolutional, and grouped-convolution layers, and couples it with a lightweight budgeted rank allocator based on Lagrangian relaxation that directly targets user-specified FLOPs or parameter budgets. The method estimates uncentered-whitening transforms from a small calibration set, performs truncated SVD in the whitened domain to minimize expected layer-output distortion, and then selects per-layer ranks globally to satisfy a compute/size constraint, replacing layers with efficient two-stage low-rank modules only when they yield real savings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- General, principled activation-aware factorization applicable to grouped convolutions, not just FC layers; the method uses whitening to minimize expected layer-output distortion.\n- Budget-aware rank allocation: sets layer-wise ranks to meet FLOPs or parameter budgets via a Lagrangian relaxation.\n- Broad empirical coverage across CNNs and ViTs, with competitive fine-tuning-free trade-offs."}, "weaknesses": {"value": "- Innovation scope: activation-aware factorization is not entirely new; the paper’s novelty lies in generalizing it to conv/grouped-conv and coupling it with an efficient budgeted allocator. This is a strong systems contribution but less of a theoretical breakthrough.\n- The model-level bound (Theorem 3 in the main text; formal version in the appendix) is derived for a sequential L-layer network and does not explicitly handle residual/skip connections typical of ResNets. While the empirical section includes ResNet-family models, the paper does not clarify how residual blocks are treated in the theoretical bound (e.g., block-level aggregation or additional terms due to additive skips) nor in the practical error accounting within a residual structure."}, "questions": {"value": "- Can BALF be composed with post-training quantization?\n- When applying BALF to ResNet families, do you fuse BN into Conv before factorization, or do you factorize Conv and then correct BN statistics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d69zW1ku6L", "forum": "OaWiP9VTgO", "replyto": "OaWiP9VTgO", "signatures": ["ICLR.cc/2026/Conference/Submission6357/Reviewer_LWbM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6357/Reviewer_LWbM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395976694, "cdate": 1761395976694, "tmdate": 1762918647653, "mdate": 1762918647653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BALF, an efficient pipeline for compressing models using low-rank matrix decomposition without fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "+ The motivation of this paper is clear, i.e., to select the optimal ranks using SVD to factorize the matrices of the target DNN models.\n+ This paper provides sufficient theoretical analysis for the proposed method.\n+ The proposed method shows effectiveness on several CNN models in the experiments."}, "weaknesses": {"value": "- Limited novelty. The idea of compressing neural networks through matrix factorization is long-established, and many prior works have investigated rank selection strategies. In addition, the use of augmented Lagrangian formulations for enforcing low-rank constraints in network compression has already been explored [R1]. The paper does not seem to introduce a fundamentally new principle or formulation.\n\n- Lack of large-scale validation. Although the motivation centers on compressing large models, no experiments are conducted on modern large-scale architectures such as LLMs. Instead, the evaluation focuses on relatively small and dated models (e.g., ResNet, ResNeXt, ViT). Given the large body of existing work applying matrix factorization to such models, the contribution appears incremental.\n\n- Missing comparison with alternative compression methods. The paper does not compare its method against other major compression approaches, such as pruning or quantization, making it difficult to assess the relative performance and practicality of the proposed technique.\n\n- Insufficient baselines in experiments. The set of comparison methods in the experimental section is very limited, and several state-of-the-art approaches, e.g., [R1] mentioned in the related work section, are not included in the empirical evaluation. This weakens the paper’s claim of superiority or generality.\n\n- Unvalidated theoretical claims. Although the paper presents several theoretical results, the lack of strong experimental evidence and comprehensive comparisons makes it hard to verify the practical value of these theoretical contributions.\n\n[R1] Low-rank compression of neural nets: Learning the rank of each layer."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0EYVW2W5en", "forum": "OaWiP9VTgO", "replyto": "OaWiP9VTgO", "signatures": ["ICLR.cc/2026/Conference/Submission6357/Reviewer_Zr6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6357/Reviewer_Zr6M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883374700, "cdate": 1761883374700, "tmdate": 1762918647133, "mdate": 1762918647133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests a unified and efficient fine-tuning-free compression framework for deep neural networks that combines activation-aware low-rank factorization with a budgeted rank allocation mechanism.\n\nTraditional low-rank factorization techniques reduce model size by truncating singular values of layer weight matrices, but they often require fine-tuning or heuristic search to recover accuracy. BALF addresses this by (1) extending activation-aware decomposition to general layer types (including grouped convolutions) and (2) introducing a scalable, zero-overhead budgeted allocator that meets user-specified parameter or FLOPs constraints without retraining.\n\nThe method computes uncentered whitening matrices of layer activations to perform activation aware SVD, minimizing output distortion rather than parameter distortion. A Lagrangian relaxation formulation then determines per-layer rank allocations to meet global resource budgets efficiently.\n\nEmpirical results are also provided, outperforming existing fine-tuning-free baselines in both accuracy and runtime. On an RTX 2070, compression of ImageNet-scale models completes in minutes without fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n\nConceptual innovation - \nBALF reframes low-rank compression through an activation-centric lens, ensuring that the projection minimizes functional output distortion rather than raw parameter deviation. This distinction aligns compression with representational behavior.\n\nGeneral framework-The authors generalize activation-aware SVD to a more general expressible layer, encompassing dense, convolutional, and grouped convolutional layers in a unified algebraic formulation.\n\nMathematical rigor: Theorems 1 and 2 show equivalence between activation aware and direct output truncation schemes, with a closed-form expression for activation distortion in terms of singular values.\n\nI especially likes the rank allocator that transforms a combinatorial multiple choice knapsack problem into a linear-time Lagrangian relaxation, which enables control over global compression budgets.\n\nEmpirical performance: The improvement is consistent compared to standard SVD baselines, maintaining high accuracy at large compression ratios.\n\nReproducibility and presentation:\nImplementation details, pseudocode, and open-source repository are provided, which enable accessibility for less theory researchers."}, "weaknesses": {"value": "- The proof assumes ungrouped layers and bounded Lipschitz constants without empirical validation.\n- The theoretical bounds may be too loose for larger networks.\n- The method is not so strong on already-optimized architectures (e.g., MobileNet-V2). The adjustment might be adapted for each architecture.\nThe description of FLOP estimation and calibration sampling could be expanded for full reproducibility."}, "questions": {"value": "The theoretical bounds depend on Lipschitz constants. Can't they be too high in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u1vaErb6ty", "forum": "OaWiP9VTgO", "replyto": "OaWiP9VTgO", "signatures": ["ICLR.cc/2026/Conference/Submission6357/Reviewer_4J3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6357/Reviewer_4J3Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980162176, "cdate": 1761980162176, "tmdate": 1762918646503, "mdate": 1762918646503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We have uploaded a revised manuscript"}, "comment": {"value": "We are pleased to communicate that we have uploaded a revised version of our manuscript incorporating solutions to the reviewers' concerns. Below, we specify the changes made. In general, these changes reflect our existing responses to each reviewer's comments and questions.\n\n**On Reviewer 4J3Z's comments.** In Appendix D, we have added clarifications regarding the source of the bound looseness and provided additional discussion of the grouped case. We have also included a short paragraph in Appendix E.3 detailing how we count FLOPs and parameters. \n\n**On Reviewer LWbM's comments.** In the contributions summary in Section 1, we now explicitly mention that we rework the theory to handle possibly redundant data. In Section 4.4, we have added a short discussion emphasizing the layer-level nature of our framework and its easy composition with other compression methods. In Appendix E.3, we explicitly address the question of BN + Conv fusion. In Appendix D, we have included a brief discussion of how to extend the bounds to residual layers, consistent with our response.\n\n**On Reviewer Zr6M's comments.** We believe these points were already addressed in the previous version of the paper (as mentioned in our rebuttal response), but we are happy to follow up if the reviewer feels otherwise.\n\n**Additional updates.** We have updated our notion of whitening matrices (as defined in Section 3) to include an additional condition (which holds in practice and does not change any theoretical result) that is required for a property used in our optimality proof to hold. We emphasize that the implications of said change are minimal; our practical setting still matches the new notion of whitening matrices and the theoretical results remain unchanged. We also corrected the tables and figures regarding the results on the ViT architecture, which previously contained an error affecting FLOP counts (we have recomputed the FLOP counts for these models; the accuracy and parameter counts remain unchanged). This error is negligible (less than a 2 percentage-point change in FLOP ratios) and is not even perceptible in the figures.\n\nFinally, we made several secondary clarification edits where needed, and numerous style changes that improve presentation but do not affect the paper's content. We want to note that, due to the style changes, the appendix section letters have shifted upwards. (E.g., the old Appendix B is now Appendix A, and so on.)\n\n**Remarks.** Overall, we addressed all reviewers' concerns with targeted clarifications, added discussions, and improved the presentation of our paper. No core claims or results were changed.\n\nAgain, we are very grateful for the time and effort of the reviewers, which has led to actual improvements in the clarity of our paper. We are still eager to answer any subsequent question or concern that might arise."}}, "id": "hlv5kd1mwT", "forum": "OaWiP9VTgO", "replyto": "OaWiP9VTgO", "signatures": ["ICLR.cc/2026/Conference/Submission6357/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6357/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission6357/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763751579429, "cdate": 1763751579429, "tmdate": 1763751579429, "mdate": 1763751579429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}