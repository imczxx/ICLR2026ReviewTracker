{"id": "MQVnitUWAb", "number": 14701, "cdate": 1758242020531, "mdate": 1759897354149, "content": {"title": "TeLLMate: Trusted Execution for Large Language Models at the Edge", "abstract": "Large Language Models (LLMs) are being increasingly deployed on edge devices to support real-time inference, preserve user privacy, and enhance energy efficiency. However, on-device deployment creates a lucrative attack surface for adversaries to steal the model. \nTrusted Execution Environments (TEEs) provide hardware-isolated enclaves to safeguard model parameters. However, existing approaches either demand costly retraining from substantial architectural modifications or incur significant communication overhead by protecting parameters across all layers within the TEE. To address these limitations, we propose TeLLMate, which helps identify the critical layers of LLMs for TEE protection. TeLLMate delivers two key capabilities: (1) a methodology for identifying layers critical to model performance using mathematical analysis. (2) a selective protection algorithm that minimizes the secure memory footprint while defending against retraining attacks. Our experimental results demonstrate that TeLLMate offers strong protection guarantees, resulting in at least a 15x increase in perplexity and close to random-guess accuracy in zero-shot downstream tasks for the attacker’s replicated model by including at most 10% of the LLM's parameters.", "tldr": "A secure and efficient framework that identifies and selectively protects the most critical layers of an LLM within a TEE.", "keywords": ["Large Language Model", "Security and Privacy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e0b5a4a26f8c4ba397ec4eca6def589db60b8f1.pdf", "supplementary_material": "/attachment/35bfe88e7564e7962c33fb53daead64d648492f8.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents TeLLMate, a TEE-based protection scheme that identifies and encloses only the critical layers of an LLM inside the enclave to resist retraining-based model-stealing attacks. Concretely, TeLLMate analyzes outlier activations to locate critical layers and then runs an iterative auto layer-selection algorithm (with cosine-similarity guidance) to choose the minimal consecutive subset of layers to protect, reducing secure-memory and TEE/REE communication costs. Across LLaMA-2/3, Mistral-7B, and Qwen-3-8B, protecting small amount of parameters forces the attacker’s retrained surrogate to ≥15× higher perplexity and near random-guess zero-shot accuracy, and the defense remains robust even if the attacker initializes from public weights."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-The paper addresses an important and timely problem of defending LLMs against retraining-based model stealing under TEE constraints.\n-The idea of identifying and protecting critical layers based on outlier activations is novel and intuitive.  \n-Experiments across multiple LLM architectures demonstrate strong defense with minimal parameter protection \n-The writing is clear and the motivation and methodology are easy to follow."}, "weaknesses": {"value": "Overall, my main concern lies in the incomplete and insufficient experimental evaluation, which fails to substantiate the authors’ central claims about efficiency and practicality.\n\n-The experimental evaluation lacks discussion or measurement of the TEE↔REE communication overhead, even though the method is proposed to mitigate this issue.  Moreover, the experiments only focus on protection effectiveness (e.g., perplexity, accuracy) but ignore the runtime or resource implications critical to the paper’s motivation.\n\n-No comparison is provided against existing TEE-based defenses such as TEESlice, making it unclear whether the claimed efficiency improvement actually holds.  \n\n-The analysis is shallow—there are no ablations on factors such as the number or position of protected layers, which could provide deeper understanding of the mechanism."}, "questions": {"value": "The paper’s main motivation is to mitigate the communication overhead between the TEE and REE compared with prior systems such as TEESlice. However, the experiments mainly focus on protection effectiveness, without reporting any results related to latency, runtime, or communication cost. Could you provide additional experiments or analysis to support your claim of improved efficiency? Moreover, have you compared your method with baseline approaches like TEESlice in terms of both protection strength and efficiency? Such results would be crucial to verify that the proposed layer-wise protection indeed achieves better practicality rather than only stronger defense."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j1T3AwG1sj", "forum": "MQVnitUWAb", "replyto": "MQVnitUWAb", "signatures": ["ICLR.cc/2026/Conference/Submission14701/Reviewer_t5h2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14701/Reviewer_t5h2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760775601096, "cdate": 1760775601096, "tmdate": 1762925067072, "mdate": 1762925067072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Trusted execution is the setup where on-device neural networks are partly executed in a slower but secure environment (TEE), which prevents leakage of sensitive data. This paper studies the placement of LLM layers between TEE environments and non-TEE environments to find the pareto optimal between resistance to model stealing attacks and latency.\n\nUsing methods from outlier activation detection, the paper proposes an algorithm to select the most critical layers, along with its neighbours to reduce the risk of model extraction. With a fixed attack budget the authors show that TeLLMate models are quite robust to model extraction, with 10-30x increases in perplexity."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on an important and timely topic: protecting LLMs deployed in edge devices from model theft.\n\n2. The paper is generally well written and easy to follow. I appreciate the presence of motivational experiments in Section 4/5 before the main experimental section to help justify design choices.\n\n3. The paper has an interesting technical connection between outlier activations and layer importance for model extraction attacks. Under some assumptions (more in weaknesses), the paper does good ablations to validate the choice of protected layers."}, "weaknesses": {"value": "1. **Very large perplexity numbers may point to a bug**: Some of the perplexity numbers reported are quite absurd, and may point towards extremely unstable training more than a model extraction failures. N-gram LLMs can easily achieve ~140 perplexity on PTB (https://arxiv.org/pdf/1412.7753), and LSTMs can touch 50s (https://arxiv.org/pdf/1708.02182). The paper reports 3559.06 on PTB for some attacks, which is quite suspicious and may point towards bugs in training setup (or related to budget/model size).\n\n2. **Why do attackers need to have a query / surrogate size budget?**: the model extraction being studied here is happening on a device the attacker has access too, unlike the typical cloud / API setting where attackers can get throttled. Given this, why should their query budget be fixed? While I agree this is useful for validating the relative strengths of different TeLLMate variants, it would be useful to see how much better the attacks get if the query budgets are significantly increased. Related question, what if a deeper network is used as the surrogate to aid learning?\n\n3. **E2E extraction attacks maybe more effective than layer copying**: my understanding is that all attacks in the paper attempt to copy individual layers in the TEE. This may be fundamentally limiting since the layers before/after are fixed and it's a less flexible optimization problem for the model. My suspicion is this is the main cause for high perplexity (weakness 1) and model collapse. How effective are the attacks at the same budget if the extraction is done e2e rather than just the TEE layers?\n\n4. **What's the pareto frontier between latency and security for TEE layers?** It would be helpful to know how much is the latency drop for every extra layer in the TEE and plot that against the attack accuracy to get a sort of pareto frontier. Latency was a big motivating argument for dividing layers between TEE / REE but I didn't see much experimental evidence for this.\n\n5. **No empirical comparisons with previous layer splitting methods**: the related work section cited a bunch of previous methods such as TEESlice. However all experiments were against ablated versions of the author's setup. It would be nice to compare against the closest method in existing literature, perhaps on the latency / accuracy pareto frontier."}, "questions": {"value": "What is the surrogate model size in the main experimental setup? Is it identical to the TEE layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LtOQc8YzzT", "forum": "MQVnitUWAb", "replyto": "MQVnitUWAb", "signatures": ["ICLR.cc/2026/Conference/Submission14701/Reviewer_v765"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14701/Reviewer_v765"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761326758814, "cdate": 1761326758814, "tmdate": 1762925066624, "mdate": 1762925066624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose TeLLMate, a framework designed to securely deploy LLMs on edge devices using Trusted Execution Environments (TEEs). Unlike existing methods that require extensive retraining or protect all model layers, TeLLMate selectively secures only the most critical layers, achieving high security under resource constraints. It first identifies a layer essential to model performance by observing “standard deviation shift,” and then selectively opts in adjacent layers based on “layer-wise cosine similarity” until it reaches a certain level of security or a certain amount of inclusion. Experiments show that protecting at most 10% of the model’s parameters can drastically degrade a stolen model’s performance, reducing accuracy to near-random levels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The scope of the work (achieving security under the resource constraint of TEE) is well explained and is easy to follow.\n2. The authors relate outliers, which are usually discussed in the model compression context, to the TEE context, which is conceptually very interesting.\n3. The attacker in their threat model is inspired by LLM-Streamline (a reasonably recent model compression technique), which indicates they are aware of the relevant field beyond TEE. Their defense experiment is built not on an overly naive baseline but on a reasonable setting (as shown in Table 1)."}, "weaknesses": {"value": "A. Ablations on their proposed method are mostly missing. For example,\n\n1. **Discussion on non-adversarial context** (Section 4.3, line324-330): The paper fully focuses on discussing the post-attack PPL, and there is no discussion about how much performance drop is achieved after removing the critical component (i.e., before the retraining attack). Adding this baseline number to Table 1 would help provide better intuition about how much drop is achieved through removing the critical component, and how much is recovered by the attacker.\n\n2. **Justification for the std-based approach** (Section 4.3, line324-330): The authors observed that there exists a critical layer based on the Wanda pruning score (which itself is promising). However, they then turn to their unique metric (Eq. 1), claiming that it solves reliance on the calibration dataset and computational intensity. I do not see why. As far as I understand, their method also relies on activations (which should be based on some calibration samples), and I’m not sure why it reduces computation.\n\n3. **Scalability of the termination protocol** (Section 4.3, line 348-350): The authors use a constant of PPL = 300 as the threshold to terminate the layer selection. I want the authors to justify the choice. Doesn’t this lead to choosing more layers as model size increases, because it could be more robust against dropping a layer? And can’t this be the exact reason why they end up choosing more layers for the larger model (Table 2)? I believe the number of chosen layers should rather be fewer for larger models (or at least remain the same), given the resource constraint of TEE. However, their design seems to go against it.\n\n4. **Potential over-complication of the method** (Section 4.3, line 359-370): They mention that the std-based approach (Eq. 1), which they proposed, is not enough, and cosine similarity gives a meaningful signal on how important each layer is. Here, I wondered why it is not entirely based on cosine similarity. For example, a simpler algorithm “choosing two adjacent layers with respect to cosine similarity” should lead to the same choice of layer 0–1. The experiments are not diverse enough to justify their method.\n\n5. **Justification for the layer-selection mechanism** (Section 5.2, general): Table 2 suggests that, although Qwen is an exception, layers 0–1 are almost always critical (e.g., layer 3 is selected for Llama2-13B, but the algorithm did not reach a satisfactory level until it removes layer 0. For the other Llama models, it always starts with layer 1, with 0 being chosen as the adjacent layer.) I think the diversity of the results is again not enough to disprove this view. I would at least like to see what happens for Qwen if layers 0–1 are selected instead of 6–7. (Table 3 does some ablation in this direction, which I think is promising, but there should be something more convincing than selecting random layers.)\n\n6. **Further doubt about the layer 0-1 issue** (Section 5.3, Table 4): They compare the PPL with and without their metric. However, given that the selected layers are 0–1 for all of the tested models here (Llama2-7B, Llama3-8B, Mistral-7B), it cannot be concluded whether their cosine approach helped, or if it is just that choosing layer 0–1 is usually a good choice.\n\nB. (Minor) I observe several points that could improve the presentation \n\n1. (Section 1, line 53) The abbreviation “REE” is used without introducing its full name. It is introduced for the first time in Section 2.1.\n\n2. (Section 4.2, line 256-261) The paragraph says Table 1 contains “MLP, Layer, and Layer+Following Layer”. However, Table 1 seems to have “Outliers, MLP, and Layer”.\n\n3. (Section 4.3, Fig.6) The blank space of Layer Index > 32, or Tensor Value >1.0, should be removed if values can never be in this range.\n\n4. (Section 4.3, general) The authors spend more than one page explaining the algorithm. When I read this, I wanted to quickly grasp the overview. Pseudocode should help with that. I found that the authors put it in App. A.3, which is nice, so I suggest pointing to it or including a smaller version in Section 4.3."}, "questions": {"value": "1. (Abstract)  The authors mention that at most 10% is needed to protect the model. I think the worst case refers to the result of Llama2-13B, where 4 layers are required. This means roughly a billion parameters need to be secured. Can the authors discuss whether this is practical, considering the current resource constraints of TEE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i9TujU2zf2", "forum": "MQVnitUWAb", "replyto": "MQVnitUWAb", "signatures": ["ICLR.cc/2026/Conference/Submission14701/Reviewer_55Dd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14701/Reviewer_55Dd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505711898, "cdate": 1761505711898, "tmdate": 1762925066091, "mdate": 1762925066091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TeLLMate addresses the challenge of protecting LLMs deployed on edge devices from model-stealing attacks using TEEs. The paper identifies that removing outlier-associated weights from just a few specific critical layers causes significant performance degradation, and proposes an algorithm that selectively places only these minimal critical layers inside the TEE while keeping the rest in the REE space. The method achieves strong protection while protecting only ~6-10% of model parameters. The approach is validated across different type of LLMs and demonstrates resistance against retraining-based model stealing attacks with minimal computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Very timely work, exciting to see use of TEEs to protect LLM at the edge\n2. Novel work, following other recent advantages in the area (e.g., TEESlice)\n3. Methodology and evaluations look reasonable"}, "weaknesses": {"value": "1. The manuscript was not easy to read. It includes complicated sentences and some grammatical errors. \n2. I believe that the paper would improve by adding some LLM relating information in the Background section, for instance mentioning the architecture of LLM networks, defining the MLP block and QKV weights that are mentioned after L267.\n3. While, as I mentioned above, I do like the contributions of this work, I was also expecting to see some real-world evaluations to explore the performance of your approach with TEE-enabled edge devices. This is particularly important since you mention low communication cost in your approach. Most of the papers you cite do that, and would be really interesting to see a comparison (for instance using an Nvidia Jetson machine).\n4. I didn’t understand the attacker’s thread model, particularly the assumption that the model architecture is known. Everything inside a TEE is/should not be accessible to an attacker. Also, you comment “Although this is not practical in real-world scenarios, it allows for rigorous security analysis”. Considering that this is a weaker access, why do you claim that it allows “rigorous security analysis”?\n5. Minor: y axis label is missing from Figure 2"}, "questions": {"value": "1. You note that assuming knowledge of the model architecture enables “rigorous security analysis,” despite this being non-practical. Could you explain how this simplification actually contributes to a more rigorous or insightful security analysis, and what limitations or assumptions are being made in this approach?\n2. Could the authors provide experimental results demonstrating the performance of their approach on a TEE-enabled edge platform (e.g., NVIDIA Jetson or similar hardware), including latency, resource consumption, and accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KHjEABIqxP", "forum": "MQVnitUWAb", "replyto": "MQVnitUWAb", "signatures": ["ICLR.cc/2026/Conference/Submission14701/Reviewer_eCRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14701/Reviewer_eCRC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230670809, "cdate": 1762230670809, "tmdate": 1762925065489, "mdate": 1762925065489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}