{"id": "3Zt2PRJZM6", "number": 15648, "cdate": 1758253573055, "mdate": 1759897291283, "content": {"title": "ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack", "abstract": "Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user’s intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming SecAlign++, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing both the undefended LLaMA baseline (78.2% utility and 43.6% ASR) and the state-of-the-art defensive model SecAlign++ (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://anonymous.4open.science/r/ReasAlign-DDC4.", "tldr": "A reasoning-enhanced guardrail for defending prompt injection attack.", "keywords": ["prompt injection attack", "defense", "LLM safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e4f13d5512919c83f29195c274deff3bcfeee78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ReasAlign, a model-level defense against indirect prompt injection attacks. \nThe method fine-tunes an LLM to follow a specific, structured reasoning process. This process involves (1) Problem Analysis, (2) Reasoning to identify conflicts and preserve user intent, and (3) Final Answer Generation.\nThe goal is to analyze user queries and external data, identify and isolate conflicting malicious instructions, and maintain the user’s original task intent, thereby avoiding the “overkill” problem of prior work (secalign++).\nThe method is augmented by a “test-time scaling mechanism” , which uses a separate, preference-optimized “judge model” to score and select the best reasoning trajectory from multiple samples.\nThe authors claim this approach achieves a superior security-utility trade-off, significantly outperforming a baseline (SecAlign++) on benchmarks like CyberSecEval2."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets a critical and timely vulnerability in modern LLM-based agents: indirect prompt injection.\n- The authors correctly identify a significant, practical weakness in existing defenses (which they categorize as “internal defenses” like SecAlign++). This is the “over-defensive” or “overkill” issue, where rigidly suppressing all external instructions severely harms utility when those instructions are benign and necessary.\n- The empirical results presented on the CyberSecEval2 benchmark look good, demonstrating a massive utility improvement under attacks (94.6% vs. 56.4%) over the SecAlign++ baseline , while simultaneously achieving a very low Attack Success Rate (3.6%)."}, "weaknesses": {"value": "- Baselines: The paper’s “state-of-the-art” (SOTA) claims hinge on outperforming SecAlign++. However, SecAlign++ is not a rigorously peer-reviewed, published method, which may weaken the claim of advancing the SOTA. Besides, the paper dismisses “external defenses” as simple detectors that just “halt task execution”. It fails to compare against a crucial, training-free baseline: a strong, undefended LLM guided by a simple prompt that instructs it to perform the exact same reasoning as ReasAlign. This “strawman” argument means the paper fails to prove its complex method is better than a simple prompt.\n- Novelty: The paper’s primary contribution is fixing the “overkill” problem. This appears to be an incremental patch for a fundamental design flaw in prior work (SecAlign++), rather than a novel defensive paradigm.\n- Questionable scalability & methodology: The method fine-tunes a fixed reasoning template onto models. This “hard-coded” reasoning path may conflict with or “straightjacket” the more advanced, inherent reasoning capabilities of current flagship LLMs like GPT-5 and Gemini-2.5-pro. There's little evidence the proposed methods are not conflict with model's internal reasonings.\n- Unjustified complexity and missing details: The “test-time scaling search”  is a core component of this paper, however, it is vaguely described. The paper mentions a “beam search tree”  but provides no algorithm or figure, making it hard to understand and assess. The overhead analysis in Sec 5.7 is misleading. It only counts final tokens , completely ignoring the significant inference cost of sampling and repeatedly calling the judge model at each step, which doesn't sound like a practical approach.\n- Judge model generalization is also unproven: The paper’s claim that its mechanism helps “ensure” logical correctness(Line 271) is a clear overstatement. This judge is trained on a synthetic preference dataset (chosen vs. rejected thoughts). There is no evidence this judge can generalize to correctly score reasoning paths for unseen, complex, or out-of-distribution tasks. The assumption that a judge model (likely to be small) can effectively cover all possible reasoning trajectories for novel tasks is a major, unsupported leap.\n- RQs are not presented with clear answers, which is a presentation issue."}, "questions": {"value": "- Missing Baseline Comparison: Why does the paper omit the most critical zero-shot baseline: using a simple prompt to instruct a powerful, general-purpose model (e.g., GPT-5) to perform the exact same reasoning as ReasAlign? How do you justify that your complex training and search method is superior to this simple, training-free alternative?\n- Overhead Analysis: The overhead analysis in Section 5.7 only appears to count final output tokens. Does this analysis account for the full end-to-end latency and computational cost of the “test-time scaling search,” including the N sampling steps plus the multiple calls to the “judge model” ?\n- Method Scalability: What evidence do you have that your “SFT-instilled,” fixed reasoning template  will not “straightjacket” or conflict with the more advanced, inherent reasoning capabilities of much stronger, frontier models (e.g., at the GPT-5 scale)?\n- Judge Model Generalization: Your “judge model” was trained on synthetic preference data. What evidence proves it can generalize to correctly assess the logical correctness of reasoning paths for complex, unseen, or out-of-distribution (OOD) tasks?\n- Algorithmic Details: The paper mentions a “beam search tree”  but provides no algorithmic details, pseudocode, or figures, making the method irreproducible. Can you please detail precisely how this search process operates in conjunction with the “judge model” ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xlqUpgHnfh", "forum": "3Zt2PRJZM6", "replyto": "3Zt2PRJZM6", "signatures": ["ICLR.cc/2026/Conference/Submission15648/Reviewer_yypD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15648/Reviewer_yypD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544017375, "cdate": 1761544017375, "tmdate": 1762925907707, "mdate": 1762925907707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReasAlign, a model-level defense mechanism designed to mitigate indirect prompt injection attacks in (agentic) large language model systems. Unlike prior approaches that rely mainly on filtering or post-hoc detection, ReasAlign integrates structured reasoning steps directly into the model’s inference process to identify and reject malicious instructions while maintaining the user’s intended task flow.\n\nThe authors conduct extensive evaluations across multiple benchmarks, and the results seem good."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The results are strong - it gets good results on many benchmarks;  compare that to the undefended model or even the previous best defense SecAlign++.\n2. The authors did a thorough job testing their approach across many different benchmarks - general tasks, security-specific tests, and agent workflows. \n3. This paper tackles a really important security issue that matters more and more as we deploy AI agents in the real world. Indirect prompt injection attacks are a serious threat to these systems, so having a solid defense is crucial for making them safe to use.\n4. The idea of using reasoning to detect attacks is reasonable and feels more principled than just adding guardrails. The test-time scaling with a judge model to pick the best reasoning path is an interesting touch that helps ensure the defense actually works reliably."}, "weaknesses": {"value": "1. The paper has some organizational problems in writing. Section 3.2 on threat model seems quite standard and repetitive compared to prior work - it doesn't need to be in the main text. Figure 2 showing prompt injection examples is also unnecessary. More critically, Section 4 on methodology lacks a clear diagram to quickly illustrate how the approach actually works, which would be much more helpful than the redundant threat model description.\n\n2. My main concern is whether this method can scale to larger models. SecAlign++ mainly reports 70B results in their paper, but the authors here seem to only compare against the 8B version, which might be unfair. Looking at Table 1, the 7B model's utility is really bad - so it's hard to tell if ReasAlign's better performance comes from the actual method or just from using GPT-4o-mini as the reasoner while other baselines don't leverage a stronger external model (even it's only for generating structured reasoning steps) . This makes the comparison somewhat unfair and raises questions about scalability.\n\n3. The authors claim reasoning is crucial for their defense, but they only test with the relatively weak GPT-4o-mini. If you used stronger reasoning models like gpt o4 series or Claude-4, wouldn't the results improve significantly? Without any experiments on this, it's unclear how much the reasoning quality actually matters versus just having any reasoning component.\n\n4. Minor issues: The camel citation appears to be wrong; Section 5.6 on node scaling lacks sufficient context - readers unfamiliar with this concept will find it hard to follow."}, "questions": {"value": "Would this method also scale to larger models?\n\nWhat would happen if stronger reasoning models were used as the reasoner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mgdWBLaEE1", "forum": "3Zt2PRJZM6", "replyto": "3Zt2PRJZM6", "signatures": ["ICLR.cc/2026/Conference/Submission15648/Reviewer_b3k7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15648/Reviewer_b3k7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586488944, "cdate": 1761586488944, "tmdate": 1762925907231, "mdate": 1762925907231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReasAlign introduces a reasoning-based safety alignment method that protects large language models from prompt injection attacks without sacrificing utility. It fine-tunes models with LoRA using structured reasoning data—each example guiding the model to analyze inputs, detect malicious instructions, and complete the user’s true task. At inference, multiple reasoning paths are generated and scored by a logic judge trained via Direct Preference Optimization, selecting the most coherent and safe output. Experiments show ReasAlign greatly lowers attack success rates while even improving task performance, demonstrating that reasoning-driven alignment can achieve both strong security and high usability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodologically simple yet highly effective—built on standard SFT and LoRA without architectural changes.\n2. Structured reasoning format (Problem Analysis → Reasoning → Final Answer) provides clear interpretability and controllability.\n3. Effectively balances safety and utility, outperforming prior defensive methods.\n4. Maintains or even improves general task performance after alignment."}, "weaknesses": {"value": "- Test-time scaling increases inference cost and latency.\n- Results rely on LLM-as-judge, which can introduce evaluation bias and reduce accuracy."}, "questions": {"value": "1. Why did the authors rely on an LLM-as-judge to compute ASR instead of using exact tool-call or parameter matching for more objective, reproducible evaluation?\n2. Why was the method not evaluated on the ASB [1]?\n3. Why did the experiments exclude Qwen models, which are now among the most widely adopted open-source LLMs?\n4. In Test-Time Scaling, why does the model generate three reasoning rollouts but the total token usage is not approximately triple? For closed-source models, this would significantly increase inference cost. Would a fairer comparison require baseline models to also perform multiple rollouts and select the best answer for consistency in computational budget?\n\n[1] Zhang et al., *Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents*, ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dxkw4X7Zh8", "forum": "3Zt2PRJZM6", "replyto": "3Zt2PRJZM6", "signatures": ["ICLR.cc/2026/Conference/Submission15648/Reviewer_5q6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15648/Reviewer_5q6g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709209475, "cdate": 1761709209475, "tmdate": 1762925906691, "mdate": 1762925906691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a defense mechanism against prompt injection attacks. It first fine-tunes a model (Llama3.1-8B-Instruct) to generate structured reasoning in 3 stages (problem analysis, reasoning, final answer) by distilling a stronger model (GPT-4o-mini). They also propose test-time scaling of best-of-n with a DPO trained judge model. Experiments on benchmarks like CyberSecEval2 show ReasAlign achieves superior performance than undefended LLaMA and SecAlign++"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper tries to address a genuine and important security problem: prompt injection attacks in LLM agents that interact with untrusted external data\n\nPaper is clear in motivation and problem setup"}, "weaknesses": {"value": "**Limited technical novelty**\n\nThis paper takes very standard approaches like using a stronger model to generate reasoning data, and does best-of-N sampling with a judge model. \n\n**Missing related work and experimental comparisons**\n\nThis paper proposes a mix of model-level and test-time scaling approach for prompt injection defense. It only cites 4-5 defense papers (primarily SecAlign series and system-level defenses) while missing significant model-level and detection-based work. In addition, no experimental comparison with these cited work has been provided.\n- LLM-LAT Robust [Sheshadri et al., 2024] Latent adversarial training improves robustness to persistent harmful behaviors in llms\n- Circuit Breaker [Zou et al., 2024] Improving alignment and robustness with circuit breakers\n- SafeChain [Jiang et al., 2025] SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities\n\n\n**Incomplete analysis**\n- Only explores N=1,2,3 (why not 5, 10, 20?). When do diminishing returns occur?\n- No analysis of what makes candidates diverse or whether judge actually selects better reasoning\n- No comparison to simpler selection strategies (e.g., majority voting, confidence-based selection)\n- No investigation of failure modes: When and why does reasoning fail?\n- With N=3 candidates + judge scoring, actual cost is >3x but this is never quantified"}, "questions": {"value": "The abstract claims you \"introduce a test-time scaling mechanism\" - but this is standard best-of-N sampling with reward models. Can you clarify what is technically novel beyond applying existing techniques to a new domain?\n\nWhy were significant prior works on prompt injection defense omitted in comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uxOnkQ1iu0", "forum": "3Zt2PRJZM6", "replyto": "3Zt2PRJZM6", "signatures": ["ICLR.cc/2026/Conference/Submission15648/Reviewer_o4LU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15648/Reviewer_o4LU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969361660, "cdate": 1761969361660, "tmdate": 1762925905967, "mdate": 1762925905967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}