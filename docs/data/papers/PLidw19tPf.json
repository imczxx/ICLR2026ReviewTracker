{"id": "PLidw19tPf", "number": 21703, "cdate": 1758320684326, "mdate": 1759896907918, "content": {"title": "How does information access affect LLM monitors' ability to detect sabotage?", "abstract": "Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that these monitors perform better when they have access to more of the monitored agents’ reasoning and actions, we find that contemporary monitors often perform better with less information, a phenomenon we call the less-is-more effect for automated monitors. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce a new extract-and-evaluate monitoring technique that improves sabotage detection rates in the BigCodeBench-sabotage environment by up to 12 percentage points over the next-best monitor without increasing the false positive rate. In addition, we find that agents that are unaware they are being monitored can be caught much more easily and that longer reasoning traces can be difficult for monitors to parse without information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.", "tldr": "We find that LLMs monitoring other LLMs for misaligned behavior sometimes perform better when given less information, and we study the conditions under which this occurs.", "keywords": ["AI safety", "LLM agents", "Chain-of-thought monitoring", "AI control", "Automated oversight"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49d680d39ef6759dd98725ccd67f11c590212025.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the question of \"how does information access affect LLM monitors' ability to detect sabotage?\". They experimented across 3 environments and showed that monitors tend to exhibit what they called the \"less-is-more\" effect. Additionally, they proposed a new monitoring framework, \"extract-and-evaluate,\" and claimed that this outperformed baselines in the BigCodeBench environment. Lastly, they conducted the cost-performance analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Ran 3 different environments\n- The background is well motivated\n- Conducted a diverse set of experiments"}, "weaknesses": {"value": "This paper claims to have 4 main contributions in Line 83, but I find 3 of them are either relatively weak or not original.\n- Contribution 1 (less-is-more effect) is not a unique contribution; The result from Arnav et al. (arXiv:2505.23575) has already shown this.\n- Contribution 2 (extract-and-evaluate monitoring) seems to be quite environment and monitor-dependent. In Fig. 3, based on the error bars, it seems to work noticeably well only for the gpt-4.1-mini monitor on the BigCodeBench and the MLE-sabotage. The other 5 monitor-environment combinations don’t show as substantial an improvement when considering the error bars.\n- Contribution 3: Didn’t Arnav et al also show the experiment results with the reasoning and non-reasoning models as attackers?"}, "questions": {"value": "- For contribution 3, didn’t Arnav et al also show the experiment results with the reasoning and non-reasoning models as attackers?\n- I'm happy to hear the authors' responses on the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HyJgBkOu8x", "forum": "PLidw19tPf", "replyto": "PLidw19tPf", "signatures": ["ICLR.cc/2026/Conference/Submission21703/Reviewer_iJFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21703/Reviewer_iJFr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609436312, "cdate": 1761609436312, "tmdate": 1762941896468, "mdate": 1762941896468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the task of monitoring for sabotage: given an LLM agent prompted to perform a hidden task, can an LLM monitor detect the presence of this hidden task?\n\nFollowing Arnav et al. (2025)’s finding that monitors with access to both CoT and actions can sometimes underperform action-only monitors, the paper proposes the “less-is-more” effect: sometimes, provisioning monitors with more information (e.g. CoT + actions) causes them to underperform monitors with less (e.g. just actions, or just CoT).\n\nThe paper experiments on four monitoring approaches: action-only, CoT-only, CoT+action, and their novel approach, extract-and-evaluate (EaE). EaA works by separating monitoring into two phases: the “Extractor” reasons about the transcript and extracts quotes, while the “Evaluator” produces a suspiciousness score based only on extracted inputs.\n\nThe paper runs experiments on three environments: SHADE-Arena, MLE-sabotage, and BigCodeBench-sabotage, with two monitor LLMs (Claude-3.7-Sonnet and GPT-4.1-mini), and seven attacker LLMs (two “traditional”/non-reasoning, five reasoning).\n\nThe paper demonstrates the less-is-more effect empirically, finding that CoT+action often underperforms other strategies, despite having the most information. It also finds that EaE sometimes outperforms other methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The abstract and introduction are clearly written, and the paper’s core claim about the less-is-more effect is easy to follow.\n2. The proposed approach, extract-and-evaluate, is simple and well-motivated in light of the less-is-more finding.\n3. Beyond the core claims about less-is-more and EaE, Section 5 gives a number of interesting empirical findings from the conducted experiments. I think these will be useful to researchers working on LLM monitoring.\n4. The studied task, monitoring for sabotage, is an important problem for safety."}, "weaknesses": {"value": "1. “extract-and-evaluate monitoring technique that improves sabotage detection rates in the BigCodeBenchsabotage environment by up to 12 percentage points over the next-best monitor\nwithout increasing the false positive rate\" - this feels like a misleadingly cherry-picked presentation of evidence in the abstract. EaE performs at a statistically indistinguishable level in many settings in Figure 3, and underperforms CoT only in BigBench traditional attacker/GPT-4.1. Without an a-priori reason to privilege this specific setting, you should present either aggregate statistics, or the full range from min to max.\n\n2. I found it difficult to find the full details of any individual experiment. Which models are used as attackers, and which are used as monitors? Details appear spread throughout the Results section; Figure 4 shows five models, but others are listed in the appendix. It would be easier to follow if there was a concise list of all models used, and which roles they were used for, in the Methodology section.\n\n3. I also found it difficult to follow the justification for some of the findings in section 5. I think it would be helpful to have a single graph for each finding, focusing on the features under study. For example, Finding 2 could show a scatterplot between monitor performance and average transcript length, with a point for each monitor-environment pair; Finding 7 could show a bar graph with monitor performance on reasoning models vs. monitor performance on traditional models. Figures could be in appendix, with links from each finding paragraph.\n\n4. Limited novelty of the less-is-more effect itself, as prior work (Arnav et al. 2025) already empirically observed cases where CoT+action underperforms action alone.\n\n5. While the less-is-more effect is demonstrated empirically, it’s still not clear to me *why* it happens. Is the problem due to distracting information? Or model performance in general declining over a certain context length? Given that prior work has already demonstrated this effect empirically, it would be a more valuable contribution to better understand its cause. E.g. if you add plausible-but-irrelevant text, does this impact performance more than adding the same number of clearly-irrelevant tokens (random characters, or just “...” repeated)?"}, "questions": {"value": "1. Figure 3, “error bars are bootstrapped 95% CIs” - how is the bootstrapping done? IIUC, Figure 3 aggregates over attacker models, but these attacker models may be evaluated on the same questions, so model-sample pairs aren’t IID, and a naive bootstrap will underestimate CIs. Could you provide additional details on how you’re computing CIs when aggregating over data? (See e.g. https://www.anthropic.com/research/statistical-approach-to-model-evals, “Cluster standard errors” for one statistical approach used in situations like this.)\n\n2. Figure 6, “We use AUROC rather than log-AUROC for compactness.” - what does this mean?\n\n3. For the results related to monitor awareness, the authors might be interested in the results of this paper, which specifically studies how monitor awareness impacts detection rates and behavior: https://openreview.net/forum?id=r4Q6o7KGdb"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U2Y1D6zp73", "forum": "PLidw19tPf", "replyto": "PLidw19tPf", "signatures": ["ICLR.cc/2026/Conference/Submission21703/Reviewer_2UvC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21703/Reviewer_2UvC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909266660, "cdate": 1761909266660, "tmdate": 1762941896122, "mdate": 1762941896122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of monitoring LLM agents for planning and executing malign side tasks alongside benign main tasks. The main goal of the paper is to evaluate which information the monitor should utilize and how that information should be utilized in order to maximize catch rates (while minimizing false positives) of the monitors. The authors introduce a novel two-step monitoring protocol, extract-and-evaluate (EaE), to improve the accuracy of weaker monitors. The authors evaluate various monitoring protocols against both traditional LLMs and reasoning models on three evaluation environments. The authors main finding is that more information is not always better, and developing various filtering methods for monitoring can be extremely beneficial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The main contribution of the paper is valuable to the AI control community: \"naively applying monitors on the full agent trajectory is often insufficient to achieve optimal monitor performance.\"\n* EaE seems to provide a Pareto improvement (performance / cost) over other monitoring strategies.\n* The paper is very well-written and the presentation is overall well-done."}, "weaknesses": {"value": "* Do not run against adaptive agents (i.e. where the agent can optimize itself knowing that it will be monitored). This makes the evaluation setup unrealistic in comparison to actual deployment scenarios. Figure 3 shows that when the agent knows more about the monitoring environment (e.g. SHADE-Arena) the monitoring performance decreases dramatically, and the proposed approaches provide much less benefit (especially when using a weaker monitor). This indicates that the proposed approaches will not necessarily be useful in practical AI control scenarios.\n* Some of the results are quite obvious. For example, Finding 2 and Finding 6 are both very intuitive given the \"lost-in-the-middle\" effect [1], especially for weaker models. I.e. it makes sense that less-is-more especially when monitoring with a weaker agent. Nevertheless, even given this intuition, it is important to show the result empirically.\n* Some of the claims are not obviously backed up by the data. For example, the authors claim that EaE monitoring improves catch rates by 12 percentage points over the next best catch rates (line 321, and mentioned earlier in the paper). Can the authors point to where this can be seen. It is not obvious from the figures in the main paper.\n* There's no one monitor setup that always wins, and the increase in performance is not always significant.\n\n\n[1] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172."}, "questions": {"value": "* How is agent success determined? Is it a deterministic function based on the evaluation environment?\n* Why do CoT-only (w/o actions?) monitors perform better on traditional LLMs (non-reasoning models)?\n* What is the monitor in Figure 4? Is it GPT-4.1-mini?\n* Can you provide a more in-depth justification for why filtering for success makes sense? Why would the CoT-only monitor have a large advantage? This is not obvious.\n* Furthermore, if the actions made by the agent could cause harm, would it not make sense to catch the agent before harm was caused? I think this might be related to the online vs offline setting (Roger and Shlegeris (2025)).\n* What are task success rates for the various models and evaluation environments?\n* Can you further justify why the experimental setup is a pragmatic proxy for the real-world AI control scenario? In practice, agents will not necessarily be prompted to perform these side actions and therefore might also tend to hide their true intentions more (because misalignment)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CLKpXfYElL", "forum": "PLidw19tPf", "replyto": "PLidw19tPf", "signatures": ["ICLR.cc/2026/Conference/Submission21703/Reviewer_uPer"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21703/Reviewer_uPer"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928720263, "cdate": 1761928720263, "tmdate": 1762941895857, "mdate": 1762941895857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how what the monitor is allowed to see (actions, CoT, both, or a filtered subset) changes an LLM monitor’s ability to catch covert sabotage in three “AI control” environments—SHADE‑Arena (assistant tasks with hidden side‑goals), BigCodeBench‑sabotage (single‑step coding with backdoors/side rules), and MLE‑sabotage (agentic ML engineering). The central empirical claim is a “less‑is‑more” effect: monitors given less raw information sometimes detect sabotage better than those given both CoT and actions. Motivated by this, the authors introduce Extract‑and‑Evaluate (EaE): a two‑stage protocol where an Extractor first quotes salient snippets from (CoT + actions), and then an Evaluator scores suspiciousness using only those quotes. EaE improves catch rates in BigCodeBench‑sabotage by up to 12 percentage points on average (and 18.2 pp on subtle side tasks) at 1% FPR, and can amplify weaker monitor models cost‑effectively. The paper further shows that CoT‑only monitors often outperform action‑only ones (contrary to some earlier results), that agents who believe their CoT is private are far easier to catch, and that on a cost–performance frontier, Gemini and Claude families with information filtering dominate. Methodologically, the paper emphasizes log‑AUROC (AUROC with the FPR axis in log‑space starting at 1%) to reflect low‑FPR operational regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A clear empirical story around information access. The paper asks a practical question: \"what should your monitor actually look at?\" and answers it with cross‑environment evidence that naively giving monitors everything (CoT+actions) is often suboptimal, with CoT‑only or extracted subsets performing better. This is useful guidance for safety teams building pipeline monitors today. \n\n---\nA simple, actionable protocol (EaE) with measurable gains. EaE operationalizes the less‑is‑more intuition via hierarchical filtering: have one model extract quotes before a second model scores only those quotes. In BigCodeBench‑sabotage, EaE beats competing monitors on TPR@1% FPR (overall +12 pp; +18.2 pp on subtle tasks), and a “weak Extractor + strong Evaluator” variant achieves large lifts at modest additional cost. This is a pragmatic contribution that teams can test quickly. \n\n---\nAgent awareness and monitorability. The finding that telling the agent its CoT is private dramatically improves CoT‑based monitoring (Δ log‑AUROC > 0.4 in SHADE‑Arena) is an important reminder that situational awareness policies matter. It also partially reconciles mixed results in the literature: unaware agents verbalize more, so CoT monitors work better. \n\n---\nCost–performance perspective. The Pareto analysis is helpful: Gemini and Claude occupy much of the efficient frontier, and information filtering appears on many Pareto‑optimal points; gains diminish after roughly $0.005/transcript, but spending ~$0.1 ensures robust performance giving practical ballparks for deployment. \n\n---\nCare with evaluation metrics. Focusing on log‑AUROC (weighted to ≤10% FPR) reflects real auditing budgets; the authors also reproduce results under standard AUROC and discuss bootstrap variance and threshold effects."}, "weaknesses": {"value": "**Contribution framing**: Beyond EaE, other “contributions” (awareness, cost analysis, reasoning vs. traditional LLMs) feel like observations rather than core advances.\n\n**CoT-only novelty claim**: The authors state that their CoT-only results are novel because prior work (e.g., Arnav et al., 2025) focused on action-only vs. CoT+action monitors. This seems fair, but the claim could be clearer: CoT monitoring itself is well-known, so the novelty lies in the systematic evaluation across environments, not in introducing CoT-only monitoring per se. Claiming novelty here is misleading.\n\n**Scope of EaE**: Gains hinge on very long CoTs overwhelming weak monitors. If CoTs are truncated or monitors scale better, EaE’s advantage largely disappears. If CoT length were capped or chunked, would EaE still matter? The authors acknowledge that on non-subtle tasks and traditional LLMs, EaE is not better than prior methods.\n\n**Mechanistic gap**: Less-is-more may reflect parsing limits and post-hoc justification bias, but no controlled ablations isolate these factors."}, "questions": {"value": "1. Adversarial extracts. How robust is EaE if the Extractor misses the key quotes or is adversarially distracted by benign snippets? Any recall metric for the Extractor (fraction of ground‑truth incriminating spans captured)?\n\n2. When total token cost is fixed, does EaE still outperform CoT+action or hybrid? Also, what happens if you swap roles (strong Extractor, weak Evaluator)?\n\n3. Mechanistic clarity: Did you run ablations to confirm that failures of CoT+action monitors are due to post-hoc justifications rather than other confounders (e.g., prompt complexity)? Can you disentangle causes of less-is-more? For example:\n\n* Truncate CoTs to fixed token budgets.\n* Remove post-hoc justification phrases.\n* Shuffle or mask irrelevant CoT spans.\n* How do these affect monitor rankings?\n\n\n4. Evaluator necessity: Why is a separate Evaluator needed? Did you test a single-step prompt that extracts quotes and assigns a suspicion score? If not, why assume two stages outperform one?\n\n\n5. Scope of novelty: CoT-only monitoring is presented as a novel contribution, but prior work has studied it. Can you clarify what’s new here?\n\n6.  If CoT length were limited to what the monitor can handle (or processed in chunks), would EaE still outperform? Is the gain mostly due to mitigating long-context degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d5M1thopmQ", "forum": "PLidw19tPf", "replyto": "PLidw19tPf", "signatures": ["ICLR.cc/2026/Conference/Submission21703/Reviewer_SMQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21703/Reviewer_SMQo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000090705, "cdate": 1762000090705, "tmdate": 1762941895606, "mdate": 1762941895606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}