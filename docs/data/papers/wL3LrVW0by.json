{"id": "wL3LrVW0by", "number": 22993, "cdate": 1758337890119, "mdate": 1759896837006, "content": {"title": "Short-Context dominance: How Much Local Context  Natural Language Actually Needs?", "abstract": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1–7k tokens from long-context documents, we consistently find that 75–80\\% require only the last 96 tokens at most .Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences.\nFinally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant.  Across Q\\&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "tldr": "", "keywords": ["Long Context", "LLM", "Short context", "Token", "Language"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e778ae440d7cc29cea23266500decb68da6de2ba.pdf", "supplementary_material": "/attachment/17e8e0b73d018ea5907dab2b1ed05069a6940bc3.zip"}, "replies": [{"content": {"summary": {"value": "The paper highlights an important phenomenon for the NLP community.\nThat is, the next token prediction in popular language models (such as Llama 3, Qwen 2, Mistral) mostly relies on local context (<= 100 tokens).\nThe authors quantify this on popular datasets, such as GovReport, News, Wikipedia, and across 8 languages (including English, Arabic, French, German, Chinese --- Figure 11 in Appendix C.2).\nAdditionally, the authors propose a JSD-based method for detecting tokens with long context dependencies and Taboo, a method for accelerating long context performance. What is important to note here is that none of the methods requires the knowledge of the next token. Instead, they rely on the measure of difference between $p( . |t_1, ..., t_n)$ and $p(.| t_{n-p} ... {t_n})$.  \n\nThe phenomenon is somewhat expected. However, this should not downweight the work's importance, as proper quantification is important."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Quantifies an important phenomenon for the NLP field\n+ Provides effective methods for identifying and exploiting long context dependencies, which can significantly aid the creation of pre-training data.\n+ Shows compelling evidence and provides a useful mathematical framework for analysis of long/short context dependence."}, "weaknesses": {"value": "The work would benefit from:\n- quantification of how much of the short context dominance comes from word breaking by the tokenizer\n- per word type quantification of short context dominance, for example, for verbs, adjectives, nouns\n-  additional quantification of the phenomenon on code"}, "questions": {"value": "Can authors quantify how much of the short context dominance comes from the word breaking by the tokenizer? What are the types of words that showcase strong short context dominance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vRumfZcK4O", "forum": "wL3LrVW0by", "replyto": "wL3LrVW0by", "signatures": ["ICLR.cc/2026/Conference/Submission22993/Reviewer_fk5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22993/Reviewer_fk5D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760715040419, "cdate": 1760715040419, "tmdate": 1762942469118, "mdate": 1762942469118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors tackle the problem of measuring natural-language local-context dependency and introduce Minimal Context Length (MCL), which quantifies how much local context an LLM needs to confidently predict the ground-truth next token. Building upon this, they further propose Distributionally-Aware Minimal Context Length (DaMCL), which removes the need for ground-truth labels. Together, these two measurements demonstrate that, for most sequences, a small local prefix suffices to predict their next tokens.\n\nMotivated by this observation, the authors introduce Targeted Boosting (TaBoo), an intuitive decoding algorithm that counters short-context bias by identifying and boosting tokens that are long-context-relevant. Across the Llama, Mistral, and Qwen families, TaBoo consistently improves performance on multiple QA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Authors introduce the MCL, DaMCL, and LSDS metrics, which offer valuable insight into how LLMs process long contexts.\n2. They further propose the enhanced decoding strategy, TaBoo, achieving consistent gains on NarrativeQA, HotpotQA, and MultifieldQA."}, "weaknesses": {"value": "1. The experiments are conducted on outdated checkpoints. Please update them with recent models such as Llama-3.1-8B, Llama-3.2-3B, Mistral-v0.3-7B, or Qwen3-8B. Moreover, the evaluation setting is rather old-fashioned. A thorough evaluation on more comprehensive and challenging datasets is better. If evaluation is required with rich long-range dependencies, LEval [1] or LongBench-v2 [2] should be preferred. Besides, including the more challenging RULER benchmark [3] would also help verify whether TaBoo remains robust when the context contains misleading information.\n2. While I find the definitions and observations of MCL and DaMCL valuable, the extensive analysis leaves little space for showing how these insights can be translated into practical improvements. I also struggle to see why the metrics are used for an enhanced decoding strategy. This may stem from my limited familiarity with contrastive decoding. I would appreciate a clearer explanation. \n3. Figure 7 is blurry. Please check and replace it with a higher-resolution version.\n\n[1] L-Eval: Instituting Standardized Evaluation for Long Context Language Models https://arxiv.org/abs/2307.11088\n\n[2] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks https://arxiv.org/abs/2412.15204\n\n[3] RULER: What's the Real Context Size of Your Long-Context Language Models? https://arxiv.org/abs/2404.06654"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LV6VtHdc97", "forum": "wL3LrVW0by", "replyto": "wL3LrVW0by", "signatures": ["ICLR.cc/2026/Conference/Submission22993/Reviewer_b2nx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22993/Reviewer_b2nx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860424646, "cdate": 1760860424646, "tmdate": 1762942468827, "mdate": 1762942468827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the short-context dominance hypothesis in Large Language Models (LLM), which proposes that for the majority of sequences, a small local context suffices to predict the next token accurately. Through extensive experiments, the authors find that about 75–80% can be predicted with no more than the last 96 tokens. The paper introduces two key concepts: Minimal Context Length (MCL) and Distributionally Aware MCL (DaMCL), the latter of which does not require ground-truth knowledge of the next token. These methods allow for the detection of short and long-context sequences. Moreover, the paper introduces a contrastive-decoding-like decoding algorithm, TaBoo, which addresses the bias induced by short-context dominance in LLMs, boosting performance in tasks with long-context dependencies such as question answering."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Insightful Findings** The authors proposed the short-context dominance hypothesis, and backed the statement by substantial experimental evidence. On a series of LLMs, most sequences rely on very localized context, even in long documents, thus challenging the idea that large context windows are always necessary for accurate token prediction.\n2. **Applicability.** The introduction of MCL and DaMCL provides a new approach to understanding and measuring context dependency in LLMs. DaMCL, in particular, offers a practical method for detecting long-context dependency without ground-truth knowledge. The proposed metrics are highly applicable, working across different models and various datasets, including both short and long-context tasks\n3. **Promising performance gains for TaBoo.** TaBoo effectively boosts the performance of LLMs in downstream tasks, offering a more delicate solution to handling context compared to existing contrastive decoding methods."}, "weaknesses": {"value": "1. **Efficiency Analysis.** Efficiency of the proposed method could be further analyzed and reported. It would be better if the authors could report the efficiency (in seconds) before/after applying TaBoo to further demonstrate the extra computational gains required by TaBoo.\n2. (Optional, as the authors left this for future works in L169.) **Performance on Reasoning Tasks.** I am curious about the distribution of short/long-context dependency, and the performance of TaBoo on more difficult reasoning tasks, such as Math and Code. This is completely optional and does not affect my overall rating to this article."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FVgCHIVmHT", "forum": "wL3LrVW0by", "replyto": "wL3LrVW0by", "signatures": ["ICLR.cc/2026/Conference/Submission22993/Reviewer_jngf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22993/Reviewer_jngf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378279545, "cdate": 1761378279545, "tmdate": 1762942468481, "mdate": 1762942468481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the “short-context dominance” hypothesis, that most natural-language predictions depend mainly on local context rather than long-context information. Using the LLM as a statistical oracle, the authors measure each sequence’s Minimal Context Length (MCL) and find that about 80% of next-token predictions require fewer than 96 preceding tokens. To address the stochastic nature of the practical generation process, they extend this with a distribution-aware variant (DaMCL). In addition, the paper proposes a Long-Short Distribution Shift (LSDS) that identifies long-context sequences using a comparative approach (local vs. global). Building on these insights, the paper introduces TaBoo (targeted boosting), a decoding algorithm that selectively boosts tokens needing long-range context. Experimental results demonstrate that TaBoo consistently outperforms baseline decoding methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* The paper clearly illustrates that the key challenge in long-context modeling is not merely handling long text inputs, but distinguishing cases that truly require long-range contextual understanding. This insight has been relatively underexplored and is an important conceptual contribution.\n* The Related Work section, particularly the discussion on N-gram language models, is comprehensive and well-situated within prior research.\n* The logical progression from MCL to DaMCL, LSDS, and finally to token-level boosting (TaBoo) is coherent and intuitive. The paper is engaging and easy to follow."}, "weaknesses": {"value": "* (minor) Because decoder-only LLMs predict the next token at every position, the analysis based on MCL and DaMCL, which evaluates prediction accuracy at selected tokens, may not fully capture the dynamic behavior of models over entire documents."}, "questions": {"value": "* Both MCL and DaMCL depend on the choice of the LLM as an oracle. Although the authors partially address model robustness through cross-model comparisons, a discussion on how to mitigate dependence on a specific LLM would strengthen the paper.\n* Rather than applying token-level boosting, one might consider increasing the proportion of long-context-dependent samples in the training data. Could such a data-level intervention provide similar benefits?\n* (minor) There are some inconsistencies in citation formatting (e.g., Section 4.1)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QsjtMx38WE", "forum": "wL3LrVW0by", "replyto": "wL3LrVW0by", "signatures": ["ICLR.cc/2026/Conference/Submission22993/Reviewer_SBU7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22993/Reviewer_SBU7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997269254, "cdate": 1761997269254, "tmdate": 1762942468237, "mdate": 1762942468237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}