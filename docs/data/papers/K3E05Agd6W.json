{"id": "K3E05Agd6W", "number": 5722, "cdate": 1757929271298, "mdate": 1759897958297, "content": {"title": "Multi-Policy Pareto Front Tracking Based Multi-Objective Reinforcement Learning", "abstract": "Multi-objective reinforcement learning (MORL) plays a pivotal role in addressing multi-criteria decision-making problems in the real world. The multi-policy\n(MP)-based approaches are widely used to obtain high-quality Pareto front approximations for the MORL problems. Relying primarily on the online reinforcement learning (RL), the traditional MP approaches usually adopt the evolutionary\nframework that requires maintaining a large policy population. In practice, however, this often leads to sample inefficiency and/or excessive agent-environment\ninteractions. To address these issues, we propose the novel Multi-policy Pareto\nFront Tracking (MPFT) framework that eliminates the need to maintain any policy population, compatible with both online and offline MORL algorithms. The\nproposed MPFT framework comprises four stages: Stage 1 approximates all the\nPareto-vertex policies whose mappings to the objective space lie on the vertices\nof the Pareto front; Stage 2 proposes a new Pareto tracking mechanism that starts\nfrom each Pareto-vertex policy to track the Pareto front, where a proof of its exponential convergence is provided; Stage 3 identifies the sparse regions in the tracked\nPareto front, and then newly designs an objective weight adjustment method to facilitate the policy tracking for filling these regions; Finally, by combining all the\npolicies tracked in Stages 2 and 3, Stage 4 approximates the complete Pareto front.\nExperiments are conducted on seven continuous-action robotic control tasks using\nboth online and offline MORL algorithms. Results demonstrate that our proposed\nMPFT approach outperforms state-of-the-art benchmarks in terms of hypervolume\nperformance, while significantly reducing the agent-environment interactions and\nhardware requirements.", "tldr": "", "keywords": ["Multi-objective optimization", "reinforcement learning", "pareto-front"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7fcf9b292366c36368fa7f1820c8e39689a95e9.pdf", "supplementary_material": "/attachment/7c9863f4518034a702a5bc2fccbb34f4c27012c4.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a Multi-Policy Pareto Front Tracking (MPFT) framework intended to be compatible with existing reinforcement learning (RL) algorithms. The approach identifies “Pareto vertex” policies, then tracks additional policies to form a “Pareto-edge” policy set, fills in sparse regions, and constructs a dense Pareto policy set."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework presents a reasonable design for two-objective cases. It first identifies the extreme “vertex” policies, then tracks intermediate “edge” solutions, and finally considers sparse regions, which is a logical progression for covering the Pareto front.\n2. The empirical results are better than baselines like PGMORL and PA2D-MORL."}, "weaknesses": {"value": "1. The framework does not appear to extend naturally to problems with more than three objectives, where additional structures such as “facets” (beyond edges) would need to be tracked.\n2. The claimed advantage over evolutionary methods such as PGMORL is limited and not clearly articulated. The main difference seems to lie in the training curriculum rather than in a fundamentally new mechanism. Moreover, Alegre et al. (2022) also presents a non-evolutionary curriculum but suffers from exponential cost growth with the number of objectives. It is unclear how the proposed MPFT framework avoids a similar issue. Although MPFT claims linear time complexity, but it is for parallel training, which could be misleading because the total training cost still grows exponentially.\n3. The paper lacks sufficient discussion and comparison with similarity-based methods such as PD-MORL [1]. The proposed approach for filling sparse regions could also be achieved through similarity constraints, but this is overlooked.\n4. The use of the term “vertex” for single-objective extreme solutions is not rigorous. In discrete spaces, even for a 2D Pareto front, multiple vertices may exist between the two extremes, making this terminology and formulation imprecise.\n\n\n[1] PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm\nToygun Basaklar, Suat Gumussoy, Umit Y. Ogras"}, "questions": {"value": "1. How does the method scale to more than 3 objectives where “facets” (beyond edges) would need to be tracked?\n2. Is there comparison against Alegre et al. (2022)？\n3. How many policy models need to be maintained after training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lv2aIlpXAr", "forum": "K3E05Agd6W", "replyto": "K3E05Agd6W", "signatures": ["ICLR.cc/2026/Conference/Submission5722/Reviewer_7Ejc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5722/Reviewer_7Ejc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761058719247, "cdate": 1761058719247, "tmdate": 1762918217035, "mdate": 1762918217035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MPFT, a multi-policy multi-objective reinforcement learning (MORL) method for approximating the Pareto front in multi-criteria decision-making problems. The framework consists of four stages, including a Pareto-tracking mechanism that locally follows the Pareto front and a subsequent stage that detects and fills sparse regions of the front. The method is evaluated on MuJoCo environments with two and three objectives. However, the work lacks comparison with closely related approaches, and several methodological and experimental details remain unclear. The evaluation is limited to low-dimensional settings (mostly two-objective tasks and only one three-objective case), making it uncertain whether the approach can effectively scale to higher-dimensional or more complex environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an important problem of approximating the Pareto front in multi-objective reinforcement learning.\n2. The proposed framework is conceptually clear and provides a theoretical stability analysis (Appendix C) for the Pareto-tracking mechanism.\n3. The experimental setup includes both on-policy and off-policy settings, showing that the framework can be applied under different training paradigms."}, "weaknesses": {"value": "1. Misleading claim about population-free training.\nThe claim that the method “eliminates the need for maintaining any population” is somewhat misleading. In practice, both Stage 2 (Pareto tracking) and Stage 3 (Pareto filling) continuously add newly generated policies and remove dominated ones, thereby maintaining an evolving set of non-dominated policies. The difference lies mainly in how this population is updated—sequentially rather than in parallel. Moreover, Stage 3 involves multiple rounds of objective-weight adjustment, which essentially follows an evolutionary process.\n2. Limited theoretical guarantee of the tracking mechanism.\nAppendix C only proves that the proposed tracking dynamics keep policies within a neighborhood of the Pareto-stationary set, ensuring local stability but not controlled tracking the entire Pareto front. The mechanism cannot determine the precise direction of movement on the Pareto front, particularly when the number of objectives exceeds two, and therefore cannot guarantee full coverage of the front. This limitation should be clearly stated when describing the claimed “exponential convergence.”\n3. Unclear theoretical basis of the Stage 3 weight-adjustment method.\nThe appendix B.2 states that previous weight-adjustment/selection techniques are heuristic, yet the proposed approach is not theoretically analyzed either. The absence of theoretical justification makes it difficult to assess how the new adjustment differs from existing heuristic methods.\n4. Strong similarity to C-MORL [1] without comparison.\nThe proposed method is highly similar to C-MORL, yet the authors provide no discussion or experimental comparison with it. Both frameworks initialize a few extreme policies, locally improve other objectives while maintaining one (Stage 2 tracking), and fill sparse regions of the Pareto front (Stage 3 filling). Appendix B.2 even suggests using PCA-based reduction or the crowding-distance rule from C-MORL when the number of objectives m>3, but no experiments are shown in such settings. Since C-MORL already demonstrated scalability up to nine objectives while this work only evaluates two- and three-objective tasks, the claimed novelty of the “Pareto-tracking mechanism” appears incremental, and the absence of a C-MORL baseline makes the claimed advantages unclear.\n5. Unspecified size of the maintained Pareto set.\nThe number of non-dominated policies maintained (often called the policy-buffer size in multi-policy MORL) is not reported. This number directly affects both hypervolume and computational cost, including the claimed CPU-efficiency improvement. Without controlling or reporting this factor, the computational comparison is incomplete.\n6. Limited experimental scope and missing ablations.\nExperiments are restricted to Mujoco tasks with two or three objectives, without tests on higher-dimensional or discrete environments. Limited ablation is provided for key hyperparameters, such as the number of training episodes in Stage 2 (tracking) and Stage 3 (filling), which are likely to affect coverage and performance."}, "questions": {"value": "1. Regarding the “population-free” claim:\n\n(a) Could the authors clarify in what sense the method “eliminates the need for maintaining any population,” given that Stages 2 and 3 still add and remove policies within an evolving archive?\n\n(b) In Stage 3, multiple episodes of objective-weight adjustment are used—does this process not constitute an evolutionary search?\n\n2. Regarding the theoretical guarantee (Appendix C):\n\n(a) The proof shows convergence to a neighborhood of the Pareto-stationary set, but not guaranteed traversal or full coverage of the Pareto front. Could the authors clarify the intended scope of the “exponential convergence” statement in the main text?\n(b) How does the proposed mechanism behave when the number of objectives exceeds two, where the direction along the front is not uniquely defined?\n\n3. Regarding the weight-adjustment method in Stage 3:\n\n(a) The appendix suggests prior weight-adjustment methods as heuristic; is the proposed one also heuristic, or is there any theoretical justification for its design?\n\n(b) How sensitive is the method to the number of weight-adjustment episodes or the choice of adjustment method?\n\n4. Regarding comparison with C-MORL:\n\n(a) Why is C-MORL not included as a baseline, given the clear methodological similarity and its demonstrated scalability up to nine objectives?\n\n(b) If the proposed method reuses the crowding-distance rule from C-MORL when m>3, what specific difference or advantage remains?\n\n(c) Would C-MORL, with only two initial extreme policies already, reproduce the same Stage 1–3 behaviour?\n\n5. Regarding computational efficiency and policy-set size:\n\n(a) What is the number of policies maintained in the Pareto set during training in all multi-policy baselines and MPFT? Is this number fixed or dynamically adjusted?\n\n(b) Since this directly affects both CPU usage and hypervolume, can the authors provide a fair comparison under equal policy-set sizes?\n\n(c) The claimed CPU-efficiency advantage may depend heavily on the chosen level of parallelization. In many multi-policy MORL baselines (e.g., PG-MORL or C-MORL), the number of policies trained or stored in parallel, for example, during the warm-up or stage, can be flexibly reduced or run sequentially to lower computational cost. Was the level of parallelization (i.e., the number of simultaneously trained or stored policies) matched across methods? \n\n6. Regarding experimental scope and ablations:\n\n(a) Why are experiments limited to Mujoco tasks with two or three objectives?\n\n(b) Can authors also evaluate the expected utility (EU) metric?\n\n(c) Can the method scale to other domains in mo-gymnasium [2]?\n\n(d) Could the authors provide ablation studies on key hyperparameters, such as the number of episodes or update ratios in Stages 2 and 3?\n\n7. The claimed benefit of reducing CPU usage seems overemphasized, while the more meaningful contribution is actually adapting the multi-policy Pareto-tracking framework to offline settings. The paper would be stronger and distinguished from previous multi-policy methods if it focused on analyzing and validating the offline performance and stability, rather than framing CPU efficiency as the main advantage.\n\n[1] Ruohong Liu, Yuxin Pan, Linjie Xu, Lei Song, Pengcheng You, Yize Chen, and Jiang Bian. Efficient discovery of Pareto front for multi-objective reinforcement learning. In The Thirteenth International Conference on Learning Representations, 2025b.\n\n[2] Felten, Florian, et al. \"A toolkit for reliable benchmarking and research in multi-objective reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 23671-23700."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KO3Q15T5dD", "forum": "K3E05Agd6W", "replyto": "K3E05Agd6W", "signatures": ["ICLR.cc/2026/Conference/Submission5722/Reviewer_KQLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5722/Reviewer_KQLK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571779176, "cdate": 1761571779176, "tmdate": 1762918216769, "mdate": 1762918216769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a decomposition-based Multi-Policy Pareto Front Tracking (MPFT) framework for MORL. MPFT introduces a Pareto-tracking mechanism which alternates between Pareto-ascent and Pareto-reverse updates to progressively approximate the Pareto front, and an objective weight adjustment procedure to identify and fill sparse regions. Experimental results are reported for seven continuous-action problems, but discrete problems or discrete Pareto fronts were not considered."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The framework is clearly structured and easy to follow.\n\nIntroduces a tracking mechanism combining Pareto-ascent and Pareto-reverse updates. Theoretical analysis, including the exponential convergence proof, adds rigor to the method. \n\nThe framework’s compatibility with both online and offline RL algorithms which may increase applicability.\n\nThe experiments show that the proposed framework can be combined with multiple RL algorithms, suggesting it could serve as a flexible plug-in mechanism for various MORL settings."}, "weaknesses": {"value": "The paper claims to focus on efficiency for real-world applications, but the experiments still use tens of millions of environment steps on standard MuJoCo tasks. This raises concerns about the claimed efficiency. How can the authors justify this efficiency claim or demonstrate effectiveness under shorter training budgets? How would the framework perform when the training steps are significantly reduced?  \n\nAlthough the theoretical time complexity is linear in the number of objectives, the Pareto-tracking process itself (alternating ascent and reverse updates) seems to naturally require many update cycles to tracking the front. In particular, this does not take into account the potentially complex topology of Pareto fronts of several objectives, so that it is practically risky to rely on the individual objectives and leave the (exponential) complexity of the Pareto front unexplored.\n\nThe proposed framework seems closely related to previous Pareto-ascent–based work: both use Pareto-ascent principles, and MPFT extends them with a reverse-tracking component. Explaining on the connection, difference and conceptual novelty more clearly between those two methods would help appreciate the distinct contribution. \n\nIf a Done_t variable is used, why also a maximal time T is needed in theory (i.e. why not T=infinity)? Or what would happen if the job isn't finished before time T? To me it seems these are aspects of the particular implementation, so, if at all, it can be discussed in the appendix (where actually T=infinity is used!), while here a less redundant formulation could be preferable.\n\nDefinitions 1 and 2, are not suitable to deal with the case of non-connected Pareto fronts (whether this is because not solutions exists in a certain region of the objective space or whether the best solutions in a certain direction are Pareto-suboptimal.\n\n256: There is no need to use metaphorical expression like '“continuous”', instead an suitable (topological) definition of continuity should be used. Also, in a continuous problem the idea to \"differ by only one state-action pair\" is not well-defined (at least not in measure-theoretic sense), so that a better formulation is needed here anyway.\n\n288: Use an algorithm environment to represent this part.\n\nThe experimental validation is limited. The experimental comparison mainly includes two external baselines, while the other algorithms are variations within the proposed framework.\n\nWhile this setup helps illustrate MPFT’s internal consistency, the empirical advantage would be clearer if the authors could discuss how MPFT relates to more recent GPI-based, preference-conditioned, or other decomposition-based MORL methods."}, "questions": {"value": "How does the Pareto-tracking (alternating ascent–reverse update) process influence computational cost in practice? \n\nHow would it work with problem that has a non-connected or discrete Pareto front? \n\nIs MPFT robust when the training steps or tracking iterations are reduced?\n\nFrom a conceptual perspective, what is the main new idea behind MPFT compared with prior Pareto-ascent–based framework?\n\nHow does MPFT compare with more recent MORL paradigms such as GPI-based, preference-conditioned, or other decomposition-based approaches in terms of sample efficiency and Pareto-front coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VNIhudZ9Qh", "forum": "K3E05Agd6W", "replyto": "K3E05Agd6W", "signatures": ["ICLR.cc/2026/Conference/Submission5722/Reviewer_yYLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5722/Reviewer_yYLm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906980745, "cdate": 1761906980745, "tmdate": 1762918216473, "mdate": 1762918216473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-stage MORL algorithm. Stage1 approximates the Pareto-vertex policies by solving a single objective optimization problem for each objective; Stage2 initializes thePareto-approximation policy set by parallelly tracking the Pareto front from each approximate Pareto-vertex policy; Stage3 fills the top-K sparse regions; Stage4 completes the Pareto-approximation policy set, by combining the tracked policies from Stages2 and 3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces a Multi-Policy Pareto Front Optimization by jointly optimizing multiple policies to cover different regions of the Pareto front. It uses a diversity-promoting objective to ensure that policies specialize toward distinct trade-offs. A mutual-learning mechanism is also adopted where policies share gradient information to stabilize learning and avoid redundant convergence.\n\nThe paper precisely identifies the limitations of existing methods. Preference-conditioned single-policy models often fail to generalize smoothly across the entire preference simplex. Independent policy training leads to poor sample efficiency and redundant exploration."}, "weaknesses": {"value": "The step of \"Approximate all Pareto-vertex policies\" may take a lot of computation, and it is practically not very efficient to track all their performances.\n\nThe sparse regions are not well defined. Are they mathematical sets?\n\nThe paper could benefit from deeper discussion linking its multi-policy optimization to areas such as policy ensembles in meta-RL, and mixture-of-experts or multi-head architectures."}, "questions": {"value": "What is the computation time for each stage? The paper shall clearly clarify and discuss them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O428BFLV3O", "forum": "K3E05Agd6W", "replyto": "K3E05Agd6W", "signatures": ["ICLR.cc/2026/Conference/Submission5722/Reviewer_ZLCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5722/Reviewer_ZLCd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762838894567, "cdate": 1762838894567, "tmdate": 1762918216227, "mdate": 1762918216227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}