{"id": "VAISvCsrvG", "number": 22907, "cdate": 1758336977580, "mdate": 1759896840554, "content": {"title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry", "abstract": "Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this “LLM-as-a-Judge” paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.", "tldr": "We introduce INSPECTOR, a probing-based framework that uses internal embeddings of small LMs to approximate evaluation scores from stronger LLMs across multiple aspects.", "keywords": ["Evaluation", "Large Language Models Probing", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/272af5e82d6a6e6e50da720f1582ef9fd565ae39.pdf", "supplementary_material": "/attachment/9f36f3215f82c475c92d6edc35f8d21e2d7ad4b3.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the conventional LLM-as-a-Judge paradigm by showing that small language models (SLMs), despite weak generative ability, encode rich evaluative signals in their hidden representations. It proposes the Semantic Capacity Asymmetry Hypothesis, suggesting that evaluation requires less semantic capacity than generation. The authors introduce INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores using internal representations of SLMs. Experiments on reasoning datasets (GSM8K, MATH, GPQA) demonstrate that INSPECTOR approaches the performance of large LLM judges while being more efficient and interpretable. This work advances a Representation-as-a-Judge paradigm that enables cost-effective, scalable, and transparent evaluation of model outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces the Representation-as-a-Judge framework which is a new way of performing evaluation without decoding, shifting the paradigm from generation-based judgment to representation-based probing.\n2. The paper demonstrates consistent and large gains over prompt-based baselines across multiple reasoning datasets, showing that evaluative signals exist in intermediate representations.\n3. The proposed approach significantly reduces inference cost, avoids dependence on proprietary LLMs (like GPT-4), and provides interpretable layer-level insights.\n4. Uses binary probing classifiers to filter low-quality data effectively, improving downstream supervised fine-tuning."}, "weaknesses": {"value": "1. Despite promoting efficiency, the pipeline still depends on large LLMs (like DeepSeek-V3) to provide \"gold\" evaluation scores. Gathering these scores is itself expensive.\n2. Experiments focus only on mathematical reasoning (GSM8K, MATH, GPQA); it’s unclear whether the approach generalizes to open-domain, creative, dialogue tasks or safety evaluation.\n3. While simple probes (logistic regression) are interpretable, they may underfit complex evaluative signals, limiting the upper bound of achievable fidelity. \n4. The paper lacks qualitative analysis and error analysis. Observing some samples where these models perform much better than baselines, or looking at error buckets would be nice."}, "questions": {"value": "1. Could multi-task or cross-aspect probing improve performance compared to aspect-specific classifiers?\n2. How stable are the layer rankings across different datasets — do the same layers consistently encode evaluative information?\n3. Can this method be extended to explain why a response was scored poorly (e.g., highlighting reasoning errors)?\n4. Could the representation-based judge itself be used as a reward model in reinforcement learning from human feedback (RLHF)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ENCrONlfE7", "forum": "VAISvCsrvG", "replyto": "VAISvCsrvG", "signatures": ["ICLR.cc/2026/Conference/Submission22907/Reviewer_DGMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22907/Reviewer_DGMU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761341551745, "cdate": 1761341551745, "tmdate": 1762942433091, "mdate": 1762942433091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new evaluation paradigm called Representation-as-a-Judge, which shifts from relying on large language model (LLM) generation to using internal representations from small LMs (SLMs) for evaluation. The authors introduce the Semantic Capacity Asymmetry Hypothesis, arguing that evaluation requires less semantic capacity than generation. They instantiate this idea in INSPECTOR, a probing-based framework that trains lightweight classifiers on SLM hidden states to predict quality scores (e.g., logicality, factuality) derived from a strong LLM judge. Experiments on GSM8K, MATH, and GPQA show that SLM probes outperform prompting baselines and closely approximate full LLM evaluations, especially in binary classification tasks. The approach also proves useful for filtering training data to improve downstream supervised fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The Representation-as-a-Judge framework is technically sound, which could be an alternative to the prevalent “LLM-as-a-Judge” approach.\n\n2. The approach enables efficient evaluation using smaller, open-source models instead of proprietary LLMs.\n\n3. The empirical results are strong, showing significant gains over some prompting and fine-tuning baselines."}, "weaknesses": {"value": "1. A limitation is that the method still requires a powerful LLM in the loop to obtain initial evaluation scores (for training data). This paper assumes the LLM’s scores are gold-standard. It would strengthen the work to either validate against human ratings or discuss the implications of this dependency.\n\n2. The probing classifiers achieve relatively low accuracy on fine-grained multiclass (1–5) predictions. This might limit the method’s use if one requires precise scoring, and also indicates some inherent issue that the approach doesn’t fully match the large model in terms of detailed gradations of quality. \n\n3. The INSPECTOR pipeline introduces additional complexity and tuning that a direct LLM judge does not require. One must decide how to pool representations, which layers to select, what classifier to use, etc. This paper doesn’t deeply explore the sensitivity to these choices."}, "questions": {"value": "1. Have the authors evaluated or considered how the probe’s judgments (or the large LLM’s scores it learns from) align with human evaluations of the responses?\n\n2. The experiments focus on mathematical reasoning problems. How well do the authors expect the Representation-as-a-Judge approach to transfer to other domains or tasks?\n\n3. Did the authors experiment with regression instead of classification to predict aspect scores on a continuous scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PlqaOIMY6X", "forum": "VAISvCsrvG", "replyto": "VAISvCsrvG", "signatures": ["ICLR.cc/2026/Conference/Submission22907/Reviewer_DiLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22907/Reviewer_DiLJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624960820, "cdate": 1761624960820, "tmdate": 1762942432898, "mdate": 1762942432898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes INSPECTOR, a probing-based framework that evaluates model outputs using internal representations instead of text generation. It introduces the Semantic Capacity Asymmetry Hypothesis, suggesting that evaluation requires less semantic capacity than generation. By leveraging small open-source LMs, INSPECTOR enables lightweight, interpretable, and scalable evaluation, achieving performance comparable to large proprietary models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper reframes model evaluation as Representation-as-a-Judge, shifting from prompt-based evaluation to representation-based probing.\n2. By leveraging internal representations from small open-source LMs instead of large proprietary models, INSPECTOR offers a lightweight and interpretable alternative that significantly reduces computational cost.\n3. The framework achieves high predictive accuracy on reasoning benchmarks (e.g., GSM8K, MATH, GPQA), demonstrating that small models (1.7B) can approximate large-scale evaluators.\n4. The proposed method can enhance LLM evaluation pipelines, reduce dependence on expensive closed models, and support data filtering for downstream tasks."}, "weaknesses": {"value": "1. Experiments are primarily focused on reasoning benchmarks; it is unclear whether the method generalizes to other domains such as dialogue, summarization, or open-ended generation.\n2. The performance quality of the judge (small LM) could be dependent on how the evaluation criteria are established."}, "questions": {"value": "1. Is there a specific reason why the paper focuses only on decoder-only models instead of considering encoder-only architectures?\n2. I believe the Semantic Capacity Asymmetry Hypothesis aligns with a well-established understanding in LLM research. It would be helpful to cite prior work that supports this perspective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2kCGY6uDt0", "forum": "VAISvCsrvG", "replyto": "VAISvCsrvG", "signatures": ["ICLR.cc/2026/Conference/Submission22907/Reviewer_CaW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22907/Reviewer_CaW1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791875612, "cdate": 1761791875612, "tmdate": 1762942432680, "mdate": 1762942432680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper observes that small LMs, although with their limited generative ability, can still provide reliable evaluations within their hidden representations. The authors then propose the Semantic Capacity Asymmetry Hypothesis, which posits that evaluation requires significantly less semantic capacity than generation and can rely on intermediate representations. The authors propose Representation-as-a-Judge, and implement this idea through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores directly from small model representations. Experimental results demonstrate that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates large model judges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel idea for LLM-as-a-Judge.**\nThe concept of semantic capacity asymmetry is creative and appealing. It provides a fresh lens for understanding evaluation-relevant signals in internal representations, suggesting that weak evaluation performance in small LMs may arise from surface-level generation limitations rather than an inherent lack of semantic understanding.\n\n**Effective probing design.**\nThe paper designs probing mechanisms to capture evaluation-related signals from model representations. These probes are empirically shown to be effective and help our understanding of how evaluation-relevant signals work in hidden layers.\n\n**Practical significance.**\nBuilding on this idea, the work demonstrates that smaller open-source models can serve as reliable evaluators, substantially reducing cost while maintaining evaluative fidelity, which is practically meaningful."}, "weaknesses": {"value": "**1. Missing intuition behind the probing design.**\nThe paper introduces probing experiments to support the idea of semantic capacity asymmetry, but the intuition for the probing setup is underexplained. It is unclear what specific representational property the probes aim to capture or why this design shows semantic capacity.\n\n**2.Limited dataset coverage.**\nThe evaluation focuses on three reasoning benchmarks (GSM8K, MATH, and GPQA), all within the mathematics and science domain. Other tasks types, such as open-ended generation, summarization, dialogue and code generation are not considered here. Incorporating a broader range of domains and task formats would strengthen the generality of the proposed idea.\n\n**3. Clarity and presentation.** \nSeveral sections are dense and could be improved with additional figures or concrete examples. For instance, a figure to illustrate how linear probes are constructed."}, "questions": {"value": "1. Can the authors provide a formal definition or metric for semantic capacity asymmetry beyond the conceptual description?\n\n2. The work uses judgments from DeepSeek-V3 as gold labels for training and evaluation, but does not incorporate human annotations as ground truth. Have the authors considered comparing (a) human judgments, (b) large LLM-as-a-Judge outputs, and (c) the proposed Representation-as-a-Judge ouputs?\n\n3. Some minor questions: \n\n   (1) In Eq (6), the symbol $\\mathcal{C}$ is missing, and the meaning of $\\mathcal{F}$ is unclear.\n\n   (2) In the ablation study shown in Figure 4, is the classifier fixed as Logistic Regression in the left plot (where pooling methods vary), and is mean pooling fixed in the right plot (where classifier methods vary)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g2wuJVOvSu", "forum": "VAISvCsrvG", "replyto": "VAISvCsrvG", "signatures": ["ICLR.cc/2026/Conference/Submission22907/Reviewer_GC61"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22907/Reviewer_GC61"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22907/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889543094, "cdate": 1761889543094, "tmdate": 1762942432492, "mdate": 1762942432492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}