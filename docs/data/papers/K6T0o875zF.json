{"id": "K6T0o875zF", "number": 14757, "cdate": 1758243115526, "mdate": 1759897350879, "content": {"title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning", "abstract": "We study what actually works and what doesn’t for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars---environment, reward, and policy---and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL.", "tldr": "", "keywords": ["reinforcement learning", "multi-turn RL", "LLM reasoning", "interactive learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9179160626200bc3f4171c28ba22d0f16176f3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic empirical study on training large language models as multi-turn agents via reinforcement learning. The authors decompose the design space into three pillars (environment, reward, policy) and explore how factors affect training performance. Experiments are conducted with various SFT:RL data ratios. The work distills a set of practical recipes for effective multi-turn RL training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work provides a comprehensive empirical analysis of design choices across multiple benchmarks and domains.\n2. The study bridges fragmented literature and delivers practical guidelines useful for practitioners in agentic RL.\n3. Writing and organization are clear with details."}, "weaknesses": {"value": "1. The work mostly summarizes founded intuitions without introducing fundamentally new algorithms or theories. It's a good work that delievers engineering experiences, but hard to bring new findings for a research submission.\n2. The authors are recommended to conduct deeper theoretical or practical analysis to one of the observation that may deliever good findings.\n3. The authors tested on severl domains of textual environments. The reviewer is curious whether the findings can be generalized to multimodal or real-world interactive settings?"}, "questions": {"value": "This work is valuable as a practical guide and empirical survey for researchers building multi-turn RL agents. However, the lack of methodological novelty and limited depth of analysis weaken its contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nzy6LlURna", "forum": "K6T0o875zF", "replyto": "K6T0o875zF", "signatures": ["ICLR.cc/2026/Conference/Submission14757/Reviewer_iMhR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14757/Reviewer_iMhR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524010969, "cdate": 1761524010969, "tmdate": 1762925115514, "mdate": 1762925115514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a timely and important problem: creating a \"recipe\" for training multi-turn RL agents. The \"three-pillar\" (Environment, Policy, Reward) framework is a clear and useful way to structure the problem. The \"Environment\" pillar (Section 5) in particular is high quality, with solid empirical work.\n\nHowever, the paper's core contributions to the \"Policy\" and \"Reward\" pillars are severely undermined by a failure to engage with (or in some cases, even cite) highly relevant, contemporaneous work. The \"Reward\" pillar's analysis is outdated, as it misses the entire SOTA on step-wise credit assignment. The \"Policy\" pillar's main experiments (both on SFT:RL ratios and algorithm comparisons) are not novel, as other papers directly covered them. The claim to a \"systematic framework\" is also scooped by prior work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper tackles an important and, frankly, very timely problem. The idea of creating a \"practitioner's guide\" or \"recipe\" for training multi-turn RL agents is definitely needed, given the many fragmented approaches.\n\nI found the paper's strengths to be in its structure, its empirical rigor in one of its pillars, and its timeliness.\n\n1. Clarity: The \"three-pillar\" framework (Environment, Policy, Reward) is a very clear and effective mental model. It’s a great way to structure the problem space, especially for newcomers or practitioners, which seems to be the target audience. It makes the paper easy to read and follow.\n2. Quality (The \"Environment\" Pillar): The analysis in Section 5 (Environment) is, in my opinion, the strongest part of this paper. The authors didn't just use standard benchmarks; they methodically controlled for different axes of complexity (spatial, object, quest length). The finding that \"object complexity proves more challenging than spatial complexity\" is a concrete, non-obvious, and valuable insight for anyone designing a training curriculum. The hyperparameter tuning detailed in Appendix A also shows solid, careful empirical work.\n3. Originality (of Motivation): I want to give the authors credit for having their finger on the pulse of the community. The motivation for Section 6.2—comparing biased vs. unbiased algorithms—is excellent. They correctly identify the very recent and important debate sparked by \"Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason\" (Oertell et al., 2025). Grounding their empirical comparison of PPO and RLOO in this very current theoretical dispute is a good piece of scholarship.\n4. Significance: The effort to validate the \"recipe\" across three very different domains (TextWorld, ALFWorld, and SWE-Gym) is commendable. This cross-domain validation is crucial if the paper is to live up to its \"practitioner's guide\" title, and it increases the potential impact of the findings."}, "weaknesses": {"value": "Despite the strengths, this paper suffers from several major, and I mean major, weaknesses related to novelty and its literature review. The research area of LLM agents is moving incredibly fast, and unfortunately, this paper has been \"scoped\" by several other key papers published in the first half of 2025.\n\nThese omissions are so significant that they call into question the paper's core contributions.\n\n1. Major Weakness 1: The \"Reward\" Pillar (Section 7) is Fundamentally Outdated. Frankly, this section is the paper's most significant weakness. The analysis only compares sparse rewards to the built-in dense rewards from TextWorld. This might have been an interesting question in 2023, but by 2025, the SOTA has moved far beyond this. We know dense rewards help; the critical research question now is how to generate fine-grained, step-wise credit when built-in rewards don't exist.\n\n   The paper completely fails to cite or compare against the entire wave of 2024-2025 research on this exact topic. This includes, but is not limited to:\n\n   - StepAgent (Deng et al., 2024), which generates step-wise rewards by comparing agent and expert policies.\n   - GiGPO (Feng et al., 2025), which introduced a \"Group-in-Group\" optimization for fine-grained, step-level credit assignment.\n   - SPA-RL (Choudhury et al., 2025), which uses a \"progress estimator\" to decompose the final reward into step-wise contributions.\n   - MT-GRPO (Zeng et al., 2025), which also introduced fine-grained turn-level advantage estimation.\n\n   For a \"practitioner's guide\" to ignore all of these SOTA reward mechanisms—the very techniques practitioners are trying to implement—is a fatal flaw. The \"recipe\" in this pillar is incomplete and already obsolete.\n\n2. Major Weakness 2: The \"Policy\" Pillar (Section 6) Lacks Novelty. This is the second major blow to the paper's contribution. The two core experimental questions asked in this section were both answered by other papers.\n\n   - On the SFT:RL Ratio (Sec 6.1): The authors explore the \"optimal SFT:RL ratio\" by testing about 6 configurations (Table 7). However, arXiv:2507.04103v1 is a large-scale study that does exactly this, running 1,370 training configurations to find the \"optimal compute allocation and hyperparameter mix\". This paper's small-scale analysis is completely overshadowed and can only be considered a minor validation, not a novel contribution.\n   - On Algorithm Comparison (Sec 6.2): The paper's comparison of PPO, GRPO, and RLOO is also not novel. arXiv:2507.14897v1 explicitly states that it \"evaluate[s] four RL algorithms:... PPO,... GRPO, and RLOO\". This is a direct experimental overlap. The authors' main contribution in this section has been done concurrently (or, from the perspective of this submission, previously).\n\n3. Major Weakness 3: The \"Systematic Framework\" Claim is Incorrect. The paper's abstract and introduction claim to provide the first \"systematic formulation\" for this problem. This is simply not true.\n\n   - RAGEN (Wang et al., 2025), explicitly introduced the StarPO framework and the RAGEN system as a \"research infrastructure for systematic analysis\" of multi-turn RL agents.\n   - The authors do cite RAGEN, but they dismiss it in a single line (\"rely on sparse terminal rewards\" - line 106), which isn't even fully accurate, as the RAGEN paper discusses the need for fine-grained rewards. The authors fail to engage with this prior work and do not clearly differentiate their \"three-pillar\" framework conceptually from the RAGEN/StarPO framework.\n\n4. Major Weakness 4: No SOTA Baseline Comparisons. For a paper that claims to be a \"guide,\" it does a poor job of benchmarking its own \"recipe.\" In Table 5 (ALFWorld) and Table 6 (SWE-Gym), the results are presented in a vacuum.\n\n   - Where is the comparison to the SOTA on ALFWorld?\n   - SPA-RL (Choudhury et al., 2025) reported 79.1% success on unseen ALFWorld tasks. This paper's best result in Table 5 is 74% (SFT+PPO).\n   - GiGPO (Feng et al., 2025) also reported a >12% gain over GRPO on ALFWorld.\n   - The paper presents a recipe that is sub-SOTA and doesn't even acknowledge, let alone discuss, why. This is a critical omission."}, "questions": {"value": "1. (Reward Pillar): Your analysis in Section 7 is limited to sparse vs. built-in dense rewards. Can you please justify why you completely omitted any discussion or comparison with the entire line of work on learned or decomposed step-wise rewards, such as StepAgent (Deng et al., 2024), GiGPO (Feng et al., 2025), and SPA-RL (Choudhury et al., 2025)? How can this be a \"practitioner's guide\" if it ignores the SOTA reward-design techniques?\n2. (Policy Pillar - Algos): This is a direct question about novelty. Given that arXiv:2507.14897v1 already provides an empirical comparison of PPO, GRPO, and RLOO, what new insight does your Section 6.2 analysis provide that is not already present in that work? To be very clear, you need to explain what your contribution is over and above that paper.\n3. (Policy Pillar - Ratios): Similarly, how do you position your findings on the SFT:RL ratio (Sec 6.1) against the large-scale, 1,370-run study in arXiv:2507.04103v1? Are your findings (from ~6 runs) simply a small-scale confirmation, or do you arrive at a different conclusion?\n4. (Framework Claim): Can you please elaborate in detail on the conceptual novelty of your \"three-pillar\" framework compared to the \"systematic analysis\" framework proposed by RAGEN (Wang et al., 2025)? The current dismissal in your related work section is insufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6uYt9jHxXl", "forum": "K6T0o875zF", "replyto": "K6T0o875zF", "signatures": ["ICLR.cc/2026/Conference/Submission14757/Reviewer_WXhe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14757/Reviewer_WXhe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964119148, "cdate": 1761964119148, "tmdate": 1762925114851, "mdate": 1762925114851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides an empirical recipe for training LLM agents using multi-turn RL. Specifically, the paper considers the impact of different designs of environment, reward, and training algorithm on overall performance, ultimately showing that dense turn-level rewareds and prior SFT training greatly accelerate RL training and reduce sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does a systematic evaluation of multiple facets of RL training to create a comprehensive blueprint that will be generally useful to practitioners trying to train LLM agents. \n\n2. The evaluation is done over three very different textual domains, which greatly increases the generalizability of the proposed blueprint. \n\n3. The analysis on the usefulness of prior SFT training and the optimal SFT:RL data ratio will be useful for other applications."}, "weaknesses": {"value": "1. I found the analysis of biased (PPO, GRPO) vs unbiased (RLOO) algorithms to be insufficient. The authors claim the improvement is due to multi-turn training, but it seems evident that algorithm choice has a noticeable effect. However, this difference in performance between different algorithms is not analyzed further or explained. \n\n2. The finding that dense reward is better than sparse rewards was only based on evidence from simple tasks. I believe testing dense vs. sparse rewards on the complex tasks is important to truly determine the usefulness of converting sparse to dense rewards. \n\n3. The authors claim that SFT training can cause rapid collapse when fine-tuned on a different environment. However, the evidence for this is on one particular policy size and architecture. It is unclear if increased model capacity still results in the same instability."}, "questions": {"value": "1. What is the effect of prior SFT training when the data is not only expert demonstrations, but instead consists of some noisy or suboptimal examples? \n\n2. The paper currently shows that pretraining on one particular complex task generalizes poorly to other complex tasks. Does the specific task used have effect on the results? I am curious of what would happen if the pretraining was only on specific skills that ultimately needed to be composed. \n\n3. The authors showed that PPO outperforms RLOO but does not give a reason why. It could be in the scaling of the rewards, as RLOO is more sensitive to the magnitude of rewards. Could perhaps increasing the scaling of the reward make the performance of RLOO match that of PPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WnwSDPedw8", "forum": "K6T0o875zF", "replyto": "K6T0o875zF", "signatures": ["ICLR.cc/2026/Conference/Submission14757/Reviewer_vYsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14757/Reviewer_vYsC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762326756982, "cdate": 1762326756982, "tmdate": 1762925114180, "mdate": 1762925114180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a systematic study of design choices for multi-turn agentic reinforcement learning. The authors first distinguish their setting from pseudo multi-turn methods and decompose their design space into three key components: environment, policy, and reward. For the environment, they analyze the impact of environment complexity and how agents trained via multi-turn RL generalize to harder complexities. For the policy, they discuss the influence of initial policies, the impact of different SFT/RL data ratios, and the effectiveness of different RL algorithms in multi-turn settings. For the reward, they study the impact of sparse/dense reward. Through extensive experiments on TextWorld, ALFWorld, and SWE-Gym, the authors derive a practical training recipe for multi-turn agentic RL with respect to the above three components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well written and easy to follow, with a good overall structure and clear motivation.\n\n2. Figure 1 provides an easy-to-understand overview of the work, making it easy for readers to understand the overall idea.\n\n3. The paper clearly distinguishes different types of “multi-turn” RL, making the scope of the work precise and well defined.\n\n4. This paper conducts a large number of experiments to validate many important design choices for agentic RL, which could be very helpful for the research community."}, "weaknesses": {"value": "1. Some conclusions seem to be closely tied to the experimental environment, therefore limited in their generalizability. For example, the concept of spatial complexity seems to be closely related to embodied reasoning, but this concept may not be always available in other agentic environments.\n\n2. The authors try to cover many different design choices in their paper. However, some of them are supported by only one experiment in a single environment, making it hard to draw general conclusions from these experiments. For example, the experiment on sparse/dense reward is only conducted in TextWorld.\n\n3. This paper is well written in terms of text, but other aspects of the presentation could be improved. For example, Tables 5 and 6 could be merged into one table to make the presentation more concise, as they discuss the same topic. Additionally, the corresponding table for lines 374-377 seems to be missing. It would also be better to increase the diversity of the presentation, all experimental results are now presented as tables. Some of them could be converted into figures to make the paper more visually appealing.\n\n4. In general, I appreciate the effort the authors have made to cover various design choices and supporting their claims with a substantial number of experiments. However, as a PRACTITIONER’S GUIDE (as indicated by the title of this paper), I believe the number of experiments still needs to be increased both in terms of breadth and depth. Specifically, more combinations of factors, such as different RL algorithms, experimental environments, model families, and model sizes, should be explored to provide more solid support for their claims."}, "questions": {"value": "1. The performance of Qwen-1.5B in w2-o3-q4 is different in Tables 1 and 2 (0.17 and 0.15). Is that just a typo, or do I misunderstand the setting?\n\n2. What's the base model for Tables 4, 5, and 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qzR03H6jIY", "forum": "K6T0o875zF", "replyto": "K6T0o875zF", "signatures": ["ICLR.cc/2026/Conference/Submission14757/Reviewer_N61V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14757/Reviewer_N61V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762342756420, "cdate": 1762342756420, "tmdate": 1762925113739, "mdate": 1762925113739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}