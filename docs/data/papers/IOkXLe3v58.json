{"id": "IOkXLe3v58", "number": 14714, "cdate": 1758242390415, "mdate": 1759897353354, "content": {"title": "Struct-G: Structural-Aware Pretraining for Graph and Task Transfer Learning", "abstract": "Transfer learning has revolutionized domains like vision and language by enabling pretrained models to adapt rapidly with minimal supervision. However, applying transfer learning to graph‑structured data faces unique challenges: graphs exhibit diverse topology, sparse or heterogeneous node attributes, and lack consistent semantics across datasets, making it difficult to learn representations that generalize across domains. Recent graph pretraining efforts including generative methods and contrastive objectives have shown promise but often rely on complex architectures, rich feature modalities, or heavy computation, limiting their applicability to structure‑only graphs and resource‑constrained settings.\nTo address these challenges, we introduce Struct-G, a lightweight pretraining framework that decouples global topology capture from local feature refinement. Struct-G first computes shallow random‑walk–based structural embeddings, then fuses them with raw attributes via an adaptive, feature‑wise gating network and a shared message‑passing backbone. By jointly optimizing multiple self‑supervised objectives such as link prediction, node classification, feature reconstruction, and structural alignment, Struct-G learns robust node embeddings that transfer effectively with minimal fine‑tuning. Our extensive experiment results demonstrate that explicit structural inductive bias and self-supervised multi-task learning provide a scalable and accessible foundation for graph representation learning.", "tldr": "A graph-structure based pre-training model with good results on graph tasks.", "keywords": ["Graph", "Pre-training", "multi-task", "transfer learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aaab471dc642d830c4512c7677c8845544b2390f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces, Struct-G, a framework that includes both a model architecture and a unique training objective. The model architecture uses Node2Vec to form structural embeddings, fuses them using a gated mechanism, and further processes the result through existing backbone models such as GCNs or GraphSAGE. The training objective is novel, and is basically a mixture of previously used SSL losses with an addition of a \"structure-embedding <-> final feature alignment loss.\"\nEvaluation is performed on Node Classification and Link Prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Shows the effectiveness of including multiple tasks in the SSL loss\n- The paper introduces a new gated operation to fuse node and position features (see weaknesses)"}, "weaknesses": {"value": "Within the use of **Node2Vec**:\n- Node2Vec is NOT this paper's contribution and yet is written to be so.\n- Node2Vec is just another structural embedding, and there are quite a few other options - e.g. Laplacian Eigenvectors [1], RWPE [2]. The paper makes a big deal about using Node2Vec but never ablates it with a comparison using another.\n\n**Gating mechanism that fuses structural encoding and node features**\n- The formulation presented seems more complicated than it really is. If we break $W_c$ as $W_c = [W_{x} || W_{z}]$, with $W_{x} \\in \\mathbb{R}^{d_h \\times d_x}$ and $W_{z} \\in \\mathbb{R}^{d_h \\times d_h}$ then the gating mechanism's equation can be simplified as:\n\n$h^{(0)}_v = g_v \\odot \\hat{h}_v + (1 - g_v)\\odot \\check{h}_v$\n\n$= g_v \\odot (W_x x_v + W_{cz} \\tilde{z}_v) + (1 - g_v) \\odot W_x x_v$\n\n$= W_x x_v + g_v \\odot W_z\\tilde{z}_v$\n\n$= W_x x_v + g_v \\odot W_z W_\\text{proj}z_v$\n\nCan the authors please explain why they chose to formatulate the operation in the way it is currently? I find the above simpler formulation to be easier to read. Additionally, the formulation above reveals the presence of redundant parameters. The $W_z$ and $W_\\text{proj}$ are two linear layer parameters that don't need be separate. A single linear layer would be able to learn the same map with less total parameters. You can find a similar redundancy in the interaction of $W_g$ and $W_\\text{proj}$ as well.\n- Since this is one of the main contributions of the paper, I believe it needs a solid proof to be an improvement over simpler alternatives. The idea of fusing structural and node features is not new, people do it with simple linear layers and MLPs the concatenated $[x_v, z_v]$. Only the use of gated mechanism is new, and thus, must be ablated.\n\n**Multi-objective Loss**\n- Lack of explaination for why the feature alignment loss does not collapse. Since both $e_v$ and $z_v$ are passed through independent MLPs, what is stopping the two MLPs to collapse and output a constant vector? Can the authors confirm that the collapse does not happen?\n\n**Evaluation**:\n- Why was only one graph evaluated for transfer learning? \n- The evaluation of Twitch-RU is not included in Tables 3 and 4, making it difficult to understand the impact of pretraining on Twitch-ES in Table 5.\n- The ablation experiments have no measure of variance. Thus, I cannot trust any inference made from these ablations. For e.g. the improvement delta from N2Valign loss seems small, requiring an appropriate meausre of variance to see if it actually had an impact.\n\n[1] https://arxiv.org/pdf/2012.09699\n\n[2] https://arxiv.org/pdf/2110.07875"}, "questions": {"value": "- The multi-task objective introduces a lot of hyperparameters in the weighting of different loss components. How were these hyperparameters tuned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "339eVN0WxA", "forum": "IOkXLe3v58", "replyto": "IOkXLe3v58", "signatures": ["ICLR.cc/2026/Conference/Submission14714/Reviewer_pJMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14714/Reviewer_pJMJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862526866, "cdate": 1761862526866, "tmdate": 1762925076373, "mdate": 1762925076373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents STRUCT-G, a structure-aware graph pretraining framework that integrates frozen Node2Vec embeddings with an adaptive gating mechanism to combine structural and feature information. The model jointly optimizes multiple objectives—node classification, link prediction, feature reconstruction, and structural alignment—to learn transferable graph representations. Experiments on several datasets demonstrate performance comparable to standard GNN baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important challenge in graph learning, transferring knowledge across graphs that differ in structure and feature space.\n* The combination of node classification, link prediction, feature reconstruction, and structural alignment losses offers a unified approach to balancing supervised and self-supervised objectives.\n* STRUCT-G achieves competitive or better performance than common GNN baselines across multiple datasets."}, "weaknesses": {"value": "* The experimental section is limited, as it lacks ablations for the first two main contributions — the structural encoding step and the adaptive gating mechanism. Without these analyses, it is difficult to assess the actual impact and necessity of these components.\n* The transfer experiments are restricted to a single pair of highly similar graphs (Twitch-ES → Twitch-RU), which does not convincingly demonstrate generalization. Furthermore, no results are shown for STRUCT-G trained from scratch on these datasets, making it unclear how much benefit transfer actually provides.\n* The experiments are conducted on relatively uncommon graph benchmark (Facebook, Deezer, GitHub, Email-EU), which makes it difficult to compare results with prior work.\n* Tables 5, 6, and 7 do not report standard deviations or variance across runs, making it difficult to draw strong or reliable conclusions from the ablation and transfer experiments.\n\nOverall, most of the claims made by the paper are not supported by solid empirical evidence due to the lack of ablation studies, the use of non-standard benchmarks, and the absence of standard deviation reporting in the results."}, "questions": {"value": "* I’m confused about the “lightweight structural encoding” contribution. It seems the paper just uses precomputed Node2Vec embeddings without modification. Why is this considered a key contribution rather than an implementation choice? Could the authors include an ablation comparing different structural embeddings (like laplacian)?\n* Could the authors include an ablation for the proposed gating mechanism? It would be helpful to compare it with simpler fusion methods (e.g., direct concatenation or addition) to demonstrate its impact.\n* The experiments are conducted on relatively uncommon datasets. Could the authors include results on more standard benchmarks such as Cora, PubMed, Amazon-CS, or Amazon-Photo to make comparisons with prior work more meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ix5rhbXjel", "forum": "IOkXLe3v58", "replyto": "IOkXLe3v58", "signatures": ["ICLR.cc/2026/Conference/Submission14714/Reviewer_zor5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14714/Reviewer_zor5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138876356, "cdate": 1762138876356, "tmdate": 1762925075963, "mdate": 1762925075963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STRUCT-G, a pretraining framework for graph transfer learning aiming to integrate structural information into GNNs. The framework combines random-walk–based structural embeddings with raw node features through a gating mechanism and trains the model using multiple self-supervised objectives: link prediction, node classification, feature reconstruction, and structural alignment."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a well-motivated challenge: enabling effective transfer learning on graph-structured data and also explains why existing pretraining methods struggle with heterogeneous graph structures and sparse attributes.\n\nThe proposed feature-wise gating module is a good contribution that allows the model to dynamically balance structural and semantic information and potentially improves robustness across different graph types and downstream tasks.\n\nThe combination of multiple self-supervised objectives, such as link prediction, feature reconstruction, and structural alignment, can provide complementary learning signals that enhance representation quality and transferability."}, "weaknesses": {"value": "- While the combination is well-motivated, many parts of STRUCT-G (e.g., random-walk embeddings, the GraphSAGE backbone, and multi-task SSL objectives) are based on known techniques. The main novelty lies in integration rather than in introducing new theoretical or algorithmic advances (not necessarily a weakness, but can be improved)\n\n- The paper claims strong transferability across graph tasks and datasets, but cross-graph experiments (e.g., Twitch-ES –> Twitch-RU) are limited in scope, as the selected graphs are very similar. Broader evaluations across more diverse domains or unseen graph distributions would be better for the “transfer learning” claim.\n\n- The adaptive gating mechanism is a key innovation, but the paper does not provide sufficient experimentation or interpretability analysis showing how gates behave across nodes, tasks, or structural regimes.\n\n- While the ablation study is good regarding self-supervised objectives, it does not isolate the effects of structural embeddings or gating in depth (e.g., what happens if gating is replaced with concatenation or attention).\n\n- The results convincingly show improved node classification and link prediction, but some other claims, such as “lightweight general foundation for graph transfer learning,” are somewhat overstated given the limited range of transfer and multi-domain tests."}, "questions": {"value": "1- The gating module is presented as a key contribution, but its behavior is not fully explored. Can the authors include ablations or statistics (e.g., gate activation distributions, correlation with node degree or feature quality) to show how gating adapts across tasks and structural regimes?\n\n2- STRUCT-G claims to be a lightweight foundation module. Can the authors provide clearer runtime or complexity comparisons (e.g., O-notation or empirical scaling trends) to quantify where the computational savings arise?\n\n3- It is not clear how critical random-walk embeddings are relative to feature-based learning. Could the authors provide an ablation comparing STRUCT-G with and without frozen Node2Vec embeddings, or using alternative structural encodings (e.g., Laplacian, PPR,...)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aZ3cKuoTfH", "forum": "IOkXLe3v58", "replyto": "IOkXLe3v58", "signatures": ["ICLR.cc/2026/Conference/Submission14714/Reviewer_Tnpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14714/Reviewer_Tnpf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409664334, "cdate": 1762409664334, "tmdate": 1762925075389, "mdate": 1762925075389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}