{"id": "br3p10VNoA", "number": 22022, "cdate": 1758325004095, "mdate": 1759896890660, "content": {"title": "BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation", "abstract": "As structured texts become increasingly complex across diverse domains -- from technical reports to generative AI prompts -- the need for text segmentation into semantically meaningful components becomes critical. Such texts often contain elements beyond plain language, including tables, code snippets, and placeholders, which conventional sentence- or paragraph-level segmentation methods cannot handle effectively. To address this challenge, we propose BoundRL, a novel and efficient approach that jointly performs token-level text segmentation and label prediction for long structured texts. Instead of generating complete contents for each segment, it generates only a sequence of starting tokens and reconstructs the complete contents by locating these tokens within the original texts, thereby reducing inference costs by orders of magnitude and minimizing hallucination. To adapt the model for the output format, BoundRL performs reinforcement learning with verifiable rewards (RLVR) with a specifically designed reward that jointly optimizes document reconstruction fidelity and semantic alignment. To mitigate entropy collapse, it further constructs intermediate candidates by systematically perturbing a fraction of generated sequences of segments to create stepping stones toward higher-quality solutions. To demonstrate BoundRL's effectiveness on particularly challenging structured texts, we focus evaluation on complex prompts used for LLM applications. Experiments show that BoundRL enables small language models (1.7B parameters) to outperform few-shot prompting of much larger models. Moreover, RLVR with our designed reward yields significant improvements over supervised fine-tuning, and incorporating intermediate candidates further improves both performance and generalization.", "tldr": "we propose BoundRL, a novel and efficient approach that jointly performs token-level text segmentation and label prediction for long structured texts", "keywords": ["Text Segmentation", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14cc35cde5a5ee1a3a4391b9b1d67e15d5b48050.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose BoundRL, a theoretically grounded framework for efficient reinforcement learning that leverages structural bounds derived from Markov decision process (MDP) decomposability. Instead of learning value functions directly, BoundRL constrains the policy search space using upper and lower bounds on the true value function, defined by state abstraction and partitioned Bellman operators. The core theoretical claim (Theorem 4.3 and 4.5) proves that the approximation error of BoundRL is tightly bounded by the granularity of the state partition, providing guarantees on both sample efficiency and performance suboptimality. Experiments on MuJoCo and gridworld tasks show that BoundRL can outperform classical algorithms such as PPO and DQN, particularly in structured or decomposable environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Instead of regenerating every segment, the system outputs only segment-start tokens and labels, then reconstructs spans by locating those starts in the input. This cuts output from O(|d|) to roughly O(n) and lowers hallucination risk, which is a neat, practical reframing for structured documents. The paper spells out the locate-and-reconstruct process and the leftmost-occurrence rule to preserve order. \n\nThe RLVR reward mixes a reconstruction-ratio term with exact-match F1 and a character-level F1; the final reward is r = ρ_rec × (EM + F1_char)/2, so improvements must both recover the document and align semantically. They further tackle entropy collapse by constructing intermediate candidates via single-step shorten/extend/label-perturb operations and only keep them when they strictly improve reward. Empirically, this yields consistent gains over SFT and matches or surpasses alternative RL baselines."}, "weaknesses": {"value": "Most training/evaluation is on LLM prompts, with a large synthetic split (14,732 train prompts) and a relatively small real-world split (197 prompts). While results on the Langchain set are encouraging, the out-of-domain test size is modest, so claims about generality to other structured texts (e.g., legal or technical docs) remain tentative. \n\n\n  The method reconstructs segments by locating generated start-token sequences; if either boundary cannot be found, the segment is discarded. This simple policy is easy to implement but may fail under noisy formatting or repeated phrases where the “unique start” assumption breaks down. The paper itself enforces unique start sequences during SFT to improve robustness, hinting at the necessity of this constraint.\n\n\n   RLVR training uses m = 4 rollouts per input at temperature 1.2 and only 25% of the training data for cost reasons, then decodes at temperature 0. These choices are reasonable, but the paper offers limited sensitivity studies (beyond a higher-temp variant) on how r’s weighting, rollout count, or selection threshold k affect stability and final quality. The selection rule admits intermediate candidates only when r_gain > 0, with model-specific k, but lacks a theoretical or principled schedule."}, "questions": {"value": "1. How robust is the boundary-generation scheme when the same start token sequence appears multiple times or when whitespace/formatting gets normalized (e.g., in code blocks)? The paper enforces unique start sequences during SFT, but what happens in the wild without that guarantee, and can the locator incorporate fuzzy matching to reduce discards?\n\n2. The selection rule accepts an intermediate candidate only if it yields a positive reward gain and up to k per batch. Could a curriculum or confidence-weighted scheme (e.g., variable k or a margin-based r_gain threshold) further reduce entropy collapse and improve generalization, especially on the small real-world split?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EvMLGDLZac", "forum": "br3p10VNoA", "replyto": "br3p10VNoA", "signatures": ["ICLR.cc/2026/Conference/Submission22022/Reviewer_6Rd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22022/Reviewer_6Rd1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864770464, "cdate": 1760864770464, "tmdate": 1762942022013, "mdate": 1762942022013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model for text segmentation. The approach is standard SFT+RL (GRPO), but the paper emphasizes (1) the novelty of predicting only the boundaries to reduce cost, and (2) careful reward design with data augmentation to prevent entropy collapse. The paper also introduces its own text segmentation dataset and shows that the proposed approach performs competitively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The model is simple. \n- The reward design (i.e., verifiability of reconstruction) is interesting and requires some ingenuity. Manually augmenting the search space is empirically sensible.\n- The paper contributes a new dataset."}, "weaknesses": {"value": "- Token-level boundary prediction with LLMs is a topic that's been explored in previous works, and not as novel as the paper makes it sound. E.g., [1] generates only the mentions in whole documents for coreference resolution, which can be viewed as a harder version of text segmentation with no label annotation. While combining this with careful RL in the specific context of text segmentation is of course novel, the paper should do a more general acknowledgement of this broad approach to this particular problem.\n- It seems a bit odd that the results are only on the proposed new dataset. Shouldn't the paper include standard existing benchmarks for the task?\n- It's a little hard to tell from Table 3 that the baselines represent the state-of-the-art. RL-PLUS is the only meaningful third-party baseline, and others are just in-house ablations, it seems? For someone who doesn't have a background on recent text seg research, it's not clear how strong the performance win is.\n\n[1] Seq2seq is All You Need for Coreference Resolution (Zhang et al., 2023)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3u8O2A6iVZ", "forum": "br3p10VNoA", "replyto": "br3p10VNoA", "signatures": ["ICLR.cc/2026/Conference/Submission22022/Reviewer_wraZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22022/Reviewer_wraZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922417849, "cdate": 1761922417849, "tmdate": 1762942021766, "mdate": 1762942021766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper applies a variant of RLVR to boundary detection in language model prompts. It shows that its method of only generating tokens that indicate boundaries is faster than using a prompt-based approach."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "In looking at text segmentation, the paper picks up a topic that nowadays receives less attention and may by many already (falsely) considered as solved. It focused on issues with blindly applying LLMs to the problem and points to an alternative that improves over that approach."}, "weaknesses": {"value": "At a high level, this paper feels like an engineering project that was overly quickly turned into the format of an academic paper. I am lacking rigor in baselines and proper justification for why many of the choices that were made.  \n\n**Why RL**\n\nThis paper presents an RLVR hammer and then turns text segmentation into a nail. The paper does not discuss why it is necessary to use RL for this approach. Even after reading it, I am left wondering if it is necessary to treat this as a text generation problem and why I should care about this approach at all. The related work at minimum should outline this argument.\n\n**Lack of Baselines and Datasets**\n\nThe probably strongest weakness of this paper is that it evaluates prompt text segmentation via a very limited set of experiments. There are a myriad classic text segmentation benchmarks with strong baselines for scientific papers, corporate filings, emails, and many others. There is no justification given beyond \"prompts are complex\". \n\n6.2 is discussing how efficient BoundRL is but it is generating a significant 119 tokens when a classic sequence tagging approach would not have to generate any of this. While the NER baseline is discussed as scoring lower and achieving only fragmented, short segments, this is in disagreement with the entire body of literature. I strongly suspect a faulty implementation. For, example, the NER baseline uses an autoregressive model. This has historically not worked as well as dedicated sequence labeling setups. But, due to the lack of baselines, it is impossible to assess this. \n\nEven if no classic models are explored, I would have expected baselines based on LayoutLM and similar models aimed to improve document understanding.\n\n**Lack of humans in the process**\n\nThe paper relies to a large degree on synthetic data. The only test on human-created prompts is via the langchain hub. But this dataset is not described or analyzed and there is no human evaluation of the results. Given that the langchain hub has templates, it will be biased in many ways. In addition, the distribution over topics is very top-heavy with a focus on certain types of agents over others."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mdXcmk3ZO8", "forum": "br3p10VNoA", "replyto": "br3p10VNoA", "signatures": ["ICLR.cc/2026/Conference/Submission22022/Reviewer_8hZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22022/Reviewer_8hZx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933169065, "cdate": 1761933169065, "tmdate": 1762942021584, "mdate": 1762942021584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for text segmentation of structured text that reduces the computational load on models as well as improving performance. The authors frame the problem as boundary identification and task a model to identify where various roles start and end throughout the text, assigning these segments the appropriate label. They then use these labels and segments to segment the entire text. They observe that by using this method they are able to improve the efficiency of text segmentation. They also observe that their method is able to segment text more accurately than few-shot prompted Claude models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is well described, and the paper is clearly written. The problem is relevant. The method improves significantly in terms of efficiency and does improve performance when compared to Claude models."}, "weaknesses": {"value": "1. While the improvements show in table 3 are significant, the method improves over Claude inconsistently across metrics. It would be good to clarify why this might be and offer some explanation as to which metrics are the most important.\n\n2. While there are lots of experimental results comparing different settings of BoundRL and different fine-tuning paradigms, and there are experimental results comparing to few-shot prompting of Claude, the performance is not compared to methods from prior work. It would be good to see how both performance and efficiency compares to these methods."}, "questions": {"value": "1. Do you have any insights for the differences in metric improvement in Table 3?\n\n2. Why do you choose few-shot prompting of Claude to compare against vs other prior work?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Annotators are used, but I see no discussion of the rate they were paid."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eZTqk95oVk", "forum": "br3p10VNoA", "replyto": "br3p10VNoA", "signatures": ["ICLR.cc/2026/Conference/Submission22022/Reviewer_1KdK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22022/Reviewer_1KdK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22022/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010647079, "cdate": 1762010647079, "tmdate": 1762942021314, "mdate": 1762942021314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}