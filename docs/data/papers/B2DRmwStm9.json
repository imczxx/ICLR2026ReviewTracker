{"id": "B2DRmwStm9", "number": 3865, "cdate": 1757557930044, "mdate": 1759898065684, "content": {"title": "QuS: Towards High-Performance EfficientViT on FPGA by  Quantization and Streamline Co-Design", "abstract": "Vision Transformer (ViT) has achieved significant success in computer vision, in which EfficientViT is widely used because of its lightweight characteristics. \nHowever, EfficientViT is still difficult to deploy on edge devices like FPGA because of its efficiency and accuracy concerns. \nFirst, from software perspective, existing quantization approaches fail to consider the inter-channel distribution relationship, which cause significant performance degradation under lower-bit setting. \nSecond, from hardware perspective, current DSP-packing methods struggle to support the diverse kernel sizes and strides of convolutions used in EfficientViT, resulting in redundant computation cycles or bit-width overflow.\nMoreover, due to the mismatch in data layouts between convolution and linear attention, existing solutions require substantial memory resources for data reordering, which often results in pipeline stalling.\nIn this paper, we propose a Quantization and Streamline Co-Design (QuS) framework for lower-bit EfficientViT deployment on FPGA.\nIt includes three main components: adaptive distribution-aware quantization strategy to provide effective quantization, multi-computing in once packing strategy to improve the DSP-packing efficiency, and low-buffer streamline for linear attention scheme to eliminate pipeline stalling caused by mismatched layout.\nExperimental results show that our QuS framework achieves over 2200 FPS on EfficientViT, which represents a $3.6\\times$ speedup over Jetson AGX Orin and also up to a $24\\%$ accuracy improvement under 4-bit quantization.", "tldr": "", "keywords": ["Quantization", "ViT", "Efficient", "Accelerator"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60c580940fe4a79c8833ac505249e9fea9ccd807.pdf", "supplementary_material": "/attachment/db463ad42a4e3702e780ecb98cdc07ad826855bf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the QuS framework for efficiently deploying the EfficientViT model on edge devices such as FPGAs.\n\nTo address the accuracy degradation that occurs at low bit depths (e.g., 4-bit), a software-based method called ADAQ is introduced. ADAQ dynamically and effectively migrates activation outliers to weights based on each channel’s variance coefficient (VC) and employs MSA to smoothly optimize weight rounding.\n\nOn the hardware side, a novel MuCO DSP-packing strategy is proposed to efficiently support convolutions in EfficientViT with various kernel sizes and strides. This method selectively groups and loads inputs according to the stride, and divides large kernels into segments to prevent DSP overflow while maintaining computational efficiency.\n\nFurthermore, to eliminate the transpose bottleneck caused by mismatches in data layout between convolution and linear attention, an LBS architecture is designed.\n\nExperimental results show that the proposed QuS framework achieves 2257 FPS at 300 MHz on the ZCU102 FPGA board, providing a 3.6× speedup over the Jetson AGX Orin and achieving up to 24% accuracy improvement in 4-bit environments. \n\nThese results demonstrate that QuS is an effective software–hardware co-design framework for low-bit quantization and FPGA-accelerated EfficientViT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed QuS framework presents a co-design approach that comprehensively optimizes both software and hardware to address the deployment challenges of low-bit EfficientViT models on edge devices. Through this design, QuS minimizes accuracy degradation even under 4-bit post-training quantization (PTQ) settings, while simultaneously achieving a 3.6× speedup compared to Jetson AGX Orin.\n\n2. The proposed ADAQ quantization method introduces a new metric called the Variance Coefficient (VC) for each channel, overcoming the limitations of prior methods that applied fixed-strength migration of activation outliers. By adaptively balancing data distribution, ADAQ effectively mitigates quantization outlier sensitivity in lightweight models using depthwise/pointwise (DW/PW) convolutions, thereby preserving model performance even under low-bit quantization.\n\n3. The proposed MuCO DSP-packing strategy and LBS design provide practical contributions that address major bottlenecks in conventional hardware accelerator architectures. MuCO overcomes the constraints of traditional DSP-packing methods optimized only for conv3×3s1, enabling efficient support for a broader range of kernel sizes and strides. In addition, the LBS design eliminates transpose overhead between convolution and attention modules, resulting in significant on-chip memory savings and improved pipeline efficiency."}, "weaknesses": {"value": "1. In Table 2, the GOPs/DSPs values of HeatViT and HG-PIPE are inconsistent with those calculated from the GOPs and DSPs figures reported in their original papers, making a fair comparison with prior studies impossible. Moreover, although the paper assumes an edge-device deployment scenario, its reported power consumption, LUT utilization, and DSP usage are relatively higher than those of existing works. These issues weaken the authors’ claim that their approach is suitable for edge-device solutions and call for a more detailed analysis.\n\n2. Although the paper mentions that specific critical layers were kept in 8-bit precision under the 4-bit setting, it does not specify which layers were applied or what proportion they represent. Since only the average bit-width is reported, it is unclear whether the observed performance improvement in the 4-bit setting stems from the ADAQ algorithm itself or from retaining 8-bit precision in the critical layers.\n\n3. The authors identify the inter-channel distribution imbalance in EfficientViT as the primary motivation for ADAQ; however, this issue could be more directly addressed through channel-wise quantization. Despite this, the authors rely solely on tensor-wise quantization for hardware efficiency. Yet, they provide no quantitative trade-off analysis showing how much additional hardware overhead channel-wise quantization would incur relative to tensor-wise quantization. The absence of such analysis weakens the claim of the ADAQ algorithm's necessity.\n\n4. The methods employed in the proposed FPGA implementation have already been utilized or discussed in prior accelerator designs, which makes their novelty somewhat limited. Moreover, since these techniques are not specific to EfficientViT and could be applied to other architectures as well, it is somewhat disappointing that the results are presented only for EfficientViT.\n\n5. The paper contains several typos and incorrect figures. In Fig. 4(a), the computation of w0 * x2 is incorrectly shown at Cycle #0, and the corresponding description also presents the wrong operation order. For example, in Appendix A.2, the term “Block Convolution” is misspelled as “bloack convolution.”"}, "questions": {"value": "1. Are the GOPs/W and GOPs/DSP values in Table 2 correct? They differ from the values obtained by dividing the reported GOPs by Power and DSP, respectively, so a clarification or verification is needed.\n\n2. Regarding the ADAQ's MSA weight reconstruction step (20,000 iterations), could the authors provide: (1) The accuracy convergence curve plotted against the number of iterations? and (2) The total time required to complete this entire calibration step?\n\n3. Although the paper mentions that certain critical layers were kept in 8-bit precision under the 4-bit setting, the explanation about which specific layers were applied and what proportion they represent is missing. Could you please provide a more detailed explanation of this part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pmqWmGV9mw", "forum": "B2DRmwStm9", "replyto": "B2DRmwStm9", "signatures": ["ICLR.cc/2026/Conference/Submission3865/Reviewer_iTdT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3865/Reviewer_iTdT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707017623, "cdate": 1761707017623, "tmdate": 1762917074072, "mdate": 1762917074072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents QuS, a novel framework for deploying the EfficientViT vision transformer on FPGAs using a software-hardware co-design approach. The novel contribution is a tightly integrated system that pairs a new quantization algorithm with a custom hardware accelerator to enable high-performance inference at low bit-widths (4-bit). The core methodology involves three components: a software-based Adaptive Distribution-Aware Quantization (ADAQ) algorithm that uses a channel's statistical properties to preserve accuracy; a hardware-based Multi-Computing in Once Packing (MuCO) strategy to efficiently process diverse convolution types on FPGAs; and a Low-Buffer Streamline (LBS) hardware design to resolve data layout mismatches between model layers, eliminating pipeline stalls."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is a good example of holistic system optimization. Instead of treating the quantization algorithm and the hardware accelerator as separate problems, it presents an integrated solution where the software (ADAQ) is designed to produce a representation that the custom hardware (MuCO and LBS) can execute with maximum efficiency.  \n* The QuS framework achieves a throughput of over 2200 FPS, which is a 3.6x speedup compared to a high-end edge GPU like the Jetson AGX Orin. It also demonstrates remarkable accuracy recovery at an aggressive 4-bit precision, a setting where baseline methods completely fail.\n* The MuCO strategy for handling convolutions with varied kernel sizes and strides is presented as a generalizable template, not just a solution for EfficientViT. This extends the potential impact of the research to other modern neural network architectures that use unconventional operators."}, "weaknesses": {"value": "* The performance comparison is primarily focused on designs on the same ZCU102 FPGA platform. The paper lacks a broader comparison against newer FPGAs, such as the VCK190. This is a notable omission compared to some baseline frameworks that have published results demonstrating much higher performance on the VCK190 platform, which would have provided a more challenging and relevant benchmark. \n\n* The paper uses the GOPs/DSP metric to claim superior hardware efficiency. However, this metric may not provide a complete picture, as some baseline frameworks use a normalized value for this calculation since their framework utilizes much more LUTs. A direct numerical comparison of GOPs/DSP can be misleading because it doesn't account for the computational work performed by non-DSP logic resources, and comparing a directly calculated value against a normalized one is not an apples-to-apples evaluation. Metrics like GOPs/kLUT, which could offer a more balanced assessment, are also not provided."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2MjbfHMYiE", "forum": "B2DRmwStm9", "replyto": "B2DRmwStm9", "signatures": ["ICLR.cc/2026/Conference/Submission3865/Reviewer_pjAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3865/Reviewer_pjAq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943862905, "cdate": 1761943862905, "tmdate": 1762917073743, "mdate": 1762917073743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents QuS, a quantization and streamline co-design framework for deploying EfficientViT on FPGA under low-bit (4-bit) precision. \nQuS introduces Adaptive Distribution-Aware Quantization (ADAQ), which uses a Variation Coefficient (VC) to measure channel-wise activation variability and dynamically adjust migration strength, improving low-bit quantization fidelity. The authors further propose Multi-Computing in Once (MuCO), a generalized DSP-packing scheme supporting arbitrary kernel sizes and strides, maximizing 4-bit parallelism without overflow. Low-Buffer Streamline (LBS) is proposed to address the shape flow of linear attention. \nThe speed gain is verified on Xilinx ZCU102 FPGA, and the authors report image classification and segmentation results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and relevant problem, i.e., deploying lightweight ViT architectures on resource-limited FPGAs.\n\n2. Results are clearly measured on real hardware with throughput, resource, and power metrics.\n\n3. The proposed pipeline (ADAQ + MuCO + LBS) is cohesive and shows reasonable engineering competence.\n\n4. The experimental section is fairly complete, including ablations and small generalization tests on MobileNetV2."}, "weaknesses": {"value": "1. My biggest concern is novelty. Overall, this work is a bit engineering, and some proposed methods are a bit trivial. ADAQ is essentially SmoothQuant with per-channel adaptation, and the VC-based scaling factor is a simple heuristic. MuCO extends existing DSP-packing methods (DSP-packing4/6, HiKonv) to a few new stride/kernel cases, which is an incremental engineering improvement. LBS removes explicit transposes by keeping the same data layout, but is a routine optimization. \n\n2. This work focuses on EfficientViT, but lacks a good justification for that. The local part of EfficientDiT, i.e., point-wise and depth-wise convolutions, is well established and highly optimized in the literature. On the other hand, for the linear attention part, though it demonstrates appealing results in some recent work, such as SANA, it is still believed to be inferior to softmax attention. In this work, linear attention is the optimization target, but why is softmax attention not good for FPGA? Is higher resolution inference achieved through using linear attention? It is better to show some comparisons and justifications."}, "questions": {"value": "1. The frequency is set up differently in Table 2. How do the claimed gains hold under equal clock frequency and identical resource budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6tlJukV6uT", "forum": "B2DRmwStm9", "replyto": "B2DRmwStm9", "signatures": ["ICLR.cc/2026/Conference/Submission3865/Reviewer_Lw3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3865/Reviewer_Lw3w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992487274, "cdate": 1761992487274, "tmdate": 1762917073406, "mdate": 1762917073406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes a quantization and an accelerator co-design for low-precision EfficientViT on FPGAs. Based on channel distribution variation, the authors propose the Adaptive Distribution-Aware Quantization (ADAQ) to improve the existing SmoothQuant approach, which adjusts hyper-parameters based on a metric called Variation Coefficient. In addition to that, they propose a fine-tune method, Multi-level Soft Approximation (MSA), to facilitate weight quantization. In hardware design, to satisfy the architecture requirement of EfficientViT, they generalize existing DSP-packing methods for larger strides and kernel sizes, and propose a method called Low-Buffer Sreamline for Linear Attention (LBS) to re-organize tensor shape and avoid pipeline stalls. In experiments, the proposed method achieves better accuracy and hardware performance compared with existing quantization and FPGA accelerator counterparts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "a) ADAQ improves existing SmoothQuant method by adapting the hyper-parameter according to channel distributions. This insight sounds reasonable and novel. Their profiling results in Fig. 2 can demonstrate this insight. It might be better if the author could provide more experiments to showcase how much ADAQ can improve SmoothQuant in terms of data distribution.\n\nb) Multi-Computing in Once Packing (MuCO) generalizes the existing methods for larger strides and kernels.\n\nc) LBS significantly reduces the BRAM cost for implementing the transpose operation in linear attention. The experiment in Fig. 6 (d) indicates 71.7% BRAMs are saved."}, "weaknesses": {"value": "a) The Multi-level Soft Approximation (MSA) method appears abruptly in this manuscript. The authors failed to sufficiently explain its role in the entire quantization method, nor the method itself. They could consider clarifying its connection with ADAQ, and also explain the equation with more details.\n\nb) Fairness of accuracy comparison. The authors compare QUS with SmoothQuant, QDrop, OmniQuant, and Trio-Vit. All of them are post-training quantization methods. However, QUS introduces fine-tuning. This may not be fair for other quantization methods. The authors never mention post-training quantization in manuscript nor clarify the fairness of their accuracy experiments.\n\nc) Potential mistake in Eq. 4. VCs are positive values. In this case, the output of the sigmoid operator will be (0.5, 1). According to the context, the output range is supposed to be in (0, 1).\n\nd) Inaccurate title. The authors call QUS a quantization and streamline co-design. However, it seems like the quantization and hardware design are completely independent. This name might lead to confusion."}, "questions": {"value": "a) QUS seems to introduce a fine-tuning process. However, it is compared with post-training quantization methods in terms of accuracy. The authors could clarify the connection between their methods and PTQ, and also explain why it is fair to compare it with other PTQ methods instead of those with fine-tuning or training.\n\nb) Why is MSA introduced? And how does MSA address the problem?\n\nc) The VCs are positive values. This will force the output of the sigmoid operator in Eq. 4 to be in (0.5, 1). Is this the expected output or an error?\n\nd) Is there any dependence or connection between the proposed quantization algorithm and hardware design? Why is it called a co-design? \n\ne) In experimental setup, some of the critical layers are kept in 8-bits. It would be better to indicate what they are explicitly.\n\nf) In line 291, the ‘k’ in ‘k=3A+2B’ should be uppercase. Please double check similar errors.\n\ng) For references that have been published, it would be more appropriate to cite them directly from their official publishers instead of arXiv. For example, ‘Hg-pipe: Vision transformer acceleration with hybrid-grained pipeline’."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GjeBVcEhVf", "forum": "B2DRmwStm9", "replyto": "B2DRmwStm9", "signatures": ["ICLR.cc/2026/Conference/Submission3865/Reviewer_K7fS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3865/Reviewer_K7fS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142383765, "cdate": 1762142383765, "tmdate": 1762917073134, "mdate": 1762917073134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}