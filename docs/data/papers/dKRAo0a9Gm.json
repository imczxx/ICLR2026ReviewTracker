{"id": "dKRAo0a9Gm", "number": 16131, "cdate": 1758260384169, "mdate": 1759897259963, "content": {"title": "AbdCTBench: Learning Clinical Biomarker Representations from Abdominal Surface Geometry", "abstract": "Body composition analysis through CT and MRI imaging provides critical insights for cardio-metabolic health assessment but remains limited by accessibility barriers including radiation exposure, high costs, and infrastructure requirements. We present AbdCTBench, a large-scale dataset containing 23,506 CT-derived abdominal surface meshes from 18,719 patients, paired with 87 comorbidity labels, 31 specific diagnosis codes, and 16 CT-derived biomarkers. Our key insight is that external surface geometry is predictive of internal tissue composition, enabling accessible health screening through consumer devices. We establish comprehensive benchmarks across six computer vision architectures (ResNet-18/34/50, DenseNet-121, EfficientNet-B0, ViT-Small), demonstrating that models can learn robust surface-to-biomarker representations directly from 2D mesh projections. Our best-performing models achieve clinically relevant accuracy: age prediction with MAE 6.22 years (R²=0.757), mortality prediction with AUROC 0.839, and diabetes (with chronic complications) detection with AUROC 0.799. Notably, smaller architectures consistently matched or surpassed larger models, while medical-domain pre-training (RadImageNet) and self-supervised pre-training (DINOv2) showed competitive but not superior performance. AbdCTBench represents the largest publicly available dataset bridging external body geometry with internal clinical measurements, enabling future research in accessible medical AI. We plan to release the dataset, evaluation protocols, and baseline models to accelerate research in representation learning for medical applications, immediately following the review period.", "tldr": "", "keywords": ["computer vision for healthcare", "radiology", "Computed Tomography (CT)", "vision transformers", "CNNs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af36f31ae5ca1355aedaa77096d34e4cac2b4b28.pdf", "supplementary_material": "/attachment/b4fd74232d5ec6c42e654bd8931c8bb2e503a514.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces AbdCTBench, a large-scale dataset comprising 23,506 CT-derived 2D abdominal surface meshes from 18,719 patients, paired with 16 quantitative CT biomarkers, 87 comorbidity labels, and 31 diagnosis codes. The core hypothesis is that external body surface geometry, capturable via consumer-grade depth sensors, can serve as a non-invasive proxy for internal tissue composition and clinical risk stratification. The authors benchmark six vision architectures (including CNNs, ViT, and medical-pretrained models) on 10 biomarker prediction tasks, demonstrating that even lightweight models can achieve clinically meaningful performance (e.g., mortality prediction AUROC = 0.839, age MAE = 6.22 years). The dataset and baselines are slated for public release to foster research in accessible, radiation-free health screening."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work directly addresses a critical gap in preventive medicine: democratizing access to body composition biomarkers without CT/MRI. By leveraging surface geometry, a modality compatible with smartphones and LiDAR, the paper aligns with real-world trends in digital health and point-of-care screening.\n2. With 23K+ samples and 104 clinical variables, AbdCTBench is the largest publicly available dataset linking external surface geometry to internal CT-derived biomarkers. The inclusion of longitudinal lab values, HCC codes, and quantitative tissue metrics enables diverse downstream tasks.\n3. The authors implement a standardized training protocol (consistent optimizer, augmentation, class balancing, threshold tuning) across architectures, ensuring fair comparison. Results include bootstrapped confidence intervals, enhancing statistical reliability.\n4. Models achieve clinically actionable accuracy on key tasks (mortality, vascular disease, diabetes complications), supporting the feasibility of surface-based screening."}, "weaknesses": {"value": "1. All CT scans originate from one private healthcare provider, raising concerns about demographic, geographic, and protocol-related biases. The authors acknowledge the lack of explicit inclusion criteria, which may limit generalizability.\n2. Surface meshes are derived from CT scans, not captured by real consumer devices (e.g., iPhone LiDAR). The fidelity gap between clinical CT-derived surfaces and real-world depth scans remains unquantified, casting doubt on real-world deployability.\n3. The benchmark excludes modern medical vision architectures (e.g., UNet variants, Swin Transformers, Mamba-based models), which could better exploit inter-biomarker correlations.\n4. From a technical and methodological standpoint, the paper's contribution lies primarily in dataset curation and empirical benchmarking using standard vision models, without novel architectures, training paradigms, or representation learning innovations. As such, it would be better suited for a dataset-and-benchmark track or a more clinically oriented venue focused on medical imaging and health informatics."}, "questions": {"value": "Please address the aforementioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9uX4gPj0b3", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Reviewer_CAtG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Reviewer_CAtG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548406822, "cdate": 1761548406822, "tmdate": 1762926300799, "mdate": 1762926300799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a large dataset consisting of surface projection images and associated biomarkers (derived from CT) and diagnosis labels. This is motivated by the opportunity to validate models that can predict the associated biomarkers and diagnosis labels using consumer grade 3D surface extraction devices. Additionally, the paper benchmarks representative architectures for the prediction tasks (classification and regression) including pretrained models on convolutional and transformer-based ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The effort to get approval, collect, preprocess and benchmark such a large dataset with a rich set of associated labels and biomarkers is commendable. The paper is mostly well written with data acquisition and preprocessing steps well explained. The benchmarking of models on the proposed datasets does cover representative architectures, including pretrained models."}, "weaknesses": {"value": "*Input Representation*: Why surface projection images? Why not train on 3D surface meshes themselves to predict biomarkers and diagnostic labels? I.e. The paper is motivated by the possibility to use consumer grade devices such as LiDAR-enabled phones which can generate 3D surface meshes but the proposed dataset and model training use 2D projections which considerably reduces the available information. No rationale or experimental validation is given for this decision.  \n\n*Disaggregated reporting*: It is unclear from the reported aggregated metrics about the performance on clinically important subgroups. Without disaggregated reporting and subgroup analysis, the benchmarking may be incomplete with potential for underdiagnosis bias, subgroup disparities and bias and fairness issues to certain subgroups which may also highlight if such a mapping from surface projection to 3D-based biomarkers and diagnostic labels even make sense. Comparison with naive baselines such as just predicting average quantities will also be useful."}, "questions": {"value": "Please refer to the input representation and disaggregated reporting bullet points in the section above.  \n\nIn addition to disaggregated reporting, Could you also report confidence interval of the reported metrics? Also, please clarify the reported performance with respect to clinical utility (readiness) and gap with respect to clinical requirements (for e.g. how small the Calcium Score should error be to be clinically usable in a specific diagnosis?). \n\nUse of heatmaps (Grad-CAM) to justify effectiveness of learnt representation has been shown to be problematic – useful for model debugging perhaps but controversial as a tool for pixel importance / visual explanations [1]. \n\n1. Kindermans, Pieter-Jan, et al. \"The (un) reliability of saliency methods.\" Explainable AI: Interpreting, explaining and visualizing deep learning. Cham: Springer International Publishing, 2019. 267-280. \n\n \n\nCertain sentences may require citations. E.g.  \n- While early implementations struggle with complex torso geometries,... \n- This limitation has led to the adoption of advanced imaging biomarkers ,... (Need citation associating tissue composition with CT) \n \nTypos \nSpacing typos: advancesspanning, modelsgeneralize,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "IRB approval was taken with Data Privacy and Deidentification done."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S9VJWAoe1W", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Reviewer_hrp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Reviewer_hrp9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714211446, "cdate": 1761714211446, "tmdate": 1762926300354, "mdate": 1762926300354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Overall Response"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and constructive feedback on AbdCTBench. We appreciate the recognition of our contribution in creating the first large-scale dataset bridging external abdominal surface geometry with internal clinical biomarkers, along with comprehensive benchmarking across multiple architectures. The reviewers made great suggestions, which we address systematically below before responding to individual reviewer comments.\n\n## Major Revisions Made Across All Reviews\n\n### 1. Expanded Architecture Coverage (Reviewers CAtG, 8PVq, yCJD)\n\nMultiple reviewers noted that our original benchmark emphasized standard CNNs. This feedback led us to expand our evaluation with modern transformer architectures. We have now added **Swin Transformer-Base** to our evaluation:\n\n- Trained and evaluated across all 10 biomarkers with full bootstrapped 95% confidence intervals\n- **New best-in-class results**: Swin Transformer-Base achieves top AUROC for Myocardial Infarction (0.742), and Hierarchical Condition Categories (HCC) codes HCC-108 (0.768), and HCC-18 (0.801)\n- Updated Tables 1 & 2 now include these results, demonstrating that modern transformer architectures can match or surpass CNNs on AbdCTBench\n\nThis addition addresses concerns about architecture diversity and shows that both CNN and transformer families can learn discriminative representations from abdominal surface geometry.\n\n### 2. Comprehensive Subgroup Analysis & Baseline Comparisons (Reviewer hrp9)\n\nWe have added a new **Section 6.4** with gender-stratified analysis across all 10 biomarkers, revealing clinically meaningful performance differences:\n\n- Age prediction: MAE 5.76 (Males) vs 6.63 (Females)\n- HCC-18: AUROC 0.824 (Females) vs 0.773 (Males)\n\nWe also now report naive baseline comparisons throughout:\n\n- Age prediction: Our models achieve R² > 0.719 vs naive baseline R² ≈ 0\n- Naive mean prediction: MAE 13.16 years (95% CI: 12.79-13.57) vs our best model MAE 6.22 years\n- Updated tables include confidence intervals in the main text (previously Appendix-only)\n\n### 3. Dataset Release Statement (Reviewer yCJD)\n\nThe dataset will be released at https://abdctbenchrepo.github.io/AbdCTBench/ (an anonymized url compliant with the double-blind submission policy) under Creative Commons BY 4.0 license immediately upon paper acceptance. The release will include:\n\n- 23,506 2D depth map projections (PNG format, ~50KB each)\n- 23,506 3D STL surface meshes (~1-2MB each)\n- Complete DICOM→STL→PNG processing pipeline (Python code)\n- OSCAR biomarker extraction pipeline\n- Pre-trained model checkpoints for all 8 architectures (6 architectures originally benchmarked, and the Swin Transformer-Base/Multi-task learning architectures added in follow-up)\n- Train/val/test splits and evaluation protocols\n- HIPAA-compliant de-identified labels (87 comorbidities, 31 diagnoses, 16 biomarkers)\n\n### 4. Single-Site Limitations & Path to Multi-Site Validation (Reviewers CAtG, 8PVq)\n\nMultiple reviewers raised valid concerns about generalizability from single-site data. We address this through transparency and enabling external validation:\n\n**Multi-Site Data Availability**: While multi-site CT datasets exist (e.g., Stanford Merlin), they lack the HCC and ICD-10 diagnosis codes required for our benchmark tasks. Integrating imaging with structured clinical outcomes at scale remains challenging.\n\n**Reproducibility Pipeline**: To enable external validation, we will release:\n\n- Complete DICOM-to-STL-to-PNG conversion pipeline\n- Full pipeline for calculating all 16 CT-derived biomarkers using OSCAR\n- Pre-trained model checkpoints for all architectures\n\nAny institution with CT DICOM series and corresponding HCC/ICD-10 codes can validate our results on their own data. We view this as the most practical path forward given current data availability constraints.\n\n### 5. Multi-Task Learning Exploration (Reviewer 8PVq)\n\nWe implemented multi-task learning experiments with shared backbone architectures (ResNet-18/34/50) covering all 10 biomarkers, using GradNorm for gradient balancing. Multi-task learning consistently worsened performance across all architectures versus single-task models (e.g., Age MAE rose from 6.22 to 14.5-72.6 years; binary AUROCs dropped 0.10-0.20 on average). This indicates substantial negative transfer between the 10 biomarkers, likely because different biomarkers require distinct geometric features. Age prediction may rely on overall body shape, while diabetes complications may depend on regional adiposity patterns. Future work should explore:\n\n- Multi-task learning on smaller, physiologically related biomarker subsets\n- Task-specific learning rates and loss weighting beyond GradNorm\n- Hierarchical multi-task architectures with task-specific early layers\n\nDetailed tables are provided in the individual response to Reviewer 8PVq.\n\n(cont...)"}}, "id": "5YqRHtlWLu", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763680762923, "cdate": 1763680762923, "tmdate": 1763683359077, "mdate": 1763683359077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Overall Response"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and constructive feedback on **AbdCTBench**. We appreciate the recognition of our contribution in creating the **first large-scale dataset bridging external abdominal surface geometry with internal clinical biomarkers**, along with **comprehensive benchmarking across multiple architectures**. Reviewers raised several valuable points, and we have made substantial revisions accordingly. Below we summarize the major changes implemented across reviews before addressing individual comments.\n\n## Major Revisions Made Across All Reviews\n\n### 1. Expanded Architecture Coverage (Reviewers CAtG, 8PVq, yCJD)\n\nSeveral reviewers noted the original benchmark’s emphasis on CNNs. In response, we expanded the architecture suite to include modern transformer-based models. We added **Swin Transformer-Base**, trained and evaluated across all 10 biomarkers with full bootstrapped 95% confidence intervals.\n\nKey findings:\n\n**New best-in-class results:**\n- Myocardial Infarction AUROC: **0.742**\n- HCC-108 AUROC: **0.768**\n- HCC-18 AUROC: **0.801**\n\nAdditional updates:\n- Updated Tables 1 & 2 now include transformer results.\n- These results demonstrate that modern transformer architectures can match or outperform CNNs on AbdCTBench.\n\nThis directly addresses concerns about architectural diversity and supports the conclusion that both CNN and transformer families learn discriminative geometric representations from abdominal surface geometry.\n\n### 2. Comprehensive Subgroup Analysis & Baseline Comparisons (Reviewer hrp9)\n\nWe added a new **Section 6.4** providing gender-stratified analyses across all 10 biomarkers. These analyses reveal meaningful performance differences:\n\n- Age prediction: MAE = **5.76 (M)** vs **6.63 (F)**\n- HCC-18: AUROC = **0.824 (F)** vs **0.773 (M)**\n\nWe also now report **naive baseline comparisons** throughout:\n\n**Age prediction:**\n- Model R²: **> 0.719** vs naive R² ≈ 0\n- Naive MAE = **13.16 years** (95% CI: 12.79–13.57)\n- Best model MAE = **6.22 years**\n\nConfidence intervals formerly in the Appendix are now included in the main tables.\nThese additions improve interpretability and strengthen comparative analysis.\n\n### 3. Dataset Release Statement (Reviewer yCJD)\n\nThe dataset will be released at https://abdctbenchrepo.github.io/AbdCTBench/ (an anonymized url compliant with the double-blind submission policy) under **Creative Commons BY 4.0 license** immediately upon paper acceptance. The release will include:\n\n- **23,506** 2D depth map projections (PNG format, ~50KB each)\n- **23,506** 3D STL surface meshes (~1-2MB each)\n- Complete **DICOM->STL->PNG** processing pipeline (Python code)\n- **OSCAR** biomarker extraction pipeline\n- Pre-trained model checkpoints for all **8 architectures** (including Swin and multi-task models)\n- Train/val/test splits and evaluation protocols\n- HIPAA-compliant de-identified labels: **87 comorbidities, 31 diagnoses, 16 biomarkers**\n\n### 4. Single-Site Limitations & Path to Multi-Site Validation (Reviewers CAtG, 8PVq)\n\nReviewers raised valid concerns about single-site generalizability. We now clarify these limitations and outline a realistic path toward multi-site validation:\n\n**Multi-Site Data Availability**: While multi-site CT datasets exist (e.g., Stanford Merlin), they lack the HCC and ICD-10 diagnosis codes required for our benchmark tasks. Integrating imaging with structured clinical outcomes at scale remains challenging.\n\n**Reproducibility Pipeline**: To enable external validation, we will release:\n\n- Full **DICOM->STL->PNG** conversion pipeline\n- **OSCAR** biomarker computation code\n- Pre-trained model checkpoints for all architectures\n\nAny institution with CT DICOM series and corresponding HCC/ICD-10 codes can reproduce the benchmark and validate results locally. Given current data availability, this is the most practical and scalable path forward.\n\n### 5. Multi-Task Learning Exploration (Reviewer 8PVq)\n\nWe conducted multi-task learning experiments using shared ResNet backbones (ResNet-18/34/50) with **GradNorm** for dynamic loss balancing. Across all settings, multi-task learning **consistently underperformed** single-task training:\n\n- Age MAE increased substantially: **6.22 → 14.5–72.6 years**\n- Binary biomarker AUROCs dropped by **0.10–0.20** on average\n\nThis suggests strong **negative transfer** between biomarkers—likely because different tasks depend on different geometric cues (e.g., global body shape vs. regional adiposity patterns).\n\nWe outline future directions:\n\n- Multi-task learning on smaller, physiologically related biomarker groups\n- More flexible task-specific learning rates and weighting schemes\n- Hierarchical architectures with early shared layers\n\nA detailed set of results is provided in the response to Reviewer 8PVq.\n\n\n(cont...)"}}, "id": "5YqRHtlWLu", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763680762923, "cdate": 1763680762923, "tmdate": 1763692189620, "mdate": 1763692189620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses ​​accessibility barriers in body composition analysis via CT and MRI​​ (e.g., radiation exposure, high costs) by introducing ​​AbdCTBench​​, a large-scale dataset containing 23,506 CT-derived abdominal surface mesh images paired with 87 comorbidity labels and 16 CT-derived biomarkers. The core method involves using ​​2D mesh projections​​ and multiple computer vision architectures (e.g., ResNet, EfficientNet, ViT) to learn representations from external surface geometry for predicting internal biomarkers. Key results show clinically relevant accuracy: age prediction (MAE 6.22 years), mortality prediction (AUROC 0.839), and diabetes detection (AUROC 0.799). The primary contribution is the ​​first and largest publicly available dataset bridging external body geometry with internal clinical measurements​​, enabling benchmarks for accessible medical AI. Experiments reveal that smaller architectures (e.g., ResNet-18) often match or surpass larger models, while medical-domain pretraining (e.g., RadImageNet) does not yield superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- ​​Originality:​​ AbdCTBench is the first public dataset to systematically connect abdominal surface geometry with internal biomarkers at scale (23,506 samples), pioneering non-invasive health screening.\n- Quality:​​ Rigorous experimental design covers 6 architectures and 10 biomarker tasks, with standardized protocols (e.g., inverse frequency weighting for class imbalance) ensuring reproducibility. Grad-CAM visualizations (Figure 3) effectively illustrate learned representations.\n- Clarity:​​ Training details (e.g., hyperparameters, loss functions) are comprehensively documented, and appendices provide full statistics (Table 3-4), facilitating replication."}, "weaknesses": {"value": "- Generalizability Concerns:​​ Data is from a single site, risking demographic biases (e.g., age/race distribution uncontrolled); multi-site validation is needed.\n- Limited Architecture Exploration:​​ Only standard CNNs/transformers are tested, omitting medical-specific models (e.g., U-Net variants) that might capture better representations. ViT-Small's competitive but suboptimal performance.\n\n- Uneven Task Performance:​​ HCC 12 prediction (AUROC ~0.59) is near-random, but no deep analysis explains why surface geometry fails here.\n\n- ​​No Multi-Task Learning:​​ The single-target framework misses biomarker correlations; multi-task learning (as noted in Section 7) could improve efficiency but is unexplored."}, "questions": {"value": "Why did ViT-Small with DINOv2 pretraining not outperform CNNs? Is this due to local feature importance in surface geometry? Could larger transformers or medical-specific pretraining help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WjI8WXCRqT", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Reviewer_8PVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Reviewer_8PVq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808278654, "cdate": 1761808278654, "tmdate": 1762926299982, "mdate": 1762926299982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the AbdCTBench, a large-scale dataset of 23,506 CT-derived abdominal surface mesh images from 18,719 patients. The AbdCTBench contains 16 biomarkers paired with 31 diagnosis codes and 87 comorbidity labels. The authors established benchmarks on age prediction, mortality prediction, diabetes detection, etc., using six computer vision architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The constructed dataset is a large-scale CT image dataset. It contains 23,506 CT-derived abdominal surface mesh images from 18,719 patients. The images belong to 87 comorbidity labels, 31 specific diagnosis codes, and 16 CT-derived biomarkers.\n\nThe authors conducted experiments using CV architectures ResNet-18/34/50, DenseNet-121, EfficientNet-B0, ViT-Small, and computed the benchmarks for non-HCC biomarkers and HCC code biomarkers."}, "weaknesses": {"value": "The AbdCTBench is not yet released/available.\n\nThe model architectures are small. It's better to test the ViT-base or larger models to evaluate their performance. \n\nIt's better to provide the internal body composition biomarkers for the 4 images shown in Figure 1."}, "questions": {"value": "Is the dataset de-identified?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This dataset contains sensitive human information, including CT images, biomarkers, Mortality, and diabetes information."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3RrxGC6I1M", "forum": "dKRAo0a9Gm", "replyto": "dKRAo0a9Gm", "signatures": ["ICLR.cc/2026/Conference/Submission16131/Reviewer_yCJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16131/Reviewer_yCJD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972126515, "cdate": 1761972126515, "tmdate": 1762926299060, "mdate": 1762926299060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}