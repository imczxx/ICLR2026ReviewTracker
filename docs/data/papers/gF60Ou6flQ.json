{"id": "gF60Ou6flQ", "number": 7889, "cdate": 1758041087909, "mdate": 1759897824176, "content": {"title": "Myna: Masking-Based Contrastive Learning of Musical Representations", "abstract": "In this paper, we present Myna, a simple yet effective approach for self-supervised musical representation learning. Built on a contrastive learning framework, Myna introduces two key innovations: (1) the use of a Vision Transformer (ViT) on mel-spectrograms as the backbone, replacing SampleCNN on raw audio; and (2) a simple yet novel data augmentation strategy—token masking—that masks 90% of spectrogram tokens (e.g., 16x16 patches). These innovations deliver both effectiveness and efficiency: (i) Token masking enables a significant increase in per-GPU batch size, from 48 or 120 in traditional contrastive methods (e.g., CLMR, MULE) to 4096. (ii) By avoiding traditional augmentations (e.g., pitch shifts), Myna retains pitch sensitivity, enhancing performance in tasks like key detection. (iii) The use of vertical patches (128x2 instead of 16x16) allows the model to better capture critical features for key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and 128x2 patches, achieving state-of-the-art results. Trained on a single GPU, it outperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16 and 64 GPUs, respectively. When scaled to 85M parameters, Myna achieves further improvements across all tasks and is competitive with models like MERT-330M, MusicFM, and MuQ despite being 3-7x smaller and trained with an order of magnitude fewer GPUs in less time. Additionally, it surpasses MERT-95M-public and MuQ$_{m4a}$, establishing itself as the best-performing model trained on publicly available data. We release our code and models to promote reproducibility and facilitate future research: https://github.com/ghost-signal/myna", "tldr": "Myna is a masking-based contrastive framework for musical representation learning on mel-spectrograms; it breaks the SOTA for self-supervised methods on music key detection and is competitive with MULE/MERT-95M across various MIR tasks.", "keywords": ["self-supervised learning", "music representations", "contrastive learning", "masking"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4df7d05cf4c4de79d96aac259ebd616b9fded7b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors proposes a new training method for the representation learning of music audio. The proposed method includes aggressive input masking that seems to allow avoiding pitch shifting as an augmentation step, making the model aware of pitch and key information. The experiment showed that even without finetuning on the downstream task's training set and despite its smaller size & training data, Myna outperforms many other methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good performance\n- Parameter-efficient\n- Trained on a public dataset only\n- The proposed method is simple"}, "weaknesses": {"value": "- Limited novelty: Some core changes such as using ViT and masked autoencoder are already proposed in other, similar work including audio domain.\n- Although the performance is strong, the margin is rather reasonable, not outstanding."}, "questions": {"value": "- I don't think we should call the used audio processor as a \"tokenizer\", no matter how the word is over-subscribed in the community. It does not tokenize (making the input a discrete representation) at all, and it's even worse because some architectures indeed discretize the input audio."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GQY5Hk1BK5", "forum": "gF60Ou6flQ", "replyto": "gF60Ou6flQ", "signatures": ["ICLR.cc/2026/Conference/Submission7889/Reviewer_va77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7889/Reviewer_va77"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616573857, "cdate": 1761616573857, "tmdate": 1762919921724, "mdate": 1762919921724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MYNA, an efficient, masking-based contrastive learning framework for musical representation. It replaces traditional music augmentations with a high-rate token masking (90%) on mel-spectrograms. This heavy masking significantly reduces the number of input tokens, allowing for large batch sizes (4096) on a single GPU, and achieves an efficiency gain over prior contrastive methods. The authors also introduce a hybrid patching scheme (combining vertical and square patches) to capture complementary features (general purpose vs. pitch structure). The model is pretrained on the public AudioSet music subset. Myna achieves competitive performance with larger private models and establishes a new public-data SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The mask-only approach is simple and allows single-GPU large-batch training (batch size 4096), which translates to an 85x increase in efficiency over traditional contrastive methods like CLMR. The model achieves competitive average scores (68.6 for Myna-Hybrid) with MERT-95M, and surpasses public baselines like MERT-95M-public and MULE.\n\n2. The hybrid patch design improves key detection (achieving SOTA among self-supervised methods) by integrating frequency-sensitive vertical patches. The method retains pitch sensitivity by avoiding traditional data augmentations (e.g., pitch shifts), which is beneficial for tasks like key detection."}, "weaknesses": {"value": "1. Table 1 mixes public and private data baselines (e.g., MERT-330M) without transparently clarifying the training resource budgets.\n\n2. The claim that \"90% masking performs best\" is not strongly supported by Figure 4. This is due to two issues: (a) Performance differences across high masking ratios looks marginal and lack verification of statistical significance; (b) The \"average across all four benchmarks\" curve can be mathematically unrigorous as it combines different metrics from different tasks.\n\n3. The model's poor performance on EmoMusic is attributed to short clip length, a hypothesis that needs empirical verification."}, "questions": {"value": "1. It would be helpful if Table 1 were explicitly partitioned to clearly distinguish models trained on public data from those trained on private or internal corpora.\n\n2. Could you provide supplementary figures showing the performance curves across different masking ratios for each of the four downstream tasks (MTT, GiantSteps, GTZAN, and EmoMusic)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xdltnrRNkm", "forum": "gF60Ou6flQ", "replyto": "gF60Ou6flQ", "signatures": ["ICLR.cc/2026/Conference/Submission7889/Reviewer_k7CR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7889/Reviewer_k7CR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874190483, "cdate": 1761874190483, "tmdate": 1762919921221, "mdate": 1762919921221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on music representation learning. It follows a contrastive learning framework, with its main contributions being the use of a Vision Transformer (ViT) as the backbone model and the application of token masking. Furthermore, considering the characteristics of music analysis, the authors extend the approach into a hybrid model that incorporates vertical filters to better capture the frequency-related features of spectrograms. Through this relatively simple training strategy, the proposed model achieves competitive performance on several downstream tasks compared to models that require more than 5× the training time and parameters. Overall, the paper is well written and presents a solid contribution to efficient representation learning for music."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed use of ViT and token masking seems promising in music representation learning. \nThe paper is easy-to-read and the illustration of the proposed method, experimental design, and results seems promising."}, "weaknesses": {"value": "The proposed method seems to be only applicable to the clip-level MIR tasks. I wonder the authors opinion (maybe discussions) on how the proposed architecture can be applied towards frame-level tasks as well."}, "questions": {"value": "I wonder the effect of the patch size variations on performances. For example, would 4x4, 96x2, 128x3, 32x32, hybrid of them, etc these kind of diverse patch size affects the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5UBOzuAWGt", "forum": "gF60Ou6flQ", "replyto": "gF60Ou6flQ", "signatures": ["ICLR.cc/2026/Conference/Submission7889/Reviewer_Xz6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7889/Reviewer_Xz6M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092356407, "cdate": 1762092356407, "tmdate": 1762919920744, "mdate": 1762919920744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}