{"id": "2fuvTuG4IV", "number": 2341, "cdate": 1757060019496, "mdate": 1763725517326, "content": {"title": "SinoMultiAffect: A Chinese Multi-Label and Fine-Grained Emotional Text Dataset with fMRI Data", "abstract": "Emotion plays an indispensable role in advancing human-AI interaction, yet the field still lacks high-quality, fine-grained Chinese datasets that integrate both language and neural modalities. We present **SinoMultiAffect** (SMA), a multi-modal emotion dataset designed to advance research on emotion, language and the emotion-related capabilities of artificial intelligence. The dataset consists of 4,500 Chinese sentences in total collected from social media platforms in China, with 4,058 of them labeled with a fine-grained taxonomy of 35 emotion categories (including Neutral) with their intensity, as well as continuous annotations along the valence-arousal-dominance (VAD) dimensions. Our dataset also includes functional magnetic resonance imaging (fMRI) recordings of the brain while human participants were reading the sampled sentences. The utility of the dataset was demonstrated by the predictive performance of large language models (LLMs) on multi-label emotion recognition. We also built a VAD-guided human-LLM alignment framework, which revealed that incorporating emotional information enhances the alignment between text and brain embeddings and improves the downstream task performance of bidirectional retrieval. By integrating text, categorical, dimensional, and neuroimaging information, SMA provides a unique resource for studies on emotion and language, offering new opportunities for interdisciplinary research in natural language processing, affective computing, and cognitive neuroscience.", "tldr": "", "keywords": ["Multi-label", "Fine-grained", "Chinese text", "fMRI data"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ff4b269d41ff39a188630d898b28368f198bbe3.pdf", "supplementary_material": "/attachment/0b36c09aa8703db903c94baa574f76084d1e126f.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents SinoMultiAffect, a Chinese multimodal emotion dataset that combines text and functional magnetic resonance imaging (fMRI) data. The dataset is used to benchmark large language models in multi-label emotion recognition. The paper also proposes a method for aligning linguistic and neural representations of emotions using VAD (valence, arousal, dominance) features. This method contributes a new resource and framework for studying the correspondence between human and AI emotions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces SinoMultiAffect, a new Chinese multilabel and fine-grained emotional text dataset that expands upon prior affective resources for the language. instance is annotated with 35 categorical emotions as well as four continuous affective dimensions (Valence, Arousal, Dominance, and Intensity) enabling both discrete and dimensional analysis of emotion.\n2. The collection of fMRI recordings for a subset of sentences adds a unique neuroimaging modality, bridging linguistic and neural representations of affect.\n3. The paper formulates a novel VAD-guided Human–LLM Alignment framework, exploring the alignment between large language model embeddings and human brain activity.\n4. The authors provide code and data resources."}, "weaknesses": {"value": "1. The proposed dataset is relatively small when compared to existing Chinese emotion corpora such as CH-MEAD, EmotionTalk, and ChineseEmoBank, which limits its representativeness and robustness.\n\n2. Although the corpus is described as multimodal, it only includes two modalities - text and fMRI, without audio or visual data, which weakens the claim of true multimodality.\n3. While comparing multiple large language models for zero-shot emotion recognition may be topical, it does not provide significantly new insights, as this has been widely explored in previous studies.\n4. The use of large LLM embeddings for alignment may not be fully justified, as simpler BERT-based emotion encoders could be more efficient feature extractors.\n5. The paper lacks explicit information about the prompts used for each LLM and does not compare with previous prompt designs, limiting the reproducibility of the study.\n6. The evaluation of LLMs is limited to categorical emotion recognition, whereas the proposed VAD-guided Human–LLM Alignment framework applies only to dimensional (valence–arousal–dominance–intensity) emotion modeling. This lack of cross-validation across these two areas reduces the conceptual consistency of the experimental design.\n7. There is no comparison with existing emotion recognition or brain–text alignment methods, making it challenging to assess the practical significance of the proposed approach."}, "questions": {"value": "1. Could the authors please provide more information about the specific prompt templates used for each language model in the zero-shot emotion recognition task?\n2. Why was it decided to evaluate LLMs only on categorical emotions, rather than on the valence, arousal, dominance, and intensity dimensions, or the VAD-guided human-LLM alignment framework for valence, arousal, and dominance recognition? What were the reasons for this decision?\n3. Would it be possible for the authors to compare their alignment framework with simpler baselines using emotional BERT or RoBERTa embeddings, to demonstrate the necessity of LLM representations for achieving affective alignment?\n4. Could the authors elaborate on the practical implications of the fMRI-based alignment framework, given its high cost and limited scalability for real-world applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FsLVNDbOWG", "forum": "2fuvTuG4IV", "replyto": "2fuvTuG4IV", "signatures": ["ICLR.cc/2026/Conference/Submission2341/Reviewer_92LJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2341/Reviewer_92LJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409224305, "cdate": 1761409224305, "tmdate": 1762916198940, "mdate": 1762916198940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced SinoMultiAffect (SMA) dataset which contains 4,500 Chinese sentences from social media, with 4,058 labeled for 35 fine-grained emotion categories. This dataset also includes functional magnetic resonance imaging (fMRI) recordings of the brain while human participants were reading the sampled sentences. This paper also introduced a VAD-guided human-LLM\nalignment framework. Evaluations on this benchmark has been conducted across multiple models, such as Qwen3, Llama3.1 and Llama3.3 models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduced a novel dataset. It is a more comprehensive dataset compared to other Chinese emotion datasets\n- This paper conducted detailed analyses across different dimensions of this dataset."}, "weaknesses": {"value": "- Analyses over different models' performance on this dataset are limited (Table 3). It would be interested to see why some models are better than others for some metrics, but worse in other metrics.\n- This paper only evaluates general instruct LLMs on this benchmark. However, emotion-related baselines also need to be evaluated on this constructed benchmark."}, "questions": {"value": "- From Table 3, it is hard to tell which model achieves the best results among all models. What are the differences between these metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fo3HbLOFwz", "forum": "2fuvTuG4IV", "replyto": "2fuvTuG4IV", "signatures": ["ICLR.cc/2026/Conference/Submission2341/Reviewer_ZFg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2341/Reviewer_ZFg7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721346667, "cdate": 1761721346667, "tmdate": 1762916198677, "mdate": 1762916198677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SinoMultiAffect (SMA), a new Chinese dataset that integrates fine-grained emotion annotations (35 categories + VAD + intensity) with neural data (fMRI) collected during reading tasks.\nIt contains 4,500 social-media sentences, of which 4,058 are annotated with multi-label emotions and dimensional ratings. A smaller subset (120 sentences) was presented to human participants under fMRI scanning to capture neural correlates of emotional language processing.\n\nThe authors benchmark 13 open-source LLMs on zero-shot Chinese emotion recognition, and propose a VAD-guided contrastive alignment model linking brain and text embeddings to study human–LLM alignment in affective representation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Unique multimodal contribution. The first fine-grained Chinese emotional text dataset, accompanied by neuroimaging data, enables research that connects affective NLP, cultural linguistics, and cognitive neuroscience.\n\nHigh-quality curation pipeline. Manual multi-label annotation, dimensional VAD scales, inter-rater analysis, and validation of emotion taxonomy demonstrate rigorous data design.\n\nCultural significance. Captures East-Asian emotion concepts (e.g., being moved, gratitude, healing), addressing the Western-centric bias of prior emotion corpora.\n\nSolid baseline benchmarking. Evaluates 13 LLMs (8B–72B) under a consistent zero-shot protocol, providing a strong empirical reference for future studies."}, "weaknesses": {"value": "1.  Extremely limited neural dataset\nThe fMRI component, though conceptually valuable, includes only three participants and 120 stimuli.\nThis scale is far below the threshold required to support statistically meaningful alignment claims, especially when it is the core contribution of the paper.\nNo within-subject or cross-subject validation, no voxel-wise encoding/decoding models, and no statistical tests (e.g., permutation or bootstrap confidence intervals) are reported.\nGiven the known noise level of fMRI (hemodynamic delay, low SNR, inter-subject variability), the presented alignment metrics (Hit@1≈0.17, Hit@3≈0.37) are effectively at or slightly above random, and cannot substantiate robust “brain–LLM alignment.”\nConsequently, the neural findings should be treated as proof-of-concept, not evidence of genuine representational correspondence.\n\n2. Experimental setting concerns.\nThe paper conflates two distinct settings, but the authors didn't explicitly mention this. The text-only emotion classification is genuinely zero-shot (LLM inference without training), but the fMRI–LLM alignment still trains two projection MLPs on paired data with a supervised contrastive objective. Therefore, this mapping does not constitute zero-shot recognition of neural signals.\nReaders may incorrectly infer that LLMs intrinsically interpret fMRI patterns, when in fact a learned mapping is mediating the correspondence. This conceptual overreach undermines the scientific precision of the claims and should be explicitly corrected. In this regard, if the zero-shot setting of the LLM is conducted on the text-only emotion classification, the technical contribution will be extremely limited. \n\n3. Limited methodological novelty\nThe VAD-guided alignment extends prior contrastive multimodal frameworks (e.g., CLIP-style InfoNCE) by adding auxiliary regression losses for emotion dimensions. While this is reasonable, it is incremental rather than groundbreaking. There is no new theoretical insight into multimodal representation learning, nor an analysis of how VAD supervision alters embedding geometry beyond sensitivity/entropy plots. The work contributes more as a dataset paper than as a methodological advance for ICLR’s main track.\n\n4. Over-interpretation of weak correlations\nThe authors report modest improvements when injecting emotion features (e.g., Brain-Only variant Hit@1=0.169 vs. 0.082 baseline), but the absolute performance remains low and unvalidated.\nWithout randomization tests or comparisons to trivial baselines (e.g., linear regression decoding, canonical correlation analysis, RSA), it is impossible to determine whether these results reflect genuine affective structure or dataset noise.\nThe qualitative statements about “strong utility of affective cues” and “alignment within a shared emotional space” are not quantitatively supported."}, "questions": {"value": "The authors can address the concerns from the following points: \nClarify and separate zero-shot vs. trained alignment claims.\nIncrease the fMRI participant/sample size or clearly present it as exploratory.\nAdd statistical tests.\nStrengthen discussion and contribution of how this dataset can drive new ML directions (e.g., emotion-grounded representation learning)."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sqIRidtazq", "forum": "2fuvTuG4IV", "replyto": "2fuvTuG4IV", "signatures": ["ICLR.cc/2026/Conference/Submission2341/Reviewer_Zvhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2341/Reviewer_Zvhr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951070874, "cdate": 1761951070874, "tmdate": 1762916198514, "mdate": 1762916198514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SinoMultiAffect (SMA), a novel Chinese emotion dataset. Its primary contribution is the dataset itself: 4,058 Chinese social media texts annotated with a four-part structure: (1) a fine-grained, multi-label taxonomy of 35 emotion categories (2) continuous dimensional ratings for Valence, Arousal, and Dominance (VAD) (3) emotion intensity scores and (4) corresponding fMRI data from human participants reading the texts. The paper validates this resourceby providing a comprehensive zero-shot benchmark of 13 large language models (LLMs) on the emotion recognition task and by proposing a VAD-guided framework to align LLM embeddings with the fMRI data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reason to Accept\n\n- Novel Multi-Modal Dataset: This is the first Chinese dataset to integrate fine-grained (35 labels), multi-label text with both dimensional (VAD) ratings and neural (fMRI) data, which is a unique resource for interdisciplinary research.\n- Addresses Critical Gaps in the Field: The dataset directly addresses the need for culturally specific, non-English emotion resources. It provides far greater label granularity (35 labels) than the largest existing Chinese dataset, CMACD (6 labels) , and adds neural/VAD data, which are missing from most text-based resources.\n- Strong LLM Benchmark: The paper provides a solid and immediately useful zero-shot benchmark of 13 modern LLMs. This validates the dataset's utility for fine-grained NLP tasks and provides a strong baseline for future models"}, "weaknesses": {"value": "Reasons to Reject\n\n\n- Low Interrater Agreement (IAA): The paper reports an average Jaccard index of 0.346 for label agreement, which is state as moderate. This score is low, indicating that the 35-label taxonomy is likely too ambiguous or subjective. This low reliability calls the validity/quality of the dataset's ground truth labels.\n- LLM Bias: The paper claims LLMs show a systematic overprediction bias. This may be a misinterpretation. Given the low human IAA (0.346) , it is more likely the LLMs are correctly identifying the same label ambiguity that individual human raters saw, and the ground truth is an artificial product of the 2/3 consensus mechanism filtering this ambiguity out.\n- N=3 for fMRI data is too low to obtain statistically meaningful results."}, "questions": {"value": "Questions\n\nSee above in Reasons to Reject. Additionally\n\n- Your correlation analysis (Figure 3) shows high similarity between certain emotion pairs (e.g., 'remorse'/'guilt'). Combined with the low IAA , do you believe all 35 categories are truly distinct and reliably identifiable from text?\n- The limitations section notes the dataset's scale. Are there concrete plans to collect fMRI data from a statistically significant sample (e.g., N > 30) to properly validate the VAD-guided alignment framework?\n- Given the low IAA , do you plan to conduct another annotation round? Will you consider collapsing similar categories into a smaller, more robust label set?\n- Instead of a 2/3 consensus , have you considered releasing the raw rater counts (e.g., 1/3, 2/3, 3/3) as soft labels? This would allow models to learn from the human-level ambiguity you identified, rather than training on a filtered and potentially biased ground truth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "upRpBnBmAq", "forum": "2fuvTuG4IV", "replyto": "2fuvTuG4IV", "signatures": ["ICLR.cc/2026/Conference/Submission2341/Reviewer_KKFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2341/Reviewer_KKFD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241369243, "cdate": 1762241369243, "tmdate": 1762916198288, "mdate": 1762916198288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}