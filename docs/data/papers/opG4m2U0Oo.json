{"id": "opG4m2U0Oo", "number": 7832, "cdate": 1758038051329, "mdate": 1759897829529, "content": {"title": "Culture in Action: Evaluating Text-to-Image Models through Social Activities", "abstract": "Text-to-image (T2I) diffusion models achieve impressive photorealism by training on large-scale web data, but  models inherit cultural biases and fail to depict underrepresented regions faithfully. Existing cultural benchmarks focus mainly on object-centric categories (e.g., food, attire, and architecture), overlooking the social and daily activities that more clearly reflect cultural norms. Few metrics exist for measuring cultural faithfulness. We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural activities (e.g., greetings, dining, games, traditional dances, and cultural celebrations). CULTIVate spans 16 countries with 576 prompts and more than 19,000 images, and provides an explainable descriptor-based evaluation framework across multiple cultural dimensions, including background, attire, objects, and interactions. We propose four metrics to measure cultural alignment, hallucination, exaggerated elements, and diversity. Our findings reveal systematic disparities: models perform better for global north countries than for the global south, with distinct failure modes across T2I systems. Human studies confirm that our metrics correlate more strongly with human judgments than existing text–image measures.", "tldr": "", "keywords": ["Text-to-Image Models", "Cross Cultural Evaluation", "Fairness", "Bias", "Social Activities"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a04c7a022d5ffaeed4f8bb7bea24a9cf7ab9733.pdf", "supplementary_material": "/attachment/1cd406275a44bff77347417789523d76ac79e54e.zip"}, "replies": [{"content": {"summary": {"value": "The paper builds a benchmark and evaluation framework to measure cultural faithfulness of T2I models when prompted to generate social activities. It also introduces an interpretable evaluation metric capturing 4 diagnostic aspects which correlates better with human judgements compared to previous metrics or using a model as a judge. The proposed pipeline uses a proposer-refiner approach for generating and verifying textual descriptions. The experiments show cultural disparities between Global North and Global South depictions and highlight failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clear. The main contribution of the paper lies in the metric, which is explainable and actionable through the decomposition of descriptor dimensions. The metric definitions (ALIGN, HAL, EXAG, DDIV/SDIV) are also well-defined and motivated.\n2. The paper includes many ablations (e.g., τ threshold, proposer vs proposer+refiner) and per-dimension analyses.\n3. There is strong evidence of the metric utility through a higher Spearman correlation with human GT labels than common ITA baselines and the model as a judge.\n4. The use case of the metric is also clear, since it enables interpretable comparison across different aspects of various T2I models."}, "weaknesses": {"value": "1. The empirical findings are incremental, several concurrent works (though reported by the authors) report similar Global North vs Global South gaps and also collect prompts and images for social activities. \n2. The same MLLM (InternVL) is used to extract image descriptors in the pipeline (L727) and also appears in baseline comparisons (InternVL as a judge). This creates a potential dependency/leakage where the extraction backbone advantages or disadvantages affect both measurement and baselines.\n3. Human evaluation was conducted for 7 countries × 9 activities × 3 models plus some real images (216 forms total). This is a small subset relative to the full 576 prompts / 16 countries. The paper reports Spearman correlation numbers and \"human-human agreement is moderate (L424)\" but lacks thorough reporting of annotator reliability (Krippendorff’s α / Cohen’s κ), per-country inter-annotator agreement, and statistical significance of the reported metric improvements.\n4. EXAG depends on LLM-produced “exaggeration candidates” and on web images as references. It is unclear how candidates are generated, how stereotypical or noisy web images affect EXAG, and how to distinguish a valid cultural variant from an exaggeration.\n5. The GN/GS split (listed countries) is reasonable for summarizing trends, but grouping heterogeneous countries into GS may obscure important nuances. For instance, China often performs well despite being placed in GS."}, "questions": {"value": "1. It will be intuitive to see some concrete examples of descriptors that the refiner removed. For descriptors that annotators marked irrelevant (from Table 9), what categories of errors occur (mislabeling, overly generic, stereotype)?\n2. To address W2, it will be helpful to run the descriptor-extraction with 1–2 additional independent MLLMs (e.g., QwenVL or another open model) and report sensitivity of ALIGN/HAL/EXAG to the extractor choice. If results are stable, it will strengthen robustness claims, but if not, discuss limitations.\n3. How do the results look at per-country level or per-activity level? Which countries drive the gap? Why do particular countries (in GS bucket) perform better/worse?\n4. Is the 7% improvement with human judgment statistically significant? Report confidence intervals / p-values for the correlation improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U2XDnCeUS0", "forum": "opG4m2U0Oo", "replyto": "opG4m2U0Oo", "signatures": ["ICLR.cc/2026/Conference/Submission7832/Reviewer_yiqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7832/Reviewer_yiqA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504270463, "cdate": 1761504270463, "tmdate": 1762919877988, "mdate": 1762919877988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces CULTIVate, a new cultural activity focused benchmark for cultural faithfulness of text-to-image models spanning 16 countries and 576 prompts. They use cultural descriptors generated by LLMs to create AHEaD metrics to measure cultural alignment, hallucination, exaggerated elements (stereotypes), and descriptor diversity. They find T2I models are more aligned to the cultures of countries from the Global North than the Global South, and show strong Spearman rank correlation of their proposed AHEaD metrics to human judgments via a user study."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This works makes contributions towards cultural benchmarking of text-to-image models, which is a relatively underexplored and important area of research \n* They provide a new benchmark that is focused on cultural activities in contrast to prior work that focuses more on cultural objects (e.g. landmarks). The benchmark size (576 prompts), categorical coverage (9 super categories of activities) and regional coverage (16 countries) is of similar order to prior work [1- 3, 5].\n* The proposed AHeAD metrics are based on interpretable cultural descriptors (Fig 4) and correlate well with human judgments when compared to baseline metrics of image-text alignment and multimodal LLM judges (Tab. 2)\n\nNote: see references in \"Weaknesses\""}, "weaknesses": {"value": "## Major Weaknesses:\n\n* I disagree with a central claim of the authors, i.e. that their benchmark \"... capture the contextual complexity **missing from object-centric benchmarks**\" (L74). Prior work has already evaluated on contextually-specific cultural activities, e.g. celebrations / festivals / performance arts [1- 5], weddings [4], religious activities [1], sports [1, 2, 6], cooking [6]. The authors claim that their activity-focused benchmark highlights \"**new failure patterns**\" such as \"hallucinated elements\" or \"heavily exaggerated scenes\" (L76 - 78). This behavior has also been established in prior work that discusses culture-specific stereotyping in detail [1, 2, 7]. They also claim that prior work [1, 3] rely on VLM metrics that inherit similar cultural biases, however these works also use visual perceptual similarity and diversity based metrics that do not use VLMs. In fact Rege et al. stresses on this scorer bias ('generative entanglement'), which is not mentioned in this work.\n\n* The authors stress that prior metrics using VLMs will inherit their biases (L85), but use multimodal LLMs (GPT-4o and Gemini 2.5 Flash) to generate culturally-specific visual descriptors (L206 - L215). Since these MLLMs are also trained on internet data, will they not also inherit similar biases as VLMs? Since the authors have highlighted VLM biases, they should also discuss if MLLMs are similarly biased, as this directly affects their AHeAD metrics. If they are not similarly biased, the authors should point to empirical evidence showing so explicitly in the paper.  \n\n* For EXAG, the authors use an LLM to generate *\"exaggeration candidates\"* (L245). How do we know that these are culturally accurate? Are these candidates rated by humans who identify with these cultures? \n\n* When comparing against image-text alignment baselines (Tab 2), what prompts are used? What configuration of CuRe I-T alignment scorer [3] was used? What prompts are used for the MLLMs (InternVL and QwenVL?) These details are very important to establish a fair comparison to baselines. \n\n## Minor Weaknesses:\n \n* The paper is generally written well, but has several vague descriptions that make the contributions hard to follow:\n    * L117: *\"Our AHEaD metrics achieve 7% better correlation with human judgment\"* - better than what, and what are the humans judging?\n    * L118: *\"The best agreement is achieved ... \"* - agreement between what?\n    * L212: *\"...maximizing recall of cultural elements\"* - recall with respect to what? In my understanding, there is no ground truth descriptor set to which the LLM predictions can be compared to?\n\n* In their user study, the authors mention that they \"conduct direct discussions with annotators when facing inconsistent scoring and explanations\"- what does this mean? Do the authors instruct these annotators on how to \"correctly\" score or write explanations?\n\n* Fig. 3a is quite hard to visualize; the legend colors do not match, and one metric is better if higher and the other if lower. Consider using a side-by-side bar chart for this.\n\n### References \n---\n[1] Khanuja et al., An image speaks a thousand words, but can everyone listen? on image transcreation for cultural relevance. *In EMNLP, 2024.*\n\n[2] Zhang et al., Partiality and misconception: Investigating cultural representativeness in text-to-image models. *In CHI, 2024.*\n\n[3] Rege et al., CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems. *In ICCV, 2025*.\n\n[4] Basu et al., Inspecting the geographical representativeness of images from text-to-image models. *In ICCV, 2023*.\n\n[5] Kannen et al., Beyond aesthetics: Cultural competence in text-to-image models. *In NeurIPS D&B Track, 2024*.\n\n[6] Romero et al., Cvqa: Culturally-diverse multilingual visual question answering benchmark. *In NeurIPS D&B Track, 2024*.\n\n[7] Jha et al., Visage: A global-scale analysis of visual stereotypes in text-to-image generation. *In ACL, 2024.*"}, "questions": {"value": "* How do the authors use the \"free text rationales\" from the user study (L354)?\n* How many annotators provide ratings for each form (L341)? This is important to know how many people agree / disagree about ratings.\n* It would be nice for L32 - 36 in the introduction to have citations to prior studies showing the effort and importance of cultural adaptation\n* What is used to compute sentence embedding similarity in the ALIGN metric (L235)? Is this metric also culturally biased?\n* The prompts in the Appendix (Tab 12 - 17, 26) are cut off and unreadable in full"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b0oJvWYK4V", "forum": "opG4m2U0Oo", "replyto": "opG4m2U0Oo", "signatures": ["ICLR.cc/2026/Conference/Submission7832/Reviewer_b2M6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7832/Reviewer_b2M6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850388792, "cdate": 1761850388792, "tmdate": 1762919877687, "mdate": 1762919877687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CULTIVate, a benchmark of social activities across 16 countries / 9 activity types / 576 prompts / 19k+ generations and propose AHEaD, four descriptor-based, automatic metrics for cultural evaluation: Alignment, Hallucination, Exaggeration, and Diversity. Their composite FAITH score correlates better with human judgments than popular image-text metrics, and they document a consistent Global North > Global South performance gap across SOTA T2I models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Interpretable diagnostics: Can list top/bottom descriptors per country/activity to see what is missing or overdone, useful for model iteration\n2) No per-image human labels are needed to compute metrics, and it shows good improvements over existing metrics."}, "weaknesses": {"value": "1) Benchmark spans 16 countries, but the human study covers only a smaller subset (7 countries), with no clear rationale or GN/GS/per-country annotation breakdown\n2) Descriptors are produced with GPT-4o and Gemini-2.5 Flash, yet “MLLM-as-judge” baselines cover only InternVL/QwenVL - no strong judge like Gemini 2.5 Flash/pro or GPT-4o-vision. Makes it hard to evaluate whether different descriptors are needed and to justify that we need more than one Judge model."}, "questions": {"value": "1) Why not representation from more countries (only 16)? Also, what's the rationale behind surveying only 7 countries? \n2) How many annotators rated each image/prompt (per-item replicates)? Also, are the 189 human forms for generated images total across models or per model? If total, the effective sample per model is small, please report per-model/per-item counts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "en3gRCT82E", "forum": "opG4m2U0Oo", "replyto": "opG4m2U0Oo", "signatures": ["ICLR.cc/2026/Conference/Submission7832/Reviewer_RoXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7832/Reviewer_RoXn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960433051, "cdate": 1761960433051, "tmdate": 1762919877129, "mdate": 1762919877129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce metrics to effectively evaluate images generated by text-to-image models. The authors first propose building a dataset of 576 prompts from 16 countries, totalling 19000 images. The authors also propose 4 metrics to measure alignment, hallucination, exaggeration, and diversity. The authors conduct a small human study to rate images and compare their metrics with human correlation. They further calculate scores for generated images using their metrics to demonstrate that the disparity between countries in the global north and south, where models perform better on the former."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce new metrics to measure how well T2I models depict cultural activities. As T2I models are being deployed in many parts of the world, this is a very important problem that needs to be tackled, and reliable metrics are essential.\n2. The idea behind the metrics is well motivated. I agree that existing works mostly quantify alignment and quality metrics and might miss some nuances. Going beyond these and explicitly calculating hallucination and exaggeration is a nice direction."}, "weaknesses": {"value": "1. **CULTIVate dataset:** The authors introduce a large scale dataset comprising 576 prompts and 19k images, but it is not clear what the utility of the dataset is beyond reporting the AHEaD metrics on them. Many of these image-prompt pairs have no human annotations to compare the correlation of metrics with. Could the authors provide more insights on this? \n2. **Human Study:** The authors have conducted a very small human study with a limited number of prompts, i.e, only 9 activities per country for 7 countries, which totals 63 prompts out of 576. Moreover, out of the 6 models used, the authors have conducted the study only for 3 public models. The authors use this to report the correlation between human scores and their metric scores. Due to the small size of the human study, which does not even cover all the countries in the CULTIVate dataset, it is not clear how significant the correlations are. This makes the claim of AHEaD metrics' efficacy weak and the overall study less reliable. I understand that getting human judgments is hard for all the prompts and images, but a representative set of prompts covering all the countries should be used. It would be great if the authors could provide insights on this.\n3. **Metric Definitions:**\n\t1. In L85, the authors contend that existing metrics have a problem in that they use VLM internal knowledge, which inherits biases. But it looks like the author's entire pipeline is based on VLM knowledge. This involves generating descriptors for a prompt and generated images, which are based on the VLM's internal knowledge of the activity. The authors also generate potential stereotype candidates for a prompt which again relies on VLM's knowledge of potential stereotypes that might be biased. This feels a bit contradictory to the initial claim. The authors should either revise the claim or provide more elaboration on why and how this method does not inherit the cultural biases of VLMs.\n\t2. The core of the metric is to come up with descriptors for a prompt. I am wondering if these capture the full scale of cultural nuances. This is because cultural nuances are also about the values and norms associated with the entities in the images. This can be something like an appropriate gesture, the way people are interacting, or about norms (which people must follow). The descriptors ask for the existence of certain elements, but do they also ask for non-existence? In certain activities, not observing some norms is as important as certain aspects being present. Does the metric account for this? Moreover, a given festival/activity can be celebrated in different ways and have different subcomponents. Generating an exhaustive set of descriptors and comparing with them might not be ideal, as there might be some really valid images that do not cover all possible descriptors generated by the model. Since the metrics divide by the sum of all descriptors, this might underreport the numbers. Hence, I feel this approach does not capture a significant part of cultural nuances. Do the authors have any thoughts on this?\n\t3. The authors generate stereotype candidates for exaggeration using VLMs and also descriptors. Have the authors verified the accuracy of the different types of descriptors? In L669, the authors mention 90% agreement with humans with their framework. It is not very clear which exact descriptors these agreements are, i.e., whether for alignment, exaggeration. Could the authors provide more details of the human study? Table 9 provides a breakdown, but these are descriptors validated by humans, i.e., precision. Is there any study for recall? It could be possible that there are some essential ones that the LLM missed?\n\t4. The authors define diversity and semantic diversity, but only report numbers using this. Since the authors introduce this as a new metric, they need to quantify how good this metric is compared to those like the Vendi score and that proposed by [1] and other relevant baselines, which has not been done.\n4. **Results:**\n\t1. The authors have compared their metrics with a limited set of baselines. Some notable baselines that the authors don't evaluate include VIEScore [2], UnifiedReward [3], which have been shown to have good correlation with human judgements [4]. Moreover, the authors have not computed the scores for GPT-4o or Gemini 2.5 (without descriptors, Table 3) to provide scores for faithfulness in Table 2. Only InternVL and QwenVL are used. It is unclear why the authors compare with these models.\n\t\t1. A minor point is why QwenVL and InternVL are used for baselines. There is Qwen2.5VL and Intern2.5VL also. Why not use these more powerful models? If the authors have used these, then due to a lack of citation, it is not clear.\n\t\t2. The authors should report the numbers of GPT and Gemini with the same prompt of faithfulness that they ask humans for a fair comparison, since they use these models to generate descriptors. [4] already show that these are very good for similar kinds of prompts, and hence it would be an interesting comparison.\n\t2. It seems from Tables 5,6,7 that the proposed metrics don't correlate very well with humans as compared to baselines like InternVL and QwenVL. It feels like for 2/3 models, QwenVL does better on hallucination (Table 6). Also, for exaggeration, QwenVL does better overall (Table 7). Moreover, it looks like the authors' metrics underperform for the global south, which again could be the result of biases in the descriptors or because of a small human study. This is especially important as authors use this to make claims for disparity between the Global South and North. Could the authors provide more insights into this behavior as this is confusing.\n\t3. The claim of disparity between Global South and North needs more substantiation. The AHEaD numbers reported are quite close in several cases and hence it would be good to report the mean and standard deviation to quantify how significant these differences are. Also what are the scores provided by humans and how do they differ across the global north and the south?\n\t4. Why is human-human correlation almost zero for humans for the Qwen model in Table 8? Similarly, in Table 9.\n5. **Minor Issues:** Here are some minor writing related errors: L425 (should be do not ask human annotators?), L240 which has an incomplete sentence, Table 7 has wrong numbers bolded. Lack of citations for models in Table 2.\n\n**References:**\n\n[1] Kannen et al. Beyond Aesthetics: Cultural Competence in Text-to-Image Models. \n\n[2] Ku et al. VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation\n\n[3] Wang et al. Unified Reward Model for Multimodal Understanding and Generation\n\n[4] Nayak et al. CULTURALFRAMES: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics"}, "questions": {"value": "Please see Weakness. I have asked the questions there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QiauxitpLF", "forum": "opG4m2U0Oo", "replyto": "opG4m2U0Oo", "signatures": ["ICLR.cc/2026/Conference/Submission7832/Reviewer_rXJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7832/Reviewer_rXJH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762516084059, "cdate": 1762516084059, "tmdate": 1762919876676, "mdate": 1762919876676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}