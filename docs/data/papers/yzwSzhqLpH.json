{"id": "yzwSzhqLpH", "number": 15761, "cdate": 1758254979159, "mdate": 1759897283938, "content": {"title": "Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding", "abstract": "Molecular understanding is central to advancing areas such as science and drug discovery, yet large language models (LLMs) struggle to understand molecular graphs effectively. Existing graph–LLM bridges often adapt a Q-Former–style connector with fixed-length static tokens originally designed for vision tasks. These designs overlook stereochemistry and substructural context and typically require costly LLM-backbone fine-tuning, limiting efficiency and generalization. We introduce EDT-Former, an Entropy-guided Dynamic Token Transformer that generates tokens aligned with informative molecular patches, preserving both local and global structural features for molecular graph understanding. Beyond prior approaches, EDT-Former enables alignment between frozen graph encoders and LLMs without tuning the LLM backbone, resulting in computationally efficient fine-tuning, and it achieves state-of-the-art results on the MoleculeQA and Mol-Instructions benchmarks, underscoring its effectiveness for scalable and generalizable multimodal molecular understanding.", "tldr": "EDT-Former: entropy-guided dynamic query tokens map molecular graphs to LLMs, capturing local and global structure features for comprehensive understanding and reasoning with backbone-free, connector-only training.", "keywords": ["Multimodal Modeling", "Graph–LLM Alignment", "Molecule Understanding", "Backbone-Free Tuning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/341a0982f8d4b04456338ebf4cba90549c104393.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ​​EDT-Former​​, a novel method for aligning molecular graphs with Large Language Models (LLMs). It addresses key limitations of existing approaches, such as Q-Former-style bridges, which use a fixed number of tokens that often lead to loss of stereochemical and substructural details, especially in larger molecules.\nThe core innovation lies in its two components: an ​​Entropy-Guided Patching​​ strategy that dynamically segments molecules into informative, variable-length tokens based on structural uncertainty, and a ​​Dynamic Query Transformer​​ that integrates these tokens with static modality anchors. EDT-Former achieves this alignment without fine-tuning the LLM backbone, enabling highly efficient training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an innovative solution to a clear limitation of existing Q-Former-style bridges.\n\n2. The results demonstrate state-of-the-art performance across multiple benchmarks.\n\n3. The framework is trained without fine-tuning the LLM backbone, which is efficient."}, "weaknesses": {"value": "1. Evaluation is centered on question-answering and property prediction. The method's effectiveness on more challenging generative tasks, such as molecule generation,  remains an open and important question.\n\n2. This paper should discuss the difference with more molecular graph-text pretraining frameworks, such as [1,2]. \n\n3. This paper uses the SMILES string to represent the molecular structure, which might not capture the structure of the molecule well. The graph structure or 3D structure could contain more structural information.\n\n[1] Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing\n\n[2] Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f06aL9FyIy", "forum": "yzwSzhqLpH", "replyto": "yzwSzhqLpH", "signatures": ["ICLR.cc/2026/Conference/Submission15761/Reviewer_XpYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15761/Reviewer_XpYi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829243814, "cdate": 1761829243814, "tmdate": 1762925993957, "mdate": 1762925993957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two major limitations of existing graph-Large Language Model (LLM) frameworks for molecular understanding: 1) information loss from using fixed-length token connectors (e.g., Q-Former), which compress complex, variable-sized molecular graphs into a static representation, and 2) the high computational cost and poor generalization resulting from fine-tuning the entire LLM backbone.\nThe paper proposes **EDT-Former**, an \"Entropy-guided Dynamic Token Transformer,\" as a novel connector. The key idea is to generate a *variable* number of tokens that are aligned with a molecule's structural complexity. This is achieved through a two-part mechanism: entropy-guided patching and a dynamic query transformer. The paper demonstrates the best performance on a wide range of benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The paper proposes a novel query Transformer. Most works simply abstract molecules into queries, not considering the stereochemistry and structural context in the molecule. Another line of work exploits rule-based algorithms to extract the meaningful substructures. Different from them, the proposed work automatically extracts substructures by applying entropy-based patching segments.\n- Experimental results demonstrate the effectiveness of EDT-Former with its superior performance compared to molecular LLMs and General LLMs.\n- The paper provides a rigorous analysis, which helps in understanding the contribution of the proposed component."}, "weaknesses": {"value": "- The entire entropy-patching mechanism is based on a 1D SMILES sequence. The properties of a SMILES string (and thus its entropy profile) can change based on the canonicalization algorithm used or if non-canonical strings are permitted. The paper does not discuss the robustness of the patching mechanism to different, yet chemically equivalent, SMILES representations of the same molecule."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OPJxVFD0iR", "forum": "yzwSzhqLpH", "replyto": "yzwSzhqLpH", "signatures": ["ICLR.cc/2026/Conference/Submission15761/Reviewer_x1GF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15761/Reviewer_x1GF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979370832, "cdate": 1761979370832, "tmdate": 1762925993601, "mdate": 1762925993601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the multimodal LLMs for aligning LLMs with molecular graphs. The authors propose  EDT-Former, a connector-only approach that generates fixed query tokens for molecules in different lengths. EDT-Former includes (i) Entropy-Guided Patching, which uses next-atom surprisal peaks from a lightweight SMILES predictor to segment molecules into substructure-aware patches, and (ii) a Dynamic Query Transformer that fuses these variable-length “dynamic tokens” with a small set of learnable modality anchors before projecting into the LLM space. Experiments across multiple tasks and benchmarks show the effectiveness of EDT-Former."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work targets on fixed-token bottleneck in graph-LLM alignment, which is timely and critical;\n\n2. The proposed approach is novel and interesting;\n\n3. There are significant empirical improvements;"}, "weaknesses": {"value": "1. The benchmarked tasks seem to be limited. For example, can this approach be applied to other tasks in Mol-Instructions?\n\n2. The empirical comparison seems not to be fair, as EDT-Former uses different training corpus with other baseline approaches. Given the efficiency of the proposed approach, can EDT-Former be applied and ablated with different instruction training data?\n\n3. Lack of comparison and discussion with a closely related work [1]. For example, can the proposed tokenization scheme mitigate the hallucination issue mentioned in [1]?\n\n- HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment, ICML'25."}, "questions": {"value": "Please find the details in the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t4DdNfZ20C", "forum": "yzwSzhqLpH", "replyto": "yzwSzhqLpH", "signatures": ["ICLR.cc/2026/Conference/Submission15761/Reviewer_x8D4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15761/Reviewer_x8D4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997556192, "cdate": 1761997556192, "tmdate": 1762925993033, "mdate": 1762925993033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDT-Former, for aligning molecular graphs with large language models (LLMs) under frozen-backbone settings. The model addresses two issues common in molecular graph–language alignment: (1) loss of structural fidelity caused by fixed-length, Q-Former-style connectors; and (2) inefficient fine-tuning due to large backbone updates. EDT-Former proposes two main mechanisms—Entropy-Guided Patching and a Dynamic Query Transformer—to generate substructure-aware dynamic tokens and integrate them with static anchors for efficient alignment. The corresponding EDT-Former shows promising results on the evaluated benchmarks, including MoleculeQA, Mol-Instructions (molecule captioning and property prediction), Pampa, and BBBP."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Good motivation\n    - This work is well-motivated by the need for a dynamic length graph token that captures molecular substructure information.\n- Methodological novelty\n    - The proposed tokenization is novel, based on entropy-guided segmentation for molecules based on uncertainty peaks from a next-atom predictor, offering a data-driven and deterministic patching mechanism. This design appears to be suitable for learning the representation of molecular functional groups, considering the dynamic size of the molecule.\n- Connector-only alignment design\n    - The Dynamic Query Transformer establishes a modular bridge between frozen molecular encoders and frozen LLMs, requiring no or minimal gradient updates to the LLM. This contributes to computational efficiency and strong performance scalability, aligning with the authors’ focus on low-cost, high-fidelity multimodal integration."}, "weaknesses": {"value": "Overall, I find the proposed method interesting and reasonable (have a different opinion regarding NAP, though); my concerns primarily relate to the experimental setting and the demonstration of the author's hypothesis.\nI hope these are properly addressed in the rebuttal phase.\n\n- Ambiguity in the Description of Experimental Settings and Results\n    - (Major) In line 312, they mention evaluating with Direct, Reasoning, and Rich Instructions prompting to reduce prompt sensitivity, but there is no information in the main body about whether these 3 prompting strategies align with the instructions used for finetuning each baseline model. This is a factor that could largely vary each model's performance and is easy to miss without background knowledge of each model's experimental setting. This descriptive gap makes it difficult to understand if the experiments were conducted fairly.\n    - (Major) Mol-LLaMA is missing from the comparison baselines in Table 4, yet Mol-LLaMA also provides evaluation results for molecule captioning and property prediction in Mol-instructions (Table 17 from Mol-LLaMA), so including it seems appropriate. In line 371, the authors claim Q1, but Mol-LLaMA's experimental results appear to show higher performance, which needs to be addressed. The following is Table 17 from Mol-LLaMA. Please let me know if I have misunderstood anything.\n| Models                         | BLUE-2 | BLUE-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | METEOR | MAE     |\n|--------------------------------|--------|--------|----------|----------|----------|---------|---------|\n| Mol-LLaMA (LLaMA-2)            | 0.478  | 0.425  | 0.761    | 0.698    | 0.750    | 0.701   | 0.0035  |\n| Mol-LLaMA (LLaMA-3)            | 0.476  | 0.426  | 0.767    | 0.708    | 0.759    | 0.707   | 0.0039  |\n\n    - In line 301, the authors state they used Mol-Instructions' evaluation benchmark; however, the provided experimental results used only 2 tasks (molecule captioning and property prediction) out of the total 17 tasks in Mol-Instructions. This could be misunderstood as results spanning the entire Mol-Instructions dataset.\n    - In many sections including Abstract line 20, Introduction lines 114, 119, etc., they describe LLMs as fully frozen, yet in C.3 line 1238, they state that the LLM embedding layer was tuned. Which explanation is correct?\n- (Major) Fairness of Experimental Settings\n    - Fair comparison of baseline models in zero-shot evaluation\n        - As mentioned above, when measuring the zero-shot performance of finetuned models in Table 2, prompting strategies that are not aligned with the finetuning dataset can severely impair model performance. This is because models finetuned on molecular tasks can easily lose natural language following ability, which can be immediately confirmed by running inference on molecular LLMs such as BioT5, LlaSMol, LLaMo, etc., using their HuggingFace open-sourced models. This could create an evaluation setting that is favorable to EDG-Former, which is the only model not finetuned LLM backbone (except for embedding layer). To easily address these concerns, it is suggested that the performance for each prompting strategy be reported before averaging.\n    - Data contamination\n        - The Mol-LLaMa-Instruct used by the authors for EDT-Former's alignment training contains GPT-4o generated molecule captions augmented from Pubchem324K, which can be used as a dataset for molecule captioning tasks. Considering that the source of the Mol-Instructions' molecule captioning dataset used in Table 4 experiments is also PubChem, there is a need to clearly describe whether data contamination exists between the train-test splits of these two datasets. Although the authors claim in D.4 that they performed character-level 13-gram analysis with reference to GPT-3, given that GPT-3 deals with general-purpose text data, not focusing on molecular tasks, this seems insufficient to verify data contamination in molecular tasks. In addition, instead of stating the exact figure for the ratio of data with overlapping 13-characters, the authors merely state it's below 5%, but a figure close to 5% could still be perceptible to humans. To clearly resolve these concerns, it is suggested that a contamination analysis based on scaffold split, which is widely chosen in the molecular domain, be provided.\n- Regarding demonstration of Q3\n    - (Major) Comprarision with fixed length token with enough molecule tokens\n        - The authors introduced dynamic molecule tokenization to address the problem of fixed length molecule tokens losing molecular features due to limited length. However, in E.1, they state through dynamic token maximum length ablation that 64 molecule tokens are sufficient. Regarding this, considering the sequence max length budget of current LLMs, using a fixed length token of 64 is not a significant burden. Is there a performance difference compared to using fixed length tokens of 64? To prove Q3, a comparison of performance between using fixed length tokens of sufficient size and EDT-Former is necessary, which I believe is an essential ablation study given the message of the paper.\n    - Ablation of dynamic token in inference time\n        - Since EDT-Former uses fixed length molecule tokens concatenated with dynamic tokens, there is a need to verify whether it might actually be bypassing dynamic tokens and only using fixed length tokens. This seems easy to check at the inference level - I'm curious about the performance when replacing dynamic tokens and fixed tokens with random or dummy tokens during inference.\n- (Major) Limited justification for entropy predictor selection\n    - The entropy estimation relies on a simplistic GPT-2–based next-atom predictor (NAP) trained on SMILES. While computationally light, the rationale for this choice is mostly under-investigated, given that next-atom in sequence does not necessarily align with molecular subgraph structure, as originally the author aimed to represent by EDT-Former.\n- Interpretive analysis remains descriptive\n    - While attention visualizations (Fig. 6) qualitatively support the claim that dynamic tokens attend to “structural transitions”, the study lacks quantitative or diagnostic analysis. Although I don’t think it is necessary to show that the hypothesis holds for almost all molecules, at least line 250 should be adjusted in light of current experimental evidence. In addition, analysis on specific failure modes (e.g., mis-segmentation, entropy misalignment, or redundant patches) is necessary."}, "questions": {"value": "- Regarding experimental setting of Figure 5. I'm curious about how the experiments in Figure 5 were conducted. Does it correspond to retraining the model excluding each component, or excluding each element only at inference time from the full model inference? There doesn't seem to be an explanation of the experimental setting for Figure 5.\n- On the property prediction benchmark choice. While EDT-Former's zero-shot performance in Table 2 is impressive, I wonder why only BBBP was used among the widely comparable evaluation benchmarks for property prediction such as MoleculeNet that are commonly used among molecular LLMs? Given that the evaluation is for zero-shot performance, using only BBBP and Pampa as property prediction benchmarks seems to both forfeit the advantages of models capable of zero-shot inference and provide a non-comprehensive evaluation. As the authors stated, since EDT-Former is not dependent on LLM finetuning, expanding the evaluation benchmarks would be a viable option as model retraining is not required for benchmark selection. Referring to widely used evaluation benchmarks such as MoleculeNet could easily demonstrate directly convincing results.\n- In cases where molecules contain highly repetitive or symmetric substructures (many molecules could have long carbon chain, resulting in CCCC…), how does entropy-guided segmentation handle indistinguishable regions? Is there a mechanism to prevent redundant substructure tokens?\n- The benchmarks used focus mainly on small molecules. Can the authors discuss or show any evidence regarding scalability to large macrocycles, where token budget and entropy dynamics might differ substantially?\n- In Figure 6, attention patterns are qualitatively aligned to high-entropy regions. Have the authors considered quantifying this alignment (e.g., attention–entropy correlation) to strengthen the link between entropy peaks and attention interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N3GDvggGsY", "forum": "yzwSzhqLpH", "replyto": "yzwSzhqLpH", "signatures": ["ICLR.cc/2026/Conference/Submission15761/Reviewer_y4JF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15761/Reviewer_y4JF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007831600, "cdate": 1762007831600, "tmdate": 1762925991889, "mdate": 1762925991889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}