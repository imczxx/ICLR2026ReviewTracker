{"id": "4KsBX9qxqe", "number": 17339, "cdate": 1758274829674, "mdate": 1763763141311, "content": {"title": "Neural Low-Discrepancy Sequences", "abstract": "Low-discrepancy points are designed to efficiently fill the space in a uniform manner. This uniformity is highly advantageous in many problems in science and engineering, including in numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. Whereas most previous low-discrepancy constructions rely on abstract algebra and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced to exploit machine learning methods for generating point sets with lower discrepancy than previously possible. However, MPMC is limited to generating point sets and cannot be extended to low-discrepancy sequences (LDS), i.e., sequences of points in which every prefix has low discrepancy, a property essential for many applications. To address this limitation, we introduce Neural Low-Discrepancy Sequences (NeuroLDS), the first machine learning-based framework for generating LDS. Drawing inspiration from classical LDS, we train a neural network to map indices to points such that the resulting sequences exhibit minimal discrepancy across all prefixes. To this end, we deploy a two-stage learning process: supervised approximation of classical constructions followed by unsupervised fine-tuning to minimize prefix discrepancies. We demonstrate that NeuroLDS outperforms all previous LDS constructions by a significant margin with respect to discrepancy measures. Moreover, we demonstrate the effectiveness of NeuroLDS across diverse applications, including numerical integration, robot motion planning, and scientific machine learning. These results highlight the promise and broad significance of Neural Low-Discrepancy Sequences.", "tldr": "", "keywords": ["low-discrepancy sequences", "Quasi-Monte Carlo", "numerical integration", "scientific machine learning", "motion planning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c1a23e0d86d3b6f8347649c27d2af77c4666725.pdf", "supplementary_material": "/attachment/45d28429619f0158769d36baaebb43f3947338d9.zip"}, "replies": [{"content": {"summary": {"value": "The paper “Neural Low-Discrepancy Sequences (NEUROLDS)” introduces the first machine learning framework for generating low-discrepancy sequences which are ordered sets of points that uniformly cover a space such that every prefix maintains low discrepancy, a property vital in numerical integration, robotics, and scientific computing. Unlike the Message-Passing Monte Carlo (MPMC) approach that only produces fixed-size low-discrepancy sets, NEUROLDS learns an extensible sequence by mapping integer indices to points using sinusoidal positional encodings and a multilayer perceptron. Training proceeds in two stages: pretraining to mimic classical sequences (e.g., Sobol’) and fine-tuning via differentiable L2-based discrepancy losses over all prefixes. Experiments show that NEUROLDS achieves substantially lower discrepancies than traditional Sobol’, Halton, and scrambled Sobol’ sequences, leading to superior performance in quasi-Monte Carlo integration (Borehole benchmark), robot motion planning (RRT), and scientific machine learning (Black–Scholes PDE). Ablation studies highlight the importance of Sobol’-based pretraining, sufficient network depth, and frequency encoding. Overall, NEUROLDS bridges number-theoretic QMC methods and neural architectures, offering a flexible, extensible, and empirically superior approach to uniform sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. NEUROLDS is the first neural framework capable of generating low-discrepancy sequences, solving a major limitation of previous learning-based methods like MPMC.\n\n2. The combination of supervised pretraining on classical sequences (e.g., Sobol’) followed by unsupervised discrepancy minimization ensures both stability and performance.\n\n3. NEUROLDS consistently achieves lower discrepancy and better integration accuracy across diverse applications, numerical integration, robot motion planning, and scientific ML, demonstrating versatility.\n\n4. The index-based formulation allows sequences of arbitrary length, unlike fixed-size MPMC sets, making it practical for adaptive sampling.\n\n5. It maintains links with classical quasi-Monte Carlo theory through kernel-based discrepancy losses and sinusoidal encodings analogous to number-theoretic digit expansions."}, "weaknesses": {"value": "1. The method heavily relies on Sobol’ or Halton prealignment; direct training from scratch fails, indicating limited robustness in initialization.\n\n2. Fine-tuning discrepancy losses over all prefixes scales quadratically $(\\mathcal{O}(N^2))$ with sequence length, which may limit scalability to very large $N$.\n\n3. While empirically superior, there are no formal proofs of discrepancy bounds or convergence rates as exist for classical sequences.\n\n4. Performance depends on tuning several parameters (network depth, sinusoidal frequencies, weighting schemes), requiring careful optimization.\n\n5.  Current formulation focuses on uniform sampling on $[0,1]^d$; extensions to non-uniform or adaptive distributions should be further discussed e.g., on the sphere [1].\n\n[1] Quasi-Monte Carlo for 3D Sliced Wasserstein, Nguyen et al"}, "questions": {"value": "1. Can we derive the rate of convergence?\n\n2. Can we make a randomized version of NEUROLDS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zg6Xus0yXc", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Reviewer_ksof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Reviewer_ksof"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760888568150, "cdate": 1760888568150, "tmdate": 1762927263187, "mdate": 1762927263187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach, NeuroLDS, for generating low-discrepancy sequences using neural networks, addressing a key limitation of prior learning-based methods like MPMC which could only produce fixed-size point sets. However, the study is marred by significant weaknesses in its experimental validation and methodological justification. The claims of superior performance are not fully supported by comprehensive comparisons, and the choice of model architecture lacks sufficient ablation analysis. Furthermore, critical practical aspects such as the computational cost and training efficiency of the proposed method are not discussed, raising concerns about its practicality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces NeuroLDS, the first machine learning-based framework for generating low-discrepancy sequences (LDS).\n2. The proposed two-stage training process—supervised pre-training on classical sequences followed by unsupervised fine-tuning on a discrepancy loss—is demonstrated to be effective."}, "weaknesses": {"value": "1.  The core discrepancy analysis is primarily conducted in 4 dimensions. The performance and scalability of NeuroLDS in both lower (1D, 2D, 3D) and higher (5D+) dimensions remain largely unexamined.\n2. The set of baseline methods is not consistent across all application experiments (e.g., NM-Greedy is absent from the motion planning study). This undermines the fairness and comprehensiveness of the cross-experimental comparison.\n3. The choice of a simple MLP over more established sequence-based models (e.g., Transformers, LSTMs) is not adequately justified. While an autoregressive GNN was tested and dismissed, the rationale for not exploring other powerful sequence-modeling architectures is lacking and should be supported by further discussion or ablation studies."}, "questions": {"value": "1. The authors use a simple MLP for a sequence-generation task. There is no explanation for why more established sequence-based models, such as LSTMs or Transformers, were not considered or tested.\n2. Inconsistent Baselines: The baseline methods used are not consistent across the different application experiments. For a fair and comprehensive comparison, all applied tests (e.g., numerical integration, robot planning) should evaluate the same set of baseline methods, including Sobol', Halton, NM-Greedy, and Uniform sampling.\n3. The paper does not discuss or compare against several important recent works in the field, such as Atanassov's methods\n4. The analysis of discrepancy is primarily conducted only in 4 dimensions. The performance and scalability of the proposed method remain unclear for other dimensions, both lower (1, 2, 3) and higher (5+).\n5.  A direct and crucial comparison with Message-Passing Monte Carlo (MPMC) is lacking. Specifically, for the same number of points (N), how does the discrepancy of a NeuroLDS-generated set compare to a set generated by MPMC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yVv2Tp4n2X", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Reviewer_wMj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Reviewer_wMj7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761055502713, "cdate": 1761055502713, "tmdate": 1762927262752, "mdate": 1762927262752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to all the reviewers"}, "comment": {"value": "At the outset, we would like to thank all five reviewers for their thorough and patient reading of our article. Their fair criticism and constructive suggestions have enabled us to improve the quality of our article. A revised version of the article is uploaded. We proceed to answer the points raised by each of the reviewers individually, below. We would also like to point out that all the references to page numbers, sections, figures, tables, equation numbers and references, refer to those in the revised version. We hope that based on the answers to their questions and the improvements made to the revised version, the reviewers will increase their scores."}}, "id": "0yCeYO5WvI", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763763188571, "cdate": 1763763188571, "tmdate": 1763763188571, "mdate": 1763763188571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes a new method to define collocation points, in particular, to construct low-discrepancy sequences, with which we can achieve good accuracy even when truncated to a moderate number of points, using a neural network. The neural network receives an index and returns a point. If the integrand lies in an RKHS, the error between the true integral and the finite-point approximation is bounded by the product of the integrand's variation and the discrepancy that depends on the chosen points. Since the discrepancy can be computed from the kernel, we can choose points that minimize it. Therefore, the neural network is trained to output a sequence that minimizes equation (2) for every N."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "As stated in the introduction, the approach is clearly supported by theory, and uses a neural network to resolve the intractable portion, extending the reach of theoretical research."}, "weaknesses": {"value": "This paper is a direct extension of Rusch et al. (2024). Equation (2) in this paper corresponds to equation [2] in Rusch et al. (2024), and the evaluation protocol is also an extension of theirs. Although there is some improvements in the neural network design, that part does not seem essential. For example, if this were a workshop submission supplementing a main-conference paper, such a proposal might be sufficient. However, it does not appear to reach the level of originality expected for the conference main track.\n\nThe paper claims that N need not be fixed, but in practice. one must train a GNN on sequences up to N, so in that sense N cannot be freely set.\n\nThe comparisons are mostly in terms of accuracy, but accuracy naturally improves as the number of points increases. What matters in point selection is the number of points (that is, the computational cost) required to achieve the same accuracy. According to Figure 2 and related results, the efficiency gain appears at most about a factor of two. Once we account for training the neural network, the total computational cost may not be favorable.\n\nIntroducing $\\gamma$ makes the method integrand-dependent, which implies retraining for each problem and, in turn, an increase in total computational cost. A fair comparison on this point is also needed.\n\nThe loss is defined as an average over all N, but this seems misaligned with the definition of low-discrepancy sequences. An approach that would align with the definition is: optimize a GNN for N=1; once that is done, fix the output for N=1 and then optimize the GNN for N=2; once that is done, proceed to N=3; and so on.\n\nFor this purpose, it may not be necessary to use a GNN at all, right? If the values of the sequence can simply be stored in a table, what is the reason to generate them with a GNN?\n\nQMC methods allow collocation points randomized, which provides variance estimates. Does the inability to randomize become a disadvantage for the proposed method? Existing methods can be randomized; for the proposed method, one could randomize the sequences for pre-training and train GNNs multiple times, then examine performance variance. Such an evaluation seems necessary.\n\nEmpirically, the method shows good accuracy, but theoretical guarantees are limited. Existing methods have known orders. What about this method? For example, using similar functions while varying the dimension and the number of points, one could regress a two-dimensional plot to estimate the order at least approximately. My guess is that the improvement is a constant-factor one.\n\nMinor Comments:\n\nThe citation style is odd. In places where citep should be used, citet is used, so it is unclear whether the citation is part of the sentence or not.\n\nThe theoretical core is written in the introduction, and the Methods section contains a long explanation of related work, after which the proposal begins. The overall structure could be improved."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vTSgEO2x6x", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Reviewer_92S7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Reviewer_92S7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574423881, "cdate": 1761574423881, "tmdate": 1762927262409, "mdate": 1762927262409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors employ a learning-based approach to generate quasi-random samples. However, the main concern lies in the fact that this approach relies on classical quasi-random methods (such as Sobol’ or Halton sequences) to generate the training data. This creates a dilemma: the performance of the trained model inherently depends on the quality of the training samples. If the proposed method truly works as claimed, it would imply that one could achieve the same integration accuracy using only 100 samples as with 10,000 samples without any additional cost, which seems questionable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "No."}, "weaknesses": {"value": "1. Lack of Distinct Advantage: Unlike other learning-based methods used in scientific applications, the proposed framework does not demonstrate a clear advantage compared to classical methods. For instance, Physics-Informed Neural Networks (PINNs) are valuable for solving high-dimensional or inverse problems compared to classical numerical methods. In contrast, it is unclear what specific capability the proposed method offers that cannot already be achieved by classical approaches.  \n2. Absence of Theoretical Foundations: The work provides no theoretical guarantees or analysis (e.g., discrepancy bounds or convergence rates). This is a critical shortcoming for a method that aims to replace well-established number-theoretic constructions, making it impossible to evaluate its reliability or general behavior beyond limited empirical observations.  \n3. Lack of High-Dimensional Validation: The experiments are primarily conducted in low-dimensional settings (e.g., 2d or 4d). The absence of results in high-dimensional cases (e.g., $d > 10$) significantly undermines the claimed generality and applicability of the method, particularly for modern problems in machine learning and computational finance."}, "questions": {"value": "see the Weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l7H0aGD1eq", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Reviewer_FumR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Reviewer_FumR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921552867, "cdate": 1761921552867, "tmdate": 1762927261587, "mdate": 1762927261587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of approximating a high-dimensional integral using Monte-Carlo methods. For simplicity, the authors assume that the function is supported on the hypercube $[0, 1]^d$, and the goal is to generalize data points that are as distributed as uniformly as possible. Since the approximation error is upper bounded by the kernel discrepancy of the point sequence $\\lbrace X_i \\rbrace_{i=1}^{N}$, a natural goal would be to generate a sequence whose kernel discrepancy is as low as possible. To generate such a low-discrepancy sequence (LDS, to be differentiated from a low-discrepancy set), the authors propose NeuroLDS, an L-layer neural network that maps an integer index $i$ to a point $X_i \\in [0,1]^d$. This \"index-driven\" architecture is inspired by classical LDS like Sobol' and Halton sequences. The authors also propose a two-stage optimization procedure for training this neural network and conduct extensive numerical experiments to demonstrate the superior performance of NeuroLDS as compared to classical methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall I think this is an interesting paper that makes contribution to an active research area. The main results are also clearly written."}, "weaknesses": {"value": "(i) I think the goal of NeuroLDS is to generate a low-discrepancy sequence, not just a low-discrepancy set of points. One would expect to generate an infinite sequence whose all N-prefixes are of low-discrepancy, is it correct? If so, I am curious how the trained network generalizes to indices $i > N$.\n(ii) The experiments are conducted in relatively low dimensions (i.e., $d=2, 4, 8$). However, the key challenge for classical LDS is the \"curse of dimensionality.\" It is still unclear if NeuroLDS offers any fundamental advantage in truly high-dimensional settings, for example, $d > 20$?\n(iii) I understand that this is a purely experimental paper, but it would be better to have some theory, even non-rigorous heuristic calculations. What is the intuition behind the empirical observation that NeuroLDS outperforms classical LDS?"}, "questions": {"value": "Please see \"Weaknesses\" section. This paper is well written so I do not have minor comments on the writting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LSplphXbhB", "forum": "4KsBX9qxqe", "replyto": "4KsBX9qxqe", "signatures": ["ICLR.cc/2026/Conference/Submission17339/Reviewer_w9Dh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17339/Reviewer_w9Dh"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064551586, "cdate": 1762064551586, "tmdate": 1762927261079, "mdate": 1762927261079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}