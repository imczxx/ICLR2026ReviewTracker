{"id": "C47sdEjWEL", "number": 16208, "cdate": 1758261674526, "mdate": 1759897254490, "content": {"title": "Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image", "abstract": "Accurate and generalizable metric depth estimation is crucial for various computer vision applications but remains challenging due to the diverse depth scales encountered in indoor and outdoor environments. In this paper, we introduce Metric-Solver, a novel sliding anchor-based metric depth estimation method that dynamically adapts to varying scene scales.\nOur approach leverages an anchor-based representation, where a reference depth serves as an anchor to separate and normalize the scene depth into two components: scaled near-field depth and tapered far-field depth. The anchor acts as a normalization factor, enabling the near-field depth to be normalized within a consistent range while mapping far-field depth smoothly toward zero. Through this approach, any depth from zero to infinity in the scene can be represented within a unified representation, effectively eliminating the need to manually account for scene scale variations.\nMore importantly, for the same scene, the anchor can slide along the depth axis, dynamically adjusting to different depth scales. A smaller anchor provides higher resolution in the near-field, improving depth precision for closer objects while a larger anchor improves depth estimation in far regions. \nThis adaptability enables the model to handle depth predictions at varying distances and ensure strong generalization across datasets. \nOur design enables a unified and adaptive depth representation across diverse environments. Extensive experiments demonstrate that Metric-Solver outperforms existing methods in both accuracy and cross-dataset generalization.", "tldr": "Propose a new and flexible absolute depth representation method that balances depth prediction accuracy and large-range long-distance depth prediction.", "keywords": ["Metric depth estimation", "sliding anchor", "unified model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d6bf29697f58ac2a41c39fe1fcc4b4cd0bb6a5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article proposes Metric-Solver, a novel sliding anchor-based monocular metric depth estimation method, to address challenges of diverse depth scales in indoor/outdoor scenes. It uses a reference anchor to split scene depth into scaled near-field (linear normalization) and tapered far-field (exponential normalization) components, enabling unified representation of depths from 0 to infinity. The anchor slides along the depth axis to adapt to inter/intra-scene variations, enhancing precision for near objects and stability for far regions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Efficient architecture**: Adopts \"one shared DINOv2 encoder + two lightweight decoders\"—shared features reduce redundancy, and lightweight decoders ensure efficiency. Only adds ~58M parameters and ~4ms inference time vs. DepthAnything, supporting 125 FPS real-time inference.\n\n**Strong generalization**: Outperforms baselines in zero-shot tests across 4 indoor/outdoor datasets (e.g., iBims, DIODE) and achieves SOTA on NYU-V2/KITTI. No need for scene-specific assumptions (e.g., fixed max depth), showing robust cross-dataset adaptability/"}, "weaknesses": {"value": "1. **Incomplete Computational Efficiency Comparison**\nThe paper presents efficiency metrics (parameters, inference time, memory) in Table 4 but lacks two key elements: a direct comparison with the baseline ZoeDepth (a frequently referenced method in metric depth estimation) and FLOPS data—critical for evaluating computational overhead. Adding these would help readers assess if performance gains are balanced with practical deployment costs.\n2. **Underutilization of Sliding Anchors in Inference**\nTo ensure fair baseline comparisons, the sliding anchor mechanism is fixed (e.g., 0.5 threshold for masks) during inference. While reasonable for consistency, this limits the mechanism’s core advantage—dynamic adaptation to scene scales. Exploring scene-adaptive anchor selection (e.g., via semantic cues) would better showcase its potential.\n3. **Insufficient Depth Scaling and Supervision Details**\nThe paper uses linear normalization for near-field depth and exponential normalization for far-field depth, but the rationale for these choices (e.g., why not other functions) is underdiscussed. Additionally, experimental error distributions across near/far ranges and theoretical derivations for depth continuity need more refinement to strengthen rigor.\n4. **Unexplained Performance Disparity on iBims**\nMetric-Solver performs notably better on the iBims dataset than others (e.g., DIODE, SYNTHIA), but the paper provides no context. This gap weakens the persuasiveness of its generalization ability.\n5. **Ambiguous Core Contribution of Sliding Anchors**\nPrior work (ZoeDepth, AdaBins, DualDepth) has explored near/far-field separation and dual-branch fusion. Ablation experiments suggest the two-decoder architecture drives more performance gains than the sliding anchor—making the latter’s unique value unclear. .\n6. **Undetailed Anchor Embedding Design**\nHow are anchor  embeddings initialized? Only channel-wise concatenation is used for feature fusion, with no testing of alternatives (e.g., adding positional encodings)—limiting the comprehensiveness of this design.\n7. **Unaddressed Depth Continuity for Large-Span Objects**\nFor objects with large depth spans (e.g., approaching trains, full walls), rigid anchor-based scaling may cause unnatural depth transitions. The paper does not test this issue or propose fixes, which could affect practical application in complex scenes."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ro4aNDbV75", "forum": "C47sdEjWEL", "replyto": "C47sdEjWEL", "signatures": ["ICLR.cc/2026/Conference/Submission16208/Reviewer_VdRe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16208/Reviewer_VdRe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643645542, "cdate": 1761643645542, "tmdate": 1762926370736, "mdate": 1762926370736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new approach for estimating a metric depth from a single image. Specifically, the authors introduce a sliding anchor-based representation that can adapt to diverse depth distributions in both indoor and outdoor scenes. The method allows flexible anchor selection during inference. Extensive experimental results support the validity of their work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "\tImportant task – metric depth estimation\n\n\tThe proposed sliding anchor mechanism introduces an interpretable way to adjust depth range. Ablation studies suggest it contributes to performance.\n\n\tThe approach can potentially benefit downstream applications requiring controllable metric depth estimation."}, "weaknesses": {"value": "Major weaknesses are as below:\n\n\tThe novelty of this paper seems marginal. The paper mostly extends existing depth estimation frameworks with sliding anchor-based control. To my understanding, the suggested idea is conceptually similar to Bi3D's range-selective scheme [1], with the primary difference being the monocular metric depth. \n\n[1] Badki, Abhishek et al. \"Bi3d: Stereo depth estimation via binary classifications.\" CVPR 2020.\n\n\tThe paper does not clearly explain why their method outperforms existing approaches for zero-shot generalization experiments (Table 1). I am curious about how much of the reported performance gain actually comes from the sliding anchor mechanism itself, especially on the indoor dataset.\n\n\tWhile the authors emphasize that users can interactively select or slide the anchor during inference with some suggested anchor selection strategies, there is no clear criterion for determining whether a chosen anchor is appropriate or incorrect for a given scene. Moreover, the experiments appear to fix 10m for indoor and 80m for outdoor, which is domain-specific.\n\nOverall, I am not fully convinced that the proposed method is powerful compared to existing works, and suspect that this anchor-based depth representation is truly dynamic as claimed. I will reconsider the score when these concerns are addressed well.\n\nSome minor typo issues are as below:\n\n\tLine 177: randonly -> randomly\n\n\tLine 195: each components -> each component"}, "questions": {"value": "With the weaknesses mentioned above, I have a few more questions about the paper.\n\n\tFor zero-shot generalization experiments, is the policy for selecting anchor value identical to that used for the NYU/KITTI comparisons, or domain-specific? \n\n\tIn line 211, the authors choose a threshold of 0.5 to obtain the binary mask. Why does it work for both indoor and outdoor scenes? \n\n\tFor practical usage, if the user can interactively select or slide the anchor, how can one determine whether the chosen anchor is appropriate or incorrect for a given scene?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1PL9HRjJR1", "forum": "C47sdEjWEL", "replyto": "C47sdEjWEL", "signatures": ["ICLR.cc/2026/Conference/Submission16208/Reviewer_TyWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16208/Reviewer_TyWr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896889230, "cdate": 1761896889230, "tmdate": 1762926369086, "mdate": 1762926369086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a sliding anchor mechanism for monocular metric depth estimation. The method combines near- and far-depth decoding branches anchored by a variable depth reference to adaptively normalize depth across varying scales. By fusing two depth maps with a learned mask and leveraging a DINOv2 backbone, the approach aims to achieve better generalization across indoor and outdoor scenes without needing scene-specific calibration. Experimental results on standard benchmarks such as NYU-V2 and KITTI as well as zero-shot evaluation sets show competitive or superior performance relative to the prior works which are considered in the comparisons."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel architectural module for robust prediction of depth values with diverse ranges. The introduction of a sliding anchor mechanism is a creative attempt to address scale normalization in metric depth estimation, a long-standing challenge. The method’s two-branch decoding and anchor-based fusion represent a novel architectural variation within transformer-based depth networks.\n\n2. Breadth of evaluation. The authors test across multiple standard in-domain as well as zero-shot datasets, demonstrating cross-domain applicability of the method."}, "weaknesses": {"value": "1. Manipulated and incomplete presentation of the previous state of the art in the main experimental comparisons. In the main comparisons on zero-shot generalization of Table 1 and in-domain estimation of Table 2, the authors:\n* have reported false figures on DIODE Indoor in Table 1 for both Depth Pro (Bochkovskii et al. 2025) and UniDepth (Piccinelli et al. 2024), invariably reporting a substantially reduced performance for both models compared to the actual figures provided by the respective papers. In particular, on DIODE Indoor, UniDepth originally reports $\\delta_1 = 77.4$%, REL $= 17.2$%, and RMSE $= 0.954 m$, whereas the authors report for it $\\delta_1 = 27.8$%, REL $=47.9$%, and RMSE $= 1.741 m$. Moreover, Depth Pro on DIODE Indoor originally achieves $\\delta_1 = 67.1$%, REL $= 19.9$%, and RMSE $= 0.900 m$, whereas the authors report for it $\\delta_1 = 40.1$%, REL $=36.3$%, and RMSE $= 1.462 m$. That is, both UniDepth and Depth Pro factually outperform the proposed method by a large margin on DIODE Indoor, but the false figures presented by the authors create the opposite picture.\n* have omitted two central very recent state-of-the-art works from the zero-shot comparison of Table 1, i.e. UniK3D [A] and Metric3Dv2 (Hu et al. 2024a). This omission is important, as both of these works substantially outperform the presented method on sets which are included in the comparison of Table 2. More specifically, UniK3D achieves on IBims $\\delta_1 = 91.9$%, REL $= 10.4$%, and RMSE $= 0.406 m$, outperforming the presented method in all examined metrics. In addition, on DIODE Indoor, Metric3Dv2 obtains $\\delta_1 = 94.0$%, REL $= 9.3$%, and RMSE $= 0.399 m$ and UniK3D obtains $\\delta_1 = 71.3$%, REL $= 16.1$%, and RMSE $= 0.767 m$, i.e. both outperform the proposed approach substantially.\n* have omitted at least three central recent state-of-the-art works from the in-domain comparison of Table 2. i.e. UniDepth (Piccinelli et al. 2024), Metric3Dv2 (Hu et al. 2024a), and UniK3D [A]. For example, UniDepth outperforms the authors' method on KITTI, obtaining $\\delta_1 = 98.6$%, REL $= 4.2$%, and RMSE $= 1.75 m$.\n\nA highly inaccurate picture regarding the experimental performance of the method compared to the current state of the art is thus created due to the above figures manipulations and method omissions. The third main contribution in the respective list claimed by the authors in the end of the Introduction section is consequently unsupported.\n\n2. No metric output. \"Metric\" originally means that the network's output lives in a metric 3D Euclidean space. However, the presented method only predicts a mere (metric) depth map, which lacks information for capturing the metric structure of the scene. Such capturing would require outputting the ray directions corresponding to the various pixels. Especially at zero-shot scenarios where intrinsics might not be given, this shortage prevents the proposed method from true metric prediction. This deficit is also evident by the absence of metric 3D evaluation measures, such as Chamfer distance or $F$-score, across the experimental evaluations of the paper. The authors could have at least used the ground-truth camera intrinsics provided with several of the evaluation sets they examine, in order to upgrade their metric depth maps to metric 3D maps and evaluate more meaningfully on 3D metrics against competing methods.\n\n[A] Piccinelli et al.: UniK3D: Universal camera monocular 3D estimation. In CVPR, 2025."}, "questions": {"value": "1. Can the authors evaluate their method for 3D-level metrics against all central competing state-of-the-art metric depth/3D estimation approaches? (Cf. Weakness 2)"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Systematic manipulation of performance figures associated with competing state-of-the-art approaches in the main experimental comparison of the paper (Table 1), in a way that the reported figures in the paper are invariably substantially inferior to the figures originally provided by the respective previous works (cf. Weakness 1 in my review)."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xqbde4Bxjl", "forum": "C47sdEjWEL", "replyto": "C47sdEjWEL", "signatures": ["ICLR.cc/2026/Conference/Submission16208/Reviewer_fTp8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16208/Reviewer_fTp8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994892773, "cdate": 1761994892773, "tmdate": 1762926368178, "mdate": 1762926368178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sliding-anchor representation for monocular metric depth. A single scalar anchor partitions depth into two ranges: a scaled-near head (linear normalization) and a tapered-far head (exponential normalization), fused via a learned mask. Conditioning on an anchor pool lets inference focus on near or far distances. The backbone uses a large-scale visual encoder with dual DPT-style decoders. training mixes indoor/outdoor real + synthetic data. Experiments report zero-shot and fine-tuned results against recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of using anchors for scene generalization makes sense. Also, the split scaled-near and tapered-far, plus a learned fusion mask, gives a simple, reusable recipe for spanning both very close and extremely far depths without saturating either branch. \n\n2. The method has good empirical results compared to current sota baselines."}, "weaknesses": {"value": "1. The novelty is incremental. The sliding-anchor near/far split with mask fusion largely repackages known ideas (adaptive binning, log/inverse-depth compression, multi-head/gating). Without automatic anchor selection or a learned taper, it reads as an engineered variant rather than a fundamentally new formulation.\n\n2. There is no automatic anchor selection. This makes the method more like a tunable tool than a fully automatic monocular metric depth estimator. The paper does not close the loop: it never shows a mechanism that chooses the correct anchor for a novel test image, nor does it quantify robustness to wrong anchor choice.\n\n3.  The exponential taper constant k is fixed. This constant strongly influences far-range shape and gradients. Also, there is no discussion of stability.\n\n4. Overclaiming SOTA despite mixed results: on the DIODE-outdoor, the proposed method loses to DepthPro, so \"SOTA everywhere\" is not correct."}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ixaKyDwZMU", "forum": "C47sdEjWEL", "replyto": "C47sdEjWEL", "signatures": ["ICLR.cc/2026/Conference/Submission16208/Reviewer_75q8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16208/Reviewer_75q8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121881359, "cdate": 1762121881359, "tmdate": 1762926367731, "mdate": 1762926367731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}