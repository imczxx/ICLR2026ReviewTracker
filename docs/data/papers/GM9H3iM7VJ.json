{"id": "GM9H3iM7VJ", "number": 17961, "cdate": 1758282479003, "mdate": 1759897142462, "content": {"title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "abstract": "Indirect Prompt Injection attacks exploit a fundamental weakness of large language models (LLMs): the inability to reliably separate instructions from data. This vulnerability poses critical real-world security risks, yet systematic evaluation against adaptive adversaries remains largely unexplored. We introduce LLMail-Inject, the first large-scale public challenge simulating a realistic email-assistant environment—a high-value attack surface in practice. Involving 839 participants, the challenge produced 208,095 unique attack prompts across multiple LLM architectures and retrieval configurations. Unlike prior benchmarks, LLMail-Inject requires end-to-end compromise: attacks must be retrieved, adaptively evade defenses, trigger unauthorized tool calls with correct formatting, and exfiltrate contextual data.\nOur findings reveal a stark gap between perceived and actual robustness: while state-of-the-art models achieve <5% success on existing benchmarks, LLMail-Inject drives success rates to 32%, exposing the fragility of current defenses under realistic conditions. We release the dataset, code, and analysis to catalyze research toward structural, practical defenses against prompt injection.", "tldr": "We organized a public challenge on indirect prompt inject. This paper presents the results of the challenge and the collected dataset.", "keywords": ["LLMail-Inject; Indirect Prompt Injection; LLM Security"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56fab740d38e054a26a76908a1570148a405d5ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a prompt injection challenge in which participants craft emails (subject and body) with the aim of tricking an email assistant agent to invoke a specific tool call. In contrast to existing prompt injection competitions this one is more end-to-end and attempts to simulate a realistic agent scenario. The authors provide an overview of how the challenge was implemented, the data that was collected, and then provide some more in-depth analyses of the data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Realistic end-to-end nature of the challenge: A strong point of this paper is that it attempts to simulate a realistic agent in an end-to-end setting.\n- Collected data is made publicly available: The dataset collected during the competition is made publicly available, which has the potential of helping researchers evaluate new defense methods."}, "weaknesses": {"value": "- Limited contribution: While I appreciate the authors' effort in summarizing and presenting the results of this challenge, I struggle to see any major contribution that this paper makes beyond the publication of the attack data. The work in its current form would, in my opinion, be better suited for a venue targeting datasets or benchmarks.\n- No clear insights or research questions: The analysis felt very much like a listing of various summary statistics, without any clear targets about what to investigate. I think the paper would benefit greatly from formulating some clearly defined questions and then trying to extract answers to those questions from the available data.\n- Limited adjustment for confounding: Given that participants were free to select which challenges to solve, there are multiple confounding factors that come into play that make it hard to draw generalizable insights from the collected results (at least without appropriately adjusting for them). Team success rate is one option for countering some of these biases, but a more detailed discussion of how this could affect results would be crucial. For example:\n  - If level 1 is easier for Phi-3 than GPT-4o, this could mean that teams focused more on the Phi-3 levels.\n  - Number of submissions before success seems highly dependent on the order in which people solved tasks.\n- No detailed utility analysis: It would have been nice to not only consider attack success rates but also false positives in all of the different analyses. Such an analysis is crucial when trying to compare, e.g., defenses as in Figure 2(a)."}, "questions": {"value": "Am I missing a major contribution in my assessment above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l8iJCNMcYk", "forum": "GM9H3iM7VJ", "replyto": "GM9H3iM7VJ", "signatures": ["ICLR.cc/2026/Conference/Submission17961/Reviewer_PpjM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17961/Reviewer_PpjM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835938525, "cdate": 1761835938525, "tmdate": 1762927758153, "mdate": 1762927758153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors hold a large-scale competition to reward humans for adaptive prompt injections, aiming at practical attacks that break multiple steps of the system execution. The competition is held under a simulated email assistant scenario, where attackers try to trigger the model’s send_email tool call with arguments: destination=contact@contact.com,\ncontent=confirmation. An attack prompt is evaluated under 4 levels of different user (benign) prompts to the LLM. The authors implement prompting/detection defenses in the competition, which test Phi-3-medium and gpt-4o-mini. After the competition, the latest Meta-SecAlign defended LLM is also tested, where the collected attack prompts give an order of magnitude higher attack success rates. The authors promise to release the LLMail-Inject benchmark with 0.2M attack prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper devotes significant efforts on building the community of prompt injection, the top-1 threat to LLM-integrated applications. With a complex competition design, the competition collected a very large human-generated high-quality prompt injection dataset, which would be a great asset for future assessment of the model, given that current attack benchmarks are saturating.\n2. The competition is built on a practical attack scenario, email assistant, where an LLM is very suitable for handling this tedious work and may be mis-directed by a malicious email. The attack goal is hard: eliciting a specific function call with proper parameters. I appreciate the efforts on implementing defenses in the competition to harden the attacker’s trails.\n3.  The paper offers great insight in its analysis about the defense effectiveness and end-to-end attacks. An analysis of a prompt injection defense system (equipped with multiple defenses as existing commercial providers do) is important for this community."}, "weaknesses": {"value": "1. The competition assumes that the attacker knows the attack target string (trigger the model’s send_email tool call with arguments: destination=contact@contact.com,\ncontent=confirmation). However, in a practical attack scenario, how does the attacker know the name/parameters of a function call that will lead to malicious actions? That information is generally kept private in the LLM system.\n\n2. The selected two victim models are not strong nor representative enough. Phi is a 14B small model without inherent function call (as the authors admit). Gpt-4o is also a stronger model than gpt-4-mini, and with instruction hierarchy defense. \n\n3. It is unclear whether the attack prompts are transferable to attack other tasks beyond email assistant.\n4. Another large-scale prompt injection challenge [1] has also been held, but the authors do not discuss the differences between that work.\n\n[1] Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition"}, "questions": {"value": "The authors mention successful attacks using the system's delimiters. This is prohibited in Meta SecAlign system, see [here](https://github.com/facebookresearch/Meta_SecAlign/blob/main/demo.py#L11). Does the attack prompts against Meta-SecAlign contain Llama 3 delimiters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "buTeOjP1Vf", "forum": "GM9H3iM7VJ", "replyto": "GM9H3iM7VJ", "signatures": ["ICLR.cc/2026/Conference/Submission17961/Reviewer_nVTh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17961/Reviewer_nVTh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957967973, "cdate": 1761957967973, "tmdate": 1762927757627, "mdate": 1762927757627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLMail-Inject, a large-scale benchmark built from a real-world red-teaming competition simulating an email-assistant environment. It contains over 200K adaptive prompt injection attempts from 292 teams, covering multiple difficulty levels and defenses. The dataset provides realistic, diverse, and context-rich attack samples, enabling comprehensive evaluation of LLM safety and revealing the weaknesses of current prompt injection defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dataset is collected from a large-scale, real-world competition, which makes the collected data highly diverse and realistic, providing valuable resources and insights for future research on LLM safety and prompt injection defenses.\n\n2. The attack strategies are contextually relevant and reflect how adaptive prompt injection attacks may occur in practical LLM applications, such as email assistants.\n\n3. The paper provides comprehensive analyses across multiple difficulty levels and defense mechanisms, offering valuable insights into the effectiveness and limitations of current prompt injection defenses."}, "weaknesses": {"value": "1. The paper could include more recent and stronger baselines for comparison, such as StruQ [1], SecAlign [2], and Meta-SecAlign [3], which represent the state-of-the-art fine-tuning-based defenses against prompt injection.\n\n2. The proposed benchmark focuses solely on the email scenario, which, while realistic, may limit the generalizability of the findings. It would be valuable to include other application contexts, such as document editing, coding, or web agents.\n\n3. Although the dataset captures a wide range of real attack prompts, the paper could further analyze attack category diversity. For example, distinguishing between direct injection, indirect instruction hijacking, and data poisoning can be better characterize what kinds of vulnerabilities the collected samples represent.\n\n[1].Chen, Sizhe, et al. \"{StruQ}: Defending against prompt injection with structured queries.\"\n\n[2].Chen, Sizhe, et al. \"Secalign: Defending against prompt injection with preference optimization.\"\n\n[3].Chen, Sizhe, et al. \"Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks.\""}, "questions": {"value": "Please see the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CQCYrGBQcv", "forum": "GM9H3iM7VJ", "replyto": "GM9H3iM7VJ", "signatures": ["ICLR.cc/2026/Conference/Submission17961/Reviewer_mg4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17961/Reviewer_mg4k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979223753, "cdate": 1761979223753, "tmdate": 1762927757018, "mdate": 1762927757018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LLMail-Inject, a large-scale dataset of indirect prompt injection attacks collected through a public challenge involving 839 participants, resulting in over 200k unique attack prompts. The authors also conduct a comprehensive evaluation of existing defense mechanisms against these attacks, revealing several key insights about the gap between benchmark performance and real-world attack complexity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. All attacks were human-generated by participants attempting to solve real challenges, avoiding the template-based limitations of existing datasets. \n2. The competition-based approach successfully gathered over 200k unique attack prompts with rich diversity in attack strategies, representing an unprecedented scale compared to existing benchmarks.\n3. The paper is well-structured with overall good visualizations (except Figure 3). The systematic comparison of defenses across multiple dimensions provides in-depth insights."}, "weaknesses": {"value": "1. **Representativeness of the Scenario:** This paper focuses on email agents as the attack scenario. While email processing is a common use case, it may not capture the full diversity of real-world applications where prompt injection attacks can occur, such as web search, coding assistants, or customer support bots. The authors are encouraged to discuss the generalizability of their findings beyond email agents and discuss whether the dataset can be adapted to other scenarios.\n2. **LLM Selection:** Only two LLMs (microsoft/Phi-3-medium-128k-instruct and GPT-4o-mini) are considered in the challenge. Given the Phi-3 does not possess the function-calling capability natively, it seems not a suitable choice for evaluating prompt injection attacks targeting function-calling agents. Including more diverse and capable LLMs, especially those with built-in function-calling features like Llama-3 would further enhance the relevance of the dataset to real-world applications.\n3. **Discussion of Threat Models:** The challenge assumes attackers have complete knowledge of defense mechanisms, which may not reflect real-world scenarios where defenders may keep their methods confidential. In this regard, findings like \"LLMail-Inject drives success rates to 32%, exposing the fragility of current defenses under realistic conditions\" may somehow overstate the practical risk. The authors are encouraged to discuss the impact of different threat models on the evaluation results.\n4. **Guidance for Dataset Usage:** With over 200k data, researchers cannot practically use the entire dataset. The authors are encouraged to provide more guidance on effectively utilizing the dataset, such as:\n   - How to construct representative subsets for different research goals\n   - Which difficulty levels or defense combinations are most informative"}, "questions": {"value": "1. Are findings from the email agent scenario generalizable to other application domains?\n2. Can the dataset be transferred or adapted to evaluate prompt injection attacks in other contexts?\n3. What guidance can the authors provide for researchers on effectively utilizing the large dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h07hSBD2Lp", "forum": "GM9H3iM7VJ", "replyto": "GM9H3iM7VJ", "signatures": ["ICLR.cc/2026/Conference/Submission17961/Reviewer_gV14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17961/Reviewer_gV14"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986652164, "cdate": 1761986652164, "tmdate": 1762927756558, "mdate": 1762927756558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}