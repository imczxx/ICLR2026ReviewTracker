{"id": "TuFjICawSc", "number": 19199, "cdate": 1758294317028, "mdate": 1759897052547, "content": {"title": "Learning Retrieval Models with Sparse Autoencoders", "abstract": "Sparse autoencoders (SAEs) provide a powerful mechanism for decomposing the dense representations produced by Large Language Models (LLMs) into interpretable latent features. We posit that SAEs constitute a natural foundation for Learned Sparse Retrieval (LSR), whose objective is to encode queries and documents into  high-dimensional sparse representations optimized for efficient retrieval. In contrast to existing LSR approaches that project input sequences into the vocabulary space, SAE-based representations offer the potential to produce more semantically structured, expressive, and language-agnostic features. By leveraging recently released open-source SAEs, we show that their latent features can serve as effective indexing units for representing documents and queries for sparse retrieval. Our experiments demonstrate that SAE-based LSR models consistently outperform their vocabulary-based counterparts in multilingual and out-of-domain settings. Finally, we introduce SPLARE, a 7B-parameter multilingual retrieval model capable of producing generalizable sparse latent embeddings for a wide range of languages and domains, achieving top results on MMTEB’s multilingual and English retrieval tasks.", "tldr": "We introduce a novel competitive retrieval model built on sparse autoencoders that generates generalizable, multilingual sparse latent embeddings.", "keywords": ["text embedding", "sparse autoencoders", "sparse retrieval", "large language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d972ecd5f0e44502efe64ca31ab04d653121a803.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SPLARE, a novel Learned Sparse Retrieval model that ingeniously integrates Sparse Autoencoders (SAEs) with existing LSR frameworks like SPLADE. SPLARE projects text representations into a \"latent feature space\" learned by a pre-trained SAE, which is designed to be more semantic and language-agnostic.Through extensive experiments, the authors demonstrate that SPLARE significantly outperforms its vocabulary-based counterparts on multilingual and out-of-domain retrieval tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper originally combines the semantic feature decomposition of SAEs with learned sparse retrieval;\n2.The experimental results are impressive. SPLARE consistently outperforms SPLADE-Llama in multilingual and out-of-domain settings and achieves a level of performance comparable to SOTA dense models on comprehensive benchmarks like MMTEB.\n3.Excellent Generalization and Efficiency: SPLARE demonstrates strong multilingual generalization even when trained only on English data."}, "weaknesses": {"value": "1. Lack of Detailed Cost Analysis: The introduction of the SAE module incurs additional computational and memory costs at inference time. While the paper mentions mitigating this by using intermediate LLM layers, it does not quantify the latency and memory overhead introduced by the SAE projection step itself. \n2. Insufficient Context for Sparsity: The paper controls document vectors to ~400 non-zero dimensions. While this is sparse in a >130k-dimensional space, its advantage is not immediately obvious when compared to dense vectors, which may only have a few hundred to a thousand dimensions in total. The paper lacks a direct, end-to-end comparison of latency, index size, and memory usage between SPLARE (with inverted indexes) and a top-tier dense retriever (with an ANN index like HNSW) at a similar effectiveness level."}, "questions": {"value": "1. Can you provide more information on the analysis and comparison of deployment costs?\n2. Regarding sparsity control, the paper notes that combining training-time regularization with inference-time Top-K pruning was superior to using Top-K alone. Could the authors elaborate on the distinct effects of these two methods on the final representations? For example, does the regularization loss encourage the model to learn a better distribution of important features, which Top-K then effectively truncates, whereas relying solely on Top-K might crudely discard features that are contextually important but have marginally lower activation scores?\n3. The model training relies on knowledge distillation from a cross-encoder. Was a contrastive learning approach, which is dominant in dense retriever training, considered or attempted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lUb9XXDqwr", "forum": "TuFjICawSc", "replyto": "TuFjICawSc", "signatures": ["ICLR.cc/2026/Conference/Submission19199/Reviewer_xNQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19199/Reviewer_xNQK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622974044, "cdate": 1761622974044, "tmdate": 1762931196444, "mdate": 1762931196444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPLARE (SParse LAtent REtrieval), a novel Learned Sparse Retrieval (LSR) approach that leverages pre-trained Sparse Autoencoders (SAEs) to represent queries and documents as sparse vectors over a latent feature space rather than the traditional vocabulary space. By inserting SAEs into intermediate layers of large language models (LLMs), SPLARE produces semantically rich, multilingual, and domain-generalizable sparse embeddings. The authors demonstrate through extensive experiments on benchmarks like MMTEB, MIRACL, and XTREME-UP that SPLARE consistently outperforms vocabulary-based LSR models—especially in multilingual and out-of-domain settings—while maintaining high retrieval efficiency. A 7B-parameter multilingual variant of SPLARE achieves competitive results against state-of-the-art dense retrievers, establishing it as the top-performing LSR model on MMTEB at the time of submission."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is the first to systematically use the pre-trained sparse autoencoder as the \"implicit vocabulary\" of LSR, replacing the traditional methods based on the original tokenizer vocabulary.\n2.\tThe proposed method demonstrates outstanding performance in multilingual and cross-domain scenarios."}, "weaknesses": {"value": "1.\tAlthough the authors find that the best performance was achieved at Layer 26 for different SAE widths, this conclusion is only drawn from the experiments conducted on the Llama-3.1-8B model. If readers wish to apply the SPLARE method to other models (such as Gemma), there is no guarantee that the selection of this layer will be successful. It is suggested that the author provide more experimental results on the models and the layer selection strategies to enhance the generalization ability of the method.\n2.\tCurrent state-of-the-art dense retrievers generally adopt contrastive learning, but SPLARE chose distillation. The paper merely refers to distillation as a \"common toolbox\", but does not provide an explanation as to why distillation is more suitable for the sparse latent space. This crucial design choice lacks an analysis of motivation.\n3.\tThe paper clearly states that only residual SAEs were used, but no ablation experiments or reasons are provided. SAEs using MLP or Attention might capture richer or different types of semantic information. Ignoring them could lead to information loss and limit the model's expressive power.\n4.\tTable 1 shows that in both the English and Multilingual benchmarks, SPLARE significantly lags behind SPLADE-Llama in the Code domain. The paper only mentions \"the advantage diminishes\" without providing any analysis (such as feature visualization, error cases). This may indicate that the SPLARE method has flaws when dealing with highly structured and symbolic text (such as code), and this needs to be emphasized.\n5.\tThe abstract and introduction have repeatedly emphasized that SPLARE is the first LSR model that can match the SOTA dense model on MMTEB. However, Table 3 shows that its score of 60.9 is much lower than that of top dense models such as Qwen-3-Embedding-8B (70.9), inf-retriever-v1 (66.5), etc. The discussion in the paper contains an exaggeration."}, "questions": {"value": "1.\tThe paper claims to propose \"Learned Sparse Retrieval\", but its core component SAE, is frozen throughout the training process. This means that the model cannot learn or optimize the \"latent vocabulary\" itself for retrieval purposes and can only be adapted to a fixed and non-designated feature space for the retrieval task through LoRA fine-tuning of the LLM's intermediate representations. This is fundamentally different from LSR methods such as SPLADE (whose LM head is learnable), and it weakens the claim of \"Learned\".\n2.\tThe paper not only employs DF-FLOPS type sparse regularization but also adds Top-K pooling during inference to forcibly control the number of activations. The authors also admit that training solely with Top-K would be worse, but still choose the \"moderately sparse\" + post-cropping approach during training. This inconsistency will introduce non-differentiable post-processing and distribution drift, potentially leading to instability near the critical threshold and sorting reversals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1HfnAhd7RZ", "forum": "TuFjICawSc", "replyto": "TuFjICawSc", "signatures": ["ICLR.cc/2026/Conference/Submission19199/Reviewer_EBaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19199/Reviewer_EBaU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794039850, "cdate": 1761794039850, "tmdate": 1762931195975, "mdate": 1762931195975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies sparse auto encoders based latent features effectiveness in representing documents and queries. Authors claim that sparse autoencoders based learned sparse retrievers outperform their vocabulary projection based counterparts. The main contributions  of this paper are:\n1. This paper introduces SPLARE a new sparse retrieval technique leveraging sparse auto encoders.\n2. Detailed investigations of benefits of using latent vocabulary vs standard LLM vocabulary.\n3. New 7B multilingual sparse retriever model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the paper are as follows:\n1. Idea of using latent features for document and query representation might have a lot of applications in text retrieval\n2. The SPLARE learned sparse retrieval framework is interesting.\n3. Detailed performance comparison with SOTA text retrieval models."}, "weaknesses": {"value": "The main Weaknesses of the paper are as follows:\n1.  Details about SPLARE working is missing. It might need some more explanation.\n2. Terms like SAE width needs some more details.\n3. Limited novels of the overall retrieval model when compared to sparse embed and other LSR techniques.\n4. How SAEs are trained is not clear from the paper.\n5. Paper writing have a lot of scope of improvement.\n6. Limited performance improvement on MTEB retrieval benchmark when compared to SPLADE-LLama"}, "questions": {"value": "1. How does the proposed approach with sparse embed  paper https://research.google/pubs/sparseembed-learning-sparse-lexical-representations-with-contextual-embeddings-for-retrieval/?\n2. Is availability of Pre-trained SAE a constraint of this model? \n3. Any comparison of latency and training time when compared to spade and other dense models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CEqiKGgb96", "forum": "TuFjICawSc", "replyto": "TuFjICawSc", "signatures": ["ICLR.cc/2026/Conference/Submission19199/Reviewer_7hiw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19199/Reviewer_7hiw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988472849, "cdate": 1761988472849, "tmdate": 1762931195535, "mdate": 1762931195535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}