{"id": "BQwGPv4DTe", "number": 10801, "cdate": 1758182185764, "mdate": 1759897627784, "content": {"title": "Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection", "abstract": "Video anomaly detection (VAD) aims to identify unusual events in continuous video streams, yet most existing systems either rely on domain-specific retraining or fail to meet strict real-time demands. We present **Flashback**, a zero-shot and real-time paradigm that reframes VAD as retrieval over an offline pseudo-scene memory. Inspired by how humans recall past experiences to judge the present, Flashback constructs a large set of normal and anomalous captions entirely offline with a language model, embeds them once with a frozen video-text encoder, and reuses this memory online. At inference, each segment is matched against the memory to produce both an anomaly score and a textual rationale, eliminating all online LLM calls and sustaining per-segment deadlines. Three lightweight controls improve robustness: _repulsive prompting_ separates normal and anomalous caption spaces, _scaled anomaly penalization_ corrects residual anomaly bias, and _certainty-driven runtime encoder selection_ maintains weakly-hard real-time guarantees by allocating extra compute only to difficult segments. On UCF-Crime and XD-Violence, Flashback achieves 87.7 AUC and 75.0 AP, outperforming prior zero-shot methods while providing human-readable explanations at up to 43.8 fps on a single consumer GPU. The result is the first VAD system that is simultaneously zero-shot, real-time, and explainable.", "tldr": "Flashback is a zero-shot, real-time, and explainable VAD system that retrieves from an offline caption memory with lightweight bias controls and runtime encoder selection.", "keywords": ["video anomaly detection", "multi-modal large language models", "zero-shot", "real-time"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f83bd19a5ca21cf19f9b4b9223e74735099e0ca7.pdf", "supplementary_material": "/attachment/f5e2a16a6e43f458305dfc7502fe982cfe4d7602.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Flashback, a novel paradigm for video anomaly detection (VAD) designed to be simultaneously zero-shot, real-time, and explainable. It reframes VAD as a retrieval task by constructing a large \"pseudo-scene memory\" entirely offline using an LLM to generate text captions . At inference, it avoids all online LLM calls, simply matching video segments to this memory to produce an anomaly score and a textual rationale . The method's robustness is improved by controls like repulsive prompting (RP), scaled anomaly penalization (SAP), and a certainty-driven runtime encoder selection . Flashback achieves state-of-the-art zero-shot performance on UCF-Crime (87.7 AUC) and XD-Violence (75.0 AP) while operating at high throughput."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Novel Unification of Critical VAD Properties. The paper's primary strength is its innovative system design that successfully unifies three properties critical for real-world deployment: zero-shot generalization, real-time inference, and explainability . This is achieved by cleverly reframing VAD as a retrieval task, moving all computationally expensive LLM-based reasoning to an \"offline recall\" stage, thus eliminating online LLM calls and solving the speed-reasoning trade-off.\n\n2.State-of-the-Art Empirical Performance. Flashback achieves outstanding zero-shot results, significantly outperforming prior work like LAVAD on both UCF-Crime and XD-Violence, and even surpassing many supervised methods. This strong accuracy is paired with exceptional real-time throughput (up to 43.8 fps) and backed by quantitative evidence that its retrieved captions are semantically meaningful explanations.\n\n3.Thorough Component Validation and Ablation. The paper provides a comprehensive set of ablation studies in Section 4.5 that strongly justify the novel design choices. The necessity of Repulsive Prompting (RP) is clearly demonstrated both quantitatively and qualitatively. Furthermore, the study validates the system's scalability with memory size and its robustness to different random subsets of the pseudo-caption memory, inspiring confidence in the method's stability."}, "weaknesses": {"value": "1.The paper's impressive results are tightly coupled with large-scale, proprietary models, raising significant reproducibility concerns. The 1M-entry pseudo-scene memory was generated using the proprietary gpt-4o-2024-08-06 , which incurs substantial cost ($181.43) and compute time (76 hours). Furthermore, all embeddings rely on the PerceptionEncoder. The paper provides no ablation to show how the Flashback paradigm itself would perform with a standard, open-source LLM or a common frozen encoder like CLIP. This makes it difficult to disentangle the contribution of the novel retrieval method from the power of its backbone models.\n\n2.The certainty-driven runtime encoder selection (Sec 3.4) presents several issues. First, its mathematical formulation in the main paper is overly dense and lacks intuition, with all critical derivations and justifications deferred to Appendix C, hindering clarity. Second, this mechanism introduces new, sensitive hyperparameters ($Q, R, \\tau$), yet the paper provides no sensitivity analysis to show how they were chosen or how robust the system is to their variation. Finally, the paper asserts the Kalman-based likelihood is effective but fails to compare it against simpler uncertainty heuristics (e.g., the entropy of the retrieved label distribution) to prove that this complex state-space model is justified.\n\n3.The paper's definition of \"explanation\" is ambiguous. The anomaly score $A_s$ is calculated by a weighted average of the anomaly flags from the Top-K (K=10) retrieved captions. However, the qualitative examples (e.g., Fig 7, 8) present this entire list of K captions as the human-readable rationale. This is problematic, as the list may contains a confusing mix of both normal and anomalous descriptions for the same segment. It is unclear if the intended rationale for an operator is just the Top-1 caption or this entire, potentially conflicting, list."}, "questions": {"value": "1.Regarding the Runtime Encoder Selection: (a) Can the authors provide a more intuitive explanation for choosing a Kalman filter over simpler uncertainty heuristics (e.g., entropy of the Top-K label weights)? (b) How sensitive is the performance of FlashbackX to the choice of the $Q, R, \\tau$ hyperparameters? (c) Could you provide a direct comparison (in both accuracy and overhead) to using a simpler metric, like the entropy of $\\{w_{s,k}\\}$, as the certainty score?\n\n2.Regarding Model Dependency: To what extent is the SOTA performance bound to the specific PerceptionEncoder and gpt-4o? Have the authors experimented with using a standard CLIP (e.g., ViT-L/14) encoder and an open-source LLM (e.g., Llama 3) for memory generation? This ablation is critical for understanding the generality of the Flashback paradigm.\n\n3.Regarding Explainability: Could the authors please clarify what the intended \"explanation\" for a human operator is? Is it (a) the Top-1 retrieved caption, or (b) the full Top-K list? If it is (b), how does the system recommend handling the common case where this list contains contradictory (both normal and anomalous) descriptions for a segment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "21Tyfd9IYZ", "forum": "BQwGPv4DTe", "replyto": "BQwGPv4DTe", "signatures": ["ICLR.cc/2026/Conference/Submission10801/Reviewer_cevy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10801/Reviewer_cevy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572308679, "cdate": 1761572308679, "tmdate": 1762922014598, "mdate": 1762922014598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flashback, a novel paradigm for Video Anomaly Detection (VAD) that reframes the task as retrieval from a large, offline, text-only pseudo-scene memory generated by an LLM. By eliminating online LLM calls, Flashback achieves simultaneous zero-shot, real-time, and explainable anomaly detection. It outperforms prior zero-shot methods on UCF-Crime and XD-Violence and incorporates three lightweight controls—Repulsive Prompting, Scaled Anomaly Penalization, and runtime encoder selection—to enhance robustness and meet real-time constraints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel and Practical Paradigm**: The major strength is its core idea of redefining VAD as a retrieval task over an offline text memory generated by an LLM. This is not only conceptually elegant but also highly practical as it directly addresses the bottleneck of online inference with VLM/LLM.\n2. **Excellent Real-Time Performance**: The paper makes a strong commitment to \"real-time\" and shows high throughput (e.g., 43.8 fps)."}, "weaknesses": {"value": "1. Ambiguity in Zero-Shot Definition:\nThe method is claimed to be \"strictly domain-agnostic,\" yet the use of domain-specific context prompts (e.g., \"university campus\" for ShanghaiTech) during memory construction implies reliance on target-domain knowledge. This conflicts with the standard zero-shot assumption and may limit true plug-and-play applicability in completely unseen environments.\n\n2. Dependence on LLM Knowledge and Coverage:\nFlashback can only detect anomalies that are pre-generated in the pseudo-scene memory. Anomalies outside the LLM's knowledge or imagination—especially novel, rare, or domain-specific events—will be missed, leading to false negatives and limited generalization.\n\n3. Scaled Anomaly Penalization (SAP) May Not Generalize:\nThe scale factor α = 0.95 was tuned on UCF and XD-Violence. It is unclear whether this value generalizes to other domains (e.g., daily-life anomalies in ShanghaiTech). This raises concerns about the need for per-domain tuning, undermining the zero-shot claim.\n\n4. Explainability May Be Noisy or Misleading:\nAs shown in Figure 7, some retrieved captions are irrelevant or inconsistent with the video content. Aggregating top-K captions without summarization or ranking can lead to confusing or redundant explanations, reducing the practical utility of the interpretability feature.\n\n5. Limited Validation of Repulsive Prompting (RP):\nThe RP ablation is supported by only one qualitative example. More cases are needed to convincingly demonstrate its necessity and effectiveness across diverse anomaly types and domains.\n\n6. Incomplete Handling of Label Ambiguity:\nIn Figure 4(c), a detection is dismissed as a \"label mismatch,\" but no comparison with other methods (e.g., LAVAD or weakly-supervised baselines) is provided to contextualize this failure. This weakens the claim of superior robustness."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6RBZ139mup", "forum": "BQwGPv4DTe", "replyto": "BQwGPv4DTe", "signatures": ["ICLR.cc/2026/Conference/Submission10801/Reviewer_G9Rn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10801/Reviewer_G9Rn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748007391, "cdate": 1761748007391, "tmdate": 1762922014006, "mdate": 1762922014006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Flashback, a model that constructs a set of normal and abnormal captions offline with the support of a LLM.\n\nThe proposed model is able to produce both an anomaly score and a textual rationale for video anomaly detection.\n\nThey conduct evaluations on two benchmark datasets such as UCF-Crime and XD-Violence, and show improved performance."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper is technical sound.\n\n+ The proposed model shows improved performance on both UCF-Crime and XD-Violence.\n\n+ Some interesting visualisations such as Fig 4."}, "weaknesses": {"value": "- The review of existing works tend to be limited. What are the current challenges in this area, why existing methods are unable to address these issues, and how the proposed model handles these challenges are unclear. Although some of the insights are provided in the last few sentences per paragraph of the related work, it could be more clearly presented.\n\n- The method section is overall clearly written. It would be better to have a notation section detailing the maths symbols and operations used in the paper, such as what are scalars, vectors and matrices etc.\n\n- A good paper should use enough figures to show the network design, modules and blocks, currently in this paper, only one figure is provided (fig. 2), and the module details, their setups and arrangements, particularly the only and output dimensions are unclear to reviewer. What are the core innovations compared to existing works that use eg prompts and templates with LLM/VLM? The discussions and comparisons regarding these are limited. How to position this work clearly in the current literature?\n\n- Most experimental results are presented in the form of tables, no any visualisations on attentions, plots to show eg the hyperparameters evaluations etc, and the comparisons to closely related SOTA models. These limit the impact of this work, and the current evaluations tend to be potentially biased, as only two datasets are being used. Fig. 3 only shows with and without the use of RP, and it is not being compared with eg existing SOTA methods. \n\n- The datasets used in evaluations tend to be small-scale and a bit old fashioned. The authors should try to explore new and more challenging datasets in evaluations such as [A]. How the proposed model handles, eg, scenario-level and anomaly-type-level detection tasks?\n\n[A] L Zhu, L Wang, A Raj, T Gedeon, and C Chen. Advancing Video Anomaly Detection: A Concise Review and a New Dataset. Advances in Neural Information Processing Systems (NeurIPS). 2024.\n\n- Ablation studies are lengthy in texts, but the core discussions and analysis such as section 4.2, 4.3 and 4.6 tend to be very limited. The paper needs to be revised to reflect on how the proposed model is robust, efficient and effective on diverse scenarios and in handling different anomaly types."}, "questions": {"value": "Refer to weaknesses.\n\n- It is suggested not to heavily use “—” as this looks like machine generated contents/patterns.\n\n- Limitations and future research directions could be included in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C2DsogE1rP", "forum": "BQwGPv4DTe", "replyto": "BQwGPv4DTe", "signatures": ["ICLR.cc/2026/Conference/Submission10801/Reviewer_xR5A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10801/Reviewer_xR5A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802659524, "cdate": 1761802659524, "tmdate": 1762922013560, "mdate": 1762922013560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Flashback, a memory-driven approach for zero-shot, real-time video anomaly detection (VAD). By reformulating VAD as a retrieval task over a pre-generated text-only memory, the method eliminates online LLM calls and achieves strong performance on UCF-Crime and XD-Violence while providing textual explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel and practical framework that effectively unifies zero-shot capability, real-time inference, and explainability.\n\n2. The proposed model achieves SOTA zero-shot accuracy, outperforming prior works significantly, with high throughput (up to 43.8 fps).\n\n3. The ablation studies convincingly validate key components such as repulsive prompting and memory scaling."}, "weaknesses": {"value": "1. The whole method heavily relies on proprietary models (GPT-4o, PerceptionEncoder) without ablation using open-source alternatives (e.g., CLIP, LLaMA), raising reproducibility concerns.\n\n2. The runtime encoder selection mechanism is complex and poorly motivated; no comparison with simpler uncertainty metrics (e.g., entropy) is provided.\n\n3. Ambiguity in the definition of “explanation”—whether it is the top-1 caption or the full top-K list, and how conflicting captions are handled."}, "questions": {"value": "1. How was the Kalman filter chosen for uncertainty estimation? Have you compared it with simpler metrics like entropy?\n\n2. To what extent does performance depend on the backbone models? Are results reproducible with open-source alternatives?\n\n3. What is the final explanation presented to users? If it is the top-K list, how should operators interpret contradictory captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ka4kPcK3ss", "forum": "BQwGPv4DTe", "replyto": "BQwGPv4DTe", "signatures": ["ICLR.cc/2026/Conference/Submission10801/Reviewer_GAsY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10801/Reviewer_GAsY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255877291, "cdate": 1762255877291, "tmdate": 1762922013102, "mdate": 1762922013102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}