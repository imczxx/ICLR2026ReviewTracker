{"id": "0vs1YLmmQa", "number": 23639, "cdate": 1758346657736, "mdate": 1759896803775, "content": {"title": "Principled and Tractable RL for Reasoning with Diffusion Language Models", "abstract": "Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +6.5\\% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.", "tldr": "", "keywords": ["diffusion", "dllm", "llm", "grpo", "rl", "reasoning", "policy gradient", "post-training", "non-autoregressive"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/805e974ee939b1a4dfba9551b1ca55f3b2ce23c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of on-policy reinforcement learning for diffusion language models. In dLLMs, computing marginal token probabilities is intractable, making autoregressive LLM RL objectives not directly applicable. The authors point out the approximations and bias in existing diffu-GRPO and UniGRPO objectives. To tackle the issues, the authors proposes Amortized GRPO (AGRPO), which rewrites GRPO’s tokenwise inner sum as an expectation over timesteps and estimates it with Monte-Carlo samples, resulting in an unbiased policy-gradient estimator. The objective also keeps the KL term inside the expectation approximated with the Schulman estimator. The method is paired with practical tricks (caching partially masked states, LoRA) for efficiency. AGRPO is tested with LLaDA-8B-Instruct across GSM8K, MATH, and Countdown, improving over the base model and outperforming diffu-GRPO and UniGRPO."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The paper outlines the problem fairly clearly, outlining issues with existing methods and proposing a relatively straightforward algorithms to address them. \n* The authors also provide some details about the practical considerations which are useful. \n* While I have various concerns over the empirical setup detailed below, the results seem promising."}, "weaknesses": {"value": "* The paper misses some closely related prior work on RL fine-tuning of diffusion language models [1, 2, 3]. I believe a comparison to these baselines would be critical. (There are several other recent papers studying the same problem but they count as concurrent work, so I do not expect the authors to compare with them) \n* There are several explanations and references in the paper that are incorrect:\n    * L184: For early work on RL with LLMs the authors refer to the WizardLM paper as a reference for the use of PPO. However, there were quite a few papers [e.g. 4] prior to that.\n    * Initial work on RL with LLMs was also focused on preference learning rather than reasoning ability in math or code.\n    * L190: Learning reward models is still done for preference tasks and it is usually decoupled from the size of the model being trained so it is not as affected by the growing model sizes.\n* In terms of baselines, simple test-time scaling methods like majority voting on the base-model are missing.\n*  A lot of details around the actual runtime (e.g. how many steps of training, comparison of training time vs baselines) are missing. Additionally it seems that the numbers in Table 1 are the result of a single seed of training and single seed for evaluation. Please clarify if this is incorrect. I understand training with multiple seeds can be expensive but I atleast expect multiple seeds for eval.\n*  There are no ablations or analysis to investigate factors affecting how the method works. For instance, analysis on the effect of k, m, n (during training), effect of low confidence remasking. \n*  Finally, the results are limited to a single model with 3 math-based tasks which is hard to get general insights from. \n\n[1] Venkatraman et al., 2024. Amortizing intractable inference in diffusion models for vision, language, and control. \n\n[2] Huang et al. 2025. Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models.\n\n[3] Zekri and Boullé, 2025. Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods.\n\n[4] Ziegler et al., 2019. Fine-Tuning Language Models from Human Preferences."}, "questions": {"value": "* Please clarify if the numbers are averaged over multiple seeds.\n* In Fig 3, why are the numbers for the base model for different response lengths missing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xGosCOVTdb", "forum": "0vs1YLmmQa", "replyto": "0vs1YLmmQa", "signatures": ["ICLR.cc/2026/Conference/Submission23639/Reviewer_9uBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23639/Reviewer_9uBq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955443260, "cdate": 1761955443260, "tmdate": 1762942741019, "mdate": 1762942741019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Amortized Group Relative Policy Optimization (AGRPO), a novel on-policy reinforcement learning algorithm tailored to advance the reasoning abilities of Diffusion Large Language Models (dLLMs).  This paper identify a key limitation in current RL approaches: while proven effective for autoregressive LLMs, they become computationally prohibitive for dLLMs, and existing dLLM-specific methods often resort to heuristic approximations that introduce bias into policy gradients and lack theoretical grounding. To address this, AGRPO reformulates the token-wise summation in the policy gradient objective as an expectation, enabling unbiased and efficient estimation through Monte Carlo sampling over generation timesteps. Empirical evaluations on mathematical reasoning tasks, including GSM8K, show that AGRPO substantially outperforms both baseline models and prior RL techniques, offering a theoretically rigorous framework that not only elevates dLLM performance but also rebalances the interplay between computational efficiency and output quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results are impressive, achieving high performance on the GSM8K benchmark (86.3) and matching the performance of models such as DeepSeekMath-Base 7B.\n2. The core idea is novel and well-motivated, with several practical considerations incorporated into the methodology. I found the approach insightful and instructive."}, "weaknesses": {"value": "1. While the idea is interesting, the writing requires improvement in several areas. Descriptions in many parts remain unclear, and the experimental section appears somewhat brief. The paper would benefit from more comprehensive ablation studies—for instance, to validate the impact of the practical considerations mentioned. A restructuring of the paper is also important to better align with academic writing standards.\n2. The version of paper currently lacks a dedicated \"Related Work\" section, which is essential for contextualizing the contribution within the existing literature.\n3. Related to the first point, the current version reads more like a technical report than a fully developed academic paper."}, "questions": {"value": "1. Can this method be effectively adapted to other diffusion-based language models, such as Dream 7B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AA8T25Q19M", "forum": "0vs1YLmmQa", "replyto": "0vs1YLmmQa", "signatures": ["ICLR.cc/2026/Conference/Submission23639/Reviewer_PuSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23639/Reviewer_PuSn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002290101, "cdate": 1762002290101, "tmdate": 1762942740827, "mdate": 1762942740827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AGRPO (Amortized Group Relative Policy Optimization), a rl algorithm designed to post-train diffusion language models on reasoning tasks by reformulating the GRPO objective as an expectation over denosing timesteps that can be estimated via Monte Carlo sampling. Empirically, AGRPO has accuracy gains on math/logic benchmarks over prior policy optimization methods for dLLMs, trading extra compute for per-timestep estimates in a way that aims to reduce bias/variance in log prob estimation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. this paper discusses caching partially masked states, gradient accumulation per MC sample, and EOS/timestep handling, which are useful for training diffusion LLMs.\n2. this paper shows their proposed method achieves better performance than baselinse UniGRPO and diffu-GRPO across 3 tasks.\n3. This paper trades off compute (number of MC estiimates) with approximation accuracy and achieves better results than fewer MC sample baselines."}, "weaknesses": {"value": "The paper’s writing is not rigorous:\n\n1. Equation 2 is AR-specific and assumes a causal left-to-right generation; mapping its per-token inner sum to dLLM timesteps needs clearer justification. As written, the swap (|o_i|\\to m) can mislead readers about where compute actually occurs in dLLMs.\n\n2. Exact sequence likelihood in dLLMs is intractable because it would require marginalizing over all denoising orders/masking patterns, but AGRPO doesn’t do that: it optimizes using conditionals on partially-masked states and estimates an ELBO-based surrogate via MC. MC is unbiased for the ELBO, but the ELBO itself is a biased lower bound to the true log-likelihood. The paper should avoid calling such MC \"exact\". Additionally, optimizing the particular sampling order might not be optimal, because if a response is good, we ideally want to increase the probability of generating this response with all possible orderings to generalize better. UniGRPO and diffu-GRPO resample new masking patterns (randomly or full mask) can somehow encourage this. \n\n3. The notation $$(\\pi_\\theta(o_t \\mid q, o_{<t}))$$ overloads AR conventions; in dLLMs (o_{<t}) should be defined as the partially masked state at timestep t. The paper uses this shorthand broadly without a crisp definition, which hurts clarity.\n\n4. this paper also lacks a related work section\n\nLimited novelty and lack of experiments:\n\n1. AGRPO replaces one-step heuristics with an unbiased per-timestep estimator—a principled step—but the paper lacks ablations on k (MC samples), estimator variance/bias, and how k drives compute–quality trade-offs. Without these, the incremental benefit over prior heuristics is harder to quantify. The experiments are lacking rigorous ablations and for the main results, the tasks they choose are a subset of prior works.\n\n2. Increasing the number of MC samples introduces additional compute, the paper doesn't analyze the efficiency and performance gain trade off."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6YjCFJubt1", "forum": "0vs1YLmmQa", "replyto": "0vs1YLmmQa", "signatures": ["ICLR.cc/2026/Conference/Submission23639/Reviewer_2DXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23639/Reviewer_2DXg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147858530, "cdate": 1762147858530, "tmdate": 1762942740638, "mdate": 1762942740638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a key issue in post-training diffusion large language models (dLLMs), that the existing RL methods like GRPO, which work well for autoregressive (AR) models, are incompatible with dLLMs because dLLMs cannot compute probabilities in a single forward pass. It introduces Amortized Group Relative Policy Optimization (AGRPO), a reinforcement learning (RL) algorithm designed for fixing this, by reformulating the GRPO objective by treating the sum over timesteps as an expectation, which is then estimated using Monte Carlo sampling over timesteps. They claim this provides an unbiased policy gradient. The paper demonstrates performance gains on mathematical reasoning tasks (GSM8K, MATH, Countdown) over prior dLLM RL methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The core idea of trying to reformulate GRPO  to enable Monte Carlo estimation is new. If correct,  it can directly addresses the basic incompatibility between standard policy gradient methods and the dLLM architecture. \nThe paper is generally well-written and clearly explains the theoretical shortcomings of existing approaches. There are additional tricks describe for efficient computation which can be valuable in other contexts as well."}, "weaknesses": {"value": "I might have misunderstood the key idea — but it seems to be based on a wrong interpretation of what the actual incompatibility between PGRL methods and dLLMs. They use the AR-incorporated PPO/GRPO loss where the propensity ratio has been simplified because of autoregressive per token likelihood decomposition. This itself becomes invalid for dLLMs. While some form of regressive factorization is still true, the model likelihood will still not match the true likelihood. This makes AGRPO another approximation (which is fine, but the paper does not claim that). Furthermore even if this term is unbiased (which I do not believe to be the case), the KL term still requires true log-likelihoods, which is not what the AGRPO  trick allows to compute. Overall the description sidesteps the fundamental issue that dLLMs do not have a tractable sequence-level probability.\n\nAdditionally, the results compare only a specific length case. While compute resource constraints make sense for training, inference is done with the same model for different lengths. So that does not seem to a big issue. Additionally they use LoRA based fine-tuning instead of full model tuning, which makes the comparison a bit apples-to-oranges. The gains can be from LoRA, and then the baselines need to be tuned with LoRA. Moreover the paper does not compare against more recent baselines such as [1,2,3], which also attempt to deal with the likelihood mismatch issue.\n\n[1] wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models\n\n[2] PADRE: Pseudo-Likelihood based Alignment of Diffusion Language Models\n\n[3] DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation"}, "questions": {"value": "Experiments:\nI do not see the contribution of low-discrepancy sampling being ablated. How much does it actually help over naive i.i.d. sampling?\nSimilarly, usage of sampling t, produces variance, there should be an ablation of performance against t/m .\n\n\nThe paper claims the new loss is more efficient scalable, but I do not see the compute-cost vs gain tradeoff. Using the m-sample likelihood is more compute expensive then the 1-step approximation used in diffu-GRPO/d1's likelihood approximation. The cost/tractability is not just from samples but from overall FLOPS, which is not compared.\n\nRelatedly, the argument that rollout generation cost \"dwarfs\" the loss computation is highly dependent and needs some empirical measures\n\nTheory:\nThe paper uses the diffusion time t, as also a generated sequence length measure (by conditioning on $$o^i_{<t}$$ . How do you then generate that with independently sampled time $t \\in [1,m]$. This is not an issue in AR models, as they generate tokens sequentially for t=1 to t=m; but that is not the case here. Is this multiple one-step approximations? Or something different? \nWhat sequence/step likelihood is used for the KL computation in the objective.\nFurthermore, the paper cites a blog-post for an unbiased KL estimate, but that post itself has multiple unbiased estimates. I think I know the version used, but this is not clear from the paper itself. \nSince the code is not with the paper, I also cannot compare the expression used compared to the one in the paper to see if I misunderstood the expression; and the paper does not give enough details to know exactly what the computations are ( for both the importance ratio and KL likelihood)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uec9S5qfbD", "forum": "0vs1YLmmQa", "replyto": "0vs1YLmmQa", "signatures": ["ICLR.cc/2026/Conference/Submission23639/Reviewer_3eva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23639/Reviewer_3eva"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762396662001, "cdate": 1762396662001, "tmdate": 1762942740457, "mdate": 1762942740457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}