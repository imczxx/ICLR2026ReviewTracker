{"id": "QjkJdcbSDe", "number": 4815, "cdate": 1757771091830, "mdate": 1759898011656, "content": {"title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "abstract": "AI-assisted programming is rapidly reshaping software development, with large language models (LLMs) enabling new paradigms such as vibe coding and agentic coding. While prior works have focused on prompt design and code generation quality, the broader impact of LLM-driven development on the iterative dynamics of software engineering remains underexplored. In this paper, we conduct large-scale experiments on thousands of algorithmic programming tasks and hundreds of framework selection tasks to systematically investigate how AI-assisted programming interacts with the software ecosystem. Our analysis reveals \\textbf{a striking Matthew effect: the more popular a programming language or framework, the higher the success rate of LLM-generated code}. The phenomenon suggests that AI systems may reinforce existing popularity hierarchies, accelerating convergence around dominant tools while hindering diversity and innovation. We provide a quantitative characterization of this effect and discuss its implications for the future evolution of programming ecosystems.", "tldr": "", "keywords": ["AI programming assistants", "large language models", "code generation", "Matthew effect", "software ecosystem evolution", "programming languages and frameworks", "multilingual benchmarking", "agentic coding"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36a68dc5ec10aec1b9ce5d41b2f58bfa193d6e46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores how AI has influenced or will influence software development by investigating AI's performance asymmetry across different programming languages and frameworks. To this end, they constructed a large-scale benchmark to evaluate models' coding capabilities across eight languages and five categories of software development tasks. They reveal a Matthew effect: the more popular a programming language or framework is, the higher the success rate of LLM-generated code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- They construct a large-scale benchmark covering various programming languages. Considering that existing benchmarks mainly focus on a limited set of languages such as C++ or Python, their benchmark is valuable for assessing models' coding capabilities across diverse languages.\n- Their evaluation is systematic, including various metrics and statistical analyses.\n- They connect their evaluation findings to a broader societal perspective by discussing the Matthew effect in AI programming assistants."}, "weaknesses": {"value": "I don't think the results themselves are very surprising. It's well known that models trained with more examples in their training data tend to show higher performance on those tasks. Even though the paper describes the situation as a \"striking\" Matthew effect, I personally don’t find it surprising. In fact, as models evolve over time, such an effect might diminish due to their generally higher capabilities. Moreover, even if the Matthew effect persists, one could create specialized models for each programming language through fine-tuning to mitigate the issue.\n\nAnother question is how significantly the Matthew effect will impact software development. In any case, whether or not there is a Matthew effect in AI assistants, software development itself has long been centralized (as shown in Table 1). It is unclear whether the Matthew effect from AI will have a substantial influence on software development, or what the negative consequences of this situation might be. I think the paper needs to discuss the meaning of the Matthew effect, its negative consequences, and whether the effect will persist."}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7NSTLNXCaQ", "forum": "QjkJdcbSDe", "replyto": "QjkJdcbSDe", "signatures": ["ICLR.cc/2026/Conference/Submission4815/Reviewer_xgRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4815/Reviewer_xgRU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427006832, "cdate": 1761427006832, "tmdate": 1762917591336, "mdate": 1762917591336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper proved empirically the existence of training data gaps/bias for full-stack engineering and algorithmic code generation tasks across various programming languages, software frameworks using five commercial LLMs. They have contributed a comprehensive benchmark dataset to track progress/improvement of this bias over time."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Excellent job on highlighting the gaps in training data for full-stack engineering and algorithmic code generation tasks across various programming languages, frameworks in the leading commercial LLMs\n- Good call out on LLMs potential to amplify these training biases in the emerging coding paradigms such as vibe coding.\n- Good presentation of the topic overall."}, "weaknesses": {"value": "- The paper does not include justification for why the leetcode & CRUD task datasets were chosen as valid proxies for the larger programming ecosystem. Empirical scope of the study based on datasets explanation in section 3 seems narrow for below reasons \n    - LeetCode algorithmic problems scope is narrow as leetcode problems are self-contained exercises that capture coding ability rather than the broader aspects of software ecosystem (or) programming language evolution which is much broader with non-algorithmic aspects like software configuration, security, performance, maintenance for example.\n    - Similarly, the full-stack benchmark comprising only CRUD-style web-application tasks does not fully represent the breadth of the software ecosystem such as data engineering, distributed systems, embedded systems, or system integrations.\n\n- The discussion in the paper does not mention the existing structural biases in the software ecosystems pre-dating the LLMs. The software industry has favored certain languages/frameworks ecosystems for structural reasons (standardizations, secure implementations, maturity, ecosystem support). Without presenting the pre-LLM baselines for “Matthew Effect”, it is difficult to establish/quantify AI-induced amplification from historical continuation of these biases.\n\n- The paper simplifies language and framework selection by treating popularity and by extension AI compatibility as a dominant determinant for language/framework adoption and suggests - (Lines 20-23) “The phenomenon suggests that AI systems may reinforce existing popularity hierarchies, accelerating convergence around dominant tools while hindering diversity and innovation.” However, technical decision-making in the software ecosystem is not driven by popularity alone. It's a multi-dimensional decision carefully evaluated by senior engineers on the projects based on various factors like language/framework features, runtime performance, maintainability, ecosystem support, and business needs. So this claim need to be proven by considering the aforementioned aspects before reaching to conclusions."}, "questions": {"value": "Supplementary material seems to be missing in the submission, Where can i find the links to the code base and dataset to replicate this experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qe8ckNAXrh", "forum": "QjkJdcbSDe", "replyto": "QjkJdcbSDe", "signatures": ["ICLR.cc/2026/Conference/Submission4815/Reviewer_7q5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4815/Reviewer_7q5h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648521929, "cdate": 1761648521929, "tmdate": 1762917590831, "mdate": 1762917590831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates different LLMs on coding tasks taken from LeetCode.\nBot surprisingly the paper reveals that the more popular a programming language or framework, the higher the success rate of LLM-generated code. Besides LeetCode, the authors also evaluate \"vibe coding\" revealing that successful code generation is related to a few popular frameworks.\n\nIn terms of languages, the authors evaluate as follows: Python, C++, Java, JavaScript, Go, Rust, Erlang, and Racket for five models: GPT-4o-mini, DeepSeek-V3, Gemini-2.0-Flash, Gemini-2.5-Flash, Qwen3-Turbo.\n\nFor the framework selection tasks, three AI programming tools are used: Cursor Pro (using Claude-4-Sonnet), CodeBuddy (using Claude-4-Sonnet), and Visual Studio Code with GitHub Copilot (using GPT-5)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "With the widespread use of llms in coding tasks it is important to understand their strengths and limitations.\n\nThe paper contributes new benchmarks that would be useful for evaluating future models particularly on less popular languages and frameworks."}, "weaknesses": {"value": "The results are not surprising so it is not clear what this paper brings in terms of novelty."}, "questions": {"value": "It is not clear to me what metrics were used for evaluating vibe coding (section 5.2). Was the evaluation done manually?\n\nIs there a reason you do not evaluate C? Or is that subsumed by C++?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iwsZCIHxHs", "forum": "QjkJdcbSDe", "replyto": "QjkJdcbSDe", "signatures": ["ICLR.cc/2026/Conference/Submission4815/Reviewer_aCdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4815/Reviewer_aCdD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949796567, "cdate": 1761949796567, "tmdate": 1762917590076, "mdate": 1762917590076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the \"Matthew Effect\" in AI programming assistants. The authors demonstrate that popular programming languages and frameworks receive disproportionately better support from LLMs compared to less popular alternatives. Through large-scale experiments (120,440 code generations across 8 languages and 5 models), the authors demonstrate that language popularity strongly correlates with AI code generation success rates, with performance gaps widening as problem difficulty increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "To my knowledge, it's the first systematic demonstration of popularity bias in AI coding assistants at both language and framework levels, with clear quantitative characterization."}, "weaknesses": {"value": "1. The authors tested GPT-4o-mini, DeepSeek-V3, Gemini-2.0-Flash, Gemini-2.5-Flash, and Qwen3-Turbo, but excluded reasoning-specialized models like o4 or DeepSeek-R1 (although Qwen3-Turbo could be reasoning model). Given that most complex coding problems today are solved using these reasoning models, I'm a bit concerned about the generalizability.\n\n2. I wonder about the effects of Chain-of-Thoughts as well. CoT reasoning is utilized in most coding cases - would the presence or absence of chain-of-thought reasoning alter the popularity bias patterns?\n\n3. The paper shows correlational evidence between language popularity and model performance, but the underlying causal mechanism driving this relationship is a bit unclear. For instance, many popular languages tend to be easier, more user-friendly, and simpler to use (e.g., Python). Perhaps it's not language popularity but rather \"language difficulty\" that determines model performance - models might simply perform better on easier languages.\n\n4. The assumed link between training data representation and performance is not empirically verified. This leads to the concern about the causality of the claim."}, "questions": {"value": "1. How can you disentangle language difficulty from popularity? Have you considered matched comparisons of \"languages with similar complexity\" but different popularity? (e.g., Python is not only popular but also easy - so it's hard to know whether the model does Python well because of its popularity or complexity)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JydTGLUsdZ", "forum": "QjkJdcbSDe", "replyto": "QjkJdcbSDe", "signatures": ["ICLR.cc/2026/Conference/Submission4815/Reviewer_j1XZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4815/Reviewer_j1XZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177092634, "cdate": 1762177092634, "tmdate": 1762917589510, "mdate": 1762917589510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper highlights the Matthew Effect in Coding LLMs, where models do better on programming languages that are predominant in use and therefore predominant in the training set\n- The paper studies it in two ways: (1) Leetcode problems and (2) building CRUD applications\n- Creating a dataset of 3011 Leetcode problems across 8 languages, the paper tests 5 models and shows that models perform poorly on less-used languages. Similar observations are made for CRUD applications"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Notwithstanding the issues highlighted below, the papers' focus on studying model performance on low-resource languages is important.\n- The paper also does substantial experiments to highlight model performances on varying languages"}, "weaknesses": {"value": "- LeetCode does not have a public API. The paper mentions in Line 174 that the GraphQL endpoint is used to retrieve problem information. Can the authors comment on what this endpoint is, and whether it is the official endpoint?\n- LeetCode additionally prohibits scraping problems, which raises Terms of Service violation concerns. For this reason, I have flagged the paper for an ethical review.\n- To evaluate model-generated solutions, the authors create 15 user accounts on LeetCode and then submit solutions using these accounts with rate-limiting and other mechanisms to avoid detection. This also raises concerns about the misuse of the LeetCode platform\n- Due to the above issues, this dataset cannot be used to benchmark future models that aim to fix this issue. This limits reproducibility.\n- Section 4.1 mentions the list of post-processing steps taken on model-generated output to strip them of comments and other steps. I am not sure why these are required, and it seemed to me that the post-processing steps were excessive. Comments do not alter the program behavior, so why remove them?\n- What is the difference between the three graphs in Figure 2? There is no indication of how they differ\n- How are the vibe coding tasks evaluated? Are there automated test cases to evaluate the various functionalities?\n- Additionally, why propose a new dataset and not use an existing one with automated evaluation, such as the SWE-Bench Multilingual dataset (https://www.swebench.com/multilingual.html) for this purpose?\n- The SWE-Bench Multilingual results do not show the Matthew effect. Resolution rate on C/C++ is the lowest, while Rust is the highest. How would the authors reconcile the findings in this work, and what SWE-Bench Multilingual shows?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "- The paper created a dataset of ~3k problems scraped from Leetcode. Leetcode Terms of Service (https://leetcode.com/terms/) prohibit scraping of any kind:\n```\nActivities such as “crawling,” “scraping,” or “spidering” any part of the Service, and attempting to decompile, reverse engineer, or discover the source code or underlying ideas of the Service, are strictly forbidden.\n```\n- The paper uses LeetCode's GraphQL endpoint to access this, and to the best of my knowledge, LeetCode does not have an official API\n- Additionally, to evaluate model-generated solutions, the authors create 15 accounts on LeetCode, and submit the solutions using those accounts."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pXW6V5hLom", "forum": "QjkJdcbSDe", "replyto": "QjkJdcbSDe", "signatures": ["ICLR.cc/2026/Conference/Submission4815/Reviewer_EznC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4815/Reviewer_EznC"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189512831, "cdate": 1762189512831, "tmdate": 1762917588982, "mdate": 1762917588982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}