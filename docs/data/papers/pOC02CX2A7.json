{"id": "pOC02CX2A7", "number": 21537, "cdate": 1758318662145, "mdate": 1759896916969, "content": {"title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts", "abstract": "With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that **inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11\\% and 8\\% of cases**, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.", "tldr": "We conduct a systematic evaluation of the robustness of LLM-based guardrails against retrieval augmentation perturbations.", "keywords": ["Guardrail", "LLM safety", "RAG"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da33e72adf6a0f03297422f5f84568344f737816.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an investigation into the robustness of LLM-based guardrails under RAG-style contexts, aiming at addressing the vulnerability of external LLM-based guardrails to data distribution shifts. The study introduces a case study approach using RAG, a systematic evaluation of 3 Llama Guard models and 2 GPT-oss models, and a separate analysis of the effects of each component in the RAG-augmented context. Experimental results confirm that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work focuses on LLM-based guardrail robustness under RAG contexts. It is well-motivated.\n2. The proposed Flip Rate—quantifying guardrail judgment flips between vanilla and RAG-augmented settings without ground-truth—is inspiring and provides a scalable tool for evaluating context robustness.\n3. The paper’s figures are clear, effectively visualizing key findings and enhancing interpretability."}, "weaknesses": {"value": "1. In Section 5.2, the rationale for claiming content safety is consistent between vanilla and RAG-augmented settings is unclear. For example, inputs/outputs are safe in the vanilla setting; if RAG-augmented version query violates guardrails’ safety principles, this flip reflects guardrails’ correct judgment rather than poor robustness."}, "questions": {"value": "1. Can you verify on the dataset that such flips stem from guardrails’ correct judgment (not poor robustness)? Reporting its proportion would further justify the experimental setup’s rationality.\n2. What is a more robust way to use guardrails models between using them with RAG context or without?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Tdwzz2pamO", "forum": "pOC02CX2A7", "replyto": "pOC02CX2A7", "signatures": ["ICLR.cc/2026/Conference/Submission21537/Reviewer_r7jG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21537/Reviewer_r7jG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645148558, "cdate": 1761645148558, "tmdate": 1762941826434, "mdate": 1762941826434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study how well external LLM-based guardrail models stay consistent when given extra context from RAG. The main question is whether adding retrieved documents changes the guardrail’s safety decisions compared to judging the query or response alone.\nThe authors propose a metric called Flip Rate to measure how often these safety judgments change. They test both input guardrails and output guardrails using harmful and safe queries. Results show that context causes decision flips in about 11% for input guardrails and 8% for output guardrails. The paper also studies how document relevance and count affect the results and tests simple fixes like prompting or using high-reasoning modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies an important and less explored issue in LLM safety — how stable external guardrails.\n\n- Evaluation are detailed and comprehensive, considering many aspects.\n\n- It presents a simple and useful metric called FR to measure how often guardrail decisions change when the context shifts. \n\n- Experiments show that even normal RAG context can strongly influence guardrail decisions. This reveals a real and practical weakness in current systems."}, "weaknesses": {"value": "- The paper only test BM25 retrieval on Wikipedia. The conclusion may change for dense retrievers or new RAG methods.\n\n- FR only indicates changes in judgment. Change can be good or bad. But this paper regard flips as safety failures sometimes, but some flips may fix mistakes and is useful. \n\n- The prompting and high-reasoning mode fixes are simple and give little improvement. \n\n- The paper only uses normal retrieved documents. It does not test when the documents themselves include adversarial cues or misinformation (knowledge poisoning).\n\n- Missing references\n\n[1] TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation\n\n[2] FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA\n\n[3] Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning\n\n[4] A Survey on LLM-as-a-Judge"}, "questions": {"value": "1. See weakness\n\n2. Could you explain more about Flip Rate? While it measures inconsistency, how can we be sure a \"flip\" represents a degradation in safety rather than, in some cases, a context-aided correction? \n\n3. This paper focuses on benign documents. How do these guardrails would perform if the retrieved documents themselves contained adversarial content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WamKhkPAlj", "forum": "pOC02CX2A7", "replyto": "pOC02CX2A7", "signatures": ["ICLR.cc/2026/Conference/Submission21537/Reviewer_hda1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21537/Reviewer_hda1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753851401, "cdate": 1761753851401, "tmdate": 1762941826116, "mdate": 1762941826116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of LLM-based guardrails under retrieval-augmented generation contexts. The authors introduce the flip rate to measure how often a guardrail’s safety judgment changes when benign retrieved documents are added. Using five guardrail models across both harmful and safe queries, the study finds that judgments flip in both input-guardrail and output-guardrail cases. Additional analyses isolate the impact of document number, relevance, query safety, and generation model, and two mitigation attempts, high-reasoning mode and RAG-aware prompting, provide only marginal improvements. The work exposes a context-robustness gap in current guardrail systems and calls for training and evaluation frameworks that are robust to retrieval composition."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper identifies and formalizes an interesting failure mode of guardrail models, bridging research on RAG safety and LLM moderation.\n-\tThe evaluation covers diverse guardrails, realistic datasets, and controlled RAG setups. The decomposition across context factors is systematic and insightful.\n-\tThe paper is well structured with clear research questions, consistent definitions, and informative figures."}, "weaknesses": {"value": "-\tThis paper presents interesting findings but provides little in-depth analysis on why these flipped prediction happens. Existing work [1] also investigates the robustness/reliability of guardrail models from the aspect of prediction uncertainty, which I believe is related to the flipped prediction. Those flipped predictions may display high uncertainty and therefore could be easily manipulated with retrieved documents. I think this could be easily verified and at least discussed in the paper. \n-\tIn addition, the RAG-style queries/responses with the larger context might also explain the weak robustness of the safety prediction, which could act like out-of-domain samples and may not appear in the training data of existing guardrail models. Even though the training data of Llama-Guard is not open-source, existing advanced guardrail models like WildGuard [2] have open-source training data, making it easy to verify them by just comparing the sequence length between the training data and RAG-style queries.         \n-\tAnother confounder could be the safety judgment of guardrail models on the documents themselves. These documents/corpora are assumed to be safe with the prior knowledge. However, guardrail models may classify some documents as unsafe due to certain unsafe words/phrases in the doc, or failed prediction of guardrail models. In this case, the guardrail models may make opposite predictions once these documents are sampled. I would recommend an ablation study for this factor.\n\n[1] On calibration of LLM-based guard models for reliable content moderation. ICLR 2025.\n\n[2] Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. NeurIPS  2024."}, "questions": {"value": "-\tNote that the reasoning model only displays a less than 5% flipped rate. Then, how about other metrics like F1 score and FNR? Does the reasoning model improve the classification performance? Is it the case that the reasoning model corrects the wrong prediction so the flipped prediction happened?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VgM4gooQLl", "forum": "pOC02CX2A7", "replyto": "pOC02CX2A7", "signatures": ["ICLR.cc/2026/Conference/Submission21537/Reviewer_QBPT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21537/Reviewer_QBPT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975379457, "cdate": 1761975379457, "tmdate": 1762941825746, "mdate": 1762941825746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of safety guardrails when integrated with Retrieval-Augmented Generation (RAG) systems. The authors argue that retrieved documents—while improving factuality—can inadvertently alter guardrail decisions, compromising safety consistency. To quantify this, they propose a label-free metric called Flip Rate (FR), measuring how often a guardrail’s safety judgment changes under varying retrieval conditions. The study evaluates five popular guardrail models (three Llama Guard variants and two GPT-based ones) across both input-level and output-level settings, using thousands of harmful and safe queries retrieved via BM25 from Wikipedia. Extensive experiments reveal that guardrails exhibit notable instability—about 10.9% FR for inputs and 8.4% for outputs—and that factors such as the number and relevance of retrieved documents, query safety level, and the generator model significantly influence robustness. The authors also test lightweight mitigation strategies, including “high reasoning” and “RAG-aware” prompting, which yield only marginal improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper formalizes guardrail robustness under RAG by defining consistency requirements for input/output guardrails and introducing a label-free Flip Rate (FR) that quantifies judgment changes; it’s simple, scalable, and clearly distinguished from accuracy. It covers 5 guardrails (3 Llama Guard versions and 2 GPT-oss variants), evaluates both input and output settings, and spans 6,795 harmful queries plus additional safe queries—providing unusually comprehensive coverage for this topic. Key findings—e.g., input FR ≈10.9% and output FR ≈8.4%—are concrete and actionable; the work also shows task-dependent robustness differences across guardrails."}, "weaknesses": {"value": "While the proposed Flip Rate (FR) is an elegant, label-free measure of guardrail robustness, it inherently cannot distinguish between correct and incorrect judgment changes. A flip may indicate either an improvement or a degradation in safety performance, but the metric treats both equally. This limitation weakens the interpretability of FR as a genuine proxy for “safety robustness,” and suggests that additional labeled or human-audited analyses would be valuable for validation.\n\nMoreover, all RAG retrieval experiments rely solely on a BM25 retriever over English Wikipedia. This setup overlooks stronger and more representative retrievers (e.g., dense, hybrid, or cross-encoder–based retrieval). Since the retrieved context strongly drives guardrail flipping behavior, it remains unclear whether the reported instability generalizes to modern retrieval architectures. Including at least one dense retrieval baseline (such as Contriever or DPR) would substantially strengthen the empirical conclusions."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RlynnbkMqf", "forum": "pOC02CX2A7", "replyto": "pOC02CX2A7", "signatures": ["ICLR.cc/2026/Conference/Submission21537/Reviewer_AgfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21537/Reviewer_AgfF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21537/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762218694001, "cdate": 1762218694001, "tmdate": 1762941825382, "mdate": 1762941825382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}