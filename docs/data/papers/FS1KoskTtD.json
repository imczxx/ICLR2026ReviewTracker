{"id": "FS1KoskTtD", "number": 6976, "cdate": 1758003993944, "mdate": 1759897880179, "content": {"title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs", "abstract": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a ∼5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while closely rivaling proprietary\nclosed systems, while also demonstrating strong performance in general reasoning benchmarks: HLE, AIME-25, GPQA-Diamond, and MedQA.", "tldr": "", "keywords": ["DeepResearch", "Reasoning", "agentic reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35fda8a5d7160c9ed407b209806d23dc9e3af75f.pdf", "supplementary_material": "/attachment/c4831fd4dee25350f0252708d4e450fe48c9bdd7.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Fathom-DeepResearch, an open-source framework for long-horizon information retrieval and data synthesis for deep research agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation is clear and well-grounded, directly addressing key limitations in current DeepResearch models, such as unstable multi-turn reinforcement learning and inefficient tool usage.\n2.\tThe proposed methodologies are sound and well-justified, with each component, RAPO, DUETQA, and the Steerable Step-Level Reward, clearly contributing to overall model stability and performance.\n3.\tThe experiments are comprehensive, spanning both DeepResearch-specific and general reasoning benchmarks, and the results convincingly demonstrate the framework’s effectiveness and competitiveness against baselines."}, "weaknesses": {"value": "1. The paper points out that GRPO suffers from reward-hacking behaviors, where the model tends to overuse tools without improving reasoning quality. However, it does not clearly demonstrate that the proposed RAPO algorithm effectively mitigates this issue from the perspective of actual tool-call frequency. In Figure 4, only the response length is presented as a proxy for the number of tool calls, which is an indirect measure. A more concrete quantitative analysis or visualization of tool-call usage would strengthen the claim that RAPO truly alleviates reward-hacking behavior.\n2. More challenging  benchmarks, such as BrowseComp, are expected to be included to better demonstrate the model’s deep research capabilities and generalization to new, open-ended evaluation settings."}, "questions": {"value": "1. What's the meaning of \"SLMs\"? The title mentions SLMs, but the paper does not define this abbreviation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MZ3xhsu0RR", "forum": "FS1KoskTtD", "replyto": "FS1KoskTtD", "signatures": ["ICLR.cc/2026/Conference/Submission6976/Reviewer_CAQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6976/Reviewer_CAQE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452905948, "cdate": 1761452905948, "tmdate": 1762919195635, "mdate": 1762919195635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two-part agentic reasoning system built from Qwen3-4B models: Fathom-Search-4B, which performs multi-turn, evidence-based web investigation, and Fathom-Synthesizer-4B, which transforms search traces into citation-dense reports. The system introduces three contributions — the synthetic DUETQA dataset for training, the RAPO algorithm for stable multi-turn reinforcement learning, and a steerable step-level reward.  This work is evaluated across multiple benchmarks, including DeepSearch and general reasoning benchmarks, on both closed-source and open-source models/agents."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work provides synthetic training data to facilitate training the deep research agents.\n2. This work proposes reward-aware policy optimization (RAPO), and the experiments show the proposed methods can achieve better performance than vanilla GRPO.\n3. This work provides extensive experiments to verify the effectiveness of the method on multiple benchmarks and compare different closed-source and open-source baselines."}, "weaknesses": {"value": "1. This work lacks a detailed analysis of whether the final trained model can truly work for long-horizon tool calls. When it succeeds and when it fails?"}, "questions": {"value": "1. How long does it take to train the model for different stages?\n2. This work is claimed to build on top of RECALL (Chen et al., 20). Why not treat it as one of the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AEQht7yLvR", "forum": "FS1KoskTtD", "replyto": "FS1KoskTtD", "signatures": ["ICLR.cc/2026/Conference/Submission6976/Reviewer_ms6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6976/Reviewer_ms6f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881861000, "cdate": 1761881861000, "tmdate": 1762919195209, "mdate": 1762919195209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Fathom-DeepResearch, an open-source deep research agent system. The system consists of two coordinated 4B-parameter components: Fathom-Search-4B, a web-search model trained with an improved stabilized reinforcement-learning framework, and Fathom-Synthesizer-4B, a structured report generator trained under a *plan-then-write* protocol. The paper further introduces a dataset (DUETQA ) of 5k self-play-generated, live-search-dependent QA examples, and a synthetic DeepResearch-SFT corpus for training long-form synthesis. Evaluations across benchmarks show competitive performance of the proposed agent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work is well-motivated and technically complete, with reasonable contributions in this crowded space (1) a simple multi-agent framework and models (while most works explore single end-to-end model) (2) proposing a new RL training framework (3) datasets"}, "weaknesses": {"value": "- As this is a quickly evolving and crowded space, it's better to compare with more recent baselines, and more importantly, to compare with more recent related research on data synthesis and recent upgrades to GRPO (such as GSPO which is now widely used in this space).\n- Relatedly, the components in this work, including the data synthesis and minor improvements on the training part, by far, have been largely explored, which undermines the contribution of this work.\n- The RAPO relative to GRPO is still minor update per se, and the improvements in Table 3 is not very substantial.\n- To thoroughly evaluate the \"deep research\" capabilities of the agent, the paper would be benefited from incorporating commonly used browsecomp benchmark. Some RAG benchmarks such as SimpleQA is already relatively too old and too short to be informative in this space."}, "questions": {"value": "1. A typo in the Table 2: systems such Kimi-Researcher should be closed source agents?\n2. Will all the models and datasets be opensourced?\n3. Broader comparison: \n   1. The authors might consider summarizing how Fathom differs from other recent research agents (in dataset construction, training paradigm, and report synthesis) to contextualize its contributions within the current surge of deep-research systems.\n   2. the same for RL algorithms, such as GSPO which is now widely used by search agents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y4VMDx0coO", "forum": "FS1KoskTtD", "replyto": "FS1KoskTtD", "signatures": ["ICLR.cc/2026/Conference/Submission6976/Reviewer_nMq5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6976/Reviewer_nMq5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991499893, "cdate": 1761991499893, "tmdate": 1762919194684, "mdate": 1762919194684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}