{"id": "IWer3Ciqkp", "number": 24354, "cdate": 1758356029541, "mdate": 1759896769989, "content": {"title": "PG-VLM: A Multi-Stage Panoptic-Graph Architecture for Detailed Visual-Linguistic Grounding in Urban Scenes", "abstract": "Describing complex urban scenes with coherent paragraphs that are both semantically rich and spatially grounded is a key challenge for vision–language research. We present PG-VLM, a modular framework that (i) builds a Hierarchical Panoptic Scene Graph (HPSG) from panoptic segmentation, (ii) distills the graph into semantic triplets using a local instruction model, and (iii) generates narratives with a structured-to-text T5 generator. We assess text quality with standard captioning metrics and grounding with a new Narrative Relevance Detection Score (NRDS) that ties detection correctness to textual mention quality. On Cityscapes, PG-VLM surpasses recent vision–language baselines (BLIP-2, LLaVA-1.5 7B, SpatialVLM) across all metrics: CIDEr 135.0 (vs. 88.0/104.5/118.2), SPICE 28.8 (vs. 19.5/21.2/23.6), and BERTScore-F1 92.5 (vs. 88.0/89.0/90.1). Hallucination is reduced, with CHAIR-s 7.2 and CHAIR-i 9.5 (vs. 16.8/20.5 for BLIP-2, 13.0/16.2 for LLaVA-1.5, 11.4/14.8 for SpatialVLM). PG-VLM achieves substantially higher grounding via NRDS 0.76 compared to BLIP-2 at 0.52. A zero-shot check on BDD100K (50 images) indicates cross-dataset generalization (CIDEr 108.4, SPICE 24.1, NRDS-ZS 0.68), maintaining margins over all baselines. These results show that enforcing a symbolic bottleneck (HPSG to triplets) before generation improves both descriptive quality and faithfulness, offering a reproducible and extensible route to interpretable visual–language grounding in urban scenes.", "tldr": "PG-VLM generates spatially grounded paragraph-level descriptions of urban scenes using a panoptic scene graph, semantic triplets, and a structured-to-text generator, improving both accuracy and faithfulness over recent VLMs.", "keywords": ["panoptic scene graph", "vision-language model", "semantic triplets", "paragraph generation", "explainable AI", "spatial reasoning", "hallucination reduction", "urban scene understanding", "NRDS"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67932ffe61b051a1d9994b145bf2d67cfe1c89ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a multi-stage framework for captioning complex urban scenes called PG-VLM. It introduces Hierarchical Panoptic Scene Graph (HPSG) to construct the semantic triplets, which is composed of things, stuff and the spatial connection. Based on that, it uses the T5-Large Decoder to transfer the triplets to paragraphs. The generated captioning is better than baseline like BLIP-2, and the hallucination is mitigated."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation is easy to understand."}, "weaknesses": {"value": "- Unreliable experiments. The compared methods are out-of-time. Authors should take more closer MLLM whether open-source or close-source like Qwen series or GPT series into consideration to prove the advantages of PG-VLM.\n\n- Limited contribution. The effect of the data from PG-VLM is unexplored. It's better to supplement the experiments like panoptic segmentation, which would strengthen the contribution of PG-VLM. The current version is more like a semi-finished research.\n\n-Poor presentation. The presentation is poor. Like the missed reference in line 308 and line 368 for NSDR. The training detail of Teacher LLM is missed."}, "questions": {"value": "- NRDS relies on the CLIP. What is the difference between NRDS and other metrics like CLIP-score or CIDEr?\n\n- How to train teacher LLM and T5-Large Decoder? How to get the high quality supervision?\n\n- The triplets are constructed with all the objects in the scene, the number of combination is large. Is there any filter policy to mitigate or refine the triplets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xsY9q8YHHx", "forum": "IWer3Ciqkp", "replyto": "IWer3Ciqkp", "signatures": ["ICLR.cc/2026/Conference/Submission24354/Reviewer_NiKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24354/Reviewer_NiKR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645390618, "cdate": 1761645390618, "tmdate": 1762943054653, "mdate": 1762943054653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PG-VLM, a multi-stage vision-language framework that introduces a symbolic bottleneck between visual perception and language generation. By converting panoptic segmentation outputs into a Hierarchical Panoptic Scene Graph and structured triplets before paragraph generation, it enhances spatial grounding, factual accuracy, and narrative coherence, outperforming prior VLMs on the Cityscapes dataset while reducing hallucination."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed multi-stage pipeline (segmentation → scene graph → triplets → text) is well-motivated and improves interpretability and controllability.\n2. The symbolic bottleneck effectively reduces hallucination and strengthens spatial reasoning, as evidenced by improved CIDEr, SPICE, and NRDS scores.\n3. The introduction of NRDS provides a more nuanced way to assess factual and spatial alignment in generated descriptions."}, "weaknesses": {"value": "1. The paper’s writing could be improved for clarity and consistency. Certain sections (e.g., Section 6.3) contain citation or formatting inconsistencies, and the overall structure would benefit from clearer transitions and more careful editing to enhance readability.\n2. The paper mainly presents quantitative metrics without sufficient qualitative comparisons. Including more visual and textual examples side-by-side with baselines would help readers better understand the qualitative strengths of the proposed method.\n3. The experimental comparisons focus on earlier LVM such as BLIP-2, LLaVA, and SpatialVLM. Evaluating against more recent models (e.g., Qwen-VL, InternVL) would provide a more complete and up-to-date assessment of performance.\n4. While the multi-stage design is conceptually clear, most components (panoptic segmentation, scene graph construction, and T5-based generation) rely on existing architectures. The novelty lies mainly in their integration rather than in new algorithmic advances, which somewhat limits the methodological contribution.\n5. The study briefly mentions human evaluation but does not detail the evaluation protocol, number of raters, or inter-rater reliability. Without this information, it’s difficult to judge the statistical validity of human assessment results."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2qnsx6eSFV", "forum": "IWer3Ciqkp", "replyto": "IWer3Ciqkp", "signatures": ["ICLR.cc/2026/Conference/Submission24354/Reviewer_bGkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24354/Reviewer_bGkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726627718, "cdate": 1761726627718, "tmdate": 1762943053878, "mdate": 1762943053878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper designs a new framework that uses an intermediate graph structure to improve the accuracy of spatial information in VLM predictions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The main idea makes sense. The experiments are small, but they still show that their approach, using a smaller T5 model, managed to beat larger models."}, "weaknesses": {"value": "This paper has two very significant flaws.\n1. First, the writing quality is poor and suggests a rushed submission. The figures are crude, typos are frequent (like line 307), and the text is confusing. They even forgot to describe the loss function, which is a critical omission.\n2. Second, the experiments are far too small to be convincing. The conclusions cannot be trusted because the scale is inadequate: the relation set is tiny (12 categories), the testset is only 50 images, and the training set is just 5000 images from one source. This is not enough to prove the method's generalizability.\n\nBesides, I think there's another major problem: the paper relies on similarity-based metrics (e.g., BLEU), which are ill-suited for this task. These metrics fail to capture the factual correctness of critical spatial information, which is the core contribution. The reported gains may simply reflect overfitting to linguistic patterns rather than true improvements in spatial reasoning. A more robust, fact-centric evaluation is required."}, "questions": {"value": "Please refer to the issues detailed in the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yL1gMrFVbj", "forum": "IWer3Ciqkp", "replyto": "IWer3Ciqkp", "signatures": ["ICLR.cc/2026/Conference/Submission24354/Reviewer_iJTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24354/Reviewer_iJTU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987330824, "cdate": 1761987330824, "tmdate": 1762943053654, "mdate": 1762943053654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper approaches the task of detailed image captioning on the Cityscapes dataset. The proposed model first builds a graph from panoptic segmentation outputs and their spatial positioning relationships. From graph triplets, a T5 generates the final detailed caption for the urban scene. This paper presents results on this task using pseudo-labels and compares them to those of other state-of-the-art image captioning models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This review evaluates the paper's quality based on the following criteria: task relevance, related work, technical novelty, technical correctness, experimental validation, writing and presentation, and reproducibility. Each aspect is discussed and highlighted as a strength or a weakness in the sections below.\n-    **Relevance of the task:**  Detailed Image Captioning is a highly relevant problem for the machine learning community."}, "weaknesses": {"value": "-    **Reproducibility and Implementation Details:** It is not indicated whether the source code will be released, and it's not included as part of the submission.\n-    **Related Work and Technical Novelty:** The Related Work section does not adequately contextualize the contributions. It is not clear how the proposed method addresses the limitations of current detailed image captioning methods.\n-    **Writing and Presentation:** This paper is not easy to read. It lacks sections on the metric formulation. Its organization does not allow the reader to adequately understand the problem/motivation, its prevalence in current state-of-the-art methods, the proposed methodology, how this methodology solves the research gap, and finally, how well the experiments support it.\n-    **Experimental Validation and Technical Correctness:** The empirical validation of this paper is not clear from the dataset used. According to the paper description (Lines 240–244), it utilizes the panoptic annotations of Cityscapes for training and pseudo-labels for the detailed image captioning task, generated using the same T5 model. Are these pseudo-labels also used for evaluation and comparison with previous state-of-the-art methods? This experimental choice will result in an obviously biased validation towards the outputs of the pretrained model, which puts the proposed method at an evident advantage against any other model not based on this generator."}, "questions": {"value": "1.\tWill the source code and pretrained models be released to support reproducibility? If so, what is the reason for not including them in the supplementary material?\n2.\tHow does the method improve upon or differ from existing detailed image captioning approaches?\n3.\tCan the paper clarify the motivation, the gap in prior work, and how the method addresses it?\n4.\tWhat metrics are used for evaluation, and can their formulation be clarified?\n5.\tAre the pseudo-labels used for both training and evaluation? If so, how is evaluation bias avoided?\n6.\tHow does the use of pseudo-labels affect fairness in comparison with other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quotvrFa90", "forum": "IWer3Ciqkp", "replyto": "IWer3Ciqkp", "signatures": ["ICLR.cc/2026/Conference/Submission24354/Reviewer_xYbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24354/Reviewer_xYbm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992467032, "cdate": 1761992467032, "tmdate": 1762943053415, "mdate": 1762943053415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}