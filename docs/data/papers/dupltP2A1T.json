{"id": "dupltP2A1T", "number": 14779, "cdate": 1758243542027, "mdate": 1759897349741, "content": {"title": "Lions and Muons: Optimization via Stochastic Frank-Wolfe", "abstract": "Stochastic Frank-Wolfe is a classical optimization method for solving constrained optimization problems. On the other hand, recent optimizers such as Lion and Muon have gained quite significant popularity in deep learning. In this work, we provide a unifying perspective by interpreting these seemingly disparate methods through the lens of Stochastic Frank-Wolfe. Specifically, we show that Lion and Muon with weight decay can be viewed as special instances of a Stochastic Frank-Wolfe, and we establish their convergence guarantees in terms of the Frank-Wolfe gap, a standard stationarity measure in non-convex optimization for Frank-Wolfe methods. We further find that convergence to this gap implies convergence to a KKT point of the original problem under a norm constraint for Lion and Muon. Moreover, motivated by recent empirical findings that stochastic gradients in modern machine learning tasks often exhibit heavy-tailed distributions, we extend Stochastic Frank-Wolfe to settings with heavy-tailed noise by developing two robust variants with strong theoretical guarantees, which in turn yields new variants of Lion and Muon.", "tldr": "We present a Stochastic Frank-Wolfe framework, encompassing Lion and Muon, and develop variants leveraging gradient clipping and variance reduction techniques to handle heavy-tailed noise in stochastic gradients.", "keywords": ["stochastic Frank-Wolfe", "stochastic optimization", "Lion optimizer", "Muon optimizer", "heavy-tailed noise", "gradient clipping", "variance reduction"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa7ba55505ceb5aa922b86af8cc45d1fd634411e.pdf", "supplementary_material": "/attachment/e4911256ce133ad3bc685ee0408ef8df931b18e6.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the popular optimizers Lion and Muon as special cases of the Stochastic Frank-Wolfe method under different matrix norms. The authors provide a theoretical convergence analysis of these methods in terms of the Frank-Wolfe gap, which is comparable to convergence toward a KKT point of the original problem under a norm constraint. Furthermore, the authors explore several modifications, such as variance reduction and clipping for the heavy-tailed noise with $p \\in (1, 2]$, which enable them to establish high-probability convergence guarantees. Finally, the paper presents a series of numerical experiments demonstrating that the clipped versions, namely Lion+ and Muon+, exhibit greater robustness compared to their unclipped counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **A interpretation of popular methods and unified analysis.** The authors provide an interesting interpretation of the Lion and Muon methods through the lens of the Stochastic Frank-Wolfe algorithm, in a rather natural form using Theorems 1 and 2. This perspective on modern optimizers allows the authors to develop a convergence analysis for various modifications, based on classical proof techniques. \n\n2. **Experiments.** The authors present numerical experiments with the clipped methods using nanoGPT and ResNet18 models, as well as results on synthetic data, which fully support their theoretical findings."}, "weaknesses": {"value": "### **Minor shortcomings**\n1. **Additional literature.** Regarding the related work, I would suggest including references on heavy-tailed stochasticity, specifically studies related to classical adaptive methods such as AdaGrad [1] and Adam [2], as clipping has been successfully applied to them as well.\n\n2. **Overloaded notation of parameters.** According to the presented theorems, it is not entirely clear why the parameter notation is defined in a way that suggests they change over time, e.g. $\\beta_{1, t}$. This creates unnecessary confusion, even though the theorems state that the parameters are taken as constants. For example, in Theorem 1, $\\beta_{1, t}$ corresponds to $\\beta \\equiv const$, while in Theorem 4, $\\beta_{1, t}$ corresponds to $1 - \\frac{1}{T^{1/3}}$, which does not depend on $t$. It would be helpful to consider reassigning the parameters for the methods under study to improve clarity and readability.\n\n3. **Experiments with VR are missing.** It would be interesting to see how Lion/Lion+ and Muon/Muon+ perform when variance reduction technique is applied.\n\n4. **The algorithms (Lion+/Lion++ and Muon+/Muon++) notation is misleading.** The use of + and ++ in the algorithm names is confusing, as these simply refer to existing implemented modifications of gradient clipping and heavy-ball momentum. It is worth considering alternative names -- such as ''Clipped Lion'' and ''Clipped Lion with double momentum\".\n\n### **Major weaknesses**\n\n1. **Dependence of the active set $\\mathcal{C}$ on weight decay.** Looking at Corollary 1, one can see that the convergence bound directly depends on the diameter of the set $\\mathcal{C}$, i.e., $D$. At the same time, in the presented theorems, the authors state that this diameter for Lion and Muon is equal to $\\frac{1}{\\lambda}$, where $\\lambda$ denotes the weight decay parameter. This implies that when $\\lambda = 0$, the analysis in this work cannot be directly applied, even though in practice the absence of weight decay does not prevent convergence, although it may become slower. Compared to other works, in [3], Muon is interpreted through a trust-region optimization framework, which does not involve any constraints over the problem domain. Moreover, even if $\\lambda \\neq 0$, this leads to a significant deterioration of the convergence bound due to a multiplicative constant, which, based on practical experience, is expected to be quite large.\n\n2. **Unified analysis suffers for the case of heavy-tailed noise.** If we look at the notation introduced by the authors and the heavy-tailed noise setting (i.e., Theorems 5 and 6), one might get the impression that the norm considered in Algorithms 5 and 6 can be arbitrary. Unfortunately, this is not true. Indeed, the authors refer to Lemma 5 from [4], but that lemma holds only when the norm is either the $|| \\cdot ||_2$-norm in a vector space or the $|| \\cdot ||_F$-norm in a matrix space. Examining the proof of that lemma in [4], we can see that while the bias bound remains valid, the variance bound no longer holds if an arbitrary norm is considered. This is due to the fact that the proof relies on variance decomposition -- or, more precisely, on the next fact: if $X$ is a random vector, then, if the second moment exists, the next equation holds:\n\\begin{align}\n      \\arg\\min_y \\left[f(y) = \\mathbb{E}[||X - y||_2^2\\right] = \\mathbb{E}X.\n\\end{align}\nFor the matrix space, the same holds with Frobenius norm. However, this does not hold for an abstract norm. This automatically leads to an implicit usage of $|| \\cdot ||_2$- or $|| \\cdot ||_F$-norm in the analysis. Therefore, the obtained results cannot be directly applied to Lion and Muon, as these methods are based on other norms. It might be reasonable to take into account the equivalence of norms in finite-dimensional spaces; however, this would lead to a noticeable deterioration of the convergence bound.\n\nTo summarize, the main shortcomings I would like to highlight are the **hidden dependence of the convergence bounds on the weight decay parameter** and the **inability to apply the theoretical results for the heavy-tailed noise to Lion and Muon in the primal form**.\n\nDespite the concerns mentioned above, I am open to discussion and ready to reconsider my evaluation based on the authors’ responses.\n\n---\n\n**References**\n\n[1] High probability analysis for non-convex stochastic optimization with clipping. Li, S. and Liu, Y. (2023) \n\n[2] Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is Heavy-Tailed. Chezhegov, S. et al. (2024)\n\n[3] Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization. Kovalev, D. (2025)\n\n[4] Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise. Liu, Z. et al. (2023)"}, "questions": {"value": "1. Did the authors attempt to develop the analysis without weight decay? If so, what difficulties did they encounter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y8dnJUq7p5", "forum": "dupltP2A1T", "replyto": "dupltP2A1T", "signatures": ["ICLR.cc/2026/Conference/Submission14779/Reviewer_dgop"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14779/Reviewer_dgop"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775969949, "cdate": 1761775969949, "tmdate": 1762925134113, "mdate": 1762925134113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper takes a closer look at two popular optimizers, Lion and Muon, and connects them to the classical Stochastic Frank–Wolfe method. It is shown that both optimizers can actually be viewed as specific cases of SFW under certain norm constraints, Lion corresponds to an l_infty-norm ball and Muon to a spectral-norm ball. Using this connection, the paper provides convergence results for both methods in terms of the Frank-Wolfe gap and show that convergence in this gap implies reaching a KKT point of the constrained problem. The paper also consider the fact that gradients in modern deep learning are often heavy-tailed, extending the SFW framework to such settings and introducing new robust versions of Lion and Muon with theoretical guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes nice connection between two popular ML optimizers, Lion and Muon, with well studied Frank Wolf method. Thus provideding nice fundation to study the properties of these more recent algorithm, since it is possible to draw on a lot of existing theory. This is a nice connection, and as far as I can see, new.\n\nThe paper supplies theoretical convergence guarantees (in terms of the Frank–Wolfe gap) for Lion and Muon in the non-convex, stochastic setting, including providing convergence rate bounds, though I had some questions about this (see Weaknesses)\n\nThe paper extends the analysis to heavy-tailed gradient noise settings, which is realistic for modern deep-learning training, and proposes robust variants of the algorithms with theoretical backing. To my understanding, this is a new algorithm, so the paper does not only analysis existing algorithms, it also uses the insights from FW to improve the existing algorithms."}, "weaknesses": {"value": "There could be better analsisis and comparison to the work exing convergence analysis of Lion and Muons. There is discussion both in the paper comparing to other work on Lion and Muons, but much of it is quite technical and difficult to read. There are no clear statements on the novelty and iprovments to these results, I mean in, e.g., in terms of better convergence rate or generalization in terms of simple structiors. Or a clear statement of lack of such improvements.  It is unclear, if similar results are in the literature, but maybe using different type of analysis. Or a simple comparison with existing theoretical work on Muons and Lions."}, "questions": {"value": "Why do you need clipping, where there is assumption that the gradient is bounded?\n\nIn the experiments, how does the convergence comperate to the theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sF5PDmxFmG", "forum": "dupltP2A1T", "replyto": "dupltP2A1T", "signatures": ["ICLR.cc/2026/Conference/Submission14779/Reviewer_FJms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14779/Reviewer_FJms"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908281671, "cdate": 1761908281671, "tmdate": 1762925133297, "mdate": 1762925133297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a unifying theoretical and algorithmic perspective on state-of-the-art neural network optimization methods, notably Lion and Muon, by casting them as special cases of Stochastic Frank-Wolfe (SFW) algorithms under specific norm constraints. The authors establish convergence guarantees for Lion and Muon (including weight decay) in terms of the Frank-Wolfe gap, and show that this gap corresponds to convergence to KKT points of constrained optimization problems. Motivated by the prevalence of heavy-tailed stochastic gradients in deep learning, the paper further augments the SFW framework with robust variance reduction and gradient clipping techniques, accompanied by non-asymptotic, high-probability convergence guarantees. Experiments on nanoGPT (Shakespeare dataset) and ResNet18 (CIFAR-10) empirically demonstrate improved training and validation loss for the clipped variants (LION+, MuON+) over their original counterparts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "__1) Unification of Modern Optimizers and Classical Theory:__ The paper offers a compelling and elegant theoretical connection between popular optimizers (Lion and Muon) and the classical Stochastic Frank-Wolfe framework through explicit norm constraints. This synthesis is precisely formalized (Theorems 1 and 2) and supported by proofs in Appendix.\n\n__2) Theoretical Rigor:__ The work provides convergence guarantees for both classical variance-bounded and heavy-tailed stochastic regimes. The resulting guarantees match or improve upon best-known complexity rates for the Frank-Wolfe gap, such as the $O(1/\\epsilon^3)$ rate achieved with variance reduction (see Theorem 4 and Corollary 2), and extend to new scenarios with only $p$-moment bounded noise (Theorems 5 and 6).\n\n__3) Algorithmic Extensions with Practical Impact:__ The proposal and analysis of robust clipped variants (LION+, Muon+, as instantiated from Algorithm 5 and 6) directly address observed issues of heavy-tailed gradients in deep learning training, with both theoretical and experimental support."}, "weaknesses": {"value": "__1) Relationship between Algorithms 1–3 and the role of parameters.__ Algorithms 1 and 2 are special cases of Algorithm 3 for particular choices of the sequences $\\beta_{1, t}, \\beta_{2, t}, \\gamma_t$. I find the current parameterization unnecessarily complicated:\n\na) $\\beta_{1, t}$ appears time-invariant in the proofs.\n\nb) $\\beta_{2, t}$ does not seem to be used in the main convergence theorems.\n\nc) $\\gamma_t$ is constrained by $\\gamma_t \\leq 1 - \\beta_{2, t} = 1 - \\beta$, which again removes time dependence.\n\nIt is not clear what particular $\\beta$ should be used to instantiate the corollaries. In particular, when $\\gamma + \\beta = 1$, the Algorithm 3 update appears to degenerate to a pure sign/gradient-like step with $\\hat g = g$. Could the authors clarify which $\\beta$ is used in Corollary 1 (and, more generally, in the statements relying on Theorem 3)? Where exactly is Corollary 1 proved? It seems to be derived by fixing a large batch $m = T$ in Theorem 3, but the precise parameter $\\beta$ is not spelled out. Please provide an explicit, self-contained proof or a fully specified set of substitutions from Theorem 3.\n\n__2) Diameter $D$ versus weight-decay $\\lambda$.__ The convergence rates (e.g. Corollary 1) scale linearly with $D=diam(C)$. For Lion/Muon, $C$ is the $\\ell_{\\infty}$ or spectral ball of radius $1 / \\lambda$ (Theorems 1–2). Typical deep-learning practice sets $\\lambda \\approx 1e−4, 1e−5$, implying $D \\approx 1e4, 1e5$ and hence an extremely large multiplicative constant. The authors do not discuss how this theoretical penalty aligns with the observed fast empirical convergence. Guidance on the “safe” range of $\\lambda$ and its interplay with the final optimization error is absent. Importantly, this diameter-sensitivity is an artefact of the present Frank–Wolfe analysis: earlier convergence proofs for Muon (e.g. in Li J., Hong M. “A note on the convergence of muon and further”) avoid the $1 / \\lambda$ factor entirely by working directly on the unconstrained problem.\n\n__3) Variance-reduction (VR) algorithm absent from experiments.__ Algorithm 4 integrates VR and improves the convergence rate. Nevertheless, all experiments in the main part employ the basic clipped versions (Lion+, Muon+) rather than the VR variants. It is unclear whether the VR machinery is only a theoretical artefact or has practical relevance.\n\n__4) Incremental value of “+” versions.__ Lion+ and Muon+ amount to the original optimisers with gradient clipping. Clipping is already the de-facto implementation in popular code-bases; consequently the “novel” variants coincide with what practitioners have been running. The reported improvements are modest and stem from a single clipping threshold tuned per optimiser. \n\n__5) Empirical scope.__ Both tasks (nanoGPT 117 M parameters on Shakespeare, ResNet-18 on CIFAR-10) are small-scale. Modern evidence of heavy-tailed gradients relies on billion-parameter Transformer pre-training. Experiments on larger models/datasets (e.g. LLaMA-style data, ViT-H ImageNet-22k) are necessary to substantiate the practical impact of the heavy-tail robust theory."}, "questions": {"value": "1) What happens to the convergence bounds when $\\lambda \\to 0$ or $\\lambda \\to \\infty$? Does the complexity blow up, and can the bounds be made adaptive to $\\lambda$?\n2) In the proof of Theorem 1 (lines 1039–1042) the term $1/m_{t+1}$ appears abruptly inside the expectation without prior definition of the mini-batch sampling protocol for Algorithm 3. Please clarify whether this is a typo or an implicit uniform-batch assumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6XxxDRw1W", "forum": "dupltP2A1T", "replyto": "dupltP2A1T", "signatures": ["ICLR.cc/2026/Conference/Submission14779/Reviewer_62Gv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14779/Reviewer_62Gv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908394174, "cdate": 1761908394174, "tmdate": 1762925132684, "mdate": 1762925132684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers show how that Lion and Moun algorithms are particular instances of a Stochastic FW method, for which authors show a number of convergence guarantees. As a result, the provided results provides a partial justification to these popular methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The unified view of Lion and Moun as instances of a Stochastic FW method (Algorithm 3), and corresponding convergence results to KKT points, provide a good partial understanding of those methods.\n\n- Convergence results with high-probability are well positioned w.r.t. the state of the art (ignoring log factors)"}, "weaknesses": {"value": "- **Mismatch between the theoretical constrained setting and practical usage of Lion and Muon.**  \nThe paper models Lion and Muon as instances of Stochastic Frank-Wolfe over *bounded convex domains*. However, in their original formulations and practical implementations, **both optimizers operate over the unconstrained space** ($\\mathbb{R}^d$ (or $\\mathbb{R}^{m \\times n}$) with a *soft* weight decay regularization term, rather than a *hard* norm constraint. Consequently, the theoretical equivalence established in this work is mostly **conceptual rather than operational**: the constrained problem studied here does not correspond exactly to how Lion or Muon are applied in practice.\n\n- **Horizon-dependent hyperparameters.**  \nAll the theoretical results rely on *horizon-dependent parameter schedules* (e.g., step size, momentum, and averaging parameters depend explicitly on $T$). This limits applicability in streaming or online scenarios where the horizon is not known in advance, or in statistical learning settings beyond ERM. It would strengthen the paper to discuss whether adaptive or online tunings could preserve similar guarantees.\n\n- **Unrealistic batch size requirements.**  \nSeveral theorems (notably Theorem 3 and Corollary 1) assume batch sizes proportional to the time horizon $T$, which is rather impractical for large-scale or resource-constrained training. While the authors later mitigate this by using variance reduction (reducing the average batch size), it would be helpful to explicitly quantify how sensitive the bounds are to smaller mini-batches and to clarify the trade-offs between theory and practice.\n\n- **Clipping parameter depends on unknown noise scale.**  \nIn the heavy-tailed regime, the clipping threshold \\(M\\) is defined in terms of the noise level \\(\\sigma\\), which is typically *unknown and hard to estimate* in practice. This makes the proposed clipped variants difficult to tune reliably. A discussion on practical heuristics or data-driven estimators for \\(\\sigma\\) would make the contribution more actionable."}, "questions": {"value": "Please answer to the weaknesses highlight in the related section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OcdFXdbCeC", "forum": "dupltP2A1T", "replyto": "dupltP2A1T", "signatures": ["ICLR.cc/2026/Conference/Submission14779/Reviewer_w8WV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14779/Reviewer_w8WV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001071654, "cdate": 1762001071654, "tmdate": 1762925132352, "mdate": 1762925132352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}