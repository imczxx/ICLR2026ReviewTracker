{"id": "VzMoNcY6Ju", "number": 14268, "cdate": 1758231586692, "mdate": 1759897379949, "content": {"title": "Deep Reinforcement Learning For Nash Equilibria in Non-Renewable Resource Differential Games", "abstract": "Characterizing Nash equilibria in oligopolistic non-renewable resource markets poses major challenges for computational economics, as traditional iterative methods face scalability limitations due to the curse of dimensionality. In this work, we propose a reinforcement learning–based approach to compute these equilibria and benchmark it against a modified iterative baseline, derived from an established algorithm for differential games and adapted to the oligopoly case. We conduct experiments in monopoly, duopoly, and multi-player settings, evaluating both reward accuracy and computational efficiency. Our results show that while iterative schemes provide good accuracy in low-dimensional problems, reinforcement learning scales more effectively to three- and four-player games, leading to a substantial reduction in computation time. This highlights the potential of reinforcement learning as a scalable tool for solving complex differential games in resource economics.", "tldr": "", "keywords": ["Reinforcement Learning", "Game Theory", "Differential Resource Games"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c96f924e30fe366bd7ff77c61711761ae21e866.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies computation of feedback Nash equilibria in oligopolistic extraction games (non-renewable resources) modeled as continuous-time differential games. It benchmarks a multi-agent deep RL approach (a MATD3-style actor–critic) against an iterative dynamic-programming/value-iteration baseline adapted to this domain. The RL method aims to scale to more players and higher state resolution with reduced wall-clock time, while achieving comparable rewards/equilibria quality. Experiments cover 2–4 players and compare solution quality, convergence behavior, and compute costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem. The model class (extraction competition with stock dynamics) is well specified and relevant to economic/operations contexts. Assumptions and reward structures are transparent.\n\nCompetent empirical engineering. The MATD3 variant is implemented sensibly; comparisons to a carefully tuned value-iteration baseline are fair. Results consistently show that RL attains comparable payoffs while scaling better with players/state grids.\n\nReproducible pipeline. The environment dynamics, training schedule, and evaluation metrics are documented well enough to reproduce, and ablations on learning rates and noise are helpful.\n\nPotential cross-domain utility. Demonstrates that modern MA-RL can approximate equilibria in continuous-time games where classical DP gets expensive, useful for practitioners in computational economics."}, "weaknesses": {"value": "Insufficient ML novelty. The method is essentially MATD3 with minor adaptations to this environment. There is no new learning rule, objective, exploration scheme, or stability result. For ICLR, the contribution reads as a domain application study rather than an ML advance. Suggestion: Introduce a principled algorithmic component—for example, a differentiable equilibrium consistency penalty, opponent modeling that enforces best-response dynamics, or a theoretically motivated regularizer exploiting game structure (e.g., resource-stock monotonicity).\n\nLack of equilibrium certification. The paper evaluates via rewards and policy stability but does not certify $\\varepsilon$-Nash-ness. Without a best-response check or KKT/VI residuals, it is unclear whether the learned policies are truly equilibria or merely good heuristics. Suggestion: Add an ex-post best-response solver (single-agent control with others fixed) and report $\\varepsilon$-gaps. Alternatively, compute stationarity/consistency residuals (e.g., HJB residuals) over a dense state grid.\n\nLimited problem scale and diversity. Experiments stop at 3–4 players with relatively low-dimensional states and simple cost structures. It’s hard to conclude general scalability from such a narrow slice. Suggestion: Include a higher-dimensional variant (stochastic prices, extraction costs with learning effects) or >5 players, and report runtime/memory scaling.\n\nComparative baselines are narrow. Only a modified value-iteration baseline is considered. No comparisons to policy iteration, FBSDE/HJB solvers, or mean-field game approximations that are standard in continuous-time settings. Suggestion: Add at least one additional numerical method (policy iteration or FBSDE-based) and a mean-field limit baseline to triangulate quality/speed.\n\nRobustness & sensitivity underexplored. The effect of model mis-specification (e.g., demand shocks, parameter drift), stochasticity, and discretization choices on equilibrium quality is not probed. Suggestion: Provide sensitivity analyses: grid resolution, noise variance, discount factor, and reward curvature; show how $\\varepsilon$-gaps change.\n\nTheory/explanation gap. No structural insight is offered about why the RL policies approximate equilibria well in this class (e.g., contraction properties, local regularity, or monotone operator structure). Suggestion: Even a partial analysis (showing that the joint greedy update corresponds to a fixed-point iteration for a variational inequality) would strengthen the paper."}, "questions": {"value": "Equilibrium gap: Can you compute an $\\varepsilon$-Nash metric via best-response optimization for a held-out state grid? Reporting $\\varepsilon$ would materially strengthen claims.\n\nSolver diversity: How does your approach compare to policy iteration or actor–critic with policy evaluation via PDE discretization?\n\nOpponent learning dynamics: Did you try independent learners vs. centralized critics vs. opponent modeling? Which variant reduces non-stationarity most?\n\nScaling beyond 4 players: What breaks first: critic instability, sample complexity, or action-space exploration? \n\nMean-field approximation: For many players, could a mean-field limit give an analytic baseline, and can your RL solution be shown close to the MFG equilibrium?\n\nStochastic shocks: How sensitive are policies to volatility in prices or reserves? \n\nAblations on discretization: Report performance as a function of state/time discretization; show whether improvements persist as the grid refines.\n\nEconomic interpretability: Can you extract comparative statics (e.g., how taxes or extraction costs shift equilibrium) to validate against economic theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rac9gRQcbU", "forum": "VzMoNcY6Ju", "replyto": "VzMoNcY6Ju", "signatures": ["ICLR.cc/2026/Conference/Submission14268/Reviewer_W6h2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14268/Reviewer_W6h2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760778588401, "cdate": 1760778588401, "tmdate": 1762924721533, "mdate": 1762924721533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the application of deep reinforcement learning to solve multi-agent problems in differential games, specifically within non-renewable resource duopolies and oligopolies. The authors first provide an analytical derivation of the theoretical equilibrium structure for their environment. They then use simulation to observe whether the agents' empirically obtained rewards attain this equilibrium."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The authors address a relevant problem in learning within economic games. The results of this study could be genuinely useful to economists, particularly for informing the development of subsequent analytical frameworks."}, "weaknesses": {"value": "- While the convergence of the policy to an optimal reward is a positive sign, it does not necessarily indicate that the policies have converged to a true equilibrium. To demonstrate this more effectively, the authors should include additional measures such as exploitability or simplex plots.\n\n- Assuming the empirical results have converged to equilibrium, the paper provides no theoretical guarantees regarding the convergence rate or the specific scenarios under which convergence will or will not occur (see [3]). In other words, the method might work for the presented set of hyperparameters and initial conditions, but not for all, or only for a subset of them.\n\n- Given that the authors base their contributions on empirical evidence, the ablation study is extremely weak. There are simply not enough ablations of configurations and parameter settings; from what is presented, only a handful of configurations and a single set of hyperparameters were tested. For instance, the authors should consider varying the discount rate, the number of agents, and various parameters of their deep reinforcement learning algorithm. This is critically important in the absence of theoretical guarantees.\n\n- Furthermore, there is no baseline comparison to previous state-of-the-art algorithms, such as Empirical Game-Theoretic Analysis (EGA) [1] or evolutionary methods [2].\n\n- Overall, the work appears neglectful of previous contributions in the field of multi-agent learning (see [1,2,3,4], to name a few). A deeper literature review and a substantial revision of this work are strongly suggested.\n\n---\n[1] Wellman, Michael P., et al. \"Empirical game theoretic analysis: A survey.\" Journal of Artificial Intelligence Research 82 (2025): 1017-1076.\n\n[2] Hofbauer, Josef, and Karl Sigmund. \"Evolutionary game dynamics.\" Bulletin of the American Mathematical Society 40.4 (2003): 479-519.\n\n[3] Bichler, Martin, et al. \"Characterizing the Convergence of Game Dynamics via Potentialness.\" arXiv preprint arXiv:2503.16285 (2025).\n\n[4] Mertikopoulos, Panayotis, and Zhengyuan Zhou. \"Learning in games with continuous action sets and unknown payoff functions.\" Mathematical Programming 173.1 (2019): 465-507."}, "questions": {"value": "- Is the knowledge of the game provided to the agents, or are they learning based on a multi-input black box?\n\n- Could a central controller be used to manage the policies more effectively and steer them toward equilibrium more quickly? Did the authors consider relaxed equilibrium notions, such as correlated equilibrium or coarse correlated equilibrium?\n\n- Was there any discretization of the agents' action space? If so, could the authors comment on its potential effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "emV2u7pF9z", "forum": "VzMoNcY6Ju", "replyto": "VzMoNcY6Ju", "signatures": ["ICLR.cc/2026/Conference/Submission14268/Reviewer_7fAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14268/Reviewer_7fAq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760952073407, "cdate": 1760952073407, "tmdate": 1762924720249, "mdate": 1762924720249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational challenge of finding Nash equilibria in multi-player differential games of non-renewable resource extraction by proposing a deep reinforcement learning approach based on the Multi-Agent TD3 algorithm, which is systematically benchmarked against a modified iterative method to evaluate its performance across various market structures from monopoly to oligopoly settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a multi-agent deep reinforcement learning approach to solve high-dimensional differential games, offering a promising alternative to traditional methods hampered by the curse of dimensionality.\n\n- The study provides a systematic evaluation across monopoly, duopoly, and oligopoly settings, delivering a clear comparative analysis between the proposed RL method and a well-designed iterative baseline.\n\n- The authors identify and analyze methodological limitations, particularly the early-stopping phenomenon, enhancing the work's credibility and providing valuable insights for future research."}, "weaknesses": {"value": "- The paper lacks a comprehensive discussion of critical implementation aspects, such as hyperparameter selection, neural network architectures, and the sensitivity of results to random seeds. This omission challenges the reproducibility of the study and limits the analysis of the RL method's training stability.\n\n- While the paper empirically identifies the \"early stopping\" phenomenon, it fails to provide a deep theoretical explanation for its fundamental causes. The analysis does not sufficiently explore how function approximation errors or exploration strategies might systematically bias the agents' policies toward premature resource depletion.\n\n- The scalability limitations of the multi-agent value iteration baseline itself undermine its utility for comparison. In 3- and 4-player games, where the baseline is computationally constrained to use coarse discretization, it cannot serve as a high-quality reference point, thus weakening the performance evaluation of the RL approach in these critical settings."}, "questions": {"value": "- While the paper acknowledges variance in RL results, it doesn't sufficiently quantify the stability of the outcomes. Could you specify the range of fluctuation in final policies and rewards when retraining with different random seeds? Is there a systematic approach to ensure consistent performance across runs?\n\n- Do you attribute \"early stopping\" primarily to value function approximation errors, or could it represent a valid alternative equilibrium discovered through exploration? Is there experimental evidence showing this behavior can be mitigated by modifying reward functions or termination conditions?\n\n- For the 3- and 4-player games, the baseline method used coarse discretization due to computational constraints. Does this mean the observed \"advantage\" of RL might partly stem from the baseline's inability to provide high-quality reference solutions in these scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kytwCa2md1", "forum": "VzMoNcY6Ju", "replyto": "VzMoNcY6Ju", "signatures": ["ICLR.cc/2026/Conference/Submission14268/Reviewer_5Gps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14268/Reviewer_5Gps"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905841921, "cdate": 1761905841921, "tmdate": 1762924718864, "mdate": 1762924718864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper applies deep RL to an economics problem, achieving results that align well with known theory where such theory exists, and scales beyond such situations to 3- and 4-agent settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and clear, and addresses an interesting and topical problem, namely, the use of MARL to solve models of economic interest. The results in the single- and two-agent settings agree with the theoretical results, demonstrating the soundness of the method; and the technique scales beyond what manual theoretical analysis has permitted, to 3- and 4-agent settings."}, "weaknesses": {"value": "My main criticism is that it is not clear to me that this paper is \"ready\" for publication. The paper has results, but little discussion of why an economist (or a computer scientist) might care. I include in the \"Questions\" section some lingering questions that I had, whose answers may strengthen the paper if included. If extra space is needed, things like Figure 1 (which, to my understanding, just depicts a standard RL setup) can be moved to an appendix or removed altogether.\n\nThe paper assumes a lot of background in economics which some readers (including myself) might not have, especially with regard to the motivation for the problem. I review the paper mostly taking for granted the underlying economic problem/model and its importance, because I do not have the expertise to evaluate that. \n\nAlong the above note, for the more ML-heavy audience that you'd find at ICLR, I'd suggest including some more relevant background information that you'd probably consider \"basic\", or to use language that this audience would be more accustomed to, e.g. \"single-agent\", \"two-agent\" instead of \"monopoly\" and \"duopoly\". But this is a relatively minor suggestion."}, "questions": {"value": "By my understanding, the game being modeled here is a perfect-information, continuous-time Markov game. Assuming I have understood this correctly:\n\n1. You are discretizing the time, but maintaining the symmetry of the game by insisting that the agents choose their actions simultaneously on every timestep. Symmetry is nice, but this choice greatly increases the complexity of the problem. Have you considered an alternating-turn disretization instead, in which each firm acts in turn at each timestep, with knowledge of the earlier firms' actions? Alternating-turn games are much easier to solve because they are amenable to backwards induction with a simple maximization instead of a Nash computation at every step. Moreover, in the RL setting, there are other better RL techniques like AlphaZero that can be applied, only to alternating-turn games of perfect information. And, since your time is continuous anyway I would imagine that it would not make too much of a difference except that it breaks symmetry. \n\n2. Related to the above: can you elaborate a bit more in text how the local equilibrium is computed at each state for the iterative method? It seems to me that the Nash computation required here may be difficult and/or interesting.\n\n3. Do you have any intuition for why the RL method \"stops early\" or depletes its resources relatively quickly? Do you think this \"problem\" would go away with more training time?\n\n4. Do you have any qualitative economic conclusions from the results computed in the oligopoly setting? If I understand correctly, there are no known theoretical results for that setting---so, perhaps, do the strategies that you see tell you anything interesting? If so, I'd include it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wFuqnvUIgr", "forum": "VzMoNcY6Ju", "replyto": "VzMoNcY6Ju", "signatures": ["ICLR.cc/2026/Conference/Submission14268/Reviewer_3ksd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14268/Reviewer_3ksd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933483307, "cdate": 1761933483307, "tmdate": 1762924718413, "mdate": 1762924718413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}