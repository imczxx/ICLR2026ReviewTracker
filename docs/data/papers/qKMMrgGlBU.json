{"id": "qKMMrgGlBU", "number": 9174, "cdate": 1758113902022, "mdate": 1759897739432, "content": {"title": "AccLoRT: Efficient Large Language Models Pretraining through Low-Rank Accumulation", "abstract": "Pretraining large language models (LLMs) poses significant computational challenges, particularly due to the memory requirements that exceed the capabilities of standard GPU devices. To address these issues, we introduce a fully low-rank approach for LLMs pretraining to improve the memory efficiency. Specifically, our approach sequentially trains low-rank matrices and accumulates them into a frozen high-rank matrix until convergence. Notably, our approach enables the low-rank traning without a warm up phase with full parameter, therefore achieving memory efficiency in the entire training process. We provide a comprehensive theoretical analysis for our proposed method by establishing the upper and lower bounds for the rank of multiple matrix sums and analyzing the rank dynamics in low-rank adapters. The results show that with finite accumulation steps, the accumulated low-rank training is equivalent to full-rank training. Extensive experiments on both synthetic reduced rank regression and practical Llama models (60M to 1B parameters) validate the effectiveness of the proposed approach in pretraining, demonstrating its potential to make LLM development more accessible and efficient.", "tldr": "", "keywords": ["Large Language Model", "Low-rank Training", "Memory Efficient Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bcf26c32b3c5919814efd2703d1dc7f3464d7e07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AccLoRT (Accumulated Low-Rank Training), a memory-efficient pretraining framework for large language models (LLMs). The method sequentially trains multiple low-rank matrices and accumulates them into a frozen high-rank matrix, enabling fully low-rank training throughout the entire pretraining process without any full-parameter warm-up. The authors provide theoretical analyses on the rank bounds of matrix sums, LoRA’s rank evolution under SGD, and the asymptotic equivalence between AccLoRT and full-rank training. Extensive experiments on Llama models (60M–1B) demonstrate that AccLoRT achieves superior perplexity and memory efficiency compared to existing methods such as GaLore, ReLoRA, and SLTrain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper  derives upper/lower rank bounds and theorems describing LoRA’s rank evolution and AccLoRT’s convergence, giving a clear mathematical understanding of why accumulation works.\n\n2. Empirical evaluations span a wide range of model sizes (60M–1B) shows improvements.\n\n3. The proposed idea is good and reasonable."}, "weaknesses": {"value": "1. The paper completely omits \"Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?\"[1], a closely related approach that also performs full-rank pretraining under low-rank constraints. Both share nearly identical experimental settings (rank, optimizer, and dataset) and similar motivation (achieving full-rank training). Without detailed discussion in motivation, methodology, memory consumption, and experimental performance comparison, the claimed advantages of AccLoRT remain incomplete and potentially overstated.\n\n2. Current model size in experiments is small. It would be better to see pretrain experiments on 7B models. In this experimental setting, both Fira [1], Galore [2],  APOLLO [3] all conduct the 7B experiments.\n\n3. Illustration is poor. For example, the font size in Figure 1 and Figure 3 is too small to read for potential readers.\n\n\n[1] Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?\n\n[2] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n\n[3] APOLLO: SGD-like Memory, AdamW-level Performance"}, "questions": {"value": "see weaknesses\n\nI will adjust my score according to the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cG6aF1eoD0", "forum": "qKMMrgGlBU", "replyto": "qKMMrgGlBU", "signatures": ["ICLR.cc/2026/Conference/Submission9174/Reviewer_13sN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9174/Reviewer_13sN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417503636, "cdate": 1761417503636, "tmdate": 1762920851950, "mdate": 1762920851950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to pretrain model by accumulating low rank updates. In the theory part, the paper provides upper and lower bounds for the weight matrix rank after accumulating the low rank updates and bounds for the ranks of adapter matrices $A,B$ with gradient updates. In experiment, the paper proposes to separate between the first initialization and subsequent ones after each merge, where in the first initialization $A,B$ are obtained by the truncated QR decomposition of a normal matrix and in the subsequent initializations, $A$ is 0 and $B$ is normal. The experiment shows that the proposed method has better perplexity on the pretrain task using Llama, and is competitive with other methods on fine-tune tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- There have been several works on using accumulating LoRA, but understanding the rank evolvement during training remains not well studied. This paper provides both upper and lower bounds for this question, which I appreciate.\n- The proposed method is simpler than similar existing approaches, such as ReLoRA. In particular, ReLoRA requires a warm up stage for the full model while this method doesn't.\n- Empirically, the method performs better than existing approaches with memory footprint."}, "weaknesses": {"value": "- First of all, the paper lacks clarity in the representation.I'm not sure I really understand the proposed method. \n  - In particular, how is the full model $W$ initialized? Is it to 0? If this is the case, it very surprising to me. \n  - Theorem 3.3 also doesn't state the necessary assumptions, such as the initialization of $A_0$, $B_0$. Its proof in the appendix is also not clearer. For example, line 1304 says the input $x$ is drawn from a discrete set referred in line 1274 - 1284 where some assumptions about this set is made, which I'm also not sure I understand.\n- The algorithmic contribution of this paper to me is quite incremental. It boils down to finding a different initialization of $A,B$. The idea of merging the adapters to the base model has been explored quite a lot before.\n- To this end, I'm not sure I understand where the improvements in the experiment comes from. In particular, compared with ReLoRA which basically uses the same merging idea, and even warms up the model before the low-rank update phase, why does the proposed method perform better?"}, "questions": {"value": "- Please see the weaknesses.\n- In table 2, can the authors provide the comparison with the other methods in Table 3?\n- Line 215 mentioned $W_{acc}$, Is there a difference with the weight $W$ in Algorithm 1?\n- In Algorithm 1, what is the goal of the if-else statement in Line 224-227? It doesn't seem the be explained in the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uOWEARpGM8", "forum": "qKMMrgGlBU", "replyto": "qKMMrgGlBU", "signatures": ["ICLR.cc/2026/Conference/Submission9174/Reviewer_Et4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9174/Reviewer_Et4y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521510697, "cdate": 1761521510697, "tmdate": 1762920851652, "mdate": 1762920851652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AccLoRT, a framework for memory-efficient pretraining of large language models (LLMs) based on sequential accumulation of low-rank matrices. The core idea is to sidestep the memory bottleneck of full-rank training by progressively training and freezing low-rank adapters. Theoretical analyses are provided, establishing upper and lower bounds for the rank of summed matrices, and elucidating rank dynamics during training. Extensive experiments on synthetic regression and Llama models from 60M to 1B parameters are presented, showing empirical benefits in memory usage and perplexity relative to prior methods. The approach is evaluated both in pretraining and fine-tuning scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The memory efficiency challenge in LLM pretraining is timely and relevant. The authors provide a compelling motivation, emphasizing the increasing computational inaccessibility of fundamental LLM research.\n2. AccLoRT’s approach to accumulating low-rank matrices is carefully described, including specific details on initialization, memory trade-offs, and update mechanisms. \n3. The experiments cover both toy and large-scale settings. Figure 3 reveals the effect of initialization on loss and perplexity for Llama models, giving actionable insight into implementation choices.\n4. Table 3 demonstrates AccLoRT achieving strong or comparable perplexity to full-rank and other efficient training methods (like GaLore, ReLoRA, SLTrain) across various Llama model sizes—often with meaningful memory savings (as detailed in Table 2)."}, "weaknesses": {"value": "1. While the related work section summarizes many recent memory-efficient fine-tuning and pretraining approaches, it overlooks several directly relevant recent advances in low-rank and model compression for LLMs.\n2. While Table 2 comprehensively compares model sizes and memory/parameter usage, Table 3's comparison is focused on perplexity only. \n3. The framing and experiments focus exclusively on LLMs and linear regression; generalizability to vision models, multi-modal LLMs, or other architectures is not tested, even though low-rank techniques are often portable."}, "questions": {"value": "1. Table 3 shows that AccLoRT marginally underperforms GaLore/full-rank at the 1B parameter scale, but outperforms on smaller models; do the authors attribute this to the hyperparameters, the method itself, or intrinsic limitations of low-rank accumulation as dimensionality increases?\n2. Is there an avenue for automating the choice of rank $r$ and accumulation frequency $T$ (possibly dynamically during training) as opposed to hand-tuning or grid search?\n3. Could the memory savings be further quantified in terms of wall-clock time, power consumption, or batch size enabled, especially on single-GPU or low-resource devices (beyond the current synthetic/LLama runs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5bjgO4alE", "forum": "qKMMrgGlBU", "replyto": "qKMMrgGlBU", "signatures": ["ICLR.cc/2026/Conference/Submission9174/Reviewer_UsMi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9174/Reviewer_UsMi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553883353, "cdate": 1761553883353, "tmdate": 1762920851295, "mdate": 1762920851295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AccLoRT, a method for memory-efficient pre-training of LLMs with fully low-rank training. The core mechanism is a periodic accumulation and re-initialization strategy, which trains the adapters from scratch without initial pretrained weights, merges into a fixed high rank matrix, and continues with reinitialized adapters. The paper provides a theoretical basis which shows that the accumulated low-rank training is equivalent to full-rank training after a finite number of accumulation steps.\n\nExperiments on pretraining of Llama models and finetuning of RoBERTa-base models validate the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Despite a simple strategy, the method achieves good results across pretraining setup.\n- Some discussion and experiments on the initialization of low rank matrices.\n- Extensive experiments including those that help understand training progression."}, "weaknesses": {"value": "- For large model pretraining, AccLoRT did not achieve significant gain over GaLore. Although the memory usage is lower at early stage, AccLoRT approaches the same memory usage level as GaLore if using the same rank. How does the extra memory at early stage benefit training?\n- Would the plateau towards the end of each training cycle slows down the training in general and complicate the choice of ranks and iteration steps? Could the authors provide perplexity progression throughout training and compare with GaLore.\n- The method is not efficient in fine-tuning. The performance is sometimes not as good as LoRA despite a larger adapter size (8 x num iterations). Also, the accumulation frequency varies across settings. What is the principal for choosing the rank and accumulation frequency? How sensitive the results are for these choices of hyper-parameters?"}, "questions": {"value": "- In table 1, for LoRA, should the total be + 4r(n + m) for both AccLoRT and LoRA?\n- L450: \"In the 1B parameter setting, while GaLore achieves the best perplexity of 15.64, AccLoRT maintains competitive performance at 16.61.\", but in the table there's no 16.61 but 15.49. Is it a typo?\n- Since ReLoRA is a very close approach, could you add more discussion for the difference in methodology that leads to the improvement. It seems that ReLoRA is only mentioned as requiring a full parameter warm up pretraining."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQEENUtw2K", "forum": "qKMMrgGlBU", "replyto": "qKMMrgGlBU", "signatures": ["ICLR.cc/2026/Conference/Submission9174/Reviewer_Kmhq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9174/Reviewer_Kmhq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070144497, "cdate": 1762070144497, "tmdate": 1762920850976, "mdate": 1762920850976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}