{"id": "z6uWNKEbo0", "number": 12031, "cdate": 1758205293687, "mdate": 1759897538169, "content": {"title": "Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression", "abstract": "Sparse linear regression is a fundamental tool in data analysis. However, traditional approaches often fall short when covariates exhibit structure or arise from heterogeneous sources. In biomedical applications, covariates may stem from distinct modalities or be structured according to an underlying graph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that integrates covariate-specific side information into sparse regression via neural networks. Nash adaptively modulates penalties on a per-covariate basis, learning to tailor regularization without cross-validation. We develop a variational inference algorithm for efficient training and establish connections to empirical Bayes regression. Experiments on real data demonstrate Nash’s improved accuracy and adaptability over existing methods.", "tldr": "We propose Neural Adaptive Shrinkage (Nash), a sparse regression method that uses neural networks to learn covariate-specific penalties that we fit using a novel variational method that is very efficient.", "keywords": ["Empirical Bayes", "variational inference", "split inference", "high dimensional", "penalized regression", "penalty learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/519cccd05271fd881fdb6f371bc55821f54fc373.pdf", "supplementary_material": "/attachment/06a0f6c0eb8a61b62a8458189d06feb95e0b44cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Neural Adaptive Shrinkage (NASH), a novel framework for high-dimensional regression that leverages neural networks to incorporate covariate-specific side information. The core idea is to learn an adaptive, structured penalty function on a per-covariate basis, guided by the side information. This is framed within an empirical Bayes perspective, and the authors propose an efficient \"split VEB\" (Variational Empirical Bayes) algorithm for model fitting, which decouples prior learning from posterior computation. The method is shown to generalize several existing penalized regression techniques and demonstrates strong empirical performance on various real-world datasets and an image denoising task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a novel and unified method to integrate diverse side information (e.g., groups, graphs, time) into regression using a neural network-based prior.\n- The proposed split VEB algorithm is scalable and efficient, making the approach practical by decoupling the prior learning and posterior computation steps.\n- The method is shown to be highly competitive and often outperforms established baselines across a comprehensive set of real-world experiments."}, "weaknesses": {"value": "- Performance can be sensitive to the neural network architecture and its hyperparameters, which adds a layer of complexity to its practical application.\n- The use of a neural network to define the penalty structure may reduce the model's interpretability compared to classical regularization techniques.\n- The paper is primarily empirical and would be strengthened by theoretical results, such as convergence guarantees for the proposed algorithm."}, "questions": {"value": "Can the trained neural network provide insights into the underlying structure of the covariates? For example, is it possible to interpret what features of the side information the model found most important for determining the regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XRtcS0KwIx", "forum": "z6uWNKEbo0", "replyto": "z6uWNKEbo0", "signatures": ["ICLR.cc/2026/Conference/Submission12031/Reviewer_7a2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12031/Reviewer_7a2y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038719921, "cdate": 1762038719921, "tmdate": 1762923012254, "mdate": 1762923012254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method to adaptively penalize the coefficients in linear regression. The authors provide a model, derive a loss and an algorithm, and validate on MNIST and tabular data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The topic is interesting and the paper is rather easy to read."}, "weaknesses": {"value": "Weaknesses:\n- Clarity:\n    - I am not sure I understood the proposed algorithm, would it be possible to encapsulate it in an environment, as done in [1], maybe it would be a good opportunity to highlight the difference with [1]\n    - In Figure 1, bottom left, where dore the GNN-based prior comes from? Is it a pre-trained GNN? On other data? Could authors provide more details on this specific figure?\n- Novelty: I am not sure I understood the difference with [1], could authors comment on that? Is it a generalization of [1] to graph-based data?\n- Experiments\n    - Would it be possible to add a vanilla CNN as a benchmark for Figure 2? (I understand this is a different kind of technique, just want to have an order of magnitude)\n    - Table 1, how significant are the performance gain?\n\n\n[1] Youngseok Kim, Wei Wang, Peter Carbonetto, and Matthew Stephens. A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression. Journal of Machine Learning Research, 25"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "33Z9zq2dIy", "forum": "z6uWNKEbo0", "replyto": "z6uWNKEbo0", "signatures": ["ICLR.cc/2026/Conference/Submission12031/Reviewer_fhSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12031/Reviewer_fhSk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088780039, "cdate": 1762088780039, "tmdate": 1762923011935, "mdate": 1762923011935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new Bayesian method for linear regression.\nThe prior distribution of the coefficients has product form. Each coefficient \\beta_j has prior that is the convolution of a Gaussian with variance \\sigma_0^2, and a distribution that depends on some side information d_j and is parametrized by common parameters \\theta.\nThe paper describes a variational method for estimating the parameters of the prior, and performing inference on the linear model coefficients:\n- The posterior is approximated by a product posterior.\n- The corresponding ELBO is  maximized via coordinate ascent.\nThe paper illustrates the application of their approach to various regression problems, and how the new prior introduced in this paper can \nmodel a variety of complex structures (in particular group- and graph- based penalies) and demonstrates effectiveness on a denoising problem."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Regression with structural information about the coefficients is an central problem in high dimensional statistics with countless applications. Any progress on this problem is welcome.\n2. The proposed framework is very general.\n3. Simulations demonstrate some promising results."}, "weaknesses": {"value": "1. The prior construction seems a straightforward extension of Wang & Stephens (2021) and Kim et al. (2024). The main innovation is the introduction of the \"side information\" d_j. While this is helpful, especially for graph-based tasks considered here, it is a very natural idea.\n2. The variational inference algorithm is an application of standard methodology."}, "questions": {"value": "1. I think Section 4, 5 describing the application and empirical results the most important of the paper. I think they should be expanded, spelling out in each case what is the architecture, how the prior was constructed, what are the x's and y's, what are the d's and so on. \n\n2. In contrast, Fig. 1 is fairly obvious/ not informative (and could be removed for reasons of space). Same consideration for the bottom panel of Fig 2\n\nMinor:\n\n1. Eq (8), you should write what expectation is over\n\n2. Also the subscript Nash to the ELBO appears in random positions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8XQeQvJnP", "forum": "z6uWNKEbo0", "replyto": "z6uWNKEbo0", "signatures": ["ICLR.cc/2026/Conference/Submission12031/Reviewer_4eXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12031/Reviewer_4eXJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101698985, "cdate": 1762101698985, "tmdate": 1762923011551, "mdate": 1762923011551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nash (Neural Adaptive Shrinkage), a framework for high-dimensional sparse regression that leverages covariate-specific side information through neural networks to learn adaptive penalties. The authors develop a split variational empirical Bayes (VEB) algorithm that decouples prior learning from posterior inference. The paper establishes connections to mr.ash and demonstrates competitive performance on four real datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The split VEB approach addresses a real computational bottleneck of  the mr.ash variational formulatio. By decoupling the updates of problems, Nash requires only one neural network update per coordinate ascent iteration per updates. Theorem A.1 provide the lower-bound relationship to mr.ash.\n\nS2. The authors successfully demonstrates how Nash can encompass various structured penalties (group lasso, fused lasso, IPF-lasso) within a single framework. \n\nS3. The authors clearly explains the variational formulation, coordinate ascent updates, and connections to exiting  variational Empirical Bayes approach."}, "weaknesses": {"value": "W1. The authors claim Nash is \"the first work to propose the use of a neural network to incorporate covariate side information when learning the penalty function,\" but fail to demonstrate why neural networks are necessary. They can compare more classical baselines including Kernel-based methods (e.g., RBF kernels on side information).\n\nW2. The author employed only 4 real datasets, no synthetic data demonstrating when/why NNs help. It would be helpful if they can provide scenarios with complex non-linear side information where NN superiority would be clear.\n\nW3. The theoretical contribution is limited to Theorem A.1. Although it shows Nash ≥ lower bound of mr.ash, but I want to understand How tight is this bound in practice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Gzhw0sPbhP", "forum": "z6uWNKEbo0", "replyto": "z6uWNKEbo0", "signatures": ["ICLR.cc/2026/Conference/Submission12031/Reviewer_976x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12031/Reviewer_976x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762510519906, "cdate": 1762510519906, "tmdate": 1762923011095, "mdate": 1762923011095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}