{"id": "xsqiDQjSvV", "number": 8565, "cdate": 1758091137253, "mdate": 1759897776180, "content": {"title": "ASMG: Data Structure-Aware Routing via Incremental Subspace Learning for MoE", "abstract": "Mixture-of-Experts (MoE) models scale model capacity efficiently by selectively\nrouting inputs to a subset of specialized experts. However, their performance\ncritically hinges on the gating mechanism, which is typically implemented as a\nshallow linear projection followed by a softmax or sigmoid activation. This minimal\ndesign lacks the representational capacity to capture structural variations in the\ninput, leading to poor expert specialization and suboptimal routing. To address this,\nwe propose Adaptive Structure-Aware MoE Gating (ASMG), a data-driven gating\nmechanism that dynamically interpolates between a standard learnable gating\nmatrix and an evolving principal subspace learned via the Generalized Hebbian\nAlgorithm (GHA). By tracking input structure with iterative basis update with GHA,\nASMG enables the gating function to remain both learnable and structure-aware\nthroughout training. We validate our method through two controlled synthetic tasks:\n(i) Gaussian mixture classification and (ii) sequence modeling from multinomial\nHMMs, demonstrating improved expert-input alignment and routing specialization.\nExtending to language and vision benchmarks, ASMG achieves consistent gains\nover MoE baselines. Moreover, optionally enabling unsupervised GHA updates at\ntest time further improves robustness under distribution shifts, offering an online\nadaptation mechanism that upgrades standard gating with stronger OOD resilience.", "tldr": "We propose ASMG, a structure-aware gating mechanism for Mixture-of-Experts that combines learnable gating with an incremental principal subspace learning, improving expert specialization and routing across synthetic, language, and vision tasks.", "keywords": ["Mixture of Experts", "Input-Aware Routing", "MoE", "GHA", "Unsupervised Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ff4edf12e88de93b888fdac89d91c12e5bc9423.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores a method for MoE gating by computing a gating weight matrix on the principal components of the inputs.  The (approximate) principal components are learned online using GHA, and the gating matrix decomposed into RV where R is in the principal component space, limited to K components.  This is combined with a vanilla gating matrix using linear combination with a learned combination parameter.  The behavior of this method is explored first on two synthetic toy data distributions (gaussian classification and HMM language stand-in), and then applied to five language tasks from GLUE and four vision datasets, with small but likely consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Limiting gating to principal components makes intuitive sense as a way to increase gating robustness, especially earlier on in training, as it could quickly align experts to work within buckets of the largest variance directions.  The approach has promise, with good analyses in the two synthetic toy sections.  The analyses in the toy data examples clearly illustrate a source of potential."}, "weaknesses": {"value": "While the real-world data seems to have some consistent improvements, these are very small right now.  The mixture coefficient between plain gating and GHA gating goes towards GHA, this is also a small change (at least for the current training time and schedule).\n\nIt's also unclear how this interacts with balancing, and if the source improvements of this method is separate or overlapping from effects of balancing.  Balancing constraints for MoE are common and important, and these also lead to better expert specializations and usage, for example by reducing assignment collapse.  So it's important to understand how this method interacts with these techniques.  Does it have similar effects and reduce the need for them?  Or is it separate and a potential source of gain on top?\n\nOverall, while there are some good ideas here, the work still seems preliminary.  Is the mixture really needed, or can it be reduced to the GHA side more aggressively (and can this or other small changes lead to larger gains)?  Does this method work with multiple expert layers (right now gains are small enough in the real-world experiments that it's unclear how much it's actually enabling itself)?"}, "questions": {"value": "3.2.3  says Z = RV is initialized with random R and \"subsequently trained\" (l.195), but then in the next sentence says \"R is fixed and not updated\".  This seems conflicting --- if R isn't updated, what is updated in Z?\n\n3.2.5:  what is the cosine similarity computed over?  the text just says it's similarity between experts.  is it between gating vectors?  or something with the outputs, or expert weights?\n\n3.3.4:  \"Since hidden representations evolve dynamically throughout training, it is no longer feasible to directly optimize the full GHA-derived gating matrix Z\".:  I don't see why this would necessarily be the case.  What are the issues that happen learning the GHA gating from the start?  also what is alpha initialized to?\n\nFig 6:  is the y axis alpha or sigmoid(alpha)?\n\n* What are the distributions of assignments to experts for each strategy (naive, SVD/GHA, ASMG)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2HujiO8Trg", "forum": "xsqiDQjSvV", "replyto": "xsqiDQjSvV", "signatures": ["ICLR.cc/2026/Conference/Submission8565/Reviewer_9td1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8565/Reviewer_9td1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761268940518, "cdate": 1761268940518, "tmdate": 1762920419026, "mdate": 1762920419026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive Structure-Aware MoE Gating (ASMG), a new gating mechanism for MoE models designed to capture structural variations in the input, thereby addressing the limitations of conventional shallow linear routers. The method leverages the Generalized Hebbian Algorithm (GHA) to incrementally learn an evolving principal subspace of the input distribution during training and, optionally, at inference. The final routing mechanism is an interpolation between this data-driven subspace and a standard learnable gating matrix. Analyses on synthetic data, alongside experiments on real-world language and vision benchmarks, confirm that ASMG yields improvements over standard MoE baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, clearly structured, and easy to follow.\n- The application of GHA is novel, enabling the routing mechanism to adapt during both training and inference. \n- A key strength of ASMG is its capacity for online adaptation at test time, which offers a practical solution for improving model robustness under distribution shifts.\n- The empirical evaluation across both language and vision tasks  demonstrates that the proposed method surpasses conventional gating strategies."}, "weaknesses": {"value": "- The motivation for introducing the mixing coefficient matrix $\\mathbf{R}$ is not well-established. The paper would be strengthened by an ablation study demonstrating the necessity of this component, as well as a clearer intuitive explanation for why a learnable linear combination of principal components is preferable to using the components directly.\n\n\n- The proposed method constructs its structure-aware basis using GHA to find the top-K principal components of the input distribution. This formulation inherently constrains the number of basis vectors (and thus experts, $K$) to be less than or equal to the input's dimensionality $d$, i.e. $K \\leq d$. This constraint runs counter to a primary advantage of sparse MoE, which is the ability to scale the number of experts far beyond the model's hidden dimension. This limitation could hinder ASMG's applicability in scenarios that require a very large pool of experts.\n\n- The first synthetic task, Gaussian mixture classification, may be overly simplistic. Given that the data is generated from linearly separable clusters and ASMG's router is initialized by extracting principal directions from the entire training set as a pre-processing step (Section 3.2.3), the problem is significantly simplified. Consequently, the superior performance of ASMG in this setting is expected and may not generalize. The validation would be more compelling if conducted on a synthetic task with non-linear decision boundaries or more complex data structures.\n\n- Furthermore, the insightful analyses on expert routing, representation, and collaboration are confined to this simplistic synthetic task. The paper would be more impactful if these analyses were extended to the real-world language and vision experiments to demonstrate that similar specialization behaviors occur in more complex settings.\n\n- The paper lacks a critical ablation study on the necessity of the interpolation mechanism itself. An analysis comparing the full ASMG model to variants using only the GHA-driven matrix or only the standard learnable gating matrix would be essential to quantify the benefits of the proposed hybrid approach."}, "questions": {"value": "In addition to the points raised above, I have the following questions for the authors:\n\n- While Section 3.2.3 mentions that $\\mathbf{R}$ is a fixed random matrix for the first synthetic task, it is not explicitly stated whether $\\mathbf{R}$ is fixed or learnable in all experiments. Could the authors clarify this? Additionally, have you investigated whether a learnable $\\mathbf{R}$ would be more beneficial than a fixed one?\n\n- How does the proposed mechanism ensure balanced expert utilization? This is a critical factor in MoE models to prevent expert under-utilization or collapse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CPKoc1KVQu", "forum": "xsqiDQjSvV", "replyto": "xsqiDQjSvV", "signatures": ["ICLR.cc/2026/Conference/Submission8565/Reviewer_H4c4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8565/Reviewer_H4c4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572257752, "cdate": 1761572257752, "tmdate": 1762920418602, "mdate": 1762920418602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Focusing on specialized MoE routing, this paper proposes a novel interpolation method via iterative GHA. It shows good performance in various tasks and great OOD adaptation. The research problem is significant in MoE community, but there are no experiments on LLMs, which weakens the contributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The research question of specialized gating is critical to MoE models.\n- Although the iterative update costs additional computations, it brings test-time adaptation for OOD settings.\n- The overall performance is good and brings new ideas to the MoE field."}, "weaknesses": {"value": "- It’s a pity that this paper only conduct experiments on encoder-only models (e.g. BERT). It would be much more significant if the authors could conduct experiments on small-sized MoE models for language modeling.\n    - Or, if it is impossible for you to train a model from scratch, could you convert a well-trained MoE model to GHA gating?\n    - For example, OLMoE-1B-7B is a good start, and it would be better to extend to DeepSeek-V2-Lite, and Qwen3-30B-A3B (2~10B tokens would converge if only routers are trained). I understand the GHA would be compatible with current LLMs.\n    - We (or at least me) really don’t care about finetuning-style GLUE at all in such an LLM era.\n- The computational analysis may be biased. Although the most computational cost lies in the expert forward pass, GHA ( O(B((m+2)Kd)) + O(K^2d) ) is greater than the vanilla routing ( O(BKd) ). And your baseline should be vanilla MoE instead of DynMoE and cosine gate in Table 3.\n- Algorithm 1 should be placed in the main content. The whole bunch of texts in section 3.1 really do not help readers understanding the whole process."}, "questions": {"value": "- line 53: develope → develop\n- line 101~104, \\eta in the equation is not properly defined.\n- What if the GHA is not utilized during inference? (i.e. set \\sigma(\\alpha) to 1.0 in evaluating GLUE benchmarks)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xNXyhiqnyG", "forum": "xsqiDQjSvV", "replyto": "xsqiDQjSvV", "signatures": ["ICLR.cc/2026/Conference/Submission8565/Reviewer_tzUN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8565/Reviewer_tzUN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970734846, "cdate": 1761970734846, "tmdate": 1762920418206, "mdate": 1762920418206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel gating mechanism for mixture-of-experts (MoE) layer which interpolates standard and principal component-aware gating mechanisms.  Generalized Hebbian Algorithm is used to approximate top-K PCs of data and allows online updates during the inference time as well. Through synthetic and real data benchmarks, they show that the proposed method can be more reliable compared to standard baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clarity:** the method and motivation behind the method is quite clearly presented.\n2. **Originality:** data structure-aware routing using GHA is an interesting touch to routing in MoE which also enables test-time adaptation at an acceptable computational cost.\n3. **Significance:** Although the OOD performance improvement does not seem very substantial, I believe it is a right step forward in that robustness direction in terms of methodology."}, "weaknesses": {"value": "1. **Motivation behind $R$:** The authors introduce a learnable mixing parameter $R \\in \\mathbb{R}^{K \\times K}$. I think it is not sufficiently well-motivated AND/OR the explanation is a bit misleading. \n- (i) Why don't we want direct alignment scores with PCs and rely on the naive gating term in terms of task specific alignment? \n- (ii) Line 156 *\"This creates a latent gating basis that spans the same routing subspace\"*... this claim may not be true in general since $span(RV) \\subseteq span(V)$ where equality holds iff $R$ is full-rank (which doesn't seem to be enforced). Would enforcing full-rankness of $R$ improve performance then?\n2. **Analyzing the effect of test-time GHA:** As mentioned in Strengths section above, I think the test-time adaptation of $V$ is an attractive approach to OOD. However, I believe the OOD performance may not always prefer adapting $V$ to the test input over using a fixed $V$ from the pretraining stage. There are two ways to make this claim more convincing:\n- (i) by including both train-time and test-time GHA versions in Figure 8. This is because the improvement in Figure 7 is not quite conclusive on its own since the improvement is modest and only average performance is reported.\n- (ii) by testing on robustness benchmarks which has a *corruption strength* parameter such as ImageNet-C [1] and showing that test-time GHA performance degrades at a slower rate as the corruption severity increases.\n\n___\n\nOverall, I believe the paper proposes an interesting design to structure-aware and adaptive MoE gating mechanism with promising empirical results. Therefore, I am open to increase my score upon satisfactory responses to the concerns raised in the Weaknesses section.\n\n___\n\n### References\n\n1. Dan Hendrycks, Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. ICLR 2019"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xK2YvJ8utb", "forum": "xsqiDQjSvV", "replyto": "xsqiDQjSvV", "signatures": ["ICLR.cc/2026/Conference/Submission8565/Reviewer_ieFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8565/Reviewer_ieFN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976232325, "cdate": 1761976232325, "tmdate": 1762920417880, "mdate": 1762920417880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel routing method in MoE models whereby the standard routing matrix of gating vectors is replaced with a set of principal basis vectors derived from the generalized Hebbian algorithm (GHA). This construction allows the router to better capture the true structure of the input distribution by better aligned with the leading principal components of the data, obtaining a data-structure-aware router which enhances routing assignments and expert specialziation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of the GHA to capture the structure of the distribution and enrich the router with data-aware dynamics is intuitive, novel, and highly interesting.\n\n2. The notation and method is excellently presented and discussed."}, "weaknesses": {"value": "**Limited experimental validation**. To properly validate the efficacy of the proposed method, the authors should dedicate more of the paper to real experiments with real data. The authors only present two benchmarks, GLUE and DomainBed, and a single backbone for each task. Furthermore, despite proposing a novel router, the authors only mention one alternate routing baseline, which is the cosine router. Additionally, use of an MoE-version of BERT is quite far, in my view, from current contemporary and frontier MoE models. To better validate the empirical benefits of the authors' method, I would strongly recommend using widespread, frontier backbones such as OLMoE [1] or DeepSeekMoE [2], or even older variants such as Switch  Transformer [3], and then validating ASMG against vanilla routers and a selection of alternative baseline routing methods such as expert-choice [4] and stable MoE [5],  to name a few. If the authors are compute constrained, all of the mentioned models are available in small sizes. As it stands, however, it is difficult to be properly assess the performance of ASMG given the limited baselines, backbones, and tasks. \n\n**Empirical benefit is highly marginal** For what real experiments we do have, the results  seem to display extremely limited performance gains. For example, we see just 0.2% gain in the OOD setting and just 0.1% relative to GMoE. My concern is then that much of the reported gains are potentially not statistically significant. \n\n[1] OLMoE: Open Mixture-of-Experts Language Models (Muennighoff et al, 2024)\n\n[2] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models (Dai et al, 2024)\n\n[3] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Fedus et al, 2021)\n\n[4] Mixture-of-Experts with Expert Choice Routing (Zhou et al, 2022)\n\n[5]  StableMoE: Stable Routing Strategy for Mixture of Experts (Dai et al, 2022)"}, "questions": {"value": "1. I'd suggest the authors reduce the emphasis on synthetic experiments and focus more on assessing their method on real tasks with frontier models and baselines. The synthetic experiments are highly comprehensive and serve as an interesting case study and motivator, but, in my view, do not need to take up such a significant portion of the paper, especially if that comes at the expense of real experiments, which are more helpful for demonstrating the true performance of the method.\n\n2. What are the consequences on load balance? Intuitively, if we're aligning the gating vectors with the principal components of the data distribution, I would be concerned that whichever experts are most aligned with the leading components will then be assigned the majority of the tokens, thereby necessarily introducing quite steep load imbalance. Is there reason why conceptually this won't happen, or some empirical results on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LB69IGPKPb", "forum": "xsqiDQjSvV", "replyto": "xsqiDQjSvV", "signatures": ["ICLR.cc/2026/Conference/Submission8565/Reviewer_r2nH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8565/Reviewer_r2nH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984193230, "cdate": 1761984193230, "tmdate": 1762920417544, "mdate": 1762920417544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}