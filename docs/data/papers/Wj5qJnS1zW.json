{"id": "Wj5qJnS1zW", "number": 8344, "cdate": 1758079251221, "mdate": 1759897790540, "content": {"title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions", "abstract": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, and this dependence limits their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario that involves unsupervised learning of cloth dynamics from sparse multi-view visual observations. \nWe further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. To enable unsupervised learning of cloth dynamics, we develop a three-stage training framework for CloDS. Moreover, to address the challenges posed by large non-linear deformations and severe self-occlusions in CDG, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting. It jointly considers the absolute and relative position of Gaussian components. \nComprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://anonymous.4open.science/r/CloDS_ICLR/. Visualization results are available at https://anonymous.4open.science/r/CloDS_video_ICLR/.", "tldr": "", "keywords": ["Neural Dynamic Simulation; Visual Dynamics Grounding; Unsupervised Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12b0c48a9321fe21c827ef7adbfac356a0516844.pdf", "supplementary_material": "/attachment/93b90a081c6f4a1cbd806a964abaf3e40999cbc7.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Cloth Dynamics Splatting (CloDS), an unsupervised, visual-only framework for Cloth Dynamics Grounding, which aims to learn cloth dynamics from multi-view videos under unknown conditions without direct physical supervision. The core strength lies in its ability to bridge the gap between 2D visual observations and 3D physical representations for highly deformable materials, a significant challenge in the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly defines a new and challenging problem, Cloth Dynamics Grounding, which focuses on unsupervised learning of cloth dynamics solely from visual data.\n\n- The introduction of Spatial Mapping Gaussian Splatting, a mesh-based Gaussian splatting module, provides a differentiable mapping between 2D pixel space and 3D geometry. The proposed dual-position opacity modulation in SMGS is a clever solution to address severe self-occlusions and large non-linear deformations inherent to cloth dynamics. For the mesh-based gaussian splatting, there are relative work that should be cited.\n\n[1]SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\n\n[2]real-time large-scale deformation of gaussian splatting\n\n[3]VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality\n\n[4]Recent Advances in 3D Gaussian Splatting\n\n- CloDS achieves performance close to fully mesh-supervised methods on the CDG task, demonstrating effective unsupervised dynamics learning."}, "weaknesses": {"value": "- The method assumes an initial mesh state ($M_1$) is available to build the initial Gaussian component representation. Although robustness to initial mesh errors is analyzed, I still suggest that some visual results should be prepared and presented to incorporate the results reported in FigureS.2.\n\n- Performance degrades under complex lighting conditions due to temporal inconsistency caused by shadows and illumination, suggesting the current approach is sensitive to visual changes beyond pure geometry and dynamics.\n\n- How is the initial mesh $M_1$ \"extracted from the initial frame $I_1^{1:N}$ via 2D Gaussian Splatting\"? Is this step fully unsupervised, or does it rely on any pre-trained model, shape priors, or a fixed template mesh? A brief explanation of how $M_1$ is obtained would clarify the unsupervised visual-only premise.\n\n- The comparison to video prediction models is strong, but since a key challenge is 3D-aware modeling, a comparison to other geometry-aware, unsupervised methods (e.g., scene flow-based approaches or other particle-based visual grounding methods like NeuroFluid, adapted for cloth) would further solidify the value of the DVC and SMGS approach for this task.\n\n- DeepFashion3D is also an impressive cloth dataset, some evaluations on the dataset are more encouraged."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NWSbSixHQD", "forum": "Wj5qJnS1zW", "replyto": "Wj5qJnS1zW", "signatures": ["ICLR.cc/2026/Conference/Submission8344/Reviewer_FVhY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8344/Reviewer_FVhY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710890168, "cdate": 1761710890168, "tmdate": 1762920261423, "mdate": 1762920261423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper learns cloth dynamics from video. The approach first reconstructs a mesh from images via Gaussian splatting, then applies a mesh-based neural simulator to model dynamics in the mesh domain. Experiments show performance comparable to or better than selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The method learns cloth dynamics directly from video.\n* The method achieves performance comparable to approaches trained on ground-truth mesh data."}, "weaknesses": {"value": "1. Clarity and consistency. The writing is unnecessarily complex. If I understand correctly, a simple and clear description would be: learn dynamics directly from videos by first performing video-to-geometry grounding, then training a dynamics model on the grounded meshes. Also, there appears to be a typo/inconsistency: Equations (7) and (9) for geometry should take the same input parameters; please verify and correct.\n2. Related work coverage (missing citations). Given the focus on data-driven, mesh-based cloth simulation, the Related Work should include additional existing work (e.g., [1,2,3]). In particular, [3] also learns cloth dynamics from multi-view videos and addresses more complex scenarios (richer appearance, human body interactions). Please discuss [3] in more detail and, if feasible, compare against the pipeline in [3].\n3. Dataset scope and realism. The current dataset appears limited (a single cloth and ~120 videos of trajectories). Compared to [3], this setting may be simplistic. Please either expand the number/diversity of training and test videos or report results on established real-world datasets (e.g., those used in [3]) to demonstrate robustness and generality.\n4. Experimental protocol and reporting. Since results are reported over 20 trajectories, include mean $\\pm$ std across trajectories to reflect variability. Additionally, please report inference time (e.g., FPS/latency on a specified GPU) to quantify simulator efficiency.\n\n\n[1]. Santesteban, et al. Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On. CVPR 2021\n[2]. Shao, et al. Towards Multi-Layered 3D Garments Animation. ICCV 2023.\n[3]. Rong, et al. Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video. 3DV 2025."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3SvuLwfL3m", "forum": "Wj5qJnS1zW", "replyto": "Wj5qJnS1zW", "signatures": ["ICLR.cc/2026/Conference/Submission8344/Reviewer_kW74"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8344/Reviewer_kW74"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957627199, "cdate": 1761957627199, "tmdate": 1762920261058, "mdate": 1762920261058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors in this paper presents a method for cloth dynamics grounding. Cloth dynamics are learned from visual observations under unknown conditions without physical supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The use of  Spatial Mapping Gaussian Splatting to establish a mapping between the 2D pixel space and 3D space is interesting. SMGS handles large deformations and severe self-occlusion by using both relative and absolute positions of the Gaussian components. This design ensures an accurate mapping between the 2D and 3D spaces during rendering."}, "weaknesses": {"value": "- The visual results are shown under wind force, it would have been interesting to see cloth dynamics under various type snd source of forces e.g. objects colliding with cloth. How to model them inside the current framework.\n- A detailed analysis on cloth-cloth collision, cloth-object collision is missing.\n- Do add following relevant references under neural garment simulator GarSim: Particle Based Neural Garment Simulator WACV 2023 and GenSim: Unsupervised Generic Garment Simulator CVPR 2023"}, "questions": {"value": "refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TsGVY9xq9Q", "forum": "Wj5qJnS1zW", "replyto": "Wj5qJnS1zW", "signatures": ["ICLR.cc/2026/Conference/Submission8344/Reviewer_bT4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8344/Reviewer_bT4z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969385131, "cdate": 1761969385131, "tmdate": 1762920260542, "mdate": 1762920260542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}