{"id": "grIvSXVJ65", "number": 16524, "cdate": 1758265556459, "mdate": 1759897235246, "content": {"title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "abstract": "Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between the data generator LLM and the judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge.", "tldr": "", "keywords": ["LLM-as-a-judge", "Preference Leakage", "Data Contamination"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/176a23214ac91d1086a8bb9ef6b047f201cb014f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes preference leakage, a contamination problem in LLM-as-a-judge. The paper identifies three common types of relatedness between data generator LLM and the judge LLM, and confirms that preference leakage causes LLM judge to prefer related student models via comprehensive experiments. In addition, LLM judges are not good at identifying responses from student models, indicating that preference leakage is hard to detect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a novel concept of contamination in the domain of LLM-as-a-judge, and identifies the preference bias caused by such contamination. The paper conducts sufficient experiments to support its claim, raising new questions to LLM-as-a-judge research community on how to prevent or identify preference leakage."}, "weaknesses": {"value": "The paper does not study in detail how to detect preference leakage, or how to mitigate preference leakage. Although they provide preliminary analyses in Section 5.7, the methods explored are general and not designed specifically for preference leakage mitigation. Considering that preference leakage is a novel concept, these weaknesses are acceptable."}, "questions": {"value": "Is there any effective method to detect preference leakage, especially the inheritance relationship? For instance, would it help to measure the correlation coefficient of the last hidden state between judge and student on the same prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8KxlMvQFeq", "forum": "grIvSXVJ65", "replyto": "grIvSXVJ65", "signatures": ["ICLR.cc/2026/Conference/Submission16524/Reviewer_fJqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16524/Reviewer_fJqb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683815391, "cdate": 1761683815391, "tmdate": 1762926612601, "mdate": 1762926612601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates preference leakage, a newly defined contamination phenomenon in LLM-as-a-Judge frameworks, where bias arises when data-generation and evaluation models are related (same model, inheritance, or same model family). Through large-scale experiments across GPT-4o, Gemini, and LLaMA-3 series, the authors show that related generator–judge pairs inflate evaluation scores, compromising benchmark reliability. They design a Preference Leakage Score (PLS) and analyze the issue under multiple conditions (data mixing, model size, learning method, real-world leaderboards, etc.), concluding that preference leakage is widespread and subtle compared to data leakage or egocentric bias."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1 Novel and important concept: The notion of preference leakage extends beyond existing bias categories and raises an underexplored but crucial reliability issue in LLM evaluation.\n\n2 Systematic empirical validation: Comprehensive experiments using diverse generator-judge relations, datasets (Arena-Hard, AlpacaEval 2.0), and mitigation trials.\n\n3 Strong methodological framework: Clear definitions of “relatedness” and formalization of leakage conditions make this paper theoretically grounded.\n\n4 Practical relevance: Direct implications for the credibility of public leaderboards and synthetic-data pipelines. It will inspire industry focus on this area of preference leakage. \n\n5: The Preference Leakage Score (PLS)  is innovative and worth to further explore."}, "weaknesses": {"value": "1 The discussion of features embedded in student models (Sec 5.5) is promising but under-analyzed; deeper probing or visualization (e.g., stylistic feature attribution) would enrich understanding.\n\n2 Some family coverage is narrow—adding DeepSeek and Grok series could strengthen generality and industry relevance.\n\n3 If “preference” is positioned as a form of affective bias, connections to affective-analysis or sentiment-evaluation benchmarks would contextualize it better.\n\n4 While mitigation methods (Sec 5.7) are valuable, quantitative comparisons among them are brief; more ablation or human-alignment results would improve credibility."}, "questions": {"value": "1 In Section 3.3, how do the authors define “same model family” when architectures diverge but pre-training corpora overlap (e.g., Mistral vs Gemma)?\n\n2 Can the authors provide significance testing or confidence intervals for the Preference Leakage Score (PLS) results in Table 1 and Figure 2?\n\n3 In the learning method analysis (Section 5.3), how do the authors distinguish the leakage reduction effects of DPO from general overfitting control—could this be further verified through ablation?\n\n4 What specific linguistic or structural features appear to be “embedded” in student models through synthetic data (Section 5.5)? Are they stylistic, lexical, or formatting-based?\n\n5 Could contextual calibration (Section 5.7) be combined with adversarial or paraphrased prompting to further mitigate preference leakage?\n\n6 Given that preference leakage may relate to model “affective preferences,” have the authors considered testing the phenomenon on affective or sentiment-analysis benchmarks?\n\n7 To what extent might cross-family transfer (e.g., DeepSeek ↔ Grok ↔ Gemini) reveal broader generalization or hidden correlations beyond single-family leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dLDgsybjDL", "forum": "grIvSXVJ65", "replyto": "grIvSXVJ65", "signatures": ["ICLR.cc/2026/Conference/Submission16524/Reviewer_HBz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16524/Reviewer_HBz8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686406846, "cdate": 1761686406846, "tmdate": 1762926612002, "mdate": 1762926612002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on \"preference leakage,\" a contamination issue in the LLM-as-a-Judge paradigm. It occurs when related data-generator LLMs (M_G) and evaluator LLMs (M_J) (same model, inheritance, same family) lead M_J to favor student models (M_S) trained on M_G’s synthetic data, inflating scores.\nUsing 3 generation/evaluation LLMs (GPT-4o, Gemini-1.5, LLaMA-3.3), 2 student models (Mistral-7B, Qwen-2.5-14B), and 2 benchmarks (Arena-Hard, AlpacaEval 2.0), the authors propose Preference Leakage Score (PLS) to quantify bias. Experiments confirm widespread leakage,worse in smaller M_S, subjective tasks, and SFT. They also analyze factors like data mixing and test mitigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper fills a gap in prior research on LLM-as-a-Judge biases (e.g., egocentric bias) by focusing on the subtle, synthetic data-mediated bias between related models, rather than simpler self-favoritism.\n\n* The paper maintains good clarity in structure and expression. It logically organizes content from problem definition to experimental design, results, and mechanism analysis."}, "weaknesses": {"value": "* Mechanistic Analysis Insufficiency: While the paper attributes leakage to \"spurious features (style/format)\" inherited by student models, it does not empirically validate these features.  It could use feature attribution methods (e.g., SHAP, LIME) to identify which specific stylistic/formatting elements (e.g., sentence structure, terminology) drive M_J’s bias, or conduct ablation studies (e.g., paraphrasing synthetic data to remove style) to test if leakage diminishes.\n\n* Real-World Impact Evidence Weakness: The claim that preference leakage impacts leaderboards relies on a single case study (AlpacaEval 2.0) with limited model coverage (only Vicuna/GPT-4-related models). It lacks: 1) analysis of other major leaderboards (Chatbot Arena, MT-BENCH); 2) longitudinal data (e.g., whether model rankings shift when using unrelated M_J); 3) human validation (e.g., comparing biased LLM rankings to human rankings for leaked vs. non-leaked model pairs)."}, "questions": {"value": "Regarding quantitative indicators：\n1. The formula defaults to a 50% fair win-rate for evaluators between two student models without leakage. However, inherent quality differences (architecture, training data) may cause deviations (a superior model naturally has a 60% win-rate). Lack of baseline calibration (e.g., human evaluations as the true benchmark) leads to misjudgment, attributing inherent quality gaps to preference leakage.\n2. The formula assumes unrelated evaluators (J_unrelated) are unbiased. In reality, all LLMs may have common biases (length bias, style preference) that skew judgments. Failure to exclude these biases undermines the accuracy of the \"normal preference\" baseline in PLS calculation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X0HhMTtaJk", "forum": "grIvSXVJ65", "replyto": "grIvSXVJ65", "signatures": ["ICLR.cc/2026/Conference/Submission16524/Reviewer_nucm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16524/Reviewer_nucm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895850087, "cdate": 1761895850087, "tmdate": 1762926611436, "mdate": 1762926611436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and defines \"preference leakage\", a novel and subtle contamination problem in the LLM-as-a-judge paradigm. Preference leakage occurs when the data-generating LLM ($M_G$) and the evaluating LLM ($M_J$) are \"related\", for example, being the same model, having an inheritance relationship (e.g., fine-tuning), or belonging to the same model family (e.g., GPT-3.5 and GPT-4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength is the paper's originality. The formalization of \"preference leakage\" as a distinct contamination vector, separate from both traditional data leakage and simple egocentric bias, is a novel and important conceptual contribution. Also, the empirical work is high-quality. The authors conduct a thorough and \"full-factorial\" investigation across multiple generators, students, and benchmarks . The further analyses (Section 5) are comprehensive and anticipate many of the reader's questions, such as the impact of data mixing ratios , relatedness types , and learning methods (SFT vs. DPO vs. ICL) ."}, "weaknesses": {"value": "1. The paper's main weakness is the preliminary and somewhat disconnected nature of the mitigation analysis (Section 5.7). While the paper excels at diagnosing the problem, the treatment section feels like an add-on. The mitigation experiments use a different setup (new datasets like PPE/MTBench, new \"Error Bias\" metric) than the main experiments (Arena-Hard/AlpacaEval 42, $PLS$ metric). This makes it hard to connect the findings. For example, how does the best mitigation (Contextual Calibration) affect the $PLS$ score on the main paper's task? This is not answered.\n\n2. The paper explicitly defers a full solution: \"We leave the exploration of detection, mitigation and calibration methods of preference leakage for future works\". While this is fair, it does make the paper feel somewhat incomplete. The community is now aware of a major problem but is not given a validated, well-integrated solution within this same work."}, "questions": {"value": "1. Could you apply your most promising mitigation method, Contextual Calibration, to your main experimental setup from Section 4? Specifically, how much does this method reduce the $PLS$ score (e.g., in the \"Mistral-7B, GPT-4o & Gemini-1.5\" case from Table 1, which was 28.7%)? Showing a direct reduction in your primary metric would make the contribution feel much more complete.\n\n2. You cleverly show that while LLM judges cannot recognize their students, a fine-tuned BERT classifier can. This is a great finding. Have you considered using this classifier as a detection or mitigation tool itself? For example, could the classifier's confidence score for \"relatedness\" be used as a penalty term in the evaluation, or to flag high-risk comparisons for human review?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SeHBNLxNMB", "forum": "grIvSXVJ65", "replyto": "grIvSXVJ65", "signatures": ["ICLR.cc/2026/Conference/Submission16524/Reviewer_F5m9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16524/Reviewer_F5m9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929565289, "cdate": 1761929565289, "tmdate": 1762926611026, "mdate": 1762926611026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}