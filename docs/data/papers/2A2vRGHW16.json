{"id": "2A2vRGHW16", "number": 5326, "cdate": 1757900814032, "mdate": 1759897981255, "content": {"title": "Transformers as Optimal Transport: Stability, Geometry, and Gauge Symmetry", "abstract": "Self-attention is row-wise entropic optimal transport: masked softmax \nexactly solves independent OT problems on each query's support with unit \nentropic regularization (ε=1)—not an approximation, but a precise \nmathematical equivalence. This yields a compositional stability theory \nvia a global ℓ∞→ℓ₁ Lipschitz bound across heads, residuals, and LayerNorm, \nproducing a conservative drift budget and explaining representation locking \nthrough local saturation when δ(P)→0. We introduce gauge-invariant coarse \nRicci curvature with τ-dependent bounds linking temperature and key scale \nto contraction, and show depth behaves as Wasserstein gradient flow via \nan evolution variational inequality. Empirically on GPT-2 variants, \nmeasured drift sits well below theoretical budgets (tightness ratio ≈ 0.043), \nlocking occurs in ~10% of samples (TV <10⁻¹⁰), Sinkhorn W₂ concentrates \nin mid-depth, and curvature gaps tighten with larger τ or smaller key \nscale as predicted. We prove depth cannot collapse: compositions generically \nlack single-layer representations with the same key dimension. We report \nextrinsic Euclidean quantities in a declared canonical gauge. The framework \nprovides actionable design principles for temperature, key scaling, and \nearly exit while organizing attention into a coherent geometric structure.", "tldr": "We formulate transformer attention as semi-relaxed entropic OT, derive stability/curvature bounds and variational inequalities linking depth to Wasserstein flow. Gauge-invariant GPT-2 experiments validate theory.", "keywords": ["Optimal Transport", "Wasserstein Gradient Flow", "Transformer Attention", "Representation Locking", "Gauge Symmetry", "Stability Analysis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4d6921c0a2cc2d017f3343dfad64e5a1f7cfca6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a novel theoretical framework viewing masked self-attention in Transformers as exact row-wise entropic optimal transport (OT) with unit regularization ($\\epsilon=1$), rather than an approximation. This perspective enables derivations of stability bounds via Lipschitz constants, explanations for representation locking through local saturation, gauge-invariant coarse Ricci curvature for contraction properties, and an interpretation of depth as a Wasserstein gradient flow with an evolution variational inequality (EVI). The authors also identify a gauge symmetry in attention heads (extended to multi-head and RoPE) that preserves OT objectives and logits. Empirical validations on GPT-2 variants confirm the bounds, locking statistics, curvature gaps, and EVI surrogates. The work provides actionable insights for design (e.g., temperature scaling, early exit) and separates intrinsic from gauge-dependent diagnostics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel and unifying perspective**: The exact equivalence to entropic OT (Theorem 2.1) is a clean mathematical insight, distinguishing from prior analogies by showing $\\epsilon=1$ emerges structurally. This organizes disparate phenomena (stability, locking, geometry) into a coherent OT-based framework, with proofs checking out via KKT conditions and scaling identities (Appendices A-B).\n\n**Comprehensive stability analysis**: The compositional $\\ell_\\infty \\to \\ell_1$ Lipschitz bound (Proposition 3.1, Equation 6) yields a conservative drift budget that composes across components (heads, residuals, LayerNorm). Local/global saturation bounds (Theorems 3.3, Remark 3.4) quantitatively explain locking as $\\delta(P) \\to 0$, with empirical tightness (ratio $\\approx 0.043$) and statistics ($\\sim10\\%$ samples locked) aligning well. The non-collapsibility proof (Theorem D.6) rigorously shows depth adds expressivity beyond single-layer rank constraints.\n\n**Geometric contributions**: Gauge-invariant Ricci curvature (Definition 4.1) with $\\tau$-dependent bounds (Proposition 4.2) links temperature and key scales to contraction, empirically validated (gaps $<0.18$, tightening with larger $\\tau$/smaller keys). The EVI (Theorem 4.3) formalizes depth as proximal steps toward Gibbs equilibria, with controlled drift— a fresh take on Transformer depth scaling sublinearly.\n\n**Gauge symmetry**: The head-level GL(d_k) $\\times$ GL(d_v) action (Proposition 5.1), extended to multi-head permutations and RoPE commutant (Corollary 5.2, Definition 5.4), clarifies invariant vs. canonical-gauge diagnostics, promoting robust reporting.\n\\item \\textbf{Empirical rigor and actionability}: Diagnostics on GPT-2/-medium/-xl (Figures 1-4) confirm predictions with medians, IQRs, and protocols (Appendix J). Early-exit certificates (Corollary 3.5) offer practical value for inference efficiency.\n\n**Motivation**: Strongly motivated by empirical puzzles (locking, stability) in Transformers, providing mechanistic explanations and design principles (e.g., tuning $\\tau$ for curvature)."}, "weaknesses": {"value": "**Limited empirical scope**: Validations rely on GPT-2 family (up to XL, 1.5B params); testing on larger/modern models (e.g., Llama, GPT-3+) would strengthen generalizability, especially for curvature/EVI in deeper architectures.\n\n**Proof specificity**: The depth non-collapsibility (Appendix D) uses a minimal 2x3 example; while generic, extending to broader mask/value classes could bolster the claim. Some bounds (e.g., Equation 12) assume declared gauges, potentially sensitive to choices.\n\n**Computational details**: While reproducible (Appendix J), Sinkhorn W2 surrogate for EVI lacks ablation on regularization; minor, but could affect proxy accuracy.\n\n**Mathematical checks**: Core math (OT equivalence, Lipschitz, EVI derivation) appears sound based on provided proofs/appendices, but curvature baseline (Equation 11) is loose ($\\geq 0$), relying on empirical gaps for insight."}, "questions": {"value": "1) The $\\tau$-dependent curvature bound (Eq. 12) assumes $\\|k_j\\|_2 \\leq K_{\\max}$ in a declared canonical gauge. How sensitive is this bound to gauge choice, and can a gauge-invariant version be derived using only intrinsic quantities (e.g., logit differences, attention patterns)?\n\n3) Locking correlates with punctuation and sentence boundaries (App. J.9). Does this suggest token-type-specific saturation dynamics? Could you report locking frequency stratified by POS tags or syntactic depth?\n\n4) The early-exit certificate (Cor 3.5) requires argmax stability and $\\hat{\\Delta}^{(\\ell)}_{\\text{TV}}(i) \\leq \\epsilon$. How does performance degrade when exiting based only on the TV bound without the argmax guard? Any empirical results on adaptive computation efficiency?\n\n5) The non-collapsibility result (Thm D.6) shows depth cannot be simulated by a single layer with fixed $d_k$. Does this hold under value-adversarial settings or learned value projections? Can you construct a counterexample where composition \\textit{is} representable in one layer with larger $d_k$?\n\n6) The gauge symmetry restricts Euclidean norms to canonical gauges. For practical diagnostics (e.g., key norm scaling in Eq. 12), what is your recommended canonical gauge (e.g., post-RoPE, pre-projection)? Is there a standardized protocol to avoid gauge artifacts?\n\n7) The Sinkhorn $W_2$ surrogate (Fig. 4) peaks mid-depth and decays. Is this consistent with the EVI step-size $\\eta_{\\text{eff}}$ being largest early and shrinking? Can you estimate $\\eta_{\\text{eff}}$ per layer from data?\n\n8) The paper uses discrete key metric ($W_1 = \\text{TV}$) for curvature. How do results change with positional key metric (e.g., $|j-j'|$ or RoPE-induced distance)? Does curvature strengthen with positional structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2hQdJgN8sX", "forum": "2A2vRGHW16", "replyto": "2A2vRGHW16", "signatures": ["ICLR.cc/2026/Conference/Submission5326/Reviewer_E7BT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5326/Reviewer_E7BT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678272062, "cdate": 1761678272062, "tmdate": 1762918009283, "mdate": 1762918009283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies masked attention as an optimal transport problem using a mapping to an entropic optimal transport problem. \nThis viewpoint allows the authors to prove that attention is globally stable, in the sense thatsoftmax changes by at most as much as its input logits, and locally self-stabilizing: when an attention row becomes sharply peaked, it effectively “locks’’ and stops reacting to further perturbations. They also interpret attention geometrically as a contractive map with curvature controlled by temperature and key norms, and they show that multiple attention layers cannot in general be replaced by one of the same width because the log-odds representation expands its subspace with depth.\nThese results are then tested on GPT-2 in a set of experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides an analytical framework to analyze attention. While simple, this treatment is exact, and could in principle be used to derive interesting insights into attention networks. The results are not incorrect and are not in contradiction with the numerical experiments."}, "weaknesses": {"value": "The paper is really hard to read for an audience that is not intimately knowledgeable of optimal transport. In the whole paper it's not clear what is the main non-technical result the authors are trying to comunicate. The sections 3,4,5 seems extremely disjointed, and I believe they should be rewritten to show the underlying idea and not read like a sequence of technical lemmas. I think the extreme lack of clarity makes this paper basically impossible to read, and there is no way I can see it published in the current form. \nConcerting the theoretical results, the experiments seem to suggest that the bounds on stability and locking are way too conservative, and in general the experiments are not really evidence for the accuracy of the theoretical predictions."}, "questions": {"value": "1. How should one think different analytical results in sections 3, 4 and 5? Each section has its own setting and lemmas which to me appear completely unrelated.\n2. Are there any practical implications of locking?\n3. Is there some correlation between the quantities under study in this paper (curvature, drift, locking constant) and the performance of the attention network (even just experimentally)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QYAN8n65lA", "forum": "2A2vRGHW16", "replyto": "2A2vRGHW16", "signatures": ["ICLR.cc/2026/Conference/Submission5326/Reviewer_HcVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5326/Reviewer_HcVw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855875791, "cdate": 1761855875791, "tmdate": 1762918008914, "mdate": 1762918008914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to interpret self-attention from the perspective of optimal transport (OT) by using the fact that Softmax is the solution to an entropy-regularized OT problem. Additionally, the work establishes certain properties of Softmax, including the fact that it is $(\\ell_\\infty, \\ell_1)$-Lipschitz with constant 1, and studies phenomena implied by these properties, including stability of self-attention and representation locking. Moreover, the authors interpret self-attention from the viewpoint of Ricci curvature and Wasserstein gradient flow. The findings are supplied with numerical experimental results on GPT-2 models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. My analysis suggests that the proof of Proposition 3.1 (which states that Softmax is $(\\ell_\\infty, \\ell_1)$-Lipschitz with constant 1) is sound. This finding constitutes a meaningful contribution, advancing our understanding of self-attention and Softmax in general.\n2. Experiments illustrate the findings well."}, "weaknesses": {"value": "1. Some of the paper's results are either well-known or follow immediately from established facts. For example, the authors present the equivalence of attention and entropy-regularized OT as a contribution and formulate it as a theorem with a proof. This finding is, in essence, a standard result that minimizer of entropy plus linear term is softmax, which is known from basic optimization courses, see, e.g., Lemma 18 in 2017 lecture notes for \"Introduction to Optimization Theory\" course by A. Sidford. Furthermore, Proposition B.1 states that multiplying an objective function by a positive constant does not change its minimizer - a straightforward property that is presented with a proof.\n2. I am concerned about correctness of Proposition D.2, see Questions section below.\n3. In my opinion, quality of presentation should be improved. Currently, introduction merely reformulates contents of the abstract. Many concepts are used without being introduced. Main part of the paper looks like a collection of remarks with constant references to appendices, and therefore, it doesn't look like a coherent paper.\n4. A more detailed overview of literature would improve the paper."}, "questions": {"value": "1. Proposition D.2 states that $(\\ell_{\\infty}, \\ell_1)$-operator norm of Jacobian $J(z)$ of Softmax $\\mathrm{sm}(z)$ equals $\\min \\lbrace 1, 2(1-p_{max}) \\rbrace$, where $p_{max}$ is the maximal component of $\\text{sm}(z)$. However, consider the vector $z=(\\log \\frac{1}{4}, \\log \\frac{3}{4})$, which yields $P:=\\mathrm{sm}(z)=(1/4, 3/4)$. According to my calculations, the vector $v=(1, -1)$ from the unit ball in $\\ell_{\\infty}$ satisfies $\\Vert J(z) v \\Vert_1=3/4$, which is greater than $2(1-p_{max})=1/2$. Could you clarify this discrepancy?\n2. Could you please provide a more detailed literature overview? See, for example, these papers and references therein:\n- N. Yudin - Pay Attention to Attention Distribution: A New Local Lipschitz Bound for Transformers\n- Tim Large et al. - Scalable Optimization in the Modular Norm\n- H. Kim et al. - The Lipschitz Constant of Self-Attention\n- Xianbiao Qi et al. - Lipsformer: Introducing Lipschitz Continuity to Vision Transformers"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PuOdHmEcQ2", "forum": "2A2vRGHW16", "replyto": "2A2vRGHW16", "signatures": ["ICLR.cc/2026/Conference/Submission5326/Reviewer_qSn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5326/Reviewer_qSn7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932665297, "cdate": 1761932665297, "tmdate": 1762918008567, "mdate": 1762918008567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to connect the transformer architecture with the field of optimal transport, which could enable the leveraging of the many fruitful results established in that field to explain the mechanisms underlying the performance of transformer-based models.\nIn particular, by focusing on how information is transformed as it passes through the transformer layers, the authors aim to decipher the internal mechanisms of these models through the lens of optimal transport and geometric analysis.\nExperimental results using practically meaningful models, such as variants of GPT-2, are presented to support the claims."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper employs solid theoretical machinery to derive rigorous and concrete results. \nThe idea of viewing forward propagation in a transformer as a discrete Wasserstein flow is insightful and provides a coherent basis for the subsequent analyses. \nI found the theoretical guarantee that sufficiently deep architectures are necessary (Proposition D.7 and related results) particularly interesting, as it offers an additional perspective on why network depth matters beyond the classical universal-approximation argument."}, "weaknesses": {"value": "While this is not a scientific weakness, it is the most immediately noticeable aspect of the paper: it would be greatly appreciated if the authors used the Times New Roman font as recommended in the formatting guide, since the current font makes the paper appear slightly off-template and somewhat harder to read.\n\nI have mixed opinions regarding the contents of this paper. Factually, there is little to criticize; the theoretical statements appear sound and internally consistent. However, once one abstracts away the advanced terminology and formalism borrowed from optimal transport and geometry, it is not entirely clear that the paper delivers substantially new insights or results to the community. Much of the contribution reads as straightforward observations once one decides to fit transformers within an OT/geometry framework. \n\nFor example, in Section 2, where the attention mechanism is cast as an entropically regularized OT problem, the main result is a direct consequence of the well-known fact that the softmax is the closed-form solution of the entropically regularized OT (as discussed in Chapter 4 of Peyré & Cuturi (2019)). \nThe formulation of forward propagation as EVI with drift (Theorem 4.3) cannot be considered a meaningful result unless the drift term is strongly bounded, since the drift term effectively accounts for all the error that cannot be explained by free energies. \nHowever, the provided drift bound (Equations 42 and 43) is essentially a trivial one-step application of the triangle inequality. \nThe local saturation/locking results also do not seem to provide much insight, as the theorem only handles sufficiently small perturbations; it is rather intuitive than surprising that small perturbations in the inputs will preserve the overall shape of the outputs. \nThe experimental results, while they do support the claims, they also reveal that the analyses are quite loose and suggest there remains considerable room for tightening.\n\nI am not saying that such observations and recastings are not meaningful, but without clear downstream advances or surprising consequences, it is not clear to me whether it constitutes a sufficiently novel or impactful contribution for publication."}, "questions": {"value": "My concerns are mostly discussed in the **Weaknesses** section, so rather than formulating an explicit list of questions, I would prefer to hear the authors’ responses to the points raised above and continue the discussion during the rebuttal phase. \nI also acknowledge that I may have misunderstood certain statements or not fully grasped the core ideas of the work, and I am open to engaging in a constructive and clarifying dialogue during rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dMAYcbaov4", "forum": "2A2vRGHW16", "replyto": "2A2vRGHW16", "signatures": ["ICLR.cc/2026/Conference/Submission5326/Reviewer_HreL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5326/Reviewer_HreL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994436497, "cdate": 1761994436497, "tmdate": 1762918008344, "mdate": 1762918008344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proves a precise equivalence between masked self-attention and a set of row-wise entropic optimal transport (OT) problems with unit regularization ($\\varepsilon=1$). This equivalence leads to (i) a compact stability/drift budget via a global $\\ell_\\infty \\to \\ell_1$ Lipschitz bound for softmax composed across residuals, LayerNorm, and multi-head aggregation; (ii) a local saturation law that quantitatively explains “representation locking” when the top attention mass is high; (iii) a gauge-invariant coarse Ricci curvature that links temperature $\\tau$ and key scale to contraction; and (iv) an EVI-style inequality showing depth behaves like a Wasserstein proximal step up to parameter drift. Empirically on GPT-2 (small/medium/XL), measured row-wise TV drift sits well below theoretical budgets, locking occurs in $\\sim 10\\%$ of samples with $TV \\ll 10^{-10}$, curvature gaps shrink with depth and with larger $\\tau$ or smaller key norms, and a Sinkhorn-\\(W_2\\) surrogate concentrates mid-depth. The paper also proves depth cannot generally collapse to one layer at fixed key dimension and formalizes a head-level gauge symmetry (extended to RoPE) that preserves logits and attention rows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proves a precise equivalence between masked self-attention and a set of row-wise entropic optimal transport (OT) problems with unit regularization ($\\varepsilon=1$)."}, "weaknesses": {"value": "- Assumptions on network differ in Section 2 (including Theorem 2.1) and after Section 3, which is misleading as the ``exact'' equivalence between OT and attention is only valid for limited setting.\n- Presentation: I felt the manuscript is a bit hard to read. Theorems/Lemmas/Propositions/Corollaries are sequentially listed without explanations what they are. Context, motivations, and research purposes are less explained. Related works seem also superficial. For example, the authors cite Raghu et al 2017, which is not related to Transformers. I guessed the major body of manuscript is written by LLMs. Reviewers are unpaid individuals, not a paid proofreading service. Before submitting a paper, explain its content to colleagues or your advisor to ensure it is comprehensible."}, "questions": {"value": "- Does Theorem 2.1 holds in more general networks?\n- What is Appendix J.6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ntQ5N7oT5A", "forum": "2A2vRGHW16", "replyto": "2A2vRGHW16", "signatures": ["ICLR.cc/2026/Conference/Submission5326/Reviewer_JWsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5326/Reviewer_JWsH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001535638, "cdate": 1762001535638, "tmdate": 1762918008101, "mdate": 1762918008101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}