{"id": "t5Nd4HiI3E", "number": 428, "cdate": 1756739109434, "mdate": 1759898261430, "content": {"title": "From Noisy Traces to Stable Gradients: Bias--Variance Optimized Preference Optimization for Aligning Large Reasoning Models", "abstract": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.", "tldr": "", "keywords": ["Large Language Models", "Large Reasoning Models", "Preference Optimization", "Alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f55c681f4c61ed6d99768df39976ce315d2774a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work concentrates on the task of aligning large reasoning models with human preferences via RLHF/DPO. The authors argue that the gradient variance of the reasoning traces impacts the performance of conventional alignment methods, and propose BVPO to jointly consider both bias and variance. Specifically, an empty-trace gradient is computed and linearly merged with the original trace-based gradient, with MSE minimization to decide the optimal combination weights. In experiments, the proposed BVPO achieves better performance on both alignment and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1)\tThe proposed idea is simple and sound. This work is easy to follow.\n2)\tThe authors have conducted various of downstream tasks to evaluate the methods’ performance on both alignment and reasoning tasks.\n3)\tThe theoretical analysis part provides a good discussion on the MSE-based selections."}, "weaknesses": {"value": "1)\tThe straightforward linear combination of L_t and L_e outperforms SimPO and DPO consistently on both alignment and reasoning tasks, which is impressive. Is the reason for the failure of baselines really due to the high variance caused by reasoning traces? Is it possible that the improvement of BVPO is mainly brought by extra prompts and additional training signals of g_e? Why does the effectiveness continue to improve in reasoning tasks?\n2)\tThe authors should provide the exact value of \\alpha in different settings. Moreover, the effectiveness of the MSE minimization should be verified (e.g., by providing experiments with different hyperparameter \\alpha on both tasks).\n3)\tThe authors conduct the experiments merely on Qwen series. The generalization ability of BVPO on other LLM structures should be evaluated."}, "questions": {"value": "1)\tIs there any better selection besides the empty trace that could further enhance the performance of BVPO? For instance, a “shorter reasoning prompt”?\n2)\tHow to extend BVPO to RLHF besides DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nDu3aBTf9b", "forum": "t5Nd4HiI3E", "replyto": "t5Nd4HiI3E", "signatures": ["ICLR.cc/2026/Conference/Submission428/Reviewer_udkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission428/Reviewer_udkN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760954888460, "cdate": 1760954888460, "tmdate": 1762915518252, "mdate": 1762915518252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses aligning large reasoning models with human preferences, focusing on the instability caused by noisy reasoning traces. It introduces Bias–Variance Optimized Preference Optimization, combining high-variance trace-based gradients with low-variance empty-trace gradients to optimize the bias-variance trade-off. Theoretical results prove variance reduction and MSE-optimal mixing, while experiments on DeepSeek-R1 models show up to 7.8 points improvement on AlpacaEval 2 and 6.8 points on Arena-Hard, with a 4.0-point boost in math reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Offers rigorous proofs of variance reduction and MSE optimization, providing a solid foundation for BVPO's design.\n2. Demonstrates significant alignment and reasoning improvements across diverse benchmarks, validating practical impact.\n3. As a drop-in method, BVPO integrates easily with existing DPO pipelines, enhancing accessibility."}, "weaknesses": {"value": "- The BVPO method is more like a data augmentation method than a improvement in algorithm. The theoretical analysis induced optimal value of $\\alpha$, but it is never used in practical, and the authors simply chose 0.5\n- Insufficient experiments. This work only compares with two baselines SimPO and DPO on ArenaHard and AlpacaEval 2.0. Some important baselines are missing, such as mixing thinking/non-thinking preference data \n- Lack of analysis. There are limited analysis conducted, only evaluation on Math reasoning benchmarks. Some important ablation studies, such as the value of $\\alpha$, $\\beta$, were not included."}, "questions": {"value": "- On the implementation of DPO and SimPO, did you augment the empty-trace datasets or only use it for BVPO? If so, there would be an unfair comparison issue since BVPO used twice more training data than the baselines.\n- The 4096 max length is quite short for reasoning models. How do you deal with longer generations? Why the evaluation used 8192 max tokens which is not aligned with training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CCRh9l8EHY", "forum": "t5Nd4HiI3E", "replyto": "t5Nd4HiI3E", "signatures": ["ICLR.cc/2026/Conference/Submission428/Reviewer_NDzH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission428/Reviewer_NDzH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706604389, "cdate": 1761706604389, "tmdate": 1762915517970, "mdate": 1762915517970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets aligning LRMs with human-preferred final answers while marginalizing intractable reasoning traces. It spotlights gradient variance caused by sampling a single trace and proposes BVPO, which optimally mixes high-variance trace gradients $g_t$ and low-variance empty-trace gradients $g_e$ by minimizing MSE to the true marginal gradient.  Theoretical analysis shows that BVPO reduces variance and improves convergence bounds. Empirically, BVPO outperforms strong baselines like DPO and SimPO on AlpacaEval 2 and Arena-Hard, and improves reasoning performance on math benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new method for preference optimization in LRMs, and derives a closed-form optimal mixing coefficient for gradient estimation. \n2. The work is presented clearly: problem formulation, method derivation, and experiments are easy to follow.\n3. The paper shows that simple gradient re-weighting simultaneously improves both chat quality and mathematical reasoning without extra data."}, "weaknesses": {"value": "1. **Distribution shift of empty traces**  \n   Answers generated under the `<think></think>` condition come from a different prompt distribution than regular reasoning traces. The paper simply interpolates the two log-probabilities without measuring the bias induced by this shift, nor does it report accuracy or consistency of the empty-trace replies on a validation set.\n\n2. **The optimal $\\alpha^*$ relies on unknown quantities**  \n   The closed-form optimal weight $\\alpha^*$ needs $Σ_{te}, b_t, b_e$, etc. In practice these expectations are replaced by mini-batch estimates whose error is not analyzed. If the estimator variance is large, the “optimal” weight can deviate far from the theoretical value and even inject extra noise; the manuscript provides no bound or ablation on this estimation error.\n\n3. **Experiments limited to SFT-only checkpoints**  \n   All results are obtained from DeepSeek-R1-Distill models that only undergo supervised fine-tuning. Their traces are short and well-formatted, so gradient variance is relatively mild. After large-scale RL (especially RLVR), reasoning models produce much longer and more exploratory chains, amplifying noise. Whether BVPO still suppresses variance under this harder regime remains untested."}, "questions": {"value": "1. **Empty-trace distribution shift** \n   \n    Have you measured the KL divergence or any task-accuracy drop between regular and empty-trace conditions?\n\n2. **RL-checkpoints test**\n\n    Can you report the result on a fully RL-tuned model (e.g. OpenMath-Nemotron-1.5B) to show whether BVPO still works when traces are longer and noisier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Is8Rl8tl1q", "forum": "t5Nd4HiI3E", "replyto": "t5Nd4HiI3E", "signatures": ["ICLR.cc/2026/Conference/Submission428/Reviewer_SRsw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission428/Reviewer_SRsw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840033245, "cdate": 1761840033245, "tmdate": 1762915517834, "mdate": 1762915517834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of aligning Large Reasoning Models (LRMs) with human preferences. The authors identify that the standard approach of using trace-based gradients suffers from high variance due to stochastic reasoning trace sampling. They propose BVPO (Bias-Variance Optimized Preference Optimization), which combines a high-variance trace-based gradient estimator with a low-variance empty-trace estimator through convex combination. The method provides theoretical guarantees on variance reduction and MSE optimality, and demonstrates empirical improvements of up to 7.8 points on AlpacaEval 2 and 6.8 points on Arena-Hard, while also improving math reasoning performance by up to 4.0 points across six benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Problem Identification**: The paper makes a valuable contribution by clearly identifying and quantifying gradient variance as a key bottleneck in LRM alignment. The empirical analysis in Appendix B provides strong evidence with variance ratios and NLL decomposition.\n\n2. **Theoretical Rigor**: The theoretical framework is comprehensive, covering variance reduction, MSE optimality, and convergence guarantees. The proofs appear sound and build appropriately on established optimization theory.\n\n3. **Empirical Validation**: Experiments are thorough, testing on three model sizes (1.5B, 7B, 8B parameters) across both alignment benchmarks (AlpacaEval 2, Arena-Hard) and reasoning benchmarks (AIME, AMC, Minerva, OlympiadBench, MATH-500).\n\n4. **Practical Impact**: The consistent improvements over strong baselines and the surprising benefit to math reasoning suggest real practical value."}, "weaknesses": {"value": "**Dataset Cost and Efficiency**:\n   - Requires generating two separate preference datasets (D_t and D_e)\n   - No analysis of whether the same improvement could be achieved with the same total data budget using only one approach\n   - No discussion of computational overhead during training\n\n **Limited Scope**:\n   - Only tested on DeepSeek-R1 family models\n   - All models share the same base architecture (Qwen)\n   - No exploration of other LRM architectures or reasoning approaches"}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nymP66Odvd", "forum": "t5Nd4HiI3E", "replyto": "t5Nd4HiI3E", "signatures": ["ICLR.cc/2026/Conference/Submission428/Reviewer_xshe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission428/Reviewer_xshe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882751220, "cdate": 1761882751220, "tmdate": 1762915517646, "mdate": 1762915517646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}