{"id": "cthzUgBUn7", "number": 13206, "cdate": 1758215083909, "mdate": 1759897456521, "content": {"title": "Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation", "abstract": "Generating realistic and diverse human-human interactions from text is a crucial yet challenging task in computer vision, graphics, and robotics. Despite recent advances, existing methods have two key limitations. First, two-person interaction synthesis is highly complex, simultaneously requiring individual human motion quality and spatial-temporal sync between the interactants. However, due to their limited scale, the current datasets cannot effectively support learning such a complex task, restricting the model's generalizing capabilities. To address this, we propose a scalable data synthesis framework, InterCompose, which leverages the general knowledge encoded in large language models and the motion priors from strong single-person generators to synthesize high-quality two-person interactions novel to the original data distribution. Second, accurately describing the intricacies of two-person motions often requires text of comparable complexity, and modeling such texts with a single sentence-level vector inevitably causes information loss. For a finer modeling of interaction semantics, we further propose Text2Interact, which features an attention-based word-level conditioning module, improving fine-grained text-motion alignment. Meanwhile, we introduce an adaptive interaction supervision signal that dynamically weighs body parts based on the interaction context, enhancing interaction realism. We conduct extensive experiments to validate the effectiveness of our proposed data synthesis and word-level conditioning pipeline. Compared to state-of-the-art models, our approach significantly enhances motion diversity, text-motion alignment, and motion realism. The code and trained models will be released for reproducibility.", "tldr": "", "keywords": ["Human Motion Generation; Two-person Motion Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86eafc48bba18f0684a2dcb8e6f1688a15fff700.pdf", "supplementary_material": "/attachment/f0e0089664c9f81e02f8db508ca589579958eaef.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces InterCompose and Text2Interact. The former synthesizes the motion of one person through text conditioning and single-person motion priors, generating human-human interaction-text paired data to alleviate data scarcity. The latter employs word-level attention to achieve fine-grained text-motion alignment and emphasizes the reproduction of interaction effects through weighted body parts, enhancing the realism of motion generation."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper pioneers the InterCompose framework, which is capable of synthesizing a large volume of dual-human interaction data. The methodology for filtering and selecting the data is also highly rigorous.\n2. Text2Interact designs a sophisticated cross-attention mechanism that injects fine-grained textual information throughout the entire generation process, ensuring precise prompt control and correct temporal resolution.\n3. The rendering results are highly impressive. Text2Interact indeed generates high-quality interactive motions, and we look forward to the authors open-sourcing their work for the benefit of the community."}, "weaknesses": {"value": "1. Section 3.1.1 of the paper is not clearly articulated and is kind of difficult to follow. **It would be better to clarify the composition method of $x_{1}$ and $x_{2}$, as well as their role in the subsequent training of $D_{θ}$.**\n2. Section 3.1.2 describes the data filtering process. **It is recommended to include more images to reduce the reading difficulty for the audience.**"}, "questions": {"value": "1. There appears to be ambiguity in Fig. 2(a) regarding InterCompose. The diagram suggests that the motion of person A is first generated using A's prompt, and then the motion of person B is generated using the reaction prompt and A's motion. However, in Section 3.1.1, line 210, it is stated that A and B are generated separately. Does the paper intend to explain that during the data preparation stage, MoMask is used to generate the motion pair ($x_{1}$, $x_{2}$) as the ground truth? Then, during the training phase, a diffusion model $D_{θ}$ is trained, with inputs including the text prompt and the motion $x_{1}$ of one person, and the training target is $x_{2}$? It is recommended to distinguish between the data synthesis and training processes in Fig. 2(a) for better clarity.\n\n2. The data filtering process described in Section 3.1.2 also lacks intuitive visual explanations. Adding relevant illustrations in the Appendix would significantly reduce the reading difficulty for the audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yKPkZ200zz", "forum": "cthzUgBUn7", "replyto": "cthzUgBUn7", "signatures": ["ICLR.cc/2026/Conference/Submission13206/Reviewer_kXxF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13206/Reviewer_kXxF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649809929, "cdate": 1761649809929, "tmdate": 1762923898236, "mdate": 1762923898236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, it introduced a scalable synthesis-and-filtering strategy to generate high-quality interaction from LLM and single-person motion prior. Later, it designed a Text2Interact module to generate two-person interaction motion. The proposed method was validated on InterHuman dataset and presented better performance than many previous works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It shows better performance than many previous works.\n2. The visual performance is better than the baseline."}, "weaknesses": {"value": "1. The method is only validated on one dataset, the generalization ability of the proposed method is not validated.\n2. Generating the second human motion given the motion of the first person has been utilized in previous multi-person motion generation. The difference should be further discussed.\n3. In the manuscript, the relationship between InterCompose and Text2Interaction should be detailed.\n4. More recent works should be considered for comparison."}, "questions": {"value": "1. Whether the proposed method design a data synthesis method to create high-quality interaction data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VdDdDdEDOL", "forum": "cthzUgBUn7", "replyto": "cthzUgBUn7", "signatures": ["ICLR.cc/2026/Conference/Submission13206/Reviewer_yhm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13206/Reviewer_yhm1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737358584, "cdate": 1761737358584, "tmdate": 1762923897749, "mdate": 1762923897749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets limitations in two-person interaction generation: (1) limited real data and weak spatiotemporal coordination, and (2) insufficient semantic grounding from sentence-level conditioning. The authors propose:\n\n- **InterCompose**, a scalable synthesis pipeline that generates and filters synthetic text-motion pairs via two-stage quality and diversity filtering;\n- **Text2Interact**, which employs word-level cross-attention and an adaptive interaction loss emphasizing semantically important cross-human joint pairs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Originality:** The two-stage data synthesis and filtering pipeline is novel and intuitive. The word-level attention and adaptive interaction loss are well-motivated and effective.\n2. **Quality:** Achieves SOTA on InterHuman with solid ablations. The user study adds credibility to perceptual quality.\n3. **Clarity:** Writing and figures are clear, and each module is conceptually coherent.\n4. **Significance:** Addresses two key challenges—data scarcity and semantic granularity—in a well-balanced framework."}, "weaknesses": {"value": "1. **No validation of unfiltered synthetic data.**\n    The paper claims that filtering improves quality but does not report the result of fine-tuning with unfiltered synthetic data. Without this, the necessity of the two-stage filtering pipeline remains unproven.\n1. **No scaling analysis of synthetic data.**\n    Although the synthesis pipeline is described as “scalable,” the paper does not analyze how model performance changes with varying data volume or filtering ratios.\n1. **δ = 0.58 lacks justification.**\n    The threshold is only said to be “empirically chosen” in Appendix B.5, with no validation or sensitivity analysis.\n4. **Minor observation:** In Table 3, the “w.o. FT” row yields slightly higher R-Precision than “Ours.” Some explanation would clarify this trade-off between text alignment and motion realism."}, "questions": {"value": "1. How does the model perform when fine-tuned on unfiltered synthetic data?\n2. Can the authors show a performance curve (e.g., FID vs data size) to demonstrate the scalability claim?\n3. Could you explain why R-Precision slightly drops after fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mVisdy2aRx", "forum": "cthzUgBUn7", "replyto": "cthzUgBUn7", "signatures": ["ICLR.cc/2026/Conference/Submission13206/Reviewer_yqjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13206/Reviewer_yqjY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828560702, "cdate": 1761828560702, "tmdate": 1762923897416, "mdate": 1762923897416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work examines the problem of learning two-person human interaction generative models conditioned on text descriptions. The work has two main parts: creating a dataset and learning a model. The dataset aspect involves creation of a large-scale synthetic dataset of two-person human interaction by combining LLM descriptions to prompt existing one-person motion generation algorithms and an interaction model conditioned on the first motion. The interaction model is trained on motion-capture data from interhuman. The synthetic data is then used to train a two-person interaction model. The two-person interaction model uses word-level text embeddings and interleaved attention between self and partner sequences and text descriptions. Experiments show the model can provide more realistic two-person interactions that previous motion generation models for a broader range of text descriptions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The first half of the approach is an interesting way to create a broader range synthetic human motions than what is available in interaction motion capture data. LLM prompting and SOTA single person human motions can cover a fairly wide range of human actions. By learning a conditional interaction model and synthesizing reactions to the single-person generations, a wider variety of interaction scenarios can be synthesized with reasonable accuracy because the single-person motion provides a strong starting point for the reaction model.\n* The quantitative and qualitative results provide reasonable evidence that the proposed method can lead to more effective modeling of two-person interactions."}, "weaknesses": {"value": "* The main weakness of this work is that learning the generative model in the second stage of the paper does not seem to provide anything beyond the first stage. In the first stage, the work essentially defines a way to sample from the distribution of two-person interactions by making use of existing models/datasets (and learning a new conditional model from Interhuman). I am not sure what the benefit of distilling this approach into a second model is (other shifting from a conditional to joint model, which for the purposes of generation is not a major advantage in my opinion). Furthermore, shouldn't we expect the second stage model to be further from the distribution of natural motions than the synthetic data it is trained on (due to imperfect learning)? And since the training data is already synthetic, what is the purpose of learning a second synthetic distribution and not just focusing on the data generation process as a model on its own?\n* The work still relies on InterHuman as a way to learn human interactions. While the novelty provided by the single person motion can increase variety of interaction situations, at the end of the day the proposed method does not have a way to model interactions that are not quite close to the interaction distribution from InterHuman."}, "questions": {"value": "What is the purpose and advantage of the second stage model? Why not just focus on the data generation process as an interaction model in its own right? What are the metrics and user preference for the second stage model vs the data it was trained on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zzvJSkR909", "forum": "cthzUgBUn7", "replyto": "cthzUgBUn7", "signatures": ["ICLR.cc/2026/Conference/Submission13206/Reviewer_LLQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13206/Reviewer_LLQ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152243989, "cdate": 1762152243989, "tmdate": 1762923896986, "mdate": 1762923896986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}