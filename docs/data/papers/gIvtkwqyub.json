{"id": "gIvtkwqyub", "number": 5607, "cdate": 1757922578994, "mdate": 1759897965323, "content": {"title": "Formal Methods-based Evaluation of LLM Systematic Generalization", "abstract": "Recent advances in large language models(LLMs) have demonstrated impressive performance on a wide range of mathematic benchmarks.\nYet a critical challenge remains: systematic generalization, which is the ability to correctly reason about novel combinations and unseen contexts.\nWe present a formal methods-based evaluation framework called FORGE for rigorously probing the systematic generalization abilities of LLMs.\nOur approach automatically synthesizes formal benchmarks from traditional datasets, ensuring that evaluation instances are both novel and valid.\nEach formal benchmark is verified for correctness and well-posedness through formal methods.\nWe further introduce a formally grounded difficulty metric and a stepwise prompting method to enhance the rigorous evaluation.\nFinally, we perform online evaluation multiple times and generates multiple benchmarks, ensuring novel combinations and unseen contexts for every run. \nExperimental results reveal a dramatic accuracy drop in top-performing LLMs, highlighting critical weaknesses of LLMs. \nMoreover, our analysis shows that this decline persists after controlling for problem hardness and multiple randomization, indicating that our framework not only mitigates contamination but also provides a principled scale for reasoning difficulty.", "tldr": "", "keywords": ["Large Language Models", "Formal Method", "Benchmark Generation", "Evaluation Framework"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e366d3a99ba8c4c4c422eb4d28e592ffdbaf156.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The goal of this paper is to address a gap in the literature of LLM evaluation – namely, the lack of rigor in the generation of new problem instances from existing problems. When generating new problem instances, the package created by the authors (1) generates code for verifying new instances that binds the problem variables to named solver variables and (2) uses a solver to verify the correctness and uniqueness of new instance variable values. Additionally, the authors exploit various quantities to estimate problem difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The approach presented here demonstrates some rigor and concern for correctness. The 3-stage template verification process is helpful. This reviewer has seen several other approach, similar to this one, that lacked this level of rigor. The problem that the paper is attempting to solve is well motivated. A method for reliably creating templates of truly general reasoning problems (broadly defined) from existing datasets would be very useful for academic and industry researchers."}, "weaknesses": {"value": "Since GSM-Symbolic was posted on the arXiv in 2024, approaches to creating problem templates has become a bit of a cottage industry. This work has some redeeming qualities, but is a member of a particularly crowded field of benchmark-generation frameworks, as evinced by Table 7 in this paper. This one, like many others, creates problem templates from problems in existing datasets. Unlike other papers, this one creates executable problem templates. While this is a helpful step, the challenge of creating templatized problems of existing math datasets is of modest difficulty, because the problems typically involve equations and bindings of quantities to variables in the equations. What’s unsolved – here and elsewhere – is the more challenging problem of creating problem templates for other types of reasoning.\n\nThe error analysis could be deeper and wider.\n* Line 348: The authors refer to a “detailed error analysis” but do not provide the details.\n* In Section 4.4, the authors report the percentage of generated problem templates that pass each of the stages in the verification pipeline, but don’t provide detailed analysis of the cases that fail verification.\n\nIf reproducible and well documented and tested, the code package could be of some use to the broader community. There are, however, signals that the code is not quite up to standard for re-use by the broader community, leading this reviewer to be skeptical. Hard-coded strings in the code (forcing the main program to run on the math dataset) suggest that the package is not truly a framework but merely a collection of scripts. Reusable packages should come equipped with detailed instructions for package setup, which is lacking here."}, "questions": {"value": "1. When the authors write that ``[their] framework is general and has been extended to physics and chemistry’’, do they mean that the framework is sufficiently general to be applied to law or philosophy?\n1. When the authors write \"Second, our current implementation leverages only three formal tools, which limits the coverage of certain domains, particularly those requiring open-ended or higher-order reasoning. Expanding support for richer formal systems is ongoing work\". What other formal systems are the authors employing in your ongoing work?\n1. Have the authors considered providing detailed instructions for the setup and use of your package?\n1. How are the coefficients of the scalar difficulty score on line 228 tuned, if at all? How, if at all, are the coefficients employed in the data presented in Table 8?\n1. What are the correlations of the expression, reasoning, time, and space quantities of the scalar difficulty score D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9PB2MP4Bf8", "forum": "gIvtkwqyub", "replyto": "gIvtkwqyub", "signatures": ["ICLR.cc/2026/Conference/Submission5607/Reviewer_emhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5607/Reviewer_emhF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760909375465, "cdate": 1760909375465, "tmdate": 1762918156677, "mdate": 1762918156677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a formal methods-based evaluation framework designed to rigorously probe the systematic generalization abilities of large language models. FORGE addresses the limitations of static benchmarks, such as susceptibility to data contamination and a lack of solution verification, by automatically synthesizing fresh and formal benchmarks from existing datasets. It uses controlled randomization to generate new, structurally equivalent problem instances for every test run.\n\nExperimental results demonstrate a dramatic accuracy drop in top-performing LLMs when tested on FORGE's randomized and verified instances. This persistent performance decline, even after controlling for problem difficulty, highlights critical weaknesses and persistent gaps in LLMs' systematic reasoning and generalization capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A complete pipeline for transforming natural language problems into dynamic, verifiable formal artifacts. It uses a three-layered verification method (static, dynamic, and semantic checks) that ensures the correctness and well-posedness of every evaluation instance through formal solvers.\n2. A metric that quantifies problem hardness along four dimensions: expression complexity, reasoning complexity, and computational space and time requirements.\n3. The proposed stepwise prompting method decomposes complex formal problems into verifiable intermediate reasoning steps. This not only provides powerful diagnostic insights into LLM failures but also offers a structured resource for future step-by-step model training."}, "weaknesses": {"value": "* The proposed framework relies on transforming problems from traditional, existing datasets into a formal, dynamic format. And this transformation process does not guarantee 100% success rate, potentially limiting the diversity and introducing bias in the generated evaluation instances.\n\n* The conclusion of the paper is not surprising and several prior works such as GSM8k-Symbolic have already shown that LLMs struggle when evaluated on perturbed version of existing benchmarks. The novelty of the findings is limited.\n\n* The requirement for formal verification using external solvers introduces additional computational overhead compared to standard, static evaluations. This higher cost could impede large-scale or real-time model evaluation and development iteration.\n\n* The paper presentation has significant room for improvement. For example, Sections 2 and 3 have a lot of overlapping content, reporting R1-R3 is a waste of paper space while the mean and standard deviation would suffice."}, "questions": {"value": "1. Section 2 and Section 3 have a lot of overlapping content. Can the authors clarify the distinction between these two sections and consider merging them for better clarity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rg5LHx4CMl", "forum": "gIvtkwqyub", "replyto": "gIvtkwqyub", "signatures": ["ICLR.cc/2026/Conference/Submission5607/Reviewer_t2No"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5607/Reviewer_t2No"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733284611, "cdate": 1761733284611, "tmdate": 1762918156422, "mdate": 1762918156422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FORGE, a framework for evaluating LLM systematic generalization by formalizing problems into executable code, generating fresh instances through parameter randomization, and verifying correctness through formal methods. The authors test on GSM8K, MATH, Physics, and Chemistry datasets, reporting significant accuracy drops (20-40%) for SOTA models on randomized versions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper tackles two fundamental problems in LLM evaluation, data contamination and systematic generalization, with a principled approach that could have broad impact on the field.\n\nS2. The verification pipeline is well-designed, combining static compilation checks, dynamic solver execution, and semantic verification via judge LLMs. This multi-layered approach increases confidence in benchmark quality.\n\nS3. The formalization enables principled difficulty metrics based on expression complexity, reasoning steps, and computational complexity (space/time). This is a significant improvement over heuristic difficulty labels in existing benchmarks.\n\nS4. Partial demonstration of the framework beyond mathematics (physics, chemistry) shows broader applicability and addresses a limitation of prior work that focuses exclusively on mathematical reasoning.\n\nS5. Consistent performance drops across 7 diverse SOTA models provide moderately compelling evidence."}, "weaknesses": {"value": "W1. Formalization coverage and limitations are somewhat unclear:\n\n-  Are there any theoretical results on what kinds of problems are formalizable with your approach ?\n- Success rates vary widely: 81.54-97.62% (Table 5) (presumably the less 'formalized' the domain the worse it gets)\n- No analysis of systematic biases in what can/cannot be formalized\n- Limited to 3 formal systems, many reasoning types may be excluded\n- Generalizability to other domains  questionable\n\nW2. Methods section needs clearer explanation of how problems are perturbed\n\n- It is clear enough to imagine how problem variables are perturbed, but better explanation on how structure (depth, complexity etc.) is modified (if at all) is needed\n\nW3. Needs better justification for their claim that LLMs exhibit poor systematic generalization. Although this is probably true, the logical step between lower accuracy on perturbed problems -> poor systematic generalization should be made more explicit by clearly defining systematic generalization in the context of the problems they evaluate\n\nW4. Needs better analysis demonstrating clearly what aspects of systematic knowledge recomposition are failing in LLMs for the evaluation part of the contributions to be meaningful\n\n- The authors should provide a better taxonomy of the various problem attributes that are being perturbed in the generated benchmarks in order to gain insights into failure modes that are better than \"LLMs fail on novel problems\"."}, "questions": {"value": "Q1. Could the authors provide more information about how the hyperparameters α, β, γ, δ were selected in difficulty quantification?\n\nQ2. a) It is unclear how general this approach can be if it relies on problems being formalizable through a relatively simple mapping. For example, most mathematical problems of interest (e.g. IMO-level and beyond) seem to be hard to formalize as a computer program. b) As I also mentioned in weaknesses, could this approach generalize to scientific domains with less rigorous formalization and self-containment (e.g. involving pragmatics and tacit knowledge within a reasoning problem)? Most reasoning problems (even in pure mathematics) tend to lack the self-containment aspect found in GSM8K-style problems.\n\nQ3. Could the authors provide clarification on whether problem complexity is perturbed on a structural level, and if so, in what ways (branching factor, recursion depth, length, maximum stack size etc.)? Did you perform any analysis on disentangling the effect of structural differences from variable perturbations?\n\nQ4: Could the authors provide more details on what kinds of systematic issues were observed in solutions for datasets with degraded performance?\n\nQ5. Could the authors give a more explicit operational definition of what they mean by systematic generalization, and how their experiments demonstrate a lack of it in LLMs? A short paragraph giving some definitions in the main text could help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2G4ydQgHVt", "forum": "gIvtkwqyub", "replyto": "gIvtkwqyub", "signatures": ["ICLR.cc/2026/Conference/Submission5607/Reviewer_ZMXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5607/Reviewer_ZMXE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856459700, "cdate": 1761856459700, "tmdate": 1762918156191, "mdate": 1762918156191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present FORGE, a framework for evaluating LLM reasoning. The work is motivated by the problem of \"data contamination,\" where LLMs may have memorized answers to static benchmarks like GSM8K. FORGE's method is to auto-formalize problems from these benchmarks into executable code for solvers (like Z3). It then generates novel, unseen problems by randomizing parameters (e.g., numbers) within these formal \"templates.\" These new problems are verified for correctness by the solvers. The authors find that LLMs, which score highly on the original benchmarks, show a large drop in accuracy on these new, randomized versions."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Solver-Based Ground Truth: The paper's best idea is using formal solvers (Z3, cv5c) for verification. This \"Tri-Verify\" process creates a rock-solid, unambiguous ground truth for every problem, which is a major step up from benchmarks that might have errors or ambiguous answers.\n\nGood Diagnostics: The \"Stepwise Prompting\" module (Table 1) is a smart way to diagnose why a model is failing. By breaking the problem into logical sub-questions, it can pinpoint the exact step where the model's reasoning breaks down, which is far more useful than a simple pass/fail grade.\n\nSolid Experimental Controls: Using a \"Difficulty Quantification\" metric to prove their new problems aren't just \"harder\" was the right move. It effectively isolates the variable, showing the accuracy drop is due to the model's lack of generalization, not a spike in problem difficulty."}, "weaknesses": {"value": "A Limited Definition of Generalization: The paper's central weakness is its narrow definition of \"generalization.\" FORGE only tests \"template-level\" generalization which is whether a model can apply the same rule to new numbers. It doesn't test true \"systematic generalization,\" which would involve combining multiple known rules in novel ways (i.e., compositionality). The \"rules\" themselves are never changed or combined, only the parameters.\n\nClaims as Training Tool Lack Experimental Support: The authors rightly suggest FORGE could be a \"potential training resource\" by generating step-by-step verified data. However, this claim is presented without any experimental support. The paper lacks a crucial experiment: fine-tuning a model on FORGE-generated data and then testing it on a new, held-out set of instances from the same templates. This makes the \"training\" aspect an unsubstantiated claim, when it might be the framework's most valuable contribution.\n\nNeeds Discussion of Parallel Work: The authors should situate their work relative to efforts like \"Certifying Knowledge Comprehension in LLMs\" (CKC, arXiv:2402.15929). Both frameworks generate novel instances from a formal specification (FORGE uses solver code; CKC uses knowledge graphs) to avoid contamination. However, their goals and methods differ: CKC tests knowledge comprehension using rich semantic noise (distractors, shuffling), which is a stronger test of robustness in that domain. FORGE tests mathematical reasoning using parametric noise. A discussion would clarify that FORGE's unique strengths are its solver-verified ground truth, TriVerify, and its diagnostic \"Stepwise Prompting,\" which are distinct from CKC's goals and not confuse the reader about the primary contribution being a template based way of evaluating models which CKC already does."}, "questions": {"value": "tGiven the authors' suggestion that FORGE is a good training resource, would they consider adding experiments to validate this? For example, an experiment showing how model performance improves after being fine-tuned on FORGE-generated synthetic data? This would help demonstrate its value for improving model robustness on specific problem classes.\n\nFollowing that, how do the authors distinguish between this \"template mastery\" (which can be trained) and the deeper, \"compositional\" generalization (applying rules to new problem structures)? The paper defines generalization as applying a rule to new parameters. Should \"systematic generalization\" require more, such as composing multiple rules or applying rules to new problem structures? I am curious how the authors see FORGE addressing this deeper, compositional aspect of generalization, which goes beyond mastering a single, fixed template"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EPFAxcujVH", "forum": "gIvtkwqyub", "replyto": "gIvtkwqyub", "signatures": ["ICLR.cc/2026/Conference/Submission5607/Reviewer_1RLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5607/Reviewer_1RLo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978330100, "cdate": 1761978330100, "tmdate": 1762918155771, "mdate": 1762918155771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}