{"id": "2tNlroZztK", "number": 9924, "cdate": 1758149348423, "mdate": 1763603894020, "content": {"title": "HOBA: Higher-Order Block-Diagonal Attention Unrolling for Transformer", "abstract": "Transformers with 2D self-attention are powerful but computationally intensive, specifically for long sequences due to their quadratic complexity. Therefore, sparse attention methods attempt to alleviate this cost by limiting attention patterns. However, they often compromise explainability and fail to generalize well to global dependencies. Therefore, we propose Higher-Order Block-Diagonal Attention (HOBA), a novel transformer variant that models triplet interactions utilizing 3D attention tensors and block-diagonal unrolling. HOBA can capture richer patterns within and across blocks while efficiently modeling long-range dependencies without high computational cost. We use knowledge distillation with RoBERTa as the teacher to train the HOBA student model. We evaluate HOBA on five NLP tasks across eight benchmark datasets, comparing it against Full-3D (without block or cross-block), standard 2D attention, and sparse mechanisms including Longformer, BigBird, Local, and Dilated attention. We further isolate the contributions of block structure and higher-order interactions, confirming HOBA’s superiority over both dense and sparse baselines. We also demonstrate that allowing cross-block interaction yields significant accuracy gains by enhancing longrange token dependencies.", "tldr": "We propose HOBA, a 3D block-diagonal attention mechanism that allows efficient cross-block interactions by reducing training time and enhancing accuracy over 2D vanilla and sparse transformers on multiple NLP tasks.", "keywords": ["Transformers", "higher-order attention", "NLP benchmarks", "sparse attention", "cross-block interaction"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08cebeac54c8e5f6f3263263107239fb4da7c803.pdf", "supplementary_material": "/attachment/a3e3c94d5c40122806cdf1fae1da2e654abd220b.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose using a higher order attention operation which creates a 3D attention matrix instead of the usual 2D. The claim is that the third dimension provides a richer representation. Efficiency is maintained by only computing the block diagonal of the 3D attention and then later adding interactions between computed outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of 3D attention is interesting, and has not been fully explored in previous work as to my knowledge.\n\n- The authors propose a way to mitigate the extreme complexity that 3D attention poses."}, "weaknesses": {"value": "- L159: this approach allows richer semantic modeling such as resolving phrase-level sentiment or subject-modifier-object interations. --> Can you justify concretely why this is the case? \n\n- An intuitive explanation as to why 3D attention is necessary is missing. It is not clear what is theoretically gained by adding the third dimension, which cannot be learned in 2 dimensions. I would find the work much more compelling  if a concrete, demonstrable, controlled experiment demonstrated the usefulness of the third dimension.\n\n- The cross block interaction mechanism essentially brings the overall complexity back to quadratic since it must perform the sum in the RHS of equation 7 over N/b blocks. Since b is a constant, I believe it should come out to $\\mathcal{O}(N^2/b^2)$.\n\n---\n\nOverall, I would like to see more time devoted to analyzing what is gained by the third dimension, as it seems like this method could easily be applied in almost an identical way in 2 dimensions."}, "questions": {"value": "- Is the 2D in table 1 the baseline RoBERTa model or is it a 2D model which is trained via distillation?\n\n- What do you use for the function $g$ in equation 2?\n\n- How does the normalization work in the 3D attention matrix? Does it normalize over the last dimension and then renormalize the second dimension once the third dimension has collapsed due to the summation?\n\n- As a simple baseline, could you add a 2D model which does the same sort of local + cross block interaction as your 3D model does? For instance, it would have the same overlapping block layout and cross block interation, except the overlapping blocks would be in 2D."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jLoCa5JLHk", "forum": "2tNlroZztK", "replyto": "2tNlroZztK", "signatures": ["ICLR.cc/2026/Conference/Submission9924/Reviewer_HYxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9924/Reviewer_HYxD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203017873, "cdate": 1761203017873, "tmdate": 1762921380123, "mdate": 1762921380123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HOBA (Higher-Order Block-Diagonal Attention), a transformer variant that models triplet interactions through 3D attention tensors with a block-diagonal structure. The authors claim that HOBA can reduce computational complexity while capturing richer contextual dependencies than standard 2D attention. The method is evaluated on seven NLP benchmarks using knowledge distillation from RoBERTa."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall design is intuitive.\n2. The paper provides quantitative evidence for computational efficiency.\n3. The experimental comparisons cover a wide range of baselines and tasks."}, "weaknesses": {"value": "1. The paper conflates “higher-order” with “3D tensors” but does not convincingly justify why triplet modeling is inherently better than pairwise attention.\n\n2. All experiments are conducted under knowledge distillation (KD) from RoBERTa. There is no “without KD” ablation, making it unclear whether the performance gains come from the HOBA architecture itself or simply from KD’s regularization effect.\n\n3. The explanation of the HOBA framework is incomplete. Some symbols (e.g., $o_i$, $h_i$) are not clearly defined. There are also notation errors: for instance, the complexity is described as $\\mathcal{O}(n^3)$ reduced to $\\mathcal{O}(B \\cdot b^3) = \\mathcal{O}(n b^2)$, but $n$ should actually be $L$. Moreover, the stated complexity does not account for the additional cost of overlap and cross-block interaction mechanisms.\n\n4. Most experiments are limited to short input lengths, 64 tokens for 3-layer models and 128 tokens for 6-layer models, with a maximum of only 512 tokens overall. This significantly weakens the claim of scalability to long-range dependencies.\n\n5. In comparisons with sparse attention models, the authors claim that HOBA “outperforms Longformer and BigBird even without support for sequences longer than 512,” yet the paper’s motivation is to efficiently model long-range dependencies. This inconsistency raises concerns about the validity of the claimed advantages.\n\n6. Although the authors briefly mention prior work on higher-order attention in the Related Works section, they do not clearly articulate the key distinctions or advantages of HOBA relative to these earlier approaches. A more direct comparison, both conceptual and empirical, would strengthen the contribution claim."}, "questions": {"value": "1. How much of the improvement remains if HOBA is trained from scratch (without KD)?\n2. How would HOBA scale beyond 512 tokens if the goal is to model long-range dependencies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "elTBI8Ykuh", "forum": "2tNlroZztK", "replyto": "2tNlroZztK", "signatures": ["ICLR.cc/2026/Conference/Submission9924/Reviewer_bfFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9924/Reviewer_bfFD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811297200, "cdate": 1761811297200, "tmdate": 1762921379822, "mdate": 1762921379822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel attention mechanism called HOBA (Higher-Order Block-Diagonal Attention) to address the quadratic complexity issue of the standard self-attention mechanism in Transformers. HOBA introduces 3D block-diagonal attention to model interactions among token triplets, thereby reducing computational complexity from cubic to near-linear while maintaining expressiveness. The authors also incorporate a cross-block interaction mechanism to capture long-range dependencies and employ knowledge distillation (using RoBERTa as the teacher model) to stabilize training. Extensive experiments on seven NLP benchmark datasets demonstrate HOBA's advantages in accuracy, efficiency, and generalization compared to various sparse attention baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: Proposes the first scalable triplet-based attention mechanism, extending self-attention from 2D to 3D via a block-diagonal structure to manage complexity. The combination of intra-block local interactions and cross-block global interactions is distinctive among sparse attention methods.\n\nQuality: The experimental design is systematic and comprehensive, covering five NLP tasks and seven datasets, with comparisons against multiple strong baselines. Detailed ablation studies validate the impact of key components like cross-block interaction, block size, and sequence length. The theoretical analysis using Rademacher complexity adds depth.\n\nClarity: The paper is well-structured. Figures illustrating attention patterns aid in understanding HOBA's workings. The methodology section details key components like block partitioning, overlapping, and cross-block interaction.\n\nSignificance: HOBA achieves performance comparable to or better than 2D baselines on multiple tasks while offering significantly improved computational efficiency. Its modular nature allows integration into existing Transformer architectures, highlighting practical value and generalizability."}, "weaknesses": {"value": "Methodological Details Could Be More Thorough: While the formula for 3D attention is mentioned, the precise derivation of the final token representations from triplet attention, especially within a multi-head setting, could be more thoroughly elaborated. The EINSUM operations in Algorithm 1 lack detailed explanations of dimensions and computational flow, hindering reproducibility.\n\nLimitations in Experimental Setup: All experiments rely on knowledge distillation from a RoBERTa teacher (using 2D attention). This setup might constrain HOBA's expressive power, potentially preventing it from fully realizing its potential for triplet modeling. Experiments on larger models (e.g., RoBERTa-large) or longer sequences (e.g., 4K+ tokens) are lacking, making it difficult to fully assess its capability on genuine long-document tasks.\n\nTheoretical Analysis May Be Somewhat Tenuous: The argument for improved generalization via Rademacher complexity relies on the premise that \"HOBA is a subclass of Full Attention.\" In practice, the block structure and overlapping mechanism might introduce specific inductive biases not strictly encompassed by this subset relationship.\n\nBaseline Comparisons Could Be More Comprehensive: Comparisons with several recent efficient attention methods (e.g., FlashAttention, Nyströmformer, Performer), which also excel on long sequences, are missing."}, "questions": {"value": "Could you provide a more concrete example of a linguistic phenomenon or dependency that the triplet attention in HOBA can capture, which a standard 2D attention mechanism would likely miss?\n\nHave you considered or attempted to pre-train HOBA from scratch, without distillation? The results from such an experiment would be very informative for judging the architecture's intrinsic capabilities, even if the final performance is lower.\n\nThe block size and overlap are key hyperparameters. Beyond the empirical finding for a specific setup, did you develop any intuition or rule of thumb for how to set these for different sequence lengths or tasks?\n\nThe cross-block mechanism is a simple and elegant solution for global context. Was there any exploration of more direct, albeit sparse, long-range triplet interactions, and if so, how did it compare?\n\nThe efficiency gains are clear at the tested sequence lengths. Do you have projections or an analysis of how the FLOPs and memory usage of HOBA would scale against a standard transformer for sequences of more than 2048 or 4096 tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f9kxqz3VKQ", "forum": "2tNlroZztK", "replyto": "2tNlroZztK", "signatures": ["ICLR.cc/2026/Conference/Submission9924/Reviewer_3RUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9924/Reviewer_3RUW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047582012, "cdate": 1762047582012, "tmdate": 1762921379486, "mdate": 1762921379486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study reducing the quadratic attention complexity in transformers while maintaining rich modeling of dependencies. They propose HOBA (Higher Order Block diagonal Attention). \n\nThey first start by considering 3D attention which is cubic complexity but then discuss how they limit the attention to (potentially overlapping) diagonal blocks, which is more sophisticated than doing the block diagonal approach in the 2D space. \n\nThe authors train a student HOBA model using a Roberta model as a teacher. Experiments on datasets such as IMDB and Yelp indicate the  advantages of the authors' approach over Longformer, BigBird etc. Other ablations are also provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem and is clearly written. I overall like the idea."}, "weaknesses": {"value": "There are two main weaknesses in the paper:\n\n-Why don't the authors train the HOBA model from scratch? Is there something unstable about their approach that makes it difficult to learn from hard labels?\n\n-The authors need to evaluate their approach on more benchmarks, e.g. Long Range Arena that has longer context lengths than what the authors study."}, "questions": {"value": "See above. I am particularly curious as to why the authors did not train their model from scratch. \n\nMoreover, I am interested conceptually why block 3D attention is fundamentally better at capturing long range dependencies than block 2D attention and would appreciate if the authors provided some more intuition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fm9AdPfh1R", "forum": "2tNlroZztK", "replyto": "2tNlroZztK", "signatures": ["ICLR.cc/2026/Conference/Submission9924/Reviewer_gB3o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9924/Reviewer_gB3o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223041255, "cdate": 1762223041255, "tmdate": 1762921379136, "mdate": 1762921379136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Shared Update: Summary of Major Revisions by the Authors"}, "comment": {"value": "Once again, we thank all reviewers for their valuable constructive feedback, as well as for recognizing the novelty of our work. We already uploaded revised paper. Below, we summarize the common concerns and the major revisions incorporated into the updated manuscript.\n\n*Note: We have already provided detailed, point-by-point responses to each question (Weakness & Additional Questions) in every reviewer’s reply thread.*\n\n**Reviewer:  gB3o, 3RUW, bfFD**\n\n- Scratch-training results: We now include experiments showing that HOBA trains stably from scratch, with new results reported in Table 3.\n- Long-range benchmarks: We added evaluations on long-sequence settings (1024–2048 tokens); these appear in Table 5.\n\n**Reviewer: bfFD, HYxD, 3RUW**\n\n- The motivation for why triplet modeling is beneficial, or how 3D gives richer semantic modeling or linguistic phenomena that 2D attention may miss, is now explained more clearly in the last paragraph of the Introduction section I.\n- All notation issues are fixed, and the complexity contributions of overlap and cross-block cost are now detailed in Appendix A.6.\n- A conceptual comparison with related works is included in Appendix A.7.\n\n**Reviewer: 3RUW**\n\n- The precise derivation of the final token representations and the einsum operations are now clearly explained in Appendix 1.3.\n- The rationale for excluding FlashAttention and Linformer is already stated explicitly in the main text (lines 381–385).\n\n**Reviewer: HYxD**\n\n- A new ablation comparing a 2D model with the same local + cross-block structure as HOBA has been added as Table 7."}}, "id": "9N2m73xT5k", "forum": "2tNlroZztK", "replyto": "2tNlroZztK", "signatures": ["ICLR.cc/2026/Conference/Submission9924/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9924/Authors"], "number": 20, "invitations": ["ICLR.cc/2026/Conference/Submission9924/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763604534349, "cdate": 1763604534349, "tmdate": 1763604534349, "mdate": 1763604534349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}