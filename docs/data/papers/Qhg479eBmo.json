{"id": "Qhg479eBmo", "number": 6338, "cdate": 1757970006298, "mdate": 1759897921180, "content": {"title": "Maximizing Confidence Alone Improves Reasoning", "abstract": "Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen, Mistral, and Llama families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is unavailable.", "tldr": "We present an unsupervised reinforcement learning method that improves LLM reasoning performance by using the model's own confidence as a reward.", "keywords": ["reinforcement learning", "reasoning", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44193357af1432e3b7ec42b1f5e396a598d87a25.pdf", "supplementary_material": "/attachment/6821b5202445edf75c6e5fb9b62ac4b3a4a08c10.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript focuses on improving the reasoning capability of LLMs without external supervision. Inspired by entropy minimization, the authors ovserved that by solely considering the token level entropy of LLMs as reward (Reinforcement Learning via Entropy Minimization, RENT), the model's reasoning capability can be largely improved. Experiments on commonly used math and STEM reasoning benchmarks are carried out to validate the proposed method. Explanatory experiments are also conducted to validate the correlation between token level entropy and correctness of reasoning traces."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of extending classic entropy minimization for RL is promising. The proposed method is intuitive, straightforward and very-easy to understand.\n- For many domains that lack of verifiable rewards, the idea of unsupervised RL can be applicable. Thus the proposed method is well motivated for me."}, "weaknesses": {"value": "#### Major concerns\n\n- Still the benchmark performance seems good, the idea of RENT is too rough and not elegant enough for the reviewer. First, entropy minimization is against the idea of entropy maximization (or in another words, entropy control) in classic RL. Thus I suspect that RENT will lead to rapidly entropy collapse during RL training without enough exploration. As a result, the performance of RENT may improve rapidly at the begining of RL and plateaus. Please correct me if I'm wrong in this point.\n- Lack of in-depth analysis or ablation study. The core motivation of RENT lies in minimizing token level entropy ignoring the various roles of different tokens. Although the authors show the correlation between correctness and entropy varies for tokens of different locations. The further ablation study is missing. Will RENT's performance  be better when we selectively minimizing the entropy of token in the tail of the whole responses?\n- Lack of experiments on long-form free-response questions. In line 118-119, the authors mentioned that majority voting is less general which cannot be applied to long-form free-response questions. I suspect that, in contrast, RENT minimizes token level entropy and thus it is not limited to close-form reasoning tasks. However, there is no such conparison in the experiments section.\n- Insufficient discussion on previous works. Since reasoning-oriented reinforcement learning is an emerging and rapidly developing field, there were many unsupervised RL works in the past six months. The authors should at least mention and discuss PFPO in ICLR'25 and EMPO in NeurIPS'25, which are released more than 5 months before ICLR'26 deadline.\n\n#### Minor concerns\n\n- RENT seems directly tuning the LLMs on test data, which is noticeably different from standard common practice. The test-time training setting is rather different from previous common practice. While TTT is reasonable in my view, I suggest to further emphasize and well discuss this point in the introduction to make it more transparent to broader readers.\n- It would be better if the authors can show more training dynamics (e.g. response length, entropy of actor) when they train models with RENT or other baselines. These metrics would be useful to judge if the training process is healthy or it is ill-conditioned."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "alrvVn36C5", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Reviewer_fPZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Reviewer_fPZT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760518947345, "cdate": 1760518947345, "tmdate": 1762918631110, "mdate": 1762918631110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RENT (Reinforcement Learning via Entropy Minimization), which uses the model’s own negative entropy as a self-supervised reward signal to improve reasoning performance without external supervision. The approach is simple and efficient, using GRPO as the optimizer. Experiments cover multiple benchmarks and models of different sizes. The results show improvements across benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple and intuitive. Using negative entropy to increase model confidence is a clear and reasonable idea.\n2. The experiments are extensive, covering many datasets and models. The results consistently show performance improvements and include comparisons with several concurrent methods, which strengthens credibility.\n3. The finding that minimizing entropy on later tokens yields the greatest benefit is insightful for understanding how confidence relates to reasoning quality."}, "weaknesses": {"value": "1. The paper shows that increasing model confidence on the evaluation set improves performance, but it does not analyze the potential side effects of overconfidence. For example, could this harm generalization? Could this method increase pass@1 while reducing pass@k, or cause performance drops on other datasets? The paper lacks experiments and case studies addressing these questions.\n2. The comparisons are limited to RL-based methods. There are no comparisons with non-training TTS methods.\n3. RENT achieves large gains on some datasets but smaller gains on others. The paper does not analyze or explain why this happens, leaving uncertainty about when the method works best."}, "questions": {"value": "1. Can you analyze how pass@1 and pass@k change during training?\n2. Can you evaluate OOD generalization to see how the model behaves on unseen data after confidence-based training?\n3. Can you include comparisons with non-training TTS methods, such as self-consistency or reranking, to better contextualize the improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PpzXkQvAYd", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Reviewer_wd8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Reviewer_wd8N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744554319, "cdate": 1761744554319, "tmdate": 1762918630753, "mdate": 1762918630753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Message to all reviewers + AC: addressing novelty concerns"}, "comment": {"value": "Dear all reviewers + AC,\n\nWe wanted to flag an important issue with the reviews.\n\n---\n\nWe would like to clarify that the arXiv version of our paper was released in May 2025, making it concurrent with every paper mentioned by reviewers Z52g, q9eP, and fPZT, who have mentioned novelty as one of their main concerns. (One exception is PFPO from 2024, but it is not fully unsupervised as it also uses pseudo-feedback from frontier LLMs on math benchmarks). We believe the existence of concurrent papers, all released between April and June 2025, should not affect the novelty of our paper, although we will add a discussion of all of them in our updated paper.\n\nWe would not like to violate double-blind reviewing so we are not encouraging reviewers to search our paper on arXiv.\n\nWe will respond in detail to the reviewers' other concerns during the rebuttal period, but we wanted to point this out first since we believe it would significantly impact our scores."}}, "id": "tcYNjjrBno", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763326458657, "cdate": 1763326458657, "tmdate": 1763326458657, "mdate": 1763326458657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reinforcement Learning via Entropy Minimization (RENT), an unsupervised reinforcement learning method designed to enhance the reasoning capabilities of LLMs without needing external rewards or ground-truth answers. It uses the model's own confidence, quantified as the negative entropy of its token distributions, as an intrinsic reward signal. This approach motivates the model to reinforce reasoning traces that yield more confident, low-entropy outputs, particularly toward the conclusion of a complex thought process. Implemented using the GRPO algorithm, RENT demonstrated notable performance improvements across diverse models and benchmarks, showing that optimizing for self-confidence alone is a meaningful and scalable way to boost reasoning quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1. The idea of minimizing entropy of token distributions is smart, though it has been explored in previous works. See Weakness 1\n- S2. The proposed approach is architecture-agnostic and label-free, it can be easily applied in real-world or open-ended scenarios where external supervision is not available\n- S3. The paper provides insightful token-level analyses, revealing that reducing entropy near the end of the reasoning process correlates most strongly with accuracy"}, "weaknesses": {"value": "- W1. Limited technical novelty and lack of baselines. First, the idea of maximizing confidence for improving reasoning capability it not new and has been explore in previous works [1,2,3] and many of those are not discussed in the paper. Second, the experiments only consider other unsupervised methods as baselines, but not standard RLVR (e.g., standard GRPO with correctness reward). Showing how close RENT comes to standard RLVR (as a fraction of the performance gap) would help position its empirical significance.\n- W2. Evaluation limitation. RENT is trained and evaluated on the same datasets (except GSM8K), raising concerns about overfitting and test contamination. Showing transfer to unseen tasks (e.g., training on MATH500, testing on others) would clarify generalization.\n- W3. Concern on effect size and statistical robustness. The reported gains are sometimes small (e.g., +0.02 absolute accuracy on MATH500). The number of samples for computing mean and std (5~10 in some cases) is limited, and there is no report of significance testing. It'd be clearer to plot the pass@k curves of different methods for a thorough comparison.\n\n\n[1]. Learning to Reason without External Rewards. arXiv:2505.19590\n\n[2]. Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization. arXiv:2504.05812\n\n[3]. The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning. arXiv:2505.15134"}, "questions": {"value": "Please address the key concerns in Weaknesses. Below are some additional questions:\n\n1. How does RENT affect calibration metrics such as ECE or Brier score? Does it systematically worsen confidence calibration despite improving accuracy?\n\n2. Could entropy minimization be combined with correctness-based rewards (e.g., as a regularizer) to yield better-calibrated and more stable training? And would combining RENT with self-consistency or majority-voting decoding further improve results?\n\n3. The experiments seem to reuse evaluation data for optimization. Could the authors include a clear “held-out” generalization test or a different domain (e.g., logical reasoning or coding) to verify transferability?\n\n4. Since entropy minimization can lead to mode collapse, did the authors observe reduced diversity or degeneracy in reasoning chains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fe5h2tZebY", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Reviewer_Z52g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Reviewer_Z52g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970515916, "cdate": 1761970515916, "tmdate": 1762918629942, "mdate": 1762918629942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RENT (Reinforcement Learning via Entropy Minimization), an unsupervised reinforcement learning framework that enhances language model reasoning without requiring external rewards or ground-truth supervision. RENT uses the model’s own confidence—quantified as the negative entropy of its output token distributions—as an intrinsic reward, encouraging the model to generate more confident and lower-entropy responses. Through experiments on diverse reasoning benchmarks—GSM8K, MATH500, AMC, AIME, and GPQA—and across multiple model families (Qwen, Mistral, and Llama), the authors demonstrate that simply minimizing entropy significantly improves reasoning accuracy. The paper also shows that confidence correlates strongly with correctness, particularly in later tokens of the reasoning chain, and that RENT outperforms alternative intrinsic reward methods such as TTRL, Intuitor, and spurious reward baselines. Overall, the study presents a compelling case that confidence-driven, label-free RL can improve reasoning capabilities in large language models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Originality: The paper is among the first to explore intrinsic rewards for reasoning, introducing the novel RENT framework that uses entropy minimization as an unsupervised reinforcement signal—removing the need for external supervision.\n\n(2) Quality: The experiments are thorough and well-executed across multiple benchmarks (GSM8K, MATH500, AMC, AIME, GPQA) and model families, with clear analyses showing that entropy (confidence) correlates strongly with reasoning accuracy and that RENT  outperforms concurrent intrinsic-reward baselines like TTRL and Intuitor."}, "weaknesses": {"value": "(1) My main concern lies in the degree of novelty. While the paper is among the first to explore intrinsic rewards for reasoning, several concurrent works (e.g., [1,2]) have proposed similar ideas, and others ([3,4]) even adopt nearly identical entropy-based formulations. This overlap raises questions about whether the contribution remains sufficiently distinct and timely for acceptance.\n\n(2) Figures 2–4 are somewhat blurry and low in resolution, making them difficult to read. Additionally, the presentation style of the tables could be improved for better readability and visual consistency.\n\n(3) In Figure 3, it would strengthen the paper to include a comparison with the baseline model, allowing readers to more clearly assess the gains achieved by RENT.\n\n[1] TTRL: Test-Time Reinforcement Learning, 2025\n\n[2] Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs, 2025\n\n[3] The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models, 2025\n\n[4] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning, 2025"}, "questions": {"value": "(1) Compared to other concurrent works using intrinsic rewards, do you have any insights why RENT can achieve better better performance as shown in Table 2?\n\n(2) What does the \"baseline\" mean in tables? you need to define it.\n\n(3) If the formulation of RENT has any difference with other entropy-based works, please highlight and let me know."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "33bMZKltsJ", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Reviewer_q9eP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Reviewer_q9eP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127575610, "cdate": 1762127575610, "tmdate": 1762918629602, "mdate": 1762918629602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose an unsupervised reinforcement learning method for improving LLM reasoning. The paper applies negative entropy as the reward for RL training and verify the effectiveness across a wide range of benchmarks and model families."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method requires no ground truth answer for training LLM with RL\n- The method is simple, clear and effective."}, "weaknesses": {"value": "- The motivation stems from that for open-ended tasks verifiable reward can be unavailable, however, all the test benchmarks in this paper have verifiable answers. It is unclear how the motivation is supported by the experiments in this paper.\n- The performance gap between standard RL and the proposed method is not provided."}, "questions": {"value": "1. Some works [1][2] have shown that RL can hurt model's diverisity and entropy, though improving pass@1, RL can hurt the model's pass@k performance. In this work, the main method is to minimize model's entropy, will this significantly hurt pass@k performance? Can the authors show some pass@k results?\n2. The GRPO objective in section 3.2 seems not the same as the original one, why is there a reference policy set {π1, π2, . . . , πK }?\n3. *We use the same dataset for training and evaluation*, does this mean you train and evaluate on MATH500 without using the remaining 12000 problems?\n4. For Figure 4, do you train the models on different selected tokens independently or simply calculate the correlation after training on all tokens?\n\n[1] Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? NeurIPS 2025\n\n[2] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning. NeurIPS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w6VP93Xq4m", "forum": "Qhg479eBmo", "replyto": "Qhg479eBmo", "signatures": ["ICLR.cc/2026/Conference/Submission6338/Reviewer_fHUc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6338/Reviewer_fHUc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762570138819, "cdate": 1762570138819, "tmdate": 1762918629080, "mdate": 1762918629080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}