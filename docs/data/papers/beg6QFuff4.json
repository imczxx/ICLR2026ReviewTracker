{"id": "beg6QFuff4", "number": 15126, "cdate": 1758248035815, "mdate": 1759897326678, "content": {"title": "VARIATIONAL QUANTUM ALGORITHMS ARE LIPSCHITZ SMOOTH", "abstract": "The successful gradient-based training of Variational Quantum Algorithms (VQAs) hinges on the $L$-smoothness of their optimization landscapes—a property that bounds curvature and ensures stable convergence. While $L$-smoothness is a common assumption for analyzing VQA optimizers, there has been a need for a more direct proof for general circuits, a tighter bound for practical guidance, and principled methods that connect landscape geometry to circuit design. We address these gaps with three core contributions. First, we provide an intuitive proof of L-smoothness and derive a new bound on the smoothness constant, $L \\le 4||M||_{2}\\sum_{k=1}^{P}||G_{k}||_{2}^{2}$, that is never looser and often strictly tighter than previously known. Second, we show that this bound reliably predicts the scaling behavior of curvature in deep circuits and identify a saturation effect that serves as a direct geometric signature of inefficient overparameterization. Third, we leverage this predictable scaling to introduce an efficient heuristic for setting near-optimal learning rates.", "tldr": "", "keywords": ["Variational Quantum Algorithms", "L-smoothness", "Optimization Landscapes"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7fc354895a88318c231f6c08ab52038d8a2a2a2.pdf", "supplementary_material": "/attachment/9cf62483f9a6091270f16dba75b86bf570ce7a13.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a quite rigorous theoretical analysis of the L-smoothness property of VQA objective functions which scales linearly on the # of layers P, providing a worst-case upper limit on curvature that holds for general circuits. The authors provide a formal proof of global L-smoothness and derive an explicit upper bound on the smoothness constant L. Furthermore, they show that for certainc classes of VQAs this bound may take a very simple form, all the way down to be proportional to the depth of the underlying circuit. This result is then connected to circuits often assumed to be relevant for practical applications, including a diagnostic for ansatz overparameterization and a heuristic for setting near-optimal learning rates. While the contributions are welcome and well-supported, the analysis is confined to an idealized, noiseless setting, which limits the direct applicability of its conclusions to contemporary NISQ hardware.\n\nThis paper maybe useful since it can help better establish, for example, learning rates. $L$, provides an upper bound on the curvature and guarantees that the landscape is not infinitely \"spiky and being able to guarantee this it is crucial for gradient-based methods because it ensures stability since if I know the maximum curvature, I can choose a learning rate small enough ($\\eta \\approx 1/L$) to guarantee that the optimization steps will not wildly overshoot a minimum.\n\nHowever, to my view, this paper does not solve any of the ever present issues of VQAs. While this paper provides a valuable formalization of L-smoothness with the potential L-informed learning rate similar to many classical ML problems, the more fundamental and unresolved problem for VQAs is the lack of a meaningful lower bound on curvature, not the upper one, a condition that manifests as  the barren plateau problem where vanishing gradients render optimization intractable regardless of the landscape's theoretical smoothness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper formalizes a foundational property for VQAs. It is true that the VQA literature frequently relies on an implicit assumption of landscape smoothness to justify the use of gradient-based optimizers. This is well supported since by construction these are smooth. Having a provable guarantee is a welcome contribution and furthermore the proof is derived from first principles by bounding the Hessian matrix elements which makes it very sound and natural. \n\n(2) The derivation of a tighter bound $L \\leq 4\\|M\\|_2 \\sum_{k=1}^P\\left\\|G_k\\right\\|_2^2$ and the detailed comparison in Appendix C demonstrates that this bound is provably never looser, and often strictly tighter, than prior results from Gu et al. (2021) and Liu et al. (2025). The bound's explicit dependence on the sum of squared generator norms, rather than the maximum norm or observable decomposition, captures the individual contributions of each gate.\n\n(3) In my view, this is a cool strength of the paper is the potential link between the saturation of landscape curvature and the saturation of ansatz expressibility (referring to Figure 1c). The proposal that the stabilization of the ratio \\tilde{L}_{\\rm max}/L_{ \\rm upper} serves as a geometric signature of inefficient overparameterization is not something I have read before and maybe it can serve as tool for ansatz design. Now, the paper establishes a geometric view of barren plateaus as follows: it shows empirically that as the number of qubits n increases, the true maximum curvature ($\\tilde{L}_{max}$) vanishes exponentially, much faster than the theoretical bound $L_{upper}$.This confirms that the entire landscape, including its \"curviest\" regions, is flattening out—a second-order signature of the barren plateas. \nThe interest finding which is illustrated in Figure 1c, comes from fixing the number of qubits and increasing the circuit depth P and the authors observe that the ratio of the true curvature to their theoretical bound ($\\tilde{L}_{max}/L_{upper}$) eventually stabilizes. This observation, which should be further tested, is key since it is known that when a circuit is underparameterized, each new parameter adds significant representative power increasing its relative curvature. However, once the circuit's expressibility saturates for a given number of qubits, adding more layers and parameters yields diminishing returns. These new parameters become redundant, and their primary effect is to deepen the circuit, pushing it further into the barren plateau regime without improving its problem solving capacity. The paper seems to propose that the plateau in the curvature ratio ( $\\tilde{L}_{\\text {max }} / L_{\\text {upper }}$ ) is the direct geometric manifestation of this inefficient overparameterization which may serve as a tool to inform the underlying hyper parameter choice in the construction of the ansatz in the first place. So, while knowing $L$ does not solve the problem of vanishing gradients, monitoring the relationship between the true curvature and the theoretical bound provides a concrete landscape-based signal which may allow a practitioner to identify the point at which adding more depth to their ansatz stops being productive and starts becoming a liability, increasing the risk of creating an untrainable, flat landscape. It would be interesting thus to see, how $L$ may inform such a circuit construction."}, "weaknesses": {"value": "(1) The entire analysis is done in an idealized noiseless setting. The authors do acknowledges this by establishing the result as a theoretical baseline. However, this is a significant limitation. The primary challenge in practical VQA optimization stems from the stochastic nature of the objective function landscape induced by shot noise and hardware errors of all shorts. An analysis of L-smoothness in a setting where these dominant, non-smoothness-inducing effects are absent provides limited guidance for optimization on actual NISQ devices. The conclusions about stable, predictable curvature scaling may not hold when the optimizer interacts with a stochastic estimator of the objective function.\n\n(2) The bound is potentially loose since the proof of Theorem 2 relies on the inequality $\\|H\\|_2 \\leq\\|B\\|_2$, where $B_{k l}=4\\|M\\|_2\\left\\|G_k\\right\\|_2\\left\\|G_l\\right\\|_2$ is an element-wise upper bound on the Hessian matrix $H$. This step can introduce a substantial gap. The paper's own empirical results as shwon in Figure 1a show that the measured maximum curvature, $\\tilde{L}_{\\text {max }}$, is often only a small fraction of the theoretical upper bound $L_{\\text {upper }}$. So while the bound correctly captures scaling, its significant looseness warrants a more detailed theoretical investigation maybe. The analysis could be strengthened by discussing the conditions under which the inequalities in the proof become equalities and what circuit physical properties (entanglement structure, parameter correlations) might govern the magnitude of this gap.\n\n(3) The empirical ground truth for maximum curvature, $\\tilde{L}_{\\text {max }}$, is estimated by taking the maximum Hessian norm over 1000 random parameter samples. While Appendix D. 2 provides a reasonable justification for the stability of this estimate, this methodology cannot guarantee that the true global maximum of $\\left\\|\\nabla^2 f(\\theta)\\right\\|_2$ has been found in general. For that problems where the global optimizer is known are useful testbeds since hiigh-dimensional landscapes may contain rare and isolated regions of extreme curvature that are unlikely to be captured by uniform random sampling. \n\n(4) The proposed heuristic is designed to set a single global learning rate. However, modern optimization heavily relies on adaptive methods like adam. So, while the existence of such a constant is proven, this framework is somewhat misaligned with the reality of modern, large-scale optimization unless we want to restrict ourselves to only talk about quantum optimization in isolation. As noted in the literature, e.g. https://arxiv.org/abs/2210.02418 for many typical problems, objective functions rarely satisfy uniform smoothness assumptions in a way that is practically useful their gradients may only be locally Lipschitz continuous, or the local curvature can vary dramatically across the parameter space. Of course, the VQA objective is usually globally L-bounded, as shown in this paper. But a global constant $L$, determined by the region of maximum curvature is excessively conservative for the majority of the landscape as far as using it for thelearning rate. Standard gradient descent with a step size derived from this global $L$ (e.g., $\\eta \\approx 1/L$) would take impractically small steps thus leading to slow convergence. This is precisely why SOTA optimizers really care to account for local geometry. The paper's proposed learning rate heuristic, while nice in principle, still provides a global rate, which does not align with modern optimization paradigms. The analysis would be significantly strengthened by contextualizing its findings within more modern frameworks, such as local or relative smoothness of the VQA objective in this sense."}, "questions": {"value": "(1) How do you expect the main results and particularly the predictable linear scaling of curvature with depth, to change in the presence of realistic shot noise and hardware noise? This is super crucial. Does the concept of L-smoothness remain a useful descriptor for the stochastic objective function that an optimizer actually interacts with?\n\n(2) Could you provide more theoretical insight into the large gap between the derived upper bound $L_{\\text {upper }}$ and the empirically observed $\\tilde{L}_{\\text {max }}$ ? Does this gap depend on properties not captured by the bound, such as the circuit's entanglement capacity or the locality of the observable?\n\n(3) The trigonometric polynomial proof route in Appendix A. 6 bounds the Fourier coefficients as $\\left|d_\\omega\\right| \\leq\\|M\\|_2$. Given that these coefficients have a specific structure ( $d_\\omega=\\left\\langle u_\\omega\\right| M\\left|v_\\omega\\right\\rangle$ ), could a more refined analysis that does not resort to this uniform worst-case bound yield a tighter overall smoothness constant? These trigonometric polynomials, note, are actually Hermitian trigonometric polynomials in $d$ complex variables and the optimization takes place over the torus $\\mathbb{T}^d$. does this not induce some \"structure\" to be exploited so as to further bound $L$? \n\n(4) Regarding the learning rate heuristic, would it be more effective to use the calibrated effective smoothness constant, $L_{\\rm  effective}$, to rescale the global learning rate of an adaptive optimizer like adam, rather than using it directly in a vanilla SGD context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GaBw94q4Ft", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Reviewer_ygvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Reviewer_ygvg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816281660, "cdate": 1761816281660, "tmdate": 1762925444555, "mdate": 1762925444555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for the time and care they have dedicated to evaluating our submission. We appreciate the thoughtful feedback and many helpful suggestions provided across the reviews. We are currently in the process of carefully addressing all comments and will provide detailed, point-by-point responses and corresponding revisions shortly.\n\nThank you again for your valuable input and for helping us improve the clarity and impact of this work."}}, "id": "s6DBm430rY", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762981348074, "cdate": 1762981348074, "tmdate": 1762981348074, "mdate": 1762981348074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores the stability of training Variational Quantum Algorithms (VQA). In the theoretical section, an explicit upper-bound of the L-smoothness parameter is derived. The numerical experiments investigate the tightness of their bound with respect to circuit depth and number of qubits and the connection of L-smoothness to expressability. In the last part, they propose a method to choose the learning rate such that stable training of the circuit is ensured, which is again numerically evaluated. The authors find that their bound is tight, up to a constant, for large enough circuit depth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and the results are presented in a clear fashion. Furthermore, there is a good connection between the numerical experiments and they represent a nice application of the theoretical results."}, "weaknesses": {"value": "My main concern are the novelty and scope of the contribution. The improvement compared to [Liu et al., 2025](https://arxiv.org/pdf/2210.06723) seems incremental and the derivation does not appear to require elaborate tools. According to my understanding, Lemma 1 is a well-known fact, see e.g. \n[Schuld et al., 2021](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.103.032430). \n\nThe theoretical section would benefit from an investigation of the theoretical dependence of the smoothness on the number of qubits. Since results like [Holmes et al., 2022](https://arxiv.org/pdf/2101.02138) suggest that the decay of the L-smoothness may be exponential in $n$, the regime in which the smoothness scales proportional to the bound may only be reached after an exponential number of gates, potentially resulting in poor choices of learning rate.\n\nFinally, overparametrization is not properly addressed. I agree with the observation that the VQE reaches an amount of parameters in the numerical experiment sufficient for exploring the Hilbert space. However, the work of [Larocca et al., 2023](https://arxiv.org/pdf/2109.11676) finds that the loss landscape resulting from sufficient overparametrization mitigates spurious local minima. This could be investigated by exploring lower-bounds of the Hessian."}, "questions": {"value": "- How does the performance of GD with fine-tuned learning rate perform compared to the algorithm in [Liu et al., 2025](https://arxiv.org/pdf/2210.06723)?\n- Is it possible give more theoretical insights about the dependence on the number of qubits? The average trace of the Hessian should correspond to the sum of the variance terms derived in [Holmes et al., 2022](https://arxiv.org/pdf/2101.02138).\n- Is it possible to give lower-bounds for the Hessian? This could have implications for global convergence of the objective, using the Polyak-Lojasiewicz condition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RDtgABLVez", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Reviewer_oVNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Reviewer_oVNx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922130733, "cdate": 1761922130733, "tmdate": 1762925444057, "mdate": 1762925444057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New Experiments, Points 1-2"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback. To ensure we address all comments, we first present major improvements addressing two core concerns (Noise & Baselines), followed by 8 Points responding to specific theoretical and methodological questions.\n\n**Note on Revision**: All key changes and new results in the revised manuscript are highlighted in red for ease of review.\n\nThe most frequent concerns were:\n\n1) The analysis is limited to an idealized, noiseless setting.\n2) The learning rate heuristic was not compared against certain baselines.\n\nWe are pleased to report that we have conducted a new set of VQE experiments that directly address both of these points. These results are now detailed in **Appendix F** of the revised paper.\n\n**Summary**:\n\n**Noise & Shot-Noise**: We re-ran the 4-qubit VQE experiment using a realistic simulation with a depolarizing channel after each single-, double-qubit gate with shot-based measurements.\n\n**New Baseline (QNG)**: We added a direct comparison against the commonly used curvature aware Quantum Natural Gradient (QNG).\n\n**Findings**: Our learning rate heuristic remains highly effective. The calibrated SGD and Adam optimizers converge significantly faster and more stably than their un-calibrated counterparts (e.g., Adam optimal-lr reached a 322% lower energy than the standard-lr during the training period). Furthermore, our heuristic makes Adam/SGD competitive with the far more expensive QNG optimizer which requires computation of the full  $P \\times P$ Fubini-Study metric at every step. Finally using the Optimal lr for Adam/SGD made the standard deviation over the 5 runs the smallest.\n\n**Clarification of the core contributions**:\nOur work addresses a critical gap in VQA research by turning established theoretical bounds into practical tools that guide optimization: revealing (i) second-order barren plateaus, (ii) linking curvature saturation to expressibility, (iii) enabling the $\\kappa(n)$ learning-rate heuristic. Unlike prior studies, which treat theory and experiment separately, we provide a unified framework that translates abstract insights into actionable strategies for faster, more stable convergence in realistic quantum systems.\n\n**1. Overview & Core Contributions (@Xmst @uhHq @Huo3 @ygvg @oVNx)**\nWe appreciate the request to clarify the main contributions. We have revised the Introduction to explicitly list the\ncore contributions as:\n\n**C1. A new, tighter global smoothness bound.**\n\nThe closed-form upper bound: \n$$L \\le 4\\Vert M \\Vert_{2} \\sum_{k=1}^{P} \\Vert G_{k} \\Vert_{2}^{2}$$\nwhich is provably never looser and often significantly tighter (up to a factor of $P$ or $\\sqrt{r}$) than prior results (Table 1, App. C).\n\n**C2. A geometric interpretation of VQA landscapes.**\n\nUsing this bound, we uncover three new geometric properties:\n\n(i) second-order barren plateau signature (curvature collapse with qubits)\n\n(ii) predictable curvature growth with depth\n\n(iii) connection between curvature saturation and expressibility / overparameterization.\n\n**C3. A practical learning-rate calibration heuristic.**\n\nLeveraging curvature-depth scaling, we derive a one-time learning rate calibration rule that significantly accelerates optimization.\n\n**C4. Empirical validation (including new noisy experiments).**\n\nOur VQE experiments, including new results added, show faster and more stable convergence for SGD/Adam, and competitive performance with the more expensive QNG optimizer.\n\nThese contributions go well beyond offering a tighter bound: they provide a geometric and practical framework for understanding and improving VQA optimization.\n\n**2. Novelty & Theoretical Contributions (@oVNx @uhHq @Huo3 @ygvg)**\n\n“Lemma 1 is known / incremental.” (**@oVNx**)\n\nWe agree that the MTP structure is known (Schuld 2021, Wierichs 2022). Our goal is not to claim novelty for the fact, but for how we use it: The MTP structure enables our new global Hessian bound. Our derivation of the bound in App A.6 uses this property. The core novelty lies in Findings 1-4:\n\n**Key theoretical findings (not in prior work such as Liu 2025)**:\n\n**Finding 1**: Second-order barren plateaus: curvature itself collapses exponentially with qubits.\n\n**Finding 2**: Predictable curvature scaling: depth increases curvature in a structured, linear fashion (Fig. 1a, 1c).\n\n**Finding 3**: Curvature saturation ↔ expressibility saturation: a new geometric marker for overparameterization (Fig. 1c).\n\n**Finding 4**: Practical heuristic: $\\kappa (n)$ uses curvature flattening to set learning rates effectively.\nThese offer a substantially deeper and more practical view of VQA landscapes than prior work focused on first-order gradients.\nWe now clarify this in the revised Introduction and Lemma 1."}}, "id": "hqZQ7FGp9F", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763675163883, "cdate": 1763675163883, "tmdate": 1763679127705, "mdate": 1763679127705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a foundational optimization question for VQAs: do their objective landscapes admit a global Lipschitz-smooth (L-smooth) constant that is tight enough to inform practice? The authors first prove that any VQA objective is a multivariate trigonometric polynomial (MTP) via an induction over gates (Lemma 1), and then derive a closed-form Hessian bound. Empirically, the paper shows the maximum curvature scales linearly with depth and exhibits a plateau that the authors use to build a calibrated learning-rate heuristic; small-scale VQE studies (1-4 qubits) indicate faster and more stable convergence under this calibration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Table 1 clearly contrasts prior L-smoothness bounds (assumptions, formulas, and tightness), and argues the new bound is never looser, with up to a factor P improvement over other comparison methods. This helps readers situate contributions precisely.\n2. Lemma 1 formalizes VQA objectives as finite Fourier series, motivating global smoothness and enabling multiple proof routes (generator-norm vs. Fourier-frequency). This is simple, general, and pedagogically valuable.\n3. Derivations and settings are documented; code is provided, with software versions and experiment details."}, "weaknesses": {"value": "1. The manuscript argues that tighter L helps gradient-based optimization but does not validate on real-world quantum datasets or hardware, and the experiments use small qubit counts; this limits the empirical case for broader utility. Consider adding larger-n simulations or a hardware study to demonstrate the robustness of the calibration heuristic and the curvature-depth scaling.\n2. The plateau and scaling claims rely on random parameter sampling (S≈1000) to estimate $L_{max}$. Although convergence of estimates is reported, an optimization-based or certified bound on estimation error would increase confidence.\n3. The learning-rate heuristic is tested with SGD/Adam on VQE only. It would be informative to compare against natural gradient / QNG or curvature-aware schedules to show the heuristic’s added value beyond simple step-size tuning.\n4. Minor clarification issue: It would be better to double-check the equation and notation in the background introduction."}, "questions": {"value": "1. You sample 1,000 parameter points to approximate the maximum curvature. What confidence guarantees (e.g., PAC-style) can you provide for the reported plateaus? Could you add a global-optimization routine (even on toy instances) to benchmark the sampler’s recall?\n2. Your Lemma 1 proof covers single-parameter gates. How do your conclusions extend to multi-parameter exponentials or parameter sharing across layers? Is the MTP structure and the bound unchanged after standard decompositions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3Obb4dNgg", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Reviewer_uhHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Reviewer_uhHq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953301381, "cdate": 1761953301381, "tmdate": 1762925443727, "mdate": 1762925443727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Points 3-5"}, "comment": {"value": "**3. Noise, NISQ Validation & New Experiments (@Xmst @uhHq @ygvg)**\n\n“Results assume an idealized, noiseless regime.” (@Xmst @uhHq $ygvg)\n\nAction taken: We agree this is important. We have now added a full suite of noisy VQE experiments in Appendix F.\n\n**New experiment summary**:\n\n* Noise model: depolarizing noise channel after every 1- and 2-qubit gate.\n* Measurements: 1000-shot sampling.\n* Systems: 4-qubit VQE.\n* Baselines: Adam, SGD (both calibrated and uncalibrated), and the Quantum Natural Gradient (QNG) optimizer.\n\n**Findings**:\n* Our LR heuristic remains robust under noise highlighting the estimated curvature remains accurate and effective in NISQ like environments.\n* Calibrated Adam/SGD converge significantly faster and more stably than uncalibrated versions.\n* Our one-time calibration yields performance competitive with QNG, which requires a per-step evaluation of the full $P \\times P$ Fubini–Study metric (Stokes 2020), far more computationally expensive.\n\nThese results directly address the reviewers’ concerns and demonstrate the heuristic’s NISQ relevance.\n\n**4. Baselines & Optimizer Comparisons (QNG, LR-Free, Schedulers) (@Xmst @uhHq @oVNx @ygvg)**\n\nOn parameter-free methods (**@Xmst**): COCOB and D-Adaptation are not yet part of the VQA optimization literature, unlike QNG and Adam. Incorporating them requires careful, domain-specific implementation and tuning; we view this as an exciting direction for future work and consider the comparison with QNG more natural given its current use. Our heuristic is compatible with dynamic schedules; it provides the optimal base learning rate ($\\eta$) from which schedules can decay.\n\nOn comparison with Liu et al. (2025) (**@oVNx**): Liu et al. focus on noise-based dynamics. Our work focuses on curvature-informed calibration. We view these as complementary approaches opposed to competing. \n\nOn Comparison with QNG (**@uhHq**): We added a direct comparison to QNG in Appendix F. Our calibrated Adam performs comparably to QNG while being orders of magnitude cheaper (QNG requires metric tensor inversion each step). This strongly validates the practical value of $\\kappa (n)$.\n\n“Modern optimization relies on adaptive methods. Do you use the calibrated effective smoothness constant to rescale the global learning rate of an adaptive optimizer like adam (**@ygvg**)”\nThis is precisely what we did and what the heuristic is designed for. $\\kappa (n)$ is not intended to replace adaptive methods but to be fully compatible with them. Its primary goal is to solve the challenge of setting their global learning rate hyperparameter, $\\eta$. We kindly refer the reviewer to our experiments in Section 5.3, Figure 10, and Table 2. These results explicitly show our heuristic being used to calibrate the learning rate for the ADAM optimizer. The noisy experiments in Appendix F show similar trends. \n\n**5. Curvature Estimation & Sampling Robustness (@uhHq @Huo3 @ygvg)**\n\n“1000 samples may miss rare curvature peaks. Provide guarantees.” (**@uhHq @ygvg**)\n\nWe appreciate this question. Our goal is not to compute the global maximum curvature, provably intractable (Bittel & Kliesch 2021), but to obtain a stable and practical estimate.\n\n**Justification (expanded in App. D.2)**:\n* Practical sufficiency: Calibration based on this sample yields faster convergence compared to smaller/larger learning rates (Sec. 5.3, App. F) indicating the curvature estimate is accurate.\n* Convergence: Fig. 5a shows $\\tilde{L}_{max}$ stabilizing well before 1000 samples; extending to 4000 samples yields negligible change.\n* Stability: Ten independent 1000-sample runs (Fig. 7b) yield mean 3.210 ± 0.050, demonstrating low variance.\n* Feasibility: Global optimization or PAC-style bounds are infeasible due to the exponential state vector (@uhHq).\n\n\nThus, our sampling-based estimate is stable, converged, and empirically validated.\n\nCost reduction (**@Huo3**): The cost on QPU is described in App B.2.3. Our heuristic is designed to overcome the computational burden of large Hessian computation. Instead of computing $L_{max}$ for the deep $P_{target}$ circuit, we compute it once for a much shallower $P_{cal}$ circuit. We use this cheap calibration to find the stable $\\kappa(n)$ ratio, which then allows us to predict the curvature for the deep circuit. As we show in Appendix F.1, this provides a quadratic computational saving while achieving a prediction error of only 6-9% (Fig 8)."}}, "id": "UbXN2A1y69", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763675758874, "cdate": 1763675758874, "tmdate": 1763678866590, "mdate": 1763678866590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proves that objectives of parametrized quantum circuits are globally L-smooth and gives an explicit upper bound on a smoothness constant. They compare against prior bounds and claim their expression is never looser and often tighter. Experiments on simulated circuits estimate the maximum Hessian norm by random sampling. It finds that curvature scales linearly with depth and observable norm, and that the ratio plateaus with depth. Th authors also propose a calibrated learning-rate heuristic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. the paper give a clear and tighter bounds on a smoothness with detailed derivations.\n2. The results are informative diagnostics for ansatz design."}, "weaknesses": {"value": "1. Global smoothness and upper bounds have been shown before; the main contribution is a tighter constant and cleaner derivation.\n2. The experiments show the ratio plateaus with depth, but there is lack of analytically analysis.\n3. Empirics are classical simulations on small widths and depths, with curvature estimated by sampling 1000 random parameter vectors."}, "questions": {"value": "1. What are matching lower bounds on the global maximum curvature for realistic VQAs?\n2. What will be the cost of curvature estimation? Can we reduce it with some estimators?\n3. Can we prove concentration of the Hessian spectral norm for broad random ansatz ensembles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IMeL0XuCef", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Reviewer_Huo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Reviewer_Huo3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971997262, "cdate": 1761971997262, "tmdate": 1762925443262, "mdate": 1762925443262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Points 6-8 and Closing Statement"}, "comment": {"value": "**6. Qubit Scaling, Barren Plateaus & the κ(n) Heuristic (@oVNx @Huo3 @ygvg)**\n\n“Provide theoretical insight into smoothness vs qubit number.” (**@oVNx**)\n\nThis is a central theme of our paper. Our empirical analysis (Fig. 1a, 1b, 5a) shows the normalized curvature $L_{max}$  / $L_{upper}$ collapses with qubits. This is a second-order barren plateau, complementing prior first-order analyses (McClean 2018; Holmes 2022) focused on vanishing first-order gradients. This finding directly motivates the need for $\\kappa (n)$: upper bounds would yield overly small learning rates. $\\kappa (n)$ provides a correction factor that captures how curvature flattens with increasing qubits.\n\n“Regime where bound is proportional to curvature may require exponential gates. Resulting in poor choices of learning rate” (**@oVNx**)\n\nOur heuristic is designed to tackle this problem. Our empirical results make the same point: curvature saturates, and the bound can become loose. Our contribution lies in:\n* identifying this saturation empirically,\n* linking it to expressibility (Fig. 1c), and\n* providing a heuristic to manage it.\n\nA full theoretical derivation of $\\kappa (n)$ dependence is excellent future work (App. F.3).\n\n“The experiments show the ratio plateaus with depth, but there is lack of analytically analysis. (**@Huo3**)”\n\nA full analytical derivation for the plateau effect (stabilization of $L_{max} / L_{upper}$) is a significant theoretical challenge. Our work provides the empirical and geometric characterization of this phenomenon. We show it occurs in lockstep with expressibility saturation (Fig 1c), providing a new, second-order geometric signature for it. As we discuss in our new Appendix F.3, a formal proof is a major undertaking for future work.\n\nBound Tightness (**@ygvg**): We clarify in Section 5 that the growing gap between the bound and true curvature is a key finding, it is the geometric signature of the barren plateau. Our heuristic is the tool designed to empirically measure and correct for this gap. This is precisely why our heuristic is necessary. A naive LR derived from $L_{upper}$ would be \"excessively conservative\" (as the reviewer notes in W4). Our heuristic, $L_{effective} = \\kappa(n) \\cdot L_{upper}$, is our practical tool designed to explicitly empirically measure and correct for this $n$-dependent gap.\n\n**7. Multi-Parameter Gates, Parameter Sharing & Expressibility (**@uhHq @oVNx**)**\n\n“How does your analysis extend to multi-parameter gates?” (**@uhHq**)\n\nThis is addressed via Corollary 1 (Sec. 4.2). Multi-qubit rotations like $R_{xx}(\\theta)$ are generated by a single Hermitian $G_k$, and the bound\n$L \\le 4\\Vert M \\Vert_{2} \\sum_{k=1}^{P} \\Vert G_{k} \\Vert_{2}^{2}$\nStill applies directly.\n\n“What about parameter sharing?” (**@uhHq**)\n\nParameter sharing is compatible with the MTP structure (App. E), but the Hessian structure changes due to tied parameters. We now clarify this assumption in Sec. 4 and identify parameter sharing as a natural direction for future work, especially for architectures like QCNNs (Cong 2019).\n\nOverparameterization & expressibility (**@oVNx**)\n\nWe emphasize in **Section 5.2** that our work complements DLA-based theory (Larocca 2023): in Fig. 1c, expressibility saturation $D_{KL}$ coincides with curvature saturation. This complements overparameterization theory by offering an intuitive, curvature-based marker of when the ansatz becomes inefficient. \n\n**8. Future Work, Limitations & Additional Clarifications (@all)**\n\nWe acknowledge and appreciate the reviewers’ suggestions. In the revised Discussion and Appendix F.3, we have added discussions on:\n\n* Potential integration with parameter-free methods.\n* The Polyak-Lojasiewicz (PL) condition and Hessian lower bounds.\n* Theoretical investigation of the $\\kappa(n)$ plateau.\n\n**Closing Statement**\nWe deeply appreciate the reviewers’ insightful comments, which have helped us improve the work. Our added noisy experiments, new QNG baseline, and clarified theoretical framing directly address the main concerns. We believe these revisions strengthen both the practical relevance and conceptual contribution of our work, and we hope our clarifications resolve the concerns raised.\nThank you for your thoughtful evaluation."}}, "id": "b322ENgSbz", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763676445334, "cdate": 1763676445334, "tmdate": 1763679053128, "mdate": 1763679053128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to formalize and strengthen the theoretical understanding of the optimization landscapes underlying variational quantum algorithms (VQAs). It establishes that the objective functions of VQAs are globally L-smooth and derives a new upper bound on the smoothness constant, $L \\le 4|M|^2 \\sum_k |G_k|_2^2$, which is shown to be tighter, or at least no looser, than previous results. The authors further relate this bound to the geometry of quantum circuits, demonstrating that the curvature scales predictably in accordance with this bound and saturates at the high-expressibility limit, which they interpret as a geometric indicator of inefficient overparameterization. Finally, they propose a heuristic for setting near-optimal learning rates based on this analysis and validate it across multiple VQE benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The derivation of the bound looks clean and well-written. The bound holds for general circuits, and the authors provide an extensive comparison with previous results.\n\n- They connect the bound with the geometry of the loss landscape and draw practical heuristics for setting the learning rate in training VQCs. Their claims are supported by extensive numerical experiments."}, "weaknesses": {"value": "Both the bound and the numerical experiments assume an ideal, noise-free regime. While deriving a theoretical bound under noise would be challenging, numerical experiments with a realistic noise model (or on real quantum hardware via gradient computed through parameter-shift rule) could be feasibly conducted. Such experiments would help ground the insights and proposed heuristics in more practical settings."}, "questions": {"value": "How would the proposed (heuristic) optimal learning rate compare to conventional methods with dynamic learning rate scheduling, or even to parameter-free methods such as [1] or [2]?\n\n[1] Orabona, Francesco, and Tatiana Tommasi. \"Training deep networks without learning rates through coin betting.\" Advances in neural information processing systems 30 (2017).\n\n[2] Defazio, Aaron, and Konstantin Mishchenko. \"Learning-rate-free learning by d-adaptation.\" International Conference on Machine Learning. PMLR, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MjeKaRmxHX", "forum": "beg6QFuff4", "replyto": "beg6QFuff4", "signatures": ["ICLR.cc/2026/Conference/Submission15126/Reviewer_Xmst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15126/Reviewer_Xmst"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975075624, "cdate": 1761975075624, "tmdate": 1762925442798, "mdate": 1762925442798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}