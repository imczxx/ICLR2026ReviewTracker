{"id": "ys80cc2N5M", "number": 12662, "cdate": 1758209350549, "mdate": 1759897495379, "content": {"title": "GoR: A Unified and Extensible Generative Framework for Ordinal Regression", "abstract": "Ordinal Regression (OR), which predicts the target values with inherent order, underpins a wide spectrum of applications from computer vision to recommendation systems. The intrinsic ordinal structure and non-stationary inter-class boundaries make OR fundamentally more challenging than conventional classification or regression. Existing approaches, predominantly based on Continuous Space Discretization (CSD), struggle to model these ordinal relationships, but are hampered by boundary ambiguity. Alternative rank-based methods, while effective, rely on implicit order dependencies and suffer from the rigidity of fixed binning.\n\nInspired by the advances of generative language models, we propose **G**enerative **O**rdinal **R**egression (**GoR**), a novel generative paradigm that reframes OR as a sequential generation task. GoR autoregressively predicts ordinal segments until a dynamic ⟨EOS⟩, explicitly capturing ordinal dependencies while enabling adaptive resolution and interpretable step-wise refinement. To support this process, we theoretically establish a bias–variance decomposed error bound and propose the **Co**verage–**Di**stinctiveness Index (**CoDi**), a principled metric for vocabulary construction that balances quantization bias against statistical variance. The GoR framework is model-agnostic, ensuring broad compatibility with arbitrary task-specific architectures. Moreover, it can be seamlessly integrated with established optimization strategies for generative models at a negligible adaptation cost. Extensive experiments on **17** diverse ordinal regression benchmarks across **six** major domains demonstrate GoR's powerful generalization and consistent superiority over state-of-the-art OR methods.", "tldr": "", "keywords": ["Ordinal Regression", "Generative Regression", "Vocabulary Design"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f807dc91cd2993faef4bc93b0e71e50a4ba1d36.pdf", "supplementary_material": "/attachment/a5bf5f86c47e319e8a8b55546e328073cb103c08.pdf"}, "replies": [{"content": {"summary": {"value": "GoR reframes OR as a sequential generation task, where the model autoregressively predicts tokens representing ordinal value segments based on Coverage-Distinctiveness Index (CoDi) and Quantile-based Vocabulary. This method is novel and interesting. Although similar methods have been applied in other visual fields such as image generation and depth estimation, the contribution of applying generative models to the general ordinal regression task is commendable. Despite this paper facing a few weaknesses in qualitative analysis and missing some related works, I think the current version of the paper has reached the acceptance bar of ICLR. If the author can address my concerns, I will consider further improving my score."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is novel and interesting.\n\n2. Experimental results show the effectiveness of the proposed method. \n\n3. Model-agnostic framework compatible with various architectures makes the model practical and flexible."}, "weaknesses": {"value": "1. There is a lack of analysis of the distribution of model improvements. In other words, which part of the corrections leads to the performance improvements? boundary samples, small category samples, or most samples in the whole distribution?\n\n2. Medical disease grading is a common OR task. The medical datasets possess boundary ambiguity and long-tail problems. Testing on medical datasets can enhance the influence and persuasion of the proposed method. \n\n3. Regarding the related work on ordinal regression, the author seems to overlook a recent development—the methods of introducing CLIP and language models, e.g, OrdinalCLIP, L2RCLIP, NumCLIP and so on. \n\n4. Lack of experimental comparison to some popular or latest OR methods like PoE (Li et al, CVPR2021), Ord2seq (Wang et al, ICCV2023), and NumCLIP (Du et al, ECCV2024)."}, "questions": {"value": "1. This method is based on discrete modeling via vocabulary construction. What if using continuous modeling methods? Existing methods show that continuous modeling may be a better choice than discrete modeling in some fields[1].  Has the author conducted any relevant experiments? \n\n   [1] Autoregressive Image Generation without Vector Quantization, NIPS2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3Z8bzJtuAF", "forum": "ys80cc2N5M", "replyto": "ys80cc2N5M", "signatures": ["ICLR.cc/2026/Conference/Submission12662/Reviewer_EExj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12662/Reviewer_EExj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465128829, "cdate": 1761465128829, "tmdate": 1762923501013, "mdate": 1762923501013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GoR (Generative Ordinal Regression), a novel generative framework that formulates ordinal regression as an autoregressive token generation task, terminating with a dynamic <EOS> token. Instead of relying on traditional continuous space discretization or rank-based classification approaches, GoR represents the target ordinal value as a sequence of additive segments drawn from a learned vocabulary. To support this framework, the authors derive a bias-variance decomposed MSE error bound and propose Coverage–Distinctiveness index for principled vocabulary construction, addressing the trade-off between quantization bias and statistical variance. The framework is model-agnostic, supporting a wide range of encoders and decoders, and is extensible to standard generative learning techniques. Extensive experiments show that GoR achieves state-of-the-art performance consistently."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposal to model ordinal regression as sequential generative modeling is a novel paradigm shift.\n2. The bias-variance decomposition provides theoretical grounding for vocabulary design.\n3. Good potential for future works as it is compatible with standard training techniques used in generative modeling\n4. Achieves consistent gains on the benchmarks."}, "weaknesses": {"value": "1. Decoding time grows linearly with the number of tokens, which itself varies with the resolution and magnitude of the target ordinal value. The author should consider to include efficiency evaluation.\n2. No comparisons with other generative-based models like DDPM or Normalizing Flows in continuous ordinal prediction.\n3. Longer sequences amplify token-level noise due to accumulating prediction errors across steps. Prediction accuracy may degrade on long sequences or in fine-resolution tasks, especially under greedy decoding. The authors should consider to analyse the trade-off of the model."}, "questions": {"value": "See Weaknesses Above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j6jIxbRGz6", "forum": "ys80cc2N5M", "replyto": "ys80cc2N5M", "signatures": ["ICLR.cc/2026/Conference/Submission12662/Reviewer_yYd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12662/Reviewer_yYd1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717370382, "cdate": 1761717370382, "tmdate": 1762923500571, "mdate": 1762923500571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel generative framework termed GoR for ordinal regression that reformulates scalar ordinal prediction as autoregressive token generation. By explicitly modeling ordinal dependencies and dynamically controlling prediction granularity via a learnable ⟨EOS⟩ token, the method overcomes the rigidity and boundary ambiguity in conventional discretization- and ranking-based approaches. The proposed CoDi-guided vocabulary construction is theoretically grounded through a bias–variance decomposed MSE bound, ensuring both representational flexibility and cross-domain adaptability. Extensive experiments across diverse tasks demonstrate consistent and substantial improvements over strong baselines, highlighting GoR as a promising and unified paradigm for future research in ordinal modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-The sequential generation with a dynamic ⟨EOS⟩ explicitly models ordinal dependencies and enables coarse-to-fine refinement, which can offer strong interpretability and flexibility.\n\n-The paper delivers rigorous analysis of limitations in rank-based methods and a principled MSE error bound, which provides solid justification for the proposed approach.\n\n-Extensive experiments across diverse domains show clear and consistent improvements over strong baselines, thereby demonstrating strong generalizability."}, "weaknesses": {"value": "-It remains unclear whether the improvements can be mainly attributed to the proposed paradigm or simply from the stronger autoregressive decoder; further controlled ablations are needed.\n\n-The effectiveness of the proposed method depends on empirical tuning, which may limit robustness across tasks.\n\n-The sequential generation introduces longer prediction paths and potentially higher latency, which poses concerns for efficiency and industrial deployment.\n\n-Autoregressive decoding may suffer from compounding errors. The paper does not sufficiently address exposure bias, nor analyze the trade-off between robustness gains from beam search and increased inference cost.\n\n-The sequence length varies with label range and distribution, potentially affecting stability and efficiency. More evidence is needed to verify consistent performance across tasks with diverse label scales."}, "questions": {"value": "-It remains unclear whether the improvements can be mainly attributed to the proposed paradigm or simply from the stronger autoregressive decoder; further controlled ablations are needed.\n\n-The effectiveness of the proposed method depends on empirical tuning, which may limit robustness across tasks.\n\n-The sequential generation introduces longer prediction paths and potentially higher latency, which poses concerns for efficiency and industrial deployment.\n\n-Autoregressive decoding may suffer from compounding errors. The paper does not sufficiently address exposure bias, nor analyze the trade-off between robustness gains from beam search and increased inference cost.\n\n-The sequence length varies with label range and distribution, potentially affecting stability and efficiency. More evidence is needed to verify consistent performance across tasks with diverse label scales."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gD3wTJvKbi", "forum": "ys80cc2N5M", "replyto": "ys80cc2N5M", "signatures": ["ICLR.cc/2026/Conference/Submission12662/Reviewer_BLVB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12662/Reviewer_BLVB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922282208, "cdate": 1761922282208, "tmdate": 1762923500225, "mdate": 1762923500225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for ordinal regression that treats it as a sequence generation task, predicting segments step by step until an END token. It avoids issues in traditional methods like boundary ambiguity and rigid binning, and introduces a way to balance bias and variance using a new metric called CoDI. It works well across many datasets and is easy to plug into existing generative models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and easy to follow. \n- The experiments show consistent improvements over SOTA across 17 datasets in 6 domains"}, "weaknesses": {"value": "- Is it possible that vocabulary pruning may lead to loss of information, especially affecting minority classes in imbalanced datasets?\n- Does GoR support control over output sequence length, or does it only rely on detecting <EOS>? If no, how to control the maximum length of the output sequence, and will it take a longer time for inference? If yes, what would be the performance when limiting the maximum length of the output sequence to a certain value, e.g., 1? What are the impacts on inference time and performance if the sequence length is limited? More evaluations should be included to demonstrate. \n- How was the threshold $\\epsilon$ and percentage $\\beta$ determined and are there any effects on the final results?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WDj5ZK4ptI", "forum": "ys80cc2N5M", "replyto": "ys80cc2N5M", "signatures": ["ICLR.cc/2026/Conference/Submission12662/Reviewer_n5TF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12662/Reviewer_n5TF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990841254, "cdate": 1761990841254, "tmdate": 1762923499268, "mdate": 1762923499268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}