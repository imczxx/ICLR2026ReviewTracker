{"id": "rCFPSIjqMf", "number": 593, "cdate": 1756751494766, "mdate": 1763482683349, "content": {"title": "SURGE: Surprise-Guided Token Reduction for Efficient Video Understanding with VLMs", "abstract": "Videos contain rich information but also high redundancy, as consecutive frames often share similar backgrounds and predictable motions. Current video-language models (VLMs) are unable to exploit this redundancy and therefore perform a significant amount of superfluous computation, processing thousands of patch tokens even when little new information is present. What is missing is an on-the-fly, model-agnostic signal of temporal predictability to decide whether tokens carry unpredictable information that merits computation. We propose SURGE, a training-free and backbone-agnostic method that measures surprise in token space. Surprise scores are defined by the prediction error of each token from its recent history; high-surprise tokens are retained, while predictable ones are pruned. Aggregating scores over time produces a surprise curve that highlights key events, which can be further refined with CLIP-based query relevance to form a compact spatio-temporal mask. Experiments on multiple video understanding benchmarks show that SURGE reduces tokens by up to 7$\\times$ and prefill cost by 86–98\\%, while maintaining accuracy within $\\pm$1 point of full-token baselines. By aligning computation with novelty, SURGE enables video VLMs to handle long contexts efficiently and without retraining.", "tldr": "", "keywords": ["Efficient Video Understanding", "Vision-Language Models", "Token Pruning", "Redundancy Reduction", "Predictive Coding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d1f1530ed56205eadae495da1677b767c6aa5e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SURGE, a training-free and model-agnostic method for improving the efficiency of VLMs. The authors propose defining \"surprise\" as the prediction error of a visual token based on its recent history. A lightweight constant-velocity temporal predictor, enhanced with affine drift correction and variance normalization, is used to calculate a surprise score for each token at each frame.\n\nBased on these scores, SURGE employs a two-stage process to create a sparse spatio-temporal mask, including token pruning and event segmentation. For query-based tasks, the method can be extended to SURGE⋆, which uses CLIP to rank these key events by their relevance to the text query, further focusing computation. The authors conduct extensive experiments on three state-of-the-art VLMs (InternVL-3.5-VL, Video-LLaVA-Qwen, Qwen2.5-VL) across five diverse video understanding benchmarks. The results show that SURGE can reduce token counts by up to 7x and prefill costs by 86–98%, while maintaining accuracy within ±1 percentage point of the full-token baseline, demonstrating its effectiveness and practicality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Soundness of the core idea: The central contribution is the formulation of \"surprise\" as a criterion for token retention. Using a simple, physics-inspired constant-velocity predictor in the token embedding space to measure novelty is both original in this context and conceptually elegant. It provides a principled way to distinguish new information from redundant content, moving beyond heuristics like attention weights which can be noisy or biased.\n\n- Practicality: SURGE is training-free and backbone-agnostic. This is a major strength, as it can be plugged into virtually any ViT-based VLM without costly retraining or architectural modifications. This makes the method highly practical and lowers the barrier to adoption for improving inference efficiency.\n\n- Impressive empirical performance: The paper demonstrates substantial efficiency gains while maintaining accuracy."}, "weaknesses": {"value": "- The submission conflicts with the format requirement: \"The table number and title always appear before the table.\" I am not sure whether it should be a reason for a desk rejection.\n\n- The paper claims \"negligible overhead\" for the SURGE module, but this is not sufficiently quantified.\n\n- The method introduces too many new hyperparameters but does not justify how these hyperparameters are chosen and how they influence the performance.\n\n- The constant-velocity model with affine drift correction is a core component. While effective for many scenarios, it has inherent limitations. It may struggle with complex, non-linear motions such as rapid rotations, non-rigid deformations, or fast zooms, potentially leading to inaccurate surprise scores. Besides, the temporal predictor uses only the displacement from the immediate previous step. This limited temporal window makes the model susceptible to noise and unable to capture more complex, non-linear dynamics such as acceleration or periodic motions.\n\n- There have been many token-level compression methods proposed in the past year, such as ToMe in the related work, and [1] [2]. Please see the survey [3] for more details. The paper should compare with these methods.\n\n- The proposed method operates on the patch embeddings after they have been processed by the vision encoder. While it effectively reduces the computational load on the subsequent large language model, it does not address the significant upstream cost of encoding every single frame of a long video. For extremely long videos, the vision encoder itself can become a major bottleneck. It would be valuable to explore whether the core idea of \"surprise\" could be adapted to operate at a lower level—perhaps on raw pixels or early-layer features—to enable dynamic frame dropping or resolution adjustment before the expensive full ViT forward pass.\n\n- The method relies on a \"global percentile\" threshold and peak-based event segmentation, both of which seem to require processing the entire video clip offline to establish a global context of surprise scores. This \"two-pass\" nature makes its direct application to true streaming or real-time video analysis challenging, where decisions must be made causally and on-the-fly with only past and present information.\n\n[1] ViLAMP: Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation\n[2] Divprune: Diversitybased visual token pruning for large multimodal models\n[3] When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios"}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WYBeFGsTsV", "forum": "rCFPSIjqMf", "replyto": "rCFPSIjqMf", "signatures": ["ICLR.cc/2026/Conference/Submission593/Reviewer_ky99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission593/Reviewer_ky99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760586543171, "cdate": 1760586543171, "tmdate": 1762915559925, "mdate": 1762915559925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SURGE, a training-free, and model-agnostic method designed to enhance the long video understanding by dynamically reducing redundant input tokens. It uses a simple feature predictor to estimate the next frame's visual features based on the current one. If the surprise score is low, the corresponding tokens are pruned, significantly reducing the VLM's computational load. Then it refines with CLIP-based query relevance. SURGE reduces tokens by up to 7× and prefill cost by 86–98%, while keeping accuracy within ±1 point of full-token baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The method is training-free and thus can be applied to any LVLMs.\n+ It reduces tokens by up to 7× and prefill cost by 86–98% while preserve the accuracy within ±1 point.\n+ The authors provide comprehensive results based on several LVLMs across multiple video understanding benchmarks."}, "weaknesses": {"value": "+ The paper lacks a discussion of related work on video token compression and frame selection methods [1–5].\n+ Although the proposed method improves efficiency, it does not yield a clear accuracy gain. Ideally, removing redundant tokens should free up contextual capacity to accommodate more informative content, thereby enhancing the model’s ability to answer queries.\n+ The proposed surprise scoring mechanism is intended to capture pixel-level changes, yet it relies on CLIP features trained in semantic space.\n+ Missing comparison with training-free token pruning methods [1,2,5].\n\n[1] DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models, CVPR 2025.\n[2] VisionZip: Longer is Better but Not Necessary in Vision Language Models, CVPR 2025.\n[3] Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding, CVPR 2024.\n[4] LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding, ICML 2025.\n[5] BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding, CVPR 2025."}, "questions": {"value": "+ Previous studies have shown that sampling more frames generally leads to better performance. Ideally, if redundant tokens are removed while key frames are preserved, the model should achieve improved results under the same context budget. Have the authors tested their method under the same context budget as the base model to verify whether token redundancy reduction effectively enhances context utilization and leads to better performance?\n+ As shown in Figure 2, why is the surprise score computed on the visual features after the linear projection? Since the projection aligns the features with the LLM’s input space rather than the pure visual space, wouldn’t this affect the intended focus? Have the authors conducted any experiments using features extracted directly from the ViT backbone?\n+ Additionally, is the frame–text similarity based on CLIP features also calculated after the linear projection?\n+ How does the proposed method perform compared with DyCoKe, VisionZip, and BOLT?\n+ The proposed surprise scoring mechanism is primarily designed to capture pixel-level changes. However, CLIP features are trained using a vision–language contrastive objective in a semantic space, which may not align well with this goal. Have the authors considered using vision-centric models, such as DINO, to compute the surprise scores instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KKvCwrPsMt", "forum": "rCFPSIjqMf", "replyto": "rCFPSIjqMf", "signatures": ["ICLR.cc/2026/Conference/Submission593/Reviewer_XSco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission593/Reviewer_XSco"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762274457, "cdate": 1761762274457, "tmdate": 1762915559706, "mdate": 1762915559706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "language models named SURGE. Specifically, SURGE includes two core modules: Surprise Estimation and Spatio-temporal Masking. Surprise Estimation leverages a constant-velocity predictor in token space with global affine drift removal and variance normalization to measure temporal predictability, where high-surprise tokens are identified by large prediction errors indicating novel content. Spatio-temporal Masking applies global percentile thresholding to retain the most surprising tokens and aggregates scores over time to produce a surprise curve for key event segmentation.\n\nThe experimental evaluation in this paper assessed the performance of the proposed SURGE on five public benchmarks (Video-MME, MLVU, MMBench-Video, TempCompass, and LongVideoBench), comparing it with various approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The definition of token surprise is considered novel.\n- The method’s performance has been validated across multiple base models, enhancing the credibility of the approach."}, "weaknesses": {"value": "- SURGE models temporal dynamics using constant-velocity prediction plus affine drift fitting. Compared to optical flow or more advanced temporal prediction methods, this simple predictor may be insufficient for complex motions or non-linear changes (e.g., rapid acceleration/deceleration, severe local deformations). Could there be missed detections of key tokens in high-complexity videos?\n- SURGE introduces several hyperparameters (γ, ∆, K, ρ), but the paper provides limited ablation discussion on their effects. Although Sec. 5.3 claims stability within reasonable ranges; in different tasks or video distributions, these parameters may require retuning.\n- SURGE⋆ adds an additional CLIP scoring step. Does this introduce extra computational overhead?\n- The FLOPs and latency analysis lack comparison with other methods.\n- The paper lacks comparison against other approaches that reduce tokens before the LLM stage, such as VisionZip [1] or LLaVA-Scissor [2].\n\n[1] VisionZip: Longer is Better but Not Necessary in Vision Language Models\n\n[2] LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs"}, "questions": {"value": "- In SURGE, it seems the decision to retain a token is based solely on its individual surprise score. If part of an object’s tokens are removed due to low surprise, could the remaining tokens lose semantic completeness and break the spatial continuity of the object representation? Why is there only a retain-or-delete operation at the token level, without a merging/fusion mechanism to preserve semantic integrity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R4Id6fVh62", "forum": "rCFPSIjqMf", "replyto": "rCFPSIjqMf", "signatures": ["ICLR.cc/2026/Conference/Submission593/Reviewer_xZY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission593/Reviewer_xZY7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898339910, "cdate": 1761898339910, "tmdate": 1762915559572, "mdate": 1762915559572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, a training-free and model-agnostic token reduction algorithm, called SURGE, is proposed. The proposed algorithm estimates the surprising scores based on the prediction error of each token from its recent history. Then, it remains the tokens with high surprising score, while prunes the tokens with low surprise. Additionally, it utilizes the CLIP similarities for query-focused applications. Experimental results on video understanding datasets show that the proposed algorithm achieves comparable or even better performance despite a lower computational budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and easy to follow.\n- The paper includes sufficient implementation details, which enhances its reproducibility.\n- The proposed method is motivated by a solid rationale and is technically well justified."}, "weaknesses": {"value": "- From Table 1, the comparison appears to be limited to a single baseline—fastV + Video-LLaVA-Qwen at one fixed budget. If comparisons with additional methods are difficult to include, it would still be helpful to report results across a broader range of budgets to better illustrate robustness and trade-offs.\n- L354-357 mentions that AKS and the proposed SURGE are complementary to each other. However, in Table 1, when comparing the results at similar token budgets, some results slightly improve while others decrease, and the others remain almost unchanged, so the overall performance appears to be roughly comparable. \n- It would be beneficial if the experimental results on benchmark datasets, such as Next‑QA or EgoSchema, are included.\n- This paper appears to lack a comparison with recent training-free token reduction methods. Including such baselines would help clarify the relative advantages of the proposed approach. For example, there are the following recent works:\n\n    [1] Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs, ICCV 2025\n\n    [2] Video, How Do Your Tokens Merge?, CVPR Workshop 2025\n\n    [3] Don’t Look Twice: Faster Video Transformers with Run-Length Tokenization, NeurIPS 2024\n\n- Typo\n  - L49 ”surprise” : the quotation marks appear to be incorrectly oriented."}, "questions": {"value": "My main concern is that the paper lacks sufficient performance comparison with existing token reduction methods. If this issue is properly addressed in the rebuttal, I would be willing to raise my score. Please also refer to the Weaknesses section for my other concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UfWpSlehup", "forum": "rCFPSIjqMf", "replyto": "rCFPSIjqMf", "signatures": ["ICLR.cc/2026/Conference/Submission593/Reviewer_ZWDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission593/Reviewer_ZWDa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037206122, "cdate": 1762037206122, "tmdate": 1762915559373, "mdate": 1762915559373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}