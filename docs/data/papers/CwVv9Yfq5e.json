{"id": "CwVv9Yfq5e", "number": 17676, "cdate": 1758279118865, "mdate": 1759897160995, "content": {"title": "RelightMaster: Precise Video Relighting with Multi-plane Light Images", "abstract": "Recent advances in diffusion models enable high-quality video generation and editing, but precise relighting with consistent video contents, which is critical for shaping scene atmosphere and viewer attention, remains unexplored. Mainstream text-to-video (T2V) models lack fine-grained lighting control due to text’s inherent limitation in describing lighting details and insufficient pre-training on lighting-related prompts. Additionally, constructing high-quality relighting training data is challenging, as real-world controllable lighting data is scarce. To address these issues, we propose RelightMaster, a novel framework for accurate and controllable video relighting.\nFirst, we build RelightVideo, the first dataset with identical dynamic content under varying precise lighting conditions based on the Unreal Engine. \nThen, we introduce Multi-plane Light Image (MPLI), a novel visual prompt inspired by Multi-Plane Image (MPI). MPLI models lighting via $K$ depth-aligned planes, representing 3D light source positions, intensities, and colors while supporting multi-source scenarios and generalizing to unseen light setups. Third, we design a Light Image Adapter that seamlessly injects MPLI into pre-trained Video Diffusion Transformers (DiT): it compresses MPLI via a pre-trained Video VAE and injects latent light features into DiT blocks, leveraging the base model’s generative prior without catastrophic forgetting.\nExperiments show that RelightMaster generates physically plausible lighting and shadows and preserves original scene content.", "tldr": "", "keywords": ["video editing", "video relighting", "video generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0686d9759c0d7d33e72cbf8d6c78b76a6b9a669e.pdf", "supplementary_material": "/attachment/5bab4d8eae4c75315a4213844b857192cbfd6083.zip"}, "replies": [{"content": {"summary": {"value": "The paper presented a RelightMaster method which introduced Multi-Plane Light Image (MPLI) to allow precise description of lighting conditions at different depths for video relighting. RelightMaster uses 4 planes of light images at 4 depths to model the irradiance in a 3D scene by a light source, which allows finer and more precise lighting condition than using text descriptions. These conditions are injected to a private T2V generation model by a Light Image Adapter (LIA). To train this model, a new synthetic RelightVideo dataset collects 7,824 pairs of videos in the same scene with different lighting conditions generated by Unreal Engine 5. The proposed RelightMaster compares favorably with Light-A-Video and TC-Light."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The major technical contribution of this paper is the new MPLI representation which does allow more precise and finer description of the irradiance in a 3D scene. Thus, it demonstrates clear advantages on controlling the lighting in terms of light source position, intensity, and changing light sources and multi-lights.  \n\nThe LIA is a reasonable scheme to integrate MPLI into the T2V model. The way to collect the RelightVideo dataset may be inspiring to other relighting works."}, "weaknesses": {"value": "The major concern is on the experiments which are hard for other researchers to reproduce and compare with, since RelightMaster is built on a private T2V generation model. MPLI introduces more detailed lighting conditions than text description, so it is expected to outperform competing methods after these light control signals are translated to vague text prompts and feed to Light-A-Video and TC-Light. \n\nThe ablation study is quite simple, just showing the performance of K=1 and with 1/3 training dataset. It is not clear what the contribution of the private T2V model and the RelightVideo dataset are. In the ablation, is it possible to replace the private T2V model with one that is publicly available to other researchers? Is it possible to finetune other relighting method with the RelightVideo dataset?"}, "questions": {"value": "If RelightVid and Lumen are closely related work to compare with?\n\nDid I miss the session of Reproducibility Statement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3MKHaAIn6q", "forum": "CwVv9Yfq5e", "replyto": "CwVv9Yfq5e", "signatures": ["ICLR.cc/2026/Conference/Submission17676/Reviewer_TeRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17676/Reviewer_TeRx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541220157, "cdate": 1761541220157, "tmdate": 1762927525924, "mdate": 1762927525924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RelightMaster, a diffusion-based framework for controllable video relighting. The goal is to achieve fine-grained lighting control using visual prompts instead of text.\n\nThe work identifies two key limitations of existing T2V models:\n\n1. text prompts cannot effectively describe lighting details.\n\n2. real controllable lighting data is scarce.\n\nThe authors introduce three main components:\n\n1. RelightVideo Dataset, a synthetic relighting dataset rendered with Unreal Engine ensuring consistent scene content under varying light conditions\n\n2. Multi-Plane Light Image (MPLI), a novel visual representation of lighting using K depth-aligned planes encoding 3D light source position, color, and intensity\n\n3. Light Image Adapter (LIA), injects compressed MPLI features into pre-trained Video Diffusion Transformers (DiT) using the existing VAE encoder weights for better alignment with the video prior. Experiments show controllable relighting effects across light position, intensity, and color, and demonstrate multi-source and temporally-varying relighting control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a clear gap in fine-grained video relighting\n\n2. Physically interpretable representation of lighting through MPLI\n\n3. The use of synthetic paired data is well justified.\n\n4. The framework demonstrates convincing qualitative results for position, color, intensity and depth control."}, "weaknesses": {"value": "1. Directional modeling: The paper assumes isotropic point lights defined by position, color, and intensity only. Modeling directional or area lights would improve realism and generality. Future work should explore non-isotropic light sources.\n\n2. Quantitative evaluation: No numerical comparison is provided. It would be important to include metrics such as (a) relighted image quality (FID or CLIP-based), (b) temporal consistency, and (c) user preference. Qualitative evaluation alone is insufficient for assessing generalization. Earlier works like Light-A-Video have provided quantitative metrics.\n\n3. MPLI design: MPLI uses 4 planes aligned with the camera to match the Video VAE compression scheme. However, it would be worth exploring having 4 light planes per frame (rather than per latent) and fusing them, as this could capture finer lighting variation over time. An ablation on the number of planes (K) is missing.\n\n4. Evaluation scope: Comparisons to Light-A-Video and TC-Light rely on textual relighting descriptions, which may not be fully fair given those methods were not designed for fine spatial control."}, "questions": {"value": "Overall: This is a strong paper with clear novelty, solid technical grounding, and compelling qualitative results. Addressing quantitative evaluation and exploring directional lighting or richer MPLI structures would further strengthen it.\n\n\n1. How sensitive are results to the number of light planes (K)?\n\n2. Could directionality or angular falloff be included in MPLI without destabilizing training?\n\n3. Can quantitative evaluation be added (even on synthetic data) to measure temporal consistency or realism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UxmXXSp2AC", "forum": "CwVv9Yfq5e", "replyto": "CwVv9Yfq5e", "signatures": ["ICLR.cc/2026/Conference/Submission17676/Reviewer_53nu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17676/Reviewer_53nu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830340957, "cdate": 1761830340957, "tmdate": 1762927525516, "mdate": 1762927525516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for precise video relighting, addressing key challenges in lighting control within video generation. The authors propose a Multi-plane Light Image (MPLI) representation to encode 3D light source properties and a Light Image Adapter (LIA) to inject this information into pre-trained video diffusion models. A synthetic dataset, RelightVideo, is constructed using Unreal Engine to enable training with consistent dynamic content under varying lighting conditions. Experiments demonstrate fine-grained control over light position, intensity, color, and temporal variation, outperforming existing methods in both accuracy and generalization."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes MPLI representation effectively encodes 3D light source properties and naturally aligns with video modality, enabling precise and dynamic multi-light control.\n2.\tThe Light Image Adapter offers a lightweight and efficient integration strategy that preserves pre-trained model knowledge and avoids catastrophic forgetting.\n3.\tThe RelightVideo dataset provides the first large-scale video relighting dataset with consistent dynamic content under varied lighting conditions."}, "weaknesses": {"value": "1.\tThe experimental section lacks quantitative results and metrics, relying solely on qualitative visual examples, which makes a rigorous performance comparison impossible.\n2.\tThe comparison with state-of-the-art methods is limited to only two other approaches (Light-A-Video and TC-Light) and relies solely on qualitative visual examples, with no quantitative metrics or user studies provided to substantiate the claimed superiority.\n3.\tThe method's performance and generalization on real-world videos, which have more complex lighting and textures than the synthetic training data, are not thoroughly evaluated or discussed."}, "questions": {"value": "1.\tIn the ablation study on LIA initialization, what is the specific performance drop when using \"zero init\" compared to \"copy init\"?\n2.\tThe paper uses a fixed number of four planes (K=4) for the MPLI representation,it was this value determined empirically?\n3.\tThe method is trained exclusively on synthetic data from Unreal Engine,how does it generalize to real-world videos with complex natural lighting and textures?\n4.\tIs there a plan to open source the dataset? This is very important for reproducing the results of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VvkEJkOsuc", "forum": "CwVv9Yfq5e", "replyto": "CwVv9Yfq5e", "signatures": ["ICLR.cc/2026/Conference/Submission17676/Reviewer_zrp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17676/Reviewer_zrp3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909943598, "cdate": 1761909943598, "tmdate": 1762927524267, "mdate": 1762927524267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RelightMaster, a video relighting framework with a novel lighting representation Multi-plane Light Image (MPLI). To address the data scarcity of relighting videos, the authors also built a dataset called RelightVideo that features synthetic rendered videos using the Unreal Engine. MPLI can be injected into pre-trained video diffusion transformers to generate relighted videos with target lighting conditions. Qualitative results are reported with comparison to Light-A-Video and TC-Light. \n\nOverall, I like the MPLI idea to represent lighting conditions, and the designed relighting framework is reasonable. On the other hand, the experiment section is weak, and the choice of using point light does not really show the potential of MPLI in the relighting task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The most interesting aspect of this paper is the proposed Multi-plane Light Image (MPLI) for lighting representation. It is extended from the idea of the Multi-Plane Image for 3D representation and adapted to lighting. Such a representation could be useful for a broader range of applications that utilize lighting. \n\n- The RelightVideo dataset could be a good contribution, if it will be released to the public."}, "weaknesses": {"value": "- While the idea of the Multi-plane Light Image is interesting, the current usage of it to represent lighting is limited. Take a look at Figures 3 and 4, the majority of the MPLI are empty (i.e., black). This seems to be a not-so-efficient way of representing *point lights*. MPLI should be able to capture much complex spatially and temporally varying lighting. I feel like this is a missed opportunity. \n\n\n- There is no quantitative evaluation in the experiment section. I understand that evaluating relighting results in general is challenging, especially for videos. But there are still common metrics that other relighting methods report, such as PSNR. In the ideal case, the authors should have both a metrics-based evaluation and a user study. The current experiment is quite weak. \n\nMinor thing\n1. Caption for Figure 2: typo of 'dataset', it should be 'An overview of our relighting framework'. Also, the caption should mention that this overview is for training."}, "questions": {"value": "1. Can the authors show some results with more complex lighting than point lights? \n\n2. Can the authors discuss the efficiency of the MPLI to represent point lights? It seems most of the images are blank (for example, Figure 2). \n\n3. Line 237 mentions that the authors use an 'internal text-to-video' model. Why not use an open source model such as WAN? \n\n4. How does the method compare to simple frame-by-frame image-based relighting methods? \n\n5. Can the authors show some quantitative results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eUQfFBICgQ", "forum": "CwVv9Yfq5e", "replyto": "CwVv9Yfq5e", "signatures": ["ICLR.cc/2026/Conference/Submission17676/Reviewer_adZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17676/Reviewer_adZs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942579072, "cdate": 1761942579072, "tmdate": 1762927523648, "mdate": 1762927523648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}