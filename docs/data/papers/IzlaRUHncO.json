{"id": "IzlaRUHncO", "number": 8613, "cdate": 1758092592632, "mdate": 1759897773304, "content": {"title": "Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting", "abstract": "Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Code will be released.", "tldr": "We propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity, surpassing state-of-the-art NeRF methods in rendering quality.", "keywords": ["Gaussian Splatting", "Novel View Synthesis", "Decoupled Radiance Fields", "View-dependent Opacity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31d77c5754c1a3bc1ecdee4735b688bdfb278b9e.pdf", "supplementary_material": "/attachment/964aac61d96e1686d67b477532937c9561770ed0.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Augmented Radiance Field (ARF), a plug-and-play framework that enhances 3D Gaussian Splatting (3DGS) for realistic view synthesis. While 3DGS achieves real-time rendering, its spherical harmonic color model struggles with specular reflections and material separation. ARF introduces a Gaussian kernel with view-dependent opacity to explicitly model specular effects. It further proposes an error-driven refinement that inserts and optimizes additional Gaussians in high-error regions. Experiments on multiple benchmarks show that ARF outperforms state-of-the-art NeRF and 3DGS variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new Gaussian kernel with view-dependent opacity that effectively models specular reflections and high-frequency lighting effects.\n\n2. The proposed 2D-to-3D error compensation mechanism adaptively adds and optimizes supplementary Gaussians in challenging regions.\n\n3. Extensive experiments across several benchmarks demonstrate consistent improvements over state-of-the-art NeRF and 3DGS methods, even with lower-order spherical harmonics."}, "weaknesses": {"value": "1. Since the method refines pre-trained 3DGS scenes rather than learning end-to-end, it may accumulate suboptimal biases from the base model and depend on prior scene quality. The image-space optimization and 2D-to-3D projection introduce extra computation and memory costs compared to standard 3DGS.\n\n2. The paper does not analyze training stability or convergence behavior when optimizing new Gaussians, especially in highly reflective or complex lighting conditions.\n\n3. The rendering quality heavily depends on the tuning of the opacity lobe parameters beta; the paper does not provide an analysis of how these parameters influence optimization."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNgPkglT9I", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Reviewer_eJHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Reviewer_eJHV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643727236, "cdate": 1761643727236, "tmdate": 1762920453233, "mdate": 1762920453233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Common issues (part 1)"}, "comment": {"value": "# Response to Common issues (part 1)\n\nWe would like to thank the reviewers for the valuable feedback on our work. We are pleased to see that the reviewers affirmed the effectiveness of our method, especially encouraged by the praise that described it as \"**a smart solution**\" and noted that \"**The idea is plausible, insightful, and highly meaningful.**\" Below, we focus on the common issues in this response. Other personal questions are replied to in the individual comments.\n\n## 1. More comparisons\nThe first and most important question is the **comparison with other baselines**, particularly **Spec-Gaussian** [1] (Reviewer zmRh also mentioned AbsGS [2]). To ensure a fair comparison (in terms of training time and FPS), for the key baseline Spec-Gaussian, we downloaded their code and reproduced the experiments on our machine. Our reproduced metrics are slightly lower than those reported in their paper. We list both results in the table (distinguished as \"official\" and \"repro.\"). For AbsGS, we directly adopted the data from their paper. The comparisons are summarized below (in bold for the best and underlined for the second best).\n\n| Method | Mip- | NeRF | 360 | \\|  | | T&T | | \\|  | Deep | Blending | | \\|  | NeRF | Synthetic | |\n| --- | :--: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| | PSNR↑ | SSIM↑ | LPIPS↓ | | PSNR↑ | SSIM↑ | LPIPS↓ | | PSNR↑ | SSIM↑ | LPIPS↓ | | PSNR↑ | SSIM↑ | LPIPS↓ |\n| Spec-Gaussian (official) | 28.18 | 0.835 | 0.176 | | - | - | - | | - | - | - | | $\\underline{34.19}$ | $\\textbf{0.971}$ | $\\textbf{0.028}$ |\n| Spec-Gaussian (repro.)   | 27.97 | 0.834 | 0.176 | | 23.77 | 0.855 | 0.166 | | 29.53 | $\\underline{0.905}$ |        0.241        |     |        34.02        | $\\underline{0.970}$ |  $\\textbf{0.028}$   |\n| AbsGS                    |        27.49        |        0.820        |        0.191        |     |        23.73        |        0.853        |        0.162        |     |        29.67        |        0.902        | $\\underline{0.236}$ |     |          -          |          -          |          -          |\n| Ours (3DGS,sh=3)         |        28.39        |        0.834        |        0.184        |     |        23.90        |        0.856        |        0.158        |     |        29.90        |        0.903        |        0.237        |     |        34.02        |        0.969        |        0.031        |\n| Ours (MCMC,sh=2)         | $\\underline{28.89}$ | $\\underline{0.848}$ | $\\underline{0.171}$ |     | $\\underline{25.04}$ | $\\underline{0.871}$ | $\\underline{0.146}$ |     |  $\\textbf{30.33}$   |  $\\textbf{0.909}$   |  $\\textbf{0.235}$   |     |        34.03        | $\\underline{0.970}$ |        0.030        |\n| Ours (MCMC,sh=3)         |  $\\textbf{28.96}$   |  $\\textbf{0.849}$   |  $\\textbf{0.170}$   |     |  $\\textbf{25.06}$   |  $\\textbf{0.872}$   |  $\\textbf{0.144}$   |     | $\\underline{30.22}$ |  $\\textbf{0.909}$   | $\\underline{0.236}$ |     |  $\\textbf{34.35}$   |  $\\textbf{0.971}$   | $\\underline{0.029}$ |\n\nAs shown above, Spec-Gaussian's reconstruction quality lags behind ours to some extent. Furthermore, their code utilizes different parameter configurations for real and non-real scenes, as well as indoor and outdoor scenes, whereas our method employs a unified hyperparameter setting.\n\nBelow is the comparison of training time, memory usage, and FPS. We use \"total time\"​ to represent the combined duration of the original scene reconstruction and the post-processing enhancement.\n\n| Method                 |      Mip-NeRF 360       |        |     | \\|  |     NeRF Synthetic      |       |     |\n| ---------------------- | :---------------------: | :----: | :-: | :-: | :---------------------: | :---: | :-: |\n|                        | Total (Post-proc.) time |  Mem   | FPS |     | Total (Post-proc.) time |  Mem  | FPS |\n| Spec-Gaussian (repro.) |        44.8 min         | 882 MB | 43  |     |        12.0 min         | 75 MB | 136 |\n| Ours (MCMC,sh=2)       |     36.5 (19.4) min     | 467 MB | 90  |     |     12.4 (7.6) min      | 44 MB | 279 |\n| Ours (MCMC,sh=3)       |     40.6 (20.6) min     | 722 MB | 86  |     |     12.8 (7.6) min      | 68 MB | 270 |\n\nSince Spec-Gaussian employs a neural network for real-time rendering, it significantly impacts the FPS. This also affects scene reconstruction speed. Our method requires less memory and achieves a much higher FPS. However, due to our two-stage strategy (first training, then applying post-processing), the initial 3DGS optimization accounts for ~50% of the overall training time. Consequently, the total training time on a single GPU is comparable to that of Spec-Gaussian. Using multiple GPUs could significantly reduce the duration of the 2D Gaussian optimization and back-projection stages.\n\n[1] Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting\n\n[2] AbsGS: Recovering Fine Details for 3D Gaussian Splatting"}}, "id": "GQGi4GVsVH", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763359836687, "cdate": 1763359836687, "tmdate": 1763359836687, "mdate": 1763359836687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses limitations of 3D Gaussian Splatting (3DGS) in handling complex reflections due to its reliance on spherical harmonics for color encoding. The authors propose an enhanced Gaussian kernel that models specular effects via view-dependent opacity, combined with an error-driven compensation strategy to improve rendering quality. Their method starts with 2D Gaussian initialization, followed by adaptive insertion and optimization of enhanced kernels to produce an augmented radiance field."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a novel post-enhancement method for Gaussian splatting based on the Phong shading model, aiming to improve the modeling of view-dependent color."}, "weaknesses": {"value": "- The paper leverages geometric information from depth maps of a pre-trained 3DGS and back-projects the screen-space 2D Gaussians into world space. However, due to the limitation of low-order spherical harmonics in 3DGS, the reconstructed scene geometry tends to be quite poor. As is well known, more accurate geometry modeling typically leads to more reliable view-dependent color estimation. Unfortunately, the paper does not provide any comparison between the optimized depth results and those from the pre-trained 3DGS. For instance, in the garden scene of the Mip-NeRF 360 dataset, the flat tabletop region could have been used as a clear example for such a comparison.\n- The proposed method does not fundamentally address the limitation of using spherical harmonics in 3DGS for separating diffuse and specular components. For example, Spec-Gaussian leverages anisotropic spherical Gaussians to better model view-dependent appearance. Instead of optimizing a pre-trained 3DGS, it might be more meaningful to design a more effective approach for modeling view-dependent effects directly, since the use of low-order SH inherently reflects a trade-off between rendering speed and quality.\n\nThe paper has some typos:\n1. Line 101, \"prossesses\"\n2. Line 155-156, \"view-dependently\"\n3. Line 188, \"...  reconstructs intricate radiance field\""}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P6TkHS5KJu", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Reviewer_LiBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Reviewer_LiBR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729098534, "cdate": 1761729098534, "tmdate": 1762920452805, "mdate": 1762920452805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Common issues (part 2)"}, "comment": {"value": "# Response to Common issues (part 2)\n## 2. Using different primitives\nWe note that reviewers hold different attitudes regarding **whether different primitives should be used**. We fully understand this disagreement. While our approach might be less conducive to downstream tasks such as inverse rendering and relighting, we have demonstrated its advantages in **improving rendering quality**. Let's draw an illustrative example from modern CG and game rendering for discussion—we often design different shaders for different material types, for two reasons: 1) No unified framework exists that can model all types of materials; 2) The computational complexity of shading differs across materials. (e.g., the Disney BSDF shading model involves far more complex calculations than simple diffuse reflection.) Using simpler shaders effectively saves computation time. This leads us to a key insight—If using different shaders is common practice in traditional triangle-based rendering pipelines, then why shouldn't Gaussian splatting employ different primitives? Given the vast diversity of materials in nature, attempting to model them all within a unified framework is likely infeasible (though neural networks could attempt this at the cost of heavy rendering overhead, as seen in Spec-Gaussian). Our solution, which introduces a second explicit primitive, offers the benefit of significantly improving rendering quality with minimal computational overhead.\n\nIn the paper, we chose DBS as the main baseline because this work utilizes Phong shading to model specular highlights and, at the time of submission, it had set the state-of-the-art in rendering quality among splatting-based methods. The goal of our work is to **push the rendering quality of 3D Gaussians to its limits**. We argue that **using different primitives represents a promising direction for this challenging problem**. Our work takes the first step along this path, and we believe the rendering quality of 3D Gaussians is far from reaching its full potential. Ample future work remains. For instance, one could explore using different primitives to model different materials from the very beginning of training, rather than introducing new primitives only at the post-processing stage. Another direction is designing a wider variety of primitives with adaptive assignment to different surfaces based on complexity needs."}}, "id": "UhhHFxAmTh", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763359902658, "cdate": 1763359902658, "tmdate": 1763359902658, "mdate": 1763359902658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper augments 3D Gaussian Splatting (3DGS) with a view-dependent opacity lobe to better reproduce specular effects. It introduces a post-enhancement pipeline that (i) detects high-residual regions in image space and optimizes sparse 2D Gaussians there, (ii) back-projects these into 3D via an “inverse splatting” procedure, and (iii) jointly optimizes the enhanced set with the original scene. The module is positioned as plug-and-play for existing 3DGS pipelines and aims to improve quality with modest runtime overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The two-stage 2D residual fixing, inverse splatting, and joint optimization could improve the final rendering quality.\n2. The proposed method could be served as a post-hoc module on top of the 3DGS-based methods."}, "weaknesses": {"value": "1. The primary concern is the design choice to separate diffuse and specular components across different primitives. In real scenes, most surfaces exhibit a mixture of both (with mirrors as a special extreme), and prior work that models these components within unified primitives enables shared optimization, coherent regularization, and cleaner support for inverse rendering and relighting. By contrast, the proposed error-driven initialization of dedicated “specular” primitives breaks this unification, ties specular capacity to residual patterns rather than material properties, and risks uneven coverage and missed interactions. As a result, the approach sacrifices some of the interpretability and downstream utility.\n2. Lobe-based models for specular appearance (notably spherical Gaussians) are well established in rendering and have been adopted within 3DGS frameworks (e.g., SpecGaussian [A]). Beyond these, there are various prior works tailored to specular scenes, including methods with broader applications such as inverse rendering and relighting. The manuscript would benefit from direct comparisons with these baselines to demonstrate the performance of the proposed design.\n3. Some statements about \"SH inherently fails to decouple diffuse and specular components\" on line 070 conflict with the definitions of the specular and diffuse components on line 138. SH0 could capture view-dependent colors, with higher orders capturing view-dependent colors. Some explanations about why SH inherently fails to decouple diffuse and specular components are needed.\n\n[A] Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting"}, "questions": {"value": "1. How to compute the gradients when performing the optimization in image space? As described in 209-210, the depth is updated via the nearest-neighbor sampling during optimization. However, since the emulated 3DGS differentiable rendering (described on line 201) requires depth sorting (in both forward and backward pass), this depth updating strategy will influence the gradients computation. Specifically, how many rasterization passes are performed during an optimization step? when to perform the depth updating, like before or after updating other parameters?\n2. What are the end-to-end training time and rendering FPS, compared with baselines (including baselines designed for specular scenes, such as SpecGaussian)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FSeMuWO0lD", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Reviewer_mqPT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Reviewer_mqPT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907138417, "cdate": 1761907138417, "tmdate": 1762920452235, "mdate": 1762920452235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a strategy to enhance the under-reconstructed regions of 3DGS. The method begins by learning a 3DGS model to fit the given multi-view images. However, due to limitations in the original densification and optimization processes of 3DGS, certain regions may not be reconstructed effectively.\n\nTo address this, the method identifies under-reconstructed regions by comparing the rendered images with the GT images. An error-weighted adaptive strategy is then employed to sample 2DGS on each view screen, targeting these problematic regions. Using the original rendered images as backgrounds, the 2DGS in each view is optimized to correct the image and align it with the GT.\n\nAlthough 2DGS is defined in screen space, the order of the splats is determined by the depth of the nearest rendered pixel. After optimization, clustering is performed for each 2D Gaussian to identify inlier pixels. These inliers are back-projected into 3D space, and a weighted PCA is applied to analyze their Gaussian distribution. The resulting 3D points are then directly used to initialize the supplemented 3DGS.\n\nFinally, these supplemented 3DGS points, along with the opacity of the original 3DGS, are jointly optimized to fit the input images, resulting in a more accurate 3D reconstruction."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses the under-reconstruction problem of 3D Gaussian Splatting (3DGS) directly and effectively by employing a smart solution: correcting problematic regions through 2D fitting and back-projection. The entire process carefully accounts for various factors in the pipeline that could lead to artifacts, making the approach both robust and practical. By \"reintegrating missing details into 3D space,\" the method achieves improved reconstruction. The idea is plausible, insightful, and highly meaningful.\n\n2. The rendered images produced by this method significantly outperform the baselines. Highlighted and specular regions are reconstructed with remarkable accuracy. This improvement is largely attributed to the proposed directional decaying opacity, which enables the method to better capture and fit high-frequency details in these challenging areas."}, "weaknesses": {"value": "The primary weakness of the paper lies in the limited comparison with baselines. The main baseline used in the study is deformable beta splatting, which explores alternative primitive representation functions to replace Gaussians. However, two additional relevant baselines are worth considering for comparison:\n\n1. **Spec-Gaussian**: Spec-Gaussian introduces a more advanced lighting function to replace SHs, enabling better handling of commonly observed anisotropic appearances. It achieves superior results in specular and highlight regions, making it closely related to the view-dependent opacity proposed in the paper.\n\n2. **AbsGS**: AbsGS leverages the absolute gradient norm accumulation as a metric to densify 3DGS. By using the absolute gradients from the 2D screen, it can effectively identify under-reconstructed regions, thereby improving the densification of Gaussians in problematic areas. This is closely related to the enhancement-on-the-bad idea proposed in the paper."}, "questions": {"value": "- I hold a positive attitude toward the proposed method; however, my main concern is the limited comparison with the closely related baselines mentioned, which weakens the overall persuasiveness of the paper. I would be willing to raise the score to accept if these comparisons are included. :)\n\n- The proposed method does not appear to be restricted to a specific representation, such as 3DGS. Beyond 3DGS, other flexible primitives, such as Deformable Beta Splatting (DBS), 3D Convex Splatting, and Deformable Radial Kernel (DRK), can also efficiently capture sharp regions and varying boundaries. These primitives could be incorporated into the 2D fixing step as well. Among them, DRK stands out as a strict projection-based representation, making it much easier to back-project and achieve accurate depth rendering. I suggest including a discussion on the potential application of the method to these more flexible primitives, as it could provide valuable insights and inspiration for future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DfFk948kji", "forum": "IzlaRUHncO", "replyto": "IzlaRUHncO", "signatures": ["ICLR.cc/2026/Conference/Submission8613/Reviewer_zmRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8613/Reviewer_zmRh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912202865, "cdate": 1761912202865, "tmdate": 1762920451696, "mdate": 1762920451696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}