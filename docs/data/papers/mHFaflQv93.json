{"id": "mHFaflQv93", "number": 11205, "cdate": 1758193265847, "mdate": 1759897601134, "content": {"title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction", "abstract": "The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present **PG-Occ**, an innovative  **P**rogressive  **G**aussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that **PG-Occ** achieves *state-of-the-art* performance with a relative  **14.3\\% mIoU improvement** over the previous best performing method. The source code and models will be made publicly available upon publication.", "tldr": "Progressive Gaussian Transformer for open-vocabulary 3D occupancy, adaptively expanding Gaussian representations to achieve detailed and scalable scene understanding.", "keywords": ["3D Gaussian Spaltting", "3D Occupancy Prediction", "Open-vocabulary"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cad7dff728674bca34979c0a4af418c0af6cf17f.pdf", "supplementary_material": "/attachment/1fae3cbe5a312f48fc63dccd90bfbb79c4f3263f.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents PG-Occ, a Progressive Gaussian Transformer for open-vocabulary 3D occupancy prediction. The method progressively densifies 3D Gaussian representations in a feed-forward manner to capture fine scene details while maintaining efficiency. An additional anisotropy-aware sampling strategy adaptively adjusts receptive fields across scales and time for better spatio-temporal feature aggregation. Integrating language-aligned features enables text-driven 3D reasoning. Experiments show that PG-Occ achieves improvement over previous state-of-the-art methods, delivering more detailed and scalable scene understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel Progressive Gaussian Transformer that progressively refines 3D Gaussian representations through a feed-forward densification process, balancing detail capture and computational efficiency.\n2. The proposed anisotropy-aware sampling adaptively adjusts receptive fields across directions and scales, enabling more accurate feature aggregation and fine-grained geometric modeling."}, "weaknesses": {"value": "1. Although progressive densification effectively enhances scene details, the increasing number of Gaussians at higher stages inevitably raises inference time and memory consumption. While the paper acknowledges this issue and plans to optimize it in future work, no quantitative memory analysis or profiling results are provided, leaving the practical cost-performance trade-off unclear.\n2. Experiments are conducted on two benchmark datasets, which is relatively limited. The evidence for cross-domain robustness and generalization remains insufficient.\n3. One of the main contributions, the anisotropy-aware sampling, provides quantitative gains in the ablation study, but the improvement is relatively small.\n4. The comparisons mainly cover works published up to 2024. Recent approaches from the past year in similar or related directions are missing from the comparison."}, "questions": {"value": "1. Could the authors provide a detailed analysis or profiling of memory consumption across different densification stages?\n2. Have the authors tested the method on additional datasets or unseen domains to further validate its generalization capability?\n3. While AFS shows limited quantitative improvement in the ablation, could the authors provide qualitative examples or visualizations that better illustrate its specific benefits or effects?\n4. Could the authors include comparisons with more recent methods (2024-2025)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "40RrFhpLYB", "forum": "mHFaflQv93", "replyto": "mHFaflQv93", "signatures": ["ICLR.cc/2026/Conference/Submission11205/Reviewer_qE6H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11205/Reviewer_qE6H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979979676, "cdate": 1761979979676, "tmdate": 1762922354646, "mdate": 1762922354646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PG-Occ, a progressive Gaussian transformer for open-vocabulary 3D occupancy. It starts from a coarse set of 3D Gaussians and, in a feed-forward way, adds new Gaussians only where depth reveals under-modeled regions, so the scene becomes denser exactly where it needs more detail. It further makes each Gaussian anisotropy-aware and keeps the model stable with asymmetric self-attention so newly added Gaussians do not disturb existing ones."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- i) The paper targets a clear gap in current Gaussian-based open-vocabulary occupancy. The proposed progressive, depth-guided densification is a straightforward and well-motivated way to increase capacity only where the scene is under-modeled.\n\n- ii) The anisotropy-aware feature sampling makes good use of the Gaussianâ€™s scale and orientation, which is sensible for driving scenes where many structures are not isotropic. \n\n- iii) The paper is well organized and clearly written."}, "weaknesses": {"value": "- i) The whole progressive pipeline relies on the quality of the pseudo depth.  A short discussion on robustness to worse depth (or to LiDAR-sparse depth) would make the claim stronger.  \n\n- ii) Because the Gaussian set can only grow, not shrink, the last layers still have to process the largest token set. This is fine at Occ3D-nuScenes resolution, but may need pruning for HD-maps or city-scale scenes."}, "questions": {"value": "- i) The current densification is driven by a depth discrepancy threshold. Have you tried combining this with a text-feature uncertainty signal, so that regions that are geometrically covered but semantically ambiguous can also trigger new Gaussians?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SyRdAlkbTP", "forum": "mHFaflQv93", "replyto": "mHFaflQv93", "signatures": ["ICLR.cc/2026/Conference/Submission11205/Reviewer_TDbw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11205/Reviewer_TDbw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985981944, "cdate": 1761985981944, "tmdate": 1762922354150, "mdate": 1762922354150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PG-Occ, a progressive Gaussian Transformer framework for open-vocabulary 3D occupancy prediction. The method couples (i) Progressive densification of Gaussians through an iterative feed-forward densification strategy, (ii) Anisotropy-aware feature sampling that selects sample points and projects them onto feature planes with varying receptive fields. Extensive experimental results demonstrate that PG-Occ achieves SOTA performance on Occ3D-nuScenes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper lie in its clear motivation and cohesive design: it addresses the trade-off in Gaussian representations via progressive densification, stabilizes the training dynamics through asymmetric attention, and improves feature alignment via anisotropy that matches each Gaussian. Empirically, main results, ablations, and efficiency comparisons reinforce one another, showing that under fixed or limited compute, the approach delivers a better balance of performance and speed than previous baselines."}, "weaknesses": {"value": "However, I also have some concerns:\n(1) The robustness of corner cases is underexplored: In autonomous driving, identifying corner cases is critical. Common classes like cars, trucks, and pedestrians are already well recognized by supervised learning, whereas less common categories, such as plastic bags or trash bins, are much harder to detect. Can PG-Occ recognize such corner cases (not limited to these two examples)?\n(2) From the paper, the motivation seems somewhat trivial, more like a data augmentation extension of GaussTR, so it should explicitly articulate the deeper thought behind this motivation.\n(3) The method shows strong performance in a camera-based pipeline. Does the method work in a multimodal setting?"}, "questions": {"value": "1. Could you show more corner-case examples, such as uncommon categories?\n2. Could you provide a deeper intuition or theoretical analysis of the motivation to justify the necessity of the method?\n2. Is it also effective in a multimodal setting with LiDAR and cameras? If experiments are not feasible due to dataset constraints, an analysis of the underlying rationale would suffice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YpjovZXddx", "forum": "mHFaflQv93", "replyto": "mHFaflQv93", "signatures": ["ICLR.cc/2026/Conference/Submission11205/Reviewer_pxcU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11205/Reviewer_pxcU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986688345, "cdate": 1761986688345, "tmdate": 1762922353667, "mdate": 1762922353667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PG-Occ, a framework for open-vocabulary 3D occupancy prediction in autonomous driving. The central challenge it addresses is the trade-off between sparse Gaussian representations, which are efficient but miss fine-grained details, and dense representations, which incur high computational costs. It resolves this with two key contributions: \n1. Progressive Online Densification (POD): A feed-forward strategy that iteratively refines the 3D Gaussian scene representation. It starts with a coarse model and progressively adds detail to regions with higher perception errors, efficiently capturing fine-grained objects without modeling the entire scene densely.\n2. Anisotropy-aware Sampling (AFS): A sampling method that adaptively assigns receptive fields to Gaussians based on their specific scale and rotation (anisotropy). This allows for more effective spatio-temporal feature aggregation.\n\nThe model is trained using only 2D supervision (pseudo-depth maps and text-aligned features) without requiring 3D LiDAR data. Experiments show PG-Occ achieves state-of-the-art performance, demonstrating a significant 14.3% relative mIoU improvement over the previous best method on the Occ3D-nuScenes dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PG-Occ introduces an efficient, online method that adaptively adds Gaussians to \"under-represented regions\" identified by depth errors. This allows the model to start coarse and progressively capture fine-grained details.\n2. The paper originally identifies a weakness in prior methods that treat Gaussians as simple points, ignoring their shape. The AFS module is a novel solution that samples features based on the Gaussian's specific scale and rotation. This allows for adaptive receptive fields and more effective spatio-temporal feature aggregation.\n3. ASA ensures training stability by allowing new, under-optimized Gaussians to learn from established ones, but not vice versa.\n4. The method achieves SOTA results on the challenging Occ3D-nuScenes dataset, with a 15.15 mIoU score."}, "weaknesses": {"value": "1. The method is trained using only 2D supervision from sparse-view cameras. This setup creates inherent ambiguity. A small, nearby object can project to a 2D feature patch similar to a large, distant object. While multi-view consistency and pseudo-depth supervision  help, they don't fully resolve this.\n2. The model is initialized using pseudo-depth maps from Metric3D V2 and also supervised using them. The authors should present experimental results using other depth prediction methods to demonstrate the robustness of the proposed method.\n3. The number of total queries should be provided in Table 4."}, "questions": {"value": "1. Table 4 reports the final method's speed as 2.40 FPS. However, Table 12 reports the inference time of the final progressive layer as only 60.6 ms, and the sum of all three layers as 146.3 ms (27.4 + 58.3 + 60.6). This sum suggests a throughput of ~6.8 FPS. What component is responsible for the major bottleneck that reduces the final speed to 2.40 FPS? Is it the ResNet-50 spatio-temporal backbone, the final Gaussian-to-voxel post-processing, or another part not listed in Table 12?\n2. You acknowledge a key limitation: \"constraining the Gaussian scale in depth is challenging, which can cause popping artifacts\". Could you elaborate on the severity and frequency of these artifacts? For example, do they primarily affect distant/occluded objects, or is it a general instability? How much do you believe this instability impacts the method's reliability for downstream tasks like motion planning, which require temporally stable geometric representations?\n3. The model is initialized and supervised by pseudo-depth. Have you investigated the model's robustness under different depth models?\n4. The ablation study shows that removing ASA (\"w/o ASA\") degrades performance. Did you also experiment with using standard, symmetric self-attention instead of just removing it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UsQ1gKPWJ1", "forum": "mHFaflQv93", "replyto": "mHFaflQv93", "signatures": ["ICLR.cc/2026/Conference/Submission11205/Reviewer_BKpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11205/Reviewer_BKpf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11205/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006815774, "cdate": 1762006815774, "tmdate": 1762922353214, "mdate": 1762922353214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}