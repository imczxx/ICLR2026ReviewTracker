{"id": "rJ6N6sunaU", "number": 615, "cdate": 1756755055538, "mdate": 1763705361317, "content": {"title": "NewtonGen: Physics-consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics", "abstract": "A primary bottleneck in large-scale text-to-video generation today is physical consistency and controllability. Despite recent advances, state-of-the-art models often produce unrealistic motions, such as objects falling upward, or abrupt changes in velocity and direction. Moreover, these models lack precise parameter control, struggling to generate physically consistent dynamics under different initial conditions. We argue that this fundamental limitation stems from current models learning motion distributions solely from appearance, while lacking an understanding of the underlying dynamics. In this work, we propose NewtonGen, a framework that integrates data-driven synthesis with learnable physical principles. At its core lies trainable Neural Newtonian Dynamics (NND), which can model and predict a variety of Newtonian motions, thereby injecting latent dynamical constraints into the video generation process. By jointly leveraging data priors and dynamical guidance, NewtonGen enables physically consistent video synthesis with precise parameter control. All data and code will be public.", "tldr": "Physics-Consistent and Controllable Text-to-Video Generation", "keywords": ["Generative Models", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a69497f1af27696305ded6a885bb3092a8a6f21.pdf", "supplementary_material": "/attachment/9cc815abed32c4037e71149b52140fdc9597336b.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a video generation pipeline, that consists on two models: 1) single-object motion generator; and 2) (frozen) foundational video diffusion model. The motion generator is parametrized to take the initial physical state of the object (position, rotation, size, velocity, etc) and predict the future ones. It is trained on a small synthetic dataset. The predicted states are then converted to an optical flow, and the video generation backbone is conditioned to generate the video based on it using the go-with-the-flow noise tweaking method. The pipeline is converted with existing open and closed source baselines and shown to generate physically more plausible . The evaluation is performed using the Physical Invariance Score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets a very important research problem, directing more community's focus towards it.\n- The writing and exposition is very clear, and also provides an interesting overview of the past literature\n- The paper releases the source code, which is a good step toward ensuring reproducibility and transparency.\n- The method compares to top-notch closed-source video generation baselines which makes the results more trustworthy. I even checked the Veo3 generation quality on the prompt \"a baseball ball flying from left to right\", and confirm that it fails to understand the prompt even for such a simple scenario in ~50% of the cases."}, "weaknesses": {"value": "- I believe the approach is overly simplistic and inherently unscalable. It would not help to generate complex realistic motions (e.g., a person juggling jellies, a cup breaking against a wall, a squirrel jumping on a thin tree branch, human dancing, etc.), which is the core physical challenge. In its current form, it is just a way to generate simple optical flow for a single object motion. Even single-object deformations (e.g., see example with the dough) are not generated realistically. The authors should clearly explain in detail how the approach could be scaled to very difficult motion cases described above for which it feels infeasible to 1) construct the pre-training dataset, and 2) create the initial physical state. I think that for simple rigid motion, it's not that modern video generators cannot render it, they just fail to understand the prompt.\n- If one treats the paper not as a general-purpose pipeline which is improving physics in video generators, but as an \"optical flow conditioning\" paper, then there is not much novelty in it. There are dozens of work that condition video models on motion, and what the current pipeline is doing is automating optical flow generation (in an overly simplistic way) instead of asking a human to provide manual input. But this could be automated in a simpler ways without the need to train a model (e.g., by asking LLMs to generate the trajectories, either directly or writing a trajectory generation code)."}, "questions": {"value": "- It is not well described how the object attributes are computed using the \"morphological analysis and OpenCV-based tools\" (L295). Would be good to include more details for completeness in the paper (without requiring a reader to study the code).\n- Type on L161: \"equation, This\"\n- \"most common physical motions in daily life (e.g., flying balls)\": I disagree with this statement, and believe that the most common physical motion in daily life are humans moving and talking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6JQjC4RL9Q", "forum": "rJ6N6sunaU", "replyto": "rJ6N6sunaU", "signatures": ["ICLR.cc/2026/Conference/Submission615/Reviewer_CpA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission615/Reviewer_CpA9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760900099539, "cdate": 1760900099539, "tmdate": 1762915566994, "mdate": 1762915566994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core bottlenecks of physical consistency and parameter controllability in state-of-the-art text-to-video (T2V) generation. Existing models rely on appearance-level motion memorization without understanding underlying physical laws, leading to unrealistic motions (e.g., upward-falling objects) and poor generalization. To solve this, the authors propose NewtonGen, a hybrid framework integrating data-driven T2V generation with physics-informed Neural Newtonian Dynamics (NND). NND leverages unified neural ordinary differential equations (ODEs) combined with a residual MLP to learn latent dynamics from small-scale \"physics-clean\" data (generated via a custom simulator). It models 9-dimensional latent physical states (position, velocity, rotation, size, etc.) to predict physics-consistent trajectories. During inference, NND’s state predictions and scene prompts guide a motion-controlled T2V model (Go-with-the-Flow) to generate videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Directly addresses two critical limitations of state-of-the-art text-to-video (T2V) models—physical inconsistency (e.g., upward-falling objects, abrupt velocity changes) and lack of parameter controllability—which scaling alone cannot resolve. This fills a key research gap, as current models only learn motion distributions from visual appearance rather than underlying physical laws.\n\n2. NND’s design (physics-informed linear neural ODEs + residual MLP) unifies diverse Newtonian motions (12 types tested, including uniform velocity, parabolic motion, rotation, deformation, and 3D motion) into a single framework. It efficiently learns latent dynamics from a small amount of \"physics-clean\" data, avoiding over-reliance on large-scale datasets.\n\n3. Compares against 5 representative baselines (closed-source: Sora, Veo3; open-source: CogVideoX-5B, Wan2.2; physics-based: PhysT2V) across 12 motion types, ensuring fairness by standardizing generation settings.\n\n4. Separates physical dynamics reasoning (NND, for state prediction) from video generation (motion-controlled model like Go-with-the-Flow). This design retains the visual quality of existing T2V models while injecting physical constraints and enabling easy integration with other T2V frameworks."}, "weaknesses": {"value": "1. Relies on continuous ODEs and thus cannot handle discrete physical events (e.g., collisions, rebounds, explosions). This restricts its applicability to real-world scenarios involving interactive or abrupt physical interactions.\n2.  NND is trained exclusively on data from a custom Python physics simulator (designed to avoid motion blur, noise, or background distractions). Performance on real-world videos (with clutter, occlusions, or motion blur) is unproven, as no experiments on real datasets are reported.\n3. While NewtonGen outperforms baselines on physical consistency (PIS), it does not quantitatively evaluate visual quality (e.g., texture realism, shadow dynamics, background coherence) against competitors. This makes it unclear if physical consistency comes at the cost of visual fidelity."}, "questions": {"value": "Please follow weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9jX0T4kXxD", "forum": "rJ6N6sunaU", "replyto": "rJ6N6sunaU", "signatures": ["ICLR.cc/2026/Conference/Submission615/Reviewer_UcEn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission615/Reviewer_UcEn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837225262, "cdate": 1761837225262, "tmdate": 1762915566771, "mdate": 1762915566771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents NewtonGen, a framework aimed at solving the lack of physical consistency and controllability in text-to-video (T2V) generation. The authors propose a framework that integrates a data-driven T2V model with learnable physical principles. Its core is the Neural Newtonian Dynamics (NND) module, a physics-informed neural ODE that models a 9-dimensional latent physical state (e.g., position, velocity, rotation, size). The NND is first trained on \"physics-clean\" synthetic data to learn these dynamics. During inference, this trained NND predicts a sequence of physical states from user-defined initial conditions, which are then converted to optical flow to guide a motion-controlled T2V generator. Experiments across 12 motion types demonstrate quantitative improvements in physical consistency and precise parameter control. The reviewer regards NewtonGen as a novel and well-motivated two-stage framework for injecting physical realism and controllability into video generation. The core contribution, the NND module, is noted as being cleverly designed, as it combines linear ODEs with a residual MLP to learn complex dynamics from synthetic data efficiently. The reviewer acknowledges that the framework shows significant quantitative improvements on the Physical Invariance Score (PIS) metric and possesses excellent qualitative controllability. However, the reviewer raises concerns about the framework's reliance on synthetic \"physics-clean\" data, which poses significant questions about the sim-to-real gap. Furthermore, the reviewer points out that the method is explicitly limited to continuous dynamics, excluding collisions or discrete interactions, which narrows the scope of its \"Newtonian\" claims"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The NND module is cleverly designed, combining physics-informed linear neural ODEs with a residual MLP. This allows it to flexibly learn a wide range of dynamical systems, from simple to complex, with high data efficiency.\n\n2. The framework achieves exceptional physical consistency across multiple motion types. In quantitative evaluations using the \"Physical Invariance Score\" (PIS) metric, NewtonGen's results significantly outperform all SOTA baselines and are very close to the simulated ground truth.\n\n3. The model achieves precise control over physical parameters, addressing a key weakness in existing T2V models. Experiments demonstrate it can generate motions that accurately correspond to user-specified initial physical states (e.g., position, velocity, size)."}, "weaknesses": {"value": "1. Sim-to-Real Gap: The NND is trained exclusively on synthetic \"physics-clean\" data. The paper does not demonstrate or discuss how the framework would handle real-world, noisy videos, suggesting its current application may be limited to generation from scratch rather than editing or predicting existing real-world videos.\n\n2. Limited Scope of Dynamics: The framework explicitly excludes discrete events such as collisions, rebounds, or multi-object interactions.\n\n3. Approximation of 3D Motion: The paper claims to handle 3D motion, but the method is \"equivalently realized through the combination of position and size control\". This appears to be a 2.5D approximation (i.e., objects scaling larger as they move closer) rather than a true 3D representation capable of handling complex perspective rotations.\n\n4. Misaligned Baseline Comparison: The paper compares NND against general-purpose T2V models like Sora and Veo3. These baselines were not optimized for fine-grained physical control, making the comparison somewhat misaligned."}, "questions": {"value": "1. The authors should explicitly state in the main paper that the framework's current application is generation from parameters, not prediction from real video. The \"continuous dynamics only\" limitation should also be candidly discussed in the main text, with potential extensions for handling discrete events like collisions.\n\n2. Add a \"Perfect Oracle\" Baseline: It is suggested to add a \"perfect simulator\" baseline to Table 1, where the ground-truth simulator optical flow is fed directly into the generator. This would quantify the performance gap between the learned NND and a \"perfect\" physics oracle.\n\n3. The authors should report standard deviations for the PIS scores in Table 1, rather than only the median, to provide a clearer measure of generation stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9vVulWQOT8", "forum": "rJ6N6sunaU", "replyto": "rJ6N6sunaU", "signatures": ["ICLR.cc/2026/Conference/Submission615/Reviewer_r8si"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission615/Reviewer_r8si"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915647651, "cdate": 1761915647651, "tmdate": 1762915566495, "mdate": 1762915566495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a hybrid generative framework that combines physical dynamics with data driven video generation. Its main contribution is Neural Newtonian Dynamics (NND), a learnable ODE based module that predicts latent physical states and enforces physical constraints during text to video synthesis. This method seeks to close the gap between visual realism and physical consistency in generative video models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The conceptual direction of embedding explicit dynamical models into video generation is strong and timely and the overall structure of the framework is clean and modular, separating physical dynamics reasoning from visual synthesis. \n\n2. The experimental evaluation is broad and convincing for their claims, with meaningful metrics and visualizations (even though generated videos are slightly on the short side). Comparisons with “Physical simulation then Generation” methods would add context, the current analysis supports the claims."}, "weaknesses": {"value": "1. The physical state encoder relies heavily on SAM2 and OpenCV to detect and extract geometric and kinematic properties from frames. While suitable for simple rigid-body scenarios, it is unclear how this mechanism could extend to richer physical properties such as elasticity, reflection, lighting, wave propagation, breaking, or liquid dynamics, leaving aside the potential extension of NND to multi-object scenarios. The current setup appears limited to tracking motion and deformation rather than modeling material or environmental physics, which constrains the framework’s generalization to more complex phenomena.\n\n2. The mapping from physical prompts to initial physical states $Z_0$ is not well-detailed. It remains unclear how the system estimates or derives physical parameters like initial position, velocity, or angular speed from the prompt, and how sensitive the final generation is to inaccuracies in these estimates.\n\n3. As indicated in Section 2, \"Physical Simulation then Generation\", \"... generative models themselves lack any inherent physical reasoing or simulation capability: users must predefine the physical simulation parameters and rules for each scenario, and these setings cannot readily generalize to other context or different physical laws\". I also fail to see how NND generalizes in this case, which I believe is also addmited in Appendix F, Question 4."}, "questions": {"value": "1. A direct comparison with a traditional physical simulation following the same pipeline would be valuable. Currently, it is unclear where the NND demonstrates a distinct advantage, is it in computational efficiency, scalability to data (which physics simulations don't have), or improved fidelity compared to directly employing a physics engine? Considering that the NND itself is initially trained on physical simulations, clarifying where it provides genuine value beyond simulation would help better position the contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aCYz5nBgZf", "forum": "rJ6N6sunaU", "replyto": "rJ6N6sunaU", "signatures": ["ICLR.cc/2026/Conference/Submission615/Reviewer_tuJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission615/Reviewer_tuJM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920525784, "cdate": 1761920525784, "tmdate": 1762915566380, "mdate": 1762915566380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}