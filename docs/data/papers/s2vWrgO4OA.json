{"id": "s2vWrgO4OA", "number": 19227, "cdate": 1758294601071, "mdate": 1759897051425, "content": {"title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP", "abstract": "Large language models (LLMs) can amplify misinformation, undermining societal goals like the UN SDGs. We study three documented drivers of misinformation ($\\textit{valence framing}$, $\\textit{information overload}$, and $\\textit{oversimplification}$) which are often shaped by one's default beliefs. Building on evidence that LLMs encode such defaults (e.g., “joy is positive,” “math is complex”) and can act as “bags of heuristics,” we ask: can general belief-driven heuristics behind misinformative behaviour be recovered from LLMs as clear rules? A key obstacle is that global rule-extraction methods in explainable AI (XAI) are built for numerical inputs/outputs, not text. We address this by eliciting global LLM beliefs and mapping them to numerical scores via statistically reliable abstractions, thereby enabling off-the-shelf global XAI to detect belief-related heuristics in LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear heuristics of increasing complexity (univariate, conjunctive, nonconvex) into popular LLMs (ChatGPT and Llama) via system instructions. This way, we find that $\\textit{RuleFit}$ under-detects non-univariate biases, while $\\textit{global SHAP}$ better approximates conjunctive ones but does not yield actionable rules. To bridge this gap, we propose $\\textit{RuleSHAP}$, a rule-extraction algorithm that couples global SHAP-value aggregations with rule induction to better capture non-univariate bias, improving heuristics detection over RuleFit by +94% (MRR@1) on average. Our results provide a practical pathway for revealing belief-driven biases in LLMs.", "tldr": "The paper benchmarks global rule extraction XAI algorithms for detecting bias-inducing heuristics in LLMs, and introduces RuleSHAP (a hybrid of SHAP and RuleFit) that significantly improves detection of nonlinear biases from LLM beliefs.", "keywords": ["Rule-based interpretability", "Global model explainability", "SHAP-based rule induction", "Misinformation drivers (framing/overload/oversimplification)"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab86e2f64f7b6712a6ed83ce680584ae0b83d5f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether global XAI methods can surface belief-driven, bias-inducing heuristics in LLMs, focusing on three major bias mechanisms: valence framing, information overload, and oversimplification. The authors propose a pipeline for abstracting LLM topics and outputs into numeric spaces, enabling the application of global XAI tools (specifically SHAP, RuleFit, and a proposed hybrid RuleSHAP) to detect such heuristics. By hard-coding nonlinear bias rules into several LLMs, they empirically compare these methods and show that RuleSHAP, which integrates global SHAP feature importance into the rule induction process, yields higher faithfulness (MRR) and more concise rule sets than the baselines. The findings illuminate key challenges and limitations for interpreting LLM biases using global XAI methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a careful abstraction pipeline that maps LLM beliefs and outputs to numeric features, allowing rule extraction tools designed for tabular data to be effectively applied to language tasks. This is a well-motivated workaround to a foundational technical barrier in applying XAI to LLM-generated text.\n\n2. The authors inject 14 ground-truth bias rules into five different LLMs, with a large, suitably sampled set of SDG-related topics. The empirical protocol is robustly justified using power analysis and correlation-based validation, adding credibility to quantative findings."}, "weaknesses": {"value": "1. RuleSHAP is a reasonable extension of SHAP + RuleFit, but the methodological contribution is mainly the combination and reweighting strategy. The core algorithmic ideas are not very new or theoretically deep.\n\n2. The approach relies heavily on manually chosen input and output features. The process is not automated, so it may be difficult to transfer this pipeline to new domains, new types of bias, or different languages.\n\n3. The paper reports performance, but does not deeply analyze how or why the methods fail in specific cases. More detailed error analysis could provide clearer guidance for future improvement."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "oixfntCJLA", "forum": "s2vWrgO4OA", "replyto": "s2vWrgO4OA", "signatures": ["ICLR.cc/2026/Conference/Submission19227/Reviewer_F6xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19227/Reviewer_F6xv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797048489, "cdate": 1761797048489, "tmdate": 1762931210868, "mdate": 1762931210868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether XAI techniques can uncover belief-driven biases embedded in LLMs. It studies three misinformation-related mechanisms and introduces a pipeline that translates LLM textual behaviors into numerical abstractions, enabling the use of global XAI methods like SHAP and RuleFit. The authors show that while SHAP identifies influential features, it lacks interpretability, and RuleFit misses complex (nonlinear) biases. To address this, they propose RuleSHAP, a hybrid approach that merges SHAP’s feature attributions with rule extraction, improving detection of non-univariate and conjunctive biases across models such as ChatGPT and Llama."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It introduces RuleSHAP, an original hybrid algorithm that combines SHAP’s theoretical grounding in feature attribution with RuleFit’s interpretable rule extraction, enabling interpretable symbolic bias detection — a combination not seen in prior XAI work.\n- The paper proposes a statistically grounded belief abstraction framework that transforms textual LLM inputs and outputs into ordered numerical spaces, bridging a known gap between text-based generative models and numeric XAI methods."}, "weaknesses": {"value": "- The belief abstraction layer converts textual behavior into numerical variables. This transformation, while necessary for SHAP, risks discarding contextual and semantic richness—especially when bias manifests subtly (e.g., through metaphor or framing tone).\n- The paper adopts MRR@1 as the main quantitative measure for bias detection performance. However, this metric assumes a rank-based relevance formulation that may not directly capture the semantic correctness or interpretability of rules.\n- While focusing on global bias detection, the paper doesn’t address how RuleSHAP complements or contrasts with local explanation frameworks. This leaves the interpretability spectrum somewhat under-theorized.\n- The study focuses on three bias mechanisms (valence framing, oversimplification, information overload). While methodologically clean, this limited taxonomy restricts claims about “global bias detection.”"}, "questions": {"value": "- The bias injection pipeline is elegant but synthetic — can you clarify how representative these injected heuristics (e.g., valence framing, oversimplification) are of naturally occurring biases in deployed LLMs? Do you expect RuleSHAP to generalize to biases like gendered language or stereotype reinforcement?\n- Since RuleSHAP relies on non-textual numerical representations, could this abstraction discard subtle contextual cues like sarcasm, metaphor, or topic-level associations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nrHlpvCroD", "forum": "s2vWrgO4OA", "replyto": "s2vWrgO4OA", "signatures": ["ICLR.cc/2026/Conference/Submission19227/Reviewer_apBs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19227/Reviewer_apBs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989538651, "cdate": 1761989538651, "tmdate": 1762931210409, "mdate": 1762931210409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether global XAI methods can detect belief-driven biases in LLMs, focusing on three misinformation-related behaviors: valence framing, information overload, and oversimplification. The authors address a key challenge: most global XAI methods work with numerical data, not text. They solve this by creating a statistically grounded abstraction pipeline that maps LLM-generated content and topics to numerical scores, enabling traditional XAI techniques to analyze LLM behavior. To establish ground truth, they inject bias-inducing rules of increasing complexity (univariate, conjunctive, and non-convex) into popular LLMs via system instructions. They find that existing methods like RuleFit struggle with non-univariate biases, while global SHAP detects biases but cannot express them as interpretable rules. The paper's main contribution is RuleSHAP, a novel rule-extraction algorithm that integrates global SHAP value aggregations with rule induction to better capture complex biases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a genuinely original approach to a critical gap: adapting global XAI methods (designed for tabular/numerical data) to work with LLMs' textual inputs and outputs.\n- The integration of SHAP into RuleFit is technically novel. This may be the first model-agnostic rule extraction method to leverage global SHAP for steering both split selection and rule pruning, bridging SHAP's theoretical rigor with RuleFit's interpretability."}, "weaknesses": {"value": "- The LLM is asked to score its own beliefs, then those scores are used to explain its behavior. This is inherently circular—you're using GPT-4o's worldview to explain GPT-4o's outputs. While the correlation certificates provide statistical validation, they don't resolve the epistemological problem: high correlation between \"GPT believes X is controversial\" and \"GPT writes controversially about X\" might simply reflect consistent bias, not meaningful explanation.\n- Section 3 states that SHAP perturbations require finding \"multiple points $j ∈ T$ for which $||u_k - u_j||_2$ is minimal ($\\approx$0)\" to mimic feature removal. What is the actual threshold for \"minimal\"? The \"$\\approx$0\" is vague. If redundancy is insufficient, does SHAP fail silently or return unreliable estimates?\n- The paper uses $T=0$, $top_p=0$ to eliminate sampling variance, but acknowledges higher temperatures cause \"off-instruction drift\" and weaken correlation certificates. The method only works for deterministic LLM usage, which is rare in practice.\n- The paper shows RuleSHAP can recover injected rules, but provides no evidence it can detect emergent biases. The leap from \"detects rules I programmed\" to \"detects real-world bias heuristics\" is a major unvalidated assumption."}, "questions": {"value": "- You report that non-convex biases are harder (MRR decreases). Can you estimate the complexity threshold where current XAI methods become ineffective?\n- Your evaluation uses exact threshold matching (e.g., \"common ≤ 0.5\" must match exactly). Can you justify why exact matching is appropriate given that gradient boosting learns data-driven splits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K1WcCh2KLz", "forum": "s2vWrgO4OA", "replyto": "s2vWrgO4OA", "signatures": ["ICLR.cc/2026/Conference/Submission19227/Reviewer_WhHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19227/Reviewer_WhHz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243457982, "cdate": 1762243457982, "tmdate": 1762931209972, "mdate": 1762931209972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a heuristic, rule-based explanation method for extracting globally interpretable rules from LLMs and identifying potential biases that may lead to misinformation. The approach combined the strengths of the rule-based explanation RuleFit and the global feature-importance method SHAP. Specifically, the authors used global SHAP values to guide the sampling probabilities of features during rule generation, making more important features more likely to appear in the rule set. Furthermore, when learning feature and rule weights, global SHAP values were used to encourage the retention of important features in the linear explanatory model. Experimental results showed that the proposed method captured biases in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a rule-extraction framework that combines the advantages of RuleFit and SHAP, improving both bias detection and interpretability.\n\n2.\tExperiments conducted across multiple LLMs provide new insights into bias formation within LLMs."}, "weaknesses": {"value": "1. As a heuristic algorithm, despite leveraging SHAP, the method still lacks a relatively reliable theoretical foundation. The main contributions lie in Step 2 and Step 3 (Lines 216–244), where Step 2 uses global SHAP values to guide feature sampling during rule selection, and Step 3 applies global SHAP value weighting into the LASSO regression within RuleFit. While intuitively, it remains unclear whether more principled or theoretically grounded integration strategies could exist. The authors are encouraged to provide theoretical analysis or additional empirical studies to clarify whether RuleSHAP achieves optimal explanatory performance among rule-based methods. \n\n2.\tThe experimental setup and evaluation choices are somewhat unclear. Why do the authors focus specifically on overload, oversimplification, and framing as the three key aspects of LLM bias? Why is rule complexity categorized into univariate, conjunctive, and non-convex types? The relationship between these rule types and real-world LLM biases should be discussed, for example, is actual LLM bias more likely to align with the third category (non-convex) bias?\n\nBesides, it is unclear why mean reciprocal rank (MRR) is used to measure faithfulness. How is faithfulness defined in this context? Can MRR reliably quantify it? What are the advantages and limitations of using this metric? And the correlation analyses suggest that RuleSHAP’s explanations may be partially trusted, but this claim should be supported more rigorously.\n\n3.\tThe paper would benefit from a comprehensive visualization of RuleSHAP's explanations for LLM biases, including the defined symbol mappings, topics, interpretation results, and usage examples. The explanatory scenarios for this method seem somewhat limited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ib8QdTWTac", "forum": "s2vWrgO4OA", "replyto": "s2vWrgO4OA", "signatures": ["ICLR.cc/2026/Conference/Submission19227/Reviewer_oCHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19227/Reviewer_oCHH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762706099097, "cdate": 1762706099097, "tmdate": 1762931209466, "mdate": 1762931209466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}