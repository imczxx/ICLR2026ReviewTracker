{"id": "52svacaBHe", "number": 15444, "cdate": 1758251393849, "mdate": 1763728437616, "content": {"title": "Efficient Stochastic Algorithms for Continual Finite-Sum Minimax Optimization", "abstract": "This paper considers the continual finite-sum convex-concave minimax optimization. We seek a sequence $(x^* _1, y^* _1 ), \\dots, (x^* _n, y^* _n )$ which corresponds to the saddle points of prefix-sum functions $\\\\\\{g _i(x, y) \\coloneqq \\sum _{j=1}^i f _j(x, y) / i\\\\\\} _{i=1}^n$, where each component function $f _j\\colon \\mathbb{R}^{d _x} \\times \\mathbb{R}^{d _y} \\to \\mathbb{R}$ is strongly-convex-strongly-concave and feasible sets $\\mathcal{X} \\subseteq \\mathbb{R}^{d _x}$ and $\\mathcal{Y} \\subseteq \\mathbb{R}^{d _y}$ are convex and compact. We propose an efficient stochastic first-order algorithm that finds a sequence of $\\epsilon$-saddle points for the continual finite-sum minimax optimization problem. In particular, our approach sparsely constructs the full gradient across all stages, and it leverages the extragradient iteration to achieve a sharper incremental first-order oracle complexity compared with existing methods. We also extend our methods to solve the continual finite-sum minimax optimization problem in the general convex-concave setting. Furthermore, we conduct numerical experiments that demonstrate the effectiveness of our approaches.", "tldr": "", "keywords": ["Minimax optimization", "Continual learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41ae88aa76247a6932ae3fab4d45c760e7a372aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyzes algorithms in the setting of continual finite sum minimax optimization. It analyzes the CSVRG algorithm and proposes a novel extension CSVRE that apart from a constant improvement in the strongly convex strongly concave setting, achieves a rate improvement in the convex concave setting. The paper also provides experimental evaluations for robust linear regression to substantiate its claims."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The papers main contributions are the analysis of the CSVRG algorithm for minimax optimization and the proposition and analysis of the CSVRE algorithm for the same setting. \n\nThe analysis of both algorithms is interesting and an addition to the minimax literature. \n\nThe proposition of the novel extra gradient step to alleviate the strong convexity-concavity constant is interesting and even more so the rate improvement it achieves for the convex-concave setting.\n\nThe experimental evaluations corroborate the theoretical findings. They are meaningful extensive and all experiments are under the assumptions and settings over which the authors provide theoretical guarantees."}, "weaknesses": {"value": "The paper has no weakness with respect to its results. The main weakness in my opinion is the writing. The paper presents CSVRG mostly as their own algorithm, while the algorithm was originally presented in [1]. The second algorithm CSVRE is in fact a completely novel contribution therefore I believe that the writing should highlight this difference. In similar cases like in [2], where the authors extended methods from minimization to minimax the algorithms were attributed to the original work.\n\n\n[1] Efficient Continual Finite-Sum Minimization, Ioannis Mavrothalassitis, Stratis Skoulakis, Leello Tadesse Dadi, Volkan Cevher\n\n[2] Frank-Wolfe Algorithms for Saddle Point Problems, Gauthier Gidel, Tony Jebara, Simon Lacoste-Julien"}, "questions": {"value": "My primary question is whether the authors believe the full gradient computation can be fully removed with a variance reduction technique such as STORM [3].\n\n[3] Momentum-Based Variance Reduction in Non-Convex SGD, Ashok Cutkosky, Francesco Orabona"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nbk1xqhVxZ", "forum": "52svacaBHe", "replyto": "52svacaBHe", "signatures": ["ICLR.cc/2026/Conference/Submission15444/Reviewer_G6Jx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15444/Reviewer_G6Jx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760457096710, "cdate": 1760457096710, "tmdate": 1762925730848, "mdate": 1762925730848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the continual incremental finite sum problem in min-max setting. It extends the technique in [Mavrothalassitis 2024 ] to min-max problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is technically sound and easy to follow. The discussion on related work is thorough and sufficient for understanding the contribution of this work."}, "weaknesses": {"value": "A first impression can potentially be that applying continual finite sum algorithm from  [Mavrothalassitis 2024 ] to another finite sum VR algorithm by [Alacaoglu 2022] can lack novelty. More discussions on technical challenge can help understanding the contribution.\n\nMy major concern for the paper, however, are about the rates:\n\n1. We know that EG is faster than GD for min-max problems, why analyze SVRG whose rate is dominated by SVRE?\n2. We know for strongly monotone problems, EG achieve exponential convergence (log(1/ep)), and SVRE in [alacaoglu 2022] achieve exponential convergence + better n dependence. Why can't the proposed method achieve the same dependence in epsilon? Would that cause worse dependence in n / kappa?\n3. How good are the proposed method? Can similar lower bounds as in [Mavrothalassitis 2024] be provided?\n\nI think the setup studied in this work is valid, and the analysis is not necessarily simple. However, it looks like that either the rates are hugely suboptimal, or the work lacks enough discussion on dependence tradeoffs. Further, it is hard to identify techinical challenges in the main text. \n\nI can update my score if the authors could address my questions.\n\nMinor:\n1.line 239: return xT\n2. Would be nice to introduce continual learning optimization setup (e.g., how is suboptimal epsilon defined)\n3. The \"continual finite sum\" is not very well aligned with \"continual training / learning\" problems in practice, because the model should not be able to see new samples when outputing early weights. It seems a more aligned model would be online decisions / streaming algorithms."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vHnc2CTSKi", "forum": "52svacaBHe", "replyto": "52svacaBHe", "signatures": ["ICLR.cc/2026/Conference/Submission15444/Reviewer_yBWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15444/Reviewer_yBWa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760593433250, "cdate": 1760593433250, "tmdate": 1762925729040, "mdate": 1762925729040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers studies continual minimax  optimization in both strongly-convex-strongly-concave and convex-concave settings. Particularly, a prefix problem is considered, where stage $i$ tries to find a solution to cumulative stages from 1 to i. Variance reduction techniques are utilized to establish convergence analysis. Experiments have been conducted on robust linear regression and fairness-aware machine learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) This paper studies minimax optimization for continual learning, which is underexplored as far as the reviewer is aware of.\n\n2) The convergence analysis and results look well-established."}, "weaknesses": {"value": "1) The requirement of full gradients in Algorithm 1, 2 and 3 make them impractical for large-scale problems. \n\n2) The scale of the experiments are generally small with a lot of them showing lower performance than baselines. Larger scale experiments could make the claims better supported. And maybe more data in one stage instead of one data point at a stage. \n\n3) The considered prefix problem needs to revisit all data of previous stages, which makes it less efficient and not consistent with popular frameworks of continual learning. \n\n4) Looks a lot of the technical results are similar to (Mavrothalassitis et al. (2024)), while the latter is not adequately discussed in the submission. For example, Lemma B.1 and others are similar to results of (Mavrothalassitis et al. (2024)). What is the technical connection between this submission to (Mavrothalassitis et al. (2024), and what are the novelty and challenges on top of the that one?"}, "questions": {"value": "How are the baseline methods implemented in experiments to fit in the continual learning setting? \nAlso, see other comments in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rmAEeTdK4U", "forum": "52svacaBHe", "replyto": "52svacaBHe", "signatures": ["ICLR.cc/2026/Conference/Submission15444/Reviewer_hrn8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15444/Reviewer_hrn8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778308867, "cdate": 1761778308867, "tmdate": 1762925728113, "mdate": 1762925728113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles continual finite-sum minimax optimization, where one must solve a sequence of prefix-sum saddle-point problems efficiently. The authors propose two variance-reduced algorithms—CSVRG and CSVRE—that reuse historical gradient information instead of re-solving each stage from scratch. By incrementally updating the full prefix gradient and sparsely refreshing snapshots, they achieve improved IFO complexities in the strongly-convex–strongly-concave case. CSVRE further refines the leading term via an extragradient mid-point."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A new problem setting that integrates continual learning with minimax optimization, addressing the challenge of efficiently updating solutions across sequential tasks.\n2. In particular, the proposed approach sparsely reconstructs the full gradient across stages, significantly reducing redundant computations.\n3. It further employs an extragradient scheme, leading to a tighter complexity in terms of the number of IFO."}, "weaknesses": {"value": "1. The presentation could be improved. For example, consider moving the main results for the convex–concave setting into the main text and omitting some of the immediate corollaries to enhance readability and focus.\n2. Regarding the experimental results, it would be more informative to report performance curves with respect to running time rather than IFO, to better demonstrate the method’s practical effectiveness."}, "questions": {"value": "1. I am a bit confused by Table 1. It seems there is no explicit $\\epsilon$-dependence in the reported results. Could you clarify what convergence metric those baselines use and how it differs from the one adopted in your analysis?\n2. The listed baselines are all designed for stochastic or finite-sum settings. Are there any existing baselines tailored to the continual learning setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YmJhEGz3g2", "forum": "52svacaBHe", "replyto": "52svacaBHe", "signatures": ["ICLR.cc/2026/Conference/Submission15444/Reviewer_Uxcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15444/Reviewer_Uxcr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15444/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882110686, "cdate": 1761882110686, "tmdate": 1762925727523, "mdate": 1762925727523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}