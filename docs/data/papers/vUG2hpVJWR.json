{"id": "vUG2hpVJWR", "number": 17735, "cdate": 1758279929282, "mdate": 1758852746023, "content": {"title": "Compressed Step Information Memory for End-to-End Agent Foundation Models", "abstract": "Large Language Model (LLM) agents excel in tasks like translation, code generation, and decision-making, but consecutive tool calls in complex scenarios lead to excessively long contexts. Despite SOTA LLMsâ€™ 128K+ token context windows, unstructured data interactions easily exceed limits, harming task focus and increasing resource costs.\nExisting solutions have flaws: forced truncation causes information loss, external memory modules lack end-to-end optimization, and context summarization wastes KV cache and loses data.\nTo address this, we propose Compressed Step Information Memory (CSIM), an end-to-end context management method. It compresses post-step context to minimize information loss, retells/updates plans to avoid forgetting and correct errors. Trained via SFT and RL, CSIM achieves strong performance on Gaia and Browsecomp.\nOur contributions: (1) CSIM boosts performance in multi-tool scenarios; (2) A data synthesis and SFT/RL framework distills SOTA agent capabilities; (3) Experiments validate the method on multiple benchmarks.", "tldr": "This paper proposes end-to-end CSIM method for LLM agent context management: it compresses post-step context, updates plans to avoid forgetting, outperforms baselines, and boosts multi-tool scenario performance to solve long-context issues.", "keywords": ["Workflow Optimization", "Agent Reasoning", "Context Management"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fd6e693b4c89c3c962f2d17b6718bda1cd2005a9.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}