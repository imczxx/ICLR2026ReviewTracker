{"id": "1Bf0tToGT1", "number": 19122, "cdate": 1758293735910, "mdate": 1759897058413, "content": {"title": "Diversity-Enhanced Reasoning for Subjective Questions", "abstract": "Large Reasoning Models (LRMs) with long chain-of-thought capabilities, optimized via reinforcement learning with verifiable rewards (RLVR), excel at **objective reasoning** tasks like mathematical problem solving and code generation.\nHowever, RLVR is known for degrading generation diversity, which causes LRMs to fall short on **subjective reasoning** that has multiple answers depending on different role perspectives.\nWhile recent studies recognize the importance of diversity-enhanced training in objective reasoning, limited attention has been given to subjective tasks.\nIn this paper, we find that subjective reasoning can be improved by introducing perspective diversity and token-level diversity, with the former one providing a coherent scaffolding anchored to a real-world stakeholder group and the latter one broadening the answer search space.\nWe propose MultiRole-R1, a diversity-enhanced training framework featuring an unsupervised data construction pipeline that synthesizes reasoning chains incorporating various role perspectives.\nIt also employs reinforcement learning via Group Relative Policy Optimization with reward shaping, taking diversity as a reward signal in addition to verifiable reward.\nTraining on subjective tasks solely, MultiRole-R1 increases the in-domain and out-of-domain accuracy by 14.1% and 7.64%, and even enhances the performance on advanced math reasoning such as AIME 2024.\nWe further show that diversity is a more consistent indicator of accuracy than reasoning length.", "tldr": "", "keywords": ["LLM", "subjective reasoning", "diversity-enhanced training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ea71a7adc34fdd7d4faf347243d1213a533040a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MultiRole-R1, a diversity-enhanced training framework that synthesizes reasoning chains incorporating various role perspectives. Experiment results show that MultiRole-R1 could increase both in-domain and out-of-domain performance. The paper also shows that diversity correlates better with accuracy than reasoning length."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important problem in large reasoning models (LRMs) — the degradation of diversity in their generations. It proposes a method that encourages the model to reason from multiple perspectives, thereby broadening its reasoning process and naturally enhancing output diversity."}, "weaknesses": {"value": "**1. Presentation and Clarity**\nThe presentation of the paper could be significantly improved. In particular:\n\n* Line 208 mentions that the final training set size is **2,700**, but it is unclear what these samples consist of. How many unique questions are included? For each question, what is the average number of responses? How are these samples distributed between SFT and GRPO training?\n* Appendix B.2 provides very limited information on hyperparameter settings and training configurations. Details such as the number of training steps, learning rate, batch size, and other settings should be explicitly reported to improve reproducibility.\n* During evaluation, For evaluating MultiRole-R1 models, it is unclear how roles are sampled from the role pool *per question*. And when evaluating other models, is the role included in the prompt for tasks like GLOQA etc. Clarifying this would help readers better interpret the reported results.\n\n\n**2. Missing Baselines**\nThe evaluation section would benefit from more comprehensive baseline comparisons. For example, in addition to the “More Think” baseline, another meaningful comparison would be a simple method that appends **“wait”**, as done in [1]. This would help isolate whether improvements arise from genuinely multi-perspective reasoning or simply from lengthened thought chains. Moreover, it would be informative to apply the same training procedure (SFT/DPO/GRPO) to this “wait”-based baseline for a fair comparison.\n\n\n**3. Missing Analysis on Inference Time Scaling**\nThe analysis could be deepened by comparing *reasoning-extension* and *perspective-extension* strategies:\n\n* In [1], the model’s reasoning is extended by appending “wait”.\n* In MultiRole-R1, inference-time scaling is achieved by encouraging the model to think from multiple perspectives.\n\nIt would be interesting to analyze how performance scales when adding more perspectives in MultiRole-R1, compared with simply appending “wait.” Such an analysis would provide more insight into *why* perspective diversification improves reasoning and whether it offers advantages beyond longer thinking sequences.\n\n---\n\n**Reference**\n[1] *S1: Simple Test-Time Scaling.*"}, "questions": {"value": "- In the data collection for SFT training, it seems that there is no utilization of the ground truth answers, only self-consistency based filtering is applied. However, I would assume that in the GRPO training, the ground truth answers are used. So why not use the ground truth answers in both cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jbv14jm2gP", "forum": "1Bf0tToGT1", "replyto": "1Bf0tToGT1", "signatures": ["ICLR.cc/2026/Conference/Submission19122/Reviewer_Wtw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19122/Reviewer_Wtw3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702470993, "cdate": 1761702470993, "tmdate": 1762931143524, "mdate": 1762931143524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MultiRole-R1, a training framework to improve Large Reasoning Model (LRM) performance on subjective tasks by addressing the diversity degradation caused by standard RL training. The method enhances both \"perspective diversity\" through multi-role supervised fine-tuning and \"token-level diversity\" via a novel reward shaping in GRPO, resulting in significant accuracy gains on subjective tasks and even generalizing to improve performance on objective math reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The research problem this paper focuses on is critical to the reasoning community.\n2. The paper is easy to follow and well-written.\n3. Experimental results are solid, and the generalization ability in the math domain is also important."}, "weaknesses": {"value": "1. The hyperparameter tuning is problematic. Based on the experimental results in Table 1, the hyperparameter is directly tuned on the test dataset of GLOQA, which is a data leakage problem and harms the reliability of the result. The hyperparameter needs to be tuned on a separate validation set; otherwise, the result is misleading.\n2. The paper’s claim of “diversity of perspectives” remains unsubstantiated. The framework depends on the characters generated in stage one, embodying genuinely diverse viewpoints. Although the authors state that the LRM is prompted to produce roles with conflicting perspectives, the examples in the appendix (Figures 3 and 5) are largely demographic labels—such as “student,” “police,” “atheist,” “female,” and “male.” It remains unclear whether these labels reflect truly distinct and conflicting thinking patterns or merely superficial role differences. No evidence is provided to support the existence of genuine ideological conflict, which is central to the paper’s claim of perspective diversity.\n3. The definition of $R_{div}$ is unclear in the main text.\n4. In the section \"Diversity Weighting\" (Line 407), the details of human annotation are absent. How many human annotators? What is the background of the human annotators? The agreement of human annotation is lacking, which makes the evaluation not so convincing.\n5. In line 42, the paper claims that subjective questions have no definitive right or wrong answers, yet in line 251-252, the proposed method uses rule-based verifiable reward (e.g., binary reward) to determine the correctness of the answer, which is contradictory.\n6. The definition of the diversity metric comprises various metrics; how to determine which plays the most important role in the metric? The equal weight chosen in line 404 is hard to reflect it."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "382GTjugFl", "forum": "1Bf0tToGT1", "replyto": "1Bf0tToGT1", "signatures": ["ICLR.cc/2026/Conference/Submission19122/Reviewer_SwTw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19122/Reviewer_SwTw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754234571, "cdate": 1761754234571, "tmdate": 1762931142999, "mdate": 1762931142999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MultiRole-R1, a two-stage training framework designed to improve subjective reasoning in large reasoning models (LRMs). The method introduces “perspective diversity” via multi-role reasoning chain synthesis and “token-level diversity” via diversity-reward-shaped Group Relative Policy Optimization (GRPO)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an under-explored problem — how to enhance reasoning diversity for subjective questions.\n\n- The proposed framework is conceptually clear and builds on recognizable methods (role-based prompting and RLVR).\n\n- The experiments cover several datasets (BBQ, GLOQA, ETHICS, CALI, CSQA, GSM8K, AIME-2024) and include multiple backbone models.\n\n- The analysis section connects diversity and accuracy correlations in an interpretable way."}, "weaknesses": {"value": "1. Marginal quantitative improvements.\nDespite an elaborate pipeline, the reported gains over strong baselines such as GRPO or “More-Think” are quite small (often within 1–2%), and sometimes inconsistent across datasets (Table 1). The paper frames these as large improvements, but the absolute differences do not seem practically significant, especially on subjective tasks where evaluation itself is noisy.\n\n2. Limited novelty in the algorithmic contribution.\nThe proposed method mainly combines known ingredients — multi-role prompting, self-consistency filtering, and GRPO with an added diversity term. The work lacks theoretical or empirical justification for why this combination uniquely benefits subjective reasoning beyond tuning for diversity.\n\n3. Questionable generality.\nThe notion of “role perspectives” appears tailored to certain cultural or ethical QA datasets, but may not generalize to broader subjective domains such as aesthetics, creativity, or preference modeling. The definition of role diversity seems ad-hoc and dataset-specific.\n\n4. Evaluation concerns.\n\n- The “diversity metric” (weighted combination of lexical and structural features) is loosely motivated and human alignment scores (Table 4) are based on very few samples.\n\n- The comparison to other diversity-aware RL methods (e.g., entropy-enhanced GRPO, entropy-regularized RLHF) is missing. It remains unclear whether the diversity reward simply duplicates existing entropy terms.\n\n- For fair comparison, the authors should clarify whether baselines used equivalent sampling budgets and diversity-reward normalization.\n\n4. Over-interpretation of correlations.\nThe observed correlation between diversity and accuracy (r ≈ 0.74) may reflect dataset artifacts rather than causal effects. No ablation shows that removing the diversity term alone consistently degrades performance across tasks."}, "questions": {"value": "1. How exactly does the proposed diversity-reward differ from the entropy-enhanced GRPO (e.g., Cui et al., 2025)? Is it redundant with policy entropy?\n\n2. How is “perspective diversity” quantified during RL? Are roles re-sampled dynamically, or fixed from SFT?\n\n3. Could the performance gains be attributed simply to longer or more diverse fine-tuning data rather than the GRPO stage?\n\n4. What is the variance of human evaluation on subjective datasets — is a 1–2% gain statistically meaningful?\n\nCui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., ... & Ding, N. (2025). The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uDUp411kTm", "forum": "1Bf0tToGT1", "replyto": "1Bf0tToGT1", "signatures": ["ICLR.cc/2026/Conference/Submission19122/Reviewer_7sKv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19122/Reviewer_7sKv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974296308, "cdate": 1761974296308, "tmdate": 1762931142522, "mdate": 1762931142522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the diversity collapse of RL-trained reasoning models on subjective questions and proposes MultiRole-R1, a two-stage framework combining multi-role reasoning path synthesis and GRPO-based reinforcement learning with diversity-aware reward shaping. Trained solely on subjective data, MultiRole-R1 yields significant gains on in-domain (+14.1%) and out-of-domain (+7.6%) benchmarks and even improves math reasoning (AIME 2024 +5.8%). The results indicate that reasoning diversity—not length—correlates more strongly with accuracy, highlighting diversity as a key factor for effective reasoning across domains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel problem focus. The paper addresses an underexplored yet important gap—reasoning diversity in subjective questions—where existing RLVR methods optimized for objective correctness tend to fail.\n2. Insightful findings. The observation that diversity correlates more strongly with accuracy than reasoning length offers a new perspective on how reasoning quality may scale, potentially influencing future RL-for-reasoning research.\n3. Strong empirical results. The model achieves large gains on multiple benchmarks, including +14.1% in-domain and +7.6% out-of-domain improvements, and even boosts mathematical reasoning (AIME 2024 +5.8%), showing surprising cross-domain generalization."}, "weaknesses": {"value": "1. Heuristic role synthesis. The generation of role perspectives is heuristic and lacks quantitative validation to ensure that the synthesized roles truly represent distinct or complementary viewpoints rather than superficial differences.\n2. Insufficient ablation and qualitative analysis. The paper lacks fine-grained ablation to disentangle the contributions of individual diversity components, and provides limited qualitative evidence that multi-role reasoning genuinely captures diverse perspectives rather than surface linguistic variations.\n3. Limited evaluation scope. Experiments focus primarily on subjective QA datasets (BBQ, GLOQA, ETHICS, CALI), leaving unclear whether the proposed method generalizes to other subjective or creative reasoning tasks."}, "questions": {"value": "1. How are the generated roles guaranteed to represent genuinely distinct perspectives rather than semantically similar paraphrases?\n2. The diversity metric combines eight heterogeneous measures with equal weighting — did you experiment with alternative weighting schemes or automatic calibration (e.g., PCA or learned weighting)?\n3. Have you tested whether the same diversity-oriented training improves performance on more open-ended or creative subjective tasks (e.g., ethical debates, story generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "swYI6VZugl", "forum": "1Bf0tToGT1", "replyto": "1Bf0tToGT1", "signatures": ["ICLR.cc/2026/Conference/Submission19122/Reviewer_EQND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19122/Reviewer_EQND"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762554194272, "cdate": 1762554194272, "tmdate": 1762931141989, "mdate": 1762931141989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}