{"id": "jjWPr6maKd", "number": 18017, "cdate": 1758282956297, "mdate": 1759897139120, "content": {"title": "XGC-AVis: Towards Audio-Visual Content Understanding with a Multi-Agent Collaborative System", "abstract": "In this paper, we propose **XGC-AVis**, a multi-agent framework that enhances the audio-video temporal alignment capabilities of multimodal large models (MLLMs) and improves the efficiency of retrieving key video segments through $4$ stages: perception, planning, execution, and reflection. We further introduce **XGC-AVQuiz**, the first benchmark aimed at comprehensively assessing MLLMs' understanding capabilities in both real-world and AI-generated scenarios. XGC-AVQuiz consists of $2,685$ question-answer pairs across $20$ tasks, with two key innovations: 1) **AIGC Scenario Expansion:** The benchmark includes $2,232$ videos, comprising $1,102$ professionally generated content (PGC), $753$ user-generated content (UGC), and $377$ AI-generated content (AIGC). These videos cover $10$ major domains and $53$ fine-grained categories. 2) **Quality Perception Dimension:** Beyond conventional tasks such as recognition, localization, and reasoning, we introduce a novel quality perception dimension. This requires MLLMs to integrate low-level sensory capabilities with high-level semantic understanding to assess audio-visual quality, synchronization, and coherence. Experimental results on XGC-AVQuiz demonstrate that current MLLMs struggle with quality perception and temporal alignment tasks. XGC-AVis improves these capabilities without requiring additional training, as validated on two benchmarks. The project page is available at: https://xgc-avis.github.io/XGC-AVis/", "tldr": "", "keywords": ["multi-agent", "quality perception", "multimodal large language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/932e85f3b6541d2e6bc64af1022f52964996810a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "XGC-AVis introduces a training-free multi-agent pipeline: (1) Perception, (2) Planning, (3) Execution and (4)Reflection, which enhance temporal alignment and quality perception in MLLMs. Inside XFC-AVis, two planners identify time segments relevant to a question, two executors reason over them, and a decider resolves conflicts, producing final answers. \n\nThe paper also presents XGC-AVQuiz, a new benchmark with 2,232 videos and 2,685 QA pairs spanning PGC, UGC, and AIGC sources across four categories (recognition, localization, reasoning, and quality perception). Experiments show consistent gains over both open- and closed-source MLLMs. Ablation results confirm that the dual-planner and decider design is key to improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Writing is logical and easy to follow, motivation is clear\n- The paper introduces a novel multi-agent pipeline (Perception–Planning–Execution–Reflection) that improves A/V reasoning without extra training.\n- Evaluations cover multiple open- and closed-source MLLMs, two benchmarks, and detailed ablations."}, "weaknesses": {"value": "- The first weakness is the multi-agent framework increases computational cost as slove one question, needs more llms/models envloves in. The author should quantifying latency or cost trade-offs.\n- I appreciate the author propose a new benchmark, and do a lot testing on this benchmark. However, some tasks in XGC-AVQuiz overlap conceptually with existing A/V QA datasets (e.g., AVQA, Daily-Omni), can the author also report on those datasets, and compare with traditional models?\n- For the model: while the multi-agent design is novel, it mainly combines existing reasoning and planning components rather than introducing a fundamentally new learning mechanism or model architecture."}, "questions": {"value": "Please see weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BCmN52HsAh", "forum": "jjWPr6maKd", "replyto": "jjWPr6maKd", "signatures": ["ICLR.cc/2026/Conference/Submission18017/Reviewer_yfqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18017/Reviewer_yfqm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974584578, "cdate": 1761974584578, "tmdate": 1762927809524, "mdate": 1762927809524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XGC-AVis, a four-stage (perception, planning, execution, reflection) multi-agent framework designed to enhance audio-visual temporal alignment and key segment retrieval for multimodal large language models (MLLMs). It also introduces XGC-AVQuiz, a benchmark with 2,232 videos (PGC/UGC/AIGC) and 2,685 QA pairs covering 20 tasks, including a novel \"quality perception\" dimension. Experimental results claim XGC-AVis outperforms existing MLLMs on XGC-AVQuiz and Daily-Omni."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The XGC-AVQuiz benchmark attempts to address the limitation of existing datasets by integrating PGC, UGC, and AIGC scenarios, and introduces a \"quality perception\" task to evaluate MLLMs’ ability to assess audio-visual synchronization and coherence—an underexplored area in current benchmarks.\n\n- The four-stage multi-agent design of XGC-AVis provides a approach to tackle audio-visual temporal alignment, which is a known challenge for MLLMs."}, "weaknesses": {"value": "- The paper is lack of novelty. It claims to enhance audio-video understanding of MLLMs via XGC-AVis, but it fails to address the fundamental issue of poor fine-grained capabilities in audio-video MLLMs. XGC-AVis only adopts a \"component-stitching\" approach (e.g., using Deepgram for subtitles and r1-aqa for audio descriptions, Section 3) to interweave unimodal outputs, without realizing feature-level or semantic-level deep cross-modal interaction—leaving the core problem of weak fine-grained cross-modal association unresolved\n- The paper explicitly states that XGC-AVis \"interweaves video frames, audio segments, subtitles, and audio descriptions to form coherent multimodal units\" (Section 3, 1-26), but it completely ignores the feature space gap between audio and visual modalities—an issue that directly undermines the \"coherence\" of the resulting multimodal units.\n- Directly concatenating or interleaving unaligned features forces the subsequent planners/executors  to handle inconsistent feature spaces, which can lead to two critical issues: The model may over-rely on one modality (e.g., visual features, which are more structured) and ignore audio features, weakening the intended \"multimodal collaboration\". The paper does not acknowledge or address these risks.\n- XGC-AVis integrates multiple expert models/tools (Deepgram, r1-aqa, Aria, Qwen2.5-Omni, Gemini 2.0 Flash) to form a multi-expert ensemble system, while the comparison baselines in Table 2 are standard single MLLMs . This comparison is unfair, as XGC-AVis leverages \"collective intelligence\" and higher computational costs that standard MLLMs lack, yet the paper only emphasizes performance advantages without acknowledging such trade-offs.\n- The paper uses Aria (Planner 1) and Qwen2.5-Omni (Planner 2), but Aria \"can only process video and text\" while Qwen2.5-Omni \"processes video, audio, and text\" (Appendix A.3). Why combine a unimodal (video-text) planner with a multimodal (video-audio-text) planner? Does this combination introduce redundancy, and if not, what unique value does each planner provide?\n- Tables 2 only report \"average accuracy\" for broad tasks (e.g., A/V Localization) but not task-specific metrics for time alignment—such as Temporal Localization Accuracy (TLA): The percentage of cases where the model’s predicted time segment overlaps with the ground-truth by ≥50% IoU\n- The \"perception\" stage uses both Deepgram (speech-to-subtitle) and r1-aqa (audio descriptor). However, the paper does not clarify whether their outputs are redundant—for example, if Deepgram’s subtitles already capture speech content, what unique information does r1-aqa’s audio description add (e.g., background noise, emotion)?\n- Generally, audio and video in datasets are consistent (Section 4.1), so audio should only bring marginal improvements for most tasks. However, Table 2 shows a significant performance boost when adding audio (e.g., Qwen2.5-Omni’s A/V Recognition accuracy jumps ~16% from Vid. to Vid.+Aud.). This is unusual. Additionally, if video-only inputs already achieve high accuracy on supposed \"audio-driven\" tasks, it implies potential flaws in the evaluation’s task design, undermining its credibility.\n- The paper does not clarify whether the questions in Table 2 differ between the \"video-only\" and \"video+audio\" settings. Without disclosing the distribution of modality-dependent questions, the observed performance boost from adding audio may stem from question design bias (forcing audio reliance) rather than true multimodal understanding, making the results uninterpretable.\n- The authors do not evaluate whether questions can be answered with a single modality (e.g., audio-only). The experimental input settings (Section 5.1) only include video-only, video+audio, and video+audio+subtitle, without testing audio-only inputs. This omission means it is impossible to verify if multimodal integration is truly necessary—for example, some A/V reasoning tasks may be solvable via video alone (facial expressions) or audio alone (tone), rendering XGC-AVis’s multimodal design redundant."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SFZ9KP8joh", "forum": "jjWPr6maKd", "replyto": "jjWPr6maKd", "signatures": ["ICLR.cc/2026/Conference/Submission18017/Reviewer_e7WQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18017/Reviewer_e7WQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982667724, "cdate": 1761982667724, "tmdate": 1762927808801, "mdate": 1762927808801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **XGC-AVis**, a multi-agent framework designed to enhance multimodal large language models’ (MLLMs) ability to align audio and video temporally and to improve the efficiency of retrieving key video segments. The system operates through four stages—perception, planning, execution, and reflection—enabling coordinated agent interaction for more robust multimodal reasoning.\n\nTo evaluate such capabilities, the authors also propose **XGC-AVQuiz**, a new benchmark that comprehensively assesses MLLMs’ understanding **across real-world and AI-generated scenarios**. XGC-AVQuiz features both professionally generated (PGC), user-generated (UGC), and AI-generated (AIGC) videos spanning multiple domains, and it introduces a quality perception dimension that tests synchronization, coherence, and audio-visual quality awareness. Experimental results suggest that existing MLLMs struggle in these aspects, while XGC-AVis achieves measurable improvements without additional training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**High-quality benchmark contribution.**\nXGC-AVQuiz provides a more comprehensive, multi-level evaluation benchmark for multimodal reasoning, covering diverse data sources (PGC, UGC, AIGC) and introducing the novel dimension of quality perception. This significantly enriches the evaluation landscape for audio–visual MLLMs.\n\n**Exploratory insights into multi-agent design.**\nXGC-AVis presents valuable design insights for constructing multi-agent collaborative systems, offering a practical exploration into how workflow-level designs can help MLLMs overcome model-level limitations when dealing with complex multimodal understanding tasks."}, "weaknesses": {"value": "- **Lack of empirical support for efficiency claims.**  \n  The paper states that **XGC-AVis** *\"improves the efficiency of retrieving key video segments”*, but presents no quantitative evidence or analysis to support this. Without metrics or user-study results, this claim remains unsubstantiated.\n\n- **Insufficient citations and context in the introduction.**  \n  The discussion of MLLMs and representative models cites mainly vision–language (e.g., Llava-OneVision, InternVL3) or pure language models , but fails to reference existing multimodal reasoning efforts that go beyond surface-level fusion [1–3]. This weakens the argumentative foundation of the introduction.\n\n- **Unconvincing “data source bias” argument.**  \n  The claim that existing datasets are dominated by user-generated content (UGC) and lack professionally generated content (PGC) is poorly justified. Given that platforms like YouTube already contain high-quality professional material, the UGC–PGC distinction should be better defined or empirically supported.\n\n- **Loose connection between Related Work and the proposed system.**  \n  The Related Work section focuses mainly on architectural summaries of MLLMs, while XGC-AVis is a workflow-level, multi-agent system. To strengthen positioning, the authors should include relevant **agent-level** or **system-level** works such as [4–5].\n\n- **Ambiguous baseline description.**  \n  It is unclear whether *VideoLLaMA2* or *VideoLLaMA2.1-AV* is used as the baseline. These two differ substantially in multimodal input handling, and the choice directly affects the validity of the comparison.\n\n- **Limited comparative and generalization experiments.**  \n  (1) Table 1 compares only VLM and Qwen2.5-Omni; adding *Qwen2.5-VL* would provide a fairer multimodal baseline.  \n  (2) Reporting XGC-AVis’s performance on an external benchmark such as *WorldSense* would help demonstrate generalizability.\n\n- **Missing ablations on key system components.**  \n  While the data ablations are informative, the paper lacks **component-level** analysis. For example, how does the *Interleave* step in “Align multimodal data” affect performance? Additionally, replacing *Gemini 2.0 Flash* (executors/decider) with open-source alternatives would help isolate whether gains stem from the multi-agent design or the strength of the base models.\n\n---\n# Reference\n\n[1] Aligned Better, Listen Better for Audio-Visual Large Language Models, ICLR 2025, https://arxiv.org/abs/2504.02061\n\n[2] HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding, https://arxiv.org/abs/2501.15111\n\n[3] SALMONN family: A suite of advanced multi-modal LLMs,https://github.com/bytedance/SALMONN\n\n[4] ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions, https://arxiv.org/abs/2505.14668\n\n[5] Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities"}, "questions": {"value": "The key concerns are outlined in the weaknesses section. Should the authors **provide convincing responses or improvements** addressing these points, **I would be open to increasing my evaluation score**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c3WdYhIMFR", "forum": "jjWPr6maKd", "replyto": "jjWPr6maKd", "signatures": ["ICLR.cc/2026/Conference/Submission18017/Reviewer_9Q2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18017/Reviewer_9Q2y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984212788, "cdate": 1761984212788, "tmdate": 1762927808456, "mdate": 1762927808456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted manuscript proposes XGC-AVis a multi-agent framework designed to enhance temporal alignment and audio-visual reasoning capabilities in MLLMs. The authors also introduce XGC-AVQuiz, a comprehensive benchmark comprising 2,232 videos covering both real-world and AI-generated content and 2,685 question–answer pairs, aimed at evaluating recognition, localization, quality perception, and reasoning abilities in audio-visual tasks. Extensive experiments verify the effectiveness of XGC-AVis, demonstrating a certain degree of novelty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript clearly defines the problem and provides a well-motivated solution.\n2. The construction of the XGC-AVQuiz benchmark is valuable and contributes to the development of the audio-visual research community.\n3. The proposed XGC-AVis framework achieves superior performance across multiple benchmarks, and the ablation studies are comprehensive.\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The related work section omits several directly relevant recent studies on multi-agent, multimodal, and video question-answering architectures and benchmarks, such as *MAGNET*, *VideoMultiAgents*, and *OmAgent*. The authors are encouraged to discuss distinctions from these works.\n2. Given that MLLMs are used to generate distractors, how do the authors ensure that the benchmark does not inadvertently favor architectures used for generation or self-reflection?\n3. Compared to single-agent or monolithic MLLM designs, how does XGC-AVis perform in terms of computational cost, especially for longer video inputs?\n4. Since each query involves multiple *planner–executor* agents, the computational cost could be significant; a discussion or analysis of scalability is suggested.\n5. Some experimental details require further clarification, for example, the input strategy for candidate time segments and the handling of inconsistent answers.\n6. It would be helpful to elaborate on how conflicting segmentation results from different planners are weighted or fused during decision integration."}, "questions": {"value": "My main questions are reflected in the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sh1p7i50Vx", "forum": "jjWPr6maKd", "replyto": "jjWPr6maKd", "signatures": ["ICLR.cc/2026/Conference/Submission18017/Reviewer_bVyF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18017/Reviewer_bVyF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018688141, "cdate": 1762018688141, "tmdate": 1762927808187, "mdate": 1762927808187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}