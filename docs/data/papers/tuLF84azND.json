{"id": "tuLF84azND", "number": 14117, "cdate": 1758228660149, "mdate": 1759897389027, "content": {"title": "Sequential Parallel Duality in Prefix Scannable Models", "abstract": "Modern neural sequence models are designed to meet the dual mandate of parallelizable training and fast sequential inference. Recent developments have given rise to various models, such as Gated Linear Attention (GLA) and Mamba, that achieve such ``sequential-parallel duality.'' This raises a natural question: can we characterize the full class of neural sequence models that support near-constant-time parallel evaluation and linear-time, constant-space sequential inference? We begin by describing a broad class of such models -- state space models -- as those whose state updates can be computed using the classic parallel prefix scan algorithm with a custom associative aggregation operator. We then define a more general class, Prefix-Scannable Models (PSMs), by relaxing the state aggregation operator to allow arbitrary (potentially non-associative) functions such as softmax attention. This generalization unifies many existing architectures, including element-wise RNNs (e.g., Mamba) and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new models with softmax-like operators that achieve O(1) amortized compute per token and log(N) memory for sequence length N. We empirically evaluate such models on illustrative small-scale language modeling and canonical synthetic tasks, including state tracking and associative recall. Empirically, we find that PSMs retain the expressivity of transformer-based architectures while matching the inference efficiency of state space models -- in some cases exhibiting better length generalization than either.", "tldr": "Propose new architecture Transformer-PSM that generalizes state space models to use softmax attention", "keywords": ["state space models", "linear RNNs", "linear transformers", "sequence modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/564c4362aecbdb2c5c45e18a916f1e0a22821d85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Prefix-Scannable Models (PSMs), a unified framework for sequence models that enables both parallel training and efficient sequential inference. By introducing a fixed parenthesization (bracket) structure in the prefix-scan algorithm, PSM extends beyond associative operations, allowing non-associative functions like softmax attention to be trained and decoded efficiently."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "– PSM generalizes a wide range of recent models under a single mathematical formulation. It reframes these architectures as special cases of prefix-scannable systems, giving conceptual clarity to an area previously defined by ad hoc designs.\n\n– By fixing a binary-tree structure for prefix computation, the authors bypass the associativity constraint that limited prior parallel scan models. This makes it possible to handle attention-like, non-linear interactions while retaining log-depth parallelism.\n\n– Experiments demonstrate that Transformer-PSM achieves near-Transformer accuracy (only slightly higher perplexity) while matching Mamba’s inference speed. This combination of expressivity and efficiency is rare and valuable for long-sequence modeling."}, "weaknesses": {"value": "– Results are limited to moderate-scale models on synthetic and mid-size text datasets; it’s unclear whether the same advantages hold at billion-parameter scale or in real industrial workloads.\n\n– The fixed-tree scanning and binary-counter inference mechanisms, while elegant conceptually, may be hard to integrate into existing frameworks without specialized kernels. I'd like to hear the author's explanation about this aspect.\n\n– Model quality strongly depends on the chosen chunk length, adding a new hyperparameter that requires tuning for each task.\n\n– The paper focuses heavily on theory and performance metrics but provides little intuition or visualization about how information actually propagates through the prefix structure. A detailed explanation or visualization would be helpful. Although Figures 2 and 3 help illustrate the overall architecture and performance trends, the paper still offers limited qualitative insight into the internal behavior of PSMs. No visual analysis of prefix states, attention dynamics, or information propagation is provided."}, "questions": {"value": "- How robust is the fixed parenthesization choice? Would learning or adapting the tree structure dynamically lead to better trade-offs between accuracy and efficiency?\n\n- Could this framework be extended to bidirectional or hierarchical scanning, potentially improving global context modeling?\n\n- Could this theory inspire a hardware-aware implementation, where the prefix-scan tree maps directly onto GPU or TPU parallel primitives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0JM4ZnrY6m", "forum": "tuLF84azND", "replyto": "tuLF84azND", "signatures": ["ICLR.cc/2026/Conference/Submission14117/Reviewer_k4pr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14117/Reviewer_k4pr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760765581647, "cdate": 1760765581647, "tmdate": 1762924586416, "mdate": 1762924586416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes Prefix-Scannable Models (PSMs), a class of sequence models whose training graph is a Blelloch prefix scan over chunk encodings with a chunk-local predictor head. This framework yields SPD-$(n, \\\\log n)$ complexity, entailing $\\\\Theta(n)$ work with logarithmic parallel depth during training. For inference, an online binary-counter algorithm reproduces the exact same computation with $O(1)$ amortized compute per token and $O(c \\\\log (n/c))$ memory (which is $O(\\\\log n)$ when $c$ is fixed). An implementation, Transformer-PSM, demonstrates on WikiText-103 that perplexity improves as $c$ increases, while per-token latency remains bounded in contrast to a full-context Transformer. The theory unifies associative scan-based models as a special case and correctly extends the framework to non-associative aggregation operators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear formalization of PSMs and their sequential-parallel duality, including precise analysis of training and inference complexity.\n* A key theoretical result is the correctness of the online binary-counter evaluation relative to the static training-time tree, which resolves the parenthesization ambiguity for non-associative operators.\n* Experimental results show that perplexity improves with a larger chunk size $c$, while inference latency remains weakly dependent on context length, unlike a standard Transformer."}, "weaknesses": {"value": "* The evaluation of efficiency lacks system-level measurements. Practical metrics such as training throughput (e.g., tokens/sec), wall-clock time, and peak memory usage are not reported, which limits the assessment of the model’s practical viability.\n* The trade-off between accuracy and latency is not fully explored. For large $c$, latency is dominated by the chunk-local predictor. The paper notes the $O(c)$ amortized compute and the quadratic ($O(c^2)$) attention cost within the Inf module, but does not map the accuracy–latency frontier across different values of $c$. This analysis is critical for deployment."}, "questions": {"value": "* On WikiText-103, Fig. 5 reports perplexity vs. chunk size $c$, and Fig. 6 shows per-token time vs. token index for one $c$ (on WikiText-2). To make the accuracy–efficiency trade-off actionable with minimal overhead, please add a small table over $c \\\\in \\\\{32, 64, 128, 256\\\\}$ reporting (i) the existing WikiText-103 perplexity, (ii) mean per-token inference latency on one long input (e.g., $L=40{,}000$ on WikiText-2; discard the first $c$ tokens for warm-up, then average per-token time over $t \\\\in [c, L]$), and (iii) peak inference memory for that run; optionally include inference throughput (tokens/s).\n* Could the authors include system-level training metrics (tokens per second, total wall-clock time, peak memory) for the Transformer-PSM and baselines on at least one of the presented tasks?\n* For the MQAR experiment, please consider adding a sliding-window Transformer baseline with a matched compute budget. A summary table of all baseline configurations would also improve reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ivdxNUhVrN", "forum": "tuLF84azND", "replyto": "tuLF84azND", "signatures": ["ICLR.cc/2026/Conference/Submission14117/Reviewer_Koqn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14117/Reviewer_Koqn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929570642, "cdate": 1761929570642, "tmdate": 1762924585903, "mdate": 1762924585903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the core tradeoff between parallelizable training and efficient sequential inference in neural sequence models. It formalizes the Sequential-Parallel Duality (SPD), a property where models support near-constant-depth parallel training and linear-time/constant-space sequential inference, and introduces a generalized model class called Prefix-Scannable Models (PSMs) to unify and extend existing efficient architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates originality through three key contributions. First, it introduces a unified theoretical framework (Prefix-Scannable Models, PSMs) that formalizes sequential-parallel duality (SPD), a core property of modern efficient sequence models (e.g., Mamba, GLA, RetNet). This unification resolves the fragmentation of prior work by showing that diverse architectures are special cases of PSMs with associative aggregation operators. Second, it generalizes PSMs to non-associative aggregation functions (e.g., softmax attention), expanding the model design space beyond existing affine/linear constraints while retaining efficient inference. Third, the instantiation of Transformer-PSM is a creative combination: it merges Transformer-style self-attention (for expressive power) with PSM’s linear-time inference (for efficiency), addressing a long-standing tradeoff between performance and scalability.\n\n2. It provides strict formal definitions (e.g., SPD, state kernel, PSM) and rigorous proofs (e.g., Proposition 3.2, Theorem 3.5) for complexity guarantees (O(n) training compute, O(log n) inference memory). Appendixes supplement detailed derivations (e.g., affine recurrence proofs, computational complexity analysis) to support core claims. Empirically, the experiment design is comprehensive and controlled: it targets three canonical tasks (state tracking S₅, multi-query associative recall, WikiText-103 language modeling) and compares against strong baselines (GPT-2, Mamba, Sliding Window Transformer). Results are interpretable (e.g., Fig. 3 shows superior length generalization; Fig. 6 validates constant inference latency) and reproducible (detailed training protocols, public code plan).\n\n3. Theoretically, it deepens the connection between parallel prefix-scan algorithms and neural sequence modeling, providing a general framework for analyzing and designing efficient models—this will guide future research on SPD-compatible architectures (e.g., log-linear attention, as noted in the discussion). Practically, it addresses critical limitations of existing models: Transformer-PSM matches Transformer’s expressive power (e.g., perfect associative recall) while achieving Mamba-like linear inference efficiency (e.g., constant per-token latency for long sequences). This balance is transformative for real-world applications (e.g., long-document processing, streaming NLP) where both training parallelism and inference scalability are essential. Additionally, the paper’s length generalization results (e.g., S₅ task scaling to 160+ tokens) open new avenues for modeling algorithmic tasks that require precise state tracking."}, "weaknesses": {"value": "1. The paper’s core contribution of generalizing PSMs to non-associative aggregation operators is underdeveloped, as it only instantiates `Aggθ` with Transformer-style softmax attention (Transformer-PSM). The design space of non-associative operators (e.g., additive attention variants, kernelized attention, or task-specific non-associative functions) remains unexplored. This raises critical questions: Are PSM’s advantages (e.g., length generalization on `S5` tasks) inherent to the prefix-scan framework, or specific to softmax attention? Without testing alternative non-associative `Aggθ` implementations, the paper fails to validate that PSMs truly expand the efficient sequence model design space beyond softmax-based variants.\n\n2. The paper demonstrates that chunk size (`c`) significantly impacts performance (e.g., Transformer-PSM with `c=64` achieves perfect accuracy on MQAR, while `c=32` degrades on long sequences; Fig. 4) and perplexity (Fig. 5). The paper does not quantify the tradeoff between chunk size, computation cost (e.g., `O(c²)` for chunk-local self-attention), and memory usage (e.g., `O(c log(n/c))`). How does `c` scale with sequence length (e.g., optimal `c` for 1k vs. 10k token sequences)?\n\n\n3. Missing Ablation Studies on Core PSM Components. \nThe PSM framework is defined by three modules: `Enc` (chunk encoder), `Aggθ` (aggregator), and `Infφ` (inference head). However, the paper does not perform component-wise ablation studies to isolate which modules drive PSM’s performance advantages. Does the simple embedding `Enc` (used in Transformer-PSM) limit performance, or is a more complex encoder (e.g., convolutional, hierarchical) unnecessary?"}, "questions": {"value": "1. The paper uses the Blelloch scan’s fixed binary-tree parenthesisation for non-associative aggregators (e.g., softmax attention). Is there a theoretical or empirical justification for this specific parenthesisation over alternative groupings (e.g., left-to-right sequential, random binary trees)? Could different parenthesisations impact task performance or complexity?\n\n2. For the Multi Query Associative Recall (MQAR) task, Transformer-PSM with chunk size 64 achieves perfect accuracy, but chunk size 32 degrades performance on long sequences (512 tokens). What causes this sensitivity to chunk size, and is there a principled way to select chunk size for different tasks without grid search?\n\n3. The paper compares Transformer-PSM to GPT-2, Mamba, and Sliding Window Transformer (SWT). Why were other efficient models (e.g., RetNet, GLA, xLSTM’s mLSTM) not included in the comparisons, especially for language modeling or associative recall? How would Transformer-PSM perform against these models on the same benchmarks?\n\n4. The paper claims O(log n) inference memory for PSMs, but chunk size (c) interacts with this scaling (Appendix C). For large chunk sizes (e.g., c=256), does the memory scaling still hold, or does the chunk-local KV cache in the Inf module dominate memory usage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rLiQZl6UBx", "forum": "tuLF84azND", "replyto": "tuLF84azND", "signatures": ["ICLR.cc/2026/Conference/Submission14117/Reviewer_LR1S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14117/Reviewer_LR1S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958397637, "cdate": 1761958397637, "tmdate": 1762924584470, "mdate": 1762924584470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an algorithmic framework, namely \"*Prefix-Scannable Models*\" (PSMs), to characterize and generalize sequence models that support both parallelizable training and efficient sequential inference. The authors term this property \"Sequential-Parallel Duality\" (SPD) and formalize it with the notation SPD(T(n), m(n)), capturing the training compute $T(n)$ and inference memory $m(n)$.\nThe authors first unify a large class of recent efficient models (e.g., Mamba, GLA, DeltaNet, etc.) under this definition. They show that these models are all instances of an associative prefix scan based on a shared \"affine state update\". Because their aggregation operator is associative, they can achieve SPD(n, 1) compared to the SPD($n^2, n$) of Transformers. Afterwards, to generalize the framework to non-associative aggregation operators (such as softmax attention), the classic Blelloch parallel prefix scan is used for training, which fixes a specific binary-tree parenthesization. The crucial insight is proposing an online \"binary counter\" algorithm for inference (Alg. 2) that exactly reproduces this same parenthesization, guaranteeing training-inference consistency. This generalization creates a new class of models, including the proposed *Transformer-PSM*, that achieve SPD(n, log n) with $O(1)$ amortized inference time. Experiments show that Transformer-PSM achieves very good length generalization on state-tracking tasks, and at the same time matching the performance of a full Transformer on associative recall and language modelling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Methodological and technical contributions.** I think that the clean, algorithmic lens it provides for understanding and unifying the recent \"zoo\" of efficient sequence models is a significant conceptual contribution to the field. Formalizing the SPD(T, m) property, unifying Linear RNNs, SSMs and Transformers, and the transition from associative to non-associative operators is a non-trivial and useful generalization. Technically, using the online binary counter (Alg. 2) to exactly match the Blelloch scan's fixed parenthesization is another instance of applying a classic algorithm to solve a modern deep learning problem. \n\n2. **Empirical evaluations**. The proposed Transformer-PSM model shows very good length generalization result on the $S_5$ state-tracking task (Figure 3), up to sequences >6x longer than the longest training sequence. Furthermore, the MQAR result in Figure 4 show that the Tranformer-PSM can match the recall power of a full Transformer, while the WikiText-103 experiment (Figure 5) shows the trade-off between chunk size and perplexity on language modelling, validating the model's ability to achieve RNN-like efficiency and Transformer-like expressivity on these tasks.\n\n3. **Clarity and Structure**: The paper is well-written, and the use of formal definitions (Def 2.5 for SPD, Def 3.1 for PSM) and algorithms (Alg 1, 2, 3, 4) make the core ideas succinct and easy to follow."}, "weaknesses": {"value": "1. **Limited scale**: The experiments are on small-to-medium datasets (WikiText-103) with relatively small models. While the synthetic results are interesting, it remains to be verified how the architectural benefits and trade-offs (especially $O(\\log n)$ memory) of Transformer-PSM scale to state-of-the-art compute with billions of parameters.\n\n2. **Missing baselines and other related work**: While the theoretical unification is broad, the baselines used in the empirical comparisons are narrow, focusing almost exclusively on GPT-2 and Mamba. I think the paper would greatly benefit if the authors would also compare to several highly relevant lines of concurrent work such as:\n   - *Transformer-SSM Hybrids*: Models like Jamba, Zamba, etc., which interleave Transformer and SSM layers.\n   - *Log-Linear Attention*: The paper cites \"Log-linear attention\" (Guo et al., 2025) as concurrent work that \"fits this view\" but provides no empirical comparison, even though it appears to be a direct competitor in the SPD(n, log n) class.\n   - *Parallel Non-linear RNNs*: Other approaches to parallelizing non-linear recurrences, such as those based on fixed-point iteration (e.g., DEER and ELK), offer a completely different algorithmic approach to the same problem (parallelizing non-linearity) and should be discussed or benchmarked.\n   - *Linear RNNs with enhanced expressivity*: Methods such as DeltaProduct, that have been shown to work great on state-tracking tasks too."}, "questions": {"value": "1. How does Transformer-PSM compare to recent hybrids which alternate Transformer and SSM layers? Do such models fall in the SPD framework too? What are the hypothesized trade-offs in terms of expressivity and parallelization between these two different ways of combining Transformers and efficient recurrences?\n\n2. You successfully parallelize a non-linear operator by restricting its dependencies to a fixed binary-tree structure. Concurrent work like DEER and ELK provides an algorithmic alternative for parallelizing general non-linear RNNs via fixed-point iteration. Could you compare these two fundamental approaches?\n\n3. I would be interested to see more results on other state-tracking tasks such as the ones used in Grazzi et al. 2025. Can the authors provide some results on those tasks, e.g. parity, modular arithmetic w/ and w/o brackets? Additionally, can you also evaluate Mamba2 and DeltaProduct with negative eigenvalues in the state-transition matrix in your experiments?\n\n**-- References --**\n\nSiems et al. Deltaproduct: Improving state-tracking in linear rnns via householder products. In NeurIPS 2025.\n\nGrazzi et al. Unlocking state-tracking in linear rnns through negative eigenvalues. In ICLR 2025.\n\nGus et al. Log-linear attention. Preprint arXiv:2506.04761, 2025.\n\nAI21 Labs. Jamba: A Hybrid Transformer-Mamba Language Model. arXiv 2024\n\nGlorioso et al. Zamba: A Compact 7B SSM Hybrid Model. arXiv, 2024\n\nLim et al. Parallelizing non-linear sequential models over the sequence length. In ICLR 2024\n\nGonzalez et al. Towards Scalable and Stable Parallelization of Nonlinear RNNs. In NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NgfHfKDafW", "forum": "tuLF84azND", "replyto": "tuLF84azND", "signatures": ["ICLR.cc/2026/Conference/Submission14117/Reviewer_8wg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14117/Reviewer_8wg9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762263059016, "cdate": 1762263059016, "tmdate": 1762924583989, "mdate": 1762924583989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}