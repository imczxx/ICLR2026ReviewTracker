{"id": "9u375F60RU", "number": 18824, "cdate": 1758291170148, "mdate": 1759897079237, "content": {"title": "Maximum Probability-driven Bandit Learning for Matching Markets", "abstract": "Security and robustness are crucial for ensuring stable and fair transactions in two-sided markets, given the complexity of preferences and uncertain returns experienced by the participants. In contrast to traditional competing bandits in two-sided markets that focus on maximum returns, we propose a maximum probability-driven bandit learning (P-learning) model that emphasizes risk quantification. Since one side of the market lacks prior knowledge about its preferences for the other, the proposed P-learning algorithm maximizes the probability of Mean-Volatility statistics lying in a preferred and attainable interval. A scalable and stable matching rule was proposed by combining P-learning with the Gale-Shapley matching algorithm that ensures secure and efficient outcomes. A detailed exploration-exploitation procedure of the matching algorithm has been presented with the support of a centralized platform. In both the single-agent setting and the multi-agent setting, our model achieves sublinear regret of $\\mathcal{O}(\\sqrt{n})$, under different conditions. This paper theoretically proves that the P-learning generates stronger statistical power than classical tests based on normality. Simulation studies demonstrate the superiority of our algorithm over the existing works.", "tldr": "", "keywords": ["Multi-armed Bandit，Matching Markets"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fef3ae381b011c2f9f550685d978fd26c9e3cb7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the setting of online matching markets, which are many-to-many discrete (i.e. bi-partite graph) assignment problems. The objective is for a central planner to match both sides of the market repeatedly, receiving bandit feedback on each assignment it makes. Unlike previous works, which focus on mean performance criteria (i.e. standard regret), this work aims to provide a new perspective on risk management in these kinds of systems by introducing a distribution approach through a mean-variance objective. Concretely, as far as I can tell, the goal of the learning problem is to maximise the asymptotic probability that this mean-variance statistic falls within a pre-specified interval for each node of the matching problem.  \n\nUnfortunately, parsing what is going on besides this is above my abilities, there are simply too many definitions missing. In so far as I can tell there is a series of numerical examples (with 1, 2, or 3, nodes on one or the other side) in section 3, while section 4 presents two regret bounds, one for the K1-2 graph and one for a K2-2 but with an assumption that boils down to a 0/1 hypothesis test on top of K1-2. Finally, there are some numerical experiments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I'm afraid I really have nothing good to say."}, "weaknesses": {"value": "I’m very sorry to have to write such an unpleasant review, but, frankly, the paper is bad. The writing quality is far below what is acceptable in a reputable publication. Disqualifyingly so, in my opinion, as it is impossible for me to assess the merits of the underlying mathematical work. The whole paper emanates a hollow sense of confusion.\n\nThe paper borders on incomprehensible; providing exhaustive feedback on all the unclear parts would not be feasible. However, I will \nhighlight a particularly symptomatic passage, from $\\ell51$ to $\\ell54$, in the hopes that it helps the authors realise the overall problem:\n- Here, there are four sentences, one after another, that simply state what was done in some other papers without any commentary! These descriptive statements don’t tell me anything: why should I care that combinatorial MABs with non-linear rewards were studied? What am I supposed to learn and remember from this that will be relevant for this work? Why should I keep reading any further? \n\nReturning to a higher level: the setting is never clearly explained, many notations are never defined, an issue which begins at the first equation (!), terminology is vague (*e.g.* « faster bandit process », $\\ell101$, « utility » for an interval $\\ell111$, *etc.*), and motivation is near-nonexistent.  The probabilistic setup and the definitions of the expectations are never made clear. I can’t even tell if some of it is wrong or not, but I have quite a few points of worry... Similarly, I have no idea what we’re actually trying to learn: apparently, the preferences are known ($\\ell238$) so why can’t we just solve the problem off-line? Why do we need to do statistics at all? \n\nThe authors are clearly not the most comfortable writing in English, which is in no way their fault and I certainly want to encourage them to persevere. However, due to the overwhelming amount of issues, I don’t think English is really the root of the paper's problems. Nonetheless, should the authors feel held back by the English language, it’s entirely feasible today to write a whole paper in one's native language and bulk-translate it with an LLM afterwards."}, "questions": {"value": "I want to make clear that, the fact that I can't *read* the paper to assess its validity is a disqualifying issue which will not be solved by anything short of a complete rewrite. I would love to read a deep revision and actually understand what the paper is about, but I'm not certain it would be feasible within the reviewing period. \n\nNevertheless, should the authors like to discuss their work, or if they would like advice and feedback, I would happily help as best I can so that they can come away from the review cycle with useful feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P9HRrwUbmO", "forum": "9u375F60RU", "replyto": "9u375F60RU", "signatures": ["ICLR.cc/2026/Conference/Submission18824/Reviewer_mpuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18824/Reviewer_mpuj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658595024, "cdate": 1761658595024, "tmdate": 1762930781907, "mdate": 1762930781907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel \"Maximum Probability-driven Bandit Learning\" (P-learning) model for two-sided matching markets. The key innovation is a shift in focus from traditional bandit algorithms that maximize cumulative rewards to one that maximizes the probability that a combined Mean-Volatility (MV) statistic of an agent's rewards falls within a pre-defined \"expected utility\" interval [α_i, β_i]. This approach explicitly incorporates risk (volatility) into the decision-making process. The authors integrate this P-learning algorithm with the Gale-Shapley stable matching algorithm within a centralized platform. They provide a theoretical analysis demonstrating that their method achieves a sublinear regret of O(√n)in both single-agent and multi-agent settings, under specific conditions. The paper is supported by simulations showing superior convergence rates compared to standard algorithms like UCB and ETC, and it includes a hypothesis testing framework based on a novel \"Bandit distribution\" to argue for stronger statistical power."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of prioritizing risk-aware objectives (probability of being within an interval) over pure reward maximization is highly relevant and timely. It addresses a gap in the literature, particularly for real-world platforms where stability and security are as important as raw profit. The motivation is well-articulated with references to fairness and driver income disparities."}, "weaknesses": {"value": "1. The writing, while technically dense, could be improved for clarity. The definition of the MV statistics presented abruptly, and its connection to the strategy ϑ_icould be explained more intuitively. The flow between sections, particularly from the algorithm description to the theoretical results, sometimes feels jumpy.\n2. A crucial component of the model is the \"expected utility\" interval [α_i, β_i]. The paper mentions this is estimated by the platform based on user needs and historical data, but provides no guidance or algorithm for how this critical interval should be set or adapted over time. An inappropriate interval could lead to poor performance, and the sensitivity of the results to this choice is not deeply explored."}, "questions": {"value": "1. The simulations focus on small-scale cases (e.g., 2 vs. 2 and 3 vs. 3 agents/arms). How does P-learning scale to markets with hundreds or thousands of participants?\n2. The paper compares P-learning to classic algorithms like UCB and ETC, but how does it fare against other risk-sensitive bandit approaches (e.g., those based on Conditional Value-at-Risk or mean-variance optimization)? Is the probability-driven objective inherently more suitable for matching markets, or are there trade-offs in terms of complexity and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uf6KLQvBuh", "forum": "9u375F60RU", "replyto": "9u375F60RU", "signatures": ["ICLR.cc/2026/Conference/Submission18824/Reviewer_tojt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18824/Reviewer_tojt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794697688, "cdate": 1761794697688, "tmdate": 1762930710787, "mdate": 1762930710787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new algorithmic framework, Maximum Probability-driven Bandit Learning (P-learning), for decision-making in two-sided matching markets. Unlike traditional multi-armed bandit (MAB) approaches that focus on maximizing expected rewards, the proposed P-learning framework explicitly incorporates risk quantification via Mean-Volatility (MV) statistics. The algorithm aims to maximize the probability that the MV statistics fall within a desired utility interval. By integrating P-learning with the Gale–Shapley stable matching mechanism, the paper constructs a centralized matching process that ensures both robustness and stability. Theoretical analysis demonstrates sublinear regret bounds in both single-agent and multi-agent cases, and simulation results show faster convergence and lower regret than UCB, ϵ-greedy, and Explore-Then-Commit (ETC) baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel formulation: The probabilistic view of matching bandits through MV-statistics is an elegant way to combine reward maximization and risk control.\n\n2. Sound theoretical development: The regret analysis is rigorous, with proofs extending classical bandit frameworks to the new probabilistic setup.\n\n3. Integration with matching markets: By embedding the Gale–Shapley mechanism, the method naturally aligns with two-sided platforms, adding practical relevance."}, "weaknesses": {"value": "1. The paper is mathematically dense, with long derivations that sometimes obscure intuition. The presentation of MV-statistics and its connection to classical regret could be simplified.\n\n2. Simulations are synthetic and relatively small-scale; there is no empirical validation on real matching-market data (e.g., Uber, Airbnb)."}, "questions": {"value": "1. It appears that the analysis relies on Gaussian reward distributions. How would the regret bounds change under non-Gaussian reward distributions or bounded rewards? \n\n2. Is there an intuition for how the “volatility” term affects exploration compared to other kinds of variance-aware bandits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RfZywyzYKI", "forum": "9u375F60RU", "replyto": "9u375F60RU", "signatures": ["ICLR.cc/2026/Conference/Submission18824/Reviewer_XW3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18824/Reviewer_XW3w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805035608, "cdate": 1761805035608, "tmdate": 1762930703995, "mdate": 1762930703995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new bandit-learning framework called P-learning, which integrates risk quantification into two-sided market matching. Instead of maximizing rewards, the algorithm maximizes the probability that Mean-Volatility statistics fall within a desired interval, ensuring robustness and fairness in uncertain environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea of incorporating risk control in the competing bandit is interesting."}, "weaknesses": {"value": "1. The motivation of P-learning, and the equation (1), is not clear and convincing, and there is a lack of discussion on the relationship between the proposed model and existing literature on risk-related bandit problems and competing bandits.\n2. Experiments are overly simplified with synthetic data on a small scale and with naive baselines, e.g., only standard algorithms ETC,\nUCB, ϵ-greedy are considered.\n3. Technical contribution is limited, as the multi-agent results essentially refer to the two-agent case. This is also very confusing, as the paper seems to express a general multi-agent setup, but offered only two-agent solutions."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SxVGyqj6Jc", "forum": "9u375F60RU", "replyto": "9u375F60RU", "signatures": ["ICLR.cc/2026/Conference/Submission18824/Reviewer_ihT8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18824/Reviewer_ihT8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921093192, "cdate": 1761921093192, "tmdate": 1762930666435, "mdate": 1762930666435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}