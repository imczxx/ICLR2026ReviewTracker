{"id": "qyS3gtL2Fx", "number": 6289, "cdate": 1757964832860, "mdate": 1759897924665, "content": {"title": "CeLLM: Can Large Language Models Achieve the AI  Virtual Cell ?", "abstract": "High-throughput single-cell sequencing has enabled large-scale cellular profiling and spurred the development of single-cell foundation models. These models, typically pretrained on transcriptomic data, learn general-purpose cellular representations but remain limited in modality coverage, causal reasoning, and interpretability, thus falling short of the vision of an Artificial Intelligence Virtual Cell (AIVC). In parallel, large language models (LLMs) have demonstrated strong potential for unifying heterogeneous modalities, adapting to diverse tasks, and generating interpretable reasoning chains in natural language, making them promising candidates toward AIVC. Recent progress in applying LLMs to tasks such as cell annotation and perturbation prediction highlights this potential, yet key challenges persist, including insufficient task coverage, narrow evaluation metrics, and limited robustness to input and prompting factors. To address these gaps, we introduce \\textbf{CeLLM}, a comprehensive benchmarking framework for evaluating \\textbf{LLM}s in the \\textbf{CeLL}ular domain. CeLLM covers a broad spectrum of tasks spanning gene, cell, and omics-level analyses, systematically assesses 15 open-source, proprietary, and biology-specialized models, and incorporates diverse evaluation criteria under multiple task settings. As a cross-scale, reproducible, and dynamic benchmark, CeLLM provides a sustainable platform to track progress, foster methodological innovation, and accelerate the development of LLMs toward virtual cell modeling.", "tldr": "", "keywords": ["Large Language Models", "AI  Virtual Cell"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c72e53eaece45febaa0aa9a610d033c1d1d0ec6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CeLLM, a benchmark for assessing large language models on cellular biology tasks with the long-term goal of informing an AI “virtual cell.” The work is motivated by limits in current single-cell foundation models, including narrow modality coverage, weak causal reasoning, and limited interpretability. The authors argue that LLMs can bridge heterogeneous modalities and provide natural-language reasoning, but that rigorous and broad evaluation is missing. CeLLM spans gene, cell, and omics levels, defines multiple task settings and metrics, and evaluates 15 models across open-source, proprietary, and biology-specific systems. The framework emphasizes robustness to input phrasing and prompting to enable apples-to-apples comparisons and reproducible tracking over time. Overall, CeLLM positions itself as a cross-scale resource that surfaces current gaps and aims to accelerate LLM methods for virtual-cell modeling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Broad coverage of related work and relevant biological tasks.\n- A diverse panel of LLMs is evaluated, including open, closed, and biology-tuned systems, which provides a useful snapshot of the field."}, "weaknesses": {"value": "- The main results compare LLMs to one another, but not to established task-specific methods. Without such baselines, it is difficult to judge whether LLMs meaningfully advance the state of the art for “virtual cell” capabilities. Please include representative baselines per task, for example CellTypist for cell annotation and foundation models such as scGPT for annotation and gene-perturbation prediction. Reporting both absolute performance and deltas would clarify where LLMs help.\n- Much of the manuscript centers on prompt design and model ranking. It is not yet clear how the benchmark reveals biological mechanisms, suggests new hypotheses, or guides experimental follow-up. Consider adding case studies that connect model outputs to potential biological discoveries (e.g., proposing gene candidates, prioritizing perturbations, or explaining pathway-level effects) and analyzing when LLM reasoning aligns with known biology.\n- Several settings are underspecified. For each task, please clearly state the dataset, input and output formats, train or evaluation splits, pre- and post-processing, metric definitions, and any filtering. Document prompt templates, sampling parameters, and the number of seeds. This will improve reproducibility and allow fair comparisons.\n- Some conclusions in the main text appear to rely on tables in the appendix, but the links are not explicit. Add precise cross-references to the relevant tables or figures and, where helpful, bring key results into the main paper for readability."}, "questions": {"value": "* How can the proposed benchmark guide biological research?\n* Why Cell2Sentence always produces results with all zero metrics? Can you provide some example output of it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8vMrFeorJZ", "forum": "qyS3gtL2Fx", "replyto": "qyS3gtL2Fx", "signatures": ["ICLR.cc/2026/Conference/Submission6289/Reviewer_yQT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6289/Reviewer_yQT6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763161945, "cdate": 1761763161945, "tmdate": 1762918594042, "mdate": 1762918594042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CeLLM presents a benchmark for evaluating large language models (LLMs) in cellular biology. It aggregates existing datasets across gene, cell, and omics-level tasks, standardizes evaluation protocols, and tests a range of general and domain-specific LLMs. The benchmark includes multiple task types, prompt-format studies, and robustness analyses (e.g., gene-order perturbations). The paper uses three categories of evaluation metrics classification, ranking, and foundation model (FM)-as-judge.\n\nRecommendation: (leaning reject)\nCeLLM is a well-executed, comprehensive evaluation study that will be useful for practitioners and newcomers to LLMs in biology. However, its scientific novelty and methodological depth are limited. The paper’s value is primarily as a resource and infrastructure contribution, which might better suit a datasets/benchmarks venue or a companion release rather than a main-track research paper at ICLR.\n\nSupporting arguments\n1. The study adds organizational and empirical clarity but not new methods or theoretical insights.\n2. The use of FM-as-judge (especially with Geneformer) is interesting but insufficiently documented and should include main-text results.\n3. Representing cells as sorted gene tokens oversimplifies biology and limits generalization, though it aligns with prior LLM-pretraining conventions. Many biomedical foundation models (e.g. scGPT) use more better data structure to capture more biomedical meaning, and should be added to the comparison. As is, all models tested are text-only LLMs, which is a very limited/biased type of model for Virtual Cell.    \n4. Despite these limitations, the work’s reproducibility and clarity make it a potentially valuable reference for future research benchmarking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive and systematic evaluation. The study is well organized and clearly written, offering a large-scale and reproducible comparison of existing LLMs on biological tasks.\n\n2. Practical contribution. While not conceptually novel, the work is valuable for practitioners who lack the resources to replicate such broad comparisons. It highlights prompt design effects, scaling trends, and robustness issues in a consistent setting.\n\n3. Reproducibility and structure. The standardized preprocessing, task definitions, and metrics make this a useful reference for future model developers.\n\n4. Breadth of coverage. The inclusion of multiple biological task types (gene-level, cell-level, cross-omics) provides a wide view of LLM performance across domains."}, "weaknesses": {"value": "1. Limited novelty. The paper mainly evaluates existing models on existing datasets with standard metrics. Its contribution is systematic rather than conceptual or methodological. This limits its fit for the main ICLR track, though it would be valuable as a benchmark paper.\n\n2. Simplistic cell representation. Cells are represented as sorted lists of genes, which is a narrow and biologically limited representation that neglects expression magnitudes, gene–gene dependencies, and spatial or pathway-level structure. (note that this is a limitation of any LLM that only takes text modality as input; this can be a deal breaker for even using text-only LLM as a candidate for Virtual Cell, which required a lot more meaningful representation)\n\n3. FM-as-judge clarity and placement. While FM-as-judge evaluation is a standard idea, the paper’s implementation using Geneformer as a judge is not sufficiently detailed, and results are buried or absent from the main text. It’s unclear how scores were computed or how they complement traditional metrics. Clearer explanations and summary results in the main body would strengthen the paper.\n\n4. Overall impression. The paper reads largely as a series of ablation studies on known models, datasets, and prompt styles. Although useful for the field, it offers limited new insights or conceptual advances."}, "questions": {"value": "1. Could you elaborate on how Geneformer is used as a judge? For example, what layer or embedding is used, how are scores computed, and how does this correlate with traditional metrics?\n\n2. Have you considered richer or more biologically grounded cell representations in addition to Geneformer style sorted list of genes? Maybe compare to those models using the same tasks and explore and explain the limitation of the sorted gene based cell representation?\n\n3. How do you ensure that evaluated models have not seen parts of the benchmark data during pretraining? Some LLMs, especially the larger ones, may have the unfair advantage of seeing the benchmark data during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k8iHUcj0YF", "forum": "qyS3gtL2Fx", "replyto": "qyS3gtL2Fx", "signatures": ["ICLR.cc/2026/Conference/Submission6289/Reviewer_jCwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6289/Reviewer_jCwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934289331, "cdate": 1761934289331, "tmdate": 1762918593667, "mdate": 1762918593667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces CeLLM, a unified benchmark for evaluating LLMs in the cellular domain. CeLLM notably includes a broad spectrum of tasks, and incorporates diverse evaluation criteria that go beyond accuracy, F1-score, and related measures. In this study, 15 models are evaluated on this benchmark, including open-source and proprietary general LLMs (ie. GPT-5, DeepSeek-V3.1), and domain-specific LLMs (ie. cell-o1). This work also provides an investigation of key factors influencing LLM performance on cellular tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tAddresses an important and clearly articulated gap in the literature, as the area of foundation models and LLMs for single-cell analysis has seen an influx of models and methods but insufficient benchmarks to properly evaluate them. \n•\tDiverse range of tasks and models evaluated, with the former in particular contributing to the novelty of this work and its potential for impact. \n•\tSelection of metrics is well thought out and represents a good spectrum of task difficulties and topic areas.\n•\tInvestigation of key factors influencing LLM performance is interesting. \n•\tMain conclusions are non-trivial and useful.\n•\tWriting is generally clear."}, "weaknesses": {"value": "•\tUnclear whether models are evaluated fairly. For example, the authors reported an accuracy of 0% for Cell2Sentence across nearly all tasks, including cell type annotation, which the model was able to perform reasonably well in the original work (https://pmc.ncbi.nlm.nih.gov/articles/PMC11565894/pdf/nihpp-2023.09.11.557287v4.pdf), and therefore has at least some capability in this setting. Ultimately, this benchmark may just be evaluating how knowledgeable these models are in cellular biology when prompted and evaluated in a specific way, rather than being a proper comprehensive evaluation of their knowledge and capabilities.\n•\tLimited actionable takeaways. Comparisons of different models often don’t lead to concrete conclusions/recommendations. Higher level observations across different tasks not significantly discussed. Limitations of this work and potential future work are not clearly outlined. \n\nUltimately, despite the benchmark proposed being interesting, these weaknesses limit this work’s potential for impact. I am therefore providing an initial recommendation of marginal accept. However, if the authors are able to better justify some of their choices, and provide more concrete conclusions, I believe this work could meet the bar for ICLR."}, "questions": {"value": "•\tWhy is the accuracy of Cell2Sentence so poor on the benchmark? Is it related to how the problem was framed, or is there a notable difference between the benchmark and the benchmarks used in the Cell2Sentence paper? \n•\tCan a more detailed discussion of actionable takeaways be provided?\n\nSome suggestions: \n•\tConsider bolding the highest values for different metrics in your tables to improve readability. \n•\tConsider the use of third level headings in section 3 (i.e.. I believe “Gene Annotation” should be 7.2.1 since it is under Main Results)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HLxORCF5BA", "forum": "qyS3gtL2Fx", "replyto": "qyS3gtL2Fx", "signatures": ["ICLR.cc/2026/Conference/Submission6289/Reviewer_J6jt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6289/Reviewer_J6jt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939491692, "cdate": 1761939491692, "tmdate": 1762918593252, "mdate": 1762918593252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CeLLM, a unified benchmark for evaluating large language models (LLMs) on core cellular-biology tasks spanning genes, cells, and multi-omics. It assesses 15 open-source, closed-source, and domain-specialized models across diverse tasks (gene annotation, multi-omics integration, cell annotation—single, batch, spatial—cell perturbation, drug response, and cell-condition generation), with multiple evaluation criteria (classification, ranking, and foundation-model–based metrics). The benchmark aims to be cross-scale, reproducible, and evolving to support progress toward “Artificial Intelligence Virtual Cells” (AIVC)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, CeLLM could be a significant benchmarking contribution for AI-for-cell biology, contingent on clarifications about novelty claims, data/preprocessing rigor, and ablation reporting. With those clarifications in an author response, I would be willing to increase the score."}, "weaknesses": {"value": "1. The paper promises public code/data upon publication; consider releasing evaluation scripts and prompts (including few-shot exemplars) earlier to encourage community adoption.\n\n2. The foundation-model–based evaluation (embedding similarity) is reasonable, but please discuss dependence on the chosen evaluator (e.g., Geneformer) and risks of circularity if any evaluated model is aligned to the same latent space."}, "questions": {"value": "1. Please make the benchmark’s novelty against prior efforts more explicit in the main text: e.g., what’s new beyond existing task suites and datasets for cell annotation, perturbation, and multi-omics (CellVerse/OP tasks, etc.)? A short table contrasting CeLLM’s tasks/metrics/models with prior benchmarks would help readers understand incremental vs. new coverage.\n\n2. The “optimal window” of 100–200 genes is interesting; please show full curves (20/50/100/200/500) with CIs across models and tasks. Likewise, for few-shot settings, report performance as a function of k-shots for both “leak” and “mask” strategies (the text notes leak helps, mask can hurt).\n\nTypos:\n1. “Biobert and ROGUE” → “BioBERT and ROUGE” (gene-annotation setup).  \n2. “From the table 14” → “From Table 14”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHrQlCHdx9", "forum": "qyS3gtL2Fx", "replyto": "qyS3gtL2Fx", "signatures": ["ICLR.cc/2026/Conference/Submission6289/Reviewer_5GKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6289/Reviewer_5GKb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762711911569, "cdate": 1762711911569, "tmdate": 1762918592758, "mdate": 1762918592758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}