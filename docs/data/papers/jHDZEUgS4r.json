{"id": "jHDZEUgS4r", "number": 23682, "cdate": 1758347122392, "mdate": 1763718365681, "content": {"title": "MedAgentGym: A Scalable Agentic Training Environment for Code-Centric Reasoning in Biomedical Data Science", "abstract": "We introduce MedAgentGym, a scalable and interactive training environment designed to enhance coding-based biomedical reasoning capabilities in large language model (LLM) agents. MedAgentGym comprises 72,413 task instances across 129 categories derived from 12 authentic real-world biomedical scenarios. Tasks are encapsulated within executable sandbox environments, each featuring detailed task specifications, interactive feedback mechanisms, verifiable ground truth annotations, and scalable training trajectory generation. Extensive benchmarking of 29 LLMs reveals substantial performance disparities in biomedical data science between commercial and open-source LLMs. Leveraging efficient multi-threaded and multi-turn trajectory sampling in MedAgentGym, Med-Copilot achieves performance gains of +43.02% and +45.28% from offline and online reinforcement learning, respectively, demonstrating MedAgentGym as an effective training ground while establishing itself as a cost-effective, privacy-preserving alternative competitive with proprietary LLMs (gpt-4o). By offering a unified execution environment with a comprehensive benchmark and accessible, extensible training resources, MedAgentGym delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical data science.", "tldr": "", "keywords": ["Medical Reasoning", "LLM Agent", "Code Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/693714542335ed9db1d2e88000ec27630f75b698.pdf", "supplementary_material": "/attachment/49c719a35824cbeaf5d130fb9f46a5af32811390.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduced MedAgentGym, a training environment for coding-based biomedical agents. It consists of three folds of contributions: (1) MedAgentGym involves 72,413 task instances across 129 categories derived from 12 biomedical scenarios. (2) This training platform allows for efficient deployment and scalable evaluation, benchmarking 29 LLMs. (3) The training data collected from MedAgentBench leads to the powerful Med-Copilot-7B/14B, which produce comparable results as the much larger proprietary LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The preparation of MedAgentGym requires a significant amount of effort, and is a non-trivial contribution to the open-source community.  \nS2: The setup in MedAgentGym is comprehensive, and the evaluation of existing LLMs on coding-based medical reasoning tasks is thorough.  \nS3: Med-Copilot-7B and -14B are also very useful open-source models for medical reasoning tasks, and their training and testing setups are solid."}, "weaknesses": {"value": "W1: How are the tasks studied in this paper fundamentally different from the general-purpose coding tasks? To what extent is the medical knowledge essential here? If an agent excels in general-purpose coding tasks, does it still perform well here? How do the rankings differ?  \nW2: Similarly, the related work lacks a discussion of existing general-purpose coding benchmarks.  \nW3: The data construction step in Section 3.2 is unclear, and particularly, it does not show the difference between the contributed benchmark and the constituent datasets from existing work. Based on Table 2, is MedAgentGym simply an ensemble of all the existing benchmark datasets? Is this paper overclaimed?  \nW4: Can you provide some example instances to illustrate MedAgentGym qualitatively?"}, "questions": {"value": "Q1: In Line 358, how exactly do you prepare the online pairs for DPO? Isn't DPO an offline algorithm?  \nQ2: What do you mean by \"accurately selects successful trajectories\" in Line 417? Can you explain the difference between Pass@K and Best@K? Are they metrics for the agent or for the verifier? And why does a small gap between the two metrics indicate that \"the verifier can effectively identify successful trajectories\"?  \nQ3: What do you mean by \"repeat this DPO step iteratively\" in Line 443, and what do you mean by \"DPO using eight new rollouts per task\"? Can you explain the setup of iDPO? And why do you need \"eight new rollouts per task\" in addition to the 4,298 pairs?  \nQ4: Can you compare the results from the self-improvement in Section 5.3 and the results using the setup in Section 5.1?  \nQ5: How exactly is Figure 10 computed? Based on what features did you calculate the cosine similarity, and what does the \"in/out-of-distribution\" in \"inter-distribution\" mean? How did you determine if a task should belong to in- or out-of-distribution? What's the average number of turns and other statistics of those tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YINP35HRtY", "forum": "jHDZEUgS4r", "replyto": "jHDZEUgS4r", "signatures": ["ICLR.cc/2026/Conference/Submission23682/Reviewer_Keuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23682/Reviewer_Keuw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761096288553, "cdate": 1761096288553, "tmdate": 1762942761958, "mdate": 1762942761958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MedAgentGym, an extensible agentic environment and benchmark designed to systematically advance code-centric medical reasoning in LLM-based agents. It encapsulates 72,413 executable medical data science tasks spanning 129 categories derived from 12 scenarios, featuring isolated Docker-based sandboxes, ground-truth verifiers, multi-turn feedback, and scalable trajectory collection. Through comprehensive empirical evaluation, MedAgentGym is used to benchmark 29 LLMs, highlighting persistent deficits for medical code generation, and demonstrating substantial improvements via agentic reinforcement learning fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MedAgentGym aggregates an exceptionally broad and diverse set of medical code-centric tasks.\n2. The environment is built around reproducible, interactive Docker sandboxes, allowing code execution, error handling, debugging, and dynamic dependency install, addressing reproducibility and privacy.\n3. Med-Copilot exhibit strong improvements over baselines, with RL strategies yielding notable boosts, and ablation studies clarifying contributions."}, "weaknesses": {"value": "1. MedAgentGym is constructed by integrating 12 existing datasets. Although it provides a division between training and test sets, the model is exposed to the task types and data patterns from these datasets during training. Therefore, its strong performance on the internal test set may partially result from memorization of specific task patterns or overfitting, rather than genuinely acquiring a universal biomedical code reasoning capability.\n2. While integrating these components into a large-scale, biomedical-oriented environment represents a significant engineering contribution, the core technical concepts underlying the environment—such as the use of Docker sandboxes, provision of interactive debugging feedback, and trajectory collection for reinforcement learning—have precedents in the AI agent domain.\n3. The medical-specific models evaluated in the paper, such as HuatuoGPT-o1-7B and MedReason-8B, are relatively small, with only 7B/8B parameters. Attributing their suboptimal performance solely to the limitations of medical specialization, while overlooking the substantial differences in model size, is logically flawed. A more equitable comparison would be to assess a large-scale, medically optimized model against a general-purpose large model."}, "questions": {"value": "1. Can the authors clarify how different error types (see Figure 7) are incorporated into the RL reward signal? Are there distinct penalties for, say, 'stuck in the loop' vs. compile/runtime/IO errors? How sensitive is final agent performance to this reward model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8BsOiY9SWO", "forum": "jHDZEUgS4r", "replyto": "jHDZEUgS4r", "signatures": ["ICLR.cc/2026/Conference/Submission23682/Reviewer_xZ94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23682/Reviewer_xZ94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216263593, "cdate": 1761216263593, "tmdate": 1762942761556, "mdate": 1762942761556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a released training environment MedAgentGym for the purposes of benchmarking and training LLMs for the use of biomedical data science coding tasks. Benchmarking against many propriatary and (varying sized) open-source LLMs demonstrates the state of the field in this task. Training demonstrates impressive gains in task performance of OS LLMs, comparable to gpt-4o. The authors conduct extensive benchmarking and experiments to demonstrate the utility of their training environment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is written to a good standard and certainly looks publication-ready\n* The consideration of open-source LLMs for the papers setting of biomedical data science is important, as a large amount of data will be under stringent data privacy rules. Many alternative similar papers operating in this field do not consider this\n* Good contribution over prior literature, encapsulating the majority of tasks that I believe would be applicable for biomedical data science\n* Extensive benchmarking of many existing open-source and proprietary LLMs gives important information regarding the capabilities of such models\n* Thorough experiments for model Med-Copilot. Results look promising"}, "weaknesses": {"value": "* I have a feeling that the the title and naming given to the training environment is slightly overstepping and too generelized. Perhaps 'BioMedAgentGym' is more suitable. \n* Since the paper is releasing a training environment for the practical real-world use of biomedical data science, I would like to see some discussion on the implications of this and reccomendations to users (please see questions below) \n* I cannot see many weakensses, though I am not familiar with the field of biomedical research nor such benchmarking papers"}, "questions": {"value": "* Table 3 demonstrates results of LLMs on MedAgentGym. Some \"best avg. scores\" (for a given LLM size) are relatively quite low. For example, the OSS <10B has Qwen3-8B at a success rate of 30.83. Given that some practitioners may only have the compute for such models, are you able to give reccomendations (complementing the writing in §4.2) regarding this? For example, what do you deem a sufficiently good performance on benchmarking for real-world deployment of a LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WL6GxDUxq3", "forum": "jHDZEUgS4r", "replyto": "jHDZEUgS4r", "signatures": ["ICLR.cc/2026/Conference/Submission23682/Reviewer_cegQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23682/Reviewer_cegQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648182404, "cdate": 1761648182404, "tmdate": 1762942760588, "mdate": 1762942760588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MedAgentGym introduces an interactive training environment for code-centric biomedical reasoning. There are numerous scenarios, with different LLMs evaluated in the environment showing gaps between especially closed vs open models. They also introduce Med-copilot trained on the env which is very strong."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Solving an important problem & provides a comprehensive benchmark on real-world medical tasks. \n\n2. Very rigorous evals across many tasks over many models \n\n3. Strong Performance Gains of Med-Copilot on the env"}, "weaknesses": {"value": "- Only execution evals, no assessment of intermediate reasoning and steps which is vital in medicine. Where the trajectory matters as much as the solution\n\n- Big OOD drops unexplained on external dataset (more validation and digging). Maybe things are just overfit?"}, "questions": {"value": "- Can you assess some trajectories for sound reasoning vs just only correctness\n\n- Can we know that none of the LLMs have already trained on the benchmark datasets? Maybe need some private data or new data\n\n- To the OOD drop please can you dig in and understand why?\n\n- Please can you add variance not just mean to understand overlap of models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ykoz9j3ga8", "forum": "jHDZEUgS4r", "replyto": "jHDZEUgS4r", "signatures": ["ICLR.cc/2026/Conference/Submission23682/Reviewer_7VJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23682/Reviewer_7VJa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865720353, "cdate": 1761865720353, "tmdate": 1762942759988, "mdate": 1762942759988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}