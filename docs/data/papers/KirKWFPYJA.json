{"id": "KirKWFPYJA", "number": 25501, "cdate": 1758368676937, "mdate": 1759896718582, "content": {"title": "High Probability Bounds for Non-Convex Stochastic Optimization with Momentum", "abstract": "Stochastic gradient descent with momentum (SGDM) has been widely used in machine learning. However, in non-convex domains, high probability learning bounds for SGDM are scarce. In this paper, we provide high probability convergence bounds and generalization bounds for SGDM. Firstly, we establish these bounds for the gradient norm in the general non-convex case. The derived convergence bounds are tighter than the theoretical results of related work, and to our best knowledge, the derived generalization bounds are the first ones for SGDM. Then, if the Polyak-{\\L}ojasiewicz condition is satisfied, we establish these bounds for the error of the function value, instead of the gradient norm. Moreover, the derived learning bounds have faster rates than the general non-convex case. Finally,  we further provide sharper generalization bounds by considering a mild Bernstein condition on the gradient. In the case of low noise, their learning rates can reach $\\widetilde{\\mathcal{O}}(1/n^2)$, where $n$ is the sample size. Overall, we relatively systematically investigate the high probability learning bounds for non-convex SGDM.", "tldr": "", "keywords": ["Momentum", "nonconvex learning", "generalization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e05ad5f0bb41097b874a4fc65a2ff358f797ed4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper establishes high probability convergence and generalization upper bounds for non-convex SGDM. Specifically, it first considers the general non-convex case. The convergence results are better than previous works and the generalization bounds are the first ones for SGDM. Then, when PL condition holds, faster rates are achieved than the general non-convex case. Finally, further assuming Bernstein condition brings sharper generalization bounds in the case of low noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper utilizes some special properties of a heavy-tailed distribution, Sub-Weibull distribution, to develop the theoretical analysis of SGDM.\n\n2.The whole paper is written clearly and easy to be understood."}, "weaknesses": {"value": "1.The inequality in line 1240 may not be true, considering that $(1-\\gamma^{t-i+1})$ is the coefficient of the gradient $\\nabla F_S(x_i)$. We can not ensure the norm with $(1-\\gamma^t)$ is larger than the one with $(1-\\gamma^{t-i+1})$. Therefore, it may greatly affect the subsequent proof.\n\n2.It is better to make a table to list all convergence and generalization results and all previous results, which facilitates readers' understanding of the advantages of their results over previous works."}, "questions": {"value": "1.Why do the authors use two symbols $f$ and $g$ in Assumption 2.1?\n\n2.There are some typos, such as “smothness”in the last line of page 3 and $k=2,...，$ in Remark 2.3.\n\n3.The whole analysis framework is very similar to [1] except for the lemma which interchanges the order of summation (Lemma C.6) to enable the analysis of [1] in the SGDM case. Due to this reason, all results are the same as those of [1]. Is my understanding right? If so, do these results give us any insights?\n\nNote: From my perspective, the major innovation of proof is mainly presented in Theorem 3.1 and Theorem 3.3, which is listed in **Q3**. If any other innovations exist in the proofs of other results compared with [1], please make them clear. If so, I will improve my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hi3gn8SfLN", "forum": "KirKWFPYJA", "replyto": "KirKWFPYJA", "signatures": ["ICLR.cc/2026/Conference/Submission25501/Reviewer_jh4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25501/Reviewer_jh4H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804298620, "cdate": 1761804298620, "tmdate": 1762943453706, "mdate": 1762943453706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Strengths\n\n1. First high-probability analysis of SGDM in non-convex settings: The paper provides the first comprehensive high-probability convergence and generalization guarantees for stochastic gradient methods with momentum under nonconvex objectives, bridging an important theoretical gap in stochastic optimization.\n\n2. Sharp convergence rates with structural conditions: The results establish an \\tilde{O}(1/\\sqrt{T}) high-probability convergence rate in the general nonconvex case, improving to \\tilde{O}(1/T) under the Polyak–Łojasiewicz condition and even \\tilde{O}(1/n^2 + F^*/n) under a Bernstein assumption—showing remarkable theoretical depth.\n\n3. Empirical results align with the theory: Experiments on multiple LIBSVM datasets systematically verify the predicted influence of the tail parameter $\\theta$ on convergence speed, offering intuitive visual confirmation of the theoretical high-probability trends.\n\nWeaknesses\n\n1. Limited experimental scope and scalability: Experiments are confined to small-scale logistic regression tasks. The absence of results on large-scale or deep learning benchmarks makes it unclear how well the high-probability bounds manifest in practical training.\n\n2. Lack of practical runtime validation: Theoretical IFO complexity and convergence bounds are not accompanied by empirical runtime or gradient-call comparisons, leaving efficiency gains largely unquantified."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "see Summary"}, "weaknesses": {"value": "see Summary"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OPyTEQY18b", "forum": "KirKWFPYJA", "replyto": "KirKWFPYJA", "signatures": ["ICLR.cc/2026/Conference/Submission25501/Reviewer_tgE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25501/Reviewer_tgE3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917877746, "cdate": 1761917877746, "tmdate": 1762943453487, "mdate": 1762943453487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies high-probability convergence and generalization bounds for SGDM in non-convex settings, achieving faster rates under the Polyak-Łojasiewicz (PL) and a Bernstein-type gradient condition.  \nSpecifically, assuming that the stochastic noise follows a sub-Weibull distribution parameterized by $\\theta$, the authors establish a slightly tighter bound at $\\theta = 1/2$ (improving the logarithmic factor from $\\log(T/\\delta)$ to $\\log(1/\\delta)$) and extend the analysis to the case $\\theta > 1$, compared with prior work."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides convergence and generalization bounds for SGDM under arbitrary values of $\\theta$, offering a unified treatment that covers a wide range of noise distributions from sub-Gaussian to heavy-tailed cases.\n\n2. The focus on SGDM rather than plain SGD is interesting, as understanding how momentum affects convergence and generalization remains an important open question in stochastic optimization."}, "weaknesses": {"value": "1. For the case $\\theta = 1/2$, the improvement over related work is essentially from $\\log(T/\\delta)$ to $\\log(1/\\delta)$ while keeping the same leading-order rate. This represents a mild tightening rather than a substantive advance.\n\n2. The extension to the case $\\theta > 1$ relies on general concentration inequalities applicable to arbitrary $\\theta$ (Appendix Lemmas C.2–C.4). It seems that the main change lies in using a more general tail inequality, which does not introduce substantial technical difficulty or genuine novelty.\n\n3. The paper is difficult to follow, with many typos and inconsistencies. Examples include:\n\n   Line 161: “smothness” → “smoothness”.  \n   Assumption 2.1: The definition of smoothness should not be embedded in the assumption itself.  \n   Theorems: All numbering “(1.) (2.) (3.) (4.)” should be corrected to “(1). (2). (3). (4).”. \n\n   Line 286: “study” should be “studied”.  \n   Theorem 5: The term $\\mu(S)$ appears before being defined.  \n   Line 373: “theFS” should be “the FS”.  \n   Table 1: The first two rows list identical assumptions but yield different error bounds—this should be clarified.\n\n\nTo enhance coherence and credibility, I recommend that the authors thoroughly revise the paper for both clarity and presentation quality."}, "questions": {"value": "1. Under the same assumptions as in the main theorems, are there corresponding high-probability convergence and generalization results for SGD (without momentum)? A direct comparison would help clarify whether momentum offers any provable benefit in this setting.\n\n2. Which concrete model classes (e.g., deep neural networks) are known to satisfy the PL condition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cCnhOzTRe8", "forum": "KirKWFPYJA", "replyto": "KirKWFPYJA", "signatures": ["ICLR.cc/2026/Conference/Submission25501/Reviewer_ctUE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25501/Reviewer_ctUE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941485559, "cdate": 1761941485559, "tmdate": 1762943453299, "mdate": 1762943453299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors have obtained high probability gradient norm bounds in a general non-convex problem for stochastic gradient descent with momentum (SGDM). Then under PL condition of loss, they show high probability bounds on the function values and achieve faster rates under Bernstein condition for gradients."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors have done a very good job in terms of literature review. The paper is overall very well written. The assumptions are clearly provided and the theoretical results are well stated. Establishing both convergence and generalization bounds in high probability with clear description is very interesting."}, "weaknesses": {"value": "This is a nice submission. While I note that the authors have focused on theory, providing a toy experiment in Appendix A will be the main weakness of this submission. I think the paper will be significantly improved by providing more relevant experiments with clear connection with the developed rates under various assumptions and potentially in a more practical setting."}, "questions": {"value": "Please check the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VREohmk8eC", "forum": "KirKWFPYJA", "replyto": "KirKWFPYJA", "signatures": ["ICLR.cc/2026/Conference/Submission25501/Reviewer_WfMC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25501/Reviewer_WfMC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762282147282, "cdate": 1762282147282, "tmdate": 1762943453010, "mdate": 1762943453010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}