{"id": "HJ6EGxNpgb", "number": 3345, "cdate": 1757405625210, "mdate": 1759898094599, "content": {"title": "Attention Head Entropy of LLMs Predicts Answer Correctness", "abstract": "Large language models (LLMs)  generate plausible, yet possibly incorrect answers, posing risks in safety-critical settings, such as medical advice. Although both LLM-as-judge and human evaluations are useful, human evaluation is expensive, whereas LLM-as-judge approaches risk introducing additional hidden errors. To address this, we introduce Head Entropy, a white-box and scalable method that uses the attention patterns inside the model itself to determine the likelihood of a correct answer while generating the answer. Our key insight is that certain attention heads exhibit distinct entropy patterns when the model gives correct versus incorrect answers. Using a sparse logistic regression classifier on per-head entropies, Head Entropy achieves 0.07–0.15 AUROC improvements over baselines on 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine. Through Shapley value analysis, we demonstrate that middle-layer attention heads contribute the most to prediction accuracy providing mechanistic insight into model failure modes. Head Entropy offers a practical, interpretable, and computationally efficient approach for real-time correctness estimation during LLM deployment.", "tldr": "We use attention head entropy as a measure to predict the correctness of a LLM's answer.", "keywords": ["Entropy", "LLM", "Attention Heads", "Correctness Prediction", "Uncertain"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1d8fabcdf88c7a7a6c264ffba2eef62e00a8f66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using the attention entropy of each head as features, concatenating them, and training a classifier on top to predict whether a model’s generation is correct or incorrect. The authors evaluate their method on various datasets, including out-of-distribution experiments. They also use Shapley-value attribution to identify which attention heads are most important."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written.\n- The idea is simple and effective.\n- There are good ablation studies and, more importantly, OOD experiments.\n- The experiments are comprehensive, and the results are promising"}, "weaknesses": {"value": "- The most important weakness of the paper is its novelty. There is already a popular work which uses attention maps to extract features and train a classifier model: https://arxiv.org/pdf/2407.07071. The only difference between this work and the other work is how to extract the feature.\n- The data scaling/low data experiments are missing. I would like to see how the performance changes with less or more data.\n - More insights about why this idea works could be helpful. For instance, why is entropy a good feature extractor?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JHYMT6hqB9", "forum": "HJ6EGxNpgb", "replyto": "HJ6EGxNpgb", "signatures": ["ICLR.cc/2026/Conference/Submission3345/Reviewer_JuPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3345/Reviewer_JuPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760906618973, "cdate": 1760906618973, "tmdate": 1762916681562, "mdate": 1762916681562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces attention head entropy as a quantitative metric to measure the uncertainty of LLMs by analyzing their internal attention patterns. Specifically, it computes the Shannon entropy of the normalized attention weights from each attention head, interpreting lower entropy as more focused and confident attention and higher entropy as more diffuse or uncertain behavior. The authors evaluate this metric on 5 instruction-tuned LLMs across 3 question-answering datasets and train logistic regression models to predict output correctness based on the computed entropies. They further use Shapley-value analysis to identify which layers and heads contribute most to prediction performance, providing insights into where reliability-related signals emerge within transformer architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow. The proposed method is conceptually straightforward and well-presented.\n\n2. The authors evaluate their approach on five instruction-tuned LLMs and three QA datasets. Results show that head entropy consistently outperforms baseline uncertainty metrics and generalizes well across different model families and sizes, demonstrating both robustness and applicability."}, "weaknesses": {"value": "1. The proposed entropy measure appears highly dependent on the specific query content. Even though entropies are averaged over all tokens within an answer, the resulting entropy-based correctness estimates should still be regarded as query-conditional rather than global indicators of confidence. The model’s attention behavior and therefore its entropy varies strongly with input semantics, which could limit generalization across queries or domains. I appreciate if the authors provide any evidence in resolving my concern here.\n\n2. The authors do not justify why their entropy metric uses only the QK-based attention distribution, nor do they test alternative definitions involving the Value matrix or post-attention activations (for example, the entropy after directly decoding the unembedding matrix using logit lens [1]). Additionally, this way of defining entropy seems not new [2].\n\n3. The proposed metric assumes that lower attention entropy implies higher model confidence, which in turn correlates with correctness. However, large language models are known to sometimes generate highly confident but incorrect outputs. This phenomenon suggests that attention entropy may not be a reliable measure of epistemic uncertainty and can fail in cases of confidently wrong reasoning, especially in hallucination-prone or overfitted regimes.\n\n4. All reported results are associational for correlations between entropy and correctness, without an intervention-based or causal analysis. To me it seems that the results cannot be interpreted as evidence of a causal effect.\n\n5. The division into “question,” “thinking,” and “answer” tokens relies on heuristic, task-specific boundaries that may not generalize across prompts or datasets. It remains unclear whether the approach performs consistently if the prompt format or token segmentation changes (for example, different delimiters or reasoning styles). An ablation over prompt variants would be necessary to verify the robustness of this segmentation-dependent analysis.\n\n[1] Nostalgebraist., “Interpreting GPT: the Logit Lens”\n\n[2] Zhang et al., “Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models” https://arxiv.org/abs/2412.16545"}, "questions": {"value": "The paper does not discuss the distribution of correct versus incorrect answers in the evaluation datasets. If correctness labels are imbalanced, the logistic regression classifier could be biased toward the majority class, leading to inflated or misleading performance metrics.\nIt would be important to report the base accuracy of the underlying model on these datasets and whether imbalance exists."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5q8H68YyOy", "forum": "HJ6EGxNpgb", "replyto": "HJ6EGxNpgb", "signatures": ["ICLR.cc/2026/Conference/Submission3345/Reviewer_vnso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3345/Reviewer_vnso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586446179, "cdate": 1761586446179, "tmdate": 1762916681189, "mdate": 1762916681189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HEAD ENTROPY, a white-box method for predicting LLM correctness during inference. The key insight is that certain attention heads exhibit distinct entropy patterns when models generate correct vs. incorrect answers. Using per-head entropies as features for sparse logistic regression, the method achieves 0.07-0.15 AUROC improvements over baselines on 5 instruction-tuned LLMs and 3 QA datasets. Shapley value analysis reveals that middle-layer attention heads are most informative, providing mechanistic insights into model failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong Empirical Results: The method consistently outperforms baselines by meaningful margins across diverse QA tasks (TriviaQA, HotpotQA, MedMCQA) and multiple model families (Qwen, Llama). The 0.07-0.15 AUROC improvements are substantial.\n\n2. Practical Efficiency: The approach adds minimal computational overhead (negligible compared to LLM inference) while requiring only a single forward pass. This makes it genuinely deployable in real systems.\n\n3. Mechanistic Interpretability: Using Shapley values to identify that middle-layer heads contribute most to correctness prediction is insightful and could inform model design and debugging. The finding is consistent across architectures."}, "weaknesses": {"value": "The evaluation scope is a significant limitation. The paper only evaluates on three QA datasets focusing on factual retrieval, which limits the generalizability of findings. More concerning, the experiments are restricted to instruction-tuned models, leaving unclear whether the approach works equally well for base models or other architectures like mixture-of-experts or retrieval-augmented models. The medical domain (MedMCQA) performance is notably weaker, showing only 0.05 AUROC improvement compared to 0.07-0.15 on other datasets, yet the paper provides limited discussion of why certain domains are more challenging for this approach.\n\nThe theoretical foundations underlying the method lack depth. The paper doesn't adequately explain why middle-layer heads are specifically informative for correctness prediction—it identifies that they are empirically, but the mechanistic reason remains unclear. More fundamentally, it's uncertain how the relationship between attention entropy and answer correctness emerges during training. The contributions of pre-training versus instruction-tuning to this phenomenon are not investigated, and there's minimal analysis of what actual patterns these supposedly important heads learn or attend to.\n\nCross-dataset transfer results reveal a worrying pattern: performance drops between 5-35% when models trained on one dataset are evaluated on another, suggesting that task-specific patterns dominate over general correctness indicators. While the paper shows that training on all datasets together improves performance, it doesn't clarify which aspects of the approach generalize and which remain task-specific. Additionally, there's no evaluation on adversarial inputs, out-of-distribution examples, or scenarios with significant domain shift, which would test the robustness of the method in challenging real-world conditions.\n\nSeveral methodological choices lack justification. The use of sparse logistic regression for classification isn't motivated—no comparison with other classifiers is provided to demonstrate why this is optimal. The paper shows limited ablation on the L1 regularization parameter, and the decision to aggregate entropy over tokens by averaging rather than exploring other aggregation schemes seems arbitrary without empirical justification."}, "questions": {"value": "1. Why do middle-layer heads specifically capture correctness signals?\n\n2. How does HEAD ENTROPY perform on out-of-distribution inputs or adversarial examples? Does the relationship between entropy and correctness hold?\n\n3. Why is performance substantially worse on MedMCQA?\n\n4. Have you tested the method on base models (non-instruction-tuned)?\n\n5. Can section-level aggregation be justified theoretically or empirically compared to alternatives?\n\n6. What happens with models that use different attention mechanisms (e.g., GQA, rotary embeddings)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2atLPI9gVP", "forum": "HJ6EGxNpgb", "replyto": "HJ6EGxNpgb", "signatures": ["ICLR.cc/2026/Conference/Submission3345/Reviewer_3nKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3345/Reviewer_3nKc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618825584, "cdate": 1761618825584, "tmdate": 1762916680981, "mdate": 1762916680981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight method to build LLM answer checkers for factual questions. The authors investigate the entropy of the multi-head attention logits for modern Transformer-based LLMs and demonstrate that with a simple linear logistic regressor, the attention entropy can be a strong cue to predict whether or not the LLMs' answers are correct or not. Extensive experiments demonstrate the effectiveness and interpretability of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is quite simple, applicable to all Transformer-based LLMs.\n- The writing is good with clear definitions and terminology."}, "weaknesses": {"value": "- About entropy calculation\n  - In Sec. 4.1, I cannot see how to process the attention entropy of different layers and different sections, and thus, I'm curious about the specific shape of $H$ $ in Equ. 8.\n- About generalizability and applicability:\n  - This paper focuses only on closed-ended factual questions with ground truth answers, and thus, we can train a binary logistic regressor. It suggests that this method cannot generalize to open-ended QA like chatting, and thus, is not as general as reward modeling.\n  - I'm wondering whether this method is only applicable for answer-extraction QA (like SQuAD), where answers should first appear in the context during responding.\n  - To demonstrate that, an applicable experiment is to 1) report the percentage of samples where the answers first appear in the responses, either thinking or answers (this will be high), and then 2) the correlation between the positions of the appearance of GT answers in the context and the positions with high entropy values.\n\n- About experiments:\n  - In Sec 4.3, although light-weighted, we still need to first finish the whole inference procedure before we can get the correctness prediction, and thus, in Sec. 6, we should also compare with reward models (e.g., LLM-as-a-judge).\n  - In Sec. 5, on all three evaluated datasets, we conduct an in-distribution setting (e.g., train on TrivalQA and evaluate on TrivalQA). I would like to see more OoD evaluation, like training on general datasets like TriviaQA and HotpotQA, and then evaluating on domain-specific datasets like MedMCQA.\n  - In Sec. 6.6, the usage of Shapley values is good, but the conclusion that mid-layers matter more cannot be a valid conclusion for the interpretability of the proposed method."}, "questions": {"value": "- In lines 76 and 80, the first characters should be capitalized.\n- Do not use different definitions of subscripts for the same RV. In Equation 5, the subscript of $H$ means head index, while in Equation 6, it means sample index."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5YFUAvPmKe", "forum": "HJ6EGxNpgb", "replyto": "HJ6EGxNpgb", "signatures": ["ICLR.cc/2026/Conference/Submission3345/Reviewer_EetY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3345/Reviewer_EetY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797192381, "cdate": 1761797192381, "tmdate": 1762916680793, "mdate": 1762916680793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}