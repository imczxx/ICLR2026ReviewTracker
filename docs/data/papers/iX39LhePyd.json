{"id": "iX39LhePyd", "number": 20143, "cdate": 1758302979569, "mdate": 1759896999344, "content": {"title": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks", "abstract": "Large language model evaluation requires thousands of benchmark items, making evaluations expensive and slow. Existing methods compute average accuracy across fixed item sets, treating all items equally despite varying quality and informativeness. We present ATLAS, an adaptive testing framework using Item Response Theory (IRT) to estimate model ability  through Fisher information-guided item selection. Our analysis of five major benchmarks reveals that 3-6\\% of items exhibit negative discrimination, indicating annotation errors that corrupt static evaluation. ATLAS achieves 90\\% item reduction while maintaining measurement precision: on HellaSwag (5,608 items), we match full-benchmark estimates using only 42 items with 0.154 MAE. Our framework maintains item exposure rates below 10\\% and test overlap at 16-27\\%, compared to static benchmarks where every model sees all items (100\\% exposure). Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with the same accuracy get different IRT scores, and 23-31\\% of all models shift by more than 10 rank positions. Code and calibrated item banks available at https://anonymous.4open.science/r/ATLAS-3210/README.md.", "tldr": "", "keywords": ["Adaptive testing", "LLM evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/654f1a32ea7da8b27ec80f4a2877c484589fab36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors:\n\n- propose and motivate an IRT framework for adaptively sampling items to use to benchmark language model (LM) capabilities\n- evaluate their algorithm on 5 benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear, concise, accurate title\n- Important problem - accurately evaluating language models' capabilities at specific tasks is good\n- The paper is well written and easy to follow. (Note: I believe some details are omitted, which I pointed out under Questions)"}, "weaknesses": {"value": "1. The goal when evaluating language models is “How good is model X on task Y?” Here, the primary metrics of interest is MAE between an IRT estimate on a subset of data and the corresponding IRT estimate on all the data. Thus, MAE is a proxy metric that doesn’t really capture what we care about.\n\n2. When considering efficiency, the real concern for practitioners is that evaluating models requires paying for accelerators (GPUs, TPUs, whatever) to run these models.  Something like “Selection Time (s)” is not a real consideration; if it takes me 60 seconds to select the next item, I could have spent those 60 seconds running inference on the models rather than choosing the next point. My guess is that, when controlling for chip-seconds, evaluating random items provides better performance than pausing for 10-75 seconds per item (Table 2) to choose the next item.\n\n3. Key experimental results are weak. Specifically, in Table 1, when evaluating how good ATLAS is, let’s first consider a reasonable null distribution. There are 7 algorithms and 3 of them are ATLAS. If all of them are equally good up to randomness (e.g., from the scores, from sampling, from model training, etc.), then we expect ATLAS to score best in 3/7 and MetaBench to score best in 2/7. It looks like ATLAS scores best in 3/5 benchmarks and MetaBench scores best in 2/5 benchmarks. **Table 1 looks to me like compelling evidence that these algorithms are basically equal.**\n\n4. Methodologically, it is unclear how finicky this methodology is or how well it works generally. The method has many degrees of freedom to play around with (e.g., which data to exclude Section 3.2, what SE threshold to use, how to set $\\tau$, etc.), leaving it ripe for unfair comparison with baselines. I also don’t see evidence of preregistration, which would preclude such post-hoc favourable treatment."}, "questions": {"value": "## Title\n\n- Solid! Thank you for a clear and concise title\n\n## Introduction\n\n- Line 051-052: Where are the citations for the benchmarks? WinoGrande, TruthfulQA, etc.\n- Line 053: Where is the citation for MetaBench?\n\n## Section 3 Methodology\n\n- Lines 141-143: “Models with extreme scores (below 0.1st percentile) are excluded to\nprevent parameter estimation instability, as IRT’s sigmoidal functions become under-constrained at\nthe boundaries.” Are models with extremely high scores (e.g., *above* 0.1st percentile) similarly excluded for the same reason?\n- Lines 171-175: Can you please add an Appendix explaining this in more detail? I can’t quite follow. What exactly does “calibrate” mean in this context? What are “linking anchors”? \n\n## Section 4 Experiments\n\n- Line 274: Why is the metric of interest (MAE) defined between $\\hat{\\theta}_{\\ell}$ and $\\hat{\\theta}_{\\ell}^{whole}$ instead of something real/tangible, such as average score on the benchmark? This MAE metric seems more focused on “Does the IRT approach yield a consistent estimate on a subset of items as it does on the full set?”, which is a proxy metric that doesn’t seem important. What we want to know is: how good is model X at task Y?\n- Why is lowered “Test Overlap Rate” considered good?\n- Table 1: nit: Please state what bold and underline and dashed underline means in the caption.\n- Table 1: When evaluating how good ATLAS is, let’s first consider a reasonable null distribution. There are 7 algorithms and 3 of them are ATLAS. If all of them are equally good up to randomness (e.g., from the scores, from sampling, from model training, etc.), then we expect ATLAS to score best in 3/7 and MetaBench to score best in 2/7. It looks like ATLAS scores best in 3/5 benchmarks and MetaBench scores best in 2/5 benchmarks. **Table 1 looks to me like compelling evidence that these algorithms are roughly equal.**\n- Table 1: Presumably, the MAEs are averaged over multiple items and/or models? If so, where are the notions of uncertainty e.g., standard errors, confidence intervals?\n- Table 1: I personally prefer visualizations over tables. You could easily and helpfully visualize this as a pointplot https://seaborn.pydata.org/generated/seaborn.pointplot.html\n- Table 2: To clarify, what does “fast selection times” measure exactly? Is this the time to select the next item? Are we presuming that we’ve already evaluated all LMs on all items?\n- Lines 359-365: The argument that small test overlap rate prevents test set contamination seems like nonsense. If I pretrain a model directly on the test set, why does the subset of items chosen for evaluation prevent the model from scoring above its true ability? \n\n## Appendices\n\n- Line 688: Why is the definition of MAE defined twice, once here and once below on line 759? Same with Average Item Exposure Rate.\n- Line 753: Why does Appendix A Evaluation Metric Definitions come after Appendix D Data Processing Details (line 727)?\n- Line 755: The reference to the Section is missing.\n- Line 786: The reference to the Section is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jXM6UWlcv7", "forum": "iX39LhePyd", "replyto": "iX39LhePyd", "signatures": ["ICLR.cc/2026/Conference/Submission20143/Reviewer_SvKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20143/Reviewer_SvKu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523231400, "cdate": 1761523231400, "tmdate": 1762933171817, "mdate": 1762933171817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ATLAS, an LLM evaluation method based on item response theory (IRT) and computerized adaptive testing (CAT). Specifically, the authors fit 3PL IRT models to LLM responses from the Open LLM Leaderboard and, during evaluation, use the IRT parameters to select items dynamically via Fisher information. In their experiments, they show that this leads to more precise ability estimates compared to recent static evaluation methods based on IRT (TinyBenchmarks, MetaBench), plus several other advantages.\n\n**NB:** ATLAS is almost identical to Fluid Benchmarking, a method proposed in a recent [COLM paper](https://openreview.net/forum?id=mxcCg9YRqj): the COLM paper also fits IRT models to the Open LLM Leaderboard and uses the IRT parameters to conduct CAT, selecting items dynamically via Fisher information, exactly as the current paper does. There are minor differences (e.g., ATLAS uses a 3PL IRT model while Fluid Benchmarking uses a 2PL IRT model), but otherwise the methods are the same. Since the COLM paper was published on July 7th, it is contemporaneous work according to the [ICLR rules](https://iclr.cc/Conferences/2025/FAQ), and I will not hold it against the authors that they did not mention it in their paper. However, given the strong similarities, I highly recommend that the authors add a discussion."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "ATLAS draws upon several decades of research in psychometrics and shows that the methods developed in that field can be fruitfully applied in the context of LLM evaluation. I liked it that the authors thought very carefully about how best to adapt IRT/CAT to the LLM domain (e.g., by using common-person calibration). The experimental setup is also sound, and the authors convincingly show that ATLAS offers advantages for LLM evaluation (but see my concerns below)."}, "weaknesses": {"value": "There are currently several weaknesses that undermine the contribution of the paper. If the authors address them, I will consider raising my score.\n\n- The experimental section misses key details. Specifically, it is unclear whether the LLMs used for evaluation were already used for fitting the IRT models (which would be problematic). Further, if there _was_ a clear train-test split, it is unclear how it was determined. This limits the credibility of the reported results.\n\n- For measuring precision, the authors solely examine how well different methods recover _ability_ as estimated on the full benchmark. However, TinyBenchmarks and MetaBench (and most other methods from the efficient evaluation literature) aim to recover _accuracy_ on the full benchmark. One reason for this is that most practitioners are interested in getting an estimate of the final accuracy, not ability. Of course, there is an intrinsic tension between the two (which the authors nicely demonstrate in the paper), and I agree in principle that ability should be preferred over accuracy. Still, this should be explicitly addressed in the paper, especially since the item sets from TinyBenchmarks and MetaBench were optimized against accuracy, meaning that the current comparison in the paper is note entirely fair. Thus, I think the authors should add an analysis of how well ATLAS recovers full-benchmark accuracy and compare against the same baselines.\n\n- The discussion on contamination is not convincing and seems to be based on wrong assumptions about contamination and LLM evaluation. Contamination happens when a model is _trained_ on items from a benchmark's test set, not when it is _evaluated_ on them during pretraining, so I do not see how CAT (a method for evaluation, not training) would offer any advantage compared to static testing. For example, the authors write that \"[e]ven smaller banks maintain low exposure rates, [...] making systematic memorization during pretraining practically impossible\" (363-365), but this does not make any sense since unlike humans, evaluation does not cause contamination with LLMs. In other words, even if a model has \"seen\" all items of a benchmark multiple times as part of evaluations during pretraining, this will not allow it to memorize any of those items."}, "questions": {"value": "- Will you release your code? What programming languages/packages did you use?\n- I was surprised that you excluded MMLU from your analysis, despite the fact that it is also part of the Open LLM Leaderboard, and TinyBenchmarks and MetaBench also examine it, so it would have been very easy for you to include it in your experiments. MMLU is also the most widely used out of all the Open LLM Leaderboard benchmarks. Can you comment on your rationale here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PLqUcK9KWJ", "forum": "iX39LhePyd", "replyto": "iX39LhePyd", "signatures": ["ICLR.cc/2026/Conference/Submission20143/Reviewer_Hs2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20143/Reviewer_Hs2V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142560531, "cdate": 1762142560531, "tmdate": 1762933171163, "mdate": 1762933171163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ATLAS, an adaptive evaluation framework that selects a subset of the benchmark for evaluation to make running evaluation cheaper. The authors propose a three parameter logistic IRT model for fitting the probability of a correct response, and then adaptively selects the most informative items using fisher information. Using ATLAS, the authors are able to prune down benchmarks by over 90% on tasks like HellaSwag."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors tackle an important problem of making evaluations cheaper for large language models, given a lot of benchmarks are used to track a given model's capabilities.\n- The proposed methodology is clear by formalizing evaluation as a latent-ability measurement with a three-parameter logistic model.\n- The results are nice compared to other baselines like TinyBenchmarks with huge reduction in size of the evaluation sets."}, "weaknesses": {"value": "- Code and calibrated items are missing in the provided link.\n- There are some inherent issues with IRT framing and using reduced sizes for evaluation of language models. See [1]\n- Inconsistent claims and results: the main text of the paper mentions good fits with RMSEA $\\leq 0.05$ but Table 4 reports otherwise.\n- Current framework is only applicable to MCQ tasks, but MCQ benchmarks have many inherent problems [2]. Generalizability to many modern evals which are free-form like math, coding, reasoning, etc. is not clear.\n\n[1]: Quantifying Variance in Evaluation Benchmarks, Madaan et al., 2024\n\n[2]: Answer Matching Outperforms Multiple Choice for Language Model Evaluation, Chandak et al., 2025"}, "questions": {"value": "I have asked most of my questions in the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gAXptuxIqF", "forum": "iX39LhePyd", "replyto": "iX39LhePyd", "signatures": ["ICLR.cc/2026/Conference/Submission20143/Reviewer_32Bm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20143/Reviewer_32Bm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159164794, "cdate": 1762159164794, "tmdate": 1762933170491, "mdate": 1762933170491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an alternative to static benchmark evaluation of LLMs using ideas from computerized adaptive testing. The method they propose is called ATLAS. They fit a 3PL IRT model to open LLM leaderboard (a collection of models, benchmarks, and binary responses); one model per benchmark. In doing so, they follow a pretty standard idea of using the Fisher information of theta (the ability score of an item) based on the fit IRT model to guide the informative item selection process. They evaluate this technique against baseline efficient LM benchmarking papers like tinyBenchmarks and MetaBench as well as against a random subsampling baseline; evaluation criteria is based on ability to accurately produce the same assessment but on fewer examples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core ideas here are interesting and can motivate new research in improvements to how we perform LLM evaluation. It’s nice to pull in techniques from other fields like psychometrics and CAT to see how we can improve our field. The experimental results showing efficiency versus baselines are good."}, "weaknesses": {"value": "I want to be upfront: There is concurrent work at COLM 2025 called Fluid Language Model Benchmarking that is highly similar (also inspiration from psychometrics & CAT, also fitting IRT models on Open LLM leaderboard, using the Fisher information for item selection, also baselining against tinyBenchmarks and MetaBench). I did not penalize this work for overlap with concurrent work as the COLM paper was published around the same time this paper was submitted.\n\nThat being said, there are some issues:\n\n**On claims that ATLAS improves data contamination**\nFirst, this paper makes a big point about “data contamination” and how this efficient LM benchmarking strategy can mitigate data contamination. Indeed, data contamination in our field is an issue, but this paper is suggesting that efficient LM benchmarking is actually a way to mitigate this issue (by simply revealing less data to model developers in the process). I don’t buy this argument at all.\n\nFor example, L037 suggests static benchmarks are easier to leak into “pretraining corpora”. The problem is, this is not solved by the proposed method. For example, let’s consider how this paper performs experiments by applying ATLAS to Open LLM leaderboard data. All of that is actually reusing benchmark data that’s already public & thus already revealed to model developers. If the research community is to adopt ideas like ATLAS for benchmarking but ultimately still rely on existing public test examples, then the contamination problem is not about efficient methods like ATLAS, it’s about public vs private test sets.\n\nOk, so now let’s consider the argument that efficient evaluation methods like ATLAS, while studied/demonstrated reusing static benchmarking data, would be deployed on new, private test sets. And thus, the fact that we make use of fewer testing examples helps prevent contamination.\n\nThere are a couple issues with this problem. First, it makes sense in computerized adaptive testing for humans. Humans, when seeing test examples, immediately learn from those observed test examples. And thus, any time you test humans, contamination happens. Language models don’t inherently learn from examples as we test them. Language model state is captured in its weights at the end of training and no amount of evaluation will update those weights unless the model developer explicitly decides to include test set examples into the training data.  So again, adaptive testing techniques like ATLAS aren’t actually addressing the contamination issue in machine learning, which is very much about (improper) practices of model developers.\n\nNow then finally, let’s consider – maybe one can argue that adaptive testing methods like ATLAS, by virtue of them withholding test examples, work well against adversarial model developers who are incentivized of including any test examples in the training data. Again, I don’t buy this argument. In the course of a normal model development cycle, let’s say fitting scaling laws or performing data mixing ablations for pretraining, model developers will train hundreds or even thousands of language models, each of which will have to undergo some ATLAS evaluation; what is the likelihood that “data contamination” is actually being addressed in this scenario? Even revealing a small percentage of the benchmark instances per model, at the large experimentation scale that naturally happens in model development, we’re likely exhausting and thus “contaminating” full static benchmark collections (or what this paper calls our “banks”) rapidly. Then the way to solve this problem is not about adaptive testing, but it’s about scalable generation of new test examples to keep up with how quickly we’re exhausting our “banks”. \n\nOverall, I find the emphasis on how ATLAS and adaptive testing improves “data contamination” is incorrect and detracts from the merits of this paper. \n\n**Unfair baseline comparisons**\nThe baseline comparison uses MAE as the target metric against the “full bank theta”. That means that the evaluation setup is assuming there exists some ground truth theta that can be estimated on the full benchmark and efficient benchmarking techniques are evaluated on their ability to approximate it with fewer examples. The problem with this is, the baselines tinyBenchmark and MetaBench weren’t designed with IRT in mind; the proposed method ATLAS was. So it is unfair to define an IRT criteria (approximate full bank theta) and show that your IRT method is better than non-IRT methods. The correct evaluation would have been to show that the proposed ATLAS method can efficiently approximate the actual benchmark evaluation metric (accuracy)."}, "questions": {"value": "I think this paper would be in very publishable state if can address those weaknesses above: Remove the claims about data contamination and add experimental results showing ability to reconstruct \"accuracy\" instead of MAE against full bank theta. Is this something palatable to the authors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SeFqutLU8z", "forum": "iX39LhePyd", "replyto": "iX39LhePyd", "signatures": ["ICLR.cc/2026/Conference/Submission20143/Reviewer_rXNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20143/Reviewer_rXNB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201481262, "cdate": 1762201481262, "tmdate": 1762933169903, "mdate": 1762933169903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}