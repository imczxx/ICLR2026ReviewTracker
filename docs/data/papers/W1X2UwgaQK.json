{"id": "W1X2UwgaQK", "number": 20481, "cdate": 1758306656184, "mdate": 1759896975456, "content": {"title": "ROPES: Robotic Pose Estimation via Score-based Causal Representation Learning", "abstract": "Causal representation learning (CRL) has emerged as a powerful unsupervised\nframework that can (i) disentangle the latent generative factors underlying highdimensional data, and (ii) learn the cause-and-effect interactions among the disentangled variables. There have been extensive recent advances in the identifiability\naspects of CRL, accompanied by some practical progress. However, a substantial\ngap remains between theory and real-world practice. This paper takes a step toward\nclosing that gap by bringing CRL into robotics, a domain that has motivated CRL.\nSpecifically, this paper addresses the well-defined robot pose estimation – the\nrecovery of position and orientation from raw images – by introducing RObotic\nPose Estimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES embodies the essence of interventional CRL by identifying those\ngenerative factors that are actuated: images are generated by intrinsic and extrinsic\nlatent factors (e.g., joint angles, arm/limb geometry, lighting, background, and\ncamera configuration) and the objective is to disentangle and recover the controllable latent variables, i.e., those that can be directly manipulated (intervened\nupon) through actuation. Interventional CRL theory establishes that variables\nthat undergo variations induced by interventions can be identified. In robotics,\nsuch interventions arise naturally by commanding actuators of various joints and\nrecording images under varied controls. Empirical evaluations in semi-synthetic\nmanipulator experiments demonstrate that ROPES successfully disentangles latent\ngenerative factors with high fidelity with respect to the ground truth. Crucially, this\nis achieved by leveraging only distributional changes, without using any labeled\ndata. The paper also includes a comparison with a baseline based on a recently\nproposed semi-supervised framework. This paper concludes by positioning robot\npose estimation as a near-practical testbed for CRL.", "tldr": "", "keywords": ["Causal Representational Learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fd4c783a8161b4dd571830098fc4639e622ed90.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that we can learn a disentangled representation of robot joints from images without using joint angle labels. The key idea is to collect data where one joint is intentionally varied at a time; by looking at how the image distribution changes under each of these controlled moves, the model learns a latent space where each dimension corresponds to a specific joint. The contribution is not about precise pose estimation, but about demonstrating that robot joint factors can be separated and identified purely from visual interventions, without supervised joint annotations. Experiments on a simulated Panda arm confirm that each latent dimension strongly correlates with its corresponding joint angle, and a simple linear probe is sufficient to recover accurate joint values from the learned representation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is built on a solid theoretical foundation, using score-based causal representation learning to ensure that each latent variable can be meaningfully linked to a joint. It introduces a practical intervention strategy for robotics, where one joint is moved at a time so the model can learn which visual changes correspond to which joint in the representation. The experiments show clear disentanglement, with each latent dimension strongly matching one joint’s variation. Finally, the model also shows robustness to occlusion, meaning it can still identify the underlying joint representation even when part of the robot in the image is blocked."}, "weaknesses": {"value": "The method still relies on knowing which joint is being varied during data collection, so while no joint angle labels are used, the learning is not fully unsupervised. The experiments are conducted entirely in simulation, leaving open how well the approach would transfer to real robot camera setups with more visual noise and complexity. The learned latent dimensions do not directly correspond to physical joint angles and require a small labelled calibration step to decode them into meaningful values. Finally, the work does not show any downstream usage of the learned representation, so it remains unclear how beneficial this disentanglement is for actual control or manipulation tasks. The paper would be significantly strengthened if these limitations were addressed through additional real-world experiments, calibration-free evaluations, and demonstrations on downstream control or manipulation tasks."}, "questions": {"value": "Based on the weakness, \n1. Since the method requires knowing which joint is being varied during data collection, how dependent is the approach on this joint-index supervision, and can it work without it?\n2. All results are in simulation. Do you have any evidence that the method can generalize to real robot images with noise and less controlled viewpoints?\n3. The latent needs a linear calibration step to recover joint angles. How stable is this mapping, and how much labelled data is needed to make it reliable?\n4. The paper does not show downstream use. Can you demonstrate how the learned representation would actually benefit robot control, planning, or imitation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "buZlFUhBhQ", "forum": "W1X2UwgaQK", "replyto": "W1X2UwgaQK", "signatures": ["ICLR.cc/2026/Conference/Submission20481/Reviewer_Nc2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20481/Reviewer_Nc2F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446857422, "cdate": 1761446857422, "tmdate": 1762933919253, "mdate": 1762933919253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper utilises recent progress of score-based causal inference in the context of learning disentangled latent variables from robot observations. The central method leverages the techniques of revealing connection between latent and observable space dimensions through score difference, and proposes to use an established binary-classifier-based method to estimate the difference. The results are validation on learning disentangled latent variables corresponding to robot joints from image observations. Causal and independent joint models are set to a simulated franka robot under single and two-camera cases. The causality-based representation learning shows good data efficiency in the domain with sparse labels and image occlusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Exploration of causality-based representation learning in robotics and general machine learning is highly needed.\n* The method design reads reasonable and not hard to grasp.\n* The results look promising under certain experiment conditions."}, "weaknesses": {"value": "* The main technical methods are from existing causal learning literature, which might be fine from an application perspective but nonetheless compromises the originality of the paper. \n* It is hard to tell whether the proposed causal learning is consistently better than baselines, especially when 100% labels are available as in Table 2. This questions the significance of the results from an empirical perspective. \n* The occlusion experiment condition looks a bit artificial and not rooted from a practical context. This may again harm the prospect of applying the proposed methods in real tasks.\n* The method seems difficult to scale to cases with a large number of latent factors. See questions below.\n* Needing interventional data, which might not always be available in typical robot datasets."}, "questions": {"value": "* Can the paper elaborate more on the results from Table 2? Why ROPES numbers are bolded even RoboPEPP with 100% labels has significantly lower MSE?\n* How can we expect the method to scale to more or an unknown number of latent factors when we have to train a binary classifier for each pair of variables? \n* Line 454 states \"robustness to real-world corruptions\" while the next line goes with \"we introduce artificial occlusions in the form of 32x32 white pixels squares\". How can such a corruption pattern be regarded as \"real-world corruption\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oHsNdjulC5", "forum": "W1X2UwgaQK", "replyto": "W1X2UwgaQK", "signatures": ["ICLR.cc/2026/Conference/Submission20481/Reviewer_xQzc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20481/Reviewer_xQzc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907558584, "cdate": 1761907558584, "tmdate": 1762933918307, "mdate": 1762933918307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a weakly supervised framework for recovering robot joint angles from images without requiring fully labeled data. The approach uses interventional causal representation learning (CRL), treating joint angles as controllable latent variables that can be identified through distributional changes induced by joint-level interventions. The method employs a three-stage pipeline: (1) dimensionality reduction via an autoencoder, (2) score difference estimation using binary classifiers, and (3) latent space disentanglement via a second autoencoder with sparsity constraints. The authors evaluate ROPES on a robot arm in simulation, demonstrating successful disentanglement of joint angles and competitive performance against a supervised baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper builds on solid theoretical foundations from score-based CRL and bridges the theory–practice gap by applying it to a robotics problem. As also discussed by the authors, the pose estimation problem has various applications within the robotics domain.\n\n\n2. The empirical results are strong, and the evaluation is fairly comprehensive (with some limitations noted below), covering multiple conditions, ablations, and comparisons against a SOTA method."}, "weaknesses": {"value": "1. While the authors claim their method is completely label-free (L161), I have doubts about this. The method requires knowing which joint was intervened on for each dataset, which creates a form of weak supervision. Moreover, in the linear calibration step, a small labeled dataset of ground truth samples is required. Additionally, the interventional distributions need to be sufficiently distinct, probably requiring careful design of the experiments by the authors. Overall, the claim of being entirely label-free and unsupervised is somewhat misleading, and I believe the supervision and labeling simply occur at a different level rather than through traditional annotations.\n\n2. It would have been useful to see results obtained on a real robot. The method also assumes static backgrounds, so its performance in dynamic environments remains unclear.\n\n3. The pose estimation results are not compared with any standard pose estimation algorithms or even classical methods. While such methods typically require detailed labels to train, including them for comparison would provide better insight into the proposed method’s capabilities.\n\n4. The three-stage training process likely introduces significant computational overhead, which is not clearly discussed in the paper. The authors should include a comparison of computational costs against standard baselines. \n\n5. The paper would benefit from a more explicit discussion of failure cases and limitations. Some example questions that could be addressed are provided below."}, "questions": {"value": "1. Can the method handle simultaneous interventions on multiple joints, potentially reducing the data collection?\n\n2. How does the method perform when some joints are occluded or not visible from any camera angle?\n\n3. How robust is the method to dynamic backgrounds? Nothing too extreme, for instance, if one set of trajectories is collected during the day and another at night, or if objects in the background are moved between robot movements?\n\n4. What are the challenges in carefully designing the interventions to ensure the distributions are sufficiently distinct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LdYLb3zMb4", "forum": "W1X2UwgaQK", "replyto": "W1X2UwgaQK", "signatures": ["ICLR.cc/2026/Conference/Submission20481/Reviewer_PMxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20481/Reviewer_PMxf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946782636, "cdate": 1761946782636, "tmdate": 1762933917586, "mdate": 1762933917586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised framework named ROPES that applies causal representation learning to robot pose estimation. The goal is to recover the interpretable latent factors such as joint angles, using only interventional distributions rather than supervised labels. The proposed method consists of two autoencoders and a log-density ratio estimator, trained jointly to enforce identifiability under interventions. Experiments on the Panda-Gym simulator show that the proposed method can recover causal latent factors and achieve competitive reconstruction accuracy compared with a supervised baseline of RoboPEPP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper considers the usage of causal representation learning into the robotics domain, considering a popular and reasonable question of whether CRL can work in practice. This is an important and timely problem in the area\n2. The proposed method is clearly presented, including theoretical insights and an end-to-end design with two autoencoders and a log-density ratio estimator. The math formulation is easy to follow."}, "weaknesses": {"value": "1. The proposed method largely applies well-known score-based CRL results to the robot application. There is no significant theoretical or algorithmic advance beyond adapting the framework to robot pose estimation.\n2. All the experiments are performed in the Panda-Gym system with grayscale synthetic images. However, the authors claim that they bridged the theory and practice. This is overstated without real visual data for validation. A small real-world test would be better to support this claim.\n3. The experiments only include one baseline RoboPEPP (a supervised method), which is not sufficient. Meanwhile, this paper lacks ablations studies, e.g., removing LDR or sparsity constraints, and deeper analysis of learned latent representations. It remains unclear which components are crucial for performance and how interpretable the learned latents truly are."}, "questions": {"value": "1. Is there any experiments on real-world tasks that apply ROPES to real image data?\n2. How sensitive is ROPES to the choice of hyperparameters or encoder architecture?\n3. Would it be feasible to extend ROPES to temporal settings with sequential observations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "51QQwKJkoO", "forum": "W1X2UwgaQK", "replyto": "W1X2UwgaQK", "signatures": ["ICLR.cc/2026/Conference/Submission20481/Reviewer_vQzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20481/Reviewer_vQzk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975897843, "cdate": 1761975897843, "tmdate": 1762933916984, "mdate": 1762933916984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}