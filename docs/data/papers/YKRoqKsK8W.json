{"id": "YKRoqKsK8W", "number": 20725, "cdate": 1758309394795, "mdate": 1759896961722, "content": {"title": "Rank-Then-Act: Reward-Free Control from Frame-Order Progress", "abstract": "We introduce Rank-Then-Act (RTA), a novel reward-free control framework that enables policy learning without extrinsic task rewards. Instead, RTA uses a progress-percentage signal derived from expert video demonstrations (evaluated via rank correlation). Specifically, we train a Vision–Language Model (VLM) progress scorer offline with a Group Relative Policy Optimization (GRPO) objective, assigning progress percentages to shuffled frames from expert gameplay. This scorer is then frozen and used to provide feedback during reinforcement learning (RL). During policy learning, the agent receives as reward the Spearman correlation coefficient between the VLM scorer's predicted progress percentages for a window of recent observations and their true environment timestamps, yielding a bounded, time-aligned progress signal without explicit task rewards. On the PyBoy Catrap environment, RTA enables a VLM-based agent to solve levels using only expert videos, without any reward engineering. Our results demonstrate that training VLMs to act in games without extrinsic rewards is a promising and scalable direction for generalizing RL to real-world settings where reward specification is impractical or impossible.", "tldr": "RTA  learns control without extrinsic rewards by ranking shuffled expert-video frames and using Spearman progress–time correlation as reward.", "keywords": ["Vision-language models", "Reward-free training", "Multi-step Agents"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03eada77be8a7ef5aa9dfeb831a62e32de385341.pdf", "supplementary_material": "/attachment/f4e814bccf14684b7a9e5aca456aa4db63eb0cd6.zip"}, "replies": [{"content": {"summary": {"value": "This paper learns a ranking-based reward from expert demonstration videos by assigning progress scores to a shuffled set of k frames. They then use this model as a reward by calculating the Spearman rank correlation between the predicted and ground truth orderings, and train a VLM-based policy."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method of learning a progress reward by using the Spearman rank correlation between the progress and ground truth frame scores is an original and distinct way of learning a reward function from expert videos. I think that this paper distinguishes itself from other work that learns a progress reward in this regard by using a novel methodology. Their methods section and explanation were clear."}, "weaknesses": {"value": "In order to verify the efficacy of their method, in the plots or tables that just illustrate their learned reward learning curve, I would like to see either a success rate or an oracle reward.  Including a comparison to a non-VLM ranking method (in the related works, it describes rank2reward [3] as similar) and a non-VLM-based policy class would help demonstrate the effectiveness and show the benefits of using a VLM.   In section 4.2, the authors claim that per-level training is more robust, which is also shown in Figure 4.  Improved generalization capabilities, as described in [2, 4], are one of the advantages of using a VLM as a base model,  so I would like to understand the benefits a VLM brings to this work.\n\nThis paper compares against two baselines. In Figure 5 and Table 1, there is a comparison against a baseline model that lacks reward, which achieves 0% success, and a reward of 0 under their learned reward function.  Based on a correlation of 0, I suspect that the baseline is selecting random actions, but I would like more details about what this baseline is.  In Table 1, they also compare expert-supervised fine-tuning to RTA (their method). Including the other five levels, providing more detail about the SFT baseline, and reporting success rates instead of learned rewards would make this comparison clearer. \n\nFor the expert SFT, they augment a dataset of 200 trajectories with chain of thought reasoning, and fine-tune their VLM on this dataset. I wonder what the effects of chain of thought reasoning are on this SFT policy performance. I think comparing against a smaller policy class, given the single-task nature of this problem formulation, or a VLA, which is the standard way to adapt a VLM to a policy [1, 2], would help demonstrate the strengths of this method. \n\nThe authors also claim that other inverse RL methods have not been adapted to the vision-language setting at scale. They do not use language conditioning in a goal conditioning way (as mentioned in section 3.1), and inverse RL methods have been robustly applied to image-based robotic manipulation, as well as Atari-style games [3, 4, 5]. I also think a comparison to a sparse reward DQN would help show the performance of RTA. \n\nThe paper also claims that they do not require any task-specific supervision, but this method does require expert demonstration videos on a per-task basis.\n\n[1] Kevin Black, Noah Brown, Danny Driess, et al. (2024). Pi0: A Vision-Language-Action Flow Model for General Robot Control.\n\n[2] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, et al. (2024). OpenVLA: An Open-Source Vision-Language-Action Model.\n\n[3] Daniel Yang, Davin Tjia, Jacob Berg, et al. (2024).  Rank2Reward: Learning Shaped Reward Functions from Passive Video.\n\n[4] Yecheng Jason Ma, Joey Hejna, Ayzaan Wahid, et al. (2024) Vision Language Models are In-Context Value Learners.\n\n[5] Aaron Tucker, Adam Gleave, Stuart Russell. (2018). Inverse reinforcement learning for video games."}, "questions": {"value": "Why in Table 1 does the SFT baseline, when it doesn’t beat RTA, have a high negative correlation?  It appears to have a higher magnitude correlation (either positive or negative) than RTA across the three levels.\n\nWhat are the reasons for using a VLM as the base policy class and the base model for the learned reward function? I would also be curious to see the effects of including vs not including the reasoning traces.\n\nWhy only score every N steps instead of at every step during policy training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FmsGbKObRJ", "forum": "YKRoqKsK8W", "replyto": "YKRoqKsK8W", "signatures": ["ICLR.cc/2026/Conference/Submission20725/Reviewer_UiJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20725/Reviewer_UiJZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761173925334, "cdate": 1761173925334, "tmdate": 1762934140031, "mdate": 1762934140031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage approach for control without environment rewards. Stage 1 trains a VLM scorer on shuffled expert demonstration frames to predict progress percentages via listwise ranking (GRPO) with an anchor–shuffle mechanism. Stage 2 freezes the scorer and uses Spearman correlation between predicted progress and temporal indices over sliding windows as the reward signal for PPO-based policy learning. Evaluated primarily on PyBoy Catrap."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, implementable correlation-based reward signal.\n\n- Anchor–shuffle mechanism to reduce trivial temporal shortcuts.\n\n- Clear method description with hyperparameter tables; the pipeline is easy to reproduce in principle."}, "weaknesses": {"value": "- Evidence too weak to justify publication. The paper reports results from only 2 seeds, EMA-smoothed, with no error bars, confidence intervals, or per-seed reporting. This violates basic standards for RL research. Evaluation for policy learning is limited to a single game (Catrap): while the progress scorer is illustrated on a few other games, the end-to-end control results (Stage-2 PPO with the learned reward) are reported only on Catrap. No Atari, continuous control, robotics, or even other PyBoy games for policy training. Additionally, no ablations on critical design choices: window size (m), query frequency (N), number of shuffles (L), temperature, anchor vs. no-anchor, or robustness to parsing failures (regex match -> R_min = -1). The LOOP variant uses “starting-point refreshing” (a save-state curriculum from best states), which isn’t standard RL protocol and appears necessary for success; the paper’s own figure suggests reward can rise without success unless refreshing is used. Required to meet the empirical bar: >=5 seeds with proper statistical analysis, broader task coverage, and principled ablations.\n\n- Related-work omissions materially overstate novelty. The paper claims to be \"the first approach that enables a VLM to learn control policies in a fully reward-free environment.\" That claim is false: prior work already uses VLMs (e.g., CLIP-based reward models) to train policies with no environment rewards. In addition, the key ingredients have prior art: (i) VLMs as zero-shot reward models for control without env rewards (VLM-RM from Rocamonde et al.), (ii) learning progress/value by ordering shuffled frames with VLMs (GVL from Ma et al.), (iii) extracting shaped rewards from passive video via temporal ranking (Rank2Reward from Yang et al.), and (iv) temporal order as self-supervision (Shuffle & Learn from Misra et al.). The specific combination of Spearman correlation over windowed predictions may be novel, but claiming \"first\" while omitting comparisons to these directly relevant methods significantly overstates the contribution. There are no baselines against VLM-RM (CLIP-style rewards), Rank2Reward, T-REX/D-REX, or XIRL/ORCA-style temporal alignment methods.\n\n- Brittle core assumption (monotonic progress). The method presumes progress increases with time inside successful demos: r_t ∝ ρ(predicted_progress[W_t], time_index[W_t]), where W_t is a sliding window. This assumption is frequently false in multi-stage manipulation (staging, re-grasps, approach–retreat) and puzzle games (backtracking, exploration). The paper neither (i) justifies the assumption’s scope, (ii) stress-tests on backtracking tasks, nor (iii) validates that $\\rho$ correlates with actual task success rather than demo tempo or superficial local coherence. The reward measures agreement with a particular demonstration’s temporal ordering, not task progress per se. A direct episodic analysis of corr($\\rho$, return) is needed.\n\n- Non-Markovian reward without proper treatment. The reward at time t depends on a window W_t = (s_{t−m+1}, …, s_t), i.e., on history the policy may not observe, while PPO is run as if the setting were standard MDP RL. At minimum, the paper should acknowledge the theoretical mismatch and either (i) include the entire reward window in the observation or use recurrence to restore effective Markovianness, or (ii) provide empirical justification that the mismatch does not harm training. As written, the assumptions behind the PPO setup are not satisfied."}, "questions": {"value": "The following requests correspond to unanswered questions in the paper which I believe must be addressed to clear the bar for publication.\n\n- Provide ≥5 seeds with proper statistical reporting (means, confidence intervals, and per-seed curves) for all main results.\n\n- Broaden control evaluation beyond Catrap: include at least one additional PyBoy game for policy learning and a non-Atari continuous-control or manipulation domain to support generality claims.\n\n- Add fair baselines with matched compute and data budgets against directly relevant methods, as judged by the authors (e.g. VLM-RM, Rank2Reward, T-REX/D-REX, and XIRL/ORCA-style temporal alignment).\n\n- Improve ablations. Window mechanics: vary window size m and query frequency N; compare stacking enough frames so the observation covers the reward window vs. adding recurrence; report sensitivity. Anchor–shuffle ablations: anchor vs. no-anchor; number of shuffles L; temperature; show effects on both scorer quality and policy performance.\n\n- Quantify how well the proposed reward aligns with task success: report episode-wise correlation/calibration between the ρ-based reward and returns/success across levels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "48jr1mitUX", "forum": "YKRoqKsK8W", "replyto": "YKRoqKsK8W", "signatures": ["ICLR.cc/2026/Conference/Submission20725/Reviewer_jdVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20725/Reviewer_jdVY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925643695, "cdate": 1761925643695, "tmdate": 1762934136843, "mdate": 1762934136843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage reward-free reinforcement learning framework that enables policy learning from expert video demonstrations without explicit task rewards. In Stage 1 (Rank), a vision–language model is fine-tuned using a GRPO objective to predict per-frame progress percentages from shuffled expert video clips. In Stage 2 (Act), the frozen progress scorer is used online to provide feedback: the Spearman correlation between predicted progress and environment timestamps over recent observation windows serves as the sole reward for policy optimization. Experiments on the PyBoy Catrap game demonstrate that RTA can train an agent to solve tasks using only video-based progress signals."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores a new formulation of reward-free control by leveraging rank correlation from expert video progress, removing the need for explicit rewards or adversarial imitation. The idea of using Spearman correlation of predicted progress as a dense reward is well-motivated.\n2. The comparison with expert SFT baselines highlights the effectiveness of the progress-based rewards."}, "weaknesses": {"value": "1. The overall design (training a model to score trajectories and using that score as a reward signal) resembles earlier works such as [A] and [B]. Although RTA’s reward shaping via video shuffle and Spearman correlation adds new intuition, it still assumes that short video clips encode the full policy value—a strong assumption that may not generalize to long-horizon or partially observable tasks.\n2. The experiments are conducted only in a simplified simulation (PyBoy Catrap). Validation on more complex or real robotic control tasks would strengthen claims about generality. The paper would benefit from broader evaluation across standard RL benchmarks or real-world data.\n3. Given that RTA relies on expert video data, comparisons with recent offline RL or inverse RL methods that use expert demonstrations are missing. Including such baselines (e.g., Rank2Reward [B]) would clarify the actual improvement over prior approaches.\n\nRef:\n\n[A] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning.\n\n[B] Rank2Reward: Learning Shaped Reward Functions from Passive Video."}, "questions": {"value": "1. How does the proposed progress–time correlation reward perform when expert videos differ in the visual domain or embodiment (e.g., human videos vs. simulated robot views)? Would the scorer still generalize?\n2. Could the authors extend RTA to handle long-horizon or hierarchical tasks, where local frame-order consistency does not imply overall progress?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xshGqe40cd", "forum": "YKRoqKsK8W", "replyto": "YKRoqKsK8W", "signatures": ["ICLR.cc/2026/Conference/Submission20725/Reviewer_Pa6W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20725/Reviewer_Pa6W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225640472, "cdate": 1762225640472, "tmdate": 1762934133587, "mdate": 1762934133587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Rank-Then-Act framework, which uses a progress-percentage signal derived from expert video demonstrations. The framework trains a Vision–Language Model (VLM) progress scorer offline with a Group Relative Policy Optimization (GRPO) objective, assigning progress percentages to shuffled frames from expert gameplay. The progress percentage is used to provide feedback during training RL policy. On the PyBoy Catrap environment, RTA enables a VLM-based agent to solve levels using only expert videos, without any reward engineering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Learning control directly from pixels without extrinsic rewards remains a significant challenge for the research community. This work is both timely and impactful, presenting a simple yet effective solution that successfully addresses several key technical obstacles.\n\n2. Through extensive experiments in PyBoy environments, the authors empirically demonstrate that the proposed rank-correlation signal, i.e., derived from progress-percentage predictions, enables agents to solve tasks entirely without relying on environment rewards."}, "weaknesses": {"value": "1. **Lack of reasoning, justification, and supporting evidence in the introduction:** The paper aims to develop a robust method for predicting progress signals from expert video snippets to facilitate VLM-based policy learning. However, the introduction does not sufficiently establish the *motivation* or *necessity* for this approach. Specifically, the authors should (1) highlight the key limitations of existing methods, (2) provide clear reasoning on why these limitations hinder achieving the desired objectives, and (3) present supporting evidence or analysis to justify the proposed solution. As it stands, the introduction primarily summarizes *what* the paper does, rather than why the approach is needed or how it advances the field.\n\n2. **Missing baseline comparisons for the proposed progress scorer:** One of the central contributions of this work is the proposed *progress scorer*. However, the experimental section does not include direct comparisons with existing progress prediction or reward modeling methods. It remains unclear how the proposed scorer performs relative to prior approaches such as:\n    - Ma et al., Vision-Language Models Are In-Context Value Learners, ICLR 2025\n    - Hung et al., VICtoR: Learning Hierarchical Vision–Instruction Correlation Rewards for Long-Horizon Manipulation, ICLR 2025\n    - Rocamonde et al., Vision–Language Models Are Zero-Shot Reward Models for Reinforcement Learning, ICLR 2024\n        \nA side-by-side quantitative and qualitative comparison is necessary to substantiate the claimed improvement.\n        \n3. **Lack of baseline comparisons in policy evaluation:** The authors should also evaluate the downstream policy learning performance by re-implementing or adapting the above baselines within the same framework. Without this comparison, it is difficult to determine whether the observed improvements stem from the proposed progress scorer or from other design factors in the training pipeline.\n\n4. **Insufficient experimental validation of the primary contribution:** Prior works [1–3] have conducted extensive experiments using their respective progress or reward models across diverse tasks. To convincingly establish the first claimed contribution, the authors should provide a more comprehensive evaluation covering multiple domains and difficulty levels, demonstrating both robustness and generality.\n\n5. **Limited diversity in evaluation domains:** While the proposed approach shows promising results within the PyBoy environment, its generalization to other application domains—such as robotic manipulation, as explored in [1–3]—remains untested. How well would the proposed framework transfer to such real-world or embodied settings? Expanding the evaluation beyond the current synthetic setup would substantially strengthen the work’s impact and credibility."}, "questions": {"value": "1. Motivation and justification: Can the authors elaborate on the specific limitations of existing progress prediction or reward modeling approaches that motivate the need for this new method? What concrete evidence supports the claim that current methods are insufficient for VLM-based policy learning?\n\n2. Comparative performance of the progress scorer: How does the proposed progress scorer quantitatively and qualitatively compare against established baselines such as Ma et al. (ICLR 2025), Hung et al. (ICLR 2025), and Rocamonde et al. (ICLR 2024)?\n\n3. Downstream policy evaluation: Have the authors evaluated how the proposed scorer impacts downstream policy learning performance compared to re-implemented or adapted versions of existing baselines? If not, how can the authors ensure that the observed gains are indeed due to the progress scorer rather than other design differences?\n\n4. Experimental comprehensiveness: The current experiments seem limited to a narrow task set. Could the authors expand the evaluation to include multiple domains and difficulty levels to better demonstrate the robustness and generality of the proposed framework?\n\n5. Generality and transferability: Beyond the PyBoy environment, how well would the proposed framework generalize to real-world or embodied domains (e.g., robotic manipulation)? Are there foreseeable limitations or required adaptations to achieve such transferability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V9BaY4Wkqw", "forum": "YKRoqKsK8W", "replyto": "YKRoqKsK8W", "signatures": ["ICLR.cc/2026/Conference/Submission20725/Reviewer_mxSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20725/Reviewer_mxSc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762266326412, "cdate": 1762266326412, "tmdate": 1762934126831, "mdate": 1762934126831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}