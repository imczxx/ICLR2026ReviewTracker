{"id": "XI1CeufywD", "number": 19997, "cdate": 1758301311242, "mdate": 1759897007339, "content": {"title": "To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration", "abstract": "The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision \\emph{floating-point} formats, which inherently provide numerical stability, memory savings, and hardware efficiency without dequantization overhead. In this paper, we present a theoretical and empirical study of an \\emph{exponent concentration} phenomenon in GenAI weights: exponents consistently exhibit low entropy across architectures and modalities. We show that this arises naturally from $\\alpha$-stable distributions induced by stochastic gradient descent, and we prove tight bounds on the entropy of exponents. Our analysis establishes a theoretical compression limit near FP4.67, which motivates the design of a practical FP8 format. Building on these insights, we propose \\textbf{Exponent-Concentrated FP8 (ECF8)}, a lossless compression framework with entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs up to 671B parameters demonstrate up to 26.9\\% memory savings and 177.1\\% throughput acceleration, with perfectly lossless computations, i.e., no deviation in model outputs. Our results establish exponent concentration as a statistical law of trained models and open a principled path for lossless low-precision floating-point design in the FP8 era.", "tldr": "By understanding that the exponents of generative AI model weights have a low-entropy structure, we developed ECF8, a lossless 8-bit floating-point format that significantly compresses these models without losing accuracy.", "keywords": ["Compression", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/192ea6ab88c378b431a69b3d06bae20a5004186c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper builds on prior observations that the exponents of weights in modern transformer models have low entropy and concentrate near zero. Motivated by this, the authors propose ECF8, a lossless FP8 weight compression format. Beyond the format itself, they implement a full pipeline that exploits ECF8 to reduce memory usage and improve latency/throughput, while preserving bit-exact outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper investigates a fairly new direction in this area, the results are promising, and the evaluation is fairly thorough."}, "weaknesses": {"value": "Please see questions below."}, "questions": {"value": "Questions for the authors:\n\n1. Entropy bounds vs. closed form.\nTheorem 2.1 claims the entropy is \"finite for all \\alpha > 0\" based on lower and upper bounds for the two-sided geometric distribution. However, the Shannon entropy of the two-sided geometric has a closed-form expression. Why derive bounds instead of using the closed form directly?\n\n2. Compute-bound regimes and FP8 tensor cores.\nThe reported throughput gains appear to come from memory/VRAM-bounded scenarios. In a compute-bound regime, FP8 weights-and-activations baselines can exploit FP8 tensor cores (with ~2× the FLOPs of BF16 cores). How would a weight-only compression scheme like ECF8 sustain the large speedups shown (e.g., Table 1) once the model is compute-bound? It would help to: (i) explicitly characterize when results are memory- vs. compute-bound, (ii) quantify decode overhead (GB/s, kernel occupancy) and any GEMM slowdowns, and (iii) report the break-even batch/sequence lengths where ECF8 no longer provides throughput gains relative to FP8 weights-and-activations.\n\n\n3. Verification beyond visuals.\nFigure 3’s visual comparisons are helpful but not sufficient to demonstrate bit-exactness. Please include bit-exact checks for LLMs (e.g., matching logits/activations over a corpus with deterministic seeds) and add accuracy evaluations using lm-evaluation-harness, LightEval, or similar to strengthen the lossless and accuracy claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vqfI1u6lBf", "forum": "XI1CeufywD", "replyto": "XI1CeufywD", "signatures": ["ICLR.cc/2026/Conference/Submission19997/Reviewer_n2wB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19997/Reviewer_n2wB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905018075, "cdate": 1761905018075, "tmdate": 1762932900899, "mdate": 1762932900899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new format, Exponent-Concentration FP8, which utilises entropy-aware exponent bit encodings. Introduction of this format is motivated by an empirical observation: exponent entropy for models stored in FP16 being significantly smaller than the number of allocated bits. Memory savings and inference speedups are demonstrated experimentally for generative models across vision and language domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper leverages exponent values concentration (as an instance of weights distribution trait) to propose a new numerical format.\n2. Validation range is solid: it spans language, vision, and multimodal models up to ~670B parameters. The authors report throughput gains via larger batch sizes under fixed memory constraints."}, "weaknesses": {"value": "1. Theoretical claim that model weights follow $\\alpha$-stable distributions is presented as an intuition with no guarantees or conditions provided: \"often exhibits\", “after many updates”, “approximately follow” (lines 136 - 140). No reference to the empirical work showing the key observation of gradient noise being power-law distributed (line 137).\n2. The derivation of the compression limit is not mathematically strict. The 2.67 entropy bound for exponent is derived from a single case of Normal distribution. Adding 2 more bits on top of that number does not give a strict bound for the format compression. Again, it’s more of an observation rather than a theoretical limit.\n3. The “statistical law of trained models” (in abstract) is an overstatement. It is an empirical observation on a relatively small set of models.\n4. Limited novelty: the key idea of exponent concentration and Huffman encodings is similar to that of DFloat11 [1] or ZipNN [3], only applied to FP8 format instead of FP16. Applying Huffman encodings to exponent streams is an implementation detail, not novel improvement. \n5. Missing baselines: no experimental comparison with other FP8-specific compression formats like [2].\n6. Mismatch in Table 1, it mentions H200 in the caption while the table lists numbers for H100. Authors should either provide H200 numbers or modify the caption.\n7. No similar analysis for mantissa values distribution or justification why mantissa compression is not worth pursuing.\n8. “Visual quality assessment” from Figure 3. is not sufficient to demonstrate lossless compression. The authors should provide numerical loss results to back that statement."}, "questions": {"value": "1. Regarding the inference acceleration: at identical batch size and hardware, what are the latency and throughput improvements over native FP8?\n2. How does your method compare with [2] ?\n\nReferences:\n\n[1] 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float, Zhang et al.\n\n[2] Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats, Heilper & Singer.\n\n[3] ZipNN: Lossless Compression for AI Models, Hershcovitch et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Ni5Vdf2RR", "forum": "XI1CeufywD", "replyto": "XI1CeufywD", "signatures": ["ICLR.cc/2026/Conference/Submission19997/Reviewer_XgxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19997/Reviewer_XgxS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920298056, "cdate": 1761920298056, "tmdate": 1762932900372, "mdate": 1762932900372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the empirical observation that FP8 model weights exhibit exponent concentration as the exponent in FP8 is used with much lower entropy (2-3 bits range) than the full 4-bit range. The authors provide an analytical justification for this approach via a heavy-tailed/α-stable argument, and then exploit it by proposing a lossless FP8-compatible encoding (ECF8) that entropy-codes only the exponent, while preserving the sign and mantissa unchanged. They implement a GPU-friendly Huffman decoder with hierarchical LUTs and show that, on high-end GPUs and large FP8 models (including large LLMs and diffusion models), they can reduce memory footprint and translate that into real throughput benefits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear motivation and analysis by showing the phenomenon (low-entropy exponents in FP8 weights) and then giving an analytical story for it.\n\n\n2. Real speedups with implemented CUDA kernels. The authors implement a GPU-friendly, hierarchical Huffman decoder and demonstrate that it can induce throughput gains.\n\n\n3. Evaluations on high-end, current GPUs using recent NVIDIA GPUs make the results relevant for current inference/serving stacks.\n\n\n4. Large-scale, end-to-end results by showing the method on large FP8 LLMs / diffusion-style models."}, "weaknesses": {"value": "1. Incrementality vs DF11 / prior compressed-float ideas. A substantial part of the contribution can be read as taking the DFloat/DF11 observation that float fields are compressible, but tailoring it to FP8. That’s useful, but it’s not a completely new compression principle. It’s a focused, FP8-era instantiation.\n\n\n2. Missing detailed kernel/micro benchmarks. The paper reports headline end-to-end numbers, but it doesn’t fully break down kernel performance across tensor shapes/sizes, so it’s hard to see where the method starts to pay off and where decode overhead starts to dominate.\n\n\n3. The scope is unclear beyond weights. The paper primarily discusses compressing weights. There is no parallel, equally detailed analysis for activations (or even gradients), even though those often have different distributions and would be interesting for offloading or distributed setups.\n\n\n4. Interaction with distribution-smoothing methods is not discussed. Recent methods that apply PTQ for making FP8 models can change exponent usage. It’s not clear whether the proposed entropy gap persists under such transformations, or whether the gain shrinks.\n\n5. Runtime measurements for the encoder are missing."}, "questions": {"value": "1. Can you provide kernel-level benchmarks across a range of matrix/tensor sizes? In particular, how do compression ratio and effective runtime change for small vs large tensors? A plot of size vs throughput would clarify the operating regime.\n\n\n2. Can you show a roofline or bandwidth/compute tradeoff (similar to MARLIN[Frantar et al.]) plots, comparing vanilla FP8 loads vs FP8+your decoder? That would make the performance story crisper.\n\n\n3. Do the reported end-to-end measurements include the prefill stage for LLMs, or are they only for decode?\n\n\n4. How exactly do you use the GPU memory hierarchy (shared/L2/registers) to hide the decode overheads? A description would be helpful.\n\n\n5. Can you report the absolute overhead of decoding (e.g. μs per MB or % of layer time) so we can tell how often this is actually free?\n\n6. Is the current implementation restricted to weights, or have you tried the same exponent-entropy analysis on activations and gradients? If not, do you expect the same level of concentration? What if outlier mitigation methods(like Hadamard) are applied to them?\n\n7. Do the quantized models (from bf16/fp16 to fp8 using PTQ methods) exhibit the same distribution in the exponent?\n\n8. Could you provide runtime measurements for the encoder? compared to the decoder, of course."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R5FEH4PcK8", "forum": "XI1CeufywD", "replyto": "XI1CeufywD", "signatures": ["ICLR.cc/2026/Conference/Submission19997/Reviewer_6N2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19997/Reviewer_6N2r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762171072274, "cdate": 1762171072274, "tmdate": 1762932899868, "mdate": 1762932899868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Exponent-Concentrated FP8 (ECF8), a lossless compression framework for generative AI model weights. The authors discover that model weight exponents have low entropy (around 2–3 bits) across architectures, a phenomenon they term exponent concentration. They explain it theoretically via α-stable distributions arising from stochastic gradient descent and derive a compression limit near FP4.67, motivating FP8 as a practical format.\nECF8 implements entropy-aware Huffman coding and a GPU-optimized decoding kernel, achieving up to 26.9% memory savings and 177.1% throughput gains with zero accuracy loss across LLMs and diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors provide a rigorous explanation for exponent concentration through α-stable distributions and derive entropy bounds analytically. This bridges an important conceptual gap between statistical properties of trained models and practical compression techniques.\n- The GPU-friendly Huffman decoding kernel and tensor management system show careful systems design. The just-in-time decompression via PyTorch hooks is clever.\n- The study spans diverse models (8B–671B parameters), modalities (text, image, video), and hardware setups, lending credibility to the generality of the proposed approach."}, "weaknesses": {"value": "- While the paper demonstrates strong GPU-side results, there’s limited discussion of how easily ECF8 could be integrated into existing frameworks or inference stacks beyond PyTorch (e.g., TensorRT, vLLM, serving pipelines).\n- The focus is exclusively on inference and storage. It remains unclear if exponent concentration also appears during training or whether ECF8 could be adapted for that phase."}, "questions": {"value": "- You claim exponent concentration is a statistical law of trained models. Could you quantify how universal this phenomenon truly is?\nFor instance, do all model families (transformers, CNNs, diffusion models, MoEs) exhibit similar entropy patterns, or are there exceptions (e.g., sparse or quantized weights)?\n- The α-stable assumption underpins your entropy bound derivation. Have you empirically verified that the tails of real model weight distributions match α-stable fits (versus log-normal or Laplacian alternatives)?\n- How easily can ECF8 be integrated into existing inference frameworks (like TensorRT, vLLM, or DeepSpeed-Inference)?\n- While ECF8 accelerates inference under memory constraints, have you measured absolute decoding overhead compared to a non-compressed FP8 baseline when memory is not a bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTqAxkVvGM", "forum": "XI1CeufywD", "replyto": "XI1CeufywD", "signatures": ["ICLR.cc/2026/Conference/Submission19997/Reviewer_m6i1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19997/Reviewer_m6i1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240040979, "cdate": 1762240040979, "tmdate": 1762932899277, "mdate": 1762932899277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}