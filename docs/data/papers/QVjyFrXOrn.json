{"id": "QVjyFrXOrn", "number": 9508, "cdate": 1758125404668, "mdate": 1759897715401, "content": {"title": "MoSEL: Modular Self-Reflective Learning for Embodied Decision-Making", "abstract": "Enabling robots to autonomously perform complex, long-horizon tasks remains challenging due to the need for hierarchical reasoning and dynamic adaptability. Humans overcome this by interacting with environment and learning from their own experience, which is infeasible for existing robots without human supervision. To enable similar capabilities in robotic agents, we introduce MoSEL, an modular self-reflective learning framework for robotic decision making. MoSEL combines hierarchical planning with multimodal foundation models, including LVLMs, video diffusion, and inverse dynamics models. These components work together to break down complex tasks, generate executable visual plans, and perform actions. We further introduce a modular self-reflective learning framework that autonomously identifies failures and iteratively refines policies with minimal human intervention. Evaluations on LIBERO-LONG and RoboTwin benchmarks demonstrate that MoSEL outperforms existing methods, achieving over $33\\%$ and $46\\%$ average performance improvements, respectively. Our results underscore the effectiveness of autonomous self-improvement and accurate failure identification in advancing robust robotic manipulation.", "tldr": "", "keywords": ["Compositional Generative Models", "Robotics", "Self-improving"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06c140c7bcfe86e5da04a2eee05f7f1ed950067d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MoSEL (Modular Self-Reflective Learning), a framework for enabling robots to autonomously improve their performance on long-horizon manipulation tasks through iterative self-reflection. Experiments on LIBERO-LONG and RoboTwin benchmarks demonstrate performance improvements of 33% and 46.7% respectively over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an innovative approach by leveraging VideoLLMs to automatically analyze robot-environment interaction videos and identify failure modes without human supervision. This modular self-reflective mechanism that jointly refines language planning, visual planning, and action planning components represents a creative combination that could reduce the dependency on manual labeling and expert intervention in robotic learning systems.\n2. The framework provides a well-structured hierarchical decomposition with clearly defined modules, and demonstrates substantial performance improvements (33-46% gains) on multiple benchmark tasks."}, "weaknesses": {"value": "1.No real-world robot experiments; all evaluations are simulation-only, raising serious questions about practical applicability and sim-to-real transfer.\n2.Computational costs (inference time, memory, convergence rate) are completely unanalyzed, making it impossible to assess practical feasibility.\n3.Insufficient baseline comparisons with only 3-4 methods; missing recent self-improving and VLM-based approaches; some baselines show 0.0% performance.\n4.VideoLLM failure diagnosis accuracy is never quantitatively evaluated; no analysis of false positives/negatives or handling of ambiguous failures.\n5.Limited generalization evidence; performance plateaus after 3-4 iterations; no testing on truly out-of-distribution scenarios or novel environments."}, "questions": {"value": "1.Can the authors provide concrete computational costs: inference time per module, total wall-clock time per task, GPU memory requirements, and average iterations to convergence?\n2.Why did the authors not include real robot validation, and do the authors have plans for physical experiments to demonstrate sim-to-real transfer?\n3.How do the authors explain baseline methods (UniPi, PaLM-E) achieving 0.0% on multiple tasks—are these implementation issues or fair comparisons?\n4.What is the precision and recall of VideoLLM failure identification, and have the authors conducted human evaluation of diagnostic accuracy?\n5.Have the authors tested on out-of-distribution scenarios with completely novel objects/environments, and can the improvements generalize beyond training variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1JS6S7Kt6e", "forum": "QVjyFrXOrn", "replyto": "QVjyFrXOrn", "signatures": ["ICLR.cc/2026/Conference/Submission9508/Reviewer_Qf7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9508/Reviewer_Qf7W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730468242, "cdate": 1761730468242, "tmdate": 1762921081522, "mdate": 1762921081522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoSEL, a hierarchical framework for robotic manipulation that incorporates a specifically designed self-reflective mechanism. Building upon prior modular frameworks such as UniPi, the main forward pipeline of MoSEL follows a hierarchical structure in which a language-conditioned video generation model (World Model) is used to predict future observations. The language input is first decomposed using an off-the-shelf Vision-Language Model (VLM). Given the predicted future observation and the current observation, an inverse dynamics model is then employed to infer the corresponding action by interpreting the difference between the two frames. The overall framework consists of three key components—the VLM for language decomposition, the world model for future frame prediction, and the inverse dynamics model for action generation. These modules are jointly optimized and iteratively refined during rollout and environment interaction, guided by a Video Analyzer that provides valuable feedback for quality assessment and failure analysis of the generated videos. Experimental results demonstrate that the proposed framework effectively improves both the quality of world model predictions and the overall performance on several tasks from the Libero and Robotwin benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper aims to enhance world-model-based hierarchical frameworks for robotic manipulation by incorporating a self-reflective mechanism. Specifically, the proposed MoSEL integrates a state-of-the-art (SOTA) Vision-Language Model (VLM), which supports video understanding, and treats it as a critic model. The sub-task planning process is refined based on feedback from the video analyzer, and the world model is retrained and optimized using filtered rollout data. In essence, the overall pipeline seeks to distill common-sense knowledge from current SOTA models, allowing the framework to self-improve, which is promising.\n2. The results demonstrate significant performance improvements after iterative self-improvement, which is highly inspiring. This suggests that domain-specific adaptation is crucial for the effectiveness of all components and models in an embodied manipulation system. It also provides valuable insights and serves as a strong reference for the practical implementation and development of embodied frameworks.\n3. The proposed method enables embodied models to interact with the environment and explore, promoting self-improvement in contrast to traditional reinforcement learning. With the guarantee of a lower bound performance of the world model, the model can explore the environment more efficiently and safely, enhancing the practical viability of embodied systems."}, "weaknesses": {"value": "1. The performance of the proposed method, as well as the baseline models, lags significantly behind current state-of-the-art Vision-Language Models (VLA), which diminishes the practical contribution of this paper. The inherent nature of world-model-based hierarchical embodied frameworks limits the use of critical information, such as proprioceptive data and wrist camera images, which could otherwise enhance the model's performance.\n\n2. The paper overlooks several important hyperparameters in the proposed framework. A more comprehensive ablation study is needed to demonstrate the robustness of the method with respect to these hyperparameters. For instance, factors like the number of trajectories during each rollout iteration, the number of refinement iterations, and how many gradient steps are taken for retraining both the world model and the policy after collecting the rollout trajectories should be analyzed. This would provide more insight into the sensitivity and stability of the framework.\n\n3. While the world-model-based framework benefits from the ability to train on large-scale out-of-distribution (OOD) video data, which provides strong generalization capabilities and facilitates seamless application across various domains, the experimental setup in this paper does not clearly demonstrate this advantage. Additionally, the proposed self-reflective mechanism runs the risk of catastrophic forgetting and overfitting to a specific domain, which could limit the framework's ability to generalize effectively.\n\n4. The overall performance of the framework heavily depends on a well-trained generalized Video Analyzer, which in this paper is implemented using closed-source VLMs. This reliance on proprietary models restricts the framework's transparency and reproducibility."}, "questions": {"value": "1. Could the authors provide more details about the pretraining procedure of the visual world model? Specifically, is it trained using large-scale out-of-distribution (OOD) video data, or is it specifically adapted to the Libero and Robotwin benchmarks? \n\n2. Why do the experiments on Libero and Robotwin employ different architectures for the inverse dynamics model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KhdkLLln2h", "forum": "QVjyFrXOrn", "replyto": "QVjyFrXOrn", "signatures": ["ICLR.cc/2026/Conference/Submission9508/Reviewer_t6rK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9508/Reviewer_t6rK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832585467, "cdate": 1761832585467, "tmdate": 1762921081142, "mdate": 1762921081142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose MOSEL which is a modular, self‑reflective framework for long‑horizon robotic manipulation that composes a language‑vision task planner, a video‑generation visual planner, and an inverse‑dynamics action policy. After each execution, a Video‑LLM evaluates sub‑goal completion and explains failures, that diagnosis triggers targeted refinement of the appropriate module i.e., re‑prompting the planner with rationales, fine‑tuning the visual model on successful rollouts, or updating the action policy using interaction outcomes. Across evaluation tasks, the authors report sizable gains over hierarchical baselines. The approach is appealingly pragmatic and modular."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The split into task planning (LVLM), visual planning (video generation), and action execution (inverse dynamics) keeps individual responsibilities clear and makes it feasible to isolate weaknesses at the right level of abstraction.\n    \n2. Labeling each skill attempt with success/failure and a textual rationale enables targeted updates rather than treating whole trajectories as monolithic successes/failures.\n    \n3. Improvements are reported on long‑horizon LIBERO‑LONG tasks and dual‑arm tool‑use in RoboTwin, which differ meaningfully."}, "weaknesses": {"value": "(1) Slight issues in the writing: (a) \\theta is being reused multiple times. (b) choose one term among self‑improving / self‑improvement / self‑strengthening / self‑reflective and use it throughout. (c) \"a modular\" (not “an modular”), \"unseen scenarios\".\n\n(2) \"Minimal human intervention\" is not quantified.The claim is compelling but lacks a concrete measure to support it.\n\n(3) Is there a potential evaluation leakage? It is ambiguous whether benchmark success is judged by ground truth from the simulator or by the same Video‑LLM used for training supervision, which risks circularity.\n\n(4) From what I understand, if the Video‑LLM mislabels borderline subgoals, training signals for both the visual planner and controller can drift. There seems to be no safeguards.\n\n(5) The subgoal ontology is not formalized. How many subgoals per skill, how overlapping subgoals are handled, and how partial credit is treated are unspecified.\n\n(6) From my understanding, the refinement procedure does not currently define a termination rule, iteration cap, or over‑fitting/oscillation checks for continual re‑prompting.\n\n(7) Exclusively reinforcing successful rollouts risks narrowing coverage to recently seen motions and eroding robustness. The author should consider discussing regularization or replay.\n\n(8) Similar to (7), as modules change, earlier labels (success/failure rationales) may become stale. There is no mention of re‑labeling or maintaining a consistent evaluation protocol.\n\n(9) The method is motivated by occlusions and dynamic scenes, but quantitative robustness tests (camera shifts, lighting changes, distractors) are not performed or included in the main paper.\n\n(10) Looks like self‑improvement seems to occur on the same task families used for evaluation. Without a clean held‑out set (new scenes/instructions), gains could reflect on‑policy overfitting rather than transferable competence.\n\n(11) Similarly, the baselines appear reconfigured rather than used in their canonical forms, and it is not stated that training/interaction budgets, prompts, and backbone sizes are matched. Does this risk unfair comparisons?"}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AiOgFNQzLM", "forum": "QVjyFrXOrn", "replyto": "QVjyFrXOrn", "signatures": ["ICLR.cc/2026/Conference/Submission9508/Reviewer_oQUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9508/Reviewer_oQUY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983679130, "cdate": 1761983679130, "tmdate": 1762921080859, "mdate": 1762921080859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a hierarchical planning method for long-horizon robotic tasks, aiming to adaptively self-improve the planning/execution and reduce human supervision as much as possible. Specifically, the authors leveraged VLM for planning, and proposed modified fine-tuning and self-improving methods for video diffusion and inverse dynamics model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work is targeting an interesting problem in task and motion planning with VLM to enable self-improvement with minimal human supervision.\n\nThe work cleanly separates task, visual, and action planning and ties them with a concrete joint objective.\n\nExperimental results show empirical gains and ablation study demonstrates the effectiveness of iterative improvements."}, "weaknesses": {"value": "The proposed method is limited to a fixed set of skills with pre-trained expert policies for execution. Therefore, the applicable task goals are limited by this skill set also.\n\nThe proposed method requires a video diffusion model trained on expert demonstration, which requires collecting expert demonstrations for each skill. Moreover, it is unclear whether the trained diffusion model can generalize to scene and task variations.\n\nThe proposed method can require expensive real-world execution during planning and can potentially be unsafe by causing damage to the world with unprecise plans, as it requires iterative execution of planned skills in the environments during planning.\n\nIt seems that both Libero and RoboTwin are simulated environments and there is no real robot experiments in real world."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2XMXoxBnhS", "forum": "QVjyFrXOrn", "replyto": "QVjyFrXOrn", "signatures": ["ICLR.cc/2026/Conference/Submission9508/Reviewer_eoRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9508/Reviewer_eoRB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995898540, "cdate": 1761995898540, "tmdate": 1762921080164, "mdate": 1762921080164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}