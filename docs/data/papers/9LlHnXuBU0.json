{"id": "9LlHnXuBU0", "number": 15471, "cdate": 1758251699675, "mdate": 1759897304712, "content": {"title": "DecompDreamer: A Composition-Aware Curriculum for Structured 3D Asset Generation", "abstract": "Current text-to-3D methods excel at generating single objects but falter on compositional prompts. We argue this failure is fundamental to their optimization schedules, as simultaneous or iterative heuristics predictably collapse under a combinatorial explosion of conflicting gradients, leading to entangled geometry or catastrophic divergence. In this paper, we reframe the core challenge of compositional generation as one of optimization scheduling. We introduce DecompDreamer, a framework built on a novel staged optimization strategy that functions as an implicit curriculum. Our method first establishes a coherent structural scaffold by prioritizing inter-object relationships before shifting to the high-fidelity refinement of individual components. This temporal decoupling of competing objectives provides a robust solution to gradient conflict. Qualitative and quantitative evaluations on diverse compositional prompts demonstrate that DecompDreamer outperforms state-of-the-art methods in fidelity, disentanglement, and spatial coherence.", "tldr": "We introduce DecompDreamer, a Gaussian-splatting-based framework for generating high-quality compositional 3D assets from text.", "keywords": ["text-to-3d generation", "gaussian splatting", "compositional 3d generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3487c625cac3dfe26a701ca01bca6d4481b9720b.pdf", "supplementary_material": "/attachment/283304a738f15b3ce3829b631a4ac797d7307946.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DecompDreamer, a staged optimization framework for compositional text-to-3D generation. The claim is that failures of prior methods originate from scheduling rather than representation: simultaneous or iterative optimization induces catastrophic gradient conflict across objects and relations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper identifies a convincing theoretical bottleneck in compositional generationâ€”gradient conflict arising from naive scheduling. This claim is not just theoretical; it is well-supported by the loss-dynamics analysis in Figure 5, which clearly visualizes the optimization divergence of GraphDreamer (Heuristic 3) versus the stable, two-stage convergence of the proposed method (Heuristic 4).\n2. The method shows a clear and significant advantage as the number of objects and relations increases. As shown in Table 1, the performance of baselines like GraphDreamer collapses when moving from $\\le3$ objects to $>3$ objects. DecompDreamer, in contrast, maintains high semantic alignment and user preference, proving its ability to handle the combinatorial complexity that cripples prior work.\n3. Explicit and Effective Disentanglement: The framework does not treat object disentanglement as a mere by-product of good generation. Instead, it is explicitly engineered through a suite of complementary techniques: object-wise Gaussian tracking, targeted relational optimization, view-aware supervision, and the use of negative prompts. This results in clean geometric isolation of individual components (as seen in Fig. 1, 6, and 13), a major improvement over the blended and entangled geometry common in baselines."}, "weaknesses": {"value": "1. The paper's primary weakness is the difficulty in attributing the final performance gains solely to the novel scheduling. The staged curriculum (Heuristic 4) is introduced concurrently with several other significant improvements (e.g., switching to flow-based guidance, view-aware alignment, negative prompts). The ablation study in Figure 7, while useful, does not fully isolate the scheduling component from these other powerful techniques. It is unclear how much of the gain comes from the \"structure-then-detail\" curriculum versus the other, more localized, improvements.\n\n2. The paper's thesis is about solving optimization failure modes (divergence, conflict, entanglement). However, the main quantitative comparison (Table 1) reports semantic alignment metrics (CLIP, Pick-A-Pic, User Study). While the loss analysis in Figure 5 is excellent, it is relegated to a secondary analysis. The main SOTA comparison would be far more convincing if it included direct metrics for these claimed optimization failures, such as a \"divergence rate\" on complex prompts, a \"geometric entanglement\" score, or a \"relational violation\" metric.\n\n3. The set of baselines is insufficient. The paper omits comparisons to several key state-of-the-art works from 2025, most notably Hunyuan3d 2.0 (Zhao et al., 2025) and STEP1X-3D (Li et al., 2025). Both of these models have demonstrated extremely high-fidelity and controllable asset generation. Without this comparison, the paper's claim to SOTA performance is unsubstantiated.\n\nZhao, Zibo, et al. \"Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation.\" arXiv preprint arXiv:2501.12202 (2025).\nLi, Weiyu, et al. \"Step1x-3d: Towards high-fidelity and controllable generation of textured 3d assets.\" arXiv preprint arXiv:2505.07747 (2025)."}, "questions": {"value": "1. How does the system behave when the scene graph or spatial priors from the VLM are partially incorrect or noisy? Could you provide some examples where the decomposition quality is programmatically degraded?\n\n2. Can you provide a qualitative and quantitative comparison against Hunyuan3d 2.0 and STEP1X-3D, especially on the complex, multi-object prompts where your method claims to excel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HzVn4pBZOp", "forum": "9LlHnXuBU0", "replyto": "9LlHnXuBU0", "signatures": ["ICLR.cc/2026/Conference/Submission15471/Reviewer_NSkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15471/Reviewer_NSkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448738763, "cdate": 1761448738763, "tmdate": 1762925763103, "mdate": 1762925763103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DecompDreamer addresses a fundamental limitation in text-to-3D generation: the failure of existing methods on compositional prompts with multiple objects and complex relationships. The paper reframes compositional 3D generation as an optimization scheduling problem rather than solely a representation problem. The key contribution is a staged curriculum that decouples optimization into two phases: (1) joint relationship modeling to establish global structure, and (2) disentangled object refinement for high-fidelity details. The method uses Gaussian Splatting with a VLM-generated scene graph and demonstrates improvements over Gala3D, GraphDreamer, and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This work provides a clear taxonomy of optimization heuristics (holistic, simultaneous, iterative, staged) and articulates why prior approaches fail under gradient conflicts, which is a valuable conceptual contribution beyond the specific method.\n2. The loss dynamics analysis (Figure 5) provides direct evidence supporting the theoretical claims.\n3. The paper is well-written with effective visualizations. Figure 3's pipeline overview and Figure 5's loss curves are particularly clear."}, "weaknesses": {"value": "1. The contribution is primarily combining existing techniques with a staged schedule.\n2. The proposed method relies heavily on VLMs for scene graph generation. What would happen with incorrect scene graphs? Is this method sensitive to spatial estimation errors? More empirical validations are required.\n3. The generation costs are unacceptable for real-world applications. 90-495 minutes per scene is impractical. While faster than Gala3D on 6-11 objects, it's much slower than feedforward methods (Trellis, Hunyuan3D 2.1, etc.). The paper doesn't explore whether the staged curriculum concept could accelerate optimization or transfer to faster feedforward models.\n 4. Janus problem and fine-grained decomposition still fail (Figure 14), which is a core problem of all SDS-based methods.\n5. Missing Reference. The idea of \"temporally decouples competing objectives\" during SDS optimization is close to eDiff-I [Nvidia] and ThemeStation [siggraph 2025]."}, "questions": {"value": "1. How does the method scale beyond 11 objects? Are there fundamental limits due to scene graph complexity or GPU memory?\n2. When VLM scene graphs are wrong, can the optimization partially recover? What's the worst-case behavior?\n3. Could the idea be transferred to feedforward 3D generation methods (Trellis, etc.) for compositional generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TTOaTrIO41", "forum": "9LlHnXuBU0", "replyto": "9LlHnXuBU0", "signatures": ["ICLR.cc/2026/Conference/Submission15471/Reviewer_wJWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15471/Reviewer_wJWm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826897055, "cdate": 1761826897055, "tmdate": 1762925762392, "mdate": 1762925762392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for compositional 3D scene generation. It can generate compositional 3D assets from text prompts using the SDS loss. The overall pipeline first employs a Language Model to generate object prompts and edge prompts, which describe the shape of each object and the relationships among them. Then, the method uses the SDS loss to compose and optimize the individual objects into a coherent 3D scene."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The method achieves superior visual quality compared to other competitive approaches, demonstrating its effectiveness."}, "weaknesses": {"value": "1. The contribution and novelty of this method are relatively weak. The overall pipeline is quite similar to GraphDreamer [1], and the proposed view-dependent SDS loss appears to be more of an engineering refinement rather than a substantial methodological innovation. Therefore, I believe it does not meet the novelty bar required for ICLR.\n\n2. The optimization process is slow and unstable, often requiring manual filtering to select satisfactory results, which limits its practicality and robustness.\n\n\n[1] Compositional 3D Scene Synthesis from Scene Graphs"}, "questions": {"value": "Because the SDS-based optimization process is unstable, and most state-of-the-art 3D generation methods are now native 3D approaches, would it be possible to compare with native 3D scene generation methods such as PartCrafter [2]? Alternatively, could the authors discuss the advantages or priorities of their method compared to these native 3D approaches?\n\n[2] PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f8J11UHGe2", "forum": "9LlHnXuBU0", "replyto": "9LlHnXuBU0", "signatures": ["ICLR.cc/2026/Conference/Submission15471/Reviewer_xadb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15471/Reviewer_xadb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894356052, "cdate": 1761894356052, "tmdate": 1762925761863, "mdate": 1762925761863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of generating 3D assets/scenes from compositional textual prompts, i.e., prompts that describe multiple objects and their spatial relationships. \n\nTo address this, the authors introduce DecompDreamer, a framework built around a composition-aware curriculum (i.e., staged optimization strategy) for structured 3D asset generation. The key idea is to decouple the optimization of inter-object relationships (the scaffold/layout) from the high-fidelity refinement of individual objects. Specifically:\n\nThe method first generates / decomposes the scene prompt into multiple objects (via a vision-language model) and identifies relationships among them. \nIt then uses a progressive optimization schedule: initially prioritize modeling the layout/structural scaffold and inter-object relations, then shift to refining the geometry and appearance of individual components. This decoupling is meant to reduce gradient conflict and improve the disentanglement of objects. \n\nIn short, this work contributes a novel pipeline and training/optimization schedule specifically targeted at the compositional text-to-3D scene generation task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work explicitly models the inter-object relationships, making the pipeline more understandable and clear.\n- The visualization results of this work have better layout planning and interaction between different objects.\n- Well-written and clear motivation. Easy to comprehend.\n- The analysis of gradients is good"}, "weaknesses": {"value": "- Though the layouts and interactions between objects seem to be better, it is obvious that the visual quality of instances is inferior. \n- Though the runtime is greatly reduced compared to GALA3D and GraphDreamer, it is still long."}, "questions": {"value": "- I am wondering if the VLMs are producing bad coarse initializations (wrong relationships between different objects). Can this pipeline correct the inter-object relationship with iterative optimization? Or the results are closer to the bad initialization results\n- A comparison and discussion on [1] and [2] is needed. What is your advantage over these works? Will it be better to give an explicit spatial guidance as [2] does?\n- What is the success rate of this work? In the failure cases, I am wondering if, first generating with a reconstruction model and then optimizing, these failure cases will be eased a little? Just like what [2] did.\n\nI am willing to increase my rating if you can provide a comprehensive discussion on these questions.\n[1]: CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians\n[2]: Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RK8o9WiC9H", "forum": "9LlHnXuBU0", "replyto": "9LlHnXuBU0", "signatures": ["ICLR.cc/2026/Conference/Submission15471/Reviewer_Hyt6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15471/Reviewer_Hyt6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941867212, "cdate": 1761941867212, "tmdate": 1762925761418, "mdate": 1762925761418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}