{"id": "0wCTDqkK8I", "number": 17101, "cdate": 1758272130543, "mdate": 1759897197754, "content": {"title": "Quantization with Purpose: Loss-Aware Bit Allocation for Gradient Compression", "abstract": "Gradient quantization is a critical technique for reducing communication overhead in large-scale distributed training. However, existing methods often employ fixed bit-width quantization or adaptive quantizers optimized with signal-level distortion metrics such as MSE, which poorly correlate with model performance. In this paper, we propose a novel layer-wise bit allocation framework for gradient quantization, formulated under a rate-distortion optimization (RDO) paradigm. Unlike prior approaches, our method introduces a loss-aware distortion metric that directly quantifies the impact of quantization on training loss, enabling task-aligned solution for bit allocation. A key insight of our work is the linear superposition property of cross-layer loss distortion, which we theoretically justify and empirically validate. This property allows us to decouple the original joint optimization problem and efficiently solve it via a Lagrangian optimization algorithm with linear complexity. Extensive experiments across vision and language tasks—using CNNs, ViTs, LSTMs, and Transformers—demonstrate the effectiveness of our approach. Moreover, our method integrates seamlessly with existing gradient compression techniques, yielding consistent performance gains.", "tldr": "", "keywords": ["Gradient Compression", "Rate-Distortion Optimization", "Bit Allocation", "Quantization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48621993a3930086326414882566d75255cb56fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a novel layer-wise bit allocation framework for gradient quantization, formulated under a rate-distortion optimization (RDO) paradigm. The method proposed introduces a loss-aware distortion metric that quantifies the impact of quantization on training. The paper provides some theory and empirical tests to validate the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a loss-aware distortion (LAD) that estimates the impact of quantization on the training loss instead of relying on the gradient magnitude.\n\n- The paper establishes that total loss distortion from jointly quantizing layers can be approximated by the sum of per-layer distortions, which allows decoupling and efficient optimization.\n\n- A Lagrangian bit allocation procedure with linear complexity in the number of layers is proposed. \n\n- The framework is orthogonal to and compatible with other compression techniques.\n\n- The experiments cover diverse architectures (CNNs, ViTs, LSTMs, Transformers) and tasks (vision and language). Demonstrating gains of the proposed approach"}, "weaknesses": {"value": "- Theoretical guarantees limited to optimality under decoupled RDO; convergence impacts are not formalized. Proof of convergence over the iterations and the convergence bounds are not reported. The algorithm optimizes bit allocation per step, but the effect on optimization dynamics is not theoretically characterized. Could you show that your approach maintains convergence guarantees similar to SotA compression techniques?\n\n- Section 2.2.1, W_orig should not be the same for both. For your approach at iteration t, you should start from \\tild{W_t} constructed using your compression in the previous iterations and not W_t \n\n- The paper introduces LAD but does not detail how it is estimated efficiently during training. Provide a precise estimator of the loss and how that affects your analysis.\n\n- The linear superposition property requires a stronger theoretical scope and more testing. The paper mentions theoretical justification and empirical validation, but the conditions under which this holds (e.g., bound on the step size, smoothness of the loss, independence of layer quantization errors, small perturbation regime…) are not clearly mentioned\n\n- Practical training often exhibits nonlinearity and interaction between layers."}, "questions": {"value": "- Theoretical guarantees limited to optimality under decoupled RDO; convergence impacts are not formalized. Proof of convergence over the iterations and the convergence bounds are not reported. The algorithm optimizes bit allocation per step, but the effect on optimization dynamics is not theoretically characterized. Could you show that your approach maintains convergence guarantees similar to SotA compression techniques?\n\n- Section 2.2.1, W_orig should not be the same for both. For your approach at iteration t, you should start from \\tild{W_t} constructed using your compression in the previous iterations and not W_t \n\n- The paper introduces LAD but does not detail how it is estimated efficiently during training. Provide a precise estimator of the loss and how that affects your analysis.\n\n- The linear superposition property requires a stronger theoretical scope and more testing. The paper mentions theoretical justification and empirical validation, but the conditions under which this holds (e.g., bound on the step size, smoothness of the loss, independence of layer quantization errors, small perturbation regime…) are not clearly mentioned\n\n- Practical training often exhibits nonlinearity and interaction between layers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q9KxBlbT0G", "forum": "0wCTDqkK8I", "replyto": "0wCTDqkK8I", "signatures": ["ICLR.cc/2026/Conference/Submission17101/Reviewer_GMhX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17101/Reviewer_GMhX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741059180, "cdate": 1761741059180, "tmdate": 1762927103823, "mdate": 1762927103823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rate–distortion optimization (RDO)–based framework for gradient quantization that employs a loss-aware distortion metric to capture the impact of quantization on training loss, enabling adaptive layer-wise bit allocation. By leveraging the linear superposition property of cross-layer loss distortion and solving via Lagrangian optimization, the method efficiently improves communication efficiency and overall model performance in distributed training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a principled rate–distortion optimization (RDO) framework that makes gradient quantization more interpretable and theoretically grounded.\n2. Introduces a loss-aware distortion metric enabling task-aligned and adaptive bit allocation.\n3. Leverages the linear superposition property and Lagrangian optimization to reduce computational complexity and improve communication efficiency and model performance."}, "weaknesses": {"value": "1. The paper lacks a dedicated Related Work section; the discussion of prior studies is scattered in the introduction without systematic comparison.\n2. The paper contains few experimental figures, and the presentation of results is not sufficiently intuitive.\n3. It is recommended to reorganize the paper’s structure and place the figures and tables in their corresponding sections."}, "questions": {"value": "1. The accuracy improvement in Tables 1 and 2 is relatively small (around 0.3%–1%), is it sufficient to offset the additional computational cost?\n2. For tasks involving stochastic perturbations (such as reinforcement learning or adversarial training), can the LAD metric still maintain stability?\n3. The authors claim that the loss-aware distortion (LAD) metric aligns better with task objectives than MSE, but is there any theoretical evidence showing that LAD has a stronger correlation with final task performance?\n4. The first-order Taylor expansion neglects the Hessian term, but is this assumption still reasonable in regions with a large learning rate or a steep loss surface?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "46Ygl2myuG", "forum": "0wCTDqkK8I", "replyto": "0wCTDqkK8I", "signatures": ["ICLR.cc/2026/Conference/Submission17101/Reviewer_i23A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17101/Reviewer_i23A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906420749, "cdate": 1761906420749, "tmdate": 1762927103379, "mdate": 1762927103379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the communication bottleneck in distributed deep learning training, a key challenge for large-scale models. It identifies limitations of existing gradient quantization methods: uniform bit-width allocation fails to account for layer-wise sensitivity, and signal-level metrics poorly correlate with model performance.\nTo resolve these issues, the authors propose a rate-distortion optimization-based layer-wise bit allocation framework. Key innovations include: 1) a Loss-Aware Distortion metric that quantifies quantization’s impact on training loss directly; 2) leveraging the linear superposition of loss distortion to decompose the intractable joint allocation problem; and 3) integrating Lagrangian optimization and a gradient-similarity-based dynamic trigger for efficiency.\nExperiments on vision and language tasks show the framework outperforms static/heuristic baselines and enhances existing quantizers. Results support its ability to balance communication efficiency and model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper effectively targets the critical limitations of fixed bit-width and signal-level metrics by proposing a loss-aware layer-wise bit allocation framework under the rate-distortion optimization paradigm, addressing the core gap between communication efficiency and model performance.\n  \n2) The paper presents a key insight that decouples the intractable joint bit allocation problem into independent per-layer subproblems solvable with linear complexity, making the framework applicable. \n\n3) The paper designs an efficient Lagrangian optimization algorithm for optimal bit assignment to balance distortion and communication budget and a lightweight dynamic reallocation trigger to monitor gradient norm similarity, which reduces unnecessary computational overhead by only updating bit allocations when gradient distributions shift significantly."}, "weaknesses": {"value": "1) The dynamic reallocation trigger depends on fixed thresholds  without adaptive adjustment mechanisms, increasing deployment complexity\n2) The Loss-Aware Distortion metric's resource cost for large-scale models remains unmeasured, risking computational bottlenecks\n."}, "questions": {"value": "NONE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VAdrcBvKJ5", "forum": "0wCTDqkK8I", "replyto": "0wCTDqkK8I", "signatures": ["ICLR.cc/2026/Conference/Submission17101/Reviewer_g5RR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17101/Reviewer_g5RR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999087073, "cdate": 1761999087073, "tmdate": 1762927102794, "mdate": 1762927102794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a task-aware quantization method, unlike prior work that uses MSE to measure the difference between actual and quantized gradients. The authors formulate this problem as a rate-distortion optimization (RDO), then show that this joint optimization can be solved as a sum of independent problems to achieve layer-wise bit allocation. To reduce cost, this optimization is performed when a regime shift in the gradients is detected. The results show that this approach is effective when compared to uniform or greedy layer-wise bit allocations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a task-aware quantization that preserves model quality better than other baselines.\n* The paper formulates the bit allocation problem as an RDO, and shows mathematically how this intractable joint optimization can be solved as a series of independent subproblems.\n* The papers show experiments over a representative set of tasks."}, "weaknesses": {"value": "* The paper motivates itself by the scale of foundation models (billions of parameters), but the proposed method targets weight and gradient compression, which primarily reduces communication and does not address the core memory/compute challenges of training very large models. Its applicability also appears limited to data parallelism.\n* The paper emphasizes model quality, but it is not clear whether the method is cost-effective. Do the end-to-end speedups from compression outweigh the method’s computational overhead? Is it more efficient than a strong greedy compression baseline that achieves similar quality?\n* The experiments use small models (ResNet18, 4 layers Transformer , 2 layers LSTM). To better match the paper’s motivation, the authors should evaluate larger models such as ViT-Large and BERT-Large.\n* The baselines used in the experiments are not sufficient. The authors claim “Given that the optimization objectives and constraint conditions in prior studies (Markov et al., 2024; Yan et al., 2022) on bit allocation for gradient compression differ from ours” still other SoTA for gradient compression should be evaluated, and the difference in assumptions and constraints should be highlighted.\n* The related work coverage is lacking. Xin et al. \"Kimad: Adaptive Gradient Compression with Bandwidth Awareness\" should be contrasted to."}, "questions": {"value": "* What is the computational overhead of computing the loss-aware distortion metric and running the search across all layers?\n* In your experiments, how frequently did regime shifts occur that triggered bit reallocation?\n* What is the runtime cost of the trigger mechanism itself?\n* What are the actual bit allocations obtain by your method and how much do they they differ from Uniform and Greedy?\n* The perplexity in Table 2 using gradient compression is large. It seems that the model quality would be greatly affected by gradient compression. Would your method be able to recover ppl around 82? If so, in what configuration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7E4NGBn0h8", "forum": "0wCTDqkK8I", "replyto": "0wCTDqkK8I", "signatures": ["ICLR.cc/2026/Conference/Submission17101/Reviewer_xQTq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17101/Reviewer_xQTq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163903002, "cdate": 1762163903002, "tmdate": 1762927102549, "mdate": 1762927102549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}