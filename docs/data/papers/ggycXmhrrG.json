{"id": "ggycXmhrrG", "number": 6982, "cdate": 1758004140003, "mdate": 1763651086832, "content": {"title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing", "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.", "tldr": "", "keywords": ["Large Vision-Language Model", "Hallucination Mitigation", "Activation Editing", "inferece-time"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba76654b100903275f5671eadd33c0afbd2182fd.pdf", "supplementary_material": "/attachment/96c6b9642f37ef70b79bd444b8dfaf892ff995b9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an activation editing method AFTER to mitigate hallucination in vision-language models. It compromises two methods: FAS to provide factual and global guidance for activation editing, and QAO to provide query-specific guidance. Through training a lightweight MLP, they obtain the offset activation vectors, which is added to the original activations for correction. Results show that AFTER outperforms baselines and previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is mostly clear and easy to follow.\n\n2. The method seems effective and substantially outperforms baseline."}, "weaknesses": {"value": "1. In FAS, how do you construct an $n$-question set for each textual description while ensuring that each of $n$ questions are indeed effective? Besides, what is the value of $n$ used in your experiments?\n\n2. What model is used to generate $t^{+}$? Also, despite $t^{+}$ is synthesized by a set of facts, hallucination can still persist in outputs of any LLMs, so validation of  $t^{+}$ is needed.\n\n3. Models and baselines compared are outdated (all in 2024) considering this is a submission in late 2025.\n\n4. The notation is quite confusing for  $t_{i,j}^{+}$ at the end of line 274. Correct me if I’m wrong: $i$ should be from 1 to $n$, denoting the $n$ questions associated with each sample; $j$ should be from 1 to $m$, which is the number of objects mentioned in the question $q_{i}$, not from 1 to $n$.\n\n5. In line 277, $z_{i}^{\\*}$ is obtained from $t_{i}^{*}$, which is a **set** of activations, but $z_{i}$ is a single activation. How is the subtraction between them possible?\n\n6. How does the selection of training sets impact the trained MLP for offset prediction? Since all of the four benchmarks (POPE, MME, AMBER, GQA) contain images from MSCOCO, it’s necessary to use a different set of images for training or benchmarks with different image sources to further prove the generalization of your method."}, "questions": {"value": "1. Are you correcting activations for all layers? Since the shallow layers are more for perception and deeper layers for cognition, it’s more interesting to see the correction effect on different layers.\n\n2. The models used to generate training data are not at all mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lyB33AJ3um", "forum": "ggycXmhrrG", "replyto": "ggycXmhrrG", "signatures": ["ICLR.cc/2026/Conference/Submission6982/Reviewer_n12p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6982/Reviewer_n12p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866954488, "cdate": 1761866954488, "tmdate": 1762919202811, "mdate": 1762919202811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles hallucination by leaning on straightforward factual textual cues, rather than only nudging activations or poking the image with perturbations. The approach first uses dense ground-truth annotations from the COCO training set to build three fact banks (category, attribute, relation). These facts are turned into text descriptions with an LVLM. For each description, the method creates trusted and untrusted pairs to get positive and negative activation examples, which yield an editing vector. A small offset estimator is then trained for Query-Adaptive Offset optimization, giving a query-specific offset at inference. In practice, it reduced hallucinations and lifts accuracy across POPE, MME, and AMBER on various LVLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The trusted vs. untrusted pairing is a smart way to bootstrap supervision without manual labeling at scale.\n- Inference behaves like a lightweight adapter, so the method is efficient.\n- Results are strong across both discriminative and generative benchmarks.\n- Analysis is thorough, with useful ablations, including performance as the image pool grows from 50 to 500.\n- The out-of-distribution check is informative and suggests the method does not degrade under domain or QA-style shifts (discriminative to generative)."}, "weaknesses": {"value": "- The pipeline relies on ground-truth object annotations during preparation, which may be unavailable in some domains. In QAO, query-focused supervision depends on extracting objects from the question and checking membership against the COCO-derived category fact set T_c; if a queried object is not in T_c, they synthesize a negative textual sub-description. Without rich annotations or a compatible taxonomy, this step becomes brittle and hard to port.\n- Implementation detail: the method is model-specific, so you must recompute the steering vector and retrain the small offset estimator for each LVLM, which adds setup overhead and limits portability across architectures."}, "questions": {"value": "- What explains the drop in Cover, the metric for answer comprehensiveness?\n- Does the learned steering direction favor frequent classes at the expense of rare or novel ones?\n- How does the method handle compositional prompts with multiple objects, attributes, and relations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qQ8VcrqFqr", "forum": "ggycXmhrrG", "replyto": "ggycXmhrrG", "signatures": ["ICLR.cc/2026/Conference/Submission6982/Reviewer_j2pv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6982/Reviewer_j2pv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892314963, "cdate": 1761892314963, "tmdate": 1762919202089, "mdate": 1762919202089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of object hallucination in Large Vision-Language Models (LVLMs).The authors think  that they arises mainly from language bias and manifests as category, attribute, or relation errors. The authors propose AFTER (Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation), a lightweight inference-time activation editing method. AFTER combines Factual-Augmented Activation Steering (FAS), which transforms ground-truth visual annotations into textual facts to provide positive, factual guidance, and Query-Adaptive Offset Optimization (QAO), which learns query-specific adjustments to better align visual-textual associations."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The figures and illustrations are excellent — the visuals are carefully designed and aesthetically pleasing, and they greatly help in clarifying certain issues I encountered during reading.\n2. The writing is strong, and the appendix provides abundant experimental details, which convinces me that the reported results can be reproduced directly based on the information given in the paper.\n3. The motivation is solid and well-founded. Similar concerns have been raised in other works, and the paper rightly emphasizes that language bias may lead LVLMs to overlook important information in the images [1].\n\n[1] Jia H, Jiang C, Xu H, et al. Symdpo: Boosting in-context learning of large multimodal models with symbol demonstration direct preference optimization[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 9361-9371."}, "weaknesses": {"value": "1. The proposed method is evaluated primarily on relatively simple datasets, whose difficulty appears notably lower than the complexity illustrated in Figure 1. It remains unclear whether the approach would retain its effectiveness on more challenging benchmarks, such as HallusionBench[1] or CRPE[2].\n2. The approach relies heavily on datasets like COCO, which are richly annotated by humans, and thus depends on the availability of high-quality manual annotations. Given that the current results are derived from COCO-like data, it is uncertain whether comparable performance could be achieved on domains where such annotations are scarce or costly—e.g., in medical imaging datasets.\n3. Regarding datasets such as BLINK[3], it is unclear whether the method could deliver strong generalization performance. In the current generalization experiments, COCO is evidently an in-domain dataset, making it difficult to assess how well the method would transfer to truly out-of-domain scenarios.\n\n[1] Guan T, Liu F, Wu X, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 14375-14385.\n\n[2] Wang W, Ren Y, Luo H, et al. The all-seeing project v2: Towards general relation comprehension of the open world[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 471-490.\n\n[3] Fu X, Hu Y, Li B, et al. Blink: Multimodal large language models can see but not perceive[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 148-166."}, "questions": {"value": "As described in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "69IcyIy44e", "forum": "ggycXmhrrG", "replyto": "ggycXmhrrG", "signatures": ["ICLR.cc/2026/Conference/Submission6982/Reviewer_cVcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6982/Reviewer_cVcN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895723826, "cdate": 1761895723826, "tmdate": 1762919201478, "mdate": 1762919201478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents AFTER to mitigate object hallucinations in LVLMs. These hallucinations, arising from language bias, are categorized into three types: category, attribute, and relation hallucinations. AFTER address these hallucinations during inference by adaptively steering visual-textual activations toward factual semantic guidance. The paper evaluates the effectiveness of AFTER on several benchmarks, including POPE and MME, where it demonstrates superior performance over baseline methods, e.g., achieving up to a 16.3% reduction in hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "AFTER stands out by combining factual textual guidance with query-specific offsets to improve visual-textual activation editing. FAS uses factual annotations from images to provide clear guidance, effectively reducing language bias. Meanwhile, QAO adapts the editing process by creating query-specific offsets, allowing the model to handle distinct visual-textual associations more effectively. This innovation overcomes the limitation of previous methods that often use a single, averaged editing vector."}, "weaknesses": {"value": "- AFTER relies on accessing activations from open-source LLMs, which limits its applicability to closed-source models. This restricts the method’s use in private large-scale models or those that are not publicly accessible. Please explore post-processing steps based on model outputs during inference \n- QAO is presented as a key component to improve editing diversity and accuracy, the paper lacks in-depth analysis of its training process and its specific impact on performance, particularly in handling different queries. Visualizing the offsets for different queries could provide a clearer understanding of how QAO enhances the editing process. \n- The paper mentions tuning the editing strength and the number of edited heads based on empirical results."}, "questions": {"value": "The method uses textual descriptions derived from ground-truth annotations to steer the activation. What is the impact of varying the quality or diversity of the factual textual descriptions (t+) on the hallucination mitigation process? Is there a specific threshold or methodology for ensuring that these descriptions remain reliable across different domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2BEIBD7Brp", "forum": "ggycXmhrrG", "replyto": "ggycXmhrrG", "signatures": ["ICLR.cc/2026/Conference/Submission6982/Reviewer_dssQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6982/Reviewer_dssQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908510052, "cdate": 1761908510052, "tmdate": 1762919200724, "mdate": 1762919200724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}