{"id": "Vr5f3kRvLD", "number": 11120, "cdate": 1758189733193, "mdate": 1759897607126, "content": {"title": "IDER: IDEMPOTENT EXPERIENCE REPLAY FOR RELIABLE CONTINUAL LEARNING", "abstract": "Catastrophic forgetting, the tendency of neural networks to forget previously learned knowledge when learning new tasks, has been a major challenge in continual learning (CL). To tackle this challenge, CL methods have been proposed and shown to reduce forgetting. Furthermore, CL models deployed in mission-critical settings can benefit from uncertainty awareness by calibrating their predictions to reliably assess their confidences. However, existing uncertainty-aware continual learning methods suffer from high computational overhead and incompatibility\nwith mainstream replay methods. To address this, we propose idempotent experience replay (IDER), a novel approach based on the idempotent property where repeated function applications yield the same output. Specifically, we first adapt the training loss to make model idempotent on current data streams. In addition, we introduce an idempotence distillation loss. We feed the output of the current model back into the old checkpoint and then minimize the distance between this reprocessed output and the original output of the current model. This yields a simple and effective new baseline for building reliable continual learners, which can be seamlessly integrated with other CL approaches. Extensive experiments on different CL benchmarks demonstrate that IDER consistently improves prediction reliability while simultaneously boosting accuracy and reducing forgetting. Our results suggest the potential of idempotence as a promising principle for deploying efficient and trustworthy continual learning systems in real-world applications. Our code will be released upon publication.", "tldr": "", "keywords": ["continual learning", "reliable", "idempotence"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4071ad11f9e9fc3ba646181c33c219e4a476fa7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Idempotent Experience Replay (IDER), a method for class-incremental learning (CIL) that exploits the mathematical property of idempotence to improve accuracy, forgetting, and uncertainty calibration. The method consists of two components: 1) a standard idempotent module that trains the current model to be idempotent on the current task; and 2) an idempotent distillation module that enforces idempotence between the previous and current model checkpoints. Experiments on standard datasets demonstrate improvements in both accuracy and calibration.\n\n\n**Recommendation**\nI lean toward rejecting the paper in its current form. While the core idea of applying idempotence to continual learning is novel and the empirical results are encouraging, the paper suffers from significant theoretical and experimental gaps which prevent a rigorous and clear assessment of the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel application of idempotence to continual learning:** while idempotence has been explored in deep learning for other tasks, this appears largely under-explored in continual learning settings.\n2. **Plug-and-play design with rehearsal-based approaches:** the method can be integrated with existing rehearsal-based approaches (e.g., ER) with minimal overhead, making it practically appealing in those cases.\n3. **Experimental evaluation on general-CIL:** the method has also been tested on more realistic conditions (i.e., general class-incremental learning), making it interesting from a practical point of view."}, "weaknesses": {"value": "1. **Limited theoretical justification:** while the paper takes inspiration from prior work on idempotence, the theoretical connection between idempotence and mitigation of both catastrophic forgetting and uncertainty miscalibration is not clear. Although being demonstrated empirically, it is unclear why enforcing idempotence specifically helps with continual learning problems in the first place. The intuition lacks formal analysis. Can you provide explanations on why idempotence should help with catastrophic forgetting and uncertainty calibration?\n2. **Equation 6:** the idempotent distillation loss uses $f_{t-1}(x, f_t(x, 0))$ rather than $f_t(x, f_t(x, 0))$ to avoid pulling predictions toward incorrect output, but this breaks the true idempotence property since $f_{t-1}$ and $f_t$ are two different functions. What happens if you use $f_t(x, f_t(x, 0))$ instead of $f_{t-1}(x, f_t(x, 0))$ in Equation 6?\n3. **Missing evaluation with calibration methods for CL:** one of the main focus of the paper is reliability (i.e., reducing the calibration error). However, the paper does not consider recent work on uncertainty calibration for class-incremental learning [1,2].\n4. **Limited comparison with other CL methods:** apart from rehearsal-based methods, it would be interesting to include comparison with, e.g., regularisation-based or parameter-isolation approaches to demonstrate broader applicability. \n5. **Incomplete experimental analysis:**\n    1. Hyperparameter sensitivity: no analysis of sensitivity to $\\alpha$ and $\\beta$ introduced in Equation 8.\n    2. Probability $P$: the probability is not reported nor ablated.\n    3. Backbone architecture: only ResNet18 is tested. What will happen when using a different architecture? Again, this would demonstrate broader applicability.\n    4. Limited experiments on GCIL (Table 2): it would be interesting to see the results on the other considered datasets.\n    5. Limited calibration results (Table 3): as anticipated above, the paper is missing recent work on calibration in continual learning. Furthermore, why is Tiny-ImageNet not included? Finally, since the results from NPCL were copied from the original work, do you assure that the experimental setting is exactly the same? Otherwise, the comparison is not meaningful. \n\n**Minor issues**\n1. No analysis on larger task sequences (e.g., 20 tasks on Tiny-ImageNet).\n2. Figures 3, 4, and 5 are difficult to parse.\n3. Writing style can be overall improved.\n\n[1] Li, Lanpei, et al. \"Calibration of continual learning models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Hwang, Seong-Hyeon, Minsu Kim, and Steven Euijong Whang. \"T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7kECVXGSsr", "forum": "Vr5f3kRvLD", "replyto": "Vr5f3kRvLD", "signatures": ["ICLR.cc/2026/Conference/Submission11120/Reviewer_WbzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11120/Reviewer_WbzD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584991734, "cdate": 1761584991734, "tmdate": 1762922293054, "mdate": 1762922293054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IDER (Idempotent Experience Replay), a novel approach for continual learning that addresses the issue of catastrophic forgetting in replay-based methods. The key idea is to make the replay process idempotent, meaning that repeatedly revisiting the same experiences does not alter the model representation undesirably. The method achieves this by enforcing idempotent updates through the standard idempotent module and the idempotent distillation module. Experiments on several continual learning benchmarks demonstrate consistent performance gains over standard replay-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using idempotent property to mitigate issues of poor calibration and recency bias is straightforward and intuitive.\n\nThe paper is well-written and easy to follow.\n\nThe proposed method is shown to be effective."}, "weaknesses": {"value": "As stated in Lines 71–74, the paper claims a strong correlation between the idempotence distance and prediction error. However, it remains unclear whether this relationship has been formally analysed. Could the authors provide empirical evidence or theoretical justification for this claim?\n\nTo enable idempotence with respect to the second input, the proposed method divides the backbone into two parts. What principles guide this division? Does choosing different partition points (e.g., splitting at shallower or deeper layers) affect model performance or stability? It would be helpful if the authors could provide theoretical insight or experimental analysis on how the division point influences the effectiveness of idempotent learning.\n\nLines 207–209 describe the probability P that determines whether the second input is set to the ground-truth label or the empty signal. Is there any hyperparameter analysis that explores the sensitivity of the method to P? How should this value be selected in practice?\n\nThe overall loss function combines three components: the Standard Idempotent Module, Idempotent Distillation Module, and Experience Replay. Are there ablation experiments isolating the contribution of each component? Furthermore, the paper states that the proposed method primarily addresses poor calibration and recency bias. However, these issues are often mitigated in modern replay-based methods. Why does this approach not integrate or compare directly with more recent baselines such as L2P [1], HIDE-prompt [2], or VQ-prompt [3]? Has the proposed method been evaluated under online continual learning settings to validate its general applicability?\n\nTable 1 shows improvements when integrating IDER with ER, BFP, and CLS-ER. However, could the proposed method also be compatible with other state-of-the-art baselines such as SCoMMER or SARL? It would be valuable to verify whether the idempotent mechanism consistently enhances these methods as well. \n\nRegarding the experiments, I noted that the evaluation is primarily conducted on small-scale datasets like the CIFAR series and Tiny-ImageNet. I encourage the authors to validate the method's effectiveness on larger-scale and more diverse datasets. Furthermore, a concrete example illustrating how the proposed method achieves error correction in practice would greatly enhance the paper’s clarity and impact.\n\n[1] Wang et al., Learning to prompt for continual learning. CVPR 2022\n\n[2] Wang et al., Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality. NIPS 2023\n\n[3] Li et al., Vector Quantization Prompting for Continual Learning. NIPS 2024"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k1P7qyuVhy", "forum": "Vr5f3kRvLD", "replyto": "Vr5f3kRvLD", "signatures": ["ICLR.cc/2026/Conference/Submission11120/Reviewer_aMu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11120/Reviewer_aMu3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814705085, "cdate": 1761814705085, "tmdate": 1762922292632, "mdate": 1762922292632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces the idempotent property to mitigate catastrophic forgetting, a fundamental challenge in incremental learning. The authors design two complementary loss functions: one enforces idempotence on current data, while the other distills this property from a previous model checkpoint using a memory buffer of historical samples. Extensive experiments are conducted to validate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• The manuscript introduce idempotence, a mathematical property, to tackle catastrophic forgetting and poor model calibration in continual learning.\n• The proposed IDER is a lightweight framework and functions as a plug-and-play module for performance gains"}, "weaknesses": {"value": "• The paper primarily relies on intuition and empirical success to introduce idempotence. It lacks a rigorous theoretical analysis or hypothesis for why enforcing output stability should directly mitigate catastrophic forgetting at a fundamental level.\n• The empirical validation is comprehensive on CIFAR-10, CIFAR-100, and Tiny-ImageNet. However, to firmly establish the method's practicality and generalizability, evaluation on a large-scale dataset, e.g., ImageNet-1K.\n• The hyperparameter sensitivity analysis for α and β is relatively brief.\n• The experimental comparisons are heavily focused on replay-based methods, which is natural as IDER is a plug-in for this paradigm. However, comparing against strong representatives from other CL fmethods, such as regularization-based methods or memory-free approaches, would more comprehensively position IDER's contribution within the entire field and highlight its unique value."}, "questions": {"value": "• A simple theoretical proposition or a more in-depth discussion connecting the idempotence loss to established continual learning theory would significantly strengthen the foundation.\n• Test IDER in settings like online continual learning or with tasks containing out-of-distribution samples to better probe its limits and robustness.\n• Perform a more systematic hyperparameter sensitivity analysis, showing how performance varies with different values of α and β across key datasets.\n• Add comparisons with regularization-based and memory-free methods in the main experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I did not identify any ethical concerns related to this paper. The work does not involve sensitive data, human subjects, or\npotentially harmful application."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XmBxIje7h0", "forum": "Vr5f3kRvLD", "replyto": "Vr5f3kRvLD", "signatures": ["ICLR.cc/2026/Conference/Submission11120/Reviewer_xwPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11120/Reviewer_xwPR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912919460, "cdate": 1761912919460, "tmdate": 1762922292109, "mdate": 1762922292109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve accuracy and calibration in continual learning by extending idempotence enforcing loss function into the continual learning setting with experience replay. A function is called idempotent if consecutive applications of it gives the same result as one application. \nIn the single task setting, the idempotence enforcing loss function encourages the model $f(x, y)$ to be idempotent with respect to its second input $y$. This work introduces a new loss function to adapt adaptation of this idea in the continual learning setting. This new loss function can be combined with some existing techniques to improve them\nThe improved calibration and accuracy claims are supported by experimental results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and organized. Adaptation of the idempotence loss function to the continual learning setting is creative and nuanced. The experimental results are promising."}, "weaknesses": {"value": "I think giving more intuition about the loss function would be helpful for the reader. For example, in lines 250-252, it is stated that minimizing equation 5 biases $f_t$ towards the wrong label (even though with probability $1-P$ that objective would be minimized in eq 5), but why would $f_{t-1}$ not have the same problems? Expanding that explanation would be great."}, "questions": {"value": "Please see the weakness section. \nIn general providing more intuition, especially on why this loss function helps with calibration would be great."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "69tcoW2wXa", "forum": "Vr5f3kRvLD", "replyto": "Vr5f3kRvLD", "signatures": ["ICLR.cc/2026/Conference/Submission11120/Reviewer_f4Fu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11120/Reviewer_f4Fu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997492677, "cdate": 1761997492677, "tmdate": 1762922291752, "mdate": 1762922291752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}