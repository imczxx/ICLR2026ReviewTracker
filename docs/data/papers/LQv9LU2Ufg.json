{"id": "LQv9LU2Ufg", "number": 747, "cdate": 1756816575135, "mdate": 1759898243916, "content": {"title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "abstract": "Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. \nThus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG.\nTo train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments. It thus exhibits more than $17\\times$ sample efficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.", "tldr": "", "keywords": ["Multi-modal Embodied Agent", "Unified Generative Model", "Auto-Regressive World Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d9b22db19c45895bfc23940e629cb2e0d914303.pdf", "supplementary_material": "/attachment/3d645604b4154d90b10c0308875f16576d544565.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RIG (Reasoning and Imagination in Generalist policy) , an end-to-end Generalist policy framework, for the first time, text reasoning, low-level action control, and visual imagination are jointly modeled in the same Transformer architecture. The authors use GPT-4o to inject reasoning and reflective annotations into trajectories through a progressive data construction process (S 0-s 4) , and train two versions: the reasoning-only RIG-basic and the RIG-lookahead with the ability of“Forward-reflection”. In the Minecraft environment, RIG achieved much better sample efficiency (17 × boost) and task success rates than existing methods such as VPT, Steve-1, MineDreamer, with only 111 h of training data, and the performance of the proposed method was better than existing methods, test-time scaling is also supported to enhance decision robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Outstanding originality: unifying “Reasoning” and “World Model Imagination” into a single autoregressive Transformer rather than splicing multiple specialized modules (e.g. , VLM + VGM) is a significant innovation in architectural design. In particular, the“Dream-review” mechanism generates a correction trajectory through GPT-4o and skillfully converts failure experiences into training signals, which is a novel paradigm of data enhancement and self-monitoring.\n- The proposed four-stage data construction process (S0-S4) is logically clear and reproducible, and each stage has a clear goal (from basic alignment → Inference Injection → failure reflection → temporal alignment). The experimental design was comprehensive, covering ablation studies, scalability analysis (data volume, number of iterations, number of prospective steps), multi-task evaluation (collection/exploration), quality of generation (FID/PSNR), and reasoning ability (VQA), and the results were analyzed, providing strong support.\n- Significant practical implications and potential: exceeding the baseline of relying on 2000 hours of video with only 111 hours of data, demonstrating the tremendous improvement in sample efficiency of “Collaborative modeling, inference, and imagination”; it has important reference value for embodied intelligent scenarios with scarce data. Lookahead also provides a viable path to reducing real-world trial-and-error.\n- Clear problem localization and comparison: the paper accurately points out the shortcomings of the existing works (e.g., Voyager, MineDreamer) in“End-to-end optimization” and“Reasoning-imagination collaboration”, and provides a new perspective for future research, and through the intuitive comparison of four types of agent architectures in Figure 1, the contribution positioning is clear."}, "weaknesses": {"value": "- All experiments were conducted in Minecraft, with clear rules and a highly structured (boxed) visual style, with gaps compared to real-world or more complex simulators (e.g., Habitat, Isaac Gym). The authors do not discuss the applicability of the method to continuous action spaces, real images, or dynamic physical environments, which weakens the universality of the conclusion.\n- S2(inferential tagging) and S3(reflective tagging) rely heavily on GPT-4o to artificially generate trajectories, but do not account for tagging costs (e.g., token consumption, manual screening time), and do not account for tagging costs (e.g, nor has there been any attempt to replace it with a more lightweight model, such as an open source LLM. This may affect the scalability and practicality of the method, especially for researchers with limited resources."}, "questions": {"value": "- Is the RIG's architecture suitable for non-boxed, continuous-control, or real-world robotic environments? Can you provide preliminary migration experiments (e. g. Carla, MetaWorld) in the appendix or in future work?\n- Have you tried annotating trajectories with smaller open-source models, such as LLAMA-3-8B with visual adapters? How does the quality of its build affect final performance? This is critical for community replication.\n- The current “Imagination” is only used to generate modified inferences, not to directly optimize action strategies (such as policy gradients based on imaginary trajectories). Are there plans to integrate RIG with reinforcement learning for tighter imagination-based planning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cCeBLPAUWw", "forum": "LQv9LU2Ufg", "replyto": "LQv9LU2Ufg", "signatures": ["ICLR.cc/2026/Conference/Submission747/Reviewer_tgFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission747/Reviewer_tgFQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703161815, "cdate": 1761703161815, "tmdate": 1762915596846, "mdate": 1762915596846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RIG, a generalist policy that aims to synergize Reasoning (text), Imagination (future frames), and action into a single, end-to-end autoregressive model. It is trained using a progressive data pipeline that distills reasoning and review labels from large language models (GPT-4o) onto existing agent trajectories. During inference, RIG can \"dream\" potential outcomes, \"review\" these imagined futures, and \"self-correct\" its action plan, enabling a form of test-time planning. The authors claim this synergy achieves SOTA performance and a 17x sample efficiency improvement on Minecraft tasks ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core idea of a single, end-to-end model that unifies reasoning, imagination, and action in one autoregressive sequence is a compelling vision for generalist agents.\n2. Table 2 provides good ablation evidence that the joint training is synergistic, where adding Reason capabilities improves Generation quality, and adding Gen capabilities improves Reasoning scores.\n3. The \"dream-review\" mechanism is a novel and interesting contribution. It provides a concrete data-driven method for teaching the model to perform internal simulation, spot its own potential failures, and then self-correct its plan before execution."}, "weaknesses": {"value": "1. The paper's single most prominent claim is its \"17x sample efficiency\" (111h of data) compared to baselines like STEVE-1 (2000h). However, the paper's own data pipeline explicitly requires a \"pretrained policy, STEVE-1\" to generate the S1 (Vision-Action) dataset. Furthermore, the S3 stage requires a \"superior-performing policy STEVE-1\" to generate positive trajectories for its corrective \"dream-review\" data. Therefore, RIG's 111h data budget requires a pre-existing 2000h STEVE-1 model as a prerequisite. The 2000h data cost of this \"teacher\" model is hidden and not included in the 111h total.\n2. The paper frames RIG as a novel learning agent that \"synergizes reasoning and imagination\". However, the data pipeline shows that the \"reasoning\" (S2) and \"reviewing\" (S3) are not learned from environmental interaction but are distilled from GPT-4o, and the vision-action training data are collected through a pretrained policy (STEVE-1) in S1 and S3. The agent is learning to imitate GPT-4o's reasoning and STEVE-1's actions. This is more of a distillation strategy rather than a novel, sample-efficient learning paradigm that the paper claims to be."}, "questions": {"value": "The imagination and self-review pipeline during inference is not that clear, can you elaborate it more concretely?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q6BpzqgTJr", "forum": "LQv9LU2Ufg", "replyto": "LQv9LU2Ufg", "signatures": ["ICLR.cc/2026/Conference/Submission747/Reviewer_JjLn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission747/Reviewer_JjLn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934709934, "cdate": 1761934709934, "tmdate": 1762915595694, "mdate": 1762915595694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed RIG, Reasoning and Imagination in an end-to-end Generalist policy method to jointly learn next-n-step imagination, text reasoning, and action predictions in the MineCraft environment. The method leverages the benefit of VLM styled reasoning as well as world model styled imagination aiming to combine all modalities together for a more efficient generalist embodied agent performance. More specifically, the paper proposed a progressive data collection strategy, training a RIG-base vision-action-reasoning model without imagination through a relabeled human trajectory dataset, and a pre-trained policy to collect image-action pairs. It leverages GPT-4o to annotate corresponding reasoning for training. Next, the paper paired negative trajectory and positive trajectory, adopting GPT-4o to generate review & revise annotation and aligned the frames temporally for long horizon training stability. The paper trained the RIG-lookahead model with this data and through rejection sampling fine-tuning. The paper conducted experiments on 6 MineCraft tasks, evaluating the data efficiency, scalability, and performance on embodied, generation, and VAQ reasoning tasks against multiple baselines respectively. The paper shows that the proposed RIG methods significantly improves data efficiency, achieves higher task performance, and is scalable."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated for embodied tasks through imagination (next frame prediction) before action prediction\n- The proposed method jointly trains text reasoning, visual prediction, and action prediction to explicitly model the imagination reasoning before action execution\n- The paper proposed a progressive data collection pipeline to enable the training of imagination and reasoning\n- The paper conducted thorough experiments to demonstrate the performance, efficiency, and scalability through multiple tasks and comparing against multiple baseline models."}, "weaknesses": {"value": "Questions below.\nSuggestions:\n1. Ln 207: \"All these trajectories are rigorously filtered based on task success, diversity across environment seeds, and manual validation of reasoning quality.\" -- (minor) might be helpful to explain how 'rigorous', how 'diverse' these procedures are before claiming them.\n2. Ln 262 Eq (4): the notation is a little confusing: \"\"wait! Let's re-observe...\" is that part of Y- or Y+? \n3. In the Experiment section, the paper highlighted the importance of 'Number of Samples' through multiple figures and tables to highlight the data efficiency. It would be helpful to connect the dots and explain more what is 'number of samples' and and why a higher number indicates better data efficiency\n4. The appendix contains a large amount of details, figures, and results. It would be more helpful to organize and highlight the important and relevant material. \n5. Ln 1133: \"Our model demonstrates strong accuracy and reasoning coherence across categories\" -- what are the evidence to back up this claim? \n6. Ln 1170: space\n7. Ln 1190: VPT (?)\n8. Ln 377: As shown in Figure 4? Figure 5?"}, "questions": {"value": "1. How generalizable is the method after training when applied on a different task or a different environment than MineCraft?\n2. In cases of multiple good answers at certain time point t, does the data collection pipeline only collects a single GT path for training? Does the model only know how to image one path forward? \n3. Figure 5 and Section 3.3 S3: when STEVE-1 only has acc 47.4% and RIG-basic has acc 93.4%, how does S3 collect sufficient positive trajectory from STEVE-1 while negative trajectories from RIG-base?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qCyjlKN4ZH", "forum": "LQv9LU2Ufg", "replyto": "LQv9LU2Ufg", "signatures": ["ICLR.cc/2026/Conference/Submission747/Reviewer_bvMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission747/Reviewer_bvMP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964380190, "cdate": 1761964380190, "tmdate": 1762915595473, "mdate": 1762915595473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RIG, a unified Transformer-based architecture that jointly learns reasoning, action prediction, and visual imagination for embodied agents. Unlike prior modular systems that combine separate reasoning and world-model components, RIG integrates both abilities end-to-end via a progressive data-collection and training pipeline (S0–S4). The method shows strong empirical gains in data efficiency (17× fewer hours) and performance across embodied control, reasoning, and image-generation benchmarks in the Minecraft environment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The explicit synergy between reasoning and imagination in a single model is original and intellectually appealing.\n2. The authors conduct extensive experiments showing consistent gains across multiple dimensions (accuracy, sample efficiency, FID/PSNR, and reasoning quality).\n3. The staged analysis (Action -> Generation -> Reasoning -> Lookahead) demonstrates how each component contributes to the final performance.\n4. The appendix describes data sources, seeds, and prompts, which is appreciated for transparency."}, "weaknesses": {"value": "1. While RIG achieves substantial sample efficiency, it also depends on a large-scale VQA/vision-language model for reasoning annotations and imagination during both training and inference. It remains unclear how much more compute and energy this incurs compared to baselines such as STEVE-1 or MineDreamer. Quantifying this overhead (e.g., FLOPs per inference step, GPU hours, or carbon estimate) would strengthen the argument that RIG is efficient overall.\n2. The paper uses GPT-4o for reasoning and reviewing annotations. Although this is explained, it is worth clarifying how much of RIG’s improvement stems from the quality of these annotations versus the architecture itself.\n3. The “imagined” trajectories clearly improve performance, but the paper could better illustrate what these visual predictions actually look like and whether they faithfully represent plausible world dynamics rather than generic textures."}, "questions": {"value": "1. How much additional computation does the integrated reasoning–imagination (RIG) pipeline require compared to a pure Dreamer or STEVE-1 baseline?\n2. Can the authors provide an estimate of GPU hours or FLOPs per training run?\n3. Given that RIG employs a VQA model for annotation and reasoning supervision, how does this affect the inference-time cost when deployed?\n4. RIG claims 17× data efficiency, but what is the corresponding energy efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jT9cpsXt0E", "forum": "LQv9LU2Ufg", "replyto": "LQv9LU2Ufg", "signatures": ["ICLR.cc/2026/Conference/Submission747/Reviewer_DPLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission747/Reviewer_DPLQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267517196, "cdate": 1762267517196, "tmdate": 1762915595252, "mdate": 1762915595252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}