{"id": "ioYdy7aghG", "number": 16465, "cdate": 1758264862579, "mdate": 1759897238932, "content": {"title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs", "abstract": "Large language models (LLMs) are increasingly trained on tabular data, which, unlike unstructured text, often contains personally identifiable information (PII) in a highly structured and explicit format. As a result, privacy risks arise, since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). While existing MIA methods primarily target textual content, their efficacy and threat implications may differ when applied to structured data, due to its limited content, diverse data types, unique value distributions, and column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Tab-MIA comprises five data collections, each represented in six different encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs fine-tuned with tabular data across multiple encoding formats. In the evaluation, we analyze the memorization behavior of pretrained LLMs on structured data derived from Wikipedia tables. Our findings show that LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases. Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.", "tldr": "We present Tab-MIA, the first benchmark to evaluate membership inference risks in LLMs fine-tuned on tabular data, revealing high memorization and privacy leakage influenced by table encoding formats.", "keywords": ["Membership Inference Attacks", "Tabular Data", "Large Language Models", "Privacy Leakage", "Table Encoding", "QLoRA", "Data Memorization", "Structured Data", "Benchmarking", "Model Vulnerability"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e48a098b9207fc344bf6fcbbed37b5b0fd0a322d.pdf", "supplementary_material": "/attachment/f1d6bde17f2d58a85b788c872ea8fb39491f66bd.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a benchmark for membership inference attacks on tabular data in LLM finetuning. \n\nAuthors argue that many prior works have focused on evaluating MIAs on unstructured text included in LLM training data, while increasingly LLMs are also trained on more structured, tabular data. At the same time, data entries in tabular data might also contain sensitive information, and the way they are encoded might lead to different trends in privacy risk. \n\nIn this work, they propose a benchmark to evaluate MIAs on tabular data encoded and included in LLM finetuning. They consider 6 encoding formats, 5 datasets, 3 MIAs, 4 models and analyze the results. They confirm that larger models, trained for more epochs memorize more, and find that encodings using less redundancies are more at risk. They also find that their attacks generalize across formats, and apply their MIAs to LLM pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of specifically looking at privacy risk associated with structured/tabular data in LLM training data is, from what I know of the literature, novel and interesting. \n- The paper is well written and clearly structured. \n- I find the related work very well written and easy to follow. It might be worth adding this recent work of reference-based MIAs against LLMs [1]. \n- The work has an extensive evaluation framework, considering 6 different encoding formats, multiple models (and sizes), different regimes of memoriation by varying the number of epochs, 3 different MIAs and 5 datasets.  \n- Section 5.3 on cross-format generalization is a nice addition. You might want to link this to what others have found to be a 'mosaic memory', i.e. LLMs are very good at piecing together near-duplicate information, even when random tokens separate the information meaningful for membership [2]. \n\n[1] Hayes, J., Shumailov, I., Choquette-Choo, C. A., Jagielski, M., Kaissis, G., Lee, K., ... & Cooper, A. F. (2025). Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models. arXiv preprint arXiv:2505.18773.\n\n[2] Shilov, I., Meeus, M., & de Montjoye, Y. A. (2024). The Mosaic Memory of Large Language Models. arXiv preprint arXiv:2405.15523."}, "weaknesses": {"value": "When discussing the results, I find that, at least in section 5.1, authors devote a substantial amount of time on memorization trends that are already quite well-known and in are in my opinion not the most interesting ones in this benchmark. For instance, the fact that AUC increases with model size or with number of epochs is already well known [1, 2, 3]. Moreover, the absolute value of AUC is ultimately a factor of the finetuning parameters such as learning rate, and is not really what matters in this case. Instead, the relative performance across tabular datasets, and encoding formats is more interesting and worth exploring a bit deeper in my opinion. \n\nFor instance, the difference in MIA performance across datasets in Table 2 is quite striking and interesting. Could you explain this by e.g. examining the length of the records (as longer sequences are more at risk [1,2,3]) or by examining how unique the records are (either in perplexity using the base model as in [3] or in overlap between members and non-members as explored in [4], or simply by analyzing the uniqueness of attributes across records). \n\nThe intuition why certain encoding formats are more at risk than others is quite interesting. However, I find the argument a bit confusing at the moment, i.e. you first mention that methods like HTML and JSON introduce structural redundancies so lower AUC, while in lines 405-406 you mention that longer context would increase the AUC. How are both arguments compatible? It might also be interesting to look at how efficiently each encoding format is tokenized by the target model. \n\n- Results in Section 5.4 might be questionable. Namely, prior work has found that if members and non-members do not come from exactly the same distribution, any MIA performance might be due to the attack being able to detect a distribution shift rather than the target model memorizing the members [4, 5, 6, 7]. To understand this, I think it's important to include a blind/model-less baseline as in [6,7]. If this baseline is substantially better than a random guess, it would be hard to interpret any of the results from Table 6. \n\n[1] Kandpal, N., Wallace, E., & Raffel, C. (2022, June). Deduplicating training data mitigates privacy risks in language models. In International Conference on Machine Learning (pp. 10697-10707). PMLR.\n\t\n[2] Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., & Zhang, C. (2022, February). Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.\n\n[3] Meeus, M., Shilov, I., Faysse, M., & de Montjoye, Y. A. Copyright Traps for Large Language Models. In Forty-first International Conference on Machine Learning.\n\n[4] Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., ... & Hajishirzi, H. (2024). Do membership inference attacks work on large language models?. arXiv preprint arXiv:2402.07841.\n\n[5] Maini, P., Jia, H., Papernot, N., & Dziedzic, A. (2024). LLM Dataset Inference: Did you train on my dataset?. Advances in Neural Information Processing Systems, 37, 124069-124092.\n\n[6] Meeus, M., Shilov, I., Jain, S., Faysse, M., Rei, M., & de Montjoye, Y. A. (2025, April). Sok: Membership inference attacks on llms are rushing nowhere (and how to fix it). In 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) (pp. 385-401). IEEE.\n\n[7 ]Das, D., Zhang, J., & Tramèr, F. (2025, May). Blind baselines beat membership inference attacks for foundation models. In 2025 IEEE Security and Privacy Workshops (SPW) (pp. 118-125). IEEE.\n\n**I hope these suggestions help improve the paper and, if addressed, I'd be willing to increase my score.**"}, "questions": {"value": "- Is there evidence that people have been finetuning or pretraining LLMs on tabular dataset including PII? It could be a nice addition to provide an overview of this. \n\n- Could authors also add the Ref or Ratio attack from [1] or as used by [2]? You could easily just divide the target finetuned model loss by the loss of the pretrained model. From my experience, this should be better than any of the other methods you currently have. \n\t\n\n[1] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., ... & Raffel, C. (2021). Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21) (pp. 2633-2650).\n\n[2] Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., ... & Hajishirzi, H. (2024). Do membership inference attacks work on large language models?. arXiv preprint arXiv:2402.07841."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mBRz0Ar9qM", "forum": "ioYdy7aghG", "replyto": "ioYdy7aghG", "signatures": ["ICLR.cc/2026/Conference/Submission16465/Reviewer_bFS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16465/Reviewer_bFS1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761152517785, "cdate": 1761152517785, "tmdate": 1762926573562, "mdate": 1762926573562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Tab-MIA, a benchmark dataset for evaluating membership inference attacks (MIAs) on large language models (LLMs) trained with tabular data. Using this benchmark, the authors conduct systematic evaluation of MIA methods across multiple tabular encoding/serialization formats. They find that LLMs can memorize structured data to an extent that enables effective membership inference, with AUROC scores approaching 90% after minimal fine-tuning. The study also reveals partial transferability of attacks across encoding formats, emphasizing the need for stronger privacy-preserving training practices when adapting LLMs to tabular data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is easy to follow and presents a useful and timely contribution, introducing the benchmark for MIAs on tabular data in LLMs.\n\n- Provides valuable empirical insights into memorization patterns and attack transferability, highlighting underexplored privacy risks in adapting tabular data to LLM.\n\n- The methodology and benchmark design have practical utility for future privacy research on structured data."}, "weaknesses": {"value": "- The bullet points in lines 257–269 repeat the content of Figure 1; consider moving these details to the appendix for brevity.\n\n- Consider including a visual example contrasting long- vs short-context table encodings to help readers intuitively understand the setup.\n\n- The paper relies heavily on MIA metrics but provides limited explanation of them. Expanding the description of attack metrics in the main text would make the results more interpretable.\n\n- Consider reordering the results section, starting with Section 5.2 (encoding comparison) would align better with the paper’s central research question.\n\nI am assigning a low score primarily due to concerns about reproducibility. For a benchmark paper, the release of code is essential to validate results and facilitate future research. Without access to the implementation, it is difficult to fully trust or reproduce the reported findings. I would be willing to raise my score if the authors make the code publicly available and provide clear implementation details of the MIA metrics in the paper, since much of the paper’s analysis depends on these metrics."}, "questions": {"value": "- The filtering step (line 246) is unclear, why does the Adult dataset shrink from 48k to 3k samples? How exactly was the filtering applied to reduce the Adult dataset size?\n- What would happen if models were fine-tuned using a combination of encoding formats, would this amplify or mitigate memorization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nsAGCIJTG4", "forum": "ioYdy7aghG", "replyto": "ioYdy7aghG", "signatures": ["ICLR.cc/2026/Conference/Submission16465/Reviewer_oEMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16465/Reviewer_oEMe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946262648, "cdate": 1761946262648, "tmdate": 1762926573212, "mdate": 1762926573212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Tab-MIA- a dataset for evaluating membership inference of tabular data in LLMs. Tab-MIA consists of a set of tables sourced from public datasets, represented in a variety of common formats used for LLM training. The authors present an overview of the dataset and run a series of experiments to demonstrate its utility for auditing publicly trained and privately fine-tuned models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The work fills an important gap in auditing LLMs for privacy risks, beyond conventional text membership tests.\n* Strong evaluation on a suite of LLMs under a fine-tuning paradigm, as well as evaluation of some public pretrained models."}, "weaknesses": {"value": "* I wonder about the realism of fine-tuning a model on some context-free tables- in most natural language tasks a table will be associated with accompanying text to set context, provide explanation, inject a user query, etc.  From this perspective I'm not sure how to interpret the significance of the attack results. \n\n* Experiments on larger models would be valuable, even just targeting large pretrained models without fine-tuning.\n\n* An experiment demonstrating a defense (eg training with DP-SGD or privatizing the data itself with a DP method) and potential privacy-utility tradeoffs would strengthen the paper."}, "questions": {"value": "1. How do you work around limitations of some formats- eg lack of ability for some formats to allow spans across multiple columns or embed sub-tables?  The universe of tables in public data, especially html-formatted tables, is much richer than simple grid format.\n\n2. Aside from table representation do you have any other insights as to the kinds of tables that are easily memorized? \n\n3. Generating synthetic non-member tables from Gpt-4o-mini: these are clearly contaminated by both the in-context sample and any public tables in GPT-4o's training set. I wonder If there are alternatives that would generate high-quality, plausble tables that aren't influenced by the in-member data.  One option that comes to mind is using a library like SmartNoise to generate tables from public seeds that meet a row-wise differential privacy guarantee, but would still be some level of contamination.\n\n4. Evaluation against larger models and/or a public model API that provides logits for a very large model would be nice to have."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "guA6qyAVYn", "forum": "ioYdy7aghG", "replyto": "ioYdy7aghG", "signatures": ["ICLR.cc/2026/Conference/Submission16465/Reviewer_P5qU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16465/Reviewer_P5qU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949853756, "cdate": 1761949853756, "tmdate": 1762926572848, "mdate": 1762926572848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark dataset, Tab-MIA, for evaluating MIAs on tabular data in LLMs. Tab-MIA includes short-context and long-context tables, each represented in six different textual encoding formats. \n\n\nShort-context tables are derived from QA benchmarks WikiTableQuestions (WTQ), WikiSQL, and TabFact. Long-context tables are derived from structured tabular benchmarks frequently used in fairness, regression, and privacy studies: Adult (Census Income) dataset and the California Housing dataset.  \n\nEncoding fromats are: JSON, HTML, Markdown, Key-Value Pair, Key-is-Value, and Line-Separated (CSV-like)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Running three exisitng MIAs (LOSS attack, Min-K% attack, Min-K%++ ) on LLaMA-3.1 8B, LLaMA-3.2 3B, Gemma-3 4B, and Mistral 7B QLoRA fine-tuned on Tab-MIA dataset. \n\n- Highlighting that Tabular data may contain personally identifiable information (PII), commercially sensitive material, or domainspecific details that are not intended for broad dissemination"}, "weaknesses": {"value": "- Lack Dataset novelty/validity: Tab-MIA is a recombination of existing tabular datasets. The construction also risks the member vs. non-member boundary, as there are no guarantees that chosen LLMs have not already seen all data.\n\n- No methodological novelty: No new membership-inference attack tailored to tabular data is proposed.\n\n- Results lack novelty: The paper largely reiterates established findings, nothing new about tabular data:\n\t- LLMs can memorize tabular data\n\t- partial transferability of attacks across encoding formats\n\t- a consistent and substantial increase in vulnerability as the number of fine-tuning epochs grows\n\t- larger models are also significantly more vulnerable to MIAs\n\n- Use of pre-training and fine-tuning is not consistent\n\n- Typo in line 121: DC-PDD("}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pyWlj0FxFq", "forum": "ioYdy7aghG", "replyto": "ioYdy7aghG", "signatures": ["ICLR.cc/2026/Conference/Submission16465/Reviewer_petP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16465/Reviewer_petP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274523695, "cdate": 1762274523695, "tmdate": 1762926572391, "mdate": 1762926572391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}