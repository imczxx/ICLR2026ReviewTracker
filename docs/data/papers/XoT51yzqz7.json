{"id": "XoT51yzqz7", "number": 10483, "cdate": 1758173327175, "mdate": 1763065117210, "content": {"title": "Interpreting Any Condition to Caption for Controllable Video Generation", "abstract": "To address the bottleneck of accurately interpreting user intent within the current video generation community, we present Any2Caption, a novel framework for controllable video generation from any condition. The key idea is decoupling various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs—text, images, videos, and specialized cues such as region, motion, and camera poses—into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K annotations for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models.", "tldr": "we present Any2Caption, a novel framework for controllable video generation from any condition.", "keywords": ["Mulit-Modal Understanding", "Video Caption", "Controllable Video Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6007d727cfcd1e79dc015c7c99c6ee43191f837a.pdf", "supplementary_material": "/attachment/c4e52f0620c3c80aa9c10467c519ffdf4fd194e1.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a pipeline to generate structured text prompts based on various input conditioning signals (text, human pose, depth, segmentation, camera trajectory, etc) for text-conditional video generation. For this, the authors curate a dataset with various synthetic annotations. Then, synthetic structured text prompts are generated for it using GPT-4V and LLaVA. Then, they are manually filtered. Then, they are converted to short captions with GPT-4V. Then, the authors train (fine-tune from existing components) an MLLM to generate long captions based on short captions and annotations. This MLLM is then used at inference time to generate structured captions for video models (MotionCtrl, CameraCtrl, ControlVideo, etc.). The authors test influence of structured captions for several existing video generators and demonstrate improvement."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a model which is able to do prompt enhancement using the side conditioning information (depth/sketch/segmentation/etc). This can help mitigate contradictions between enhanced captions and the corresponding side annotations.\n- The paper presents a dataset with synthetic annotations which could be helpful for video model fine tunings in the industry (depending on the quality and the license: the paper does not disclose these details)."}, "weaknesses": {"value": "- I am convinced that this paper should be a technical report, rather than an academic submission. I do not understand what insights the paper intents to convey. The fact that a VLLM can generate structured captions from diverse inputs is obvious. The fact that structured captions improve the video generation performance is well-known.\n-  It is misleading to say that the method converts any condition into a caption, because it makes a reader think that such dense conditions like depth are converted to some intricate caption and the video generator does not need to be conditioned on a depth map anymore. At least it was my original impression after reading the paper for the first ~20 minutes. Because of that, I find the \"Any2Caption\" pipeline to be misleading.\n- The qualitative examples from the supplementary do not include the accompanying dense prompts which were used to produce them.\n- It would be good to include more qualitatives\n- It's unclear why the method is tested on outdated backbones (MotionCtrl/CameraCtrl, ControlVideo, etc) and why Wan2.1/Wan2.2 model is excluded from the comparisons\n- It's not obvious why we need to generate long captions using the conditioning signals. I guess it is only necessary for dense conditioning like depth/sketch/human pose because otherwise long captions can contradict the conditionings. But if that's the case, it should be clearly shown. Overall, i find the vanilla prompt enhancement baseline to be missing from the comparisons."}, "questions": {"value": "- Am I getting it right that for depth2video, the model is still conditioned on the original depth map? (e.g., for ControlVideo, the original ControlNet is still being used with the corresponding depth map input).\n\nTypos: \"how well\" => \"how good\" (in Sec 6.2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UbE6RomUus", "forum": "XoT51yzqz7", "replyto": "XoT51yzqz7", "signatures": ["ICLR.cc/2026/Conference/Submission10483/Reviewer_rStu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10483/Reviewer_rStu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673313984, "cdate": 1761673313984, "tmdate": 1762921774660, "mdate": 1762921774660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wq2Aq8ufRz", "forum": "XoT51yzqz7", "replyto": "XoT51yzqz7", "signatures": ["ICLR.cc/2026/Conference/Submission10483/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10483/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763065116312, "cdate": 1763065116312, "tmdate": 1763065116312, "mdate": 1763065116312, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Any2Caption, a framework that turns a vision–language model into a structured re-captioning model conditioned on multiple modalities (depth, pose, camera, identity, etc.). The method generates six-part structured captions (dense, object, background, action, style, camera) from arbitrary multimodal inputs, which can then be fed into existing text-to-video models to improve controllability and visual quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and clearly explains motivation and method.\n\n - The proposed structured-captioning paradigm is intuitive yet effective, successfully turning a general VLM into a condition-aware re-captioning model.\n\n- The implementation is efficient, requiring no modification to downstream video generators.\n\n- The results on multiple generators show that longer, structured captions can improve controllability and consistency to some extent."}, "weaknesses": {"value": "**Distribution Shift and Potential Suboptimal Improvement**\n\nThe paper evaluates multiple video generation models in a zero-shot setting, but the structured caption model was likely never exposed to them during training.\nSince different generators prefer different caption styles, the improvement may simply come from length improvement rather than genuine interpretive ability(as many video generators are trained on dense captions, and Any2Caption’s outputs are also long and verbose, which could coincidentally fit those models better).\nThis suggests that the observed improvement might be partly artefactual and potentially suboptimal rather than reflecting a true generalization capability.\n\n**Limited Necessity Demonstration**\n\nIn RQ1 (“Is the structured caption necessary?”), the authors intentionally choose the multi-ID condition, which is a case where structured rewriting may help the most due to better seperation of semantic entities among input identities (though such seperation can potentially be achieved without structured caption).\n\nThis makes this ablation reasonable but also biased: it only shows necessity for this task, potentially most favorable setting.\n\nThe same necessity is not demonstrated for other settings, where structured captions may be unnecessary or even redundant.\n\nThus, RQ1 provides only partial evidence and does not establish the claimed general necessity of structured captioning.\n\n\n**Inadequate Baselines and Limited Practical Impact**\n\nAs a paper essentially proposing a prompt enhancer, Any2Caption should be compared not only with short prompts but also with existing prompt enhancers. Most well-known open-sourced video generation models/projects already include prompt enhancers, often large LLMs fine-tuned or prompted via In-context learning to generate model-preferred prompts (e.g., CogVideoX’, Hunyuan’s, etc.). Additionally, as author mentioned, many video recaptioning methods are also proposed to enhance video generation ( ShareGPT4Video, InstanceCap, etc.)\nWithout such baselines, it remains unclear whether Any2Caption provides benefits over existing prompt optimization strategies.\n\nFrom a more practical standpoint, for instance, if a user is already employing models such as HunyuanVideo or CogVideoX, both of which feature built-in prompt enhancers optimized for their respective training data, it is not obvious why one would replace them with Any2Caption. In the absence of clear evidence of superior generalization, adaptability, or usability, the practical contribution and real-world impact of this work appear limited.\n\n**Unreliable Evaluation**\n\nThe evaluation pipeline is also not convincing to me.\n\nFor text/caption quality, Table 3 already shows that higher caption metrics do not correlate with better video results, undermining the relevance of Table 4.\n\nAlthough the appendix includes VDC benchmark results for captioning evaluation, the compared baselines are largely outdated, and notably, AuroraCap (introduced in the VDC paper itself and reported results better than Any2Caption) is missing. Thus VDC results is still not meaningful to me.\n\nFor video quality, most of the reported gains are minor and may fall within metric variance (many of these metrics are known to be not stable, e.g. aesthetic quality, etc), while qualitative examples in the paper are relatively limited considering the large amount of tasks the method claimed to tackle.\n\nI also checked the provided videos in the supplementary materials, but in many cases I can't find significant differences/improvements comparing the short caption version and the structured caption version (e.g, camera to video, ids + depth to video).\n\nGiven these issues, the evaluation is relatively weak and potentially misleading. A proper human study (e.g. voting) comparing videos from short prompts, existing enhancers, and Any2Caption-generated captions would provide much more credible evidence, and a more qualitative comparison is also needed per task.\n\n**Minor Issues**\n- The camera pose visualization uses overly large frustum cones, making motion changes almost invisible.\n\n- Several typos:\n    - Fig 1 “normal bae” → “normal base”\n    - Table 4 “METER” → “METEOR”\n    - L307 “access” → “assess”?\n    - Table 2 “vieo” → “video”"}, "questions": {"value": "- How is CLIP-T for long structured caption calculated (As CLIP textual encoder can not encode long sequence without losing information)? Is it still short-caption-video similarity score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LEslfkbJiv", "forum": "XoT51yzqz7", "replyto": "XoT51yzqz7", "signatures": ["ICLR.cc/2026/Conference/Submission10483/Reviewer_WxPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10483/Reviewer_WxPH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808733600, "cdate": 1761808733600, "tmdate": 1762921774298, "mdate": 1762921774298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Any2Caption, a novel framework designed to enhance controllable video generation by accurately interpreting diverse user inputs—such as text, images, videos, and specialized conditions like motion and camera poses—into dense, structured captions. The core idea is to decouple the interpretation of these multimodal conditions from the video synthesis process, leveraging modern multimodal large language models (MLLMs) to bridge the gap between user intent and video generation.\n\nThe authors also present Any2CapIns, a large-scale dataset containing 337K instances and 407K conditions, specifically curated for training Any2Caption in an any-condition-to-caption instruction tuning paradigm. Comprehensive evaluations demonstrate that Any2Caption significantly improves both the controllability and quality of generated videos compared to existing models. The framework can seamlessly integrate with various backend video generators without requiring additional fine-tuning, making it a versatile and efficient solution for controllable video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work presents a compelling and original formulation of controllable video generation by decoupling multimodal condition understanding from the video synthesis process, a design that leverages the strengths of modern MLLMs and avoids overburdening diffusion/DiT models with multimodal reasoning. This “any-condition-to-caption” paradigm is conceptually clean and practical, and represents a creative generalization of prompt enrichment and recaptioning techniques: rather than fine-tuning generators or relying solely on textual prompts, the system converts arbitrary input signals (text, images, depth, pose, camera motion, multiple identities) into structured captions to serve as universal control signals.\n\n2. The paper demonstrates high technical quality with substantial engineering and system design effort, including the construction of Any2CapIns, a large multimodal dataset (337K videos, 407K condition annotations) curated through a mixture of GPT-4V and human verification. The method includes carefully considered architectural choices (dedicated pose and camera encoders, progressive mixed training, alignment stage) and ablations support these decisions. Empirical results are broad and convincing: the approach consistently improves controllability, instruction fidelity, and video quality across diverse backbones (e.g., HunyuanVideo, CogVideoX, Ctrl-Adapter, CameraCtrl) and generalizes to unseen controls such as sketches and segmentation masks.\n\n3. The pipeline is well illustrated, training stages are clearly decomposed, and evaluation includes a mix of lexical, semantic, intent-based, and perceptual metrics, along with qualitative examples. The significance is high as controllable video generation is a rapidly developing area, and this architecture offers a plug-and-play solution that aligns with the trend toward modular generative stacks. The work has clear potential to influence future research, especially where multimodal control and human-intent grounding intersect."}, "weaknesses": {"value": "1. A primary concern is dependence on MLLMs for accurate interpretation and structured captioning. While the paper shows strong results, caption hallucinations or subtle misinterpretations may degrade downstream generation; however, the paper does not deeply quantify or analyze such failure modes (e.g., how errors propagate through the pipeline). Providing a systematic robustness study—e.g., noisy or ambiguous conditions, conflicting signals between modalities—would strengthen confidence in real-world deployment.\n\n2. While the plug-and-play nature is a strength, practical system overhead—running a large MLLM per request before video synthesis—is not quantified. Reporting latency costs or FLOPs would provide transparency regarding scalability for interactive creative workflows."}, "questions": {"value": "1. Did the authors test dynamic or learned templates?\n\n2. In scenarios where visual conditions convey information that is hard to verbalize (fine geometry, fashion textures), does text bottleneck expressiveness?\n\n3. Have the authors conducted or considered user studies to verify usability and perceived control quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Il5ttzeNuz", "forum": "XoT51yzqz7", "replyto": "XoT51yzqz7", "signatures": ["ICLR.cc/2026/Conference/Submission10483/Reviewer_5Cq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10483/Reviewer_5Cq1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830592210, "cdate": 1761830592210, "tmdate": 1762921773738, "mdate": 1762921773738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In text + any condition to video generation, the alignment between text prompts and additional conditions is often overlooked. This paper proposes an MLLM fine-tuning recipe and dataset for generating structured text prompts that are aligned with additional conditions in video generation models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It addresses the alignment between text and additional non-text conditions, resulting in superior performance across various conditional tasks such as camera controllable video generation and depth-to-video generation.\n- Extensive analysis and experiments are provided.\n- The effects of structured captions and experimental strategies are well ablated."}, "weaknesses": {"value": "Comparisons in video generation tasks are mostly limited to short naive captions (baseline) vs. proposed structured enriched captions. There is a concern that if structured captions generated from non-fine-tuned VLMs, or simply enriched (but not structured) prompts also achieve good performance on video generation tasks, the justification for the proposed method could be somewhat diminished. It would be helpful if the thorough comparisons with those possible approaches."}, "questions": {"value": "What are the failure cases of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P60YS7T8GK", "forum": "XoT51yzqz7", "replyto": "XoT51yzqz7", "signatures": ["ICLR.cc/2026/Conference/Submission10483/Reviewer_vmYy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10483/Reviewer_vmYy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929154209, "cdate": 1761929154209, "tmdate": 1762921773243, "mdate": 1762921773243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}