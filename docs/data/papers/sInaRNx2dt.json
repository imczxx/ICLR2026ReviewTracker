{"id": "sInaRNx2dt", "number": 8695, "cdate": 1758095257427, "mdate": 1759897769265, "content": {"title": "Un-Attributability:  Computing Novelty from  Retrieval & Semantic Similarity", "abstract": "Understanding how language‑model outputs relate to the pretraining corpus is central to studying model behavior. Most training‑data attribution (TDA) methods ask which training examples causally influence a given output, often using leave‑one‑out tests. We invert the question: which outputs *cannot* be attributed to any pretraining example? We introduce *un*-attributability as an operational measure of semantic novelty: an output is *novel* if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top‑$n$ candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around *un*-attributability enables efficient analysis at pretraining scale. We release code and $\\sim$20 TB of embeddings and index artifacts to support replication and large‑scale extension.", "tldr": "We quantify which outputs cannot be attributed to the training corpus at all-- termed as novel. We study semantic novelty of LLM outputs", "keywords": ["novelty", "embedding", "similarity", "indexing", "TDA", "attribution"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af387c2275a893f9e0fe58c418e7089d73d2bf72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new approach for measuring the novelty of LLMs' outputs. It focuses on unattributability to the pretraining corpus and asks which outputs cannot be traced to semantically similar contexts in the training data. \n\nThe pipeline includes two main stages: a retrieval pipeline using GIST embeddings and re-ranking with ColBERTv2 to find the nearest corpus item. Then the novelty is defined as if the output is less similar to the corpus than a human-generated baseline. \n\nThe authors find that novelty varies by domain and size. Besides, instruction tuning increases novelty beyond mere style shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Scalable using fast retrieval with FAISS and reranking using ColBERTv2.\n\nGood normalization technique by computing the ratio w.r.t. the human-written baseline. This solve the issue of finding a threshold for novelty and makes the result more interpretable by indicating how much more/less novel the output is wr.t. human.\n\nComprehensive analysis showing that -> novelty varies by task domain; Models reuse pretraining data across longer spans than previously reported; Instruction tuning increases novelty not just stylistically, but semantically."}, "weaknesses": {"value": "Didn't compare results with baselines like InfiniGram\n\nThe choice of models is limited to one family and small sizes. It would be better to consider models from the OLMo family, as they are also fully open."}, "questions": {"value": "Have you tried other embeddings and re-rankers?\n\nHave you examined the effect of the in-context novelty itself on the novelty of the model output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fUYZPFsLTo", "forum": "sInaRNx2dt", "replyto": "sInaRNx2dt", "signatures": ["ICLR.cc/2026/Conference/Submission8695/Reviewer_CSwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8695/Reviewer_CSwo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738053540, "cdate": 1761738053540, "tmdate": 1762920503316, "mdate": 1762920503316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces “un-attributability”: a new way of thinking about how novel model generations are in relation to the corresponding pre-training data. This approach involves indexing the entire corpus with GIST embeddings and for new model generations measuring semantic similarity (with some reranking). Any time a human reference ranks higher than retrieved chunks, that generation is deemed un-attributable. They run experiments on SmolLM and SmolLM2 with publicly available pre-training data and report findings on unprompted vs. prompted generations, fine-tuned models and for domain-specific tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Thinking about what is not attributable seems interesting, though it might be hard to prove a negative.  \nGist + faiss + colbert seems like a nice bit of infra to contribute to the community."}, "weaknesses": {"value": "The discrepancy between the question: “which outputs cannot be attributed to any pretraining example” and then measuring semantic similarity to quantify this is confusing. What’s being measured here is less-constrained than vanilla token memorization but much weaker than TDA. Un-attributability seems like an over-claim here and since it is a key part of what is being proposed, it would be helpful to flesh this out."}, "questions": {"value": "Could you clarify what the human references are and what kind of coverage they have in relation to the generations being evaluated for novelty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eOmehnvkLX", "forum": "sInaRNx2dt", "replyto": "sInaRNx2dt", "signatures": ["ICLR.cc/2026/Conference/Submission8695/Reviewer_AHZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8695/Reviewer_AHZ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868969668, "cdate": 1761868969668, "tmdate": 1762920502871, "mdate": 1762920502871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies language model generation un-attributability: which LM outputs are not similar to spans of text in the pretraining corpus? The paper proposes a two-stage embedding-based retrieval pipeline: first, candidate documents are retrieved via GIST-embedding cosine similarity, and then candidates are reranked using ColBERTv2. This second stage takes care to account for the chunk-based nature of the comparisons by considering multiple different chunk sizes. Experiments on SmolLM and SmolLM2 in open-domain settings show that generations continuing from a context are more novel than human baselines, among other findings. In domain-specific settings, the results show that smaller models yield  more novel generations than larger models, and novelty varies by task domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper operationalizes a new notion of un-attributability to study the novelty of LLM generations that goes beyond the surface form metrics used in prior work. It estimates novelty through a two-stage embedding retrieval pipeline to identify training documents that are similar to a generation. \n2. The experiments are sound, and novelty values are contextualized against a human baseline that is known to be novel.\n3. The experimental takeaways are interesting and help shed light on the relationship between training data and model generations, and I can imagine other approaches could build on the proposed notion of un-attributability."}, "weaknesses": {"value": "1. More thorough evaluation of the retrieval pipeline would bolster the claims about novelty. For example, the claim \"Novelty increases with sequence length in unprompted generation\" (line 310 and Figure 3) could be true, but it could also be the case that retrieval gets worse with increasing sequence length. Can you provide evidence that the latter is not the case?\n\n2. Novelty may not be enough on its own to understand language model outputs. Take the claim in line 378: \"Smaller SmolLM2 models are more novel than larger ones.\" Novelty doesn't seem like it's useful on its own here---higher novelty could be explained by smaller models yielding disfluent text, which would be dissimilar from pretraining corpus text. Perhaps contextualizing novelty with model performance (or even qualitative examples of more novel and less novel outputs and their retrieved neighbors) would be useful.\n\n3. The paper mentions that its results go beyond lexical methods for assessing generation novelty like [1] (lines 85-87). Providing concrete quantitative comparisons to findings from n-gram tools like Rusty-DAWG would strengthen the impact of this paper.\n\n[1] Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG, Merrill et al., EMNLP 2024."}, "questions": {"value": "1. I really like the attention paid to chunking for stage 2 of the retrieval pipeline. Do you have any analysis for how much chunking boundaries can affect stage 1, embedding retrieval?\n\nSmall comments:\n- There's a mismatch between human generation baseline values: it's defined as 1 in lines 242-243 but 0 in Figures 3 and 4.\n- I suggest making the title more specific, perhaps even a small change like \"Un-attributability: computing **generation** novelty from retrieval & semantic similarity\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LICrPlSl04", "forum": "sInaRNx2dt", "replyto": "sInaRNx2dt", "signatures": ["ICLR.cc/2026/Conference/Submission8695/Reviewer_tt4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8695/Reviewer_tt4B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885838342, "cdate": 1761885838342, "tmdate": 1762920502543, "mdate": 1762920502543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines novelty as un-attributability: an LLM output is “novel” if the pretraining corpus contains no close semantic match under a two-stage retrieval pipeline—GIST+FAISS for top-n candidates, then ColBERTv2 for token-level re-ranking and scoring. Scores are length-normalized and calibrated against a human baseline (domain-matched text); the per-chunk similarity ratios are summarized by the median, and values < 1 are interpreted as “more novel than human baseline.”       \nThe authors show experiments on SmolLM / SmolLM2 across open-ended and benchmarked settings (e.g., GSM8K, TruthfulQA, OpenRewriteEval). They report trends like domain-dependent novelty, smaller models sometimes appearing more “novel,” and instruction-tuned models showing lower similarity (hence higher “novelty”) than bases, all relative to the human baseline."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Scalable technique: This recasts novelty as a retrieval problem that is feasible at pretraining scale; easy to implement and the pipeline and artifacts are open-sourced for reproducibility. \n\n\n- Empirical results: domain effects and instruction-tuning effects on the similarity-to-corpus curves, with baselines are shown in terms of how finetuning affects novelty"}, "weaknesses": {"value": "- Semantic similarity is not causal attribution. While the authors acknowledge the method “is not a drop-in replacement” for causal attribution, it is quite contradictory to call it “un-attributable”. Could the authors clarify this ?\n\n- The papers they cite in the paper such as [1] already show that TDA can be scaled to pretraining data, which already negates their fundamental premise of that TDA is not scalable\n\n\n- The paper also assumes that failure to retrieve a neighbor is taken as novelty - this aspect is very much pipeline-dependent (embeddings, chunking, reranker). If the GIST embeddings aren’t accurate enough, the assumption that a chunk is novel might not hold. \n\n\n- The paper also assumes that only “one” retrieved fact should be responsible for a generated text chunk. There are papers that challenge this assumption [2]. This makes the authors’ assumption weak, unless they rebut this strongly. \n\n\n- No causal validation: There are no ablations (e.g., remove or paraphrase specific shards and re-measure) to connect the proxy to actual training-data influence, so findings remain correlational. The paper itself flags this limitation while also claiming this relates to unattributability\n\n\n[1] Scalable Influence and Fact Tracing for Large Language Model Pretraining. Chang et al, 2024\n\n[2] Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. Guu et al, 2023"}, "questions": {"value": "- Baseline dependence - The “1.0” anchor moves with the chosen human baseline, dataset and chunk size; Can the authors provide a more detailed explanation how this works?  \n\n- What exactly is the motivation to choose the human baseline ? Why was it done this way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "exrN8Y8twd", "forum": "sInaRNx2dt", "replyto": "sInaRNx2dt", "signatures": ["ICLR.cc/2026/Conference/Submission8695/Reviewer_1DVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8695/Reviewer_1DVE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973447107, "cdate": 1761973447107, "tmdate": 1762920502087, "mdate": 1762920502087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}