{"id": "nczgZiF4XR", "number": 3143, "cdate": 1757342065721, "mdate": 1759898106368, "content": {"title": "Securing Transfer-Learned Networks with Reverse Homomorphic Encryption", "abstract": "The growing body of literature on training-data reconstruction attacks raises significant concerns about deploying neural network classifiers trained on sensitive data. However, differentially private (DP) training (e.g. using DP-SGD) can defend against such attacks with large training datasets causing only minimal loss of network utility. Folklore, heuristics, and (albeit pessimistic) DP bounds suggest this fails for networks trained with small per-class datasets, yet to the best of our knowledge the literature offers no compelling evidence. We directly demonstrate this vulnerability by significantly extending reconstruction attack capabilities under a realistic adversary threat model for few-shot transfer learned image classifiers. We design new white-box and black-box attacks and find that DP-SGD is unable to defend against these without significant classifier utility loss. To address this, we propose a novel homomorphic encryption (HE) method that protects training data without degrading model's accuracy. Conventional HE secures model's input data and requires costly homomorphic implementation of the entire classifier. In contrast, our new scheme is computationally efficient and protects training data rather than input data. This is achieved by means of a simple role-reversal where classifier input data is unencrypted but transfer-learned weights are encrypted. Classifier outputs remain encrypted, thus preventing both white-box and black-box (and any other) training-data reconstruction attacks. Under this new scheme only a trusted party with a private decryption key can obtain the classifier class decisions.", "tldr": "We design and analyse data reconstruction attacks that work in a realistic threat model and for the few-shot transfer learning scenario we find a novel defense via reverse homomorphic encryption which incurs negligible privacy-utility tradeoff.", "keywords": ["data reconstruction", "differential privacy", "transfer learning", "homomorphic encryption"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f70a5ac78c4d2b4d6885c8e31070b9fd95e3313.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* This paper makes two primary contributions. First, it identifies and demonstrates a significant vulnerability in few-shot transfer learning (TL). The authors introduce new white-box and black-box training-data reconstruction attacks and show empirically that Differentially Private Stochastic Gradient Descent (DP-SGD) fails to provide an acceptable privacy-utility tradeoff in this regime. Specifically, they show that for small, few-shot datasets (e.g., $N=10$ or $N=40$ samples), defending against their attacks with DP-SGD incurs a severe classifier accuracy drop of.\n* Second, to address this gap, the paper proposes a defense mechanism named \"Reverse Homomorphic Encryption\" (RHE). This approach is tailored for the TL use case where the training data is sensitive, but the inference data is not. RHE involves encrypting only the weights and biases of the transfer-learned \"head\" neural network, while leaving the large base model and the inference inputs unencrypted. The authors claim this method is computationally efficient, as it only applies HE to a shallow network (at most 3 layers) and uses a square activation function ($x^2$) to avoid costly bootstrapping. They claim this defense completely blocks training-data reconstruction attacks (both white-box and black-box) with minimal degradation in model accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper's biggest strength is its extensive and rigorous supplementary material. The appendix provides a detailed and significant amount of ablations, experiments, and proofs covering many aspects of the white-box and black-box attacks, an overview of previous methods for both attacks and homomorphic encryption, and the supporting theoretical proofs.\n* The motivation is compelling. It highlights a critical, practical, and under-appreciated gap in privacy-preserving ML: the failure of DP-SGD to protect few-shot transfer-learned models. The authors correctly identify that this scenario is common in sensitive domains, which are frequent applications for TL.\n* The paper provides a valuable and novel contribution by designing and demonstrating sophisticated reconstruction attacks (both white-box and black-box) under a realistic \"weak adversary\" threat model. The empirical results in Figure 2, which plot the sharp trade-off between privacy ($\\epsilon$) and utility (accuracy) when defending against these attacks with DP-SGD, are convincing."}, "weaknesses": {"value": "* The HE component's of this work, however, feel somewhat disconnected from the core contribution, as if it were an add-on rather than a deeply integrated part of the workflow. While portions of the solution are pragmatic, its engagement with recent advancements in the field appears limited.\n    * The strategy to replace ReLU with a square activation ($f(x)=x^{2}$) is a point of concern. While this avoids the cost of bootstrapping, it presents a trade-off. The validation (Table 4) is a direct comparison between $x^2$ and ReLU for shallow networks. This could be strengthened by exploring other HE-friendly activation strategies from the literature  (e.g., binary-tree ReLU implementation in CKKS [1], a fuller investigation on Chebyshev polynomials [2], etc.), which often provide a better accuracy-depth trade-off.\n    * The justification for avoiding bootstrapping (due to cost) seems to be based on older benchmarks. Could the authors comment on why more recent, efficient bootstrapping methods (which can handle exact ReLU) were not considered as an alternative to replacing the activation function? (e.g., with TFHE programmable bootstrap [3] or somewhat new CKKS functional boostrapping [4]).\n    * The claim of \"no degradation\" 3 is supported for the 1- and 2-layer heads, but the $x^2$ activation is known to introduce accuracy challenges in deeper networks. This raises questions about scalability. A study on how RHE's accuracy and performance are impacted by more complex, deeper head-NNs would significantly strengthen the paper's claims of practical utility, similar to [5].\n\n* The performance evaluation in Table 3 would be more impactful if contextualized with other privacy-preserving frameworks.\n    * Currently, the table only shows the overhead of RHE against its own unencrypted baseline. This makes it difficult to interpret the practical efficiency of the proposal relative to the state-of-the-art in private inference.\n    * For instance, while the authors correctly note in Appendix A.2 that the use case for HETAL [6] is different, including its performance as a point of reference would help the reader gauge the computational cost of RHE. Could the authors provide such a comparison or benchmark against other relevant Privacy-Preserving Inference (PPI) frameworks?\n\n* The paper's motivation hinges on attacks in the few-shot regime, which are demonstrated on dataset sizes of $N=10$ and $N=40$.\n    * It would be helpful for the reader to understand how representative these small $N$ values are of real-world, sensitive transfer learning applications.\n    * The authors rightly note in the limitations (Appendix B) that the attack's effectiveness decreases as $N$ increases. This prompts a key question: at what approximate dataset size $N$ does this specific attack vector become unreliable, and DP-SGD (which the paper critiques) become the more practical defense?\n\n* Section 3, which details the novel attack, is quite dense. It synthesizes several complex components, including shadow models (Algorithm 1), a bespoke reconstructor NN architecture, the softmin loss function , and black-box weight extraction. The paper's clarity would be significantly enhanced by including a high-level diagram or summary box that illustrates the end-to-end pipeline of the attack.\n\n* A minor point on terminology: the name \"Reverse Homomorphic Encryption\" may be somewhat confusing for readers (especially from the cryptography community). The innovation appears to be in the application (encrypting weights rather than inputs) rather than a new cryptographic \"reverse\" primitive. This is a clever form of partial model encryption, which has been previously explored [7], and perhaps a more descriptive name (\"Weight-Space HE\" or similar) might aid clarity.\n\nReferences: \\\n[1] Eunsang Lee et al., ”Low-complexity deep convolutional neural networks on fully homomorphic encryption using multiplexed parallel convolutions”. *International Conference on Machine Learning*, 2022. \\\n[2] L. Rovida and A. Leporati, “Encrypted Image Classification with Low Memory Footprint Using Fully Homomorphic Encryption”. *International Journal of Neural Systems*. 2024 \\\n[3] Ilaria Chillotti, Marc Joye, and Pascal Paillier. “Programmable bootstrapping enables efficient homomorphic inference of deep neural networks.” *Cyber Security Cryptography and Machine Learning*, 2021. \\\n[4] Alexandru, A., Kim, A., Polyakov, Y. \"General Functional Bootstrapping Using CKKS\". *Advances in Cryptology*, 2025 \\\n[5] Njungle, Nges Brian, and Michel A. Kinsy. \"Activate me!: Designing efficient activation functions for privacy-preserving machine learning with fully homomorphic encryption.\" *International Conference on Cryptology in Africa*. 2025. \\\n[6] Lee, Seewoo, et al. \"HETAL: Efficient privacy-preserving transfer learning with homomorphic encryption.\" *International Conference on Machine Learning*. 2023 \\\n[7] Dongwoo Kim and Cyril Guyot. “Optimized privacy-preserving cnn inference with fully homomorphic encryption”. *IEEE Transactions on Information Forensics and Security*, 2023"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WwXdtDt24m", "forum": "nczgZiF4XR", "replyto": "nczgZiF4XR", "signatures": ["ICLR.cc/2026/Conference/Submission3143/Reviewer_YfZy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3143/Reviewer_YfZy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533114700, "cdate": 1761533114700, "tmdate": 1762916571637, "mdate": 1762916571637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper experimentally demonstrates that existing differential privacy defense methods are ineffective in small-sample transfer learning. It also proposes a defense mechanism based on Reverse Homomorphic Encryption (RHE), which prevents data reconstruction attacks without degrading classifier performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes effective white-box and black-box reconstruction attacks, which demonstrate stronger performance under realistic threat models compared to existing methods.\n2. The experiments validate the applicability of the proposed attack methods on different datasets, showcasing their effectiveness and robustness against existing defense methods, such as DP-SGD.\n3. The paper introduces the Reverse Homomorphic Encryption (RHE) mechanism, which effectively defends against data reconstruction attacks while providing high security without significantly degrading classifier performance."}, "weaknesses": {"value": "1. The paper primarily focuses on the few-shot transfer learning scenario, but both the abstract and introduction fail to provide a systematic description of the research background.\n2. The authors propose a novel reconstruction attack method, but there is a lack of detailed steps for the specific attack process. It appears to be a simple combination of several existing works, lacking novelty.\n3. The paper proposes using Reverse Homomorphic Encryption (RHE) to defend against reconstruction attacks, but it assumes that only the head neural network is fine-tuned during the transfer learning process. There is no discussion on the generalizability of this assumption in practical applications."}, "questions": {"value": "1. Provide a more systematic description of the research background. Does the proposed approach generalize to non-transfer learning or non–few-shot transfer learning scenarios?\n2. Provide a detailed description of the proposed reconstruction attack process, emphasizing the innovative aspects.\n3. Since the method assumes that only the head neural network is fine-tuned during transfer learning, clarify whether this assumption holds broadly in practical applications. How does the proposed approach perform compared to fine-tuning the entire model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ys1wmmieJJ", "forum": "nczgZiF4XR", "replyto": "nczgZiF4XR", "signatures": ["ICLR.cc/2026/Conference/Submission3143/Reviewer_dMgT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3143/Reviewer_dMgT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917794144, "cdate": 1761917794144, "tmdate": 1762916571082, "mdate": 1762916571082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerability of transfer-learned neural networks to training-data reconstruction attacks and proposes a novel encryption-based defense mechanism named Reverse Homomorphic Encryption (RHE). The authors first demonstrate that differentially private stochastic gradient descent (DP-SGD), the de facto standard for privacy-preserving training, fails to protect few-shot transfer learning (TL) models without severely degrading accuracy. They design sophisticated white-box and black-box reconstruction attacks that succeed under a realistic “weak adversary” threat model, recovering sensitive training data from TL classifiers trained on small per-class datasets. To counter these threats, the authors introduce RHE—a role-reversed form of homomorphic encryption that encrypts the transfer-learned weights rather than the input data, ensuring that only a trusted party with a private key can access decrypted outputs. Experiments on MNIST, CIFAR-10, and CelebA demonstrate that RHE blocks all reconstruction and inference attacks while maintaining model utility and providing efficient inference on commodity hardware."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  They introduce new and effective reconstruction attacks (both white-box and a novel hard-label black-box) specifically tailored to a realistic adversary model in the few-shot transfer learning setting.\n2.  The authors show that the de facto defense, DP-SGD, fails to mitigate these attacks without incurring a severe, unacceptable loss in classifier utility.\n3.  The paper proposes a highly effective defense mechanism: Reverse Homomorphic Encryption (RHE). The properties of this defense are particularly impressive, as the authors claim it provides a complete defense against a wide range of attacks (reconstruction, MIA, and property-inference). The authors show it incurs no degradation in classifier utility. Furthermore, they demonstrate that this HE-based approach is computationally practical, even without high-performance hardware.\n4.  The paper's claims are further strengthened by a principled, Neyman-Pearson-based scheme for evaluating reconstruction attacks. By rigorously accounting for both false positives and false negatives, this evaluation framework adds a layer of robustness and credibility to the attack results presented."}, "weaknesses": {"value": "Please refer to my questions below."}, "questions": {"value": "1\\. Questions Regarding the DP-SGD Baseline Comparison\n======================================================\n\nI note that the paper draws a very strong conclusion in Section 3.2 regarding the catastrophic utility loss of DP-SGD (e.g., >30 p.p. accuracy drop), which serves as a primary motivation for the RHE solution.\n\nMy questions are focused on the setup of this baseline comparison:\n\n   It is well-established that the utility of DP-SGD is exceptionally sensitive to its hyperparameter configuration, particularly the gradient clipping norm ($C$) and the noise multiplier.\n   The authors state that an \"optimal $C$\" was used, but the manuscript does not appear to present the detailed tuning experiments, ablation studies, or sensitivity analyses (e.g., plotting accuracy vs. $\\\\epsilon$ for different $C$ values) that would be needed to substantiate this claim of optimality.\n   More critically, the authors explicitly state in Appendix J that, \"For simplicity, in our experiments we have not performed the final adjustment of the learning rate\".\n\nI am left wondering whether the performance reported is truly the strongest baseline (best achievable utility) for DP-SGD under this privacy budget, or if it might reflect a sub-optimal parameter choice. Clarifying this is crucial for a fair assessment of RHE's comparative advantage. Could the authors please provide more experimental details to support this argument?\n\n2\\. Questions Regarding the Generalizability to Non-Frozen Paradigms\n====================================================================\n\nThe proposed black-box attack and the RHE defense both appear to ingeniously exploit a specific property of the \"frozen backbone\" transfer learning paradigm: that the base model remains static and all sensitive information is isolated within the Head-NN.\n\nThis raises significant questions about the generalizability of these methods to other widely-used, modern fine-tuning scenarios:\n\n1.  Regarding the Black-Box Attack: The attack's success (Section 3.1) hinges on the adversary's ability to locally compute the precise Head-NN input features ($f(X)$) using an identical, publicly available base model. However, if a victim employs Full Model Fine-tuning or Parameter-Efficient Fine-Tuning (PEFT) techniques (e.g., LoRA), the base model's parameters are modified by the sensitive data. Would this not break the identity between the victim's and attacker's base models, thus invalidating the attacker's local computation of $f(X)$? Could the authors comment on whether the entire black-box attack chain would fundamentally fail in such scenarios?\n    \n2.  Regarding the RHE Defense: The security model of RHE appears to be based entirely on encrypting the Head-NN. In PEFT paradigms, however, the influence of sensitive data \"leaks\" into and is stored within unencrypted components of the base model (e.g., the LoRA weights). Would the security guarantees of RHE still hold in this case?\n    \n\nCould the authors please clarify whether the proposed attack and defense methodologies are inherently limited to the \"frozen backbone\" transfer learning paradigm? Or, do they (and if so, how) have the potential to be generalized to these other, more mainstream fine-tuning techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gcZHHGD2Bm", "forum": "nczgZiF4XR", "replyto": "nczgZiF4XR", "signatures": ["ICLR.cc/2026/Conference/Submission3143/Reviewer_HBNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3143/Reviewer_HBNF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934510926, "cdate": 1761934510926, "tmdate": 1762916570798, "mdate": 1762916570798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies training-data reconstruction for few-shot transfer learning (TL). It builds strong white-box and hard-label black-box reconstruction attacks, shows that defending with DP-SGD in the few-shot regime requires very noisy settings that hurt accuracy, and then proposes “Reverse Homomorphic Encryption” (RHE): keep inputs and the frozen base model in the clear, but homomorphically encrypt the TL head’s weights (and logits), claiming this blocks reconstruction/MIA/property-inference while remaining practical for shallow heads."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Attack contribution & evaluation. New hard-label black-box path (weight extraction → reconstructor) and a cleaner Neyman–Pearson ROC criterion for quantifying reconstruction at low FPR; results show sizable TPR at 1% FPR in few-shot TL.\n\nPragmatic defense idea. RHE is conceptually simple (encrypt the TL head, not the inputs), and the prototype reports per-prediction latency that’s in the tens to hundreds of ms for small heads—reasonable for many TL scenarios."}, "weaknesses": {"value": "Reading the title \"Securing Transfer-Learned Networks with Reverse Homomorphic Encryption\", I assume the paper is about a defense. However, most pages develop and measure the attack and the DP-SGD tradeoff. RHE is introduced later with a design sketch, implementation notes (CKKS/TenSEAL), and timing/accuracy tables, but there is no head-to-head evaluation demonstrating the attack failing under RHE (the defense claim is largely by construction: encrypt weights/logits ⇒ attacks can’t run). The defense section is comparatively brief (Sections 4–4.2) and focuses on mechanics/latency rather than a formal security proof or empirical “we tried our own attack and it cannot proceed.”\n\nBased on this weakness, I cannot say a sound experimental support for your claimed contribution 4, 5. \n\nMeanwhile, I think the paper concentrate to a trivial problem, the reconstruction threats. I think the author should reorganize the paper and focus on the RHE. Cause HE can not only protect the data from reconstruction attacks, but some other attacks, like model stealing attack, or can be used to watermark the model, etc. Why you focus on only one specific attack but not your protection? For me, it's like missing the forest for the trees. I suggest the author reorganize the paper following the title - focus on the protection and validate the protection. There are many content in appendix should be in the main pages."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "stM2Knafri", "forum": "nczgZiF4XR", "replyto": "nczgZiF4XR", "signatures": ["ICLR.cc/2026/Conference/Submission3143/Reviewer_KKYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3143/Reviewer_KKYk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955917285, "cdate": 1761955917285, "tmdate": 1762916570534, "mdate": 1762916570534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}