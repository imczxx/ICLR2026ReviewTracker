{"id": "F63ARIi43t", "number": 21459, "cdate": 1758317815267, "mdate": 1759896920990, "content": {"title": "rbio1 - training scientific reasoning LLMs with biological world models as soft verifiers", "abstract": "Reasoning models are typically trained against verification mechanisms in formally specified systems such as code or symbolic math. In open domains like biology, however, we lack exact rules to enable large-scale formal verification and instead often rely on lab experiments to test predictions. Such experiments are slow, costly, and cannot scale with computation. In this work, we show that world models of biology or other prior knowledge can serve as approximate oracles for soft verification, allowing reasoning systems to be trained without additional experimental data. We present two paradigms of training models with approximate verifiers: RLEMF:  reinforcement learning with experimental model feedback and RLPK: reinforcement learning from prior knowledge. Using these paradigms, we introduce rbio1, a reasoning model for biology post-trained from a pretrained LLM with reinforcement learning, using learned biological models for verification during training. We demonstrate that soft verification can distill biological world models into rbio1, enabling it to achieve state-of-the-art performance on perturbation prediction in the PerturbQA benchmark. We present rbio1 as a proof of concept that predictions from biological models can train powerful reasoning systems using simulations rather than experimental data, offering a new paradigm for model training.", "tldr": "Training scientific reasoning LLMs with biological world models as soft verifiers", "keywords": ["reasoning models", "virtual cell models", "transcriptomics", "scientific reasoning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e02ef3e5fd72772e0338c772c307ae7a6d91a7c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces rbio1, a reasoning model for biology trained using reinforcement learning with soft verification as a training signal for scientific reasoning LLMs in domains (such as biology) where ground-truth, executable verifiers are scarce. The authors propose RLEMF (reinforcement learning with experimental model feedback) and RLPK (reinforcement learning from prior knowledge) using curated databases such as Gene Ontology. Empirically, the approach is evaluated on perturbation prediction tasks from the PerturbQA benchmark, demonstrating that rbio1 surpasses the current state-of-the-art metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The paper presents a novel and highly interesting approach to training scientific reasoning models. It proposes methods for training reasoning in scientific domains that lack formal verifiers. The method is relatively convincing and promising, both practically and empirically. The distinction between RLEMF and RLPK provides a well-structured framework for different types of soft verification. Empirically, the model outperforms current baselines, and the leave-one-out evaluation across cell lines provides good evidence of out-of-distribution generalisation."}, "weaknesses": {"value": "1. There are grammatical errors and typos in the paper. I would suggest rereading it carefully. For example:\n– “presemt” → “present” on line 19\n– “QWEN Team” → “Qwen Team” on line 45\n\n2. While the methodology seems promising, the computational resources required are not reported in the paper. Is the training generally computationally expensive?\n\n3. The paper does not thoroughly analyse what the soft verifiers are learning, or how their prediction quality affects downstream reasoning performance. It would be helpful to elaborate further on this.\n\n4. While the baselines are comprehensive, the paper could include stronger reasoning baselines (e.g., other RL-trained reasoning models using the same backbone)."}, "questions": {"value": "1. GEARS performs surprisingly poorly (F1 = 0.296), which seems inconsistent with published results. Is there any explanation or analysis regarding this?\n\n2. Could you elaborate more on the hyperparameters, computational requirements, seeds, and other details of the training process?\n\n3. How well does the approach generalise? For instance, have you tested it on other perturbation datasets beyond PerturbQA?\n\n4. Could you provide a more complete learning curve showing how performance scales from 1/15 to the full dataset?\n\n5. What are the limitations and failure cases? Could you provide some examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Uijmh2htK0", "forum": "F63ARIi43t", "replyto": "F63ARIi43t", "signatures": ["ICLR.cc/2026/Conference/Submission21459/Reviewer_AHen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21459/Reviewer_AHen"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312746735, "cdate": 1761312746735, "tmdate": 1762941789062, "mdate": 1762941789062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "&nbsp;\n\nThe authors introduce rbio1, a suite of reasoning models for molecular biology. The main contribution of the work is to demonstrate that soft verification techniques, using either simulations, prior knowledge, or knowledge sources such as gene ontologies, enable improved OOD performance of large reasoning models. While the novelty and impact of the current work are exceptional, the paper is very well written and presented, and the empirical results appear convincing, I have some concerns regarding the reproducibility of the results given that the code is not released. If this issue is addressed (e.g. if an anonymous GitHub link or similar is provided - **the AC can confirm whether this is admissible under the ICLR guidelines**) in the rebuttal I will raise my score and champion the paper for acceptance.\n\n&nbsp;"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "&nbsp;\n\nThe main contribution of the current work is that it introduces a suite of techniques for making use of easily accessible data to improve the performance of large reasoning models for molecular biology. The empirical results are impressive, with rbio1, the authors' model, achieving state-of-the-art performance on the PerturbQA benchmark. It should be emphasized that the potential impact of the work is much greater than the applications considered solely in the paper since in principle the techniques should apply to any biological domain where simulations, prior knowledge, and ontologies are available as soft verifiers.\n\n&nbsp;"}, "weaknesses": {"value": "&nbsp;\n\nBelow, I demarcate between major and minor issues I see with the paper in its current form. So that the authors can prioritize their time for their rebuttal, it is the release of the codebase that will lead me to increase my score.\n\n&nbsp;\n\n**__MAJOR POINTS__**\n\n&nbsp;\n\n1. **Scope**: Is rbio1 a descriptive name if the authors' model just considers molecular biology? Could the authors give some indication as to the nomenclature of forthcoming models that focus on other biological domains?\n\n2. **Reproducibility**: I have concerns about the reproducibility of the work given that the code for the paper is not released. As an example reproducibility issue, the architecture of the MLP described in Section 4.1 is not fully defined. What are the activation functions?\n\n3. **Validity of Claims**: It is not clear what is meant by the claim of state-of-the-art performance in Figure 4 when the models are trained with 1/5 of the data sample. SUMMER appears to perform better on the TNR metric? Could the authors add their interpretation as to which metrics are the most relevant for PerturbQA? Additionally, using 1/5 of the data sample as in Figure 4, the difference in F1-score and the MCC metric does not appear to be statistically significant?\n\n&nbsp;\n\n**__MINOR POINTS__**\n\n&nbsp;\n\n1. There are missing capitalizations in the references e.g. \"rna\" in place of \"RNA\", \"Grpo\" in place of \"GRPO\", \"llms\" in place of \"LLMs\".\n\n2. Typo line 20, \"present\".\n\n3. In the abstract, line 26, \"rbio1\" is not bolded whereas above it is bolded twice.\n\n4. Throughout the paper the authors appear to be using narrative citations e.g. Abramson et al. (2024) in place of parenthetical citations e.g. (Abramson et al. 2024). The latter should be used where the author's name does not comprise part of the sentence.\n\n5. Line 64, typo, \"(VCMs)\".\n\n6. Line 76, the stated motivations are not very concrete e.g. what does it mean to \"aggregate diverse models of biology into a universal space\"?\n\n7. Line 95, typo, \"rbio-1\".\n\n8. Figure 1, typo, \"The answer is either yes or not\" presumably.\n\n9. In Figure 1(b) it is not clear what the double helix above the arrow is meant to signify?\n\n10. In the related work when discussing inference time scaling, it would also be worth mentioning [1].\n\n11. Line 147, typo, \"Wu et al.\" given twice.\n\n12. Line 157, typo, \"the model\".\n\n13. Missing full stop at the end of Equation 5.\n\n14. The explanation of soft verification at the beginning of Section 3 is easily understandable by a layreader. Perhaps it would be worth including such a motivating example to the introduction so that the reader may more easily grasp the concepet of a soft verifier in a biology context?\n\n15. For clarity line 187/188 should read, \"$G$ a set of outputs generated during training by the reasoning LLM $\\pi_{\\theta}$ in response to $q$\" or similar.\n\n16. When citing GRPO, the source paper [2] should be referenced in place of the current citation.\n\n17. lowercase \"exp\", $D_{exp}$, may be a less harsh notation for the experimental dataset $D_{EXP}$.\n\n18. Missing full stop at the end of Equation 8.\n\n19. The originating paper for the ROUGE score [3] should be cited given that it is used.\n\n20. On line 263, what is the function $X$?\n\n21. Missing full stop at the end of Equation 12.\n\n22. In Equation 12, how is $LL$ distinct from $LL_{\\pi_{\\theta}}$?\n\n23. In Equation 14, what are $z_{max}$, $z_{min}$, and $s_{min}$?\n\n24. Wu et al. 2025 was accepted at ICLR 2025.\n\n25. For Figure 2(e) it would help if the same colors were used for each method relative to Figure 2(d).\n\n26. In Figure 3, it would be helpful to add how the errorbars are computed to the caption. \n\n27. The authors should cite the originating paper for chain-of-thought [4] given that it is featured in the authors' experiments or more accurately, it appears as though the authors are using zero-shot chain-of-thought originating in [5]. If indeed, the authors are not using few-shot examples, it may be more correct to use the terminology, \"zero-shot chain-of-thought\" in place of \"chain-of-thought\".\n\n&nbsp;\n\n**__REFERENCES__**\n\n&nbsp;\n\n[1] Muennighoff, N., Yang, Z., Shi, W., Li, X.L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E. and Hashimoto, T., 2025. [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393). arXiv preprint arXiv:2501.19393.\n\n[2] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y.K., Wu, Y. and Guo, D., 2024. [DeepSeekMath: Pushing the limits of mathematical reasoning in open language models](https://arxiv.org/abs/2402.03300). arXiv preprint arXiv:2402.03300.\n\n[3] Lin, C.Y., 2004, July. [ROUGE: A package for automatic evaluation of summaries](https://aclanthology.org/W04-1013.pdf). In Text summarization branches out (pp. 74-81).\n\n[4] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., 2022. [Chain-of-thought prompting elicits reasoning in large language models](https://proceedings.neurips.cc/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html). Advances in Neural Information Processing Systems, 35, pp.24824-24837.\n\n[5] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2022. [Large language models are zero-shot reasoners](https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html). Advances in Neural Information Processing Systems, 35, pp.22199-22213.\n\n&nbsp;"}, "questions": {"value": "&nbsp;\n\n1. In Equation 11, the Keyword score (KWS) is given as the intersection over the magnitude of $s_1$. The reason for not using intersection over union is presumably because one only cares about the length of the reasoning trace and not the length of the ontology?\n\n2. For Figure 2(d) the authors first mention that the metrics are aggregated over cell line and second, mention that the metrics are averaged over 5 runs. How are the errorbars computed in this case?\n\n3. In the context of Figure 5, could the authors explain how the outputs of the model are actually scored? Despite being prompted to output a binary answer, all completions appear not to have provided a definitive answer? \n\n&nbsp;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "&nbsp;\n\nNo ethical concerns.\n\n&nbsp;"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WzM5kYjljI", "forum": "F63ARIi43t", "replyto": "F63ARIi43t", "signatures": ["ICLR.cc/2026/Conference/Submission21459/Reviewer_tnsY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21459/Reviewer_tnsY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865761226, "cdate": 1761865761226, "tmdate": 1762941788557, "mdate": 1762941788557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes reinforcement learning on LLMs from soft reward signals for scientific question answering / prediction tasks, specifically in the case of biological perturbation (effects of gene downregulation on the expression of other genes), a knowledge-intensive, multi-hop reasoning task.\nThe considered reward signals are: results of experimental interventions, predictions by a specialized tiny MLP model on the same task, similarity scores from the information retrieval literature."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a relevant problem in reinforcement learning with LLMs: moving beyond “hard” rewards. This is useful in science, business and everyday life.\n- The evaluation scores on the PertubQA benchmark are comparable to the current SOTA (SUMMER).\n- Experiments on mixes of training rewards could be useful beyond the domain of perturbation prediction."}, "weaknesses": {"value": "- Despite claims to the contrary, the method of similarity scores does not directly translate to other domains: it is only useful for predicting binary connectedness labels.\n- The exposition needs to be improved strongly: the biological question and setup is not explained, making evaluation of the claims difficult. Examples: Do all cell lines have the same genes? Do the same genes behave similarly across cell lines? How realistic is the scenario of training on one cell line and evaluating on it? …\n- Key details are not included in the paper: how was the tiny MLP trained (input / output / loss?) whose “soft RL distillation” led to strong benchmark results? What are the numbers of the MLP on the task? Could supervised training like probably used for the MLP also be used for the LLM?\n- Equations are often nonsensical / unhelpful / ill-defined. The explanation of the method should be reworked with fewer equations that do not say much and clearer explanations instead. The paper cannot be reimplemented based on the given explanations. Examples: Sect. 3.2 is a complicated way of saying “reward by accuracy with respect to ground truth label”. The notation with vertical “conditioning” bars does not make sense. Eq. 10 introduces nothing but an alias but doesn’t say what the alias actually means."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4tJYUaaZA2", "forum": "F63ARIi43t", "replyto": "F63ARIi43t", "signatures": ["ICLR.cc/2026/Conference/Submission21459/Reviewer_jVCL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21459/Reviewer_jVCL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958590217, "cdate": 1761958590217, "tmdate": 1762941788009, "mdate": 1762941788009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes rbio1, a post-trained biology reasoning LLM that combines hard experimental supervision (when available) with two biology-derived soft verifiers: (i) RL with experimental-model feedback (RLEMF) and (ii) RL from prior knowledge (RLPK, e.g., GO), within a GRPO-style objective. The paper shows that composing these signals improves performance, and that on PERTURBQA the soft-supervised models approach experimental supervision, and that a mixed soft+experimental model with CoT can surpass SUMMER using only 1/15 of the data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important gap as existing RL/RLVR setups for LLMs assume exact or high-fidelity verifiers, which biology typically lacks. Using biology models and curated knowledge as approximate oracles is a practical way to obtain training-time signals in this domain.\n\n- Instantiates multiple biology-grounded rewards (experimental, MLP surrogate, GO-based) within a single RL objective rather than relying on one weak heuristic.\n\n- Shows that the ordering and composition of verifiers affect final performance, indicating that the supervision source contributes meaningfully rather than merely regularizing.\n\n- CoT at inference provides a clear empirical boost and allows the model to outperform SUMMER with substantially less data."}, "weaknesses": {"value": "- The approach assumes biology models and GO signals are accurate enough to guide RL, but the paper does not analyze sensitivity to verifier fidelity or miscalibration (e.g., if the MLP is slightly wrong or GO retrieval is noisy). A study relating reward-model quality to final task accuracy is missing, despite the signals being described as \"approximate oracles\".\n\n- All experiments are within PERTURBQA (four cell lines, perturbation prediction). As written, results mainly show that soft verifiers help when drawn from the same distribution as the target task (i.e., the evaluation is tightly coupled to the training/reward distribution). Even a smaller secondary biology reasoning task would make the generality claim more credible.\n\n- All improvements are shown for one base model (Qwen2.5-3B) fine-tuned with biology-specific rewards. It is unclear whether generic RL or RLAIF signals (format, helpfulness, self-consistency) would yield similar gains. This weakens the claim that the biology-specific rewards are the true driver of improvement.\n\n- The paper claims the method can extend to any queryable biology or virtual-cell model, limited only by available queries, but experiments use only a small MLP surrogate and GO (both close to the target task) and do not test harder or OOD biological models. The empirical evidence does not yet support the scope of the claim.\n\n- There is no analysis of whether RL amplifies verifier errors, no plot of verifier vs model accuracy on held-out perturbations, and no check of verifier disagreement. This omission makes it difficult to judge robustness in realistic noisy-biology settings."}, "questions": {"value": "- How sensitive is the method to errors in the MLP/experimental-model verifier or noisy GO retrieval? Did the authors measure reward–accuracy correlation anywhere?\n\n- The authors report that ordering/composition of verifiers affects the final model — can the authors clarify whether this is due to different reward scales or truly different biology signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I1vJFR8W5S", "forum": "F63ARIi43t", "replyto": "F63ARIi43t", "signatures": ["ICLR.cc/2026/Conference/Submission21459/Reviewer_cju1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21459/Reviewer_cju1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966959663, "cdate": 1761966959663, "tmdate": 1762941787445, "mdate": 1762941787445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}