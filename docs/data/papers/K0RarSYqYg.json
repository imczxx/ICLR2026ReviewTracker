{"id": "K0RarSYqYg", "number": 3673, "cdate": 1757495057212, "mdate": 1763115419210, "content": {"title": "EmbodiedGS: Reconstruct Unified Embodied Representation from RGB Stream", "abstract": "This paper addresses the challenge of incrementally reconstructing object-centric 3D representations from only a pose-free RGB video stream. Existing dense SLAM methods face a dual challenge: they are constrained by a reliance on precise camera poses and RGB-D input for initialization, and they lack precise instance-level scene understanding. Moreover, the quality of their reconstruction and perception is fragile to systematic errors.\nTo this end, we propose EmbodiedGS, a pipeline that jointly performs incremental 3D reconstruction and perception from RGB stream to constructs an Object-Centric 3D Gaussians (OCGS) representation that is both geometrically accurate and rich with instance-level information.\nSpecifically, our approach leverages MASt3R-SLAM for Gaussian geometric initialization and  introduces a Global-Associated Instance Memory (GAIM) to consistently track objects across views using multi-modal cues. We then construct the initial OCGS by lifting instance information to 3D Gaussians via optimizable binary embeddings. Finally, this representation is refined through a joint optimization process that leverages the synergy between reconstruction and perception to mutually correct inaccuracies, yielding a robust, high-fidelity OCGS.\nExtensive experiments are conducted on TUM-RGBD and ScanNet datasets and a real-world robotic platform, where EmbodiedGS demonstrates competitive performance even compared with RGB-D SLAM methods and offline 3D instance segmentation methods.", "tldr": "", "keywords": ["3D Reconstruction", "3D Instance Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/10514defadd9e53ce5a1005c283f3649c57cb8dd.pdf", "supplementary_material": "/attachment/21bb86898a273dc2fd05edc8b98fce93f16b1e98.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces EmbodiedGS, an object-centric online reconstruction and perception framework based on RGB video inputs. The methods combine SOTA SLAM with a new instance association mechanism and propose a novel reconstruction-perception joint optimization process to enhance the geometry and perception results. Extensive experimental results demonstrate the superiority of the proposed EmbodiedGS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduce a dedicated object-centric Gaussian Representation and reconstruction-perception joint optimization process strategy to improve reconstruction and perception performance in various scenes. The designs are well-supported by experimental results. The methods used are appropriate for the task, and the authors provide a thorough evaluation of the effectiveness of the proposed method."}, "weaknesses": {"value": "1.What is the difference between the setting of the proposed method and semantic SLAM, just use or not use depth? The experimental setting is also same with semantic slam, this is no need to re-name this task?\n\n2.The authors do not provide a thorough evaluation of the effectiveness of the proposed method. What are the instance association mechanism and bidirectional optimization spereate influence on perception and reconstruction. Additionally, there is no clear results analysis on how bidirectional optimization improve perception and reconstruction.\n\n3.The novelty of instance association mechanism is limited and there are lots of hyper-parameters to be tuned, resulting in instable performance in different scenes.\n\n4.The explanation of bidirectional optimization is unclear. How exactly reconstruction and perception optimize each other?"}, "questions": {"value": "1.It seems that instance embedding is not used after optimized with Eq. 7, how the instance embedding affect the reconstruction and perception results?\n\n2.How the \\hat{M_m(p)} is obtained, how b() maps a mask to d-bit binary encoding?\n\n3.How Eq.7 and Eq.8 optimize each other? It seems they all try to make instance embedding/Gaussian align to 3d mask, reducing intra-object spatial variance.\n\n4.There is no embodied related experiments such as navigation, manipulation, the title “EmbodiedGS” is not suitable for this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r0nQCOYKnV", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Reviewer_bCqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3673/Reviewer_bCqj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646886559, "cdate": 1761646886559, "tmdate": 1762916912825, "mdate": 1762916912825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "J5fYD4d62g", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763115418443, "cdate": 1763115418443, "tmdate": 1763115418443, "mdate": 1763115418443, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method reconstructs object segmentation from an unposed RGB stream. Given an RGB video, it employs MASt3R-SLAM to compute camera poses and depth maps, and utilizes YOLO-E for segmentation prediction. The segmentation results are then projected onto the depth maps and used to train a Gaussian Splatting model. The visualizations demonstrate promising results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is easy to follow.\n2. The visualization looks good."}, "weaknesses": {"value": "1. The computation of the bounding box is not mentioned in the paper.\n2. In Line 253, although the ablation study shows that visual similarity contributes to the result to some extent, there are no explanations or experiments to validate the multi-view feature descriptor strategy. Previous literature indicates that feature descriptors can vary significantly across different views.\n3. In Line 279, the term \"Binary Instance Embedding\" (BIE) is questionable. It is unsurprising that BIE works in a submap Gaussian Splatting system, as it appears to overfit, similar to an image representation, and fails in global coordinates. The primary reason is that Gaussian representations are sensitive and prone to overfitting the given observations. Even if we render RGB images from the Gaussian Splatting system from limited views, it may not generalize well to novel views with slightly larger baselines. Given the inherent discreteness of binary representations, I have strong doubts about the performance of Equation 6. Additionally, upon reviewing the figures, the authors only provide visualizations of instance IDs on point clouds.\n4. The overall technical contribution of this paper is limited. It seems more suitable for a robotics conference rather than ICLR, because there are tons of paper with similar instance fusing strategies published on iros or icra.\n5. The authors report numerous results on rendering quality and runtime. However, the PSNR and runtime metrics are fully provided by the Mast3R-SLAM framework, and based on Equations 7 and 8, the author's contribution to these aspects appears minimal.\n9. Some baselines are missing, for example, BoxFusion [1].\n\n[1] BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion"}, "questions": {"value": "1. Based on the paper, the yolo-e only used to extract the mask(line180). Why the visual descriptor is extracted from mast3r but not yoloe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dUf8U8XAex", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Reviewer_Kr1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3673/Reviewer_Kr1V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819976653, "cdate": 1761819976653, "tmdate": 1762916911304, "mdate": 1762916911304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper authors presented an efficient pipeline for online reconstruction from RGB video streams. Sufficient experiments are shown  on real-world datasets demonstrate the proposed method effectively constructs high-fidelity 3D, achieving superior performance in both segmentation and rendering quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of Global-Associated Instance Memory (GAIM) by incrementally associating instance proposals across views with multi-modal matching criteria to ensure global consistency is the key highlight. Additionally key strength is in joint optimization approach of the OCGS by leveraging the synergy between perception and reconstruction, and simultaneously refining Gaussian geometry and instance embedding to produce a high-fidelity representation."}, "weaknesses": {"value": "- experimental could have been better. Currently, all scenes considered are static, what about those scenarios where there are moving objects.\n- How varying light conditions and texture vs texture less environment will affect the overall performance of the system. \n- An analysis on time taken for inference is missing. A section talking about time profiling will help understand the utility of this system for real world applications"}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D9USUwAqsy", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Reviewer_Gje1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3673/Reviewer_Gje1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974219978, "cdate": 1761974219978, "tmdate": 1762916911000, "mdate": 1762916911000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper authors presented an efficient pipeline for online reconstruction from RGB video streams. Sufficient experiments are shown  on real-world datasets demonstrate the proposed method effectively constructs high-fidelity 3D, achieving superior performance in both segmentation and rendering quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of Global-Associated Instance Memory (GAIM) by incrementally associating instance proposals across views with multi-modal matching criteria to ensure global consistency is the key highlight. Additionally key strength is in joint optimization approach of the OCGS by leveraging the synergy between perception and reconstruction, and simultaneously refining Gaussian geometry and instance embedding to produce a high-fidelity representation."}, "weaknesses": {"value": "- experimental could have been better. Currently, all scenes considered are static, what about those scenarios where there are moving objects.\n- How varying light conditions and texture vs texture less environment will affect the overall performance of the system. \n- An analysis on time taken for inference is missing. A section talking about time profiling will help understand the utility of this system for real world applications"}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D9USUwAqsy", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Reviewer_Gje1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3673/Reviewer_Gje1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974219978, "cdate": 1761974219978, "tmdate": 1763099603231, "mdate": 1763099603231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a pipeline for incremental object-centric 3D Gaussian reconstruction from monocular RGB streams. Specifically, it combines MASt3R-SLAM for initialization with a GAIM for cross-view tracking, and introduces a joint optimization that refines OCGS. The experiments validate its effectiveness on several datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The pipeline integrates geometry, instance tracking, and rendering within a unified Gaussian-splatting framework.\n* The proposed pipeline is validated on TUM-RGBD and ScanNet, as well as real-world demonstrations. \n* The bidirectional optimization (R4P/P4R) between reconstruction and perception is conceptually interesting.\n* The paper is clear in general and easy to follow."}, "weaknesses": {"value": "* The method essentially replaces the geometric backbone of prior object-centric SLAM systems with MASt3R-SLAM and integrates standard 2D object tracking (YOLO-E + mask association). Similar systems have appeared in semantic/instance-aware SLAM  works (e.g., SGS-SLAM, SemGauss-SLAM). Acutally, the idea of utilizing 2D segmentation to improve VO/SLAM is quite old and standard in both CV and robotics communities, like VSO [Lianos et al. 2018] and its following works. \n* The framework relies heavily on 2D instance segmentation quality. The robustness of the system depends on YOLO-E’s accuracy. In cluttered or textureless scenes with few distinct objects, GAIM would likely fail, and the pipeline may degrade to vanilla MASt3R-SLAM performance.\n* Since MASt3R-SLAM itself still suffers from pose drift and unstable geometry, errors will propagate through the entire system. The “joint optimization” seems unable to fully correct such foundational errors in principle since it also highly depends on the initialization.\n* The multi-branch architecture (GAIM + submap + joint optimization) substantially increases system complexity and computational overhead. The performance gain may not justify the added complexity for practical deployment. Actually, as for the real application for robotics, the system efficiency is of great importance. \n* Table 5 and some other ablations do not specify on which dataset or sequence they were performed. Moreover, the reported improvements are marginal."}, "questions": {"value": "Please refer to the weaknesses in the weaknesses part. I am open to being persuaded based on the feedback from the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ToVmoMguiV", "forum": "K0RarSYqYg", "replyto": "K0RarSYqYg", "signatures": ["ICLR.cc/2026/Conference/Submission3673/Reviewer_TtH8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3673/Reviewer_TtH8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979758656, "cdate": 1761979758656, "tmdate": 1762916910459, "mdate": 1762916910459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}