{"id": "OEyPLlIrtd", "number": 3687, "cdate": 1757497015902, "mdate": 1759898074974, "content": {"title": "Visual Geometry Transformer in the Wild: Distractor-Free 3D Reconstruction", "abstract": "Current end-to-end multi-view 3D reconstruction methods achieve impressive results, but are built on a restrictive assumption: the scene is entirely static with dense correspondence.\nThis reliance on idealized inputs causes even the most advanced methods to fail in real-world settings, where transient distractors and occlusions present. To address this, we propose \\emph{Visual Geometry Transformer in the Wild} (VGTW), an end-to-end framework for robust reconstruction from inconsistent views. At its core, we isolate and suppress distractor-affected regions while preserving the consistent components across views. Specifically, we introduce a distractor-aware training strategy that separates clean features from distractor-contaminated ones in the attention mechanism while enforcing feature consistency across images. To enable this, we train the model with an auxiliary mask prediction head, using supervision from a new dataset we collected with pixel-level distractor masks. The resulting VGTW model is a feed-forward network that directly outputs clean, distractor-free point clouds. Remarkably, it requires no additional 3D supervision, remains computationally efficient, and is compatible with existing pipelines.\nExtensive experiments validate our approach, demonstrating state-of-the-art performance and robust generalization in diverse, real-world scenarios.", "tldr": "", "keywords": ["3D reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f49fc28a4f6f2f19f5d20ec4646b199bc39d00.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents *VGTW (Visual Geometry Transformer in the Wild)*, a feed-forward transformer-based pipeline that aims to produce distractor-free 3D reconstructions from multi-view images. The authors fine-tune existing feed-forward 3D models (VGGT, π3) with LoRA and introduce two losses (Distractor Suppression and Cross-View Consistency) plus a distractor mask prediction head. They claim improved robustness to transient distractors and strong results on NeRF-on-the-go and RobustNeRF.\n\nWhile the paper has some strengths, such as insightful analysis of attention leaking to distractors, intuitive loss formulations, there are several concerns. The claims about prior feed-forward methods appear overstated, discussion about several highly related work is missing, and the evaluation is limited and sometimes unclear.\n\nOverall, I would recommend reject as the contribution is incremental, the evaluation has gaps, and some claims are overstated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Insightful analysis of attention and loss design.** The observation that attention leaks to distractors is both insightful and clearly illustrated in Figure. The design of the two losses (Distractor Suppression and Cross-View Consistency) demonstrates benefits for achieving distractor-free predictions.  \n\n2. **Clarity and presentation.** The paper is generally well-written, with clear figures and intuitive formulations for the proposed losses, and the methodology is easy to follow.  \n\n3. **Strong experimental results.** The method achieves improved metrics on the RobustNeRF and NeRF-on-the-go datasets, showing its effectiveness in handling scenes with distractors."}, "weaknesses": {"value": "1. **Overstated claim.** The paper repeatedly suggests that VGGT/π3 conceptually cannot handle dynamic scenes. This is not true, as existing feed-forward architectures are already designed to be robust to non-static inputs (especially π3). Therefore, the premise of the paper appears overstated.  \n\n2. **Missing related work.** The paper overlooks directly relevant studies that adapt DUSt3R-style frameworks for dynamic scenes, such as Monst3R [a] and Easi3R [b]. In particular, Easi3R discusses attention mechanisms for handling dynamic objects in reconstruction. This omission makes the contribution appear less novel.  \n\n3. **Limited and questionable evaluation.**  \n- Dataset mismatch: Both datasets were originally designed for \"distractor-free\" NeRF-based **novel view synthesis** rather than feed-forward reconstruction evaluation. The paper mentions that ground truth is \"generated using pretrained π³ on distractor-free images\" but does not clarify:  \n  a. During inference, how are \"distractor-free\" point clouds obtained, are they using confidence filtering?  \n  b. If so, what confidence threshold is used (0.5 in L358 in training section?), and is there a specific reason for choosing that threshold?  \n  c. The methods still output distractor point clouds without explicit occlusion area generation. Would simply removing the distractor regions  and comparing it with \"GT\" point cloulds be a fair evaluation setting?\n- Small scale: Only a few cases/scenes are used for evaluation, which seems insufficient for feed-forward methods that claim generalization.\n\n---\n\n[a] Zhang, J., Herrmann, C., Hur, J., Jampani, V., Darrell, T., Cole, F., Sun, D., & Yang, M.-H. (2025). *MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion*. In ICLR 2025.  \n[b] Chen, X., Chen, Y., Xiu, Y., Geiger, A., & Chen, A. (2025). *Easi3R: Estimating Disentangled Motion from DUSt3R Without Training*. In ICCV 2025."}, "questions": {"value": "1. The idea of generating distractor mask is interesting. Could the authors provide qualitative results for the mask? Additionally, I am interested in seeing how using only the mask head (without distractor-aware training, or just use the mask for pretrained π3 predictions) would affect the results.  \n\n2. Regarding training, could the authors specify the computational resources used (e.g., number of GPUs, total training time, and learning rate)? It also appears that the fine-tuning is performed without 3D supervision. Could the authors clarify how much this affects the 3D reconstruction performance?\n\n3. Could the authors explain the evaluation procedure in more detail, as mentioned in *Weakness 3*?  \n\n4. For *Tables 1–3*, the authors might consider adding average scores to facilitate easier comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SswCtfnMGB", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["ICLR.cc/2026/Conference/Submission3687/Reviewer_qMPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3687/Reviewer_qMPh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913390304, "cdate": 1761913390304, "tmdate": 1762916923742, "mdate": 1762916923742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "An additional discussion"}, "comment": {"value": "HI, thank you for the excellent work on VGGT! I would like to respectfully suggest that \"Distractor-free Generalizable 3D Gaussian Splatting\" (https://arxiv.org/abs/2411.17605) might be worth mentioning in Sec. 2.1, as this work similarly focuses on feed-forward distractor-free 3DGS reconstruction and was released last November. I hope this suggestion is helpful."}}, "id": "FaG7KN4Kg0", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["~Yanqi_Bao1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Yanqi_Bao1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762997226231, "cdate": 1762997226231, "tmdate": 1762997226231, "mdate": 1762997226231, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to improve the robustness of feed-forward 3D reconstruction models to view-inconsistent distractors such as moving objects and occluders. To this end, the authors first investigate the attention mechanism under distractors: given ground-truth distractor masks, they set the attention logits of the corresponding regions to negative infinity, which markedly improves existing models when distractors are present. Building on this observation, they introduce Distractor-Aware Training (DAT) that fine-tunes attention layers via LoRA and adds distractor-suppression and cross-view consistency losses. To demonstrate effectiveness, the authors annotate a new dataset with distractor masks, and the resulting models outperform strong baselines on real scenes with distractors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Originality:\n\n  * To the best of my knowledge, this paper is the first to systematically investigate distractors in attention for feed-forward 3D reconstruction and to propose a corresponding solution that directly addresses the identified failure mode.\n\n* Quality:\n\n  * The experiments are comprehensive, covering different types of distractors and demonstrating effectiveness across multiple strong baselines.\n\n* Clarity:\n\n  * The paper is well organized: it states assumptions, conducts oracle experiments with distractor masks to validate them, and then proposes a practical method informed by these findings.\n  * The method is clearly introduced with sufficient technical detail to understand and reproduce.\n\n* Significance:\n\n  * The problem addressed is complementary to existing feed-forward 3D reconstruction efforts; by improving robustness to distractors, the approach has the potential to become a general building block for this family of models.\n  * The annotated RobustNeRF-Mask dataset can further catalyze follow-up research on making feed-forward 3D reconstruction methods more robust to distractors."}, "weaknesses": {"value": "* The notion of “distractor” for attention needs a more precise quantitative definition. Unlike NeRF-style, per-scene optimization on a single sequence, this paper targets large-scale, feed-forward transformers with attention. In such settings, dynamic objects with continuous motion can still induce high cross-view correlation and be incorporated into reconstruction. Therefore, I would recommend to quantify distractor “lifetime” (number of frames or views), apparent speed (pixels/frame or m/s under known intrinsics/poses), and spatial extent, then analyze performance as these variables change. \n\n* From the qualitative results, improvements in static regions appear marginal. To make the gains more pronounced and measurable, I suggest: (1) also evaluating camera pose accuracy (ATE/RPE) to show benefits beyond point/depth; (2) reporting metrics on static-only regions by masking out distractor areas per frame, so improvements to clean content are not diluted by scene-wide averaging.\n\n* The robustness gains rely on additional supervision from annotated masks, but the current dataset scale is limited; generalization needs stronger support:\n\n  * Assess whether fine-tuning degrades performance on distractor-free data (report deltas on clean subsets).\n  * Evaluate generalization to “atypical” distractors absent or rare in training (hold-out categories, cross-domain scenes), and document any regressions.\n\n* It remains unclear why the distractor mask head is not explicitly injected into attention, akin to the “oracle” masking in Section 3. At present, correlation between distractor and static regions is reduced via feature-space objectives alone, which may struggle when distractors share similar appearance or texture with static content. Actionable: add an inference-time attention gating variant that consumes the predicted mask (soft or hard), compare against DAT-only, and analyze cases with look-alike distractors to test whether explicit gating closes the gap to the oracle."}, "questions": {"value": "- First, please see my weaknesses above.\n- Second, Figure 3 is confusing. Since the paper uses LoRA, the intra-frame and inter-frame VGGT transformer blocks should be frozen; however, in Figure 3 these blocks are labeled as trainable. Please clarify.\n- Third, how does the method perform on video data with continuously persistent moving objects, e.g., the DAVIS dataset? The experiments on real dataset is important to clarify some aforementioned weaknessness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pRzgFzRF5H", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["ICLR.cc/2026/Conference/Submission3687/Reviewer_Lh4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3687/Reviewer_Lh4W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928443188, "cdate": 1761928443188, "tmdate": 1762916923526, "mdate": 1762916923526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Visual Geometry Transformer in the Wild (VGTW), a feed-forward multi-view 3D reconstruction framework designed to handle real-world scenarios with transient distractors such as moving people or vehicles.\nBuilt upon prior transformer-based methods like VGGT and π³, VGTW adds:\n\na Distractor-Aware Training (DAT) strategy that fine-tunes attention via LoRA to suppress dynamic regions;\n\ntwo novel loss functions—Distractor Suppression Loss and Cross-View Consistency Loss;\n\nan auxiliary mask prediction head for identifying dynamic regions;\n\na new RobustNeRF-Mask dataset constructed using SAM2 segmentation and optical-flow consistency to generate pixel-level distractor annotations.\n\nThe resulting model can directly output clean, distractor-free 3D point clouds and camera poses without requiring any 3D supervision, showing strong performance and robustness across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a real-world gap: VGTW is the first feed-forward 3D reconstruction framework that explicitly handles transient distractors, a limitation of prior models like VGGT or DUSt3R.\n\nConceptually simple yet effective: By introducing DAT and LoRA-based fine-tuning, the authors enhance robustness without altering the backbone structure.\n\nNo 3D ground-truth supervision: The model only relies on 2D distractor masks, keeping the training lightweight and practical.\n\nSolid empirical results: Experiments show consistent improvements over strong feed-forward baselines, particularly in scenes with heavy occlusions or dynamic content."}, "weaknesses": {"value": "Dependence on mask supervision:\nThe method heavily relies on the new RobustNeRF-Mask dataset, where distractor masks are generated using SAM2 and optical-flow consistency, followed by partial manual refinement.\nThis external dependence limits generalization and raises concerns about the method’s robustness if mask quality degrades or manual curation is reduced.\n\nLimited and potentially unfair comparisons:\n\nFeed-forward baselines (VGGT, MASt3R, DUSt3R, etc.) do not use any distractor supervision, so performance gains might stem from additional training signals rather than methodological novelty.\n\nThe paper does not compare against optimization-based dynamic reconstruction methods (e.g., NeRF-W, WildGaussians, SpotLessSplats). While paradigms differ, such a comparison would contextualize VGTW’s efficiency–quality trade-off.\n\nAblation incompleteness:\nWhile DAT and mask head ablations are shown, there is no study on varying mask quality, missing masks, or cross-domain generalization, making it unclear how resilient the model is to imperfect annotations."}, "questions": {"value": "Could the authors provide details on the extent of manual correction performed during RobustNeRF-Mask construction? How significant was human intervention relative to SAM2 + optical flow auto-labeling?\n\nTo ensure fairness, have the authors tested a VGGT or π³ baseline trained with the same distractor masks (e.g., applying identical supervision but without DAT) to isolate the contribution of the proposed training strategy?\n\nHave the authors compared VGTW against dynamic-object-robust NeRF or Gaussian Splatting methods (e.g., NeRF-W, WildGaussians)? Even if paradigms differ, a runtime–quality comparison would help clarify the practical advantage of feed-forward reconstruction.\n\nHow robust is VGTW to noisy or imperfect masks? Have the authors quantified how performance drops when mask quality decreases or when no masks are available for fine-tuning?\n\nSince the model is designed for “in-the-wild” data, have the authors tested it on completely unseen domains (e.g., night scenes, handheld videos, or fast-moving dynamic scenes) to validate generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0RF13jUk5n", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["ICLR.cc/2026/Conference/Submission3687/Reviewer_pADS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3687/Reviewer_pADS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944478872, "cdate": 1761944478872, "tmdate": 1762916923300, "mdate": 1762916923300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VGTW (Visual Geometry Transformer in the Wild), an end-to-end feed-forward 3D reconstruction framework for handling inconsistent multi-view images with transient distractors.\nIt employs a Distractor-Aware Training (DAT) strategy with two tailored losses and an auxiliary mask head for distractor suppression, trained on the proposed RobustNeRF-Mask, a dataset with pixel-level distractor annotations that enables distractor-free 3D reconstruction without 3D supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, clearly structured, and easy to follow.\n\n2. The motivation is clear, focusing on the challenge of transient distractors in real-world multi-view 3D reconstruction.\n\n3. The technical design is reasonable, combining distractor-aware attention, consistency losses, and a mask head in a lightweight manner.\n\n4. The experiments demonstrate consistent improvements over baseline methods, yielding cleaner and more reliable 3D reconstructions."}, "weaknesses": {"value": "1. **Insufficient evaluation on standard benchmarks**  \nThe method is mainly evaluated on the dataset introduced in the paper for distractor-free 3D reconstruction. It remains unclear whether the proposed approach maintains comparable performance when input images contain no distractors (i.e., fully static scenes). An additional evaluation on standard benchmarks such as DTU [1] or ETH3D [2] would help verify generalization.\n\n2. **Unclear dataset annotation and segmentation process**  \nThe paper (around lines 187–188) mentions that “This may include dynamic objects, occluders, and non-rigid deformations.” as part of its distractor definition, but it does not clearly explain how the corresponding motion masks or annotations are obtained. Although the paper indicates the use of SAM for segmentation, it remains unclear how complex interactions or partial deformations are handled. For example, when a standing person opens a refrigerator, is the entire refrigerator segmented as dynamic, or only the moving door? Similarly, is the whole person segmented as dynamic, or only the moving hand? Clarification is needed on whether the segmentation results were manually refined, automatically filtered, or heuristically selected to ensure labeling correctness.\n\n3. **Lack of comparison with dynamic reconstruction baselines**  \nThe paper omits discussion and comparison with dynamic scene reconstruction methods such as MegaSAM [3] and MonST3R [4], which process full video sequences rather than image collections. Including at least one comparison or discussion with these approaches would strengthen the empirical analysis and better clarify the positioning of the proposed method.\n\n4. **Potential bias in evaluation setup**  \nThe ground truth is labeled using π³ [5], which is also included as one of the baselines. This raises potential fairness concerns in the evaluation process. One possible solution is to follow PAGE-4D [6] and perform additional evaluation on an independent dataset such as DyCheck [7] to ensure unbiased comparison.\n\n5. **Citation formatting issue**  \nThe citation in line 125 appears to be incorrect and contains unresolved reference symbols (“;?”). \n\n**Reference**\n\n[1] Large-Scale Multi-View Stereopsis Evaluation.  \n[2] A Multi-View Stereo Benchmark with High-Resolution Images and Multi-Camera Videos.  \n[3] MegaSAM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos.  \n[4] MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion.  \n[5] π³: Permutation-Equivariant Visual Geometry Learning.  \n[6] Monocular Dynamic View Synthesis: A Reality Check."}, "questions": {"value": "1. The method based on π³ fails to improve the NC metric on the evaluation datasets. Could the authors explain the reason behind this?\n\n2. Although the method is described as lightweight, inference time comparisons are not reported. Could the authors provide quantitative runtime results to support this claim?\n\n3. Could the authors provide more detailed w/o experiments to isolate the effect of each proposed component and demonstrate how each contributes to the overall performance improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gH1AUIQ5dl", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["ICLR.cc/2026/Conference/Submission3687/Reviewer_oJJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3687/Reviewer_oJJM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955938828, "cdate": 1761955938828, "tmdate": 1762916922473, "mdate": 1762916922473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a fine-tuning approach based on the state-of-the-art geometric foundation models (e.g., VGGT), to tackle the distrator regions without point correspondence across multi-view input. Author first demonstrate the visualization on the attention map of original inference between using the distrator mask and w/o the mask. Then a couple of loss supervisions are proposed to enhance the feature similarty on the real matched points while suppressing the distractors. Besides, a dataset containing distractor mask is built and released for better problem setup. Extensive experiments showcase the effectiveness of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper is well motivated and handling the non-matched regions is an important yet challenging problem in multi-view geometry in a long run. The visualization of the cross attention map for those regions are admirable, to make readers better elaborate the challenge. \n\n(2) The introduced loss functions are techncially sound to enhance the feature similarity on the true positive matched regions, while suppressing the false positive ones. The loss functions are simple yet effective to be designed and implemented. \n\n(3) The experiments have demonstrated the validness of the design."}, "weaknesses": {"value": "(1) The scale and diversity of constructed dataset with the distractor mask is limited (1000 annotated images on based on a single RobustNeRF) dataset, making the problem hard to be scaled up and extended. I was wondering whether using some pretrained optical flow network or dense point matching network, or simply SAM2, can scale up the annotation dataset effectively. By training on diverse dataset, the proposed method could demonstrate the generalizability over wild images.\n\n(2) The method is only evaluated on NeRF-on-the-go dataset, which is hard to measure the generalizability and zero-shot capacity of the proposed method. Besides, authors train and evaluate the framework on the same dataset (RobustNeRF), which makes the technical contribution less convincing since all the baseline method are not trained on this dataset. I would suggest authors to do multiple cross-dataset evaluation to validate the generalizability of the method."}, "questions": {"value": "Overall the motivation of the paper is encouraging and the technical contribution is clear. However, I still have concerns on the scalability and evaluation protocol as presented in the 'weakness' part. I hope authors could supplement more experiments to further demonstrate the generlizability of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dx33L4MlDC", "forum": "OEyPLlIrtd", "replyto": "OEyPLlIrtd", "signatures": ["ICLR.cc/2026/Conference/Submission3687/Reviewer_qYpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3687/Reviewer_qYpT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051534709, "cdate": 1762051534709, "tmdate": 1762916921673, "mdate": 1762916921673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}