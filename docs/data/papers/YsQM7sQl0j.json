{"id": "YsQM7sQl0j", "number": 2548, "cdate": 1757143127502, "mdate": 1759898141662, "content": {"title": "Generative Human Geometry Distribution", "abstract": "Realistic human geometry generation is an important yet challenging task, requiring both the preservation of fine clothing details and the accurate modeling of clothing-body interactions. To tackle this challenge, we build upon Geometry distributions—a recently proposed representation that can model a single human geometry with high fidelity using a flow matching model. However, extending a single-geometry distribution to a dataset is non-trivial and inefficient for large-scale learning. To address this, we propose a new geometry distribution model by two key techniques: (1) encoding distributions as 2D feature maps rather than network parameters, and (2) using SMPL models as the domain instead of Gaussian and refining the associated flow velocity field. We then design a generative framework adopting a two-staged training paradigm analogous to state-of-the-art image and 3D generative models.\nIn the first stage, we compress geometry distributions into a latent space using a diffusion flow model; the second stage trains another flow model on this latent space. \nWe validate our approach on two key tasks: pose-conditioned random avatar generation and avatar-consistent novel pose synthesis.\nExperimental results demonstrate that our method outperforms existing state-of-the-art methods, achieving a 57% improvement in geometry quality.", "tldr": "We introduce the first method that integrates geometry distributions into generative modeling.", "keywords": ["3D Generation", "Human Generation", "Geometry Encoding"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daaa16ff2e916faa4e25636981d7a8cc42926173.pdf", "supplementary_material": "/attachment/45707080ba677ac8b497f666da98db7585d42c56.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of 3D human geometry generation by proposing a new geometry distribution model that extends this representation from single characters to large-scale dataset learning.  \n\nSpecifically, its proposed representation incorporates two key designs:  \n(1) Encode geometry distributions as 2D feature maps instead of network parameters to enable dataset-level generalization.  \n(2) Replace the Gaussian source distribution with the SMPL human template (plus refined flow velocity fields) to align the source closer to the target geometry distribution.\n\nBased on this representation, this paper also proposes a two-stage generative framework:  \n(1) Compress each human geometry distribution into a compact 2D feature map using a flow model, from which a high-fidelity human geometry can be sampled through a denoising process.  \n(2) Train a second flow model on the latent feature space to support generative tasks.  \n\nThe method is evaluated on two tasks: pose-conditioned random avatar generation (THuman2 dataset) and avatar-consistent novel pose synthesis (4DDress dataset). Quantitative results show a 57% improvement in geometry quality and a 7% improvement in visual appearance compared to SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The technical designs in the proposed geometry distribution are solid:  \n(1) Replacing Gaussian with SMPL (a human-specific template) reduces the \"distance\" between source and target distributions, minimizing the need to learn extraneous flow paths. This, combined with training pair construction (matching target points to nearest SMPL points + Gaussian perturbation) and distribution normalization (zero-centered Gaussian for source, dense displacement fields for target), solves spatial imbalance in training and accelerates convergence.  \n(2) Unlike prior geometry distributions that store shape information in network weights, encoding into 2D feature maps enables efficient dataset-level learning. Plus, the idea of using the decompressed feature map to serve as a condition to generate the final geometry distribution makes sense to me.\n\n2. The quantitative and qualitative experiments are comprehensive, showcasing the effectiveness of the proposed method as well as each design component."}, "weaknesses": {"value": "Please refer to the question section."}, "questions": {"value": "1. I'm kind of uncertain about the application scenario. This method is built upon you already have the SMPL parameters, right? If SMPL can already provide human geometry information, what's the role of this proposed method? Is it more like a refiner?\n\n2. I'm a little confused about the decoding process. Looks like you're optimizing the flow model $\\theta$ and the condition (i.e. the 2D feature map $\\textbf{z}_{T|S}$) simultaneously. Will the training process be stable since I recall that normally in diffusion training, the condition should be fixed, right?\n\n3. I'm wondering do you have a mathematical analysis about why recovering target geometry by conditioning on SMPL can be modeled as a 2D feature map? What does each pixel in the feature map mean? Maybe the author should give more introduction or preliminary knowledge on the 2D feature map representation of 3D objects like SMPL.\n\n4. I'm curious about the influence of feature map size. The paper fixes the feature map to 8×24×24. Are 576 embeddings too few to encode 50,000 points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R6DQYEkU1l", "forum": "YsQM7sQl0j", "replyto": "YsQM7sQl0j", "signatures": ["ICLR.cc/2026/Conference/Submission2548/Reviewer_aJPB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2548/Reviewer_aJPB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760731102625, "cdate": 1760731102625, "tmdate": 1762916277508, "mdate": 1762916277508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has proposed a novel 3D human generative model built on top of Geometric Distribution. Specifically, it learns a conditional flow model from the SMPL parameter to the loose cloth real geometric distribution. Some training tricks (pair sampling, distribution normalization, and auto-decoder) are proposed to improve the performance. Extensive experiments have demonstrated the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is novel, and the problem is worth exploring.\n2. The proposed training tricks are reasonable and effective.\n3. The final quality is surprisingly good."}, "weaknesses": {"value": "1. Since some baselines (Like EVA3D) also model the texture distribution, directly comparing the geometry with it is somewhat unfair. But this is not a big issue.\n2. Some of the proposed training tricks are actually already well established in the previous works. E.g., the SMPL -> final distribution idea is similar to the diffusion bridge, and the DistNorm is very popular in the current diffusion model training (vae rescaling). \n3. The proposed method has not demonstrated the performance on the animation / temporal consistency."}, "questions": {"value": "1. If autodecoder is used, when new data comes, does it mean that this proposed method cannot handle this case? Or it requires some test-time optimization, which is unscalable.\n2. The UV-space diffusion + autodecoder pipeline for 3d avatar generation is very relevant to Gaussian3Diff (ECCV 24), and should be discussed in the related work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vKeFI1xRek", "forum": "YsQM7sQl0j", "replyto": "YsQM7sQl0j", "signatures": ["ICLR.cc/2026/Conference/Submission2548/Reviewer_2RbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2548/Reviewer_2RbH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426648302, "cdate": 1761426648302, "tmdate": 1762916276251, "mdate": 1762916276251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Building upon the method Geometry Distribution, this paper introduces a human generation method via a flow-based diffusion model. To enable efficient training over the existing human datasets, the authors first propose to encode the human geometry distribution into a feature map. Leveraging the pre-trained feature map, the authors further adopt the SMPL model distribution as the learning base while refining the flow velocity field for more efficient training. The paper also introduce an efficient strategy for constructing training pairs, further improving the generated quality. Extensive experiments have been conducted to demonstrate the performance of the proposed method across various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the method can be summarized as:\n\n+ The adoption of geometry distributions and the introduction of feature map \"representation\" is novel and demonstrated to be useful.\n\n+ Both the qualitative and quantitative evaluations are thorough and demonstrate the performance of the proposed method, establishing a new state-of-the-art.\n\n+ The paper is well-written and easy to follow."}, "weaknesses": {"value": "The reviewer would like to point out several weaknesses of the paper:\n\n- The method may still inherit some limitations from the SMPL template, such as the modeling of fine details like hair, accessories, hats, etc.\n\n- Based on the first weakness, the reviewer would like to see the rotating results of loose clothing, complex poses.\n\n- Considering the method is trained on THUman2 dataset, it might be beneficial to compare with the 3D reconstruction methods (like PIFu, ECON, ICON) in terms of both qualitative and quantitative quality to further demonstrate the method or present the gap. Additionally, more metrics like P2S and chamfer distance may be useful to help demonstrate the 3D quality."}, "questions": {"value": "1. In Figure 7, we could still observe some artifacts in \"Ours\", especially for the frontal view. However, in Figure 6, it's presented that the proposed strategy for constructing training pairs could address (or actually minimize) this issue. THe reviewer would like to ask the potential and actual factors that lead to this problem, and also the possible solutions.\n\n2. Considering the method employs the SMPL model as the base, will that be possible to generate different shapes (tall, short, fat, thin) or even genders for a human subject?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HKa0TZ7Acy", "forum": "YsQM7sQl0j", "replyto": "YsQM7sQl0j", "signatures": ["ICLR.cc/2026/Conference/Submission2548/Reviewer_epmJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2548/Reviewer_epmJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982582925, "cdate": 1761982582925, "tmdate": 1762916275838, "mdate": 1762916275838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn the generative human geometry distribution from 3D human dataset, which can further be applied for random human generation and animation. At the core of the method is a diffusion model which is conditioned on the UV space of SMPL model to predict the geometry distribution. The authors conducted experiments on different dataset to verify the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow.\n\nGenerating 3D humans is an interesting task with practical applications.\n\nThe method is technically sound by leveraging a UV-conditioned diffusion model to learn the geometry distribution of humans."}, "weaknesses": {"value": "Insufficient experiments. The paper only evaluates the method on studio 3D data, and does not illustrate whether the proposed method can generate out-of-domain 3D humans. The paper does not answer the question of whether the method overfits to the training dataset, and whether it can sample humans with novel clothing styles. Only random sampling and novel pose generation are shown. \n\nIn addition, in Fig. 4, the authors claim that the pipeline supports multiple condition,s such as images/texts, which, however, are not illustrated in the paper. Can the method generate 3D humans given single in-the-wild images, or generate text-conditioned humans like joint2human?\n\n\nExisting methods can learn 3D human generations from 2D data such as images and videos, including GAN-based (e.g., EVA) and diffusion-based method (e.g., Primitive Diffusion [Zhao et al. NeurIPS, StructLDM), or generate both geometry and textures using 3D Gaussians in a feed-forward way, such as LHM, AniGS. Compared with these work, what are the advantages of the proposed method which require 3D ground truth data in training and can only generate 3D geometry?\n\nThis paper (video demo) claims that it is the first method that learn the human geometry distribution of a dataset. However, this statement is not true. Previous 3D human generation methods such as GDNA [Xu et al 2022] for geometry reconstruction, or EVA/PrimDiff for both geometry and texture reconstruction all learn the distribution of human geometry."}, "questions": {"value": "The method utilizes SMPL model. How does this handle loose clothing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m89O91b5tp", "forum": "YsQM7sQl0j", "replyto": "YsQM7sQl0j", "signatures": ["ICLR.cc/2026/Conference/Submission2548/Reviewer_P4Gu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2548/Reviewer_P4Gu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017633922, "cdate": 1762017633922, "tmdate": 1762916275217, "mdate": 1762916275217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}