{"id": "qCIg2WGudx", "number": 6557, "cdate": 1757988861221, "mdate": 1759897908118, "content": {"title": "Mapping Post-Training Forgetting in Language Models at Scale", "abstract": "Scaled post‑training now drives many of the largest capability gains in language models (LMs), yet its effect on pretrained knowledge remains poorly understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S. president or an API call) does not “average out” by recalling another. Hence, we propose a sample-wise paradigm to measure what is forgotten and when backward transfer occurs. Our metric counts 1→0 transitions (correct before post‑training, incorrect after) to quantify forgetting and 0→1 transitions to quantify backward transfer. Traditional task averages conflate these effects and obscure large changes. For multiple‑choice benchmarks, we add chance‑adjusted variants that subtract the expected contribution of random guessing from pre‑ and post‑training accuracies. We apply this framework across post‑training stages, model sizes, and data scales. Our large‑scale analysis shows that: (1) Domain-continual pretraining induces moderate forgetting with low backward backward transfer; (2) RL/SFT post-training applied to base models and Instruction tuning yield substantial backward transfer with minimal forgetting; (3) Applying RL/SFT to instruction‑tuned models is sensitive on data scale: at small scales, both forgetting and backward transfer are small; at larger scales, effects are mixed and warrant further study with better controls; (4) Model merging does not reliably mitigate forgetting. Overall, our framework offers a practical yardstick for mapping how post‑training alters pretrained knowledge at scale -- enabling progress towards generally capable AI systems.", "tldr": "We quantify forgetting of pretraining knowledge during post-training using simple samplewise metrics -- providing an extensive empirical analysis and open problems in continual learning for foundation models", "keywords": ["continual learning", "foundation models", "reasoning", "forgetting", "pretraining knowledge"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a25d44cc9937e8437b2203a8e6167561e4b3db0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a sample-wise framework to measure knowledge forgetting and backward transfer in language models during post-training, using chance-adjusted metrics to account for guessing in multiple-choice evaluations. Through large-scale experiments across various post-training stages, the study finds that instruction tuning and reasoning training from base models cause minimal forgetting but significant backward transfer, while domain-continual pretraining leads to moderate forgetting with limited improvement. The authors also show that model merging does not reliably mitigate forgetting, challenging common assumptions in continual learning and providing a practical tool for tracking knowledge retention in evolving AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiment is extensive\n2. The research topic is realistic"}, "weaknesses": {"value": "1. Limited Conceptual Contribution and Analytical Depth\nThe paper reads more as an extensive experimental report than a work offering novel theoretical insights. The findings—such as \"moderate forgetting\" or \"lower forgetting\"—are described using vague, qualitative terms and do not reveal unexpected patterns or advance conceptual understanding of forgetting mechanisms in language models.\n\n2. Methodological Rigor in Experimental Design\nThe evaluation framework does not adequately control for key training variables, such as learning rates or data volume across stages. For instance, observing less forgetting in SFT and RL compared to pretraining is unsurprising given the typically smaller learning rates used in later stages. Similarly, greater backward transfer in reasoning training may simply reflect the base model's initial lack of exposure to such data, rather than meaningful generalization."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f9LCaoGv3w", "forum": "qCIg2WGudx", "replyto": "qCIg2WGudx", "signatures": ["ICLR.cc/2026/Conference/Submission6557/Reviewer_e6NM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6557/Reviewer_e6NM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658913211, "cdate": 1761658913211, "tmdate": 1762918898970, "mdate": 1762918898970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the core issue of knowledge change during LLM post-training, proposing an innovative and practical analytical framework. The authors first correctly identify two major shortcomings of existing average accuracy-based evaluation methods: the inability to decouple forgetting and backward transfer, and susceptibility to random guessing in multiple-choice question evaluation.\n\nTo overcome these problems, the paper proposes a sample-level metric paradigm that provides a more granular analytical perspective by tracking the transitions of individual samples from correct to incorrect and from incorrect to correct before and after training. A chance-adjusted method is introduced to model random guessing behavior, extracting the chance factor from observed transitions to estimate F_true and BT_true, which more closely approximate the actual knowledge change.\n\nBuilding on this framework, the authors conducted the largest empirical study to date, systematically evaluating knowledge change across different model families and sizes in various mainstream post-training scenarios, including domain-based continuous pre-training, instruction fine-tuning, inference training, and model merging. The study yielded several important findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Shifting the evaluation perspective from the macro-level average accuracy of the task to the micro-level of the sample is a highly insightful transformation. This allows us to understand more precisely the dynamic changes in the knowledge within the model, rather than simply seeing a vague final score. \n\n2. The authors evaluated nearly 30 model-training combinations, covering the vast majority of mainstream post-training paths in the current LLM ecosystem. This work itself is a massive undertaking, providing the community with extremely valuable data and baselines. The \"forgetting map\" constructed in the paper provides practitioners and researchers with a practical \"benchmark\" to measure the \"knowledge cost\" of different post-training strategies. This has direct practical value in guiding future designs for more efficient and less forgetting training processes."}, "weaknesses": {"value": "1. The model's assumption of \"uniform random guessing when unable to solve a problem, with independent guesses before and after\" is somewhat idealistic. Discussions could be made regarding robustness checks using multiple sampling/deterministic decoding, or question-level chance correction based on logits, to relax the independence and uniformity assumption.\n\n2. There are some shortcomings in the experiments. The current version reports a flipped variance/confidence interval, requiring significance and multiple comparison correction. Using random decoding with a temperature of 0.6 and a nullus value of 0.95, and running only once without multiple seed repetitions, introduces randomness leading to flipping. The variance and stability of the flipped statistics are not evaluated. The current evaluation primarily covers MCQs and has not yet been extended to tasks closer to the post-training objectives, such as open generation, code/tool ​​calls, and procedural derivation."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGinrmlX2C", "forum": "qCIg2WGudx", "replyto": "qCIg2WGudx", "signatures": ["ICLR.cc/2026/Conference/Submission6557/Reviewer_wF4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6557/Reviewer_wF4s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894458611, "cdate": 1761894458611, "tmdate": 1762918898575, "mdate": 1762918898575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the critical problem of pretrained knowledge forgetting and backward transfer in large-scale language model (LM) post-training, addressing the limitations of traditional evaluation methods. Unlike traditional task-averaged metrics that obscure individual knowledge changes, the paper defines forgetting as 1→0 transitions (correct before post-training, incorrect after) and backward transfer as 0→1 transitions (incorrect before, correct after), recognizing that knowledge samples (e.g., facts about U.S. presidents, API syntax) are non-fungible. To eliminate the interference of random guessing in multiple-choice benchmarks, the paper proposes \\(\\boldsymbol{F_{true}}\\) (true forgetting) and \\(\\boldsymbol{BT_{true}}\\) (true backward transfer). These metrics subtract the expected \"chance transitions\" (e.g., lucky guesses turning to errors) from raw transitions, with additional ceiling metrics (\\(F_{max}\\), \\(BT_{max}\\)) to contextualize the theoretical upper limit of knowledge change."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1-The sample-wise paradigm directly addresses the non-fungibility of pretrained knowledge (a long-overlooked limitation of task-averaged metrics), and chance-adjusted metrics rigorously correct for random guessing—an essential step for reliable evaluation of multiple-choice benchmarks (the dominant format for knowledge-intensive LM tests).\n\n2-The methodology is both theoretically sound (e.g., explicit assumptions about uniform guessing and independent pre/post events) and practically feasible (no need for logits or repeated sampling, enabling large-scale deployment).\n\n3-The findings provide clear practical guidance: e.g., \"instruction tuning is low-risk for knowledge retention\" and \"large-scale RL/SFT on instruction-tuned models requires better controls\"—directly informing how practitioners design post-training pipelines.\n\n4-The work reconciles conflicting views between continual learning literature (predicting catastrophic forgetting) and LM practice (observing minimal forgetting in many regimes), highlighting the need to adapt continual learning theories to LM-specific post-training."}, "weaknesses": {"value": "1-The paper identifies what post-training regimes cause forgetting/transfer but rarely explains why. For example:\nIt notes \"culture\" is the most forgettable category across regimes (e.g., Llama-3.1-8B-Instruct has 18.9% forgetting in culture), but does not investigate whether this stems from cultural knowledge being less \"entrenched\" in pretraining, or post-training data mismatching cultural contexts. It finds reasoning training on base models outperforms instruction tuning in transfer, but does not clarify if this is due to the \"scratchpad\" mechanism, data quality, or objective function—limiting the paper’s ability to guide how to design better post-training objectives.\n\n2-The paper states that for RL/SFT on instruction-tuned models \"at larger scales, effects are mixed and warrant further study with better controls\" (1-5, 1-116), but provides no actionable path to resolve this ambiguity. It does not test variables like data diversity (e.g., mixed vs. narrow domains), training duration, or objective function (RL vs. SFT)—leaving a critical gap in understanding a common post-training scenario.\n\n3-The paper concludes \"model merging does not reliably mitigate forgetting\" but only tests two-checkpoint merging (pre vs. post-training). This contrasts with prior work (e.g., Yadav et al., 2023) that uses 8+ checkpoints to mitigate weight drift. The failure to explore multi-checkpoint merging or alternative methods (e.g., TIES-merging) weakens the conclusion, as it does not rule out merging as a viable strategy—only the specific implementation tested.\n\n4-All evaluations use MCQ formats, but real-world LM use cases often involve generative tasks (e.g., open-ended QA, text synthesis). The paper does not validate whether its sample-wise framework extends to generative settings, limiting its generalizability to practical applications where \"forgetting\" might manifest as ungrammaticality or factual errors in free text."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EtPHl2UKE5", "forum": "qCIg2WGudx", "replyto": "qCIg2WGudx", "signatures": ["ICLR.cc/2026/Conference/Submission6557/Reviewer_6fZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6557/Reviewer_6fZS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919679953, "cdate": 1761919679953, "tmdate": 1762918898298, "mdate": 1762918898298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study introduces a sample-wise framework to quantify how LLMs forget or retain knowledge during post-training. By measuring 1→0 (forgetting) and 0→1 (backward transfer) transitions, it finds that instruction tuning and reasoning post-training improve capabilities with minimal forgetting, while domain-continual pretraining causes moderate forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper introduces a sample-wise, chance-adjusted metric to precisely quantify forgetting and backward transfer, overcoming the limitations of aggregate accuracy measures.\n- This paper conducts a broad empirical studies across multiple model sizes, training regimes (SFT, RLHF, instruction tuning, continual pretraining), and datasets, providing a systematic view of post-training dynamics.\n- This paper offers interest findings: instruction tuning and reasoning post-training yield strong backward transfer with little forgetting, while domain-continual pretraining induces moderate loss.\n- This work challenges the assumption of “catastrophic forgetting” in post-trained LLMs, showing that most modern post-training does not erode pretrained knowledge as severely as previously thought."}, "weaknesses": {"value": "- This study focuses mainly on MCQ datasets, which may not generalize to open-ended tasks or generative reasoning.\n- The uniform random-guessing correction assumes independence and equal likelihood of options, which oversimplifies model behavior and may distort real forgetting dynamics.\n- While correlations between data scale, post-training type, and forgetting are documented, the paper does not deeply analyze why certain stages (e.g., reasoning training) cause particular effects."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s1ExZreK3r", "forum": "qCIg2WGudx", "replyto": "qCIg2WGudx", "signatures": ["ICLR.cc/2026/Conference/Submission6557/Reviewer_f1yS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6557/Reviewer_f1yS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097623155, "cdate": 1762097623155, "tmdate": 1762918898020, "mdate": 1762918898020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}