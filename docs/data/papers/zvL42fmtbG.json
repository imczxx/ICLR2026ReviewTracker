{"id": "zvL42fmtbG", "number": 2340, "cdate": 1757059955970, "mdate": 1759898155018, "content": {"title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks", "abstract": "The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released publicly.", "tldr": "A comprehensive benchmark for evaluating deep research agents on academic survey tasks, focusing on reference quality and coverage alongside the factuality of cited and non-cited statements.", "keywords": ["Deep Research Agents", "Benchmark", "Survey Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55441a98c82fba30b636617825e8e1c3bcf32658.pdf", "supplementary_material": "/attachment/13047a8070c78d8e16790683dfbd4b873aa9faeb.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a benchmark for evaluating research reports generated by LLMs. It assesses reports along two dimensions—the quality of cited literature and the factual accuracy of statements—using survey papers from arXiv as gold-standard references. An automated agent framework verifies citation faithfulness and fact correctness. Results show that commercial Deep Research agents (e.g., OpenAI, Google) produce more reliable reports than standalone LLMs, though issues with coverage and consistency remain."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new benchmark dataset comprising hundreds of survey papers, which serves as ground-truth for evaluating the performance of LLMs. This dataset could be a contribution that could benefit the research community.\n\n2. The paper proposes an automatic evaluation framework for assessing the deep research capabilities of LLMs, focusing on two key dimensions: citation quality and statement faithfulness.\n\n3. Several state-of-the-art LLMs are evaluated on the proposed benchmark, and the results highlight both their strengths and the remaining gaps, providing useful insights and directions for future improvement."}, "weaknesses": {"value": "1. The overall quality of the dataset appears to be highly dependent on the performance of LLMs. However, the paper does not provide sufficient evaluation of the intermediate steps involved in dataset construction (see my detailed questions below). Inaccuracies in these intermediate steps could substantially affect the overall dataset quality.\n\n2. More human evaluation should be incorporated into the data creation process to ensure reliability and to validate the quality of the automatically generated data.\n\n3. The paper lacks in-depth analysis of why the evaluated models fail across the three tasks. The discussion remains rather superficial, without providing deeper insights into the underlying causes of model errors.\n\n4. The paper demonstrates limited novelty. The dataset lacks human annotations, and the work reads more like a project report, where the authors simply apply a few LLMs and report estimated accuracies, rather than presenting deeper methodological or analytical contributions."}, "questions": {"value": "1. In Section 2.1.2, the quality of the designed prompts plays a crucial role in determining the overall quality of the dataset. Therefore, this section would benefit from more detailed descriptions. For example, how were the prompts constructed when incorporating additional explicit instructions? How were the three prompt templates applied to each paper—was every paper processed using all three, or just one of them? Moreover, are these prompts model-specific or generally applicable across different LLMs?\n\n2. In Section 2.1.3, it is not entirely clear how the ten categories are defined. The definitions and boundaries between these categories should be clarified, as it is difficult to determine whether there is any conceptual or content overlap among them.\n\n3. In Line 239, the authors adopt an LLM to match URL links to corresponding articles for calculating the quality score. However, incorrect URL matching could substantially impact the reliability of these quality scores. It would therefore be important to include a quantitative analysis of this step—for instance, reporting the overall accuracy of the URL mapping process and analyzing the proportion of errors attributable to different causes (e.g., hallucination, mapping failure, or missing data)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vKKJmUm54b", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_RA99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_RA99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562050021, "cdate": 1761562050021, "tmdate": 1762916199397, "mdate": 1762916199397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReportBench, a benchmark that evaluates Deep Research agents on generating academic reports (survey-style outputs) by verifying both citation accuracy and factual consistency. The benchmark is constructed by reverse prompt engineering from 678 arXiv survey papers across 10 domains, forming 100 gold-standard tasks. It measures each model’s ability to retrieve relevant literature and generate factually faithful statements. Six commercial and open-source Deep Research models (including OpenAI and Gemini) are evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clever use of existing structured survey data to derive gold references.\n2. Covers diverse scientific domains and defines clear, interpretable metrics.\n3. The benchmark could potentially scale to thousands of topics with minimal human effort."}, "weaknesses": {"value": "1. Low recall severely limits its evaluation coverage; Deep Research systems retrieve only a fraction of ground-truth citations.\n2. No diversity or coverage analysis—it’s unclear whether models repeatedly cite the same high-frequency papers.\n3. The benchmark could easily double as a training dataset for Deep Research agents, but no such experiments or discussions are provided."}, "questions": {"value": "1. Could iterative multiple deep searches raise recall and diversity?\n2. Have the authors explored using this dataset for fine-tuning or reinforcement of Deep Research models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pEihn49oOP", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_qrQz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_qrQz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625980839, "cdate": 1761625980839, "tmdate": 1762916199257, "mdate": 1762916199257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ReportBench, a benchmark for assessing deep-research agents that generate academic-style survey reports. It builds reverse-engineered prompts from expert-written arXiv survey papers and evaluates model outputs along two axes: (i) whether the cited literature is relevant to the target survey and (ii) whether both cited and non-cited statements are factually supported through a multi-stage agentic verification pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using high-quality, already peer-reviewed survey papers as “gold” to synthesize realistic research tasks and ground-truth citation sets is pragmatic, which gives the benchmark an immediately credible target, avoiding costly human labeling.\n2. The evaluation pipeline is thoughtfully decomposed: cited statements are checked against the retrieved source, while non-cited statements are validated via multi-model web voting, which makes the evaluation more interpretable than a single “LLM-as-a-judge” score."}, "weaknesses": {"value": "1. The benchmark rests on a strong assumption that published survey papers can universally serve as ground truth; in domains or time ranges where the original survey is itself incomplete or biased, the benchmark will inherit that bias, and current results may over-penalize models that find reasonable but non-overlapping sources.\n2. Although the framework claims to evaluate “report quality,” it explicitly defers writing-style, structure, and coherence evaluation to future work, so the current benchmark covers only the content/veracity slice and cannot support end-to-end judgment of report usefulness.\n3. The final test set is relatively small (100 prompts sampled from 678 candidates) and reflects the topic skew of arXiv surveys after 2020, which may limit its generalizability.\n4. The reported results show very low recall of ground-truth references (≈3–4%) even for the strongest commercial agents, which suggests that either the task is calibrated to be quite hard or the metric does not yet align with how human researchers judge adequacy; this point deserves more analysis before the benchmark can be used as a definitive leaderboard."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TXO7Dv0zJK", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_BgNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_BgNR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867793106, "cdate": 1761867793106, "tmdate": 1762916198890, "mdate": 1762916198890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReportBench, a systematic benchmark for evaluating the quality of research reports generated by large language models and Deep Research agents. The benchmark assesses two critical dimensions: the quality and relevance of cited literature, and the faithfulness of statements within generated reports. Using high-quality arXiv survey papers as gold-standard references, the authors develop an automated agent-based evaluation framework that extracts citations and statements, verifies cited content against original sources, and validates claims using web resources. Empirical results show that commercial Deep Research agents from OpenAI and Google generate more comprehensive and reliable reports than standalone LLMs with search tools, though significant room for improvement remains in research coverage breadth, depth, and factual consistency. The benchmark provides an useful evaluation methodology for assessing AI-generated research as these systems become more widely adopted for complex research tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology of the data construction process is a clear strength of the paper. By restricting the corpus to post-2020, peer-reviewed, and officially published arXiv survey papers, they effectively filter out noise and ensure a high-quality baseline for both content and references. The use of GPT-4o for binary classification of survey papers and the robust extraction of references directly from LaTeX sources are technically sound choices. The prompt design stage also reflects careful consideration of common evaluation pitfalls, particularly through two mechanisms: temporal consistency by including publication-date cutoffs to prevent unfair advantage from newer data and anti-cheating instructions that prohibit citing the original survey itself, which helps avoid data contamination. Together, these decisions greatly enhance the credibility and robustness of the ReportBench dataset.\n\n\n2. ReportBench’s automated pipeline provides a practical path toward building dynamic, low-cost, and continuously extensible benchmarks. By linking the framework to the ever-expanding arXiv corpus, the authors’ approach can generate new and timely tasks as new survey papers are published. This represents a substantive and pragmatic contribution to the practice of LLM evaluation, introducing the possibility of benchmarks that evolve with the research landscape rather than becoming obsolete as the field progresses.   \n\n\n\n3. The evaluation framework is intuitively designed yet highly aligned with the benchmark’s core objectives. It clearly separates citation-based and non-citation factual assessments, allowing for transparent interpretation of each competency being tested. Moreover, when applying the LLM-as-a-judge paradigm, the authors employ a majority-vote strategy across multiple judgments, which effectively mitigates individual model hallucinations and enhances the overall reliability of the automatic evaluation. This thoughtful combination of simplicity and robustness makes the framework both easy to reproduce and trustworthy in assessing deep research agents."}, "weaknesses": {"value": "1. The paper does not adequately discuss potential limitations of using arXiv survey papers as gold standards. Survey papers inherently represent synthesis and interpretation rather than primary research, and different surveys on the same topic may emphasize different aspects or reach different conclusions, which may lead to different 'gold ground-truth citation'. Therefore, these evaluations based on cited article matching are inevitably biased. Additionally, the reverse prompt engineering approach assumes that the original survey structure represents the \"correct\" way to organize information about a topic, which may not always hold. The paper should discuss these limitations and their potential impact on evaluation validity.\n\n2. The proposed ReportBench shares significant conceptual overlap with one previous paper DeepResearch Bench (Du et al., 2025), which also evaluates factual accuracy, citation reliability, and comprehensiveness of LLM-generated research reports. So the paper should more clearly articulate how it goes beyond DeepResearch Bench in terms of scope, evaluation methodology, or insight. The current claim in related work section that DeepResearch Bench \"falls short of evaluating core competencies\" appears overstated, as both benchmarks assess similar levels of factual and citation-based competence rather than higher-order scientific reasoning. A more detailed comparative analysis showing concrete differences in evaluation granularity, coverage, or the types of errors detected would strengthen the contribution.\n\n3. The citation recall reported in the experiments is notably low, highlighting the high difficulty of the task. This result also suggests that the agents’ limited citation coverage may partly stem from the conservative nature of the task setup or the prompt design. In addition, the authors mention that each benchmark survey paper contains an average of 153 references, which further increases the challenge of achieving high citation recall.\n\n4. While the paper reports two representative failure types (statement and citation allucination), it lacks a detailed taxonomy of error types and failure modes observed in generated reports. Understanding these error categories would provide actionable insights for improving Deep Research agents. The paper should include a qualitative analysis section that categorizes and analyzes representative failure cases from different systems in the appendix."}, "questions": {"value": "1. Could the authors elaborate on the potential limitations of using arXiv survey papers as gold standards for factual and citation-based evaluation? Since survey papers inherently reflect synthesis and interpretation rather than primary research, different surveys on the same topic might highlight different aspects or reach divergent conclusions, potentially leading to inconsistent or biased “ground-truth” citations. How do the authors address such variability to ensure fairness and validity of the benchmark?\n\n2. Could the authors clarify, in more concrete terms, how ReportBench goes beyond DeepResearch Bench (https://arxiv.org/abs/2506.11763) in terms of scope, evaluation methodology, or insight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9jSVNgZIUa", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_J4c9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_J4c9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979468730, "cdate": 1761979468730, "tmdate": 1762916198728, "mdate": 1762916198728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReportBench, a benchmark for evaluating long-form, research-style reports produced by deep research agents. The authors start from expert-written survey papers (filtered from arXiv, with publication/acceptance signals), reverse-engineer three granularities of prompts from them, and add two anti-shortcut constraints (time cutoff, and “do not cite the original survey”). Systems (commercial deep research products and base LLM+search setups) are then asked to write a report, and the output is checked along two objective axes: (i) citation quality, by comparing cited URLs against the survey’s reference list; (ii) statement-level factuality, by separating cited sentences (cross-checked against the cited source) and non-cited sentences (checked by a small ensemble of LLM judges). Experiments on OpenAI Deep Research, Gemini Deep Research and several baselines show that productized agents are clearly better than “LLM+search”, but they still exhibit two concrete error modes: statement hallucination and citation hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed is real and timely: current deep research agents can produce long reports, but we lack an automatic and reasonably objective way to check whether those reports are actually faithful to sources. Turning this into a benchmark is a useful contribution to the agent-eval community.\n\n2. The data construction pipeline is clear and defensible: start from human survey papers, parse LaTeX to get the ground-truth reference list, reverse-prompt the task, and explicitly constrain the model not to cite the source survey and not to use material after a given date. This makes the task anti-leak by design and reduces the chance that models just “find the original answer”.\n\n3. The evaluation is broken down into auditable steps instead of a single opaque LLM score. In particular, the paper distinguishes cited sentences from non-cited ones and uses different verifiers for them; this makes the resulting signals more interpretable for system builders.\n\n4. The paper actually evaluates real commercial deep research products (OpenAI, Gemini) and compares them with their underlying base models wired to web tools. This gives credible evidence that those products are not simply “chat model + search API”, but have extra structure/workflow that improves coverage and factuality.\n\n5. The error analysis is concrete. The paper shows that systems can misattribute authorship when stitching related papers, and can fabricate plausible-looking but nonexistent URLs. Having a benchmark that reliably surfaces these two failure modes is valuable."}, "weaknesses": {"value": "1. The current definition of “report quality” is relatively narrow. The benchmark mostly measures faithfulness-to-sources and citation correctness, but does not touch discourse-level aspects that are also important for survey-like reports (organization into sections, synthesis of competing lines of work, articulation of open problems, or taxonomic clarity). Since the task itself is framed as “academic survey–style reporting”, it would help to state more explicitly that the benchmark only targets the automatically verifiable slice of that task.\n\n2. The final evaluation set is only 100 tasks, obtained by down-sampling from 678 candidate surveys after domain balancing and expert checking. The paper does describe this at a high level, but the exact sampling policy (per-domain target, random seed, rejection criteria) is not spelled out. That makes it a bit harder to judge how sensitive the reported results are to this particular 100-sample snapshot.\n\n3. All systems show very low recall against the human survey reference lists, because the human surveys often cite over a hundred papers, while current agents typically cite 10–30. The authors conclude that recall should be treated as a secondary signal; this is reasonable, but it would be stronger if supported by a more fine-grained analysis (e.g., recall over recent papers, recall over core/methodological papers, recall over the top-k citations in the survey). Without that, it is hard to tell whether agents are missing genuinely important references or only the long tail.\n\n4. For non-cited factual statements the evaluation uses two Gemini-family models (Pro and Flash) in a voting setup. This is practical, but when the evaluated system is also a Gemini deep research variant, this creates a potential family-level bias (shared retrieval behavior, shared priors, shared error modes). A small sensitivity check with a model from a different provider, or with an open-source judge, would make the results easier to trust.\n\n5. The paper caps tool calls for baseline LLM+search systems at five, citing context-length considerations. That is a reasonable engineering constraint, but it does mean the baselines are operating under a stricter and more visible limit than the commercial systems whose internal agent graphs and tool budgets are hidden. A short ablation (5 vs. 10 calls) or a clearer justification of why 5 is representative would make this part of the comparison look fairer.\n\n6. The benchmark is currently STEM/AI/ICT–heavy because of the arXiv source. This is fine for a first version, but the paper should be a bit clearer that scores on ReportBench should not yet be read as “general deep research ability” across law, social sciences, or multi-lingual scholarship."}, "questions": {"value": "1. For the 100-task subset: could you provide the concrete per-domain target counts and the rejection/adjustment rules used by the four experts? This will help others reproduce approximately the same task mix and judge how much variance a different 100-task sample would introduce.\n\n2. You argue that recall should be considered a secondary signal because human surveys contain many more references than current agents can reasonably cite. Could you add a stratified recall analysis (e.g., by year, by citation rank in the survey, by section) to show that agents are at least recalling the most salient items?\n\n3. For the non-cited factuality check, would it be feasible to re-run a small subset with a non-Gemini judge (e.g., OpenAI or an open-source model with web access) to show that the main conclusions about Gemini vs. OpenAI do not hinge on using Gemini-family judges?\n\n4. Since the commercial systems were evaluated in a specific two-week window (July 14–25, 2025), how do you plan to keep ReportBench comparable over time? Will you version the prompts and scoring scripts, or provide page snapshots, so that future runs can be meaningfully compared even if the online products evolve?\n\n5. All outputs are normalized to URL-style citations to make automatic checking possible. How would your pipeline handle systems that produce BibTeX, arXiv IDs, or DOI-only references? A short note on extensibility here would help people adopt the benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Irm1B9oYV", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_cKfQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_cKfQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000570840, "cdate": 1762000570840, "tmdate": 1762916198562, "mdate": 1762916198562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReportBench is a benchmark that evaluates how faithfully deep research agents can generate reports/survey papers based on literature online, assessing both the quality of cited references and the factual accuracy of report content. First, research questions are reverse-engineered from expert-authored arXiv survey papers using LLMs at three levels of detail (sentence-level, paragraph-level, and detail-rich prompts). The original papers' bibliographies serve as ground-truth citations. Evaluation of the generated reports by different deep research agents works on two dimensions: citation quality (comparing retrieved references against ground-truth citations, precision and recall) and the factuality of statements (verifying cited and non-cited statements in the report using either the cited sources or with LLM agents with internet access.) Evaluations reveal that commercial deep research agents (like OpenAI Deep Research, and Gemini Deep Research) significantly outperform base models augmented with search tools, while all systems struggle with low recall, citation hallucinations, and maintaining factual consistency across statements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on a practical use case of deep research agents as survey paper writers, as it is easier to evaluate and with today's limitations in the deep research agents, could be a major use case of the agents for scientific research. Without such tools, to complete a survey paper would take significant manual effort (days/weeks).\n2. The conversion from paper back to the research question with LLMs achieved efficient data collection, which is more efficient than experts summarizing the research questions, and could scale up easily.\n3. The emphasis on preventing temporal leakage. The authors successfully identified the potentials for deep research agents to cite what would be impossible for the original paper and used prompting to cutoff any citations after the date of the release of that original paper. \n4. The framework appropriately distinguishes between cited statements (verified against actual sources via semantic matching) and non-cited statements (fact-checked via multi-model voting with 6 independent judgments), which acknowledges other websites as alternate sources of information, which makes sense since individuals might post their findings on platforms such as Medium, and companies could publish their research on their own websites.\n5. The evaluation exposes important findings, such as OpenAI Deep Research producing 5 times more cited statements than o3 despite similar retrieval performance, suggesting specialized writing/synthesis modules. These observations are valuable for understanding how commercial DR products differ from base models."}, "weaknesses": {"value": "1. While the paper includes temporal cutoff dates in prompts (Section 2.1.2) to prevent post-publication leakage, the authors acknowledge that 'the model disregards the imposed temporal constraints' during evaluation. Even with more intense wording, this is still essentially an suggestion and not anything that is enforced.\n2. This is about the analysis on low recall. It might not necessarily be bad, as a large amount of similar, and redundant research exists nowadays. Even if the deep research agents might have cited different papers, they might still be on the same topic and suggesting similar methods. Perhaps aggregating to find the most symbolic citations in the original paper, and also to find the most representative paper group in the ones used by the generated report before computing the semantic similarity between these two groups might be a thought. \n3. While LLM-as-a-judge promises efficiency, the paper lacks human validation to ground its automated evaluation. A human study measuring the correlation between LLM ratings and human expert ratings should be conducted so there can be more confidence in this benchmark's results. This is particularly important given that the evaluation relies on GPT-4o for semantic consistency checking and Gemini models for fact-checking; without human validation, it's unclear whether these automated judgments align with expert assessments of research quality.\n4. Time and cost efficiency are crucial factors for practical deployment of deep research agents. Measuring the average time, tokens generated, and monetary cost for each system when completing one report would provide valuable insights for practitioners. This information would complement the quality metrics and help users make informed decisions about which systems offer the best performance-cost trade-offs.\n5. Notably, the evaluation does not assess the content depth, synthesis, or insights of the generated survey/report. For survey papers, while the factuality of each claim is essential, what is equally cardinal is the insight and generalization that represents current trends on a topic, as well as potential future directions, expressed in compressed, synthesized language that aids researchers in obtaining information and inspirations efficiently. However, this framework only tests factuality, not the depth and insight of such claims. As such, the benchmark is vulnerable to gaming: an agent could simply restate obvious and loosely related facts, as simple as selecting one sentence at random from each retrieved reference paper. Such a report would score highly on all factuality metrics but would not qualify as a survey paper, as it would lack synthesis, critical analysis, and logical coherence."}, "questions": {"value": "Here are my suggestions:\n\n1. Implement post-hoc temporal filtering (similar to the suggestion for Dr.Mi-Bench). After report generation, automatically filter all retrieved citations to remove any papers published after the source survey's publication date. \n2. Implement citation clustering and semantic grouping: aggregate citations to identify the most representative papers in both the original survey and the generated report, then compute semantic similarity between these representative groups rather than exact citation matches. This would better assess whether agents achieve similar coverage through different (but valid) citation choices. For example, citing five recent papers on the same specific method might be equivalent to citing the seminal paper plus two comprehensive reviews.\n3. Conduct a human validation study on a stratified sample (e.g., 20-30 reports across different domains and quality levels). Have domain experts independently assess: (1) citation relevance and coverage, (2) semantic consistency of cited statements with sources, and (3) factual accuracy of non-cited statements. Calculate inter-rater agreement between human experts and automated LLM judges, report correlation coefficients, and identify systematic biases in LLM judgments. If strong correlation is found (>0.8), this validates the automated approach; if not, refine the evaluation prompts or consider hybrid human-LLM evaluation for high-stakes assessments.\n4. If expansion is feasible, prioritize adding survey papers from social sciences, humanities, and interdisciplinary fields to improve generalizability. If resource constraints prevent expansion, explicitly acknowledge the STEM bias as a limitation in the paper and discuss implications for generalizability. Additionally, consider releasing the data construction pipeline openly so the community can contribute domain-specific extensions, enabling organic growth of the benchmark across disciplines while maintaining consistent quality standards.\n5. Add a comprehensive efficiency analysis section that reports: (1) average completion time per report (in minutes), (2) total tokens generated per report, (3) estimated monetary cost per report (based on published API pricing where available), and (4) efficiency-quality trade-off analysis using Pareto frontier plots showing which systems offer optimal performance-cost balance. For systems where costs cannot be determined, clearly state this limitation. This would parallel the efficiency analysis in Dr.Mi-Bench and provide crucial practical information for real-world deployment decisions.\n6. There will be many methods to fix this issue of not evaluating the content quality and correlation with that of the original survey paper used to generate the research question, and I am actually not sure which would be the best. Please decide upon yourself, but I believe this is an important issue that determines the scientific value of a good survey paper. The other suggestions 1-5 might be hard to implement, but I'd like to suggest that you might at least address 6. due to its importance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EyCBIM8ukc", "forum": "zvL42fmtbG", "replyto": "zvL42fmtbG", "signatures": ["ICLR.cc/2026/Conference/Submission2340/Reviewer_bGMo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2340/Reviewer_bGMo"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission2340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174814538, "cdate": 1762174814538, "tmdate": 1762916198375, "mdate": 1762916198375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}