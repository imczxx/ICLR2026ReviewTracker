{"id": "BhfIg0tuti", "number": 24494, "cdate": 1758357387747, "mdate": 1763480636364, "content": {"title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning", "abstract": "Memory-efficient training of deep neural networks has become increasingly important as models grow larger while deployment environments impose strict resource constraints. We propose TraDy, a novel transfer learning scheme leveraging two key insights: layer importance for updates is architecture-dependent and determinable a priori, while dynamic stochastic channel selection provides superior gradient approximation compared to static approaches. We introduce a dynamic channel selection approach that stochastically resamples channels between epochs within preselected layers. Extensive experiments demonstrate TraDy achieves state-of-the-art performance across various downstream tasks and architectures while maintaining strict memory constraints, achieving up to 99\\% activation sparsity, 95\\% weight derivative sparsity, and 97\\% reduction in FLOPs for weight derivative computation.", "tldr": "We propose a dynamic channel selection algorithm to perform learning given a memory constraint.", "keywords": ["Efficient Learning", "Energy Saving"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6682b121dbab12d2c91b048ced9bcbf0928fa219.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TraDy (Training Dynamics), a memory-efficient fine-tuning approach for deep neural networks under strict resource constraints. The method leverages two key insights: (1) layer importance for updates is architecture-dependent and can be determined a priori, and (2) dynamic stochastic channel selection provides superior gradient approximation compared to static approaches. The authors introduce a Reweighted Gradient Norm (RGN) metric that accounts for both gradient magnitude and memory costs. TraDy operates by pre-selecting important layers based on architectural properties, then dynamically resampling input channels between epochs within these layers. Experiments on CNNs and transformers across multiple vision and NLP tasks demonstrate that TraDy achieves competitive performance while maintaining up to 99% activation sparsity, 95% weight derivative sparsity, and 97% reduction in FLOPs for weight derivative computation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea is both creative and elegant. The central challenge of memory-constrained training is that one cannot afford to compute the very importance metrics needed to decide what to update. TraDy's solution—decoupling the problem into a static, a priori layer selection and a dynamic, stochastic channel selection—is a novel and highly effective way to break this circular dependency.\n\n2. The authors compare TraDy against a comprehensive set of baselines, including static/dynamic and random/deterministic variants, across 3 CNN architectures, 7 vision datasets, 3 memory budgets, and 3 random seeds.\n\n3. Well-structured paper with clear problem formulation and notation."}, "weaknesses": {"value": "1. The main comparison is against Sparse Update (SU). More recent and relevant works are mentioned but not included in the main experiments. For example, how is the proposed method compared with \"SMT: Fine-Tuing Large Language Models with Sparse Matrices\".\n\n2. LoRA and other parameter-efficient fine-tuning methods are not discussed or compared.\n\n3. The authors acknowledge this limitation explicitly. The paper lacks actual on-device experiments showing latency, energy consumption, or throughput on target edge devices.\n\n4. The paper is compactly formatted and the template style is changed."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sDLylnJFoa", "forum": "BhfIg0tuti", "replyto": "BhfIg0tuti", "signatures": ["ICLR.cc/2026/Conference/Submission24494/Reviewer_LAwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24494/Reviewer_LAwL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662759927, "cdate": 1761662759927, "tmdate": 1762943102694, "mdate": 1762943102694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We wish to thank all reviewers for the time and consideration they have devoted to reviewing our work, as well as for their constructive feedback. We have made a series of modifications in the main paper according to the reviews and revisions are marked as blue text.\n\nWe address two major themes raised across multiple reviews below:\n\n## PEFT Methods Comparison\n\nReviewers izYB, eoHC, and LAwL raised the important concern regarding our lack of discussion and comparison with Parameter-Efficient Fine-Tuning (PEFT) methods. This is a valid criticism, and we have added a dedicated section in the appendix to discuss this topic in the revised submission.\n\nMost notably, Reviewer izYB mentioned Woo et al. (2025)'s work: \"PaCA: Partial Connection Adaptation for Efficient Fine-tuning.\" From our understanding, PaCA performs random selection of channels to update, thus allowing for both activation and weight memory savings alongside FLOPs reduction. However, they do not mention restriction of channel sampling to specific layers or resampling between epochs, which suggests that PaCA essentially corresponds to our **S-Full Random** baseline.\n\nIn their work, Woo et al. explain how other PEFT strategies like LoRA or DoRA:\n- Do not yield any activation memory savings\n- Result in storage memory overhead for the adapters\n- Incur inference computational overhead to process the forward pass of both the original layers and their adapters\n\nThese limitations make such adapter-based methods impractical for our extreme memory-constrained scenarios. Additionally, Woo et al. demonstrate that PaCA largely outperforms LoRA and DoRA in both memory and latency while yielding comparable accuracy.\n\nSince PaCA is essentially equivalent to our S-Full Random baseline, which stands as the **worst performing strategy** explored in our paper (as shown in Figure 5), and given the theoretical support provided in our method section, we can confidently conclude that our proposed TraDy outperforms other PEFT methods by transitivity. This is further validated by our comprehensive ablation studies showing that:\n1. Dynamic selection consistently outperforms static selection\n2. Layer-based prioritization (TopK) substantially improves over full-network random selection\n\n## Velocity Method Comparison\n\nReviewers eoHC and FTbs mentioned the lack of comparison with Quélennec et al.'s Velocity method. We have added Velocity as one of the baseline methods in the revised submission, demonstrating that it yields one of the best accuracy, only outperformed by TraDy while reaching significantly lower levels of activation sparsity and FLOPs savings.\n\nThis performance gap stems from a fundamental difference in approach: Quélennec et al.'s method focuses on reducing the number of updated parameters by selecting **output channels**, which leads to high activation storage requirements. Specifically, when a single neuron of a layer is selected for update, the **full activation memory of the previous layer** must be stored during the forward pass. This makes activation memory management highly suboptimal compared to our input channel perspective. Moreover, due to its reweighting focusing solely on weight memory, the Velocity selection strategy results in the selection of computationally expensive neurons to update resulting in higher FLOPs requirements than the other strategies explored in our paper (and that is without taking into account the computational overhead of computing the velocity metric of all neurons as well as the memory overhead of storing all activation values that are necessary to compute the Velocity metric)."}}, "id": "VjdAGNODbz", "forum": "BhfIg0tuti", "replyto": "BhfIg0tuti", "signatures": ["ICLR.cc/2026/Conference/Submission24494/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24494/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24494/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763480700272, "cdate": 1763480700272, "tmdate": 1763480700272, "mdate": 1763480700272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a dynamic training method (TraDy) for memory-constrained scenarios. . By stochastically resampling channels between epochs within architecturally important layers, the experiments demonstrate the effectiveness of the proposed method across various transfer learning scenarios. This article is well-written and well-organized."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is grounded in a solid theoretical foundation, featuring detailed derivations and rigorous argumentation.\n2. This article is well-written and well-organized."}, "weaknesses": {"value": "1.Fig. 2 is positioned too far from the corresponding text section. It is recommended to optimize the image layout.\n2.The baseline for existing studies compared in this paper is limited, with only SU available.\n3.It is recommended to highlight the best-performing results in Tables 1-5."}, "questions": {"value": "1.Although the author claims that lennec et al.’s implementation excludes activation memory from their budget calculations, it doesn't seem to affect adding it as an additional baseline to Table 1-5. Why wasn't this done?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oN5Dkn9JDA", "forum": "BhfIg0tuti", "replyto": "BhfIg0tuti", "signatures": ["ICLR.cc/2026/Conference/Submission24494/Reviewer_FTbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24494/Reviewer_FTbs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833529714, "cdate": 1761833529714, "tmdate": 1762943102449, "mdate": 1762943102449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents TraDy, a novel approach for fine-tuning pre-trained deep neural networks to downstream tasks under memory-constrained budgets. TraDy proposes a dynamic channel selection scheme that exploits the gradient sparsity of input channels to achieve both weight and activation sparsity during fine-tuning for downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis of the stochastic gradients' heavy-tailed behaviour during fine-tuning of a pre-trained network, relative importance of the network layers, consistency across downstream tasks and channel importance distribution is rigorously presented and well explained."}, "weaknesses": {"value": "- The paper is currently lacking comparison with the state-of-the-art fine-tuning reported in section 2 (i.e., Lin et. al (2022), Kwon et al. (2024) and Quèllenec et al. (2024)). \n- TraDy performances are reported in the main paper only for CNN models.  \n-  Unfortunately, the plots reported in Fig. 3, Fig.4 and Fig. 6 are not easy to read and to position within the main contributions of TraDy."}, "questions": {"value": "- Could you maybe compare the fine-tuning to downstream tasks accuracy of TraDy against the state of the art memory constrained fine-tuning methods cited in the related works Section 2? It would be interesting to compare the computational benefits vs fine-tuned accuracy of TraDy with those of other state-of-the-art fine-tuning methods. \n- Could you maybe add the results for vision attention-based models (i.e., ViTBase), BERT and RoBERTA in the main paper? \n- How does TraDy perform when compared to low-rank fine-tuning methods (i.e., LoRA)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dnHZUn53Xy", "forum": "BhfIg0tuti", "replyto": "BhfIg0tuti", "signatures": ["ICLR.cc/2026/Conference/Submission24494/Reviewer_eoHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24494/Reviewer_eoHC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916019337, "cdate": 1761916019337, "tmdate": 1762943102176, "mdate": 1762943102176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TraDy, a novel hybrid approach for memory-constrained fine-tuning of pre-trained models. The core idea is to combine (1) a static, a priori layer selection process, which the authors argue is architecture-dependent, with (2) a dynamic, stochastic channel selection process, which is argued to be task-dependent.\n\nThe methodology is justified by a solid analysis of training dynamics, identifying three key insights: the heavy-tailed nature of gradients, the task-invariance of layer importance, and the task-dependence of channel importance. Experimentally, TraDy demonstrates state-of-the-art performance against static sparse update baselines (like SU) across various vision tasks and architectures, achieving significant efficiency gains, including up to 99.5% activation sparsity and 97% reduction in weight gradient FLOPs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Strong Theoretical and Experimental Grounding:** The paper's primary strength lies in its thorough justification. The authors do not just propose a method but provide a robust analysis of *why* it should work. The experimental validation of the three key insights (heavy-tailed gradients, task-invariant layer ranks, task-dependent channel ranks) is convincing and provides a solid foundation for the proposed hybrid design.\n\n2.  **High Efficiency and Strong Performance:** The method achieves impressive results, demonstrating SOTA accuracy while operating under severe memory constraints. The reported efficiency metrics (e..g., >99% activation sparsity) are highly significant for the target application of on-device learning and data-drift adaptation.\n\n3.  **Rich Ablation Studies:** The paper's claims are well-supported by a comprehensive set of ablation studies. The comparison of `TraDy` (TopK Random) against alternatives like `Full Random` and static methods (like `S-Det RGN`) clearly isolates the benefits of both the static layer selection and the dynamic channel selection components, strengthening the paper's overall argument."}, "weaknesses": {"value": "1.  **Evaluation on Simple Tasks:** The empirical evaluation, while broad in terms of datasets (CIFAR-10/100, CUB, Flowers, etc.), is primarily limited to relatively simple, small-scale classification tasks. To truly validate the robustness and scalability of TraDy, an evaluation on more complex, large-scale benchmarks (e.g., ImageNet-1K) is necessary.\n\n2.  **Missing Comparison to Key PEFT Methods:** The paper's related work and experimental comparisons focus almost exclusively on *sparse update* methods (like SU). However, it overlooks a major and highly relevant category of Parameter-Efficient Fine-Tuning (PEFT) methods. A discussion and, ideally, a comparison against popular *adapter-based* methods that also target memory and compute efficiency would be crucial. Key missing comparisons include:\n    * **LoRA** (Hu et al., 2021)\n    * **DoRA** (Liu et al., 2024)\n    * **PaCA** (Woo et al., 2025)"}, "questions": {"value": "1.  **Scalability to Large-Scale Models (LLMs):** The paper successfully demonstrates TraDy on CNNs and, in the appendix, on ViT and smaller BERT models (though with mixed results for NLP). A key question is whether this framework can be effectively scaled to the fine-tuning of modern, massive-scale models, such as LLMs with >7B parameters? How does the static layer selection and dynamic channel selection interplay in such homogeneous, transformer-heavy architectures?\n\n2.  **Runtime Overhead of Dynamic Selection:** The paper clearly demonstrates the *memory* and *FLOPs* advantages of TraDy. However, it does not discuss the potential *wall-clock time* overhead. Does the dynamic resampling of channels at every epoch (including random number generation, index selection, and mask creation) introduce a non-negligible runtime cost compared to static methods (like SU) that perform this selection only once offline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m3hiedPz3L", "forum": "BhfIg0tuti", "replyto": "BhfIg0tuti", "signatures": ["ICLR.cc/2026/Conference/Submission24494/Reviewer_izYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24494/Reviewer_izYB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973728806, "cdate": 1761973728806, "tmdate": 1762943101840, "mdate": 1762943101840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}