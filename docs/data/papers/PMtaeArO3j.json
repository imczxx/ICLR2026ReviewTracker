{"id": "PMtaeArO3j", "number": 15643, "cdate": 1758253521928, "mdate": 1759897291478, "content": {"title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "abstract": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. Beyond empirical gains, we present several insights into the role of RL in symbolic table reasoning, highlighting the broader potential of formula-driven RL to advance reasoning capabilities in LMs.", "tldr": "Formula Tuning (Fortune) is a reinforcement learning approach that enables language models to perform symbolic table reasoning by deriving executable spreadsheet formulas.", "keywords": ["Table Understanding", "Formula Learning", "Symbolic Reasoning", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b56a6710c718a4af25d28cab1e182db3304825da.pdf", "supplementary_material": "/attachment/1295999dd93ed77668e5b6bb2a4afa08450296be.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces **FORTUNE**, a reinforcement learning (RL) framework for training large language models (LLMs) to generate **spreadsheet formulas** as an explicit symbolic reasoning mechanism for table-based question answering. Instead of relying on annotated for-mulas, FORTUNE uses **binary answer correctness** as the reward signal, teaching the model to derive executable formulas that yield correct results when evaluated. The authors pro-vide **theoretical justifications** showing the superiority of symbolic over textual reasoning and RL over supervised fine-tuning (SFT), followed by **experiments** across seven table reasoning benchmarks. Results indicate substantial gains over both SFT and prior state-of-the-art (SOTA) systems, with the FORTUNE++ variant (combining symbolic and textual reason-ing) outperforming larger closed-source models such as OpenAI o1."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel conceptual framing of formula-driven RL**\n   The idea of representing symbolic table reasoning through spreadsheet formulas, rather than the more typical SQL or Python program synthesis, is both novel and practically motivated. The paper convincingly argues that formulas provide a more lightweight and accessible sym-bolic interface, with empirical results validating this claim.\n\n2. **Comprehensive empirical evaluation**\n   Results span seven datasets (WikiTQ, TabFact, HiTab, FinQA, MultiHiertt, AIT-QA, TableBench), covering both in-distribution and out-of-distribution settings. The comparisons are thorough—against prompting, supervised, and hybrid baselines—and the improvements are both consistent and meaningful.\n\n3. **Clear analysis and ablation**\n   The paper goes beyond headline results to provide analysis of textual vs symbolic reasoning, the effects of RL vs SFT, and the relative strengths of SQL, Python, and formula-based reason-ing. These comparative results lend strong credibility to the claims.\n\n4. **Strong empirical impact**\n   Achieving performance that surpasses OpenAI o1 with a 7B open model is a compelling re-sult and likely to attract interest from both academia and industry."}, "weaknesses": {"value": "1. **Limited novelty in RL methodology**\n   The reinforcement learning setup is a relatively standard application of PPO with scalar cor-rectness reward. While effective, the technical innovation on the RL side is incremental. The contribution is more conceptual (using formulas as the medium) than algorithmic.\n\n2. **Limited theoretical depth**\n   The two presented theorems (symbolic ≥ textual, RL ≥ SFT) are intuitive restatements of re-ward-maximization principles rather than new theoretical insights. Proofs lack quantitative bounds or convergence guarantees and do not address RL instability or reward-variance issues.\n\n3. **Spreadsheet-formula limitations underexplored**\n   While the paper highlights the flexibility of formulas, it underplays practical drawbacks—limited scalability on large tables and potential maintainability issues compared to structured systems like SQL/Pandas.\n\n4. **Reward design and training stability**\n   The reward structure (1 / 0.2 / 0) is coarse and heuristic, with no reported analysis of training stability, reward variance, or convergence behavior. It remains unclear how robust FORTUNE is to sparse or noisy rewards and whether different reward formulations would yield similar improvements.\n\n5. **Experimental comparability**\n   Some baselines (e.g., TabAF, TableGPT) may differ in data scale or compute budgets, mak-ing it unclear whether observed improvements stem purely from reinforcement learning or broader training differences. The paper reports its own configurations but cannot guarantee parity across external baselines."}, "questions": {"value": "1. **Reward Sensitivity:**\n   Have you experimented with alternative reward functions (e.g., graded rewards based on par-tial correctness or token-level formula similarity)? How sensitive is FORTUNE’s performance to the specific (1 / 0.2 / 0) reward scheme? Also, why did you choose PPO over newer RL variants such as GRPO or REINFORCE++?\n\n2. **Training Stability:**\n   Did you observe instability, mode collapse, or reward hacking during RL optimization? How did you ensure convergence, given the sparse binary reward?\n\n3. **Fairness of Comparison:**\n   Can you clarify whether FORTUNE and baseline models were trained on comparable data scales and compute resources? Were all models fine-tuned on the same merged corpus de-scribed in Section 4.1?\n\n4. **Quantitative Effect of RL:**\n   Beyond accuracy, do you track executability rate, average formula length, or entropy to show how RL qualitatively changes formula generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dNDQooJaJB", "forum": "PMtaeArO3j", "replyto": "PMtaeArO3j", "signatures": ["ICLR.cc/2026/Conference/Submission15643/Reviewer_btVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15643/Reviewer_btVc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130726506, "cdate": 1761130726506, "tmdate": 1762925903097, "mdate": 1762925903097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a LLM framework that firstly use symbolic reasoning in reinforcement learning. By using the correctness of formula execution results as the reward signal, the framework reduces dependence on supervised formula annotations, guiding models to learn formula derivation through reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is the first to use symbolic reasoning in RL and gains huge enhancement.\n- The tabular dataset used is sufficiently large in scale and the experimental setup is detailed, effective and complete enough.\n- The proofs in appendix is detailed.\n\n- This paper is the first to integrate symbolic reasoning (via spreadsheet formulas) with RL for table reasoning tasks, yielding significant performance improvements.\n- The tabular datasets employed in this study are comprehensive, covering 7 diverse table reasoning benchmarks (e.g., WikiTQ, TabFact, FinQA). Moreover, the experimental setup is detailed and rigorous—including clear descriptions of model backbones, training protocols and evaluation metrics to ensure the reproducibility and validity of the results.\n- The theoretical proofs presented in the appendix are comprehensive and detailed."}, "weaknesses": {"value": "- Robustness is one of the key features in LLMs. However, this paper don't design experiments for this feature. When it comes to real world tables like NAN or nil data, it's important to know the results.\n- While the paper reports performance under zero-shot, supervised fine-tuning, and RL settings, the evaluations are limited to unimodal LLMs. With the rapid advancement of multimodal large language models which can process tabular data alongside other modalities (e.g., image charts, textual descriptions in table captions)—the paper fails to extend its scope to MLLMs. This omission limits the framework’s generalizability to increasingly common multimodal table understanding tasks."}, "questions": {"value": "- Symbolic methods are widely used in TableQA. For instance, GSM8K, a well-known mathematical reasoning dataset, can be converted into tabular formats to test table-based numerical reasoning. Given this, if the proposed Formula Tuning framework directly adopts formula-based parameterization instead of relying on LM-generated formula exploration, would this constitute a more effective modeling approach? If so, how might it impact the framework’s ability to handle complex multi-step reasoning tasks?\n- For zero-shot reasoning, GPT-series models (e.g., GPT-4o, GPT-4o-mini) exhibit notably stronger performance compared to open-source models like Qwen2.5-Coder7B. First, could the authors explain the potential reasons for this significant zero-shot gap? Second, have the authors tested other state-of-the-art models such as Gemini in zero-shot settings to further validate whether the observed performance trends are specific to GPT-series models? Third, including more powerful models in the baseline comparisons would help better contextualize the superiority of the proposed FT framework. Are there plans to supplement such experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VGZtr5pyRU", "forum": "PMtaeArO3j", "replyto": "PMtaeArO3j", "signatures": ["ICLR.cc/2026/Conference/Submission15643/Reviewer_LkyD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15643/Reviewer_LkyD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858123323, "cdate": 1761858123323, "tmdate": 1762925902590, "mdate": 1762925902590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training framework called Fortune to enhance reinforcement learning prompt models' ability to process tabular data. By having the model output executable formula outputs and using executability and correctness as reward functions, the model's tabular data processing capabilities significantly improve after reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed training method enables a 7B model to achieve strong tabular data processing capabilities, outperforming commercial models on certain datasets."}, "weaknesses": {"value": "Support for tabular operations is insufficient; features like sorting should be added to better align with practical software like Excel. The specific implementation of the Formula Executor lacks description. Exploration of OOD (Operations Over Data) symbols is missing."}, "questions": {"value": "1, Is there a specific reason for setting executable but incorrect results to 0.2 in the reward function? \n2, A description of the Formula Executor's concrete implementation should be included. \n3, It is recommended to test the model's performance before and after encountering data requiring operations not included in training to demonstrate its generalization ability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "irC4tkTY0s", "forum": "PMtaeArO3j", "replyto": "PMtaeArO3j", "signatures": ["ICLR.cc/2026/Conference/Submission15643/Reviewer_9DMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15643/Reviewer_9DMv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995875970, "cdate": 1761995875970, "tmdate": 1762925902239, "mdate": 1762925902239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}