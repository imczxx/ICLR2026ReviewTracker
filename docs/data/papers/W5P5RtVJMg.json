{"id": "W5P5RtVJMg", "number": 7333, "cdate": 1758016364167, "mdate": 1759897859310, "content": {"title": "To the Best of Trust: Full-Stage Trusted Multi-modal Clustering", "abstract": "Multi-modal clustering (MMC) aims to integrate complementary information from different modalities to uncover latent consistent structures and improve clustering performance.However, existing methods mainly rely on predictive(result) uncertainty to improve robustness, while often neglecting aleatoric(data) uncertainty introduced by sample noise and epistemic(model) uncertainty induced by model parameters and structural variations.To this end, we propose a novel Full-Stage Trusted Multi-modal Clustering (FSTMC) method. To the best of trust, we jointly utilize aleatoric, epistemic, and predictive uncertainties to optimize the model, learn more reliable feature representations, and obtain more reliable clustering results. In the representation learning stage, probabilistic modeling is employed to capture stable latent representations that account for aleatoric uncertainty, while structured stochastic perturbations are introduced to estimate epistemic uncertainty. In the clustering stage, we replace conventional feature-level fusion with an evidence-based strategy: soft labels from each modality are mapped into categorical evidence, class distributions are parameterized via a Dirichlet model, and dynamic cross-modal fusion is achieved through Dempster–Shafer theory. To mitigate overconfidence and modal conflicts, prior constraints guided by aleatoric and epistemic uncertainty are imposed, resulting in calibrated predictive uncertainty. Finally, we exploit predictive uncertainty to selectively incorporate pseudo labels for optimization, forming a virtuous cycle. Benchmark experiments on a large number of multi-modal datasets demonstrate that our approach significantly improves credibility and accuracy compared to state-of-the-art methods.", "tldr": "we propose a novel Full Stage Trusted Multi-modal Clustering (FSTMC) method.", "keywords": ["Multi-modal Clustering", "Trusted Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee737ab19416047f02fffcfb86ff6ad659946c53.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FSTMC, a multi-modal clustering method aiming to model and utilize three types of uncertainty (aleatoric $\\sigma^2$, epistemic $\\tau$, and predictive $u$) in a \"full-stage\" manner to enhance trustworthiness and robustness1. While the paper addresses an important problem and reports strong empirical results on some metrics, I have reservations regarding its technical soundness, methodological novelty, and experimental validity. The core methodology suffers from a circular dependency, key technical descriptions (especially regarding the VAE) are misleading, and the experimental results (specifically the ablation study) contradict the paper's central thesis. My recommendation is Reject."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is relatively clearly written."}, "weaknesses": {"value": "The paper's contribution seems derivative, as it primarily consists of an amalgamation of established methods rather than offering significant innovation.\n\nThe core methodology has a severe technical flaw and circular dependency. The authors' proposed \"virtuous cycle\" is logically circular. Point 1: The Aleatoric Uncertainty Loss ($L_{AU}$) relies on pseudo-labels that are filtered by the predictive uncertainty $u$. Point 2: The predictive uncertainty $u$ is generated via DS evidence fusion. Point 3: This DS fusion process is modulated by a \"reliability gate\" $r^m$. Point 4: This gate $r^m$ is defined as $r^{m}=\\exp(-(\\sigma^{m})^{2}-\\tau^{m})$, which directly depends on the aleatoric uncertainty $\\sigma^2$. In short: the learning of $\\sigma^2$ (via $L_{AU}$) depends on $u$, but the calculation of $u$ depends on $\\sigma^2$. This is an invalid circular argument. The authors fail to explain how this loop is computationally unrolled, making the core mechanism technically unsound.\n\nThe paper's description of its VAE-based method is misleading. The paper claims to \"adopt a VAE encoder\" for $L_{AU}$. However, the defined loss function $\\mathcal{L}_{a}=\\frac{1}{(\\sigma^{m})^{2}}CE(...) + log(\\sigma^{m})^{2}$ is not a VAE loss. A VAE objective must include a reconstruction loss (from a decoder) and a KL divergence term. The paper mentions no decoder, and its loss function is completely different, resembling an aleatoric uncertainty loss for classification (e.g., Kendall & Gal, 2017) rather than a VAE. \n\n\nThe paper makes misleading claims about being \"hyperparameter-free\". This is entirely false. The pseudo-label threshold $\\epsilon=0.6$ is a critical hyperparameter. The MC Dropout rate $\\zeta$ is a hyperparameter. The authors even dedicate Section 3.3 and Figure 3 to analyzing it. The number of MC passes T. The 1:1 weighting in $r^m$ is an implicit, fixed hyperparameter choice. These claims are highly misleading.\n\n The use of $L_{EU}$ is questionable. The paper uses $L_{EU}$ to \"penalize high-uncertainty predictions\" and suppress \"abnormally high uncertainty\". This is counter-intuitive to Bayesian principles, which aim to quantify uncertainty, not penalize it. Forcing the model to be confident on difficult samples may reduce trustworthiness, not improve it.\n\nThe visualization is insufficient. The t-SNE plot in Figure 5 is standard but uninformative for a paper on uncertainty. The authors should have provided t-SNE plots colored by the learned uncertainty values ($\\sigma^2$, $\\tau$, or $u$) to visually validate that the model correctly identifies ambiguous or noisy samples."}, "questions": {"value": "I don't have other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9MEPpZNHri", "forum": "W5P5RtVJMg", "replyto": "W5P5RtVJMg", "signatures": ["ICLR.cc/2026/Conference/Submission7333/Reviewer_2wU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7333/Reviewer_2wU2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714977975, "cdate": 1761714977975, "tmdate": 1762919453417, "mdate": 1762919453417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for clustering data that comes in different modalities. The method is intended to account for aleatoric and epistemic uncertainty in each modality, and to combine modalities in a principled way while accounting for such uncertainty. The key steps are roughly as follows:\n1. Each modality is first encoded using a VAE, i.e. each sample of each modality is mapped to a Gaussian of a certain mean (the feature representation) and variance (interpreted as the \"aleatoric uncertainty\"). A loss $L_{AU}$ is used that is essentially a kind of pseudo-clustering objective that tries to regularize appropriately by the AU.\n2. The variance in the feature representation across multiple MC-dropout samples is interpreted as the \"epistemic uncertainty\". A loss $L_{EU}$ (based on a Huber loss) is used to penalize feature representations that are excessively unstable.\n3. After such encoding, a Deep Divergence Clustering (DDC) approach is used to \"soft cluster\" the embeddings separately per modality. This results in $M$ soft clusterings, where $M$ is the number of modalities.\n4. Each soft clustering generated by DDC (one per modality) is mapped to an \"evidence\" vector. These are then combined across modalities into a Dirichlet distribution using Dempster-Shafer theory. This essentially yields the final multimodal clustering.\n\nExperiments on various multimodal datasets indicate that this method performs well as compared to prior methods when measured in terms of clustering accuracy and normalized mutual information."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Using Dempster-Shafer theory to combine information across modalities (by viewing each modality's clustering as providing one form of \"evidence\") seems like an interesting and novel idea. The empirical performance of the method also seems quite favorable at least on the datasets tested."}, "weaknesses": {"value": "The paper is unfortunately quite challenging to follow and suffers from a lack of conceptual clarity, with the presentation emphasizing engineering complexity over scientific insight. It combines a variety of techniques (VAEs, pseudolabeling, MC-dropout, Dirichlet evidence modeling, reliability gating, Dempster–Shafer fusion, Deep Discriminative Clustering, and many more) without a clear unifying principle or justification. The roles and interactions of these components are insufficiently motivated, and mechanisms such as the reliability gate appear somewhat ad hoc. Moreover, it is not always clear what models are being used in different components and what the learnable parameters of the whole system are.\n\nOverall, while the empirical results are strong, the paper does not manage to communicate the essential scientific insights in an accessible way, and I cannot recommend acceptance in this current state. In particular, the work would benefit from a clearer formulation of the underlying modeling assumptions, a simplified core mechanism, and a more focused and systematic presentation that keeps unnecessary complexity to a minimum. Also, the paper needs to provide a much more detailed comparison with related work."}, "questions": {"value": "One of the biggest things missing from the paper is a clear discussion of the modeling assumptions and the meaning of the various kinds of uncertainty. Specifically:\n- Aleatoric uncertainty is always a property of the true data distribution, not of the method. What links the estimated aleatoric uncertainty (i.e. the variance of the VAE output) to true aleatoric uncertainty (e.g. noise in the input, etc)? Why would noisy inputs have high variance? This requires a discussion of the modeling assumptions and what the \"true\" aleatoric uncertainty is under those assumptions.\n- Epistemic uncertainty being estimated via MC-Dropout variance is also insufficiently motivated. The original MC-Dropout line of work considered Bayesian models, where there is a full posterior over predictions. As far as I can tell, this work does not use a Bayesian network for the VAE. So what is the form of epistemic uncertainty that is being modeled? Why is MC-Dropout a useful proxy for it? (I suspect a more useful term for it is merely \"representation stability\", although I am again not sure why we care about stability under random perturbations induced by MC-Dropout.)\n- Typically, predictive uncertainty is a combination of aleatoric and epistemic uncertainty. However, in this paper it seems to be an orthogonal kind of uncertainty, and in fact is never formally defined. Can the authors provide a formal definition and discuss what exactly is being captured by it, and how it relates to AU and EU?\n\nSome other questions:\n- What exactly is the $L_{E}^{(m)}(x)$ term used in Eq 9? It is never formally defined. How does it relate to $\\tau(x)$ defined in Eq 8?\n- In Eq 10 and 11 and other places, what is $C$? Is it the same as $K$, the number of clusters? Is the number of clusters known a priori?\n- What exactly are the different models and the associated learnable parameters in the whole system? The loss function is simply written down as an algebraic expression without making it clear what the parameters are.\n- Minor: Why does the predictive uncertainty component of the loss (Eq 16) have subscript RU instead of PU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l8E9PJpaeF", "forum": "W5P5RtVJMg", "replyto": "W5P5RtVJMg", "signatures": ["ICLR.cc/2026/Conference/Submission7333/Reviewer_3Z3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7333/Reviewer_3Z3a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807147051, "cdate": 1761807147051, "tmdate": 1762919452422, "mdate": 1762919452422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical yet often overlooked issue in the field of multimodal clustering (MMC): end-to-end trustworthiness. The authors propose a unified framework that models three core types of uncertainty, such as aleatoric, epistemic, and predictive, and integrates them coherently throughout both the representation learning and clustering optimization stages. This is a novel and conceptually meaningful approach that carries substantial theoretical and practical significance. From a technical standpoint, the paper skillfully combines Variational Autoencoders (VAE), MC Dropout, and Dempster–Shafer Theory, providing a sound modeling basis for the quantification and utilization of each type of uncertainty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall structure and organization of the paper are well-designed and logically presented.\n- The motivation is clearly stated and easy to follow, effectively highlighting the significance of the work.\n- The figures are visually appealing, with color schemes that enhance readability and overall presentation quality.\n- The experimental section is thorough and well-executed, offering convincing empirical support for the proposed framework."}, "weaknesses": {"value": "- The proposed model integrates multiple components (e.g., VAE and MC Dropout, which requires T forward passes), implying that its training cost may be higher than that of some baseline methods. \n- In the description of L_EU, the paper mentions a “one-sided Huber robust”, but its explicit mathematical formulation is not provided in the main text. It would be beneficial to include a clear and intuitive explanation or a simplified expression of this penalty term in the main body of the paper to help readers better understand its role and behavior.\n- It would be highly beneficial to include a visualization of the different types of uncertainties—namely aleatoric, epistemic, and predictive—to illustrate their respective values across representative samples. Such a figure would help readers intuitively understand how the model distinguishes among these uncertainties and why it assigns specific uncertainty levels to different modalities or data instances."}, "questions": {"value": "- In Section 2.5, the paper states that “…we first map its latent representation … to an evidence e^m….” This evidence serves as the input to the entire evidence fusion module. However, the specific form of this mapping function is not clearly described. Is it implemented as a simple linear layer, or as a MLP with a nonlinear activation function (e.g., Softplus, to ensure the evidence values remain positive)?\n- The paper empirically demonstrates that the joint modeling of all three types of uncertainty yields the best performance. However, it lacks a deeper theoretical or intuitive analysis explaining why these three components are indispensable. Why are all three uncertainties necessary? \n- What are the potential risks if only aleatoric uncertainty is considered? \n- Conversely, what issues would arise if only epistemic uncertainty were taken into account?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQm3Z36nSl", "forum": "W5P5RtVJMg", "replyto": "W5P5RtVJMg", "signatures": ["ICLR.cc/2026/Conference/Submission7333/Reviewer_bv6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7333/Reviewer_bv6T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911403979, "cdate": 1761911403979, "tmdate": 1762919451663, "mdate": 1762919451663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel full-stage trusted multi-modal clustering method, namely FSTMC, a framework that integrates three types of uncertainty, including aleatoric, epistemic, and predictive, throughout the entire pipeline of representation learning, evidence fusion, and pseudo-label optimization, thus establishing an end-to-end trustworthy clustering paradigm. Generally, the proposed FSTMC method gives some new insights, theoretically well-founded, and extensive experimental results demonstrates significant performance improvements across multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes the novel “Full-Stage Trusted” paradigm, extending uncertainty learning from the representation learning stage to the clustering and pseudo-label optimization stages, thereby achieving an end-to-end trustworthy constraint.\n\n2. The study introduces model uncertainty into the variational autoencoder structure, enabling trustworthy modeling at the latent representation level and enhancing the reliability of feature learning.\n\n3. The model shows insensitivity to dropout rate parameter setting, maintaining stable convergence and high clustering accuracy across different configurations.\n\n4. Extensive experiments on multiple large-scale multi-modal datasets demonstrate that the proposed method significantly outperforms existing approaches in terms of clustering accuracy, validating its superior performance and broad applicability.\n\n5. Two types of ablation studies are conducted to comprehensively demonstrate the advantages of computing and utilizing uncertainty within the framework."}, "weaknesses": {"value": "1.\tThe selection basis for the pseudo-label confidence threshold \\varepsilon is not explained.\n2.\tThe paper does not explore the model’s performance under modality-missing scenarios, which limits understanding of its behavior in incomplete multi-modal environments.\n3.\tSome symbols are not consistently defined upon their first appearance, affecting the readability. Also, the text in some figure annotations (e.g., Figure 3) is too small, making it difficult to read."}, "questions": {"value": "1.\tHow is the pseudo-label confidence threshold \\varepsilon selected, and can a detailed description or experiment be added in the paper to demonstrate the rationality of its choice?\n2.\tIs uncertainty estimation performed within each modality, and what are the advantages of doing so?\n3.\tDoes any correlation exist among the three types of uncertainty, and how can it be interpreted?\n4.\tCan this framework be extended to other tasks, such as classification, and if so, how should it be implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DvGV3Tu0nl", "forum": "W5P5RtVJMg", "replyto": "W5P5RtVJMg", "signatures": ["ICLR.cc/2026/Conference/Submission7333/Reviewer_Zs29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7333/Reviewer_Zs29"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912128101, "cdate": 1761912128101, "tmdate": 1762919450945, "mdate": 1762919450945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}