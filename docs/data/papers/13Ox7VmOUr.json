{"id": "13Ox7VmOUr", "number": 1776, "cdate": 1756918578117, "mdate": 1763695044595, "content": {"title": "Causally-Enhanced Reinforcement Policy Optimization of Large Language Models", "abstract": "Large language models (LLMs) trained with reinforcement objectives often achieve superficially correct answers via shortcut strategies, pairing correct outputs with spurious or unfaithful reasoning and degrading under small causal perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a drop-in reward-shaping framework that augments policy optimization with a differentiable proxy for causal coherence along the generation pathway from prompt ($Z$) to rationale ($X$) to answer ($Y$). CE-PO estimates model-internal influence with Jacobian-based sensitivities, counterfactually hardens these signals to suppress nuisance cues, and fuses the resulting coherence score with task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single tunable between accuracy and coherence trade-off. The unified reward integrates with PPO/GRPO without architectural changes. Across reasoning benchmarks and causal stress tests, CE-PO reduces reward hacking and unfaithful chain-of-thought while improving robustness to correlation--causation flips and light counterfactual edits, all at near-parity accuracy. Experimental results across 4 datasets show that CE-PO improves accuracy over baselines by 5.49 % on average (up to 9.58 %), while improving robustness to correlation–causation flips and light counterfactual edits.", "tldr": "We propose Causally-Enhanced Policy Optimization (CE-PO), a reinforcement learning framework for large language models that integrates causal inference signals with standard rewards to improve reasoning coherence and mitigate reward hacking.", "keywords": ["Large Language Models", "Reinforcement Learning", "Causal Inference"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e564cdc0e8c46c08fa1b5a716df2b289c5a62cc0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to solve the prevalent problem of \"reward hacking\" in the reinforcement learning training of large LLMs. Specifically, models tend to generate answers that are superficially correct but are based on spurious or unfaithful reasoning. To address this, the paper proposes the Causally-Enhanced Policy Optimization (CE-PO) framework. The core of this method is to apply \"reward-shaping\" to the standard PPO/GRPO reward function by fusing scores from two dimensions: 1).  task accuracy e.g., using BERTScore.\n2) Causal coherence, A novel, Jacobian-based proxy signal used to measure the internal influence along the generation pathway $Z \\rightarrow X \\rightarrow Y$ (prompt $\\rightarrow$ rationale $\\rightarrow$ answer).\n\nThe method's primary innovation is \"Counterfactual Hardening\". The authors argue that the raw Jacobian signal (gradient sensitivity) is easily \"gamed\" by superficial features (like length or format). Therefore, they construct counterfactual inputs (e.g., by permuting tokens) to estimate and project out these \"nuisance cues,\" resulting in a more robust coherence score.\n\nFinally, the paper uses a Minkowski power-mean combiner to flexibly fuse the accuracy and coherence scores, and this unified reward $R_p$ is used for PPO/GRPO training. The experiments claim that this method improves accuracy and robustness across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The problem the paper addresses—reward hacking and unfaithful reasoning in RL training—is one of the most central and pressing challenges in LLM alignment today. Its motivation holds very high research value.\n2.  The paper does not stop at using raw gradient signals but acutely recognizes the fragility of the gradient signal itself (i.e., its susceptibility to being gamed by factors like length). The \"Counterfactual Hardening\" proposed to solve this is an intricate and novel idea. Attempting to separate semantic signals from nuisance signals via projection is an insightful approach.\n3.  The experiment in Figure 2 strongly demonstrates (at least in the authors' setup) that an un-hardened, Jacobian-based reward (Non-CF-GRPO) quickly learns to \"game\" the system by increasing generation length.\n4.  The ablation studies in Table 2 effectively demonstrate the necessity of the three Jacobian signals $S(Z,X)$, $S(X,Y)$, and $S(Z,Y)$ within the CE-PO framework. Removing any one of them leads to a catastrophic drop in performance, proving the importance of \"closing the causal loop\" within the authors' framework."}, "weaknesses": {"value": "I am not an expert in the RL area. Therefore, the following comments are based on my current understanding, and I welcome any corrections to potential misunderstandings on my part.\n\n1. The paper's title and text claim \"Causally-Enhanced,\" but its core proxy is the Jacobian matrix $J_{AB}$. A Jacobian measures \"local sensitivity,\" which is fundamentally a correlational metric, not a measure of causal intervention. A model might be sensitive to an input (high Jacobian score) merely because it learned a superficial shortcut, not because of true causal reliance. The paper lacks a comparison validating its proxy signal against causal measurement methods\n\n2. The method's core assumption is that permuting tokens is sufficient to create a counterfactual that preserves \"nuisance\" cues while destroying \"semantics.\" This definition is overly narrow and lacks justification. The paper provides no experiments showing why permutation is the best or only way, nor does it compare against other methods (e.g., deletion, replacement). \n\n3. The method projects out nuisance signals, which implicitly relies on an unproven assumption: that the \"semantic subspace\" and \"nuisance subspace\" are (approximately) orthogonal. However, in deep networks, valid signals (like detailed reasoning) and superficial features (like long text) are likely highly entangled. Projecting out the \"length\" signal could very well \"injure\" the legitimate reasoning signal at the same time.\n\n4. In RL training, reward calculation must be nearly instantaneous. To compute a single $R_p$, the method requires 3 Jacobians, and each of those requires $K=4$ counterfactual Jacobians to build the nuisance subspace. This means every reward calculation requires over a dozen gradient computations (VJPs), an extremely high overhead for practical training, especially on large models. The analysis of training cost will be helpful."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ttjf4kiZnk", "forum": "13Ox7VmOUr", "replyto": "13Ox7VmOUr", "signatures": ["ICLR.cc/2026/Conference/Submission1776/Reviewer_x5eh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1776/Reviewer_x5eh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666472129, "cdate": 1761666472129, "tmdate": 1762915887251, "mdate": 1762915887251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "Dear reviewers and AC:\n\nWe would like to thank the reviewers for their insightful feedback, which has provided us with excellent guidance towards crafting a more comprehensive and competitive paper! In the revised version, all changes are highlighted in blue. As highlighted by the reviewers, CE-PO directly addresses reward hacking and spurious correlation of CoT reasoning in RL for LLMs by explicitly promoting causal coherence along the prompt–rationale–answer pathway. We are encouraged that they find our framework “clearly written and well-motivated” , appreciate the novel Jacobian-based and counterfactually hardened plug in reward shaping, and stable consistent accuracy and robustness gains across models and reasoning benchmarks.\n\nWe are committed to sincerely addressing all of the reviewers’ concerns and have provided general responses to each of your comments below. In light of the feedback, we believe the enhancements we’ve made will resonate even more profoundly with our audience and meet an even higher standard, positioning our work to inspire a broader readership.\n\n(1) Quantitative analysis of the computation cost of Jacobian scores (reviewer 5jTM & x5eh)\n\nWe compute Jacobian-based scores via efficient vector–Jacobian products (vJPs), rather than materializing full Jacobian tensors. As detailed in Appendix C, this yields $O(Td)$ space and $O(C_{fwd})$ time complexity, while Hessian-based scores would require repeated Jacobian evaluations and $O((Td)^2)$ space, which is impractical.\nEmpirically, on Qwen-3-14B-Thinking (4×A100, 100 samples), a BERTScore baseline takes 1.20s / 28.00 GB per case, whereas our Jacobian step takes 6.98s / 29.77 GB (5.82× wall-clock overhead but only a 1.063× memory multiplier). These results indicate that CE-PO’s causal Jacobian signals remain practical for large-scale training.\n\n\n(2) Usage of BERTScore as factual accuracy may be less common in previous literatures\n\nWe clarify that CE-PO’s Minkowski combiner (Eq. 7) prefers a continuous base reward $\\(r_{\\text{base}}\\in[0,1]\\)$: our ablations studies (Table 2) show that discrete 0/1 LLM-as-judge rewards lead to unstable and ineffective learning. We also add references showing that BERTScore has already been successfully used as a reward model in RL for language models (e.g. RL4LM).\n \nTo further demonstrate the scalability of CE-PO pipeline, we added new experiments where we replace BERTScore by a standard pretrained reward model (DeBERTa-v3-large) as $r_{\\text{base}}$. Across three models (Qwen-3-4B-Thinking, Phi-3.5-mini-Instruct, Llama-3-8B-Instruct) and four benchmarks as in Table 1. CE(RM)-GRPO consistently outperforms GRPO-RM (see the new table in the rebuttal), demonstrating that CE-PO’s gains are not tied to BERTScore.  \n\nAdditionally, we also add results on two more challenging, reasoning-centric datasets (CodeXGLUE and HARP). CE-GRPO and CE-PPO consistently outperform both vanilla training and BERTScore-based GRPO/PPO, showing the scalability of more various datasets.\n\n(3) Clarification of motivation, methodology, and contributions\n\nReviewers raised concerns about our motivation (e.g. design of Z-X-Y diagram in cases like common sense rationale by oskw), methodology (Jacobian faithfulness and necessity by fWbQ) , and overall contribution (results are beyond length hacking and reshuffling counterfactual validity by x5eh and fWbQ). We now clarify point by point.\n\nMotivation:  we now clarify that CE-PO is not merely “scoring CoT” but *prescribing* faithful reasoning by explicitly separating input \\(Z\\), rationale \\(X\\), and answer \\(Y\\) and jointly optimizing \\(S(Z,X)\\), \\(S(Z,Y)\\), and \\(S(X,Y)\\) to enforce input-dependent, causally grounded, and self-consistent chains of thought. \n\nMethodology: Jacobian scores are used as first-order Taylor approximations to direct causal effects, chosen as a practical alternative to computation expensive and unstable higher-order estimators. Empirically, they are both necessary and robust: ablating any Jacobian term leads to collapse, whereas adding Gaussian noise or moderate reweighting only mildly harms performance. \n\nContributions: Our counterfactual hardening is designed to preserve nuisance statistics that drive reward hacking while destroying semantics; in this setting, permutation is more appropriate than deletion, semantic replacement, or [PAD] masking, and our projection step is justified by its observed gains. Conceptually, length hacking is only one visible symptom; our main contribution is a causal coherence objective that aligns training with faithful CoT. CE-PO consistently improves both accuracy and all three coherence signals across models and benchmarks, and continues to outperform strong GRPO/PPO baselines even when replacing BERTScore with a reward model and moving to harder datasets (Figure 8, Table 1–2, Figure 5).\n\n\nThank you very much.\n\nwith best Regards,\n\nAuthors"}}, "id": "E8Eb4wkZpU", "forum": "13Ox7VmOUr", "replyto": "13Ox7VmOUr", "signatures": ["ICLR.cc/2026/Conference/Submission1776/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1776/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1776/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763340805991, "cdate": 1763340805991, "tmdate": 1763340805991, "mdate": 1763340805991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Causally-Enhanced Policy Optimization (CE-PO), a training-time reward shaping scheme for LLMs that augments standard PPO/GRPO with a differentiable proxy for causal coherence along the prompt-rationale-answer pathway. The method measures Jacobian-based sensitivities, hardens them with source-side counterfactual permutations to filter nuisance directions, and fuses the resulting scores with an accuracy reward via a power-mean combiner. Experiments report accuracy gains over PPO/GRPO baselines, reductions in length-driven reward hacking, and modest out-of-distribution improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presentation is clear and easy to read.\n\n2. The paper turns internal, differentiable signals (block Jacobians of span log-likelihoods with respect to upstream token embeddings + counterfactual hardening) into a scalar reward, which is a clear path making “how” matter during RL.\n\n3. The proposed reward shaping can be plugged into PPO/GRPO without architectural changes, and the power mean combiner exposes a single knob to trade accuracy against coherence."}, "weaknesses": {"value": "1. **Dependence on BERTScore as the base accuracy reward:** BERTScore is convenient and differentiable, but it is not a task-ground truth for many reasoning datasets and can mis-rank semantically brittle outputs. Results with verifiable rewards in common reasoning-heavy domains like math and code should be included.\n\n2. **Beyond length hacking:** The paper shows resistance to length-based reward hacking, which aligns with known reward-model over-optimization effects. However, claims in the paper, e.g., in abstract \"reduces unfaithful chain-of-thought\" is not well-supported by results beyond length hacking.\n\n3. **Faithfulness of gradient proxies:** The method relies on first-order Jacobians as a stand-in for directed causal effects. These signals are known to be fragile in some contexts, and the projection-based “direct-effect” isolation could over- or under-correct depending on geometry and scale [1].\n\n[1] Basu, Samyadeep, Phil Pope, and Soheil Feizi. \"Influence Functions in Deep Learning Are Fragile.\" International Conference on Learning Representations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WYVi346smy", "forum": "13Ox7VmOUr", "replyto": "13Ox7VmOUr", "signatures": ["ICLR.cc/2026/Conference/Submission1776/Reviewer_fWbQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1776/Reviewer_fWbQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958434416, "cdate": 1761958434416, "tmdate": 1762915886977, "mdate": 1762915886977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Causally-Enhanced Policy Optimization (CE-PO), a novel, drop-in reward-shaping framework designed to mitigate reward hacking and unfaithful reasoning in LLMs trained with RL. The method augments the standard policy objective (like PPO/GRPO) with a differentiable proxy for causal coherence along the prompt-to-rationale-to-answer ($Z \\rightarrow X \\rightarrow Y$) generation pathway. This is achieved by computing Jacobian-based internal influence signals, applying counterfactual hardening via residualization to remove spurious sensitivities (e.g., to output length) , and finally fusing the coherence score with task accuracy using a tunable Minkowski (power-mean) combiner. Experiments show that CE-PO successfully reduces reward hacking, improves robustness to causal perturbations, and yields an average accuracy improvement of 5.49% over baselines on reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a novel, differentiable, and annotation-free method using Jacobian-based sensitivities and counterfactual hardening to create an internal proxy for causal coherence across the $Z \\rightarrow X \\rightarrow Y$ generation path.\n\nThe core technique of counterfactual hardening successfully and empirically prevents reward hacking from spurious non-semantic factors like generation length, a known major issue in outcome-centric RL for LLMs.\n\nCE-PO demonstrates consistent performance gains, improving average accuracy by 5.49% across four datasets and multiple LLM backbones, while also improving robustness to causal stress tests."}, "weaknesses": {"value": "Calculating the block Jacobians and performing the counterfactual hardening process, especially with multiple source-side counterfactuals ($K$), significantly increases the computational cost of the reward function computation compared to a simple task accuracy reward.\n\nThe composite reward structure, which relies on a Minkowski combiner with four weighted terms and a tunable exponent $p$, introduces high complexity and potential sensitivity to the extensive hyperparameter search required for the optimal weights ($w_i$) and $p$.\n\nThe core coherence signal relies on Jacobians, which are only a first-order Taylor approximation of the Direct Causal Effect (DCE). While computationally efficient, this approximation may limit the precision of the causal alignment and potentially restrict the model's ability to capture complex, higher-order causal dependencies."}, "questions": {"value": "Can the authors provide a quantitative analysis (e.g., wall-clock time overhead or GPU memory usage multiplier) of the Jacobian and counterfactual hardening steps when scaling up to much larger models (e.g., >30B parameters) where gradient computation becomes significantly more resource-intensive?\n\nThe Minkowski combiner introduces four weights ($w_i$) and the exponent $p$. Is there a principled method or guideline for selecting these hyperparameters to achieve a desired trade-off, or must this be treated as a purely empirical tuning problem for each new task and model architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r4kg9XoyJi", "forum": "13Ox7VmOUr", "replyto": "13Ox7VmOUr", "signatures": ["ICLR.cc/2026/Conference/Submission1776/Reviewer_5jTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1776/Reviewer_5jTM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991810228, "cdate": 1761991810228, "tmdate": 1762915886819, "mdate": 1762915886819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Causally Enhanced Policy Optimization, which is a reward framework that modifies policy optimization with a proxy for causal coherence. The authors explain how they use counterfactual hardens and a power-mean combiner to improve this modification for empirical use, and test on a series of models to find improvements in both multistep arithmetic and more general domains such as legal and QA problems."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and the motivation for coherence is reasonable. Furthermore, the question tackled makes important steps towards improving a core aspect of LLMs - CoT faithfulness. \n\nThe modifications (e.g., counterfactual hardening) demonstrates that the authors have put significant efforts into investigating this space in order to make it work."}, "weaknesses": {"value": "1. The current chain is Z -> X -> Y. However, rationale may sometimes exist independently of question; e.g., rules such as common sense. Can the authors clarify how this interacts with the training signal?\n\n2. For the results in Figure 2, can you also show that high-length outputs simply have higher rewards, instead of just showing that the chosen generations are longer? This would directly confirm the findings that there is reward hacking.\n\n3. Line 285, \"To avoid random guesses\": If I understand correctly, this is also because you need to generate X\n\n4. For your experiments, you are using GPT-4o-mini for LLM as a judge: can you measure inter rater reliability with either a stronger model or some human judgments?\n\n5. Your core baseline is PPO and GRPO with BERTScore reward. To the extent of my knowledge, this isn't a common reward signal used for these optimization procedures. This is illustrated by PPO and GRPO baselines reducing accuracy on truthfulQA (Figure 5). Can you justify your choice of BERTScore, or are there other baselines you can consider?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ghmAoVUs3f", "forum": "13Ox7VmOUr", "replyto": "13Ox7VmOUr", "signatures": ["ICLR.cc/2026/Conference/Submission1776/Reviewer_oskw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1776/Reviewer_oskw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184414453, "cdate": 1762184414453, "tmdate": 1762915886660, "mdate": 1762915886660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}