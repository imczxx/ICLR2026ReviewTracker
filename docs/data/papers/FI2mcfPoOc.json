{"id": "FI2mcfPoOc", "number": 17310, "cdate": 1758274585307, "mdate": 1759897183041, "content": {"title": "How to Mitigate the Distribution Shift Problem in Robotics Control: A Robust and Adaptive Approach Based on Offline to Online Imitation Learning", "abstract": "Distribution shift in imitation learning refers to the problem that the agent cannot plan proper actions for a state that has not been visited during the training. This problem can be largely attributed to the inherently narrow state-action coverage provided by expert demonstrations over the full environment. In this paper, we propose a robust offline to adaptive online imitation learning framework that handles the distribution shift problem in a lifelong, multi-phase scheme. In the offline learning phase, we leverage supplementary demonstrations to broaden the state-action coverage of the policy by utilizing a discriminator to effectively train the policy with supplementary demonstrations, thereby enhancing the robustness of the policy to distribution shift. In the subsequent online inference phase, our framework detects the occurrence of distribution shift and conducts self-supervised imitation learning from online experiences to adapt the policy to the online environments. Through extensive evaluations in MuJoCo environments, we demonstrate that our method exhibits better robustness to distribution shift and better adaptation performance to online environments than the baseline algorithms, which indicates superior performance of our framework against the distribution shift.", "tldr": "", "keywords": ["Imitation learning", "Reinforcement learning", "Continual learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f62fc724b9c5740dbf325b673e439d9003c169d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces \"RAIL\", a robust offline to adaptive online imitation-learning framework for handling distribution shift in control. In the offline portion, RAIL augments narrow expert demonstrations with supplementary non‑expert/sub‑optimal trajectories and learns a discriminator‑weighted objective for BC. (Authors add a regularization term to stabalize early training) The results are evaluated on MuJoCo (Hopper, HalfCheetah, Walker2d, Ant) with Gaussian state noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper uses the same mechanism for offline to online imitation learning as in they use a discriminator to produce weights for BC both offline which includes expert + supplementary demos as well as online which includes agent’s own experience. This is important because it's one shared pipeline (the behavior of which we can track and predict) as opposed to two unrelated stages.\n- One original contribution (which is an incremental but helpful improvement) is the regularized discriminator. It's a principled stabilization of the optimality estimator used for BC weights and is especially helpful early in training.\n- Another important contribution is shift detection + update‑time management. It's an intuitive way of formalizing shifts and updates occur only when low‑κ states continue and is relatively easy to implement."}, "weaknesses": {"value": "- The paper states a fixed 3‑hour wall‑clock budget on a single RTX 3090 per environment, but sample‑efficiency and stability for TRPO‑style (GAIL) vs. weighted‑BC can be different based on environment interactions and optimizer steps. Actionable: report environment steps, updates per second, and episode counts for each baseline; include curves vs. steps. \n- Although the paper does show an ablation where their regularizer is added to ISWBC’s discriminator loss, improving returns, and discriminator evaluation loss curves vs. ISWBC, this only demonstrates the regularizer helps relative to no-regularizer setup. The contribution would be stronger if there were comparisons to alternative stabilizers commonly used for density‑ratio/discriminator learning under overlap. \n- The paper defines κ(s) and says online learning triggers when κ(s) falls below a threshold $κ_{TH}$ and persists for some steps (UTM), but it does not state the actual $κ_{TH}$ value, the persistence length, or provide any sensitivity sweep detection analysis."}, "questions": {"value": "- State the exact $κ_{TH}$ value and UTM persistence length, how they were chosen, and add sensitivity sweeps showing returns and adaptation latency vs. these values.\n- Sweep the regularizer weight schedule (start value, decay) and show impact on discriminator calibration and policy performance. Also compare your regularizer against at least one GAN‑style stabilizer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KbAEtEyX01", "forum": "FI2mcfPoOc", "replyto": "FI2mcfPoOc", "signatures": ["ICLR.cc/2026/Conference/Submission17310/Reviewer_49GV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17310/Reviewer_49GV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857194836, "cdate": 1761857194836, "tmdate": 1762927245105, "mdate": 1762927245105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach in the offline-to-online imitation setting to tackle the issue of distribution shift. In the offline phase, the method employs an additional objective to regularize the discriminator output, resulting in faster convergence. In the online phase, the method leverages the discriminator to identify distribution shifts and finetunes the policy on online data with conservatism. Experiments are conducted in Mujoco simulation with existing benchmark datasets.\n\nOverall, the motivation is clear, and the method is sound. Experiments on simple test-time distribution shift show moderate improvements over prior methods. However, I have a few concerns that I hope the authors can help address."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is easy to follow, and the overall presentation is good.\n* The online objective to adaptively update the policy and discriminator seems novel, although more ablations could be done.\n* Offline-to-online imitation learning is a challenging problem, and tackling distribution shifts in BC is important.\n* Results across different environments show consistent improvement over the baselines."}, "weaknesses": {"value": "* Distinction from prior work in the offline phase is not made very clear. How does the $L^{off}_{disc}$ loss differ from (Li et al. 2023 [1])? If it is from prior work, then the authors should not claim it as their approach.\n* The offline regularization seems a bit too minor for the method. I am not convinced that adding the regularization helped with performance, as the discriminator loss from Figure 2 seems to converge to a similar point, and there are no ablation studies on this component.\n* The online portion of the method is interesting, but a major limitation is that with the current formulation, states that are within coverage of offline data will be used to update the discriminator/policy, but states that are OOD will be heavily discounted. This formulation could be limited when the coverage of offline data is small.\n* The setting for distribution shift is too easy. The noise is only on the observations and not on actions or dynamics. Further experiments on other types of test-time distribution shift can strengthen the authors' claim.\n* The authors should also consider more challenging baselines such as PWIL[2] that outperforms GAIL.\n\nReferences\n\n[1] Li et al. Imitation Learning from Imperfection: Theoretical Justifications and Algorithms (2023)\n\n[2] Dadashi et al. Primal Wasserstein Imitation Learning (2021)"}, "questions": {"value": "* How to select the threshold $\\kappa_{TH}$?\n* Does the method work when the coverage of offline data is not sufficient for most online states that the agent encounters?\n* What does the distribution of $\\kappa(s)$ look like at different levels of distribution shifts?\n* What are the assumptions/reasonings behind modeling state visitation distribution with Gaussian mixture models?\n* Does the method work well in robotic manipulation domains, where the OOD issue can be essential to task performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKHoQvxA3m", "forum": "FI2mcfPoOc", "replyto": "FI2mcfPoOc", "signatures": ["ICLR.cc/2026/Conference/Submission17310/Reviewer_NPpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17310/Reviewer_NPpk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886419899, "cdate": 1761886419899, "tmdate": 1762927244797, "mdate": 1762927244797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a robust offline-to-adaptive online imitation learning framework that employs a lifetime multi-stage approach to address the distribution shift problem. In the offline learning phase, supplementary demonstrations are utilized to effectively train the policy through a discriminator, thereby expanding the policy's state-action coverage and enhancing its robustness to distribution shifts. In the subsequent online inference phase, the framework detects distribution shifts and leverages online experience for self-supervised imitation learning to adapt the policy to the online environment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The experimental part is very solid."}, "weaknesses": {"value": "The theoretical part is relatively lacking."}, "questions": {"value": "Have you considered some error estimation and convergence analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NdFwHKVXso", "forum": "FI2mcfPoOc", "replyto": "FI2mcfPoOc", "signatures": ["ICLR.cc/2026/Conference/Submission17310/Reviewer_BVGf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17310/Reviewer_BVGf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967197842, "cdate": 1761967197842, "tmdate": 1762927244451, "mdate": 1762927244451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAIL, a two-phase imitation learning framework combining a regularized discriminator for stable offline learning with an online adaptation phase triggered by distribution-shift detection. The approach seeks to bridge the gap between offline and online IL, aiming to maintain robustness under domain shifts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe distribution shift issue in imitation learning is a well-documented challenge, and the attempt to connect offline IL with adaptive online learning is conceptually reasonable.\n\n2.\tThe empirical section is reproducible and includes reasonable baselines like Stable-BC and CCIL. The code structure and methodology show attention to engineering detail.\n\n3.\tThe offline-online integration is ambitious, and the inclusion of shift detection rather than naive fine-tuning demonstrates awareness of overfitting risk."}, "weaknesses": {"value": "1.\tThe regularization term lacks formal motivation. The authors suggest it stabilizes the discriminator but provide neither analytical intuition (e.g., bias–variance trade-off analysis) nor empirical sensitivity results. Without such grounding, it reads as a tuning trick rather than a contribution.\n\n2.\tThe κ(s) detection metric, derived from GMM density averaging, lacks statistical rigor. There is no analysis of false alarms or missed detections. Better-established OOD metrics (e.g., likelihood ratio, Mahalanobis distance) could serve as baselines but are absent.\n\n3.\tReported improvements are small and lack statistical significance analysis. Moreover, all environments are low-dimensional continuous control benchmarks (e.g., Hopper), which do not exhibit meaningful distribution shift."}, "questions": {"value": "1.\tHow does RAIL perform on visual imitation datasets (e.g., DMControl suite with pixel observations)?\n2.\tWould integrating uncertainty estimation (e.g., ensemble critic variance) improve adaptation reliability?\n3.\tCould you quantify computational overhead introduced by online adaptation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x1Lc0Y2nei", "forum": "FI2mcfPoOc", "replyto": "FI2mcfPoOc", "signatures": ["ICLR.cc/2026/Conference/Submission17310/Reviewer_nWag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17310/Reviewer_nWag"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995880458, "cdate": 1761995880458, "tmdate": 1762927243979, "mdate": 1762927243979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a unified framework that seamlessly integrates offline and online imitation learning to improve adaptability across both phases.In the offline IL phase, the authors propose a GMM-based regularization technique to stabilize and enhance the discriminator’s learning, thereby improving the policy’s robustness to distribution shift. In the online IL phase, they introduce a metric to quantify the degree of covariate shift, enabling the policy to adapt its learning behavior accordingly."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes an interesting attempt to overcome the inherent limitations of offline imitation learning, particularly its vulnerability to distribution shift and limited state-action coverage. By integrating supplementary demonstrations and discriminator-based regularization, the proposed framework aims to enhance robustness and generalization beyond standard offline IL settings. The unified design that bridges offline and online IL provides a practical perspective on how policies trained from static data can be further adapted to dynamic environments."}, "weaknesses": {"value": "**1. Unclear problem definition**\n    \nThe paper lacks a precise formulation of the problem. While it claims to address the *distribution shift problem* in imitation learning, it remains unclear *which type* of shift is considered (e.g. covariate, transition, or reward, ... ). The introduction loosely connects “lifelong” or “continual” adaptation with robustness, yet the actual experiments only inject Gaussian noise into the state to emulate distribution shift. Consequently, the notion of “robustness” and “offline-to-online adaptation” is not concretely defined or theoretically motivated. Moreover, the paper does not clearly analyze why conventional offline IL fails when applied to online adaptation (e.g., due to pessimism, limited coverage, or exploration constraints).\n    \n**2. Insufficient motivation and justification of the method**\n    \nThe proposed discriminator regularization term $L_{\\mathrm{reg}}$ lacks a clear theoretical basis. It heuristically enforces the discriminator output to approximate the empirical ratio of expert and supplementary densities, but the paper provides no formal analysis of how this term alleviates instability or improves density-ratio estimation. Similarly, the shift detection criterion $\\kappa(s)=\\frac{p_E(s)+p_S(s)}{2}$ is defined as a simple average of state likelihoods, without justification as a principled measure of covariate shift. The proposed approaches are heuristic and are only empirically validated.\n    \n**3. Limited experimental validation**\n    \nThe evaluation setting is overly narrow. The only distribution shift considered is additive Gaussian noise on the state, which does not reflect more realistic transition or policy shifts in online control. The adaptation scenario is therefore limited to minor perturbations rather than genuine task or environment changes. Moreover, the comparisons omit several relevant robust IL baselines (e.g., robust IL methods). As a result, it is difficult to conclude that the proposed framework achieves general robustness or effective online adaptation."}, "questions": {"value": "Q1. The paper repeatedly mentions *distribution shift* as the central issue, yet it remains unclear which specific distributions are shifting. Can the authors formally characterize this shift—for example, whether it refers to the discrepancy between $p_{\\text{train}}(s,a)$ and $p_{\\text{test}}(s,a)$, between $p_E(s)$ and $p_\\pi(s)$, or between transition dynamics $P_{\\text{train}}(s’|s,a)$ and $P_{\\text{test}}(s’|s,a)$?\n\nQ2. Given the definition of *distribution shift* provided in Q1, could the authors elaborate on how the proposed mechanisms (either in the offline regularization or in the online adaptation stage) specifically addresses the distribution shift?\n\nQ3. Can the proposed method handle dynamics shifts beyond simple Gaussian noise injection? It would be helpful to clarify whether the approach can generalize to other types of dynamics changes or environment variations. If possible, the method should also be empirically evaluated and compared with relevant baselines under such dynamically changing settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0uYRpP4t5A", "forum": "FI2mcfPoOc", "replyto": "FI2mcfPoOc", "signatures": ["ICLR.cc/2026/Conference/Submission17310/Reviewer_LJzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17310/Reviewer_LJzn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146400763, "cdate": 1762146400763, "tmdate": 1762927243547, "mdate": 1762927243547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}