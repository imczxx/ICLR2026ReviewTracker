{"id": "UDeDdxZOwl", "number": 2732, "cdate": 1757227266428, "mdate": 1759898130629, "content": {"title": "MathViz-Bench: Evaluating Text-to-Image Models on Visually Solving Math Problems", "abstract": "We present MathViz-Bench, a comprehensive benchmark for evaluating Text-to-Image (T2I) models' capability to visualize step-by-step solutions for high school mathematics problems.\nMathViz-Bench comprises 500 carefully curated problems sampled from levels 1-3 of the MATH dataset, spanning seven mathematical domains: Prealgebra, Algebra, Number Theory, Counting \\& Probability, Geometry, Intermediate Algebra, and Precalculus.\nWe transform these problems into prompts requiring models to generate visual step-by-step solutions with proper mathematical notation and logical flow. \nOur automated assessment pipeline employs three metrics: Sequential Consistency for logical flow, Symbol Fidelity for notation accuracy, and Mathematical Correctness for calculation validity, each scored 0-5. \nOur evaluation shows that models with built-in language understanding (GPT-Image-1: 84.05\\%, Gemini-2.5-Pro: 75.13\\%) perform much better than diffusion models (FLUX1.1-Pro: 35.23\\%, WAN2.2: 28.05\\%, Stable Diffusion 3.5 Ultra: 22.05\\%), achieving 2-3 times higher scores.\nAll models exhibit high Symbol Fidelity (1.81-4.57) but fail at Mathematical Correctness (0.65-4.05), indicating they process mathematical symbols as visual patterns rather than semantic operators. \nDiffusion models demonstrate complete difficulty invariance and 33.8\\% critical failure rates, confirming absence of mathematical reasoning. \nThese findings establish that mathematical visualization requires architectural integration of symbolic reasoning with visual generation, beyond current T2I capabilities.", "tldr": "", "keywords": ["Text-to-image models", "foundation model", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ea4aca17939da98e657eb5d20dee68015c34f88.pdf", "supplementary_material": "/attachment/81e19955d88ce103dc65f73bef5ceebe6525e3d6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new benchmark, named MathViz-Bench, to assess the abilities of text-to-image (T2I) models for accurate and consistent rendering step-by-step solutions of high school mathematics problems. To do this, the authors sample 500 problems from existing MATH dataset, and then utilize the existing five T2I models to generate the visualizations. Finally, they use Grok4 as a judge to assess the qualities of generated results. In experimental result, it was shown that multimodal models (GPT, GEMINI) perform better than diffusion models (FLUX, SD, WAN)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper tackles the missing gap of the current benchmarks and studies the interesting task of visualizing math solutions.\n- The paper presents comprehensive analysis of evaluation comparisons with five T2I models.\n- Writing is easy to read and good, in general."}, "weaknesses": {"value": "- Even though it is interesting, the significance of the proposed benchmark is questionable. Visualizing math solutions may not be necessary as it is possible to directly rendering them by using LaTeX. This may be easier to understand and generate more accurate visualizations. \n- This paper generates benchmark by using T2I models. But there is no human annotation/evaluation for it, so the value of this created benchmark is also questionable. \n- Furthermore, there is no human evaluation for TI2 model outputs, which limits the effectiveness of the presented analysis.\n- In addition, the findings from the experiment show no surprise. It is expected that multimodal models perform better than diffusion models and no further analysis or suggestion to improve was presented.\n- Finally, this paper lacks to present a new T2I model to improve the current T2I models. It is expected, with the newly proposed benchmark, to present a new improved T2I model perform better on the new benchmark."}, "questions": {"value": "Please see and address the weaknesses above.\n\n- Typo\n#042: bosheah2025challenges -> typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMVDPikhvX", "forum": "UDeDdxZOwl", "replyto": "UDeDdxZOwl", "signatures": ["ICLR.cc/2026/Conference/Submission2732/Reviewer_ytbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2732/Reviewer_ytbB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878217521, "cdate": 1761878217521, "tmdate": 1762916351115, "mdate": 1762916351115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MathViz-Bench, a benchmark for evaluating text-to-image models on sequential mathematical visualization. The benchmark contains 500 high-school problems from MATH levels 1 to 3 and uses three metrics to assess sequential consistency, symbol fidelity, and mathematical correctness via an automated VLM judge. Experiments demonstrate that language-integrated models significantly outperform diffusion-only approaches, exposing a substantial gap between visual polish and symbolic precision while offering valuable guidance for improving model design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper raises an important and underexplored problem: evaluating text-to-image models on step-by-step mathematical visualization.\n\n2. The paper is well-written, with clear figures and detailed explanations that make the work easy to follow.\n\n3. The evaluation framework is thoughtfully designed, with diagnostic metrics that disentangle presentation quality from mathematical validity.\n\n4. The study provides well-controlled comparisons of leading multimodal and diffusion models. This is complemented by a fine-grained error analysis and suggests actionable research directions."}, "weaknesses": {"value": "1. The dataset scope is limited to high-school difficulty, and its overall size is relatively small. Because the benchmark includes only MATH levels 1â€“3, the results may not generalize to more advanced mathematics. In addition, MathViz-Bench is smaller than several widely used T2I or VQA benchmarks, which may potentially be less able to show true gaps and less comprehensive in scope.\n\n2. The evaluation employs a fixed blackboard presentation style without testing alternative layouts such as paper worksheets, whiteboards, slides, or textbook formatting, which may introduce style-induced bias.\n\n3. The evaluation relies on a VLM to assign scores without human validation, potentially introducing bias and errors that could affect model rankings and conclusions."}, "questions": {"value": "1. The evaluation is limited to a single image generation per problem for each model. I strongly encourage the authors to conduct multiple sampling runs and systematically investigate how randomness and various decoding strategies affect performance. \n\n2. The current evaluation is quite limited in scope. It would be stronger to include a broader set of T2I systems, especially more open-source and fine-tuned models, to improve external validity and potentially reveal more informative findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ixNaHbmINE", "forum": "UDeDdxZOwl", "replyto": "UDeDdxZOwl", "signatures": ["ICLR.cc/2026/Conference/Submission2732/Reviewer_KCE7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2732/Reviewer_KCE7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984206252, "cdate": 1761984206252, "tmdate": 1762916350924, "mdate": 1762916350924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MathViz-Bench, a benchmark sampled from the MATH dataset that evaluates text-to-image models on step-by-step mathematical visualization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper provides empirical analysis that reveals the architectural limitations in the task of step-by-step mathematical visualization."}, "weaknesses": {"value": "1. The motivation of this dataset is confusing. It is built on (i.e., sampling) the existing dataset MATH. Moreover, based on the description in Section 3, it looks like the inputs to VLMs include the math problem, complete step-by-step solution, and additional style constraints. Is this dataset only testing models' capability for solution visualization without solution generation?\n\n2. The studied dataset has limited scope with only 500 problems restricted to \"blackboard-style\" solution visualization. Moreover, the math problems are limited to algebraic questions without the critical mathematical visualization types such as geometric diagrams, graphs, plots, and visual proofs. \n\n3. The presentation is unclear and important details are missing. For example, Figure 1 is intended to illustrate the limitation of T2I models, but it didn't discuss what limitations are and why it is challenging. Moreover, the evaluation details are missing in Section 4: what does score 0-5 mean? Why are \"educational viability\" defined by \"all scores > 3\"? Is this based on pedagogical justification or empirical validation?\n\n4. The evaluation is based on LLM judge (Grok4), but it is unclear why it is selected as the judge model and how reliable the evaluation method is. Human evaluation is needed and it is important to report the agreement scores with human judge.\n\n5. The evaluation experiment is limited with only commercial T2I models tested. \n\n6. The authors attribute performance differences to architectural factors (language integration vs. pure visual) without controlling other factors such as model size the same."}, "questions": {"value": "1. One typo in Line 42."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "54rjxpvG8A", "forum": "UDeDdxZOwl", "replyto": "UDeDdxZOwl", "signatures": ["ICLR.cc/2026/Conference/Submission2732/Reviewer_Pj9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2732/Reviewer_Pj9e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2732/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144591929, "cdate": 1762144591929, "tmdate": 1762916350740, "mdate": 1762916350740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}