{"id": "olStIJa2ok", "number": 14959, "cdate": 1758246133892, "mdate": 1759897339212, "content": {"title": "Federated Feature Transformation with Sample-Aware Calibration and Local–Global Sequence Fusion", "abstract": "Tabular data plays a crucial role in numerous real-world decision-making applica-\ntions, but extracting valuable insights often requires sophisticated feature transfor-\nmations. These transformations mathematically transform raw data, significantly\nimproving predictive performance. In practice, tabular datasets are frequently\nfragmented across multiple clients due to widespread data distribution, privacy\nconstraints, and data silos, making it challenging to derive unified and generalized\ninsights. To address these issues, we propose a novel Federated Feature Transfor-\nmation (FEDFT) framework that enables collaborative learning while preserving\ndata privacy. In this framework, each local client independently computes feature\ntransformation sequences and evaluates the corresponding model performances.\nInstead of exchanging sensitive original data, clients transmit these transforma-\ntion sequences and performance metrics to a central global server. The server\nthen compresses and encodes the aggregated knowledge into a unified embedding\nspace, facilitating the identification of optimal feature transformation sequences.\nTo ensure accurate and unbiased aggregation, we employ a sample-aware weight-\ning strategy, assigning higher weights to clients with larger, more diverse, and\nnumerically stable datasets, as their performance metrics are statistically reliable\nand representative. We also incorporate a server-side calibration mechanism to\nadaptively refine the unified embedding space, mitigating bias from outlier data\ndistributions. Furthermore, to ensure optimal transformation sequences at both\nglobal and local scales, the globally optimal sequences are disseminated back to\nlocal clients. We subsequently develop a sequence fusion strategy that blends\nthese globally optimal features with essential non-overlapping local transforma-\ntions critical for local predictions. Extensive experiments are conducted to demon-\nstrate the efficiency, effectiveness, and robustness of our framework. Code and\ndata are publicly available", "tldr": "", "keywords": ["Automatic Feature Transformation", "Tabular Data", "Federated Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4fb6053a7f3e6f81816b8eea34dae67e948b159.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework named FEDFT for federated feature transformation (FFT) on tabular data. Instead of sharing raw data or model gradients, clients generate feature transformation sequences and associated performance scores, which are uploaded to a central server. The server aggregates these records via a sample-aware weighted scheme, trains an encoder–decoder–evaluator network to embed sequences into a shared latent space, and performs gradient-based optimization to explore better global transformation sequences. The globally optimized transformation is then fused with local distinctive features to improve local performance. Experiments on 13 tabular datasets demonstrate improvements over local transformation baselines and some FL methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a joint embedding–reconstruction–performance estimation design, plus local–global fusion, showing architectural completeness.\n\n2. This paper moves beyond traditional parameter aggregation and explores federated feature transformation, which is underexplored but practically relevant for tabular settings."}, "weaknesses": {"value": "1. Although raw data is not shared, transmitting performance values can leak distributional statistics. No comparison with established privacy-preserving FL techniques such as differential privacy or secure aggregation. The paper itself acknowledges lack of formal guarantees. \n\n2. The transformation sequences are symbolic and non-differentiable. Updating continuous embeddings and decoding them does not guarantee validity or monotonic improvement. Valid decode rate is low (Figure 7), and the optimization might be unstable or ill-defined.\n\n3. Works like FLFE and Fed-IIFE are cited but not compared experimentally, limiting the strength of the claimed contribution.\n\n4. Broadcasting sequences to all clients for re-evaluation can be expensive, contradicting efficiency motivations.\n\n5. Some methodological descriptions lack clarity. For example, the KL term is referenced but not precisely defined, hyperparameters (e.g., p, q in weighting) appear heuristic without sensitivity studies, potential leakage in using aggregated validation performance for server optimization\n\n6. The introduction highlights three major challenges that this work aims to address. However, the experiments do not fully demonstrate that the proposed method effectively resolves all of these challenges. As a result, the underlying working mechanism remains unclear, and the claimed contributions are significantly weakened."}, "questions": {"value": "Does the method truly provide privacy-preserving global feature transformation?\n\nDoes the proposed sample-aware weighted aggregation truly reduce bias under non-IID data?\n\nWhat is the causal mechanism by which global-to-local fusion brings improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cJ9iWe4HwM", "forum": "olStIJa2ok", "replyto": "olStIJa2ok", "signatures": ["ICLR.cc/2026/Conference/Submission14959/Reviewer_YYNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14959/Reviewer_YYNX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445250323, "cdate": 1761445250323, "tmdate": 1762925297250, "mdate": 1762925297250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel federated feature transformation framework designed to construct an optimized and generalizable feature space without sharing raw data.\nEach client locally generates feature transformation sequences  and their corresponding predictive performances . These records are uploaded to a central server, where an encoder–decoder–evaluator network jointly learns to embed transformation knowledge into a continuous space .\nA gradient-based search in this embedding space identifies the globally optimal transformation sequence that maximizes the aggregated weighted performance across all clients.\nThe framework aims to improve generalization under heterogeneous data distributions by transferring transformation knowledge rather than raw features or gradients."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Well-designed framework — The encoder–decoder–evaluator architecture is conceptually sound and allows transformation knowledge to be encoded and optimized collaboratively without exposing local data.\nPotential for cross-domain generalization — By learning transformation patterns rather than model weights, the framework could generalize better across heterogeneous or non-IID clients.\nPrivacy-preserving design — The method respects the federated setting and does not transmit raw data, which aligns with privacy-by-design principles."}, "weaknesses": {"value": "(1) High computational complexity of the RL search process. The optimization of feature transformation sequences through RL or gradient-based exploration in embedding space is likely to be time-consuming. The paper lacks an explicit analysis of the computational and communication costs, which raises concerns about scalability to high-dimensional tabular datasets or large numbers of clients.\n(2) Lack of theoretical analysis. Lack of convergence or optimality guarantees for the global optimal sequence search.\n(3) Limited comparison with recent federated learning algorithms. The experiments mostly benchmark against older FL baselines (e.g., FedAvg, FedProx). Moreover, since the method is related to feature engineering, the comparisons do not include papers related to feature engineering.\n(4) Reinforcement learning design is underexplained.The paper mentions “postfix encoding” and search but does not provide sufficient detail about the RL reward structure, policy updates, or exploration-exploitation balance."}, "questions": {"value": "Could the authors provide complexity analysis for both local generation of records and server-side embedding training?\nPlease add comparisons regarding the latest feature engineering related work in federated learning.\nPlease provide a detailed explanation of the training process in the reinforcement learning part.\nAdd proofs for the convergence or optimality guarantees of the global optimal sequence search."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dWVzNRpW3s", "forum": "olStIJa2ok", "replyto": "olStIJa2ok", "signatures": ["ICLR.cc/2026/Conference/Submission14959/Reviewer_RWz2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14959/Reviewer_RWz2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849200852, "cdate": 1761849200852, "tmdate": 1762925296840, "mdate": 1762925296840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes federated feature transformation for tabular data. The authors point out that both federated learning and feature transformation have been investigated but their intersection has relatively been unexplored. To alleviate the data heterogeneity issue among various clients, the proposed FedFT employs sample-aware weighting strategy and global-to-local feature integration. Experiments show that FedFT improves performance over the selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Sample-aware weighting that considers both data size and quality seems effective.\n2. Exploring unique challenges for tabular data can be interesting. \n3. Federated feature transformation is relatively unexplored area."}, "weaknesses": {"value": "- FL has largely been explored for image and text benchmarks but this paper emphasizes tabular data. What is the unique challenge of tabular data compared to other data, such as images and texts? Why not reusing existing techniques for image and text data? Why is the proposal not applied to other data formats? \n\n- Contributing to local clients via federated learning has been also investigated in personalized federated learning (PFL). How is the proposal differentiated from PFL and what is the performance gap?\n\n- FL typically trains a deep learning model itself in a federated manner. In my understanding feature transformation is already included in the model since it extracts deep representations for each input data. What is the unique value of federated feature transformation compared to federated model training? Conceptual illustration comparing feature transformation and other directions can be helpful.\n\n- References are old considering exploding publications on ML these days, which means the research topic may not be important or outdated. Please add more recent papers and if relevant papers do not exist, the authors should justify why this field has been overlooked.\n\n- Similarly, the FL baselines are old; FedNTD published in 2022 is the latest method in the paper. Baselines should include more recent and advanced methods published in 2024 and 2025."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5NwgENLoML", "forum": "olStIJa2ok", "replyto": "olStIJa2ok", "signatures": ["ICLR.cc/2026/Conference/Submission14959/Reviewer_2gHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14959/Reviewer_2gHq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223324085, "cdate": 1762223324085, "tmdate": 1762925296271, "mdate": 1762925296271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}