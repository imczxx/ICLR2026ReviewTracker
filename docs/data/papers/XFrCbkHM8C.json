{"id": "XFrCbkHM8C", "number": 12056, "cdate": 1758205439118, "mdate": 1759897536994, "content": {"title": "Random Feature Mean-Shift", "abstract": "Locating the modes of a probability density function is a fundamental problem in many areas of machine learning. However, classical mode-seeking algorithms such as mean-shift and its variants exhibit quadratic complexity with respect to the number of data points due to exhaustive pairwise kernel computation - a well-known bottleneck that severely restricts the applicability. In this paper, we propose Random Feature mean-shift (RFMS), a novel linear complexity mode-seeking algorithm. We give a sampling-based estimator using random feature kernel approximation and zeroth-order gradient method that allows us to provably achieve linear runtime per iteration, with comprehensive theoretical guarantees for mode estimation and convergence behavior. Empirical evaluations on clustering and pixel-level image segmentation tasks show RFMS is up to 12x faster when compared with other mean-shift variants, offering substantial efficiency gains while producing near-optimal results.", "tldr": "This paper proposes Random Feature Mean-Shift (RFMS), a novel linear complexity density-based mode-seeking algorithm.", "keywords": ["Machine Learning", "Mode-seeking", "Kernel Density Estimation", "Random Feature Method", "Zeroth-Order Optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5df3c6cb42aa4a87f21216f2802d3e501eebc79.pdf", "supplementary_material": "/attachment/af9084f4f6614e8667aabd8d0c59f811dd8b2461.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a scalable, approximate version of the classical mean shift algorithm. This is based on approximating the Gaussian kernel utilised in kernel density estimators by means of Random Fourier Features (RFF). A theoretical analysis shows that, in the limit, the approximation recovers the modes of the \"true\" density. Experiments on clustering tasks show that the approach is faster and more effective than alternative fast mean shift variants in some cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The authors introduce a simple approximate version of mean shift based on random Fourier features. Surprisingly, I could not find prior work that attempted this approach before (but I might have missed something).\n\n* The authors attempt to justify their approximation theoretically, showing that, in the limit, it should recover the same grouping as the original mean shift algorithm.\n\n* Experiments show empirically that the algorithm is competitive against other recent fast mean shift variants."}, "weaknesses": {"value": "* I am not sure why the zero-th order approximation of the density gradient is needed. It should not be hard to take the analytical derivative of Eq. (2) w.r.t. $x$, nor should it be expensive to compute.\n\n* The theorems in Sect. 5 show that the models of the original PDF map to modes of the approximated PDF, which is fine. However, they do not show that the approximated PDF does not introduce *additional* modes that could incorrectly trap the mean shift iteration, thus changing the resulting data clusters. In practice, I would expect that an approximation based on a truncated sinusoidal basis would (aka the RFFs) introduce additional modes with high likelihood.\n\n* The paper would benefit from additional toy visualisation. Instead of only considering the final clustering results in Figure 4, it would be valuable to show the actual approximation of the PDF (in 1D and 2D). This would have the added benefit of illustrating additional unwanted local modes in the approximation, if any (although in practice these might be much more likely in higher dimensions). Likewise, particularly in a 2D case, it could be possible to visualise the original Comaniciu iteration vs the one implied by the approximation for a few selected points. This would also be rather illustrative.\n\n* Mean shift has never been a very good algorithm for high-dimensional data clustering, and this method seems to further exacerbate the problem. Specifically, in Table 1, it seems that, for a large number of dimensions, the *original* mean shift is both more accurate and faster in two out of three cases.\n\nMinor issues:\n\nMean shift was quite relevant in applications several years ago, but now it is much less so. This will somewhat limit the practical impact of this contribution.\n\nThere is likely a normalisation factor missing to the right of Eq. (2)."}, "questions": {"value": "* The authors invoke the \"modern fixed point interaction\" of mean shift [14] (line 126). I gather that, in their notation (eq. 1), this is obtained when $\\eta = 1/(h^2cn)$. Given that the fixed-point iteration is guaranteed to converge, why should we choose anything but this value for $\\eta$?\n\n* Can the authors explain why one needs a randomised numerical approximation of the gradient instead of just computing it analytically (see also above)?\n\n* Can the authors comment on the possibility that the approximated PDF has additional modes, and how likely these are to capture clusters in an unwanted manner?\n\n* Can the authors discuss whether this technique is, in fact, useful with high-dimensional data (compared to just using the original mean shift)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6TAt7kZiTC", "forum": "XFrCbkHM8C", "replyto": "XFrCbkHM8C", "signatures": ["ICLR.cc/2026/Conference/Submission12056/Reviewer_kFhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12056/Reviewer_kFhF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289307960, "cdate": 1761289307960, "tmdate": 1762923029462, "mdate": 1762923029462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the mode finding and mean-shift problems using a Gaussian kernel.  \nThe main idea is to use Rahimi-Recht's Random Fourier Features to represent the n points in a D dimensional space so that the core kernel computation of for the mode finding and mean-shift can be computed in O(D) time per point instead of O(n) time.  \n\nThe algorithm RFMS shows moderate speed-up over regular mean shift when the data set grows from about n=2000 to about n=5000, with D = 500, 750, or 2000 -- without much loss in accuracy empirically. \n\nThere are also theoretical bounds.  About half are standard implications of RFFs, which show that it gets additive error about eps = 1/sqrt{D}.  \nThere are also some potentially more interesting bounds about convergence -- but it was written in a form that made it hard to understand how long the algorithm would need to run in order to achieve a desired error bound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is a nice perspective an a core problem in ML that I would appreciate to be better understood."}, "weaknesses": {"value": "The main downside of this paper is that it explored one out of many possible ways to approximate a KDE.  There are variety of ways based on \n\n - coresets  e.g., https://arxiv.org/abs/1802.01751\n - LSH  e.g., https://arxiv.org/abs/1808.10530\n - data structures e.g., https://arxiv.org/abs/2107.02736\n\nand some of these ideas have also been applied to mode finding  https://arxiv.org/abs/1912.07673\n\nWhat is notable about a number of these approaches (as demonstrated on the mode finding approach) is that \n  - with more careful analysis, then one can find a *relative* error approximation, not the weaker additive approximation that this paper finds.  \n  - it is possible to approximate the KDE using roughly 1/eps time per query.  This would be roughly equivalent to needing sqrt(D) dimensions instead of the D required by RFF -- although the mechanism is different.  \n\nThese settings are slightly different, so it is not immediately clear that they would directly achieve similar sorts of results.  But I suspect at least some of them are possible, and with improved bounds.  The main problem is that there is not a comparison.  \n\n\n\nThe experimental section uses a decent number of datasets.  But the choice of D (the critical dimension parameter for RFFs) is chosen differently for just about each experiment without a lot of explanation.  This makes it so a potential user of this does not have much guidance on how to tune this parameter.  An explanation of this, or a fixed recommended choice, along with an ablation study is needed."}, "questions": {"value": "Can you say anything formal or empirical about how this approach would compare to potential approaches based on coresets, LSH, or other data structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6JdKtYoq66", "forum": "XFrCbkHM8C", "replyto": "XFrCbkHM8C", "signatures": ["ICLR.cc/2026/Conference/Submission12056/Reviewer_xt4T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12056/Reviewer_xt4T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423624875, "cdate": 1761423624875, "tmdate": 1762923029040, "mdate": 1762923029040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm named Random Feature Mean-Shift (RFMS) for mode-seeking. This paper solved the poor scalability of the traditional mean-shift algorithm. RFMS successfully reduces the computational complexity from $O(n^2)$ to $O(n)$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates a sophisticated method that unites kernel approximation techniques with derivative-free optimization methods to solve a fundamental problem.\n2. The framework delivers a complete theoretical structure which ensures algorithm reliability through its core approximation quality and iterate convergence properties."}, "weaknesses": {"value": "1. The method produces kernel values that match the actual values within an error margin, which decreases at a rate of $O(1/\\sqrt{D})$ and depends on both the number of dimensions and the kernel length-scale. The computation of gradients and derivatives, which mean-shift depends on, becomes more complex because it needs additional components or more stringent conditions. The convergence of your iterates to biased stationary points becomes more likely when the dimensionality is not sufficiently large.\n2. The methods for small/medium dimension sizes produce deterministic acceleration results through IFGT and dual-tree FGT and grid-based MeanShift++, which provide precise control over errors at high speeds. The methods operate at linear-time complexity per iteration during practice without adding stochastic feature noise to the system. Thus, this method may underperform specialized fast KDE/mean-shift engines in low-d.\n3. The combination of Low-discrepancy (Quasi Monte-Carlo) sampling with orthogonal/structured features decreases both variance and constants, but the approximation maintains its D-dependent asymptotic behavior. The approximation method faces difficulties when using small kernels because narrow bandwidths create challenges."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xeERc85EUF", "forum": "XFrCbkHM8C", "replyto": "XFrCbkHM8C", "signatures": ["ICLR.cc/2026/Conference/Submission12056/Reviewer_m4YN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12056/Reviewer_m4YN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847835134, "cdate": 1761847835134, "tmdate": 1762923028697, "mdate": 1762923028697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Random Feature Mean-Shift (RFMS), a scalable and theoretically grounded variant of the classical mean-shift algorithm. By employing random Fourier feature approximations and zeroth-order stochastic optimization, the authors reduce the computational complexity of mean-shift updates from quadratic to linear time. The proposed approach eliminates the need for expensive pairwise kernel evaluations and naturally extends to both standard and blurring mean-shift formulations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the quadratic time complexity of the standard mean-shift algorithm by proposing a linear-time solution that leverages random feature approximation and zeroth-order optimization.\n\nFor the proposed RMFS method, the authors present comprehensive theoretical analyses, including concentration bounds and convergence guarantees.\n\nEmpirical evaluations demonstrate substantial efficiency improvements across multiple datasets while preserving accuracy comparable to existing approaches over clustering tasks and image segmentation."}, "weaknesses": {"value": "The authors provide several theorems establishing upper bounds on the estimation errors. However, the sharpness or order of these bounds is not fully discussed. \n\n\\\nIt would be helpful if the authors could elaborate on how tight these bounds are, both in theory and in practice, and clarify whether they differ from or improve upon existing theoretical results in prior work. \nA more detailed discussion on the asymptotic order and optimality of the bounds would strengthen the theoretical contribution."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FLJSyrXMRh", "forum": "XFrCbkHM8C", "replyto": "XFrCbkHM8C", "signatures": ["ICLR.cc/2026/Conference/Submission12056/Reviewer_7jCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12056/Reviewer_7jCs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961732226, "cdate": 1761961732226, "tmdate": 1762923028386, "mdate": 1762923028386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}