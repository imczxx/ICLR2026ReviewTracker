{"id": "lfO9M3dTVS", "number": 1464, "cdate": 1756884980080, "mdate": 1759898207834, "content": {"title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation", "abstract": "Autoregressive image generation models like Janus-Pro produce high-quality images, but at the cost of high memory and computational demands due to the large number of visual tokens. \nWhile KV cache compression has been extensively studied in language modeling, it remains largely unexplored for image generation.\n\nIn this work, we begin by identifying a distinct attention phenomenon, which we term spatial locality and emergent semantic sink. \nTo leverage this, we introduce a novel KV cache compression framework. \nSpecifically, we compress the KV cache for visual tokens by decoupling attention heads into two types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it preserves a compact set of highly-attended tokens. \nExperiments demonstrate that our method achieves a 5$\\times$ reduction in memory usage and a 6.6$\\times$ speedup in throughput with negligible performance loss, enabling efficient native autoregressive image generation.", "tldr": "We design an efficient kv cache compression method for autoregressve image generation.", "keywords": ["Autoregressive Image Generation", "KV Cache Compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7af4b9064c28a5b95eebc1dbcf16f0febdac5069.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SSD (Spatial-Semantic Decoupling) — a framework for Key-Value (KV) cache compression in autoregressive (AR) image generation models like Janus-Pro.\nSSD aims to reduce memory and computation costs during image generation without hurting image quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "SSD’s greatest strength lies in connecting a real hardware bottleneck (KV cache bloat) with a novel structural insight (spatial–semantic head specialization), and turning that into a principled, empirically validated framework that achieves major efficiency gains without sacrificing image quality."}, "weaknesses": {"value": "1 The paper identifies “spatial locality” and “semantic sink” as two distinct attention behaviors in autoregressive image generation.\nHowever, the spatial locality aspect is not novel — similar locality patterns have been discussed in NAR, LPD, and ZipAR, which all highlight that visual attention predominantly focuses on nearby spatial tokens. The authors should clarify the difference between their “spatial-locality heads” and previously observed locality mechanisms, and properly cite these works to avoid overclaim. The attention sink is also old-fashioned since the ViT era [4,5], making it expected in AR. If the authors just use this proposed or similar observation for caching, I think the novelty is quite limited since many similar works have been extensively done in LLM [6].\n\n[1] Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation\n\n[2] Neighboring Autoregressive Modeling for Efficient Visual Generation\n\n[3] ZipAR: Parallel Auto-regressive Image Generation through Spatial Locality\n\n[4] See What You Are Told: Visual Attention Sink in Large Multimodal Models\n\n[5] Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing\n\n[6] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n2 The paper’s analysis and method are entirely built on raster-order autoregressive generation. However, this paradigm is now computationally suboptimal and is being replaced by faster alternatives such as MAR, LPD, and VAR. The authors should discuss whether their observed attention patterns — particularly spatial locality and the semantic-sink behavior — hold across different generation orders. More applications on these frameworks are necessary. Besides, if the work is raster-specific, more applications on Llamagen are necessary."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0kEESGxaxH", "forum": "lfO9M3dTVS", "replyto": "lfO9M3dTVS", "signatures": ["ICLR.cc/2026/Conference/Submission1464/Reviewer_tLGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1464/Reviewer_tLGE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308911054, "cdate": 1761308911054, "tmdate": 1762915776277, "mdate": 1762915776277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KV cache compression methods for next-token-prediction-based autoregressive (AR) image generation. The authors observe that attention heads in AR image models are highly sparse, and can be broadly categorized into two types: \"semantic heads,\" which focus on periodic anchor tokens, and \"spatial heads,\" which concentrate on spatially-local tokens. Based on this observation, the paper proposes two distinct KV cache compression techniques tailored for each head type: a sliding window approach for spatial heads and a heavy-hitter-retention method for semantic heads. Experimental results on the Janus-Pro model demonstrate a superior Pareto frontier on the DPG-bench compared to existing KV cache compression methods developed for text LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, intuitive, and easy to understand.\n- This is the first work that tries to analyze characteristics of KV-cache in AR image models, and found interesting attention patterns (spatial and semantic). This observation aligns well with intuition.\n- Also, this paper propose intuitive KV cache compression methods tailored for two distinct attention types."}, "weaknesses": {"value": "- **Limited Generalizability** : All experiments were conducted solely on the Janus-Pro model. It is uncertain whether the paper's findings, including the observed attention patterns and the efficacy of the proposed compression methods, generalize to other AR image generation models. Experiments on other AR image models, such as llamaGen, Emu3, Anole, and Lumina-mGPT (1, 2), are necessary. I believe experiments on llamaGen are essential, and additional validation on Lumina-mGPT would be welcome.\n\n- **Insufficient Evaluation** : Performance evaluation was restricted to the DPG-bench, which may not adequately capture the perceptual quality of the generated images. An experiment using standard image generation metrics, such as FID or IS, on datasets like MS-COCO, is required.\n\n- **Novelty** : While tackling this problem for the first time and identifying the sparse attention patterns is novel, the proposed methods lack originality. They appear to be direct applications of existing KV cache compression techniques."}, "questions": {"value": "- **Semantic Concentration Metric** : The definition of the \"Semantic Concentration\" metric is not clear. Why is the difference between the KV cache values of CFG and \"native\" (non-CFG) generation representative of semantic concentration?\n\n- **Sliding Window Implementation** : In a flattened 1D token sequence, the local \"neighborhood\" tokens should include not only tokens immediately to the \"left\" (preceding in the sequence) but also spatially adjacent tokens from previous rows. Figure 1 seems to confirm this. However, why sliding method of SSD just retain the preceding(left) tokens in the 1D sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4E9J9mR2ji", "forum": "lfO9M3dTVS", "replyto": "lfO9M3dTVS", "signatures": ["ICLR.cc/2026/Conference/Submission1464/Reviewer_hsdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1464/Reviewer_hsdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639256604, "cdate": 1761639256604, "tmdate": 1762915775944, "mdate": 1762915775944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ​​SSD​​, a framework for compressing the KV cache in autoregressive image generation models. The method categorizes attention heads into two types: (1) spatial locality heads, which focus on spatially adjacent tokens, and (2) semantic sink heads, which attend to a few critical tokens. The KV cache for each type is then compressed using a dedicated strategy. Experimental results show that SSD effectively reduces the KV cache size and accelerates the decoding process, with only minimal performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed methods are simple and effective.\n2. The paper is easy to follow."}, "weaknesses": {"value": "1. The experimental evaluation is conducted exclusively using Janus-Pro models. To fully establish the robustness and general applicability of the proposed methods, validation across a broader range of model architectures is necessary.\n2. The concept of exploiting spatial locality to accelerate autoregressive (AR) image generation has been widely adopted in methods such as PAR [1], ZipAR [2], and NAR [3]. These works, which also employ parallel decoding by restricting the attention window, are highly relevant yet are not discussed or compared against in the paper.\n3. The core motivation of the paper may require reconsideration. While KV cache size presents a significant challenge for LLMs with long contexts, the context length for AR image generation models is typically much shorter, often comprising only hundreds or a few thousand tokens. Furthermore, increasing the batch size does not alter the model's computational intensity (i.e., the compute-to-memory-access ratio). From this perspective, the necessity of compressing the KV cache for AR image generation appears debatable. The paper's primary contribution likely stems instead from the throughput gains achieved via sparse attention during decoding.\n\n[1] Parallelized Autoregressive Visual Generation, CVPR 2025.\n\n[2] ZipAR: Parallel Auto-regressive Image Generation through Spatial Locality, ICML 2025.\n\n[3] Neighboring Autoregressive Modeling for Efficient Visual Generation, ICCV 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hTmSkbmrqO", "forum": "lfO9M3dTVS", "replyto": "lfO9M3dTVS", "signatures": ["ICLR.cc/2026/Conference/Submission1464/Reviewer_UEyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1464/Reviewer_UEyc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738709066, "cdate": 1761738709066, "tmdate": 1762915775721, "mdate": 1762915775721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant memory and computational overhead of the KV cache in autoregressive image generation models , noting that existing language-focused compression methods are suboptimal for visual tokens. The authors empirically identify a novel attention phenomenon: a functional dichotomy where some heads focus on spatial locality and others act as emergent semantic sinks . Crucially, they find semantic information is preferentially anchored at the margin columns of the token grid . Based on these insights, the paper proposes SSD, a framework that classifies heads as spatial or semantic and applies distinct, tailored compression policies to each type . Experiments demonstrate that SSD achieves up to a 5x memory reduction and 6.6x speedup with negligible quality degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: The paper's primary strength is its originality. Instead of merely adapting language-based KV compression, it presents a new, empirically-grounded understanding of attention mechanisms in visual AR models. The identification of the \"spatial-semantic dichotomy\" and the \"margin column anchoring\" phenomenon is a novel and significant finding.\n\n- Quality: The work is of good quality, with strong empirical validation for its claims.Notably, Figure 2(b) provides exceptionally clear and intuitive evidence for the \"semantic anchor\" hypothesis. By plotting the MSE between the KV caches of the CFG and non-CFG branches, it accurately visualizes the periodic spikes in semantic content at the margin column positions . The experimental setup is robust, using competitive baselines (H2O, StreamingLLM) and standard benchmarks (GenEval, DPG-Bench).\n\n- Clarity: The paper is well-written, logically structured, and easy to follow. The problem is clearly defined , and the core concepts are introduced intuitively. The figures (especially 2(b)) and Algorithm 1 effectively illustrate the method.\n\n- Significance: This work addresses a critical and practical bottleneck for the deployment of large-scale AR image generators. The reported efficiency gains (5x memory, 6.6x throughput) are substantial and could significantly advance the practical usability of unified multimodal models on resource-constrained hardware."}, "weaknesses": {"value": "- Generalizability: The paper's primary weakness is the limited scope of its validation. All analyses and experiments are conducted exclusively on the Janus-Pro model family. It remains unclear whether the core findings—the spatial-semantic dichotomy and margin column anchoring—are fundamental properties of visual AR generation or emergent properties specific to the Janus-Pro architecture. The claim needs to be validated on other visual AR models to be considered general.\n\n- Static Head Classification: The classification of heads as spatial or semantic is performed offline and remains static throughout inference. This approach ignores the possibility that a head's function might be dynamic and context-dependent. A static policy may be suboptimal compared to a dynamic one.\n\n- Hyperparameter Sensitivity: The method introduces several key hyperparameters, including the spatial window size $W$, the semantic budget $M$, and the classification threshold $\\tau$. While the paper provides a good sensitivity analysis for $\\tau$, it lacks a detailed discussion on the selection and sensitivity of $W$ and $M$. It is unclear how these are balanced to meet a specific cache budget and how performance is affected by their interplay."}, "questions": {"value": "1. On Generalizability: Can you comment on the generalizability of your findings? Is there any evidence or strong reason to believe that the \"spatial-semantic dichotomy\" and \"margin column anchoring\" phenomena are also present in other visual AR models?\n\n2. On Profiling Cost: Regarding the offline step, \"Classify all heads via sparsity profiling\": what is the computational cost of this process? Specifically, how many prompts were used to gather the statistics what hardware was used, and approximately how much time did this profiling take? Does this step pose a significant barrier to applying SSD to new models?\n\n3. On Hyperparameters $W$ and $M$: Could you please elaborate on how the spatial window size $W$ and the semantic budget $M$ were selected? For instance, in the 20% cache budget scenario in Table 1, what were the typical values or ratio for $W$ and $M$? How sensitive is the model's performance to this allocation ratio?\n\n4. On CFG Dependence: Your core analysis for identifying semantic injection (Figure 2(b)) is heavily dependent on CFG. Does the SSD framework remain effective, and does the margin anchoring phenomenon persist, during non-CFG sampling ($\\gamma=1$) or at very low guidance scales?\n\n5. On Static Classification: The head classification is static. Did you consider or experiment with a dynamic classification strategy, where a head's role could be re-evaluated or adapted based on the generation context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xiqkPc1q92", "forum": "lfO9M3dTVS", "replyto": "lfO9M3dTVS", "signatures": ["ICLR.cc/2026/Conference/Submission1464/Reviewer_ZLyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1464/Reviewer_ZLyX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069978235, "cdate": 1762069978235, "tmdate": 1762915775513, "mdate": 1762915775513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}