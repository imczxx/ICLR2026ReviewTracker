{"id": "AGVH53xu2n", "number": 22776, "cdate": 1758335286865, "mdate": 1759896846891, "content": {"title": "Building the dual graph of the activation regions in a deep neural network: what it means for interpretability", "abstract": "Understanding the geometric representations of deep neural networks (DNNs) which employ a piecewise linear activation function has become a popular research direction in network interpretability. \nA complete geometric picture of the representations of a DNN would include both the polytope regions formed by the network partitions and the set of neighboring regions, i.e., a dual graph.\nPrior work has resulted in algorithms which enumerate all of the activation regions formed by a network, but no algorithms have been proposed for constructing the dual graph in its entirety. \nThis may be due to a naive assumption that because identifying neighboring regions is trivial in shallow networks, it too is trivial in deep networks. \nIn this work, we demonstrate that this assumption is false; finding neighboring regions in a deep network is in fact a difficult problem due to the conditional nature of the partitions in the deep layers. \nWe introduce a method to solve the difficult problem of neighbor finding in DNNs.\nWe implement this algorithm along with our own approach for region enumeration, which together constructs the dual graph.\nFurther, we demonstrate the usefulness of the graph in the context of generalization. \nWe show that test data that are near training data, as measured by path length along the graph, tend to yield the best generalization results.", "tldr": "", "keywords": ["activation regions", "interpretability", "generalization"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1d2899ba0a2eb81d4147686f1f4f0ddc2bf273d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the geometric representations of deep neural networks by introducing a method to compute the dual graph of the input space partition induced by a continuous piecewise affine deep network on its input space. The authors then implement this method to compute the dual graphs of deep neural networks trained on the Extended MNIST dataset. They find that measuring the paths being training samples along the dual graph provides insights into the generalisation of the deep neural network."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well Motivated: The challenge of computing the dual graph of an input space partition is well motivated, and the proposed method is outlined clearly.\n2. Novel Approach that Builds Upon Existing Work: The authors effectively leverage prior results/algorithms where possible, but then derive a novel neighbour finding algorithm that extends existing work."}, "weaknesses": {"value": "1. Correctness of Algorithm: The proposed method is described; however, no formal statement or validation is provided on a toy example.\n2. Small Experiments and Practical Scalability: As deep networks become large, the number of activation regions grows exponentially; therefore, computing the dual graph would seem intractable. Furthermore, supposing obtaining a dual graph is feasible, algorithms operating on these graphs are likely to be computationally expensive. Perhaps evidenced by the fact that only small-scale experiments are considered. \n3. Overstating the Importance and Applicability to Interpretability: Using the term 'interpretability' is somewhat misleading. Interpretability is typically used in reference to understanding the learned features of a deep network. Although this application is mentioned in the conclusion, it is not the main focus or application considered in the paper."}, "questions": {"value": "1. What is the computational complexity of the proposed method for finding the dual graph?\n2. How would you consider adapting the algorithm for deep networks that are not continuous piecewise affine?\n3. How does the utilisation of this exact algorithm compare to just sampling points along a linear interpolation of input points and using the number of unique input-output Jacobians as a proxy for the shortest path?\n4. Is there a property of a deep neural network that you foresee we can only explain using the dual graph, rather than just using the input space partition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vHHlpWZZCg", "forum": "AGVH53xu2n", "replyto": "AGVH53xu2n", "signatures": ["ICLR.cc/2026/Conference/Submission22776/Reviewer_bhka"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22776/Reviewer_bhka"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409420650, "cdate": 1761409420650, "tmdate": 1762942382027, "mdate": 1762942382027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first exact algorithm for constructing the dual graph of activation regions in deep neural networks (DNNs) with piecewise linear activations (e.g., ReLU). Unlike prior work that focuses solely on enumerating activation regions, this work identifies neighboring regions—a much harder problem due to conditional layer dependencies. The authors leverage results from computational geometry (e.g., Sleumer 2000, Rada & Černý 2018) to design an output-polynomial algorithm that enumerates regions and discovers neighbors via \"tight hyperplane\" detection. They then demonstrate the interpretive utility of the dual graph by correlating graph-based path distances between training and test regions with generalization performance on EMNIST."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed work has its novelty in that the authors are the first to explicitly formalize and solve the neighbor-finding problem for DNN activation regions. Prior work cited in Related Work (p. 2 L107–L141) enumerates cells but not adjacency—this paper extends the geometric picture to the full dual graph.\n\n* The construction of the network hyperplane arrangement and the use of linear programs for interior-point search are mathematicallly consistent with computational geometry practice."}, "weaknesses": {"value": "* The paper claims the algorithm is output-polynomial (pg. 6) but gives no empirical runtime or complexity scaling. \n* Experiments seem to be confined to 2-layer MLPs with width = 11. There is no timing or memory analysis across larger architectures.\n* Adding the comparative evaluation based on some of the related works (e.g. Balestriero & LeCun (2024)) would strengthen the work. Without such comparisons, it is unclear if the new algorithm improves over existing enumeration methods\n* The observed correlation between path length and accuracy (Figure 6) is very interesting but not anlayzed in depth. The discussion section uses strong claims (\"operationally meaningful definition of generalization\") without rigorous justification.\n\nThe dual-graph formalism represents a meaningful conceptual and algorithmic advance over prior geometric analyses of neural networks.\nHowever, the work remains computationally limited and empirically under-demonstrated (Sections 5–6, p. 6–8).\nIf scalability evidence and stronger experimental validation are added, I would be happy to raise the score. \n\n** Some minor points ** \n* Sometimes the input is denoted as $\\theta$ but sometimes it's $\\mathbf{x}$\n* Minor typographical inconsistencies (e.g., “tesselation” (pg. 8) vs. “tessellation” (pg. 6)) and overuse of the term “network sign vector” without shorthand notation make some passages verbose.\n* The “bounded domain” discussion (Appendix D) could be integrated earlier for completeness.\n* References to recent theoretical works on piecewise-linear region geometry (e.g., Raghu et al. 2017, Poole et al. 2016) are missing."}, "questions": {"value": "* Please provide empirical scaling results (runtime vs. number of neurons/layers) and compare them with prior region enumeration methods.\n* Approximation or Sampling: Can the proposed algorithm be adapted into an approximate dual-graph construction for larger models? If so, outline how accuracy vs. computational cost would trade off.\n* Beyond the EMNIST correlation, can the dual graph capture interpretable clusters (e.g., digits with similar morphology) or identify adversarial transitions across edges?\n* Does the “output-polynomial” complexity claim hold under degenerate network configurations where many neurons are inactive or redundant?\n* How sensitive is the neighbor-finding step to numerical precision (e.g., floating-point stability in LP solvers)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz2ZE4iGR6", "forum": "AGVH53xu2n", "replyto": "AGVH53xu2n", "signatures": ["ICLR.cc/2026/Conference/Submission22776/Reviewer_oZdQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22776/Reviewer_oZdQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766679416, "cdate": 1761766679416, "tmdate": 1762942381797, "mdate": 1762942381797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the full geometric representation of piecewise-linear deep neural networks by building dual graphs. The authors show that finding neighbors in deep networks is nontrivial because of the conditional, layer-wise partitioning that makes adjacency harder to detect than in single-layer hyperplane arrangements. They present what they claim is the first exact algorithm to construct the dual graph for such DNNs, and combine it with a region-enumeration procedure to recover the complete graph. Using this graph, they define a path-length distance on the manifold of activation regions and empirically show that test points that are close to training points under this metric tend to generalize better."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies the dual graph of activation regions, moving beyond mere enumeration of regions to their adjacency structure.\n\n2. It provides an exact algorithm for neighbor finding in deep networks, which is harder than commonly assumed.\n\n3. The dual-graph viewpoint is intuitive and positions path length as a natural, geometry-aware metric that can capture relationships missed by Euclidean distances in input space."}, "weaknesses": {"value": "1. A few sentences are slightly awkward and could be tightened for clarity: “This may be due to a naive assumption that because identifying neighboring regions is trivial in shallow networks, it too is trivial in deep networks.” reads as clunky.\n\n2. Experiments are restricted to relatively simple, shallow architectures and MNIST. It is unclear how the enumeration + neighbor-finding pipeline scales to practical modern networks (e.g., deeper convnets, transformers, large input dimensions) or to richer datasets; the paper should either provide evidence of scalability or clearly delimit the claimed scope."}, "questions": {"value": "1. I am not a specialist in every low-level technical aspect of region enumeration algorithms, so I cannot fully vouch for implementation subtleties or edge cases. In particular, I would like the authors to explain the broader significance of studying the dual graph for DNN theory and practice, given their focus on simple networks. Why should practitioners or theorists prioritize this object, and what concrete insights does it unlock beyond what regions already provide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5k7i623ZEu", "forum": "AGVH53xu2n", "replyto": "AGVH53xu2n", "signatures": ["ICLR.cc/2026/Conference/Submission22776/Reviewer_CgHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22776/Reviewer_CgHs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803936479, "cdate": 1761803936479, "tmdate": 1762942381607, "mdate": 1762942381607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the interpretability approach of activation polytopes induced by the ReLU activations within a network, and describes a method for not only finding such regions, but for constructing a graph to describe the full set of regions and their connectivity to one another."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Lots of technical detail"}, "weaknesses": {"value": "- The paper didn't do a good job of clearly articulating why and how activation polytopes could be used for interpretability \n- The paper focused too much on minutia that is confusing to a non-expert, and not enough on the ways that such a technique could be used"}, "questions": {"value": "- How well does this method scale to larger networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c8LnBSnS1P", "forum": "AGVH53xu2n", "replyto": "AGVH53xu2n", "signatures": ["ICLR.cc/2026/Conference/Submission22776/Reviewer_qBKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22776/Reviewer_qBKY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956389304, "cdate": 1761956389304, "tmdate": 1762942381363, "mdate": 1762942381363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}