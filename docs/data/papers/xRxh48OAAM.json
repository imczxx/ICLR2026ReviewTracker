{"id": "xRxh48OAAM", "number": 25381, "cdate": 1758367379345, "mdate": 1759896722857, "content": {"title": "Eliminating the first moment state in Adam optimizer", "abstract": "The Adam optimizer and its variants are widely used in large-scale machine learning, but their memory footprint is high because they maintain two state variables per parameter. In Adam, the exponential moving average (EMA) of gradients (m) serves as a first-moment estimator, but it also carries variance information that can be exploited to estimate the second moment. Furthermore, the gradient buffer can be repurposed to handle both gradient accumulation and a proxy for the first moment, effectively folding m into the gradient buffer itself. These modifications reduce the number of optimizer state variables from two to one, yielding Half-Memory Adam (HMAdam) and its decoupled-weight-decay variant (HMAdamW). Both variants retain the Adam update rule and hyperparameters. Experiments across discriminative and generative tasks including CNNs, transformers, and diffusion models show that HMAdamW matches the performance of standard AdamW in convergence speed, final accuracy, and runtime, while substantially lowering memory usage. Moreover, this version of Adam retains its convergence properties. This makes it a practical choice for memory-constrained training scenarios such as large-scale language modeling.", "tldr": "We present a novel variant of Adam optimizer that uses one state variable, instead of two", "keywords": ["Half-memory Adam", "efficient Adam", "Memory efficient optimizer"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c066a66ffe593be4edc42cd97e09428bd5f1246.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new algorithm called Half-Memory Adam (HMAdam) and its decoupled weight decay variant (HMAdamW), which eliminates the first moment state and reduce memory. The algorithms are tested on various deep learning architectures, including CNNs, transformers, and diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Memory-efficient optimizers are important in the era of large models, so reducing the memory footprint will have strong impact in the field of machine learning.\n\n2. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The technique of reducing optimization state was already studied in the literature. For example, the Appendix E.2 of the paper [r1] studied how to reduce the memory footprint by avoiding the operations of zero_grad. Therefore the proposed zero_grad override technique is not novel compared with [r1].\n\n2. The state-of-the-art optimizers such as MUON or SCION [r1, r2] does not need to store the second-order moment as in Adam but outperforms Adam significantly. In addition, the technique of avoiding memory footprint caused by zero_grad is already proposed in [r1]. Therefore, the practical importance of the proposed method is further diminished compared with SCION [r1].\n\n[r1] Pethick, Thomas, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. \"Training deep learning models with norm-constrained lmos.\" ICML 2025.\n\n[r2] Liu, Jingyuan, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin et al. \"Muon is scalable for LLM training.\" arXiv preprint arXiv:2502.16982 (2025).\n\n3. The derivation of the second order moment in Appendix A needs to assume that the new gradient comes from a Gaussian distribution with a very low signal-to-noise ratio (SNR goes to zero), which is unrealistic. In addition, the estimator $g_t^2$ is only a unbiased estimator of $\\tilde{g}_t^2$ in expectation, it is unclear whether this is still a good estimator with high probability. It is possible that the two quantities have very different magnitudes. Could the authors numerically test this?\n\n4. The authors did not run enough iterations for imagenet (e.g., Figure 11). For example, the default imagenet training takes at least 90 epochs, but the authors only ran for 20 epochs."}, "questions": {"value": "Can you address my concerns in the weaknesses section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JgTciX2tU9", "forum": "xRxh48OAAM", "replyto": "xRxh48OAAM", "signatures": ["ICLR.cc/2026/Conference/Submission25381/Reviewer_dgd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25381/Reviewer_dgd3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760929371912, "cdate": 1760929371912, "tmdate": 1762943418998, "mdate": 1762943418998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows how to re-use the gradient buffer to store the first order momentum buffer, essentially eliminating one of the momentum buffers without incurring any change to SGD or Adam(W). This is simple idea, but one that makes total sense. I'm only surprised this is not been noted before. Certainly this is not what is currently being done in the standard SGD and Adam implementations in PyTorch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is simple to understand, and saves a significant amount of memory at no cost or change to the AdamW. It is also easy to implement."}, "weaknesses": {"value": "I checked the references Sohoni 2019 and that previously mentioned how it could be possible to store the momentum buffer in the gradient, and I agree they do not really formalize this idea, or make it sufficiently clear."}, "questions": {"value": "1. Could  you also double check how these parameter rescaling work with unbiased momentum estimates that do not rely on the \"debiasing trick\". By this I mean, if you initialize $m_0 <- g_0 $ (instead of $m_0 <- 0$), then $m_t$ is an unbiased estimate of the gradient without having to correct by dividing by $1/(1-\\beta^t_1)$. To be more clear, by unbiased estimate I mean that if the gradient are identical with $g= g_t$ for all $t$, then $m_t = g$. \n\nTo see how initialization affects this bias, assume by induction that $m_{t-1} =g$, which is true for $m_0 =g$ by initialization. Consequently \n$$ m\\_{t} = \\beta\\_1 m\\_{t-1} + (1-\\beta\\_1)g = \\beta\\_1 g + (1-\\beta\\_1)g =g. $$\nI'm not sure where this idea first appeared, but I first read it in a 2024 ICLR paper:\n\nFabian Schaipp et al, ``MoMo: Momentum Models for Adaptive Learning Rates'', ICLR 2024.\n\n\n2. I think you should expand more on how gradient clipping can be incorporate for the micro batches. You mentioned you could use backward hooks, but wouldn't this then increase the memory?   Also you state \"micro-batch-clipping .. is often preferable.\" Could you explain why or back this up with a reference?\n\n3. I see a few issues around Eq (3). First, you are not open about your assumption that we are in a steady state, that is the distribution of $\\tilde{g}_t$ is fixed. Furthermore, I don't follow your proof. On line 785 I the appendix, to arrive at Eq (5) you state that you \"solve for $E[g\\_t^2]$\", yet you assume that $g\\_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$, consequently $E[g\\_t^2] = \\sigma^2+\\mu^2$, but I see no $\\sigma^2$ in Eq (5). Moreover, I don't even understand why you need this, and  I don't see the need for any of the text between lines 238-260. Perhaps it just needs to be re-written, and I missing something here. On a bibliographic note, under your same assumption that $g\\_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$, the statistical role of the Adam buffers was recently formalized in:\n\n\nAntonio Orvieto, In Search of Adam's Secret Sauce, Neurips 2025.\n\n4. To really make a point about how HAdamW and AdamW have \"essentially \" the same execution, you need to at least run multiple seeds, and report some basic statistics. For instance, on the small scale experiment on MNIST Figure 3, you could run multiple seeds, and even run a hypothesis test to see if the methods are statistically the same. Given this is the main message of your work, it's worth doing a proper statistical evaluation. Although, since AdamW and HAdamW are mathematically the same, the only source of difference would be from finite precision and rounding issues, which is unlikely (though not impossible) to cause a significant difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zK05724QAi", "forum": "xRxh48OAAM", "replyto": "xRxh48OAAM", "signatures": ["ICLR.cc/2026/Conference/Submission25381/Reviewer_gh1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25381/Reviewer_gh1R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761322668385, "cdate": 1761322668385, "tmdate": 1762943418630, "mdate": 1762943418630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to reduce the memory costs of Adam methods. It is based on simple observation: using a change of variable to remove the first-order moment variable. As a result, the new variable can be maintained and used the gradient buffer. The idea was first illustrated on momentum SGD and then extended to Adam. Because no gradient buffer is explicitly maintained, they used another method to calculate the second-order moment from the gradient accumulator."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The strengths of the paper: for momentum SGD, it clearly removes additional gradient buffer during the backprop, which reduce the memory costs. The experiments demonstrate the new Adam method performs similarly with the original Adam but with much reduced memory."}, "weaknesses": {"value": "The weakness of the paper: it is unclear why Half-memory Adam reduce the memory by half. My understanding is that Adam needs to maintain m, v and g during backprop, Half-Adam maintains \\tilde g, v, which is 2/3 memory of Adam.  Please clarify in the rebuttal."}, "questions": {"value": "The authors also noted that the proposed trick has been noted by earlier works, e.g.,  Sohoni et al, which makes the paper less novel. Maybe the key contribution is the half-memory Adam, which was not considered in prior works? \n\nOverall, the idea is neat though not significant theoretical analysis. Its practical benefit could be significant. I will recommend an acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eyO8m3wAKg", "forum": "xRxh48OAAM", "replyto": "xRxh48OAAM", "signatures": ["ICLR.cc/2026/Conference/Submission25381/Reviewer_rgom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25381/Reviewer_rgom"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706661046, "cdate": 1761706661046, "tmdate": 1762943418383, "mdate": 1762943418383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Half-Memory ADAM (HMADAM) and HMADAMW, variants of ADAM/ADAMW that remove the explicit first-moment state by turning the gradient buffer into an exponentially decayed accumulator $\\tilde g_t$ and estimating the first moment from it. The key idea is to update the second-moment accumulator using information in the variance of $m$ or $\\tilde g_t$, leveraging that $E[g_t^2] \\approx (1-\\beta_1^2)E[\\tilde g_t^2]$ under typical low-SNR regimes. After re-estimating the moments from $\\tilde g_t$, the update rule remains the same as ADAM up to a derived scaling that can be absorbed into the learning rate, so tuned hyperparameters transfer. Implementation-wise, the change is to replace the usual zeroing of gradients with a decay step $g \\leftarrow \\beta_1 g$, making the buffer both a per-step accumulator and a cross-step EMA. Experiments on CNNs, diffusion models, and LLMs show convergence curves, final accuracy, and runtime that closely match ADAMW while halving optimizer-state memory. On a 3B-parameter LLM, optimizer state drops from about 12 GB to 6 GB with essentially identical step time. The authors position this as orthogonal to activation KV-cache and other memory costs, and discuss extensions to other optimizers and compatibility with quantization or low-rank techniques as future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Drop-in compatibility and unchanged updates\n\nThe method preserves ADAM’s parameter-update rule after a small analytic scaling that can be absorbed into the learning rate, and the learning rates tuned for ADAM/ADAMW transfer directly. In practice, the optimizer overrides zero_grad with a decay step and otherwise keeps training code unchanged, remaining compatible with autograd, gradient accumulation, and distributed training.\n\n### Exact optimizer-state halving with matched behavior\n\nBy folding $m$ into the gradient buffer and re-estimating $v$ from $\\tilde g_t$, HMADAMW removes one of the two auxiliary states per parameter. Across benchmarks, this yields a precise 50 % reduction in optimizer-state memory while keeping convergence speed, final accuracy, and total training time statistically indistinguishable from ADAMW.\n\n### Evidence on large models and concrete numbers\n\nFor LLMs up to 3B parameters, optimizer state drops from about 12 GB to 6 GB and step time remains essentially unchanged, with the optimizer phase taking 19 ms for ADAMW vs 18 ms for HMADAMW at a 128k-token batch. Similar findings are reported for ImageNet and diffusion setups, with runtime parity also confirmed.\n\nOverall, I think the proposed method is a simple yet clever idea to reduce the memory cost of ADAM-style optimizers."}, "weaknesses": {"value": "### Reliance on low-SNR justification\n\nThe central proportionality $E[g_t^2] \\approx (1-\\beta_1^2)E[\\tilde g_t^2]$ is motivated by the common low-SNR condition $\\mu \\ll \\sigma$ and supported empirically, yet tasks or phases with sustained high SNR could reduce the tightness of this approximation. The paper’s analysis and appendix emphasize the low-SNR regime."}, "questions": {"value": "- Recent optimizers like ADOPT [1] have a little different definition of $m$, in which the raw gradient $g$ is scaled by $v$ before the update of $m$ to ensure the theoretical convergence guarantee. In such cases, the proposed technique does not seem applicable. Do the authors have any ideas to overcome this limitation?\n\n[1] Taniguchi, Shohei, et al. \"ADOPT: Modified Adam Can Converge with Any $\\beta_2 $ with the Optimal Rate.\" Advances in Neural Information Processing Systems 37 (2024): 72438-72474."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RBfDvae5Ql", "forum": "xRxh48OAAM", "replyto": "xRxh48OAAM", "signatures": ["ICLR.cc/2026/Conference/Submission25381/Reviewer_wDwi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25381/Reviewer_wDwi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890117743, "cdate": 1761890117743, "tmdate": 1762943418099, "mdate": 1762943418099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}