{"id": "aIULmVHnl8", "number": 20728, "cdate": 1758309413662, "mdate": 1759896961484, "content": {"title": "FaithShield: Defending Vision–Language Models Against Explanation Manipulation via X-Shift Attacks", "abstract": "Vision–Language Models (VLMs) such as Contrastive Language–Image Pre-training (CLIP) have achieved remarkable success in aligning images and text, yet their explanations remain highly vulnerable to adversarial manipulation. Recent findings show that imperceptible perturbations can preserve model predictions while redirecting heatmaps toward irrelevant regions, undermining the faithfulness of the explanation. We introduce the X-Shift attack, a novel adversarial strategy that drives patch-level embeddings toward the target text embedding, thereby shifting explanation maps without altering output predictions. This reveals a previously unexplored vulnerability in VLM alignment. To counter this threat, we propose FaithShield Defense, a two-fold framework: (i) a dual-path redundant extension of CLIP that disentangles global and local token contributions, producing explanations more robust to perturbations; and (ii) a novel faithfulness-based detector that verifies explanation reliability via a masking test on top-$k$ salient regions. Explanations that fail this test are flagged as unfaithful. Extensive experiments show that X-Shift reliably compromises explanation faithfulness, while FaithShield restores robustness and enables principled detection of manipulations. Our work formalizes explanation-oriented adversarial attacks and offers a principled defense, enhancing trustworthy and verifiable explainability in VLMs.", "tldr": "We propose X-Shift Attack, which manipulates CLIP explanations by forcing patch embeddings toward target text, and FaithShield Defense, a dual-path and novel detection framework for robust, faithful explanations.", "keywords": ["Vision–Language Models", "CLIP", "Explainable AI", "Adversarial Attacks", "Trustworthy Machine Learning", "Robustness XAI", "Multimodal Learning", "Faithfulness Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e73b7df59be2b5ac8315b1351f8a4320801e333f.pdf", "supplementary_material": "/attachment/19507dddaa4679b49eb65d2105c081a4477bcdc8.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies adversarial attacks and defenses for explanatory methods. It first builds the X-shift attack, an attack method that maintains classification predictions while changing the alignment of intermediate features towards the target text, under constraints such as $\\ell_{0}$ and class-dominance. Then it proposes a robust explanatory method, refined from Li et al. [1], for robust visualization under X-shift attacks.\n\nLi, Yi, et al. \"A closer look at the explainability of Contrastive language-image pre-training.\" _Pattern Recognition_ 162 (2025): 111409."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper investigates a novel area that has not been explored yet.\n2. The motivation of this paper is clear.\n3. An attack method along with a robust defense method is proposed, facilitating further research."}, "weaknesses": {"value": "1. Presentation. Fig. 1 and 2 are not Vector Graphics. They get blurred when zooming in, especially Fig. 2. Sec. 3.2 is mainly composed of bullet points. It should be connected with coherent words and formulas, thus bullet points take up the space of the algorithm, which can only be presented in the appendix. Table 1 is also way too big. \n2. Limited innovation. The FaithShelf Stage 1 largely overlaps with existing method [1], while FaithSelf Stage 2 mainly applies a drop-out test. \n3. Lack of baseline methods. From Fig. 3, it seems like FaithShelf Stage 1 has already moved the concentration of the adversarial sensitivity map to the bench. Also, the raw patch similarity looks messy anyway, and few people will use it as an explanatory tool, so there is no reason to attack it. It should be that first X-shift Attack move the ***concentrated*** sliency map (produced by other baseline methods) towards some irrelevant object. It becomes crucial for a robust explanatory method. Currently, I can not see this point clearly. \n\nThe current quality of this paper is well below the required level for acceptance. A significant refinement is needed for resubmission."}, "questions": {"value": "1. How does the stage 1 differ from the existing method [1]? \n\nLi, Yi, et al. \"A closer look at the explainability of Contrastive language-image pre-training.\" _Pattern Recognition_ 162 (2025): 111409."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dgjk4qanZT", "forum": "aIULmVHnl8", "replyto": "aIULmVHnl8", "signatures": ["ICLR.cc/2026/Conference/Submission20728/Reviewer_RN6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20728/Reviewer_RN6w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412059173, "cdate": 1761412059173, "tmdate": 1762934143889, "mdate": 1762934143889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces X-Shift, a novel adversarial attack that manipulates explanation maps in VLMs like CLIP without altering their predictions. The attack shifts attention heatmaps toward irrelevant regions, undermining the trustworthiness of model explanations. The paper also propose FaithShield, a two-stage defense framework. The first stage enhances robustness through a dual-path refinement while the second stage detects unfaithful explanations using a confidence-drop test. Experiments across multiple datasets show that FaithShield significantly improves explanation stability and enables reliable detection of adversarial manipulations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a interesting task in the safety of VLMs, which has the risk of being manipulated at the interpretation level, especially the heat map may be misleading without affecting the prediction results.\n\n2. The proposed FaithShield framework is technically sound and clearly explained, combining dual-path refinement and a faithfulness-based detection mechanism to enhance robustness and verifiability."}, "weaknesses": {"value": "1. In the second paragraph of the Introduction, the authors introduce the value of the attack with the phrase \"remains largely unexplored,\" which seems insufficient in terms of research value. Could the authors provide concrete research to truly demonstrate the value of this attack? For example, could they explain the potential impact of this attack when implemented within a specific research context and with specific objectives?\n\n2. In Figure 1, in the Transformer Encoder, why doesn't the dual path go through the MLP? Also, are all these modules optimizable? If there are any frozen parameters, it is suggested to mark them in the figure.\n\n3. It is recommended to use vector graphics for Figure 1 and Figure 2, and use fonts that align with the article.\n\n4. The experiment part lacks the ablation of the weight parameters in Equation 8.\n\n5. As a two-stage approach, is there any time or efficiency comparison for FaithShield Defense?\n\n6. What do the metrics CosSim (CLS) and Max $\\Delta$ Prob mean? In Table 1, they are identical to the CLIP values. Are there no special circumstances? Can you provide a naive baseline, such as one with only an attack, to show how they differ?"}, "questions": {"value": "Overall, the author's work is interesting, but the method and experiments need to be enhanced. The relevant suggestions are listed in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mW86g1x2jy", "forum": "aIULmVHnl8", "replyto": "aIULmVHnl8", "signatures": ["ICLR.cc/2026/Conference/Submission20728/Reviewer_UA9t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20728/Reviewer_UA9t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639393258, "cdate": 1761639393258, "tmdate": 1762934142942, "mdate": 1762934142942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a CLIP-based vision–language model that aims to modify the image embedding toward the target text embedding without changing the model’s maximum output, thereby shifting the explanation maps. Based on my understanding, this can be regarded as an adversarial attack on interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is the first to consider this problem setting under the CLIP framework."}, "weaknesses": {"value": "1. Lacks sufficient ablation studies, especially on how different weight magnitudes in Equation (8) affect the results.\n\n2. The figures are not professionally prepared and appear somewhat blurry. They should be replaced with vector-format images.\n\n3. The font size in the figures (e.g., Figures 3–5) is too small to read clearly.\n\n4. The motivation is not clearly written, and several claims are overextended or insufficiently explained (see questions below)."}, "questions": {"value": "1. I am not an expert in this area, and this is my first time encountering this problem setting. Is this setting truly meaningful? From my understanding, as long as the prediction remains accurate, the explanation map should still mainly focus on the target object. Even if its intensity decreases. What is the practical significance or application of this problem formulation?\n\n2. In Equation (2), the authors state that “the primary goal is to force patch embeddings toward the target text embedding,” but then they also write “we maximize similarity of the top-K patches while suppressing others.” If the goal is to align all patches with the target embedding, why suppress some of them? The paper does not explain the role or motivation for suppressing certain patches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v2e8fiMYwN", "forum": "aIULmVHnl8", "replyto": "aIULmVHnl8", "signatures": ["ICLR.cc/2026/Conference/Submission20728/Reviewer_4kAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20728/Reviewer_4kAu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835379493, "cdate": 1761835379493, "tmdate": 1762934142129, "mdate": 1762934142129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper exposes a new vulnerability in CLIP-style vision–language models: explanations (patch–text heatmaps) can be adversarially shifted without changing predictions. It introduces X-Shift, an inference-time attack that pulls patch embeddings toward a target text embedding while enforcing prediction consistency, per-patch margins, sharpness, and sparse valid perturbations.\n\nTo defend, FaithShield has two stages:\n\n* Stage I: Dual-path refinement that replaces standard self-attention with consistent V–V attention, skips FFNs in the explanation path, and removes redundant features, yielding sharper, foreground-focused, and more robust heatmaps.\n* Stage II: A faithfulness test that masks top-ρ% salient regions and flags explanations as unfaithful if the confidence drop is below a threshold.\n\nAcross ImageNet, Flickr30k, and COCO with CLIP ViT-B/16, B/32, L/14, X-Shift strongly alters heatmaps while keeping predictions stable; FaithShield restores heatmap robustness (higher Top-k IoU) without harming accuracy and detects manipulated explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality and problem framing: The paper defines a new explanation-focused threat model for VLMs (X-Shift) that shifts patch–text heatmaps without changing predictions. It tailors objectives to text-conditioned similarity (patch steering, entropy sharpening, patch-margin, sparsity). The joint attack-plus-defense (FaithShield) with dual-path refinement and a causal masking detector is novel for multimodal XAI.\n\n2. Technical quality and empirical rigor: The attack and defense are precisely specified with clear losses, constraints, and algorithms enabling reproducibility. Defense mechanisms (consistent self-attention, dual-path aggregation, redundancy removal) are well-motivated and operationalized, with a principled confidence-drop test. Experiments span multiple datasets/backbones with appropriate metrics, showing consistent IoU gains without harming accuracy.\n\n3. Clarity and significance: The paper clearly separates prediction robustness from explanation robustness and explains why patch–text similarity is a natural manipulation surface. Mathematical formulation and detection criterion are easy to follow, aided by figures and stepwise algorithms. The work elevates VLM explainability to a security concern, with practical implications for trustworthy deployment."}, "weaknesses": {"value": "1. While the combination of consistent self-attention, dual-path aggregation, and redundancy removal is adapted for robustness, portions build on Li et al. (A closer look at the explainability of contrastive language-image pre-training). The paper would benefit from a more explicit ablation and attribution of gains: which components (consistent attention vs skipping FFNs vs redundancy removal) contribute most to adversarial robustness (not just interpretability), and how this differs empirically from Li et al. ’s formulation.\n2. The masking-based detection echoes causal deletion tests used in saliency evaluation. Clarify novelty relative to established faithfulness tests and justify design choices (cosine similarity normalization, thresholding strategy) versus alternatives (logit/probability drops, energy-based measures).\n3. Threshold selection: The detection threshold θ and masking ratio ρ appear fixed but selection criteria are not detailed. Provide systematic calibration (ROC, AUC, FPR at fixed TPR) across datasets and backbones, and analyze sensitivity to θ, ρ, and masking method (zeroing vs blurring vs inpainting).\n4. False positives/negatives: Quantify detection trade-offs, especially in naturally challenging images where explanations may be diffuse or multi-object. Report detection under distribution shift and for clean samples to ensure low false alarm rates.\nAttack-transfer to detector: Evaluate whether small, structured perturbations can spoof high ∆conf (e.g., by concentrating heatmap on benign-but-causal pixels) to evade detection."}, "questions": {"value": "1. Adaptive-attacker robustness and ablations:\n* How does FaithShield perform against an adaptive adversary that differentiates through Stage I and uses a surrogate for Stage II’s masking to keep Δconf above θ? Please report results where the attacker augments its loss with the detection term, randomizes ρ/θ during optimization, and employs stronger perturbation sets (e.g., larger k, ℓ∞/ℓ2 bounds, spatial/color transforms). Also ablate consistent self-attention, dual-path aggregation, and redundancy removal to quantify each component’s contribution under adaptive attacks.\n\n2. Generality beyond CLIP and across XAI methods\n* Do X-Shift and FaithShield transfer to other VLMs (e.g., SigLIP, ALIGN, BLIP-2) and tasks (e.g., grounding, VQA)? How does the attack affect gradient-based explanations (Grad-CAM, IG), and does Stage I still help when similarity maps are not the explainer? Please include cross-model and cross-explainer transfer results and discuss any architectural assumptions (e.g., ViT patching, attention pooling) that constrain applicability.\n\n3. Detection reliability and operational thresholds\n* How sensitive is the masking-based detector to ρ, masking strategy (zeroing vs. blur), and heatmap sharpness α, and how should θ be calibrated in a label-free deployment? Please report FPR/FNR on clean vs. attacked data under distribution shift and natural corruptions, provide ROC/PR curves with confidence intervals, and evaluate attackers that explicitly minimize Δconf to probe worst-case detection performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D8BaWVdaze", "forum": "aIULmVHnl8", "replyto": "aIULmVHnl8", "signatures": ["ICLR.cc/2026/Conference/Submission20728/Reviewer_GZ9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20728/Reviewer_GZ9a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912924813, "cdate": 1761912924813, "tmdate": 1762934141467, "mdate": 1762934141467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}