{"id": "UEamVVlLBd", "number": 6763, "cdate": 1757994809797, "mdate": 1759897895627, "content": {"title": "Chain-of-Learngene: A Scalable Learngene-based Paradigm for Building and Initializing Variable-Sized Language Models", "abstract": "Large language models (LLMs) show strong performance across a wide range of tasks, yet their deployment remains costly in resource-constrained environments. A common alternative is to pre-train small language models (SLMs) from scratch, but this approach demands substantial computation and often suffers from limited model capacity. Knowledge distillation (KD) improves SLMs' performance by transferring knowledge from LLMs, but generating SLMs of varying sizes typically requires repeated teacher (LLMs) inference, which remains computationally expensive.\nTo address these challenges, we propose \\textbf{Chain-of-Learngene (\\textbf{CoL})}, a scalable framework for efficiently initializing multi-scale SLMs for diverse resource-constrained settings. \\textbf{CoL} is inspired by the Learngene framework, which extracts expressive and tiny components (\\textit{learngene}) from a pre-trained ancestor model (AnsNet) to initialize descendant models (DesNets) of different sizes. Building on this idea, \\textbf{CoL} constructs a sparse sequence of intermediate models, forming a \\textit{learngene chain}, through a few stepwise distillation steps from the AnsNet. Besides, a \\textit{bridge distillation} mechanism is introduced to support AnsNets with different architectures or vocabularies. Finally, \\textbf{CoL} initializes variable-sized SLMs via parameter interpolation between adjacent models in the chain, thereby eliminating duplicate access to the LLMs.\nExperiments show that \\textbf{CoL} significantly improves efficiency, scalability, and downstream performance. For instance, a 138M DesNet initialized by \\textbf{CoL} without any recovery pre-training outperforms scratch-trained models on a 10B-token corpus.", "tldr": "", "keywords": ["Learngene", "Large Language Model", "Model Initialization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dcaedd587d18038836c5bb6955c471df27f9b88b.pdf", "supplementary_material": "/attachment/722b0c0529c0114ad3e9f100e95aedb61a3e3326.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Chain-of-Learngene (CoL), a novel framework for efficiently building and initializing variable-scale small language models (SLMs) tailored to diverse resource-constrained deployment scenarios. Building upon the Learngene methodology, CoL constructs a sparse sequence of intermediate models—termed a \"learngene chain\"—through stepwise and bridge distillation techniques adaptable to heterogeneous architectures. Target SLMs are then initialized by interpolating parameters across checkpoints in this chain. The approach is designed to minimize repeated reliance on large teacher models, enhance scalability, and accelerate training. The authors provide theoretical justification, ablation studies, and comprehensive experiments showing that CoL achieves faster convergence, higher efficiency, and superior performance on downstream tasks compared to both from-scratch training and existing Learngene or knowledge distillation approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CoL introduces an efficient pipeline for producing SLMs at multiple scales, dramatically reducing the need for repeated pre-training or teacher access.\n\n2. The use of intermediate models and the bridge distillation mechanism for architectural and vocabulary mismatches a practical and well-justified extension over vanilla distillation or previous Learngene-style methods.\n\n3. Experimental results show that CoL-initialized SLMs outperform models trained from scratch, achieving faster convergence and requiring fewer pre-training tokens on core benchmarks."}, "weaknesses": {"value": "1. Despite some comparisons, the paper does not directly discuss or empirically benchmark against several recently published methods designed for scalable SLM training or variable-size initialization, such as HyperCloning[1], or Stacking Small LMs[2].\n\n[1] Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization\n\n[2] Stacking Small Language Models for Generalizability\n\n2. The authors do not elucidate whether the learner-gene chain or stepwise distillation in CoL offers superior transfer, convergence, or robustness compared to these approaches, especially under tight computational constraints\n\n3. The authors do not provide broader ablation in even more severely limited resource regimes and clearer analyses underpinning why CoL degrades as training token numbers shrink further"}, "questions": {"value": "1. Will the authors present head-to-head empirical results, including training tokens, downstream accuracy, and wall-clock efficiency versus HyperCloning, MiniCPM, or other scalable SLM frameworks?\n\n2. Is CoL applicable and effective for very small (e.g., <50M) or much larger (e.g., >1B) SLM architectures? Are there situations where the learnergene chain or bridge distillation is counterproductive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hK6WsUY7VP", "forum": "UEamVVlLBd", "replyto": "UEamVVlLBd", "signatures": ["ICLR.cc/2026/Conference/Submission6763/Reviewer_ZnfQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6763/Reviewer_ZnfQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760581773833, "cdate": 1760581773833, "tmdate": 1762919042571, "mdate": 1762919042571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chain-of-Learngene (CoL), a scalable framework designed to build and small language models (SLMs). The authors identify the costs associated with training SLMs from scratch or repeatedly applying knowledge distillation for each target model size. To overcome these challenges, CoL constructs a \"learngene chain\" through a stepwise distillation process from a large ancestor model (AnsNet). This stepwise approach mitigates the capacity gap between teacher and student models. The framework also incorporates a \"bridge distillation\" mechanism to accommodate AnsNets with different architectures. Experiments show that CoL saves training corpora, accelerates convergence, and achieves better performance on downstream tasks compared to training from scratch and other KD methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this paper is its the idea of CoL. The idea of chains is intuitive and powerful. The stepwise distillation is an elegant solution to the capacity gap issue\n- The paper provides a theoretical analysis to support the use of stepwise distillation. It proves that this multi-step approach yields tighter error bounds compared to direct distillation from a large teacher to a small student.\n- The efficiency of CoL looks impressive.\n- The experimental evaluation is comprehensive. The good performance across multiple DesNet sizes is convincible."}, "weaknesses": {"value": "While the framework works on a heterogeneous AnsNet, the learngene chain itself is constructed from a single, homogeneous architectural family (GPT-2). This may imposes a structural constraint, as all generated DesNets are  interpolations within this fixed architectural space.\n\n- The stepwise distillation process uses a sequential chain for knowledge transfer. Each distillation step is probably lossy, yet the paper does not show some metrics for cumulative degradation of knowledge as it propagates down the chain. It is plausible that checkpoints from the AnsNet retain a less faithful representation of the original knowledge.\n\n- The parameter interpolation is described at a high level (layers, heads, hidden dimensions) . It is unclear how the method would handle more functionally architectural differences, such as varying activation functions (e.g., SwiGLU vs. GeLU) or normalization layers (e.g., LayerNorm vs. RMSNorm), which could limit the architectural variety of the target DesNets.\n\n- The related work part is a bit short, consider citing more related papers. The core idea of CoL is novel in distillation, but is also partially reflected in some of previous works. For example, a recent work named Chain-of-Model [1] also shows similar ideas in building models of different size.\n\n\n[1]: Song et al. Chain-of-Model Learning for Language Model. NeurIPS 2025."}, "questions": {"value": "Could you provide a precise some more details for calculating the \"relative distance\" used to set the interpolation coefficient $\\alpha$? Have you explored alternative, more automated methods for setting this hyperparameter, such as making it learnable?\n\n- What was the rationale for selecting three checkpoints (GPT2-L, M, B) for the learngene chain? Have you investigated how the number and size distribution of checkpoints affect the trade-off between the initial chain construction cost and the performance of the final interpolated DesNets?\n\n- The paper uses reverse KL divergence for the stepwise distillation process. Did you experiment with other distillation objectives, such as matching hidden states or attention maps? Given that the goal is to create a rich knowledge repository in the chain, could feature-based distillation methods help preserve more granular information at each step compared to only matching the output distribution?\n\n- Regarding LInit method, is it limited to generating DesNets whose architectures are a direct interpolation between two checkpoints, or it is also possible to adapt the framework to initialize different architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LrCXiLVLgN", "forum": "UEamVVlLBd", "replyto": "UEamVVlLBd", "signatures": ["ICLR.cc/2026/Conference/Submission6763/Reviewer_JkBQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6763/Reviewer_JkBQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575440841, "cdate": 1761575440841, "tmdate": 1762919042137, "mdate": 1762919042137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chain-of-Learngene (CoL, in short), which progressively distils intermediate checkpoints and then creates a target-sized model by interpolating the two intermediate models. This is to counter the need to distill a source LM multiple times to obtain variable-sized models, effectively reducing computational costs. Experiments show that CoL is more effective than training from scratch, single adaptation, and other knowledge distillation baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes an approach to progressively distill intermediate checkpoints and then create a target-sized model, inspired by Learngene. Given some variable-sized models from the same family, the approach allows us to create an effective target-sized model compared to directly distilling a model from a single checkpoint.\n\n2. The paper is mostly well-written and easy to read. (But, fine-tuning and prompt details should be briefly explained in the main body of the paper.)\n\n3. Extensive experiments clearly show the effectiveness of the approach against (i) models trained from scratch; (ii) single adaptation; and (iii) other knowledge distillation baselines."}, "weaknesses": {"value": "1. Given that the heterogeneous setup often exhibits poorer performance against the homogeneous setup (Figure 3, Table 1), I do not see any point in using heterogeneous models in the proposed method. I think this makes the contribution of the paper less impactful.  \n\n2. While I understand the motivation behind the proposed method, i.e., instead of distilling N separate models to get N small LMs, distilling 3 (a static number) checkpoints to save compute, I do not think this scenario occurs frequently. I think this limits the perceived impact of the study.  \n\n3. While the paper provides comparisons against Vanilla Learngene and Auto-Learngene, it does not provide comparisons against more recent approaches like Learngene Pool (Shi et al., 2024) and SWS (Xia et al., 2024), mentioned in the related work. This warrants justification.  \n\n4. The employed checkpoints are quite old: GPT-2 (2019), despite the fact that there are many other alternatives like Pythia, Qwen3, etc. This selection warrants justification.  \n\n5. While I acknowledge the effectiveness of having multiple intermediate checkpoints to boost performance in Table 2, the experimental setup for Single sounds unfair. Specifically, it does not account for the total amount of compute required to obtain the corresponding CoL models."}, "questions": {"value": "1. On Weakness 1, what are the benefits of using heterogeneous models?  \n\n2. On Weakness 2, would it be able to showcase some concrete examples that require distilling multiple models at the same time?  \n\n3. On Weakness 4, do the presented results and trends hold for more recent models like Pythia and Qwen3?  \n\n4. On Weakness 5, what happens if the comparison for Single models uses the same or a similar amount of compute? Do CoL models still exhibit better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "842rU2tuhh", "forum": "UEamVVlLBd", "replyto": "UEamVVlLBd", "signatures": ["ICLR.cc/2026/Conference/Submission6763/Reviewer_kK3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6763/Reviewer_kK3B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983476927, "cdate": 1761983476927, "tmdate": 1762919041779, "mdate": 1762919041779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}