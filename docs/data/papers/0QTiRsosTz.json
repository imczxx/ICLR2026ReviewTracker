{"id": "0QTiRsosTz", "number": 15431, "cdate": 1758251279052, "mdate": 1759897307515, "content": {"title": "Neural SDEs as a Unified Approach to Continuous-Domain Sequence Modeling", "abstract": "Inspired by the ubiquitous use of differential equations to model continuous dynamics across diverse scientific and engineering domains, we propose a novel and intuitive approach to continuous sequence modeling. Our method interprets timeseries data as discrete samples from an underlying continuous dynamical system, and models its time evolution using Neural Stochastic Differential Equation (Neural SDE), where both the flow (drift) and diffusion terms are parameterized by neural networks. We derive a principled maximum likelihood objective and a simulationfree scheme for efficient training of our Neural SDE model. We demonstrate the versatility of our approach through experiments on sequence modeling tasks across both embodied and generative AI. Notably, to the best of our knowledge, this is the first work to show that SDEbased continuous-time modeling also excels in such complex scenarios, and we hope that our work opens up new avenues for research of SDE models in high-dimensional and temporally intricate domains.", "tldr": "We proposed a new sequence modeling approach tailored to continuous domain, as a unified framework for embodied and generative AI tasks.", "keywords": ["Neural Stochastic Differential Equations", "Flow Matching", "Diffusion Models", "Continuous-Time Sequence Modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4024c46ff1aee8b99ee01ee64ac5cd4d330dd3c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel Neural Stochastic Differential Equation (Neural SDE) framework for continuous-domain sequence modeling. It treats time-series data as samples from an underlying continuous dynamical system and models its evolution using an SDE with neural networks for both the drift (deterministic trend) and diffusion (stochastic fluctuation) terms. Key contributions include a simulation-free maximum likelihood training objective and a decoupled two-stage optimizer, which eliminate the need for costly forward simulations during training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Neural SDE framework introduces a simulation-free maximum likelihood approach, enabling more efficient modeling of continuous-time sequences.\n\n2. Its effectiveness has been validated across multiple experiments, including imitation learning and video prediction tasks.\n\n3. The manuscript is well-written, with clear and concise presentation, ensuring strong readability."}, "weaknesses": {"value": "1. Generative models do not focus on intermediate processes; consequently, their efficacy in sequence modeling tasks remains somewhat limited.\n\n2. To better verify the effectiveness of the proposed Neural SDE approach, it is necessary to design ablative experiments concerning its innovations."}, "questions": {"value": "1. In Equation 7, $ùëã_t$ is constructed using stochastic interpolation (Equation 5). However, since stochastic interpolation lacks physical realism, how can the consistency between its gradients and the vector field be guaranteed?\n\n2. The paper proposes two simplifying assumptions in Section 5. Could the authors comment on the framework's potential for future extensions regarding these two aspects?\n\n3. When the Œît in the first term of Equation 11 varies, the effect of the loss function is expected to differ. For instance, when Œît is relatively large, Œîx/Œît may not adequately approximate the gradient, potentially leading to a significant bias in the first term.\n\n4. See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D5tjeFscOI", "forum": "0QTiRsosTz", "replyto": "0QTiRsosTz", "signatures": ["ICLR.cc/2026/Conference/Submission15431/Reviewer_CvJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15431/Reviewer_CvJZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480196142, "cdate": 1761480196142, "tmdate": 1762925706974, "mdate": 1762925706974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a maximum-likelihood (MLE)‚Äìbased unified Neural SDE framework for sequence modeling. From the single-step transition NLL, it derives a two-stage decoupled training objective: a log-squared residual regression for the drift L_f (Eq. 14) and a ‚Äúresidual matching‚Äù loss for the diffusion L_g (Eq. 15), which avoids forward/backward SDE unrolling (Eqs. 11, 13, 14, 15; ¬ß3.2). The authors introduce a desingularization constant \\delta and prove a temporal scale invariance theorem, enabling training on datasets without explicit timestamps (Appx. D, Thm. 1). Experiments span 2D bifurcation, Push-T imitation learning, and KTH/CLEVRER video prediction, with NFE‚Äìquality tradeoffs, a unified FVD protocol, and ablations (Fig. 2; Tab. 1‚Äì3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Decoupled optimization from the NLL: analytic diffusion optimum leads to a log-squared drift loss and diffusion residual matching; no SDE unrolling needed (Eqs. 11, 13‚Äì15).\n* Scale-invariance and large-error robustness of the log-squared loss, plus desingularization \\delta (Appx. D, Eq. 23).\n* Temporal scale invariance theorem supports training with an arbitrary uniform \\Delta t when timestamps are missing.\n* Unified FVD protocol with repeated evaluation mitigates sample sensitivity.\n* Strong sampling efficiency: about 17 steps achieve quality comparable to 100-step baselines (Fig. 2 and notes).\n* Scaling behavior and ‚Äúimplicit interpolation‚Äù highlight advantages of continuous-time modeling (Fig. 3; Appx. F)."}, "weaknesses": {"value": "* On two video datasets, FVD is often worse than Flow Matching or PFI; results are competitive rather than clearly superior (Tab. 2).\n* Push-T lacks variance and statistical tests, limiting evidential strength (Tab. 1).\n* No systematic ablation or convergence curves comparing decoupled vs. joint training.\n* Diagonal diffusion and Markov assumptions limit cross-dimensional noise correlations and long-range dependencies (¬ß6).\n* The denoiser weight \\alpha is hand-tuned; the generalization cost is unclear (¬ß6)."}, "questions": {"value": "1. How does decoupled versus joint training perform on video tasks? Please provide side-by-side training curves and final metrics under matched budgets.\n2. Can \\alpha be learned or made adaptive? Can it be linked to g(x) or state uncertainty to reduce tuning overhead (cf. Appx. I)?\n3. For ‚Äúfree interpolation,‚Äù can you report quantitative metrics and a visualization set, compared to interpolation or retraining baselines?\n4. Beyond NFE‚Äìperformance, can you report end-to-end wall-clock latency and throughput, including different batch sizes?\n5. For the 20 repeats in Tab. 2, do they span seeds and data resampling? Did you run significance tests (e.g., paired tests on FVD)?\n6. For Push-T, can you add more metrics (e.g., success rate, path length minimization) and report confidence intervals over multiple runs?\n7. Possible typos in Eqs. 11/13/14/15: should \\Delta t_i be \\Delta t instead?\n8. Line 232: remove the trailing period.\n9. The derivation mixes Hadamard products with g(x_t) treated as a diagonal matrix, which is inconsistent. For clarity, replace the Hadamard product with standard matrix multiplication.\n10. In Eq. (15), consider removing the factor 1/2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0c2UyQMn9o", "forum": "0QTiRsosTz", "replyto": "0QTiRsosTz", "signatures": ["ICLR.cc/2026/Conference/Submission15431/Reviewer_dx3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15431/Reviewer_dx3f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914747819, "cdate": 1761914747819, "tmdate": 1762925706562, "mdate": 1762925706562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach to train neural SDEs to model continuous sequential data. Unlike existing approaches for training neural SDEs which require simulating the SDE at training time, the authors propose a two-stage simulation-free optimization scheme derived from negative log-likelihood based on Euler discretization. This is done via cleverly exploiting the Markovian assumption in observations, and using a time-invariant SDE with diagonal diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I find the proposed two-stage optimization framework for fitting neural SDEs quite elegant and intuitive. The idea of decoupling the training of the drift and diffusion terms can potentially solve the current optimization, stability, and diffusion term interpretability challenges with current approaches.\n\n- The claimed empirical benefits of the approach (e.g., fewer NFEs, modeling multi-modal distributions, scale invariance) are well supported by experiments.\n\n- The authors clearly list the limitations of the proposed approach."}, "weaknesses": {"value": "The main challenge I have with this paper is that the motivation/the problem that it is trying to solve is not very clear to me.\nFirst, the authors generally argue for adopting neural SDEs as a unified approach to sequence modeling connecting both dynamical systems in science and engineering described as differential equations as well as modern generative models. It is not quite clear to me how this is achieved with the proposed approach and conducted experiments. Prior approaches which argued for this [1,2] argue for viewing SDEs as differentiable mathematical objects that can incorporate neural networks. This enables developing models that are domain-informed or comparing white, grey, and black box models. The proposed approach and experiments focus on black box neural SDEs, and tasks that are not based on science/engineering applications.\n\nSecond, the authors argue that the proposed approach enables simulation-free training for generative modeling of sequence data. They argue that existing approaches either require expensive SDE simulation during training or modern bridge-based approaches applied on sequential data can be expensive due to uninformative priors or don't respect true temporal progression. While these statements are fundamentally true, there are several methods over the last 2 years that aim to address these problems and show superior performance on modeling sequential data[3,4,5,6], while already incorporating partial observability, external covariates, and long-range dependencies. These approaches are never discussed in the paper.\n\nReferences\n\n[1] Rackauckas, Christopher, et al. \"Universal differential equations for scientific machine learning.\" arXiv preprint arXiv:2001.04385 (2020).\n\n[2] ElGazzar, A., & van Gerven, M. (2024). Universal differential equations as a common modeling language for neuroscience. arXiv preprint arXiv:2403.14510.\n\n[3] Bartosh, G., Vetrov, D., & Naesseth, C. A. (2025). SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations. arXiv preprint arXiv:2502.02472.\n\n[4] Zhang, X. N., Pu, Y., Kawamura, Y., Loza, A., Bengio, Y., Shung, D., & Tong, A. (2024). Trajectory flow matching with applications to clinical time series modelling. Advances in Neural Information Processing Systems, 37, 107198-107224.\n\n[5] El-Gazzar, A., & van Gerven, M. (2025). Probabilistic Forecasting via Autoregressive Flow Matching. arXiv preprint arXiv:2503.10375.\n\n[6] Kollovieh, M., Lienen, M., L√ºdke, D., Schwinn, L., & G√ºnnemann, S. (2024). Flow matching with gaussian process priors for probabilistic time series forecasting. arXiv preprint arXiv:2410.03024."}, "questions": {"value": "- Can the authors elaborate further on how their approach unifies sequence modeling? \n- Can the dervied optimization framework work for non-neural sdes or hybrids?\n- Following the unfied approach argument, can the author comment on how the proposed optimization framework can be extended for partial obsevations?\n- I found the use of the term flow coefceint to desribe the drift function rather confusing and dont belive its standard in  the literature. Can the authors elaboarte on that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "- Can the authors elaborate further on how their approach unifies sequence modeling?\n- Can the derived optimization framework work for non-neural SDEs or hybrids?\n- Following the unified approach argument, can the authors comment on how the proposed optimization framework can be extended for partial observations?\n- I found the use of the term flow coefficient to describe the drift function rather confusing and don't believe it's standard in the literature. Can the authors elaborate on that?"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SXGxoqku9m", "forum": "0QTiRsosTz", "replyto": "0QTiRsosTz", "signatures": ["ICLR.cc/2026/Conference/Submission15431/Reviewer_WZku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15431/Reviewer_WZku"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762353884703, "cdate": 1762353884703, "tmdate": 1762925705457, "mdate": 1762925705457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Neural SDE framework for sequence modeling that treats data as samples from an underlying continuous-time dynamical system and trains without simulating the SDE. Using Euler‚ÄìMaruyama, it models one-step Gaussian transitions to form a maximum-likelihood (NLL) objective, then decouples learning into two stages: fit the drift via a log-squared residual loss and estimate the diffusion via residual matching. Under time-invariant dynamics with diagonal diffusion for efficiency, the method enables fast, parallelizable sampling and low inference cost. Evaluated on trajectory prediction, imitation learning, and video prediction, it delivers performance comparable to diffusion/flow-matching approaches while exhibiting favorable (power-law) scaling behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and easy to follow. The authors also provide an anonymized code base.\n- The proposed method appears simple and efficient. In particular, the idea of decoupled two-stage optimization simplifies the approach.\n- The authors did a good job on the practical part of the work, presenting diverse experiments on different benchmarks and providing both qualitative and quantitative analyses."}, "weaknesses": {"value": "The main issue with this work, in my opinion, is the lack of novelty. I believe that training an SDE by treating consecutive samples from an Euler‚ÄìMaruyama (EM) discretization and performing (quasi-)maximum likelihood estimation is a long-established idea. For instance, a very similar approach was described as early as 1995 [1].\n\nMoreover, the authors do not discuss more recent literature such as Trajectory Flow Matching (TFM) [2], which proposes a very similar simulation-free training method for Neural SDEs (they even have a similar separate optimisation of the diffusion coefficient), or the more general SDE Matching [3] and ARCTA [4], which enable simulation-free training of non-Markovian processes.\n\nThe second major issue is the limited applicability. As the authors admit, their method only works with Markov processes. However, they also rely on EM discretization, which means it only allows training with relatively dense observations. This limitation is particularly unfortunate, since it seems that it could be relatively easily mitigated by conditioning on the last K observations and constructing interpolations between consecutive observations (as done in TFM).\n\nMinor:\n- Figure 1 is never cited in the text.\n\n[1] Pedersen, Asger Roer. \"Consistency and asymptotic normality of an approximate maximum likelihood estimator for discretely observed diffusion processes.\" Bernoulli (1995): 257‚Äì279.\n\n[2] Zhang, Xi Nicole, et al. \"Trajectory flow matching with applications to clinical time series modelling.\" Advances in Neural Information Processing Systems 37 (2024): 107198‚Äì107224.\n\n[3] Bartosh et al. \"SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations.\" The 43rd International Conference on Machine Learning (2025).\n\n[4] Course, K. and Nair, P. \"Amortized reparametrization: efficient and scalable variational inference for latent SDEs.\" Advances in Neural Information Processing Systems, 36 (2023)."}, "questions": {"value": "- Please address the Weaknesses section above.\n- Could you comment on Equation 14? The text describes it as a mathematically correct approach. However, it seems to be a mathematically motivated heuristic, since the optimal diffusion coefficient depends not only on the current state but also on Œî. Therefore, we cannot simply substitute an optimal diffusion coefficient into Equation 11 and derive Equation 14. If my understanding is correct, I believe this point should be discussed more accurately in the text.\n- Could you provide more details about the setup for DDIM and Rectified Flow in your 2D branching trajectories experiment? I got the impression that you trained these models using samples from your branching trajectories as noisy samples. However, these models were not designed to operate in this regime. Since your goal is to compare your time-series modeling approach with other time-series modeling methods, it would be more appropriate to train DDIM and Rectified Flow to generate the entire sequence as a single high-dimensional object. While this would be computationally more expensive than your approach, I would not expect it to have any issues with sample quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q14jkWcBzy", "forum": "0QTiRsosTz", "replyto": "0QTiRsosTz", "signatures": ["ICLR.cc/2026/Conference/Submission15431/Reviewer_JUk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15431/Reviewer_JUk1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762434582827, "cdate": 1762434582827, "tmdate": 1762925703705, "mdate": 1762925703705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}