{"id": "dDHnO3Vhyj", "number": 12987, "cdate": 1758212470752, "mdate": 1759897472326, "content": {"title": "Closing the Gap Between Text and Speech Understanding in LLMs", "abstract": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts—and even cascaded pipelines—on language understanding tasks. We term this shortfall the text–speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD—Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation—which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from publicly available corpora.", "tldr": "", "keywords": ["Speech language models", "large language models", "multimodal language models", "modality alignment", "cross-modal alignment", "cross-modal transfer", "cross-modal distillation", "modality gap", "speech processing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/016bdbb26f78086395ed7ff8b2bba10f42daff5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the text-speech understanding gap—the performance drop when speech-adapted LLMs process spoken inputs versus text inputs to text-based LLMs. \nThe authors decompose this gap into two factors: (i) catastrophic forgetting of text capabilities during speech adaptation, and (ii) cross-modal misalignment between speech and text representations.\nBased on this analysis, they propose SALAD (Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation), which combines cross-modal knowledge distillation with active data selection to improve alignment while mitigating forgetting. Applied to Qwen2.5 3B and 7B models, SALAD achieves competitive performance with recent speech-adapted LLMs while reportedly using much less training data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem decomposition: The two-factor analysis separating forgetting from cross-modal misalignment is conceptually clear and provides a useful framework. The mathematical formalization using KL divergence (Equations 2-3) enables quantitative measurement of both factors.\n2. Rigorous empirical analysis (Section 3): The systematic study of how different training objectives (α parameter) affect forgetting and misalignment is valuable. The scaling law analysis with fitted curves (Table 2) and cross-validation provides insights into training dynamics.\n3. Clear presentation and organization: The paper is well-written with logical flow from problem analysis to solution. Mathematical formulations are precise and figures effectively communicate key results."}, "weaknesses": {"value": "1. Missing validation of core motivation: The paper motivates end-to-end approaches over cascaded systems by citing ability to capture \"paralinguistic richness essential for natural spoken interaction\" (Introduction). Yet no experiments evaluate paralinguistic understanding (emotion, prosody, speaker characteristics). The distillation objective enforcing identical text-speech distributions may actually suppress these cues, contradicting the motivation. This is very important because if content is the only thing that you want to model, then you can simply do cascaded system and in fact ASR+LLM backbone (Qwen2.5) is still significantly better as shown in Table 3.\n2. Limited experimental scope: Only multiple-choice QA tasks—no open-ended generation. Would be curious to see if findings still hold in open-ended generation.\n3. Entirely synthetic evaluation undermines validity: All benchmarks evaluate TTS-generated speech rather than natural speech. This raises questions about whether improvements generalize to real speech with acoustic variability, accents, and spontaneous characteristics. Table 8's limited test on VoiceBench shows performance degradation with different TTS speakers, but more analysis is needed.\n4. Active selection assumes high-misalignment clusters represent domain gaps, but they could equally represent intrinsically difficult content or TTS artifacts. Some discussion around this would be helpful."}, "questions": {"value": "Check weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lOagNIXLJs", "forum": "dDHnO3Vhyj", "replyto": "dDHnO3Vhyj", "signatures": ["ICLR.cc/2026/Conference/Submission12987/Reviewer_VbQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12987/Reviewer_VbQm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522466242, "cdate": 1761522466242, "tmdate": 1762923738764, "mdate": 1762923738764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent performance gap between large language models (LLMs) adapted for speech input and their original text-based counterparts. While LLMs excel at text-based language understanding, adapting them to process speech directly results in a notable drop in performance - which is termed as \"text - speech understanding gap\".  The key contributions of this paper include:\n1. it analyzes the text–speech understanding gap in a quantifiable way and diagnoses 2 main factors that may cause this gap: forgetting and cross-modal misalignment.\n2. it proposes a 2-stage training strategy called SALAD that first does cross-modal distillation on natural speech data to improve alignment and mitigate forgetting, followed by an active synthetic speech sample selection to address domain coverage issue.\n\nEmpirical results show that SALAD achieves competitive performance compared to strong open-source speech-adapted LLMs, with much less speech data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\nThis paper formalize the common phenomenon in speech-adapted LLMs where the LLMs exhibits significant gap between text and speech understanding. The paper also proposes a quantifiable metric to measure this phenomenon and provides statistical measure for both forgetting and cross-modal misalignment.\n\nQuality:\n1. The paper presents a thorough empirical evaluation, benchmarking SALAD against a wide range of open-source speech-adapted LLMs.\n2. The analysis appears to be rigorous, accompanied with clear quantification of the factors contributing to the \"text - speech understanding gap\". \n\nClarity:\nThe paper is well organized and easy to follow. \n\nSignificance:\n1. This work attempts to address a challenge when training a speech-LLM: it requires massive amount of speech data to achieve on-par speech understanding capabilities which is often not accessible. SALAD exhibits comparable performance while being less data-hungry.\n2. The insights into the root cause of the text-speech understanding gap could be influential for future research on speech-LLMs."}, "weaknesses": {"value": "1. The experiments focus on English speech and text, which couldn't address the generalizability to other languages or domains. Similarly, the synthetic data only contains 1 voice, which raises question about how the model can adapt to speaker variance.\n2. Besides the quantitative results, it would be beneficial to provide some qualitative analysis to intuitively demonstrate how SALAD training minimize the text speech gap."}, "questions": {"value": "1. In Table 3, How to interpret that SALAD-3B often achieves better/smaller text-speech gap than SALAD-7B? Doesn't larger model require more training steps?\n2. Could there any negative impacts on model's capabilities in understanding audio/paralinguistic inputs when adopting SALAD training?\n3. In Table 4, does the fact that SALAD-7B gets boosted on text task after SALAD training indicate that the training sets employed is giving SALAD models advantage in these tasks? (i.e. in-domain vs out-of-domain?)\n\nI am giving a rating of 6 but am open to reconsider when authors answer these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fIooGt8Sdn", "forum": "dDHnO3Vhyj", "replyto": "dDHnO3Vhyj", "signatures": ["ICLR.cc/2026/Conference/Submission12987/Reviewer_TPWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12987/Reviewer_TPWY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871522728, "cdate": 1761871522728, "tmdate": 1762923738325, "mdate": 1762923738325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies and analyzes the \"text-speech understanding gap\" in speech-adapted Large Language Models (LLMs), attributing it to two quantifiable factors: forgetting of pre-trained text capabilities and cross-modal misalignment. To bridge this gap, the authors propose SALAD, a two-stage method comprising: (1) Cross-modal distillation from the text-based LLM teacher to the speech-adapted student model, and (2) Active selection of a minimal amount of synthetic speech data to target residual domain misalignment. Experiments on 3B and 7B models show that SALAD achieves competitive performance with state-of-the-art models while using significantly less speech data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Formally defining and quantifying the text-speech gap via forgetting and misalignment.\n- The paper is well-written and easy to follow.\n- The paper shows that high performance can be achieved with significantly less data."}, "weaknesses": {"value": "- The paper does not meet the standard for a thorough related work section, failing to properly situate itself within the current literature and justify its novelty.\n- The paper fails to discuss and contrast its approach with highly relevant work, such as BLSP-KD and TASTE.\n- The paper lacks of an ablation study on the hyperparameters, such as K and γ."}, "questions": {"value": "- Given that  BLSP-KD already demonstrated the power of cross-modal distillation for this problem, what is the marginal contribution of the active learning component? \n- Please provide an ablation on the cluster count K and the exponent γ.\n- The paper choses a \"worst-case\" encoder to make a strong claim. However, how does SALAD compare against state-of-the-art non-causal encoders with built-in alignment mechanisms (e.g., TASTE)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VaU8OclD0v", "forum": "dDHnO3Vhyj", "replyto": "dDHnO3Vhyj", "signatures": ["ICLR.cc/2026/Conference/Submission12987/Reviewer_nz78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12987/Reviewer_nz78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970397532, "cdate": 1761970397532, "tmdate": 1762923737730, "mdate": 1762923737730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}