{"id": "MIRtxjuZF6", "number": 929, "cdate": 1756823954783, "mdate": 1763701588642, "content": {"title": "SEDiT: Mask-Free Video Subtitle Erasure with Prompt Instruction", "abstract": "Recent breakthroughs in video diffusion models have significantly accelerated the development of video editing techniques. However, existing methods often rely on inpainting video frames based on masked input, which requires extracting the target video mask in advance. The precision of the segmentation directly affects the quality of the completion. In this paper, we present $\\textbf{SEDiT}$, a novel one-stage video \n$\\textbf{S}$ubtitle $\\textbf{E}$rasure approach via $\\textbf{Di}$ffusion $\\textbf{T}$ransformer. We introduce a mask-free training approach, driven by prompt instructions, that enables direct erasure of targeted subtitle. The proposed one-stage framework mitigates the suboptimality inherent in the two-stage processing of prior models. To address the challenge of long-term temporal consistency, we adopt a hybrid training strategy by occasionally conditioning the model with clean first-frame latent. This facilitates temporal continuity, allowing each segment during inference to leverage the output of its predecessor. To avoid visible seams caused by cropping and reinserting processed targets, particularly in scenarios involving substantial motion, we feed the original video directly into SEDiT. Thanks to the highly compressed Variational Autoencoder (VAE) in the base model and chunk-wise streaming inference, our method can efficiently handle naive 1080p video with infinite length.", "tldr": "end-to-end mask-free video subtitle removal", "keywords": ["video inpainting", "subtitle removal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc74f97fba13fd5f520010cd08b99ab01711a621.pdf", "supplementary_material": "/attachment/39db440dd5a2feb8367981bdb9b988d52bd754c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SEDiT, a DiT–based framework for inference-time mask-free video subtitle removal. Instead of relying on optical character recognition (OCR) and the inpainting mask, SEDiT performs subtitle erasure through prompt-guided conditioning within a pretrained video generative backbone (LTX-Video). The model processes latent representations of subtitle-containing videos and reference videos by concatenating them along the sequence dimension, allowing training and inference without architectural modification to the original video DiT backbone. A rendering-based data synthesis pipeline is proposed to create large-scale paired data, covering diverse font attributes, layouts, and transition effects for realistic subtitle simulation. To handle long videos, the authors use a chunk-wise inference strategy with single-frame conditioning to maintain temporal coherence.\n\nThe claimed contributions are:\n- A mask-free, prompt-guided framework for video subtitle erasure using a pretrained DiT backbone;\n- A sequence-concatenation conditioning strategy enabling fine-tuning without changing model structure;\n- A chunk-wise processing mechanism for arbitrary input video length and a single-first-frame condition for mitigating temporal discontinuity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method should be easy to generalize to different DiT backbones since it does not require architectural changes, which makes the method adaptive to better DiT backbones in the future to achieve better results.\n- The paper shows a carefully engineered long-video processing pipeline — integrating shot detection, adaptive chunking, and first-frame conditioning to maintain temporal coherence across segments. While such components have appeared individually in prior works (e.g., ProPainter, VideoCrafter, Stable Video Diffusion), the authors present a more systematic and reproducible implementation suitable for large-scale or industrial scenarios.\n- Tab.1 demonstrates noticeable inference-time efficiency, which is valuable for long-video processing.\n- The data synthesis pipeline is comprehensive, considering diverse font attributes, layouts, and transition effects for realistic subtitle simulation.\n- A key practical strength is the mask-free inference pipeline, which avoids the need for external OCR or segmentation models. This design reduces error propagation and simplifies large-scale deployment, although supervision masks are still used during training."}, "weaknesses": {"value": "- I am worried that the second and third points of contributions might be overclaimed. The second (“sequence-wise concatenation of conditional video without architectural modification”) directly follows the conditioning design of FLUX-Kontext and Qwen-Image, representing an adaptation rather than a new method. The third (“chunk-wise processing with first-frame conditioning”) reflects practical engineering design for long videos, not algorithmic innovation. The paper should clarify this hierarchy to avoid overclaiming.\n- The paper does not clarify whether the evaluation videos in VSR-Bench were synthesized using the same rendering pipeline, subtitle styles, or font templates as the training corpus of 400k synthetic videos. If the training and evaluation sets share generation parameters, the model could implicitly overfit to the rendering artifacts rather than generalizing to unseen subtitle styles. Moreover, the authors do not state whether the compared baseline (Minimax-Remover) was retrained or fine-tuned on the same synthetic dataset. Without ensuring consistent data exposure, the comparison risks favoring the proposed model, making it difficult to assess true generalization performance or fairness across methods.\n- Only Minimax-Remover is compared. Recent diffusion-based open-source baselines such as [1][2] or strong inpainting backbones like ProPainter are omitted. Including these would make the comparison credible.\n\n[1] Towards Language-Driven Video Inpainting via Multimodal Large Language Models (CVPR'24)\n[2] Any-length Video Inpainting and Editing with Plug-and-Play Context Control (SIGGRAPH'25)"}, "questions": {"value": "- The paper does not explicitly claim that the mask-free variant outperforms mask-based approaches, but it repeatedly suggests qualitative robustness advantages (e.g., in motivation and qualitative results sections). The argument that mask-based methods are prone to error propagation under imperfect masks is reasonable, yet no controlled experiment isolates whether the observed improvements come from the mask-free formulation itself or the more advanced DiT backbone. A stronger evaluation would include a mask-based baseline built on the same DiT backbone and dataset to clarify whether the benefits arise from the absence of masks or from other architectural or training factors.\n- The fixed-format prompt (“remove subtitles in the video”) is manually chosen, and its functional role is unclear. Does the prompt still function as semantic guidance? Meaning that changing the prompt to instruct other tasks (like \"remove watermark in the video\") would result in a different application. Or is it just a condition token now? Then I wonder how much it contributes to the final performance, and is it really necessary since the paired data should make the model understand the task enough? The paper would be more comprehensive if the authors could include comparisons among (i) fixed-prompt, (ii) prompt-free, and (iii) learnable-token (as in Minimax-Remover) settings to clarify prompt contribution.\n- Some terms are potentially overstated. For example, the method is only \"mask-free\" during inference time, and the pipeline is not technically \"end-to-end\" since it requires external shot-boundary detection and chunk-wise processing with first-frame reinjection. The authors may consider rephrasing to avoid misunderstanding.\n- The commonly used metric, VFID, to assess video quality/perception in the literature is missing.\n- The paper asserts that stylized or blurred subtitles defeat OCR detectors but provides no detection statistics or visual comparisons. Reporting detection recall/F1 of representative OCR algorithms on the same data would substantiate this statement.\n\nMinor writing issues:\n- The paper constantly uses \\citet for all citations. Use \\citet{} when the citation is part of the sentence, e.g., “Zhou et al. (2023) propose xxxxx.” and \\citep{} when the citation is in parentheses, e.g., “Video inpainting has been widely studied (Zhou et al., 2023).”.\n- L72: OCR abbreviation first appears, but no full name is given.\n- Fig.6, left bottom case third column, I think the input and ours show different frames.\n- In Fig.7 and 8, what does the bounding box mean? And what do the green highlights mean? The captions should explain the notations on the figures more clearly.\n- In the supplementary material, I think the third clip of demo 1 mixed the input and output videos.\n- typos: precessing (L92), The reference video latents are patchified ... as the noisy video tokens (L205), Firt-frame conditionin (Fig.5 caption)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5eSB8gQuXx", "forum": "MIRtxjuZF6", "replyto": "MIRtxjuZF6", "signatures": ["ICLR.cc/2026/Conference/Submission929/Reviewer_Hrxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission929/Reviewer_Hrxk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903811090, "cdate": 1761903811090, "tmdate": 1762915642758, "mdate": 1762915642758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents an end-to-end video subtitle erasure model based on a Diffusion Transformer, eliminating the need for additional masks for the subtitle region. More specifically, the authors concatenate the source video latents with noise along the sequence dimension and input them into the DiT backbone, avoiding modification of the model architecture and allowing it to be applied to other video backbone models. During training, the authors applied a focal loss to impose a higher loss penalty on the text regions. Meanwhile, the authors also use first-frame conditioning to enable chunk-wise long video processing. In addition, the authors propose a data synthesis pipeline to simulate various subtitles in real-world videos. From the experimental results, the proposed method achieves highly competitive performance compared to general video object removal methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed solution for removing the video subtitles is straightforward and easy to implement. That would be very helpful if the authors release the constructed dataset."}, "weaknesses": {"value": "1. The reviewer is a bit concerned about the robustness of mask-free subtitle removal. Although the authors proposed a data construction pipeline to generate various kinds of subtitles, this data may not cover all types of subtitles in real-world videos. Meanwhile, if the video contains other text (not subtitles), does the model also remove this text? The authors should conduct more experiments on videos with various types of text to test this.\n\n2. The proposed model is straightforward, but I recommend the authors explore more designs for the reference video conditioning strategy and the long video inference strategy. Meanwhile, it seems that the prompt instruction in this setting is not necessary, so what would happen if the input prompt were removed? In addition, although the method is faster than existing diffusion-based video object removal models, it is still slow and far from real-time inference, which is important in practical application scenarios.\n\n3. The evaluation is insufficient. The authors only compared their model to only one existing general video object removal model (Minimax-Remover). The related work section mentions several other relevant video inpainting and object removal methods (e.g., ProPainter, DiffuEraser, EraserDiT), none of which are included in the comparison. Moreover, the authors should also include results from classical (non-deep learning) methods to better demonstrate the superiority of the proposed method."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VEIPgFqWLv", "forum": "MIRtxjuZF6", "replyto": "MIRtxjuZF6", "signatures": ["ICLR.cc/2026/Conference/Submission929/Reviewer_5uKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission929/Reviewer_5uKc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983489426, "cdate": 1761983489426, "tmdate": 1762915642623, "mdate": 1762915642623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DiT-based end-to-end video subtitle erasure network (SEDiT). To this end, the clean latent of the reference video and the noise latent of the target video are merged using a token-level concatenation in the sequence dimension. Then, they are conditioned by user-prompt subtitle erasing prompts. Additionally, new datasets for video subtitle erasure are introduced with a proposed data synthesis pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Belows are the strong points that this paper has:\n\n1. The proposed architecture for video subtitle erasure is simple and straightforward, requiring no special modifications to the base model.\n\n2.  As illustrated in Figure 3, the data synthesis pipeline is systematically designed, effectively considering the various characteristics and artifacts introduced during subtitle acquisition."}, "weaknesses": {"value": "Belows are the weak points that this paper has:\n\n1. mask M$_{subtitle}$ in Equation (2), which denotes the subtitle area, appears to contradict SEDiT’s claimed mask-free training approach. The authors should clarify this inconsistency.\n\n2. The paper lacks sufficient discussion of related work and baseline comparisons. Subtitle erasure closely relates to video decaptioning, for which several prior studies exist (e.g., BVDNet [1], Deep Video Decaptioning [2]). The authors should provide a self-contained explanation of the decaptioning task and demonstrate the proposed model’s advantages over existing decaptioning methods.\n[1] Kim et al., Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence\n[2] Chu et al., Deep Video Decaptioning\n\n3. Although the proposed architecture is straightforward, it shows limited innovation specific to subtitle erasure. The method primarily combines an existing flow-based model with conditioning techniques, without offering substantial architectural contributions or theoretical insights expected for an ICLR-level paper.\n\n4. The paper lacks ablation studies on key design choices, such as the 3D RoPE used for reference and noisy latents, the number of conditioning frames in the chunk-wise strategy, and the performance on long-video subtitle erasure.\n\n5. While the authors emphasize the importance of efficient VAE modeling, details regarding the design and implementation of the “highly compressed VAE” are missing and should be elaborated."}, "questions": {"value": "Please answer my questions listed above in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ACvZm6brQS", "forum": "MIRtxjuZF6", "replyto": "MIRtxjuZF6", "signatures": ["ICLR.cc/2026/Conference/Submission929/Reviewer_2h8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission929/Reviewer_2h8Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058708969, "cdate": 1762058708969, "tmdate": 1762915642511, "mdate": 1762915642511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mask-free video editing object-removal approach (whole-video, not just a few frames) based on prompt. This is much simpler for the user, and reduce risks of artifacts from the mask. They apply their approach specifically to remove hardcoded subtitles. This could be especially useful in situation were an old fringe movies is only available through a torrent with hardcoded Russian subtitles. I would definitively use this. This would be very useful to archivists who tried to keep high-quality of old media alive. This is high impact.\n\nThey concatenate the reference video with the noisy target video. Text embedding is injected as cross-attention. They have the loss focus more on the subtitle region in order to not focus on reconstruction of the whole scene but specifically the subtitle region. They generate different font at different position and styles. It used chunk-wise of frames to ensure good quality. They found that a single-frame of overlap between chunks is good enough for satisfactory temporal consistency. Based on 400k videos from Pexel website. They make a benchmark from 200 videos. They use LoRA (19% trainable params)\n\nThis is amazing work: this is a simple solution to a simple but important problem. This is high-impact work. \n\n12sec seems slow for 65 frames though. But its not that bad actually. A 30fps movie would take 6h to remove the subtitle. I would add that kind of number to the paper because its quite powerful. This means that with 4 gpus, you could do it in less than 2h.\n\nIt would be great to add some \"worst cases\" where we see artifacts because nothing is perfect, there must be some. Its interesting to see that minimax-remover has clear artifacts sometimes, but not your method.\n\nWhy not remove the user prompt? Since you are training for a single task this seems unnecessary to have the T5 model on the same prompt."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "See \"Summary\""}, "weaknesses": {"value": "See \"Summary\""}, "questions": {"value": "See \"Summary\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QQCijstEgg", "forum": "MIRtxjuZF6", "replyto": "MIRtxjuZF6", "signatures": ["ICLR.cc/2026/Conference/Submission929/Reviewer_Hfxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission929/Reviewer_Hfxz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199608108, "cdate": 1762199608108, "tmdate": 1762915642386, "mdate": 1762915642386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}