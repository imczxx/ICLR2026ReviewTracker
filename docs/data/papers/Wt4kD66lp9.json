{"id": "Wt4kD66lp9", "number": 20861, "cdate": 1758311084613, "mdate": 1763754832635, "content": {"title": "On Improving Neurosymbolic Learning by Exploiting the Representation Space", "abstract": "We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers’ scores. However, the space of label combinations can grow exponentially, making learning difficult. We propose the first technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach is orthogonal to existing training algorithms and can be seamlessly integrated with them. Experiments on three state-of-the-art neurosymbolic engines, Scallop, Dolphin, and ISED, demonstrate up to 74% accuracy gains across diverse tasks, highlighting the effectiveness of leveraging the representation space in neurosymbolic learning.", "tldr": "", "keywords": ["neurosymbolic learning", "weak supervision", "latent representations"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/636f3e918318d1ed334d34114fae7f92dcdc8c71.pdf", "supplementary_material": "/attachment/750d65ffa3a87e30297a1f9843a7f76966e6a73f.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a key challenge in neurosymbolic learning (NESY): the exponential growth of possible label combinations (pre-images) that satisfy the logical constraints during training, which weakens supervision. The authors propose a novel technique, CLIPPER, which prunes this space of candidate pre-images by leveraging the intuition that instances with similar latent representations should share the same label. The core of the method involves constructing a proximity graph based on instance representations and formulating the pruning problem as an Integer Linear Program to discard inconsistent pre-images while ensuring no training sample loses all its candidates. The approach is framework-agnostic and can be applied pre-training or during training. Experiments on three neurosymbolic engines across multiple benchmarks show substantial accuracy improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty is good. The work is the first to systematically exploit the representation space to prune pre-images in a general neurosymbolic learning setting. While similar intuitions exist in partial label learning (PLL), their application under complex logical constraints is novel and non-trivial. The technical approach is well-motivated and formally grounded. The formulation of the pruning problem as an ILP is sound and provides a clear, optimizable objective.\n\n2. The paper is well-written. The running example effectively illustrates the core problem and the proposed solution. The definitions (proximity graph, consistency, and pruning) are clear and build logically towards the problem statement and the proposed ILP formulation.\n\n3. Experiments. The reported performance gains are impressive and demonstrate the potential of this approach to enhance the scalability and accuracy of neurosymbolic systems."}, "weaknesses": {"value": "1. Computational Complexity. The paper formulates the pruning as an ILP, which is NP-hard in general. For large-scale datasets with many training samples and pre-images, solving this ILP could become computationally prohibitive.\n\n2. Dependence on Encoder Quality. The effectiveness of CLIPPER is inherently tied to the quality of the encoder h. Suppose the encoder produces poor representations (e.g., maps instances of different classes close together). In that case, the proximity graph will be noisy, leading to the pruning of correct pre-images or insufficient pruning."}, "questions": {"value": "As stated in Line 315, \"Fourth, as stated in Section 1, our approach can run in a training-free manner or by simultaneously updating the encoder during training.\" Could you please further elaborate on it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rNGvHu54lF", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_jDFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_jDFH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804752673, "cdate": 1761804752673, "tmdate": 1762999991278, "mdate": 1762999991278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of neurosymbolic learning (NeSy) in weakly supervised settings, specifically where the vast number of valid label combinations (pre-images) satisfying a logical formula makes learning prohibitively difficult. The authors propose a novel technique, CLIPPER, to prune this exponentially large space of pre-images by exploiting the latent representation space. The core intuition is that instances with similar latent representations should share the same gold label, and this consistency constraint is used to prune the candidate set. The authors claim that this method significantly improves accuracy, particularly in few-shot settings, when applied to state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a significant and practical challenge in NeSy. The ambiguity arising from a combinatorial explosion of valid pre-images in weakly-supervised settings is a known bottleneck for learning."}, "weaknesses": {"value": "Weaknesses\n\nMy main concern with this paper lies in the experimental evaluation, which feels highly specialized and potentially misleading, casting doubt on the general applicability and true performance gain of the proposed method.\n\nSuspiciously Low Baseline Performance: The reported baseline accuracies for SOTA models like Dolphin and Scallop are extremely low (e.g., ~30-35% on SUM-M and MUGEN). This is in stark contrast to the performance reported in their original papers (e.g., the Dolphin paper), where these models achieve >95% accuracy on the same benchmarks.\n\nArtificial Experimental Setting: The reason for this discrepancy seems to be twofold, and both points suggest the experimental setting was intentionally designed to make the baselines fail:\n\nExtreme Few-Shot Setting: The authors appear to have used an extremely small number of data points (e.g., $n=100$ for SUM-M, $n=250$ for MUGEN). While this is a valid \"few-shot\" setup, it is not the standard way these benchmarks are evaluated.\n\n\"Hardest\" Samples: The authors also state they use samples associated with \"more than 100 pre-images on average,\" creating a \"worst-case\" scenario of ambiguity.\n\nWhile CLIPPER shows gains in this specific, harsh environment, this evaluation fails to demonstrate if the method provides any benefit in the standard, full-dataset setting where baselines are known to perform well.\n\nQuestionable Experimental Graphs (Figure 1): The graphs in Figure 1 are highly concerning:\n\nThe baselines (solid lines) appear to just \"not converged,\" This is because\nThe proposed method, C(ENC) (dotted lines), shows a sudden, dramatic spike in accuracy (e.g., on MUGEN) immediately before the training is stopped (e.g., after epoch 25). How do you guarentee this rise of performance is maintained, and baseline performance is maintained low after long training?\n\nThis is highly suspicious. The MNIST SUM-3 graph (Fig 1a) shows the C(ENC) method's accuracy peaking and then dropping, suggesting instability or overfitting. Stopping the MUGEN experiment right at the peak, and reporting \"best acc,\" hides whether this performance is stable or if it would have collapsed with further training.\n\nThe Core Doubt (Lack of Generalizability): The paper's entire premise rests on showing gains in a few-shot setting where baselines fail. However, my strongest suspicion is that if this method were applied to the standard, large-scale benchmark (as used in the Dolphin paper), the performance gain would be negligible or non-existent, as the ambiguity problem is already solved by the large data. Furthermore, the ILP-based pruning likely adds significant computational overhead, making the method slower than the baselines in a fair \"10-hour fix\" comparison like the one used in the Dolphin paper."}, "questions": {"value": "1. My strongest suspicion is that this method provides no significant gain in a standard setting. The authors must provide a direct comparison against Dolphin using the exact same standard, full-dataset benchmark setting from the Dolphin paper (e.g., MUGEN 5K, HWF-15). This experiment must:\na.  Use the same 10-hour fixed time limit.\nb.  Report the final accuracy at 10 hours (not the \"best\" accuracy).\nc.  Report the additional computational overhead (in time per epoch) caused by the ILP-pruning step.\n\n2. Extending Existing Experiments: To address the highly suspicious graphs in Figure 1, the authors must, at a minimum, re-run their existing few-shot experiments (e.g., MUGEN $n=250$, SUM-M $n=100$) for a full 10-hour time limit.\na.  They must provide graphs showing the full 10-hour training run for both the baselines and their method.\nb.  They must report the final epoch's accuracy at 10 hours, not the \"best\" accuracy. This is crucial to verify if the baselines eventually converge and if the C(ENC) spike is stable or merely a volatile peak."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e9txaFI1bT", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_a97x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_a97x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817241015, "cdate": 1761817241015, "tmdate": 1762999991697, "mdate": 1762999991697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "We thank the reviewers for their feedback and suggestions, and will work on incorporating them in our revised paper. We have added all the tables mentioned in our reply to the revised paper, and will continue updating results for ongoing experiments. All revised text in our paper will be highlighted in blue.\n\n## ILP overhead\nIn Table 1 below, we report the average time to solve the ILP in Eqn 1 and pruning the pre-images per batch for SUM-3 and SUM-4 under different batch sizes and encoders. We add this table to our Appendix D (Tables 7 and 8). Our empirical analysis shows that\n**the runtime of CLIPPER is small despite the fact that the number of pre-images is quite large.**\n\nTable 1: Ablations on SUM-3 and SUM-4 for different encoders, batch sizes, and frozen & pre-trained vs randomly initialized and trainable encoders. In the rows “+ C(ResNet18) Pretrained & Frozen” and  “+ C(ResNet50) Pretrained & Frozen”, the encoder is pretrained and frozen. In the row “+ C(MNISTNet) Trainable”, the encoder is randomly initialized and trainable. MNISTNet is the CNN encoder used in the standard MNIST classifier. NA stands for non-applicable. Times are in seconds.\nExperiment |Algorithm | Classification Accuracy | % Retained Preimages | % Preimages with Ground Truth | Avg Pruning Time (s) | Avg Time to Solve ILP (s)\n-|-|-|-|-|-|-\n|SUM-3, n = 100, Batch size = 64 |DOLPHIN | 31.6 | NA | NA | NA | NA\n| |\\+ C(ResNet18) Pretrained & Frozen   | 47.09 | 77.99 | 91.56 | 0.49 | 0.03\n| |\\+ C(ResNet50) Pretrained & Frozen | 41.89 | 75.7 | 88.38 | 0.51 | 0.03\n| |\\+ C(MNISTNet) Trainable | 41.51 | 78.12 | 94.81 | 0.53 | 0.03\n|SUM-3, n = 100, Batch size = 128|DOLPHIN | 32.86 | NA | NA | NA | NA\n||\\+ C(ResNet18) Pretrained & Frozen   | 49.24  | 80.17  | 92  | 1.88  | 0.06\n||\\+ C(ResNet50) Pretrained & Frozen | 42.56  | 74.21  | 90  | 1.5  | 0.07\n||\\+ C(MNISTNet) Trainable | 36.84  | 77.85  | 96.21  | 1.39  | 0.07\n|SUM-4, n = 100, Batch size = 64|DOLPHIN | 31.43  | NA | NA | NA | NA\n||\\+ C(ResNet18) Pretrained & Frozen   | 31.28  | 87.5 | 96.19 | 3.39 | 0.39\n||\\+ C(ResNet50) Pretrained & Frozen | 28.69  | 86.46 | 92.9 | 3.42 | 0.42\n||\\+ C(MNISTNet) Trainable | 35.19  | 85.88 | 98.58 | 3.34 | 0.39\n\nRegarding the complexity of our ILP formulation, notice the following: \n1. Without any approximations, the complexity of the SOTA NeSy loss, Semantic Loss Xu et al. (2018), is #P-complete (Mark Chavira and Adnan Darwiche. On probabilistic inference by weighted model counting. Artificial Intelligence, 2008). In contrast, ILP is an NP-hard problem. \n2. There are multiple heuristics to solve ILP efficiently, e.g., “Local Branching Relaxation Heuristics for ILP” (Huang et al., 2023).\n3. We can do clever batching of the samples to maximize the effects of pruning as we did in VQAR. In particular, we can group in the same batch samples with very few pre-images (e.g., samples with one pre-image only) and samples with many pre-images. Due to (1) the property of soundness (Definition 3.4), (2) our problem formulation (Problem 3.7) and (3) the properties of our ILP formulation (Proposition 3.8), the pre-images of the former NeSy samples will be maintained, forcing the pre-images of the latter NeSy samples to be pruned. We will elaborate on this in the revised version of our work. \n4. As we explain in lines 230–241, naive greedy pruning compromises soundness. \n\nSince our work focuses on a plug-and-play approach to improve NeSy learning using latent representations, we consider the choice of ILP solver to be orthogonal."}}, "id": "B7iGWPwt1G", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754027930, "cdate": 1763754027930, "tmdate": 1763754873608, "mdate": 1763754873608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLIPPER, a method to improve the efficiency of neuro-symbolic learning by pruning redundant label combinations using representation-space similarity. In typical NeSy systems, each instance has multiple logically consistent label combinations, leading to an exponentially large search space. CLIPPER constructs a proximity graph over latent embeddings and formulates pruning as an integer linear programming (ILP) problem that removes inconsistent candidates while ensuring each instance retains at least one valid label. Integrated with systems such as Scallop, Dolphin, and ISED, CLIPPER achieves notable gains across several benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed approach is technically solid. The integration of representation-space information into neuro-symbolic learning is handled in a careful and systematic way. Experimental results are convincing, showing consistent improvements across multiple benchmarks and engines."}, "weaknesses": {"value": "## Substantial conceptual overlap with NeurIPS 2021 ABLSim; the claimed \"first\" contribution does not hold.\n\nThe paper's stated motivation and contribution closely mirror those of \"Fast Abductive Learning by Similarity-based Consistency Optimization\" [Huang et al., NeurIPS 2021], sharing both the same problem formulation and the same central idea. In both works, the key challenge lies in the exponentially growing space of candidate label combinations, which makes learning in neurosymbolic systems computationally expensive and difficult to scale.\n\nBuilding directly upon this context, the authors claim novelty by stating: \n> \"We propose the first technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label.\"\n\nHowever, this intuition—the use of latent representation similarity to reduce the candidate label space—was precisely the foundational principle of ABLSim. That earlier work already pruned the space of abduced label combinations based on the idea that samples close in representation space should share the same label, introducing a similarity-based mechanism to guide the pruning process and remove inconsistent label configurations. The present paper employs the same conceptual mechanism, reformulated through an ILP-based global pruning approach rather than a similarity-scoring heuristic or beam search. While the implementation differs in its optimization formalism, the underlying conceptual contribution is identical: both methods exploit latent-space similarity to constrain combinatorial search in neurosymbolic learning.\n\n(For context, the paper studies neural classifiers in a neurosymbolic setting where hidden labels must satisfy a logical formula—precisely the Abductive Learning (ABL) setup [Dai et al., NeurIPS 2019], where a neural perception module predicts latent symbolic labels that must conform to a knowledge base. The authors highlight that the space of label combinations satisfying the formula grows exponentially, making training inefficient. This issue has long been recognized as the main computational bottleneck in NeSy learning (also, ABL), and ABLSim was specifically designed to address it by leveraging representation-level similarity to restrict the candidate label space. In other words, ABLSim already tackled the same combinatorial explosion that this paper presents as its core motivation.)\n\nGiven this lineage, **the contribution claim (in lines 60–69) is unsupported**. The conceptual contribution of using latent-space similarity to guide or prune candidate label combinations in neurosymbolic learning was already introduced, formalized, and empirically validated in ABLSim. **While this paper presents a different implementation (an ILP-based formulation and integration with additional NeSy engines), these are implementation-level refinements rather than a genuine conceptual innovation.** The novelty statement should therefore be revised to acknowledge ABLSim’s precedence and to clarify that this work extends an existing idea through a new optimization formulation, rather than being the first to introduce it.\n\n---\n\nOther points:\n\n1. Since the pruning mechanism relies on embedding-space proximity, its effectiveness is strongly tied to how well the encoder captures semantic similarity. However, the paper does not analyze or control for this dependency, with no experiments comparing different encoders or examining performance sensitivity to embedding quality, making it unclear whether the reported improvements stem from the pruning mechanism itself or from stronger underlying representations. Consequently, the empirical results may not substantiate the contribution's key premise that closeness in latent representations reliably implies label equality.\n\n2. *(minor)* The theoretical analysis in this paper, which formulates neuro-symbolic learning as a partial-label learning problem, is not new. Beyond the works mentioned in lines 72–74, approaches applying PLL to NeSy analysis have been presented before. See examples in Section 3.4 of He et al., ICML 2024.\n\n3. *(presentation)* Most of the citation commands (e.g., \\citep, \\citet and \\cite) are not properly used. Most citations should appear in parenthetical form, for example, line 127 should read: *However, in one of the benchmarks that we consider in our experiments, namely VQAR (Huang et al.\n2021), K is commonsense knowledge from CRIC (Gao et al. 2019).* Only a few instances should use the textual form, for example, on line 135: *More details on abduction are in Tsamoura et al. (2021).* \n\n---\n\nHuang et al., Fast Abductive Learning by Similarity-based Consistency Optimization. NeurIPS 2021.\n\nDai et al., Bridging Machine Learning and Logical Reasoning by Abductive Learning. NeurIPS 2019.\n\nHe et al., Ambiguity-Aware Abductive Learning. ICML 2024."}, "questions": {"value": "The method involves potentially expensive steps (e.g., proximity graph construction, ILP solving), yet the paper lacks a discussion on runtime or scalability. How efficient is the overall training compared to the baselines, and can the proposed method scale to large datasets?\n\nOther questions see weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The paper shows substantial motivational and conceptual overlap with prior work (NeurIPS 2021 ABLSim), while claiming novelty for essentially the same idea without proper acknowledgment."}}, "id": "MByZQoTt2E", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_xgWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_xgWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159803393, "cdate": 1762159803393, "tmdate": 1762999991536, "mdate": 1762999991536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the problem of learning neurosymbolic systems where input instances must satisfy logical constraints. Traditional approaches compute all possible label combinations (pre-images) that satisfy these constraints, but this space grows exponentially. The authors propose to  prune inconsistent label combinations by using the proximity of latent representations. The key insight is that instances close in representation space are likely to share labels. The proposed pruning approach is formalized as an integer linear program that maximizes discarded pre-images while ensuring each sample retains at least one valid candidate. Experiments across benchmarks like SUM-M, MAX-M, VQAR, and MUGEN show accuracy improvements of up to 74%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, and the background and proposed approach are very well explained\n2.  The proposed idea is well-motivated and intuitive. The challenges associated with the practical implementation of pre-image space pruning are well-discussed, and the proposed approach effectively addresses these challenges in a simple yet effective manner. \n3. The evaluation sections show very promising results!\n4. CLIPPER is complementary to existing NESY engines and can operate in a training-free or iterative manner."}, "weaknesses": {"value": "1. Overhead of solving ILP is not discussed\n2. Dependence on the quality of pretrained encoder or robustness to encoder noise is not discussed."}, "questions": {"value": "1. What is the overhead of solving the proposed ILP formulation? How does it increase with the batch size?\n2. How does the quality (and size) of the encoder impacts the effectiveness of CLIPPER? \n3. What are the network sizes used in the work? I am curious about whether the size of the model affects the effectiveness of the proposed approach.\n4. What hyperparameters does the proposed linear programming formulation introduce? What values for these hyperparameters are used in the evaluation? An ablation study would also be interesting. \n5. In most of the benchmarks, the accuracy of ISED is very low, and the improvement by using CLIPPER is also minimal, can you provide more details in the evaluation section?\n6. It seems from Figure 1b that the accuracy changes quite abruptly with epochs. For the final reported accuracies, how is the number of epochs determined?\n7. How are the values in the evaluation tables reported? Are they the average of multiple runs? If yes, can you also include the confidence interval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pEy56o5wVX", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_E58h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_E58h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229117126, "cdate": 1762229117126, "tmdate": 1762999991870, "mdate": 1762999991870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the problem of learning neural classifiers whose outputs are subject to logical constraints. In this setting, the space of label combinations can grow exponentially, making learning difficult. The authors propose pruning the space of label combinations by exploiting the intuition that instances with similar latent representations are likely to share the same label"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Overall I find the paper to be quite well-written, barring the critique mentioned below\n\n- The problem is well defined, and the solutions is derived from first principles\n\n- The use of a running example makes it easy to follow the exposition"}, "weaknesses": {"value": "- The authors should use \\citep and \\citet as appropriate throghout the paper. Currently, it appears as though they only make use of \\citet, which makes the paper quite a bit harder to read.\n\n- I find lines 117-121 to be very restrictive. For instance, [1], [2], and [3] do not require that we create facts of the form digit(d, x1). Rather the characterization given is specific to logic programming, which does not encompass the entirety of NeSy.\n\n- \"Unlike supervised learning, in NeSy... the gold labels of the input instances are unknown to the learner.\" I only partially agree with this characterization. Most NeSy settings actually deal with semi-supervised learning where we have some seed labelled data and we have a lot of unlabelled data that we want to use. One very interesting example of this is in weakly-supervised learning [6] and [7]\n\n- \"We aim to reduce the number of candidate pre-images of the NeSy Training sampling by exploiting inconsistencies with the representation space.\" Isn't that addressed through semi-supervised learning using semantic loss? The exploitation of the reasoning space also bears great resemblance to NeSy entropy-regularization [6], which I expected to see as a baseline. Note that an enumeration version of this can be implemented in small domain, just to compare against the proposed approach.\n\n- In my opinion, that is a lot of NeSy work that the authors are missing in the related works section.\n\n- I'm not really sure how scallop and dolphin implement their losses, and how they differ semantically. Also, have the authors tried to use semantic loss as their base approach (one might achieve that through using the sampling version of semantic loss [7]). In the appendix, the authors claim that Scallops implements a scalable version of semantic loss, which I am not sure is entirely true.\n\n- The paper could really use some figure to help illustrate the introduced concepts and methodology. It gets very hard and tedious to keep track of the added training samples in the running example.\n\n- I found it quite a bit confusing disambiguating the use of the variable $l$ for labels as well as to range over the data points in $\\mathcal{D}$. Consequently, while I intuitively understand what is meant be consistency, I failed to fully grasp example 3.3. (what does (1, x_1) -> (2, x'_1) mean?). I understand it to mean the set of non-overlapping assignments to x1 and x'_1 given that we know they are similar. \n\nReferences:\n\n[1] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, & Guy Van den Broeck. A Semantic Loss Function for Deep Learning with Symbolic Knowledge. ICML 2018.\n\n[2] Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, & Antonio Vergari. Semantic Probabilistic Layers for Neuro-Symbolic Learning. NeurIPS 2022.\n\n [3] Tao Li and Vivek Srikumar.. Augmenting Neural Networks with First-order Logic. In ACL 2019.\n\n[4] Jessa Bekker and Jesse Davis. Learning from positive and unlabeled data: A survey. Machine\nLearning 2020.\n\n[5] Vinay Shukla, Zhe Zeng, Kareem Ahmed, & Guy Van den Broeck. A Unified Approach to Count-Based Weakly-Supervised Learning. In NeurIPS 2023.\n\n[6] Kareem Ahmed, Eric Wang, Kai-Wei Chang, & Guy Van den Broeck. Neuro-Symbolic Entropy Regularization. In UAI 2022.\n\n[7] Kareem Ahmed, Tao Li, Thy Ton, Quan Guo, Kai-Wei Chang, Parisa Kordjamshidi, Vivek Srikumar, Guy Van den Broeck, & Sameer Singh. (2022). PYLON: A PyTorch Framework for Learning with Constraints. NeurIPS 2021 Competitions and Demonstrations."}, "questions": {"value": "- Regarding example 3.6, isn't the encoder mapping different instances to very close representation almost unavoidable when using neural embedding models?\n\n- I fail to grasp the implications the absence of the optimality guarantee as stated in the first paragraph of section 3.3\n\n- \"Proposition 3.5 offers [guarantees on preserving the gold pre-images] but the formulation of problem 3.7 does not focus on that aspect.\" do I take that to mean that there are no such guarantee? Am I correct in my understanding that this could potentially eliminate/preclude the correct label by overpruning? Could that result in a case where a data sample is assigned a label that violates the constraint? or the set of labels is empty?\n\n- Doesn't using a gold proximity graph in the experiments essentially solve the problem given enough data points?? A figure would be very helpful here.\n\n- I'm very surprised by the results in Table 5. First off, why is C (Gold) worse than C (Enc)? Second, why does using Clipper with Dolphin increase the accuracy so much compared with Scallop?\n\n- Could you say more on how your approach is used for training? Is it also used for inference? It was not clear how it is integrated into the pipeline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f59wEENaez", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_yU2T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_yU2T"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762555650206, "cdate": 1762555650206, "tmdate": 1762999991538, "mdate": 1762999991538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for improving neurosymbolic learning (NESY) by pruning inconsistent label combinations through representation-space similarity. Traditional NESY systems must handle exponentially many label combinations that satisfy logical constraints, making training inefficient and ambiguous. CLIPPER leverages the intuition that instances close in latent space likely share labels. The method constructs proximity graphs among instances based on encoder similarity, defines consistency constraints for pre-images (label combinations), and formulates an integer linear program (ILP) that discards inconsistent pre-images while preserving at least one valid combination per training sample. Experiments across multiple NESY engines and benchmarks (SUM-M, MAX-M, HWF-7, VQAR, and MUGEN) demonstrate large performance gains, suggesting that pruning inconsistent label configurations improves learning efficiency and disambiguation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method addresses the exponential explosion of label combinations in NESY with an elegant pruning approach.\n- The authors provide clear definitions, soundness guarantees, and ILP optimality proof."}, "weaknesses": {"value": "- The method leverages standard ideas (representation similarity + graph consistency + ILP) without major theoretical innovation.\n- ILP optimization over large NESY datasets may be computationally expensive; no runtime or complexity analysis is reported.\n- The effectiveness heavily depends on the quality of latent representations; the paper does not explore failure cases or encoder ablation.\n- While numerical gains are large, the work lacks intuitive analysis or visualization of what pre-images were pruned and why.\n- Heavy formalism and overloaded notation could be streamlined for clarity."}, "questions": {"value": "- How scalable is CLIPPER to large NESY datasets when solving the ILP on full mini-batches? Is there an approximate or relaxed formulation (e.g., LP relaxation or greedy pruning)?\n- How sensitive is performance to encoder choice (e.g., random vs. pretrained vs. jointly trained)?\n- Does pruning risk discarding gold pre-images under imperfect proximity graphs? Are there empirical checks or safety mechanisms?\n- Could the method generalize to multi-modal encoders or symbolic rules with uncertainty (e.g., probabilistic logic)?\n- How does CLIPPER interact with end-to-end differentiable reasoning frameworks like DeepProbLog or Neural Module Networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DG31iflbfI", "forum": "Wt4kD66lp9", "replyto": "Wt4kD66lp9", "signatures": ["ICLR.cc/2026/Conference/Submission20861/Reviewer_Mwcy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20861/Reviewer_Mwcy"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission20861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762611762530, "cdate": 1762611762530, "tmdate": 1762936928297, "mdate": 1762936928297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}