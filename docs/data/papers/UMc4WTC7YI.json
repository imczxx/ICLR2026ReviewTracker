{"id": "UMc4WTC7YI", "number": 19008, "cdate": 1758292708248, "mdate": 1762934164004, "content": {"title": "GQA: Generation Quality Assessment of AIGC Videos based on Human Assessment: Dataset, Scoring and Explanation", "abstract": "Recent advances have significantly elevated the quality of AI-generated videos; however, existing evaluation metrics still struggle to align closely with human perceptual judgments. While prior work has repurposed deep learning models or borrowed algorithms from other domains to assess generative content, their outputs often exhibit noticeable discrepancies with real human evaluations. To address this critical gap, we introduce the GQA dataset — a human-aligned benchmark comprising: (1) videos generated by dozens of state-of-the-art models, including those from the VAE and Diffusion Model (DM) families; (2) dozens of refined evaluation metrics systematically categorized into three core dimensions — Video-Text Consistency, Realism, and Traditional Quality; and (3) a prompt-adaptive metric selection mechanism that ensures evaluations are contextually relevant, avoiding misaligned assessments across semantically unrelated dimensions. GQA enables more accurate, interpretable, and perception-aware evaluation of AI-generated video content.", "tldr": "", "keywords": ["AIGC", "Video Generation Quality Assessment", "Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e2816c1b7562a137263bcff0cc71da9564c7393a.pdf", "supplementary_material": "/attachment/b6a69eb8b6916f21243eddb9d138195c299c0b47.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a human-aligned dataset for AI-generated videos, GQA, which comprises 32 evaluation metrics with 9,474 videos from 945 prompts, together with 39,724 merged human annotations. An annotation-efficient evaluation protocol combining categorical and comparative labeling is proposed to enhance annotation efficiency and accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The GQA encompasses 35 generative models and 32 evaluation metrics, which is comprehensive."}, "weaknesses": {"value": "a). The writing and organization of this paper is poor, making it hard to read:\n\n    1. The design of Figure 1 is indeed somewhat confusing. After all, it is an overall overview of this project. It is difficult to understand the “Construction Methodology” (Right) through such simple presentation. What kind of comparison does the symbol in the picture represent? Why use this particular 3, 2, 1, -1 grading system?\n    2. The introduction section devoted a significant amount of space to reviewing existing work, but failed to provide a detailed explanation of the motivation behind this study and a clear description of the work itself.\n    3. Table 1 should be moved to appendix. Is column “number” means index?\n    4. I suggest that in Section 3.2, the introduction of generative models be presented in chronological order for better readability.\n    5. The information density in Figure 2 is too high. It is recommended to describe each aspect separately according to the three types of evaluation proposed.\n    6. Why adopt labels such as -1, 1, 2, 3? Is there any theoretical basis for this? Why not use 0, 1, 2, 3 or other commonly used grading systems?\n    7. What do the x- and y- axes in Figure 4 represent?\n\nb). The contribution is limited, lacking original technical contributions:\n\n    1. Section 2, although titled \"Rethinking\", merely provides a descriptive review of the vbench series, lacking in-depth perspectives. The proposed human-centered evaluation has already been presented in existing work. The AI-generated video evaluation based on subjective assessment is a mainstream approach.\n    2. The data volumn in Fig. 3 exhibits significant long-tailed distribution, which may introduce bias into the evaluation results.\n    3. The technical contribution of the proposed baseline is poor.\n\nc). Lack of experimental analysis:\n\n    1. Lack details of human annotations. See Details Of Ethics Concerns.\n    2. Given that this paper has drawn upon the design of vbench, it is necessary to conduct a comparison of the results with that of vbench.\n    3. Lack of comparison with other methods and baseline"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This work includes a human annotation process. Although implemented by a certified annotation company, some details are missing.\nProviding details about instructions, cases, annotation UI, and the results of quality examination may facilitate readers' understanding and ensure the rationality of the process."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VuUnU1DROF", "forum": "UMc4WTC7YI", "replyto": "UMc4WTC7YI", "signatures": ["ICLR.cc/2026/Conference/Submission19008/Reviewer_niTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19008/Reviewer_niTX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760863030049, "cdate": 1760863030049, "tmdate": 1762931057539, "mdate": 1762931057539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "eAyMChTEQ3", "forum": "UMc4WTC7YI", "replyto": "UMc4WTC7YI", "signatures": ["ICLR.cc/2026/Conference/Submission19008/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19008/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762934160989, "cdate": 1762934160989, "tmdate": 1762934160989, "mdate": 1762934160989, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GQA (Generation Quality Assessment), a human-aligned benchmark for evaluating AI-generated videos, focusing on three key dimensions: video-text consistency, realism, and traditional quality. It presents a comprehensive dataset of 9,474 videos from 35 generative models, evaluated across 3 main dimensions.  The paper also presents a baseline model that demonstrates the feasibility of using the GQA dataset for training quality assessment models. Despite its strengths in human-centered evaluation and broad model coverage, the paper highlights challenges such as data imbalance and the need for improvements in long-term coherence assessment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Human-Centered Evaluation: The introduction of the GQA dataset is a significant strength, as it aligns AI-generated video assessments with human perceptual judgment.\n2. Wide Model Coverage: The dataset includes evaluations from 35 generative models, representing a broad spectrum of advancements in the video generation field.\n3. Baseline Model Development: A baseline model using VideoMAE and CLIP is presented to show the feasibility of using GQA for supervised learning tasks, demonstrating the utility of the dataset for further research and model improvement.\n4.  Comprehensive Evaluation Dimensions: The paper systematically categorizes 32 evaluation dimensions into three core dimensions: Video-Text Consistency, Realism, and Traditional Quality."}, "weaknesses": {"value": "1. Concerns on Annotation Quality: The paper does not explicitly validate the quality of the data annotation using metrics like inner annotator agreement or SRCC (Spearman Rank Correlation Coefficient) between different annotators. While the authors describe a quality control mechanism involving a review panel and random audits of annotations, there is no direct mention of calculating the inter-rater reliability or measuring SRCC to quantify the consistency between annotators.\n2. Imbalance in Data Distribution: There is a significant imbalance in the distribution of annotation samples across different dimensions, which could affect model performance and convergence, particularly for rare evaluation criteria like rigid body collisions.\n3. Lack of Cross-Dataset Validation: The paper does not include cross-dataset experiments to verify the generalizability of the dataset and model. The results primarily focus on the performance within the GQA dataset, without evaluating how well the models trained on GQA perform on other datasets. \n4. Lack of  Ablation Studies: There is a lack of ablation studies that would provide insight into the contribution of different components of the model or evaluation framework. \n5. Unverified Model Effectiveness: The effectiveness of the proposed model has not been thoroughly validated. While the paper presents a baseline model, it does not provide detailed performance validation or comparisons with other state-of-the-art methods across different evaluation tasks.\n6. Insufficient Information in Tables and Figures: Table 2 and Figure 6 include valuable information, but their content would be better suited for supplementary materials rather than the main body of the paper."}, "questions": {"value": "1. Can you provide details on whether inter-rater reliability (such as SRCC) was calculated to assess the consistency of annotations across different annotators?\n2. Why there is a significant imbalance in annotation samples across different dimensions?\n3. The results in the paper are primarily based on the GQA dataset. Have you conducted any cross-dataset validation to evaluate the generalizability of the models trained on GQA?\n4. Could you provide a more detailed comparison of the performance of the proposed model with existing methods, particularly in terms of SRCC/PLCC?\n5. What are the contribution of different components of the model and the impact of different features (e.g., video-text consistency, realism, etc.) on the overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kZs5gMaPu7", "forum": "UMc4WTC7YI", "replyto": "UMc4WTC7YI", "signatures": ["ICLR.cc/2026/Conference/Submission19008/Reviewer_dzVd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19008/Reviewer_dzVd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039551544, "cdate": 1761039551544, "tmdate": 1762931057081, "mdate": 1762931057081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GQA, a new benchmark dataset for the quality assessment of AI-generated videos. The authors define a hierarchical set of 32 evaluation metrics across three main categories: Video-Text Consistency, Realism, and Traditional Quality. The dataset annotation follows a two-stage process: an initial coarse categorization of videos followed by fine-grained pairwise comparisons to derive ranked scores. Additionally, the paper presents a proof-of-concept baseline model, which uses a pre-trained VideoMAE for video features and CLIP for text features to predict quality scores for each of the 32 dimensions. The main contribution of this work is the dataset itself, intended to facilitate more structured and human-aligned evaluation of generative video models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Important Problem: The paper addresses a critical and timely problem: the need for better, more fine-grained, and human-aligned evaluation of AI-generated videos. The limitations of existing metrics are well-recognized, and the goal of creating a multi-dimensional benchmark is laudable.\n\nComprehensive Taxonomy: The authors have put considerable effort into defining a detailed taxonomy of 32 evaluation dimensions. This hierarchical structure (Consistency, Realism, Quality) is a reasonable way to conceptualize the different facets of video quality."}, "weaknesses": {"value": "Fundamentally Flawed Dataset Construction: Tying evaluation dimensions to specific prompt keywords is a critical design flaw. A video's \"Fluid Motion Realism\" should be evaluable regardless of whether the prompt contained the word \"water\" or \"river\". This approach tests a model's ability to handle specific prompts rather than its general capability to produce realistic videos, making the resulting dataset of limited use for general evaluation.\n\nLack of Novelty: The paper does not introduce any novel methods. The annotation scheme is a variant of standard pairwise comparison. The baseline model is a simplistic combination of existing, off-the-shelf components. The core ideas are either borrowed from prior work (e.g., multi-dimensional evaluation from VBench) or are not sufficiently developed (e.g., the \"prompt-adaptive metric selection\").\n\nInsufficient Detail and Unsubstantiated Claims: The paper makes claims in the abstract that are not supported in the main text. For instance, the \"prompt-adaptive metric selection mechanism\" is a core advertised feature but is never explained, designed, or tested. The process of converting pairwise comparisons into final scores is also not detailed, which is a critical piece of information for a dataset paper.\n\nPoor Baseline and Lack of Comparison: The provided baseline is very weak and is not compared against any other state-of-the-art video evaluation models (e.g., VideoScore, VideoReward, etc.). Without such comparisons, it is impossible to situate the difficulty of the GQA dataset or to understand if the baseline's poor performance is due to the dataset's challenges or the model's simplicity. The paper essentially exists in a vacuum, without proper context within the current literature."}, "questions": {"value": "Could you please clarify the rationale behind tying the data collection for specific evaluation dimensions to the content of the text prompts? This seems to introduce a strong bias. For example, how would you evaluate \"Fluid Motion Realism\" on a video that realistically portrays a waterfall, but was generated from a prompt that did not explicitly mention water?\n\nThe abstract mentions a \"prompt-adaptive metric selection mechanism,\" which sounds like a key component of your framework. However, this mechanism is not described anywhere in the methodology or experiments. Could you please explain what this is and where it is detailed in the paper? If it is not part of the current work, it should be removed from the abstract.\n\nCould you provide a detailed explanation of the algorithm used to convert the pairwise comparison results (A is better than B, B is equivalent to C, etc.) into the final numerical scores? What ranking algorithm (e.g., Bradley-Terry, Elo) was used, and how was the mapping to a continuous or discrete score range performed?\n\nWhy did you choose not to compare your baseline model with any existing video quality assessment models on your new dataset? Such a comparison would be crucial to benchmark the difficulty of GQA and to understand its relationship with existing evaluation paradigms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kfQ1Ss7fAN", "forum": "UMc4WTC7YI", "replyto": "UMc4WTC7YI", "signatures": ["ICLR.cc/2026/Conference/Submission19008/Reviewer_McvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19008/Reviewer_McvQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554131783, "cdate": 1761554131783, "tmdate": 1762931056675, "mdate": 1762931056675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GQA, a new dataset and evaluation framework for assessing the quality of AI-generated videos. The authors argue that existing automated metrics often fail to align with human perceptual judgment. To address this, they propose a human-centric approach. The core contrubution is GQA dataset, featuring videos from various generative models and are evaluated by human annotators across 32 distinct quality dimensions. In addition, a two-stage annotation process that combines coarse-grained classification with fine-grained pairwise comparisons is proposed to improve both the efficiency and accuracy of human labeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Adopting wide evaluation dimensions categorized into Video-Text Consistency, Realism & Plausibility, and Traditional Quality.\n2. Propose an innovative and efficient two-stage annotation methodology using coarse-grained classification and fine-grained pairwise comparisons.\n3. The newly-created datasets include diverse videos from 35 generative models.\n4. The paper is well-written, logically structured, and easy to follow."}, "weaknesses": {"value": "1. Imbalance data: the data ranges from over 7,000 samples for some metrics to as few as 100 for others. It may be very difficult to train reliable, generalizable automated models.\n2. It does not report any quantitative metrics for inter-annotator agreement.\n3. The videos and prompts are from the VBench dataset, limiting the diversity of content and prompts in GQA.\n4. The baseline models are trained separately for each of the 32 dimensions, which are computationally inefficient."}, "questions": {"value": "1. How to solve or mitigate the imbalance data? \n2. Does it will be help when expanding the prompt diversity?\n3. Why not design a unified model to learn to assess video quality holistically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3xOLq9UUvH", "forum": "UMc4WTC7YI", "replyto": "UMc4WTC7YI", "signatures": ["ICLR.cc/2026/Conference/Submission19008/Reviewer_FH31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19008/Reviewer_FH31"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19008/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880434199, "cdate": 1761880434199, "tmdate": 1762931056194, "mdate": 1762931056194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}