{"id": "QCRWjbp4p0", "number": 12199, "cdate": 1758206281157, "mdate": 1759897525726, "content": {"title": "FlowNar: Scalable Streaming Narration for Long-Form Videos", "abstract": "Recent Large Multimodal Models (LMMs), primarily designed for offline settings, are ill-suited for the dynamic requirements of streaming video. While recent online adaptations improve real-time processing, they still face critical scalability challenges, with resource demands typically growing at least linearly with video duration. To overcome this bottleneck, we propose FlowNar, a novel framework for scalable streaming video narration. The core of FlowNar is a dynamic context management strategy for historical visual context removal, combined with our novel CLAM (Cross Linear Attentive Memory) module for streaming visual history retention, ensuring bounded visual memory usage and computational complexity, crucial for efficient streaming. We also introduce a realistic autoregressive evaluation protocol and complementary evaluation metrics to assess streaming narration models under deployment-like conditions. Experiments on Ego4D, EgoExo4D, and EpicKitchens100 datasets demonstrate that FlowNar substantially improves narration quality over strong baselines while being highly efficient, supporting processing of 10$\\times$ longer videos and achieving 3$\\times$ higher throughput (FPS).", "tldr": "", "keywords": ["streaming video narration", "vision language models", "long-form video understanding", "cross linear attentive memory"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33715673654d77ce5c71c0b023b102b687f63817.pdf", "supplementary_material": "/attachment/ede413cc477a2e7ecfede22474788c8ae2def60c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents FlowNar, a framework designed to generate narrations for streaming videos. The key challenge addressed is that existing methods for video narration require memory and computation that grows linearly (or worse) with video length, making them impractical for long videos. FlowNar introduces two main contributions: (1) DCM, which removes detailed visual information from past video segments after generating each narration, and (2) CLAM, a novel module that compresses historical visual information into a fixed-size memory bank. The authors also propose evaluation protocol where the model generates narrations based on its own previous outputs rather than ground truth. Experiments on three egocentric video datasets (Ego4D, EgoExo4D, and EpicKitchens100) demonstrate that FlowNar can process videos 10× longer than baseline methods while achieving 3× higher throughput and improved narration quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The CLAM module is an interesting adaptation of linear attention mechanisms for streaming video. The design maintains constant memory and per-step computation, also the combination of DCM and CLAM provides a complete solution to the scalability challenge.\n- The paper introduces a realistic autoregressive evaluation protocol that better reflects deployment conditions. The \"first-align-then-evaluate\" procedure is a thoughtful solution to the challenge of evaluating predictions with different temporal boundaries than ground truth.\n- The paper includes extensive ablations that validate each component's contribution. The analyses of visual history strategies, memory designs, and trigger mechanisms provide good insights into why the approach works."}, "weaknesses": {"value": "- The dynamic threshold mechanism requires setting several hyperparameters ($\\theta$, $\\theta_\\mathrm{low}$, refractory period). Table 11 in the appendix shows that performance varies with threshold values. The paper does not provide clear guidance on how to set these parameters for new domains or video types, which could limit the method's generalizability.\n- All three datasets focus on egocentric videos, primarily of kitchen and daily activities. The evaluation does not include other important video types such as sports, movies, educational content, or surveillance footage. This raises questions about how well the approach generalizes to different video genres.\n- I'm not convinced about $\\theta_\\mathrm{low}$. Consider a scenario with dense information content (e.g., a streaming educational lecture with continuous new information). After each narration, the method applies $\\theta_\\mathrm{low}$, which discourages triggering. Could this cause the model to miss important content that should trigger the next narration? How does the method handle videos with consistently high information density?"}, "questions": {"value": "- Does the \"first-align-then-evaluate\" scheme specifically favor FlowNar over the baselines? Could this evaluation approach be inherently biased toward methods that generate narrations at specific temporal patterns?\n- The CLAM module uses a fixed M×D memory regardless of video length. Consider two scenarios: 3 previous frames vs 30 previous frames. Does the 30-frame scenario suffer from more information loss since memory size is fixed? Or is this similar to language models where the last token contains information from all previous layers, so there's no information loss?\n- For the fluency and LM-correctness metrics, both FlowNar and baselines use the same fixed LLM (Llama-3-8B or 1B). Since these metrics depend on the LM used and all methods use the same one, does comparing these metrics provide meaningful differentiation? Could you clarify what these metrics are actually measuring?\n- In Table 3, using no past frames outperforms using all past frames by a large margin (30.40 vs 28.04 CIDEr), which is a bit surprising. Can the authors explain why having no historical visual information would be way better than having complete history?\n- In lines 428-429, the authors write that \"providing past visual context generally improves narration quality for the oracle protocol (cf. row 1 vs. row 2/3).\" However, looking at Table 4, I don't see a clear pattern supporting this conclusion. The differences in PPL and TimeDiff are very small (2.122 vs 2.115 vs 2.118). Could the authors clarify this claim?\n- The paper compares against Videollm-online, Videollm-mod, and LION-FS. Several other recent streaming video methods are mentioned (ProVideLLM, Dispider, LiveCC) but not experimentally compared. Could the authors clarify why these methods were not included in the comparison? Is it due to code availability, or fundamental differences in task formulation that make direct comparison difficult?\n- The paper acknowledges that training time is increased due to longer sequences and less optimized attention kernels, but does not provide concrete numbers or comparison. Providing more information and experimental statistics will be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kbJWy7g8EN", "forum": "QCRWjbp4p0", "replyto": "QCRWjbp4p0", "signatures": ["ICLR.cc/2026/Conference/Submission12199/Reviewer_jfH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12199/Reviewer_jfH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200855247, "cdate": 1761200855247, "tmdate": 1762923147995, "mdate": 1762923147995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FLOWNAR, a novel framework for scalable streaming video narration using large multimodal models (LMMs). It addresses the core challenge that existing online video narration methods (e.g., Videollm-online) scale linearly in computational and memory cost with video length, limiting their use for long-form streaming scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FLOWNAR combines dynamic pruning with learned visual compression, achieving both constant memory and stable narration quality.\n2. Extensive tests on three long-form datasets, with both oracle and autoregressive protocols.\n3. Comprehensive analyses of memory design, triggering strategy, and DCM effects.\n4. The writing is concise and technically precise; mathematical notation is consistent and readable."}, "weaknesses": {"value": "1. While CLAM’s design is intuitive, there is limited theoretical analysis of its representational capacity or stability compared to standard linear attention mechanisms.\n2. Although major baselines (Videollm-online, Videollm-mod) are included, additional recent streaming methods (e.g., Dispider, ProVideLLM) are only cited but not empirically compared.\n3. While qualitative examples are shown (Fig. 5), more failure analysis or long-horizon error accumulation visualization would strengthen interpretability."}, "questions": {"value": "1. How would CLAM compare to recurrent state-space models (e.g., Mamba or RetNet) for streaming visual summarization?\n2. Have the authors evaluated FLOWNAR on non-egocentric datasets (e.g., HowTo100M or ActivityNet) to test domain transferability?\n3. Does FLOWNAR exhibit “drift” in narration over multi-hour streams, and how does DCM frequency impact long-term consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B80w8ysDE6", "forum": "QCRWjbp4p0", "replyto": "QCRWjbp4p0", "signatures": ["ICLR.cc/2026/Conference/Submission12199/Reviewer_VDde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12199/Reviewer_VDde"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796113496, "cdate": 1761796113496, "tmdate": 1762923147160, "mdate": 1762923147160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FLOWNAR, a comprehensive system designed for streaming long-form video narration. Its core components include Dynamic Context Management (DCM) and a newly proposed memory module, Cross Linear Attentive Memory (CLAM). Furthermore, the authors introduce a more realistic autoregressive evaluation protocol to better simulate real-world deployment scenarios. Experiments conducted on three major benchmarks — Ego4D, EgoExo4D, and EpicKitchens-100 — demonstrate significant improvements over existing streaming methods under the autoregressive setting, while also achieving substantial efficiency gains in memory usage and throughput (supporting up to 10× longer videos and approximately 3× higher FPS). These findings are well supported by quantitative results and extensive ablation studies presented in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a more rigorous and realistic autoregressive evaluation protocol, establishing a stricter yet more meaningful benchmark for this research domain. \n\n2. In this paper, the authors address the issue of unbounded visual KV cache accumulation in existing streaming models by introducing *Dynamic Context Management (DCM)*, which enables the practical processing of arbitrarily long videos from an engineering standpoint. \n\n3.The experiments in the paper demonstrate a remarkable improvement in efficiency, highlighting the strong practical value of the proposed approach. Moreover, the ablation studies are thorough and well-designed, clearly validating the necessity and superiority of each module, which lends strong credibility to the paper’s conclusions. \n\n4. The work addresses a critical scalability bottleneck in streaming video models and demonstrates a feasible pathway toward real-time, long-duration video narration."}, "weaknesses": {"value": "1.The experiments are conducted exclusively on Ego4D, EgoExo4D, EpicKitchens-100. While these benchmarks are standard for video narration, they represent a narrow domain. It remains unclear whether the proposed system generalizes effectively to more diverse visual conditions, such as outdoor scenes, dynamic camera motion, or different frame rates. The paper would benefit from broader evaluation to substantiate its claim of scalability to arbitrary long-form videos. \n\n2.The paper briefly mentions increased training time but omits quantitative details about GPU hours, memory usage, or batch configurations. Without such data, it is difficult to assess the method’s scalability and resource efficiency in real-world scenarios. Full reproducibility would require more transparent reporting of hyperparameters, implementation details, and training cost metrics. \n\n3. The paper lacks an in-depth analysis of the relationship between video length and the performance of CLAM. Since CLAM compresses variable-length visual histories into a fixed-size representation, it remains unclear whether this process leads to significant information loss. The discussion of CLAM’s potential failure cases is insufficient, and the newly introduced triggering mechanism lacks a detailed sensitivity analysis of its parameters."}, "questions": {"value": "1. The experiments focus exclusively on Ego4D, EgoExo4D, and EpicKitchens-100, which primarily involve egocentric indoor scenes. Could the authors provide additional evidence or discussion regarding the generalization of FLOWNAR to more diverse video domains—such as outdoor environments, dynamic camera movements, or variable frame rates? If conducting new experiments is infeasible, a qualitative analysis or case study on different video distributions would help clarify the scalability claim. \n\n2. The paper briefly mentions increased training time but does not provide quantitative details. Could the authors include a summary table reporting the GPU type, total training hours, peak memory usage, batch size, and learning rate configuration? Such information is crucial for assessing the practical scalability and reproducibility of the proposed system. Additionally, have the authors considered releasing training logs or scripts to ensure transparency and facilitate community validation? \n\n3. Since CLAM compresses variable-length visual histories into a fixed-size representation, how does its performance scale with increasing video length? Is there a point where compression leads to noticeable degradation in temporal or semantic fidelity? It would also be valuable to understand any observed failure cases where CLAM struggles to maintain long-range consistency. Furthermore, could the authors provide a sensitivity analysis for the parameters of the new triggering mechanism (e.g., thresholds or update intervals) to better illustrate its stability and robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JEQ68Yrkp2", "forum": "QCRWjbp4p0", "replyto": "QCRWjbp4p0", "signatures": ["ICLR.cc/2026/Conference/Submission12199/Reviewer_YeT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12199/Reviewer_YeT5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968275835, "cdate": 1761968275835, "tmdate": 1762923146584, "mdate": 1762923146584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a vision-language framework to support real-time scalable video narration. It addresses the main limitation of prior streaming based vision-language model, e.g. videollm-online whose memory and computation costs grow linearly with the video length. In this paper, the authors proposed two main architecture changes to address this: \n* It introduced a cross linear attentive memory mechanism that reformulates the linear attention into a sequential compressor. It can uses fix number of tokens that attend to new observations and gets updated with time on the fly. \n* It uses dynamic context management strategy that prunes the past visual and text caches after each narration segment. This prevents memory built up and make the system can handle long-context windows. \n\nThe paper performs extensive ablation studies to show the effectiveness of the two proposed modules on Ego4D, EgoExo4D and EpicKitchen datasets. It can achieve 10x longer videos and 3x higher throughput than videollm baselines. In addition, the authors propose an autoregressive protocol that evaluate the full narration autoregressively on long context. In this evaluation, the method outperform previous methods reasonable well (except in the MAC metric)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall the method is well motivated and technically sound.\n\nThe proposed Cross Linear Attentive Memory and Dynamic Context Management can compound to each other and summarize the context on the fly with a fixed size latent space. The evaluation also demonstrates its ability to achieve better frame alignment and narration quality.\n\nOn the evaluation of narration, the method uses a combination of autoregressive long context generation and oracle evaluation. I like the autoregressive evaluation, which is a good indication of narration quality at inference time."}, "weaknesses": {"value": "One main question (or weakness I hypothesis) as the limitation for the proposed mechanism is that the proposed summarization and pruning strategy will lead to loss of historical information, which may prevent it being used in broader tasks other than narration. The task of video narration can benefit from better historical information but may not necessarily need them in fine-grain details. Currently there is very little information about this.\n\nMy other hypothesis is that the narration may benefit from faster FPS (which is actually a strength) sampling at runtime, in particular for temporal alignment. I did not see corresponding experiments demonstrating how higher FPS will affect the result in some way."}, "questions": {"value": "The two parts I raised in weakness are primarily the two areas I'd like to get clarity from the authors. \n\nI am not sure whether the authors will intend to release the model for future reference. It will be good to get a confirmation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KZcXWHjhGh", "forum": "QCRWjbp4p0", "replyto": "QCRWjbp4p0", "signatures": ["ICLR.cc/2026/Conference/Submission12199/Reviewer_vKgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12199/Reviewer_vKgD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968302234, "cdate": 1761968302234, "tmdate": 1762923146122, "mdate": 1762923146122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}