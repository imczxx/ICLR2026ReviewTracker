{"id": "LDRcGTAF4s", "number": 18860, "cdate": 1758291569052, "mdate": 1759897077110, "content": {"title": "Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions and Online Opponent Modeling", "abstract": "While multi-agent reinforcement learning (MARL) has been proven effective across both collaborative and competitive tasks, existing algorithms often struggle to scale to large populations of agents. Recent advancements in mean-field (MF) theory provide scalable solutions by approximating population interactions as a continuum, yet most existing frameworks focus exclusively on either fully cooperative or purely competitive settings. To bridge this gap, we introduce MF-MAPPO, a mean-field extension of PPO designed for zero-sum team games that integrate intra-team cooperation with inter-team competition. MF-MAPPO employs a shared actor and a minimally informed critic per team and is trained directly on finite-population simulators, thereby enabling deployment to realistic scenarios with thousands of agents. We further show that MF-MAPPO naturally extends to partially observable settings through a simple gradient-regularized training scheme. Our evaluation utilizes large-scale benchmark scenarios using our own testing simulation platform for MF team games ($\\texttt{MFEnv}$), including offense–defense battlefield tasks as well as variants of population-based rock-paper-scissors games that admit analytical solutions, for benchmarking. Across these benchmarks, MF-MAPPO outperforms existing methods and exhibits complex, heterogeneous behaviors, demonstrating the effectiveness of combining mean-field theory and MARL techniques at scale.", "tldr": "A novel, scalable MARL algorithm leveraging finite-population mean-field approximation for large-population competitive team games with online mean-field estimation for opponent modelling", "keywords": ["Multi-agent reinforcement learning", "game theory", "large-population systems"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85f0d87a9cb898a4a4e727c964d05de8bb65eda2.pdf", "supplementary_material": "/attachment/b15e246de23b7666b16658bf357e5a08d9490f69.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes MF-MAPPO, a mean-field extension of PPO tailored to zero-sum mean-field team games that mix intra-team cooperation with inter-team competition, and trains it directly in finite-population simulators. The core ideas are a shared actor with a minimally informed, mean-field-only critic per team (so value estimation depends only on the two population distributions), theoretical guarantees linking finite-population training to the infinite-population limit (policy-gradient consistency and ε-optimality that improves with team size), and a gradient-regularized variant that stabilizes policies under partial observability. The authors also introduce D-PC, a decentralized opponent mean-field estimator with convergence and regret bounds, and release MFEnv, benchmark environments (constrained RPS and a grid-based battlefield) where MF-MAPPO consistently outperforms DDPG-MFTG and exhibits heterogeneous, coordinated behaviors at scale. The most significant novelty is the combination of (i) a PPO-style shared actor/minimal-critic architecture specialized to competitive mean-field team settings with (ii) theory that justifies training on finite populations and (iii) a provably correct opponent MF estimator that works under limited communication."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies a reasonable and timely setting: large-population competitive team games, approached via finite mean-field approximations.\n\nThe proposed algorithm feels natural given the theory (e.g., Theorems 2 and 3) and is a tidy fit to the mean-field structure.\n\nThe method combines an actor–critic algorithm tailored for mean-field games with a decentralized mean-field estimation framework (D-PC), which is an interesting and coherent design."}, "weaknesses": {"value": "The paper would benefit from a stronger high-level overview that clearly explains the problem setup, why it matters, and how the pieces (finite-population training, minimally informed critic, D-PC) fit together; this would better convey the value of the work.\n\nExperiments are confined to numerical/custom environments, and the set of baselines is limited; broader comparisons and ablations (including against adapted mainstream MARL baselines) are needed to support generality and impact claims.\n\nThe introduction of the partially observable setting needs clearer motivation and justification. While it is intended to model the lack of access to opponents’ mean-field statistics, the paper should provide a more explicit comparison to the fully observable case and a discussion of performance with and without accurate opponent mean-field estimation (e.g., sensitivity to estimation error and communication constraints)."}, "questions": {"value": "Could you provide a clear description of the problem setup and how the three components of finite-population training, minimally informed critic, and D-PC interlock?\n\nWhat specific failure modes in standard MARL (e.g., leakage, non-stationarity, variance in value estimates) does each component address, and how did these considerations drive your architectural choices?\n\nBeyond DDPG-MFTG, how do you perform against adapted mainstream MARL baselines (e.g., MAPPO, QMIX/VDN variants, IPPO, PPO-centralized-critic, FACMAC) when extended to your mean-field team setting? Please include learning curves, sample efficiency, and final returns.\n\nHow does performance degrade as the opponent mean-field estimate is corrupted by controlled noise, delay, or bandwidth limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N2XhXxUDqv", "forum": "LDRcGTAF4s", "replyto": "LDRcGTAF4s", "signatures": ["ICLR.cc/2026/Conference/Submission18860/Reviewer_B2tA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18860/Reviewer_B2tA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954789778, "cdate": 1761954789778, "tmdate": 1762930827547, "mdate": 1762930827547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF-MAPPO, a mean-field extension of PPO tailored for large-scale competitive team games (ZS-MFTGs). The authors propose a shared actor-critic architecture with minimally informed critics and demonstrate scalability and robustness through finite-population training. They also present a gradient-regularized variant for partially observable settings, supported by a decentralized estimation algorithm (D-PC). The work is evaluated on custom benchmark environments designed to reflect collaborative-competitive dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well-written and clearly structured, making complex ideas accessible.\n* The theoretical framework is sound, with appropriate use of mean-field approximations and convergence guarantees.\n* The integration of PPO with mean-field theory is executed cleanly, and the proposed architecture is computationally efficient."}, "weaknesses": {"value": "* The paper lacks significantly novel theoretical contributions. Most theorems are either direct consequences of existing results or minor extensions.\n* The proposed MF-MAPPO algorithm is a relatively straightforward adaptation of PPO to a mean-field setting, and the use of shared actor-critic networks is not conceptually new.\n* The benchmark environments, while tailored, do not convincingly demonstrate capabilities beyond prior work in terms of emergent behavior or strategic complexity.\n* The D-PC estimation method is a modest variation on existing consensus algorithms and does not introduce fundamentally new ideas."}, "questions": {"value": "I appreciate the idea of integrating the idea of PPO into Mean Field RL. However, it appears that the algorithm presented in the paper merely applies a PPO - style (gradient - clipping) policy update mechanism to Mean Field Games (MFG), without delving into the underlying mechanism. In other words, similar to the theorem discovered by Trust Region Policy Optimization (TRPO) in the classical Reinforcement Learning (RL) domain, is it possible to identify the fundamental principle that reveals Mean Field Games (MFG) also possess a \"basis\" for justifying the application of a PPO - style algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i965Ce8MGb", "forum": "LDRcGTAF4s", "replyto": "LDRcGTAF4s", "signatures": ["ICLR.cc/2026/Conference/Submission18860/Reviewer_vvaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18860/Reviewer_vvaB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136684085, "cdate": 1762136684085, "tmdate": 1762930826948, "mdate": 1762930826948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF-MAPPO, a mean-field extension of PPO, to address the scalability issues of multi-agent reinforcement learning in large-scale mixed cooperative-competitive tasks. The main contributions include: 1. Proposing the base version of MF-MAPPO, a scalable shared actor-critic algorithm for large-scale zero-sum MFTGs. 2. Extending the MF-MAPPO by gradient-regularized training and a decentralized mean-field estimation framework D-PC for partially observable MFTGs. 3. Constructing novel MFTG benchmarking environments named MFEnv for scalability. 4. Demonstrating performance and efficiency of method above through numerical experiments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper aims to addresse mixed collaborative-competitive mean-field game, rather than purely non-cooperative or fully cooperative settings, with broad applicability to real-world domains.\nS2. It innovatively proposes MF-MAPPO, a PPO-based algorithm with a minimally-informed critic and shared-team actor, enabling effective scaling to large populations of agents, while guaranteeing convergence.\nS3. It extends the algorithm to partially observable settings through a gradient-regularized training, and is coupled with the decentralized mean-field estimation framework D-PC, further broadening the application scope.\nS4. The author provides a large number of rigorous theoretical analyses, and conducts numerical experiments to enhance persuasiveness.\nS5. The paper is well-structured, with clear organization and rigorous logic."}, "weaknesses": {"value": "W1. The setup of experiments is simple, only contains a cRPS game and a grid-based battlefield game, it maybe need to be further extended to other benchmark to better demonstrate generality and robustness.\nW2. The paper does not analyze the cost of computation and decision-making overhead in such large-scale agent scenarios.\nW3. As mentioned in the paper's conclusion section, the method struggles to extend to large state or action dimensionality, which to some extent limits its application in realistic scenarios."}, "questions": {"value": "Q1. The MFenv benchmark only contains two tasks and is developed by yourselves, how does the method perform in other benchmarks?\nQ2. How about the cost when the number of agents is extremely large?\nI think the paper and the method described in it are good enough, and it could be better if the questions above are solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pB0Ib1KXBe", "forum": "LDRcGTAF4s", "replyto": "LDRcGTAF4s", "signatures": ["ICLR.cc/2026/Conference/Submission18860/Reviewer_MyrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18860/Reviewer_MyrG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327915785, "cdate": 1762327915785, "tmdate": 1762930826141, "mdate": 1762930826141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a scalable multi-agent reinforcement learning algorithm designed for large mixed collaborative–competitive team games. MF-MAPPO employs shared team actors and shared critics that depend only on team-level mean-field distributions, enabling efficient training without full agent-level observability. The framework provides theoretical guarantees of near-optimality, showing that identical team policies learned in finite populations approximate the infinite-population limit. To address partial observability, the authors propose a Dynamic Projected Consensus estimator that allows agents to infer opponent distributions with limited communication. Experiments trained directly on finite-population simulators demonstrate the algorithm’s effectiveness in rock-paper-scissors and battlefield environments."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- It demonstrates scalability in large population environments. The shared actor–critic framework that depends only on mean-field (team-level) distributions allows the algorithm to scale to thousands of agents.\n- Its finite-population guarantees ensure that policies trained on small populations generalize effectively to larger teams without additional retraining.\n- The authors do include a degradation comparison between fully observable and partially observable settings, showing only minor performance loss under limited communication (battlefield - Fig 4)."}, "weaknesses": {"value": "- As the authors note, despite the progress, the algorithm scalability is still limited as the number of state variables increases, and learning accurate mean-field distributions becomes computationally expensive, motivating the need for dimensionality reduction techniques.\n- The empirical evaluation is limited to tasks that are discrete, low-dimensional, and noise-free, which is a common limitation. However, for MF-MAPPO this restriction is particularly significant because both the mean-field formulation and the Dynamic Projected Consensus estimator rely on assumptions that do not easily extend to continuous or noisy environments."}, "questions": {"value": "- Despite it might be a bit out of scope, how sensitive is MF-MAPPO to deviations from the mean-field assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dz0Mfkkf9q", "forum": "LDRcGTAF4s", "replyto": "LDRcGTAF4s", "signatures": ["ICLR.cc/2026/Conference/Submission18860/Reviewer_Hdbj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18860/Reviewer_Hdbj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762499433269, "cdate": 1762499433269, "tmdate": 1762930825652, "mdate": 1762930825652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}