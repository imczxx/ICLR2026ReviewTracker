{"id": "ZVlTIyRe35", "number": 1692, "cdate": 1756906563798, "mdate": 1759898194497, "content": {"title": "Neural Graduated Assignment for Maximum Common Edge Subgraphs", "abstract": "The Maximum Common Edge Subgraph (MCES) problem is a crucial challenge with significant implications in domains such as biology and chemistry. Traditional approaches, which include transformations into max-clique and search-based algorithms, suffer from scalability issues when dealing with larger instances. This paper introduces ``Neural Graduated Assignment'' (NGA), a simple, scalable, unsupervised-training-based method that addresses these limitations. Central to NGA is stacking of differentiable assignment optimization with neural components, enabling high-dimensional parameterization of the matching process through a learnable temperature mechanism. We further theoretically analyze the learning dynamics of NGA, showing its design leads to fast convergence, better exploration-exploitation tradeoff, and ability to escape local optima. Extensive experiments across MCES computation, graph similarity estimation, and graph retrieval tasks reveal that NGA not only significantly improves computation time and scalability on large instances but also enhances performance compared to existing methodologies. The introduction of NGA marks a significant advancement in the computation of MCES and offers insights into other assignment problems. Code is open-sourced at https://anonymous.4open.science/r/NGA-10E3.", "tldr": "This paper proposes a neural graduated assignment approach to address the problem of finding the maximum common edge subgraph.", "keywords": ["maximum common edge subgraphs", "quadratic assignment programming", "graph similarity", "graduated assignment"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fcc716d7ab77632887ad2b7f893b31e9590806e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the *Neural Graduated Assignment* (NGA), a neural optimization framework designed to solve the MCES problem efficiently. Traditional algorithms for solving MCES suffer from scalability issues, especially when dealing with large graphs. The NGA approach overcomes these limitations by leveraging a learnable temperature mechanism and unsupervised training, allowing the algorithm to scale efficiently and avoid computational bottlenecks. The paper provides theoretical justification for the efficacy of NGA, demonstrating its ability to balance exploration and exploitation in the search process. Experimental results show that NGA significantly improves computation time, scalability, and performance compared to existing methods, making it a promising solution for MCES and related problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Approach: NGA represents advancement in solving the MCES problem by integrating neural components with traditional optimization frameworks. The use of a learnable temperature schedule and the end-to-end trainability of the model is an innovative approach.\n\n2. Theoretical Foundation: The authors provide a strong theoretical analysis of NGA, including its convergence properties and the mechanisms through which it escapes local optima. This adds robustness to the method and makes it more reliable in practical use.\n\n3. Extensive Experiments: The paper includes thorough experimentation across multiple datasets and tasks, such as MCES computation, graph similarity estimation, and graph retrieval, showing that NGA outperforms existing methods in terms of both accuracy and computational efficiency."}, "weaknesses": {"value": "1. The paper's reliance on modeling MCES as a QAP leads to O(N²) space complexity due to the affinity matrix. For large graphs (e.g., those with more than 100 nodes), this could pose significant memory challenges, making the method less feasible for very large-scale graphs. The paper does not provide an analysis of memory usage, which is a crucial aspect when considering practical applications for large datasets.\n\n2. While the paper claims that NGA is interpretable, the exact mechanisms through which the model learns to assign graph correspondences are not fully clear. The interpretability of the learned assignments, especially in complex graph structures, could be more thoroughly discussed.\n\n3. While the paper compares well against traditional solvers and some learning-based graph matching models (NGM), it could be strengthened by a comparison to other recent neural approaches for combinatorial optimization on graphs, e.g., unsupervised methods for graph matching."}, "questions": {"value": "1. Could you provide more details on how the learnable temperature schedule in NGA compares to other dynamic temperature annealing methods in optimization? Is there a specific advantage in using this formulation over others?\n\n2. Could you provide an analysis of CPU/GPU memory usage w.r.t. the scale of graphs?\n\n3. How does the performance of NGA change when applied to graphs with significant amounts of noise or missing data? How would NGA perform in scenarios with highly asymmetric graph pairs (e.g., when the graphs have very different numbers of nodes/edges)? Could you analyze the robustness of NGA?\n\n4. What impact does the choice of neural network architecture have on the overall performance of NGA? Would other architectures, like graph transformers, yield better results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ExiBJ3gFAS", "forum": "ZVlTIyRe35", "replyto": "ZVlTIyRe35", "signatures": ["ICLR.cc/2026/Conference/Submission1692/Reviewer_xBg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1692/Reviewer_xBg1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542199639, "cdate": 1761542199639, "tmdate": 1762915857350, "mdate": 1762915857350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackles the Maximum Common Edge Subgraph (MCES) problem: given two labeled graphs, the goal is to find the largest shared subgraph with the maximum number of common edges. This problem is especially relevant in molecular analysis and is NP-complete. The proposed method, Neural Graduated Assignment (NGA), is an unsupervised, learnable approximation with three main components: (i) it constructs an Association Common Graph (ACG) that only considers node and edge pairs that are label-compatible; (ii) it learns a soft node-to-node assignment that is iteratively refined using a learnable, high-dimensional temperature, which the authors argue improves convergence behavior; and (iii) it discretizes this soft assignment into a final match, using multiple sampled candidates at inference time to encourage exploration. The experiments evaluate NGA both on MCES quality and on downstream tasks such as graph similarity prediction and graph retrieval. NGA outperforms prior baselines under time-constrained scenarios on molecular datasets. Ablation studies support several design choices (use of high-dimensional temperature, sampling strategy, etc.) and provide insight into optimization dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method explicitly balances exploration and exploitation of the solution space.\n- The paper provides a clear analysis of the optimization dynamics, which gives provable guarantees for convergence.\n- The experiments are thorough: multiple tasks (MCES, similarity, retrieval), strong ablations, and consistent gains over competitive baselines.\n- It provides a better trade-off between scalability and performance than the prior works."}, "weaknesses": {"value": "1. The method is not reusable for more than one pair of graphs at a time.\n2. The paper assumes molecular settings, which have particular properties: the ACG is sparse, the node and edge labels are known and meaningful, etc.\n3. Runtime is mainly evaluated under a fixed time cap; it would be useful to see quality vs. time curves or difficulty-stratified results.\n4. Minor: some citation formatting issues in the appendix (e.g. lines 719,855,885)."}, "questions": {"value": "1. Why would supervised methods perform badly with multiple ground truths? Don't most tasks in deep learning have multiple equally good solutions anyway?\n2. How do the methods compare in number of learnable parameters?\n3. Can the labels/features be anything? How would the method perform if they are e.g. too fine-grained or noisy? For instance, in molecular datasets we know that the atom types are a good label, but in other instances we might not know what to choose.\n4. Relatedly, if the graphs are unlabeled, would it make sense to create the labels by another procedure (e.g. by structural cues) and then run the method? Would this perform better than not giving them any labels?\n5. How does complexity change if the ACG is not sparse? In which (non-molecular) settings can that occur?\n6. Which (non-molecular) domains are other realistic targets for NGA, and which remain out of scope under the current assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0RgKKcUlnO", "forum": "ZVlTIyRe35", "replyto": "ZVlTIyRe35", "signatures": ["ICLR.cc/2026/Conference/Submission1692/Reviewer_6SDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1692/Reviewer_6SDK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557352475, "cdate": 1761557352475, "tmdate": 1762915857170, "mdate": 1762915857170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Neural Graduated Assignment (NGA), a novel unsupervised neural optimization framework for solving the Maximum Common Edge Subgraph (MCES) problem. The authors formulate MCES as a Quadratic Assignment Problem (QAP) via the construction of an Association Common Graph (ACG). The core innovation lies in replacing the fixed temperature parameter in classical Graduated Assignment (GA) with a learnable, high-dimensional temperature parameterization, which enables adaptive exploration and exploitation during optimization. The method is unsupervised, scalable, and theoretically analyzed for its convergence and local optima escape behavior. Extensive experiments on molecular datasets demonstrate that NGA significantly outperforms existing methods in accuracy, scalability, and efficiency, and shows strong performance in downstream tasks like graph similarity computation and retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, Novel Formulation and Guarantees: The introduction of the Association Common Graph (ACG) is a crucial contribution. It provides an elegant way to ensure that any valid subgraph extracted from it is a correct common subgraph of the input graphs. This formulation cleanly transforms the MCES problem into a QAP with inherent structural guarantees, a foundational step that enables the subsequent neural optimization.\n2, Rigorous Theoretical Underpinning: The paper goes beyond empirical results by providing a solid theoretical analysis. It explains how NGA escapes local optima (Theorem 1) by leveraging the variance of the gradient, and why the product parameterization accelerates convergence (Proposition 2) compared to a scalar one. This theoretical grounding significantly strengthens the methodological claims.\n\n3, Differentiable and Adaptive Optimization Core: The proposed Neural Graduated Assignment (NGA) mechanism is the paper's central innovation. By replacing the static temperature in classical GA with a learnable, high-dimensional parameterization (  β_l = W_1^T  W_2), the method dynamically balances exploration and exploitation. This design eliminates cumbersome manual scheduling and allows the model to adapt its optimization trajectory to the specific problem instance, leading to faster convergence and better performance."}, "weaknesses": {"value": "1.Limited Discussion on Unlabeled Graphs: The method assumes labeled graphs (node/edge features). While this is reasonable for molecular data, many real-world graphs are unlabeled or partially labeled. The paper does not discuss how NGA might be adapted to such settings, which limits its generalizability.\n\n2, Computational Overhead of ACG: The construction of the ACG is central to the method but may become computationally expensive for very large graphs. The paper does not analyze the scalability of ACG construction in depth, nor does it compare its overhead relative to the overall optimization.\n\n3, Theoretical Assumptions: The theoretical analysis relies on small ∣ β_l ​ ∣ assumptions (Lemma 1), which may not always hold in practice. The empirical distribution of  ∣ β_l ​ ∣  (Fig. 11) shows both small and large values, so the applicability of the theory across all layers is not fully justified."}, "questions": {"value": "1，ACG Scalability: What is the time and space complexity of constructing the ACG? How does it scale with graph size and label dimensionality? Could approximate or sparse ACG constructions be used for very large graphs?\n\n2， Generalization to Unlabeled Graphs: Have you considered or experimented with unlabeled graphs? Could structural embeddings (e.g., positional encodings) replace or complement label information in such cases?\n\n3， Training Stability: The product parameterization β_l = W_1^T  W_2 can lead to unstable gradients. Did you observe such issues during training? Were any techniques (e.g., gradient clipping, normalization) used to stabilize training?\n\n4, Choice of Parameterization: Why was the product form W_1^T  W_2 ​ chosen over other parameterizations (e.g., MLP or direct scalar)? Was this motivated by empirical performance or theoretical insights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6wiDvaLNCK", "forum": "ZVlTIyRe35", "replyto": "ZVlTIyRe35", "signatures": ["ICLR.cc/2026/Conference/Submission1692/Reviewer_iuLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1692/Reviewer_iuLj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1692/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762674212150, "cdate": 1762674212150, "tmdate": 1762915856960, "mdate": 1762915856960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}