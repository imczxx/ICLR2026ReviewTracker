{"id": "epfwDqw7wn", "number": 13861, "cdate": 1758223917076, "mdate": 1759897407915, "content": {"title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "abstract": "As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, a crucial one being theory of mind (ToM), or the ability to reason about the \"mental\" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments. \n  \nWe validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents. Code at anonymous.4open.science/r/decrypto/.", "tldr": "We introduce Decrypto, an interactive language-based benchmark for LLMs to evaluate multi-agent reasoning and theory of mind.", "keywords": ["theory of mind", "pragmatics", "multi-agent reasoning", "interactive evaluation of LLM agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6950626aaeec3fcb6545dc0db8776bcc61107a5.pdf", "supplementary_material": "/attachment/8a33bf7d24e60769357cf5d4f36f63699dba315c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DECRYPTO, a new benchmark for evaluating multi-agent reasoning and theory of mind (ToM) in large language models (LLMs). The authors design a simplified, language-based environment in which two cooperative agents need to communicate using word associations while avoiding interception by a third agent. The benchmark aims to isolate ToM abilities by minimizing factors like symbolic or spatial reasoning. The paper presents extensive experiments with both open- and closed-source LLMs, human-AI comparisons, and ToM-specific adaptations of classical psychology experiments (e.g., Smarties Task). Results show that even advanced LLMs underperform humans and simple word-embedding baselines, revealing current deficiencies in ToM and multi-agent reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a new and creative approach by transforming a social deduction game into an interactive benchmark for theory of mind (ToM), effectively bridging the gap between static ToM tasks and multi-agent environments.\n2. The experimental bench is comprehensive and carefully designed, including baselines, human studies, and ToM experiments, with rich methodology and reproducibility details.\n3. Overall presentation is clear and well-structured, with intuitive examples that illustrate game mechanics and reasoning processes. The authors also explain the prompts and protocols for clear explanation.\n4. The proposed benchmark addresses a key limitation in evaluating agentic LLMs about reasoning about others’ thoughts and offers a scalable, open-ended platform for future research in human-AI collaboration and multi-agent reasoning."}, "weaknesses": {"value": "1. The paper would benefit from a deeper quantitative analysis of model behavior. Specifically, explaining why certain models fail systematically in particular scenarios and providing more concrete examples or error case analyses.\n2. The experimental evaluation could be strengthened by including a broader range of recent and advanced LLMs, both open-source and closed-source, to provide a more comprehensive comparison.\n3. The paper could offer more practical insights into how DECRYPTO can inform or guide LLM fine-tuning for improved ToM abilities, for example by proposing or testing preliminary training or adaptation strategies."}, "questions": {"value": "- The authors could analyze whether different hint or keyword types (e.g., abstract vs. concrete) affect miscommunication or interception rates?\n- The paper could also discuss potential LLM ability biases that may exist in the benchmark’s design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xEoMXQgZz8", "forum": "epfwDqw7wn", "replyto": "epfwDqw7wn", "signatures": ["ICLR.cc/2026/Conference/Submission13861/Reviewer_HFtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13861/Reviewer_HFtN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880183855, "cdate": 1761880183855, "tmdate": 1762924380690, "mdate": 1762924380690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the game-based benchmarks, DECRYPTO, for evaluating LLMs in multi-agent reasoning and Theory of Mind capabilities without confounders. Through this benchmark,  the authors show that existing LLMs still suffers from ToM skills thus perform poorly in DECRYPTO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work pick DECRYPTO , which is simple, to clearly investigate the LLM's performance on theory of mind. \n- Extensive experiments and analysis are executed to analyse LLMs' performance during competition, coordination and doing ToM, demonstrating useful conclusions."}, "weaknesses": {"value": "- The paper claim they are the first benchmark that designs interactive ToM experiments. In fact, there are some related works may have explored but not very comprehensively on this topic, e.g. \n\nMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\n\n- The fixed roles (Encoder, Decoder, Interceptor) might limit the exploration of richer multi-agent dynamics (e.g., negotiation, deception, coalition)."}, "questions": {"value": "- The paper suggests LLMs fail to model other agents’ beliefs. Do the authors have evidence that this is due to architecture, training data, or prompting limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PcwOg8K3eZ", "forum": "epfwDqw7wn", "replyto": "epfwDqw7wn", "signatures": ["ICLR.cc/2026/Conference/Submission13861/Reviewer_E5CK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13861/Reviewer_E5CK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984555276, "cdate": 1761984555276, "tmdate": 1762924379940, "mdate": 1762924379940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DECRYPTO, a novel game-based benchmark designed to evaluate large language models’ (LLMs) ability to reason about other agents—known as Theory of Mind (ToM)—in interactive, multi-agent settings. Inspired by the board game Decrypto, it tasks AI agents with encoding, decoding, and intercepting word-based messages, requiring pragmatic inference and belief modeling. The benchmark minimizes confounding factors such as mathematical or spatial reasoning to isolate ToM performance. Experiments show that even advanced reasoning models (e.g., GPT-4o, o3-high, Claude 3.7) underperform compared to humans and simple word-embedding baselines. The study extends classic cognitive psychology tasks (Smarties and Three Mountain experiments) to interactive AI environments, demonstrating that state-of-the-art LLMs still fail to model others’ beliefs effectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies a missing point in prior static or text-based benchmarks. By adapting the board game Decrypto, it provides a clear, interpretable, and engaging framework that tests pragmatic inference, cooperation, and competition among agents.\n\n2. The benchmark is carefully designed to remove confounding factors such as mathematical, spatial, or symbolic reasoning, focusing purely on language-based reasoning and perspective-taking.\n\n3. The paper is validated on extensive experiments involving open- and closed-source LLMs, human-AI interactions, and classic psychology-inspired tasks."}, "weaknesses": {"value": "While DECRYPTO is elegantly designed, it remains an artificial language game that may not fully capture the complexity or ambiguity of real-world multi-agent communication. The constrained, turn-based structure and reliance on predefined keywords could limit its ecological validity and applicability to open-ended human interactions."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hc4Xe63fBS", "forum": "epfwDqw7wn", "replyto": "epfwDqw7wn", "signatures": ["ICLR.cc/2026/Conference/Submission13861/Reviewer_fmHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13861/Reviewer_fmHC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102015261, "cdate": 1762102015261, "tmdate": 1762924378959, "mdate": 1762924378959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DECRYPTO proposes a multi-agent, word-association game benchmark (inspired by the board game Decrypto) to test cooperation, competition, and theory of mind (ToM) in LLMs.  This work evaluates a range of open/closed models, adds specialist embedding baselines (GloVe/Word2Vec), and designs two interactive ToM experiments (representational change/false belief; perspective taking). They report that LLMs trail simple embedding baselines in cooperative play, excel as interceptors against other LLMs, and perform weakly on stronger ToM variants."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- an interesting ToM setting with a clean, language-only, interactive testbed for multi-agent reasoning and ToM that avoids many common confounds\n- broad evaluations across model families/sizes; cooperative and competitive regimes; human-AI cross-play; prompt variants; also two adapted experiments (RC/FB and PT) provide diagnostic granularity"}, "weaknesses": {"value": "- section 4 feels overlong and under-integrated with ToM claims. Much of it re-states known concepts (zero-shot, OOD) without showing how these choices sharpen or test ToM hypotheses. Lines 190–209 are especially verbose, and the assertions that “specialists can overfit DECRYPTO” are plausible but not verified in this setting; also, how do authors ensure some foundation models do not see DECRYPTO in their training?\n- the linkage between word-association mismatch and ToM is unclear. The paper attributes cross-play failures to “different word associations” but it’s not shown how this specifically implicates ToM rather than lexicon alignment. If ToM is the target construct, the paper should formalize ToM competence operationally (beyond game wins) and disentangle it from vocabulary grounding\n\n- How do wins reflect ToM instead of public-knowledge heuristics? It remains possible models succeed by exploiting hint history or broad world knowledge rather than reasoning about another agent’s beliefs. The RC/FB/PT tasks help, but the main game results are not causally tied to ToM competence (no interventions that selectively alter others’ knowledge to see if Alice adapts).\n\n- the benchmark is named after a game by others. The paper introduces a variant (three players, specific rules), claims RSA grounding, and an interactive ToM platform. The contribution would read cleaner if the name distinguished the original game from the benchmark extensions here. Also what is RSA formalization is used for?\n\n- results discussion (sec. 5) is often post-hoc and shallow. Coordination/competition subsections describe outcomes but rarely test why (e.g., ablations that manipulate Eve’s access). Several insightful observations (e.g., o3-high overestimates interception rate) are stated without follow-ups.\n\n- ToM experiments (Sec. 5.1) could be structured more cleanly; that is, the two procedures and findings are interwoven, and it is recommended to move the PT and RC/FB flow diagrams (Fig. 5/6) into the main text (instead of using heavy text for a better clarity) and present each experiment as Protocol → Metrics → Results → Takeaway would improve clarity.\n\n- some claims need tightening. In lines 244–245, shared embeddings may yield shared similarity structure, not necessarily ToM. Any empirical support? Also, can authors justify “Tweak K to operate in a regime …” (lines 249–250)?\n\n-  Sec. 3 (benchmark description) repeats mechanics and metrics (miscommunication/intercept) more than needed; the “future-proof” subsection collects heterogeneous points. Consider tightening and moving important, frequently referenced appendices (L, M; ToM diagrams) into the main text.\n\n- Some citations (e.g., “Hu et al.” at line 43) lack full details; figures 5, 6 and sections L, M (critical to ToM) sit in the appendix, but intensively discussed in main text"}, "questions": {"value": "- How do you empirically separate belief modelling from shared word associations? \n\n- Can you run interventions that alter Eve’s accessible history (hide a hint; inject misleading public info) and check whether Alice systematically adapts hints? That would more directly tie wins/losses to ToM.\n\n- For the PT finding where many models predict near-certain interception on turn 1: did you verify whether they use that prediction to change hint strategy? If not, why is PT not feeding back into decisions? Can authors elaborate more on the failure: \"that of integrating ToM reasoning in decision-making\", how ToM is integrated in decision-making\n\n\n- Can you demonstrate overfitting with a specialist agent?\n- which concrete design choices (hint constraints, history visibility, metrics) follow from RSA predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ozqztcyp5", "forum": "epfwDqw7wn", "replyto": "epfwDqw7wn", "signatures": ["ICLR.cc/2026/Conference/Submission13861/Reviewer_vmgS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13861/Reviewer_vmgS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192351232, "cdate": 1762192351232, "tmdate": 1762924378451, "mdate": 1762924378451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}