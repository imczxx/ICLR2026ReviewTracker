{"id": "J9cwjzmiEK", "number": 23416, "cdate": 1758343474031, "mdate": 1759896816016, "content": {"title": "Expand Neurons, Not Parameters", "abstract": "This work demonstrates how increasing the number of neurons in a network without increasing its number of non-zero parameters improves performance. We show that this gain corresponds with a decrease in interference between multiple features that would otherwise share the same neurons. To reduce such entanglement at a fixed non-zero parameter count, we introduce Fixed-Parameter Expansion (FPE): replace a neuron with multiple children and partition the parent’s weights disjointly across them, so that each child inherits a non-overlapping subset of connections. On symbolic tasks, specifically Boolean code problems, clause-aligned FPE systematically reduces polysemanticity metrics and yields higher task accuracy. Notably, random splits of neuron weights approximate these gains, indicating that reduced collisions, not precise assignment, are a primary driver. Consistent with the superposition hypothesis, the benefits of FPE grow with increasing interference: when polysemantic load is high, accuracy improvements are the largest. Transferring these insights to real models—classifiers over CLIP embeddings and deeper multilayer networks—we find that widening networks while maintaining a constant non-zero parameter count consistently increases accuracy. These results identify an interpretability-grounded mechanism to leverage width against superposition, improving performance without increasing the number of non-zero parameters. Such a direction is well matched to modern accelerators, where memory movement of non-zero parameters, rather than raw compute, is the dominant bottleneck.", "tldr": "We show that taking a model and increasing its number of neurons without changing the number of non-zero parameters improves accuracy via a reduction in superposition", "keywords": ["Interpretability", "Feature Coding", "Superposition", "Sparsity"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56efebfcf9f68c48857af93cc9142126de6fa523.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Fixed-Parameter Expansion (FPE), a method to improve neural network performance without increasing the total count of non-zero parameters. The core hypothesis is that smaller, dense networks suffer from \"polysemanticity,\" where individual neurons represent multiple unrelated features, causing interference that degrades performance. FPE addresses this by creating a wider, sparser network with the same parameter budget. The authors show that FPE improve accuracy on both symbolic Boolean tasks and real-world image classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **S1:** From my knowledge, the concept of decoupling the number of neurons from the number of non-zero parameters appears to be a novel contribution.\n* **S2:** The authors provide direct evidence that FPE leads to more disentangled representations, connecting the architectural change to a concrete improvement in the network's internal feature geometry."}, "weaknesses": {"value": "* **W1. Limited Experimental Setting:** The empirical validation on real-world tasks is exclusively performed on MLP classifiers that operate on pre-computed, frozen CLIP embeddings. This fails to demonstrate if the FPE principle holds for the other models. Without such evidence, the claim that FPE can can guide in improving architecture design, is not yet fully substantiated.\n\n* **W2. Lack of Absolute Performance Metrics:** The paper exclusively reports relative improvement in accuracy. While scientifically valid for isolating the effect of FPE, this makes the results difficult to interpret from a practical standpoint. Without knowing the baseline accuracy, it is hard to understand the significance of the improvements. Including a simple table with absolute accuracies for at least one key experiment would greatly enhance the paper's transparency and impact. \n\n* **W3. Clarity:** The paper’s overall clarity is limited, and parts of the presentation are at times confusing. Additionally, the paper's central claim is that FPE is a \"post-training\" procedure. However, in the caption of Figure 5, the authors claim to pre-train the model for 25 epochs before FPE. My understanding is that the baseline model is trained for a short period, then expanded via FPE, and then training is continued for a substantial number of epochs (e.g., 1000 in the symbolic task). This lengthy \"fine-tuning\" phase makes it unclear whether FPE is a technique for improving a converged model or, rather, a structured re-initialization method used early in a much longer, combined training process. \n\n* **W4. Unclear Practical Integration:** While successfully establishing a proof-of-concept, the paper lacks a discussion on how FPE could be integrated or how it might interact with other optimization techniques like quantization or knowledge distillation. This omission, combined with the lack of absolute performance data and the limited experiments, makes it difficult for a reader to assess the effort versus reward of adopting this method in an applied setting.\n\n* **W5. Reproducibility:** Providing the implementation would be a valuable contribution to the community."}, "questions": {"value": "* **Q1. Choice of CLIP and Additional Results:** Could the authors elaborate on the decision to use only frozen CLIP embeddings? Additionally, what's the authors hypothesis on how FPE would perform in an e2e training scenario where the feature extractor's weights are also being updated. Would the benefits of FPE be amplified or diminished? Could the authors provide results with other embeddings?\n\n* **Q2. Absolute Accuracy:** Would the authors be willing to provide the final absolute test accuracies for both the dense baseline and the FPE models for the image classification experiments presented in Figure 5?\n\n* **Q3. Clarification on the Training Methodology and the Role of FPE:** I would appreciate further clarification on the multi-stage training methodology, which I found to be a key source of ambiguity in the paper. A more detailed explanation would help in understanding the precise nature and contribution of the FPE intervention. Specifically:\n    *   **Q3.1 Rationale for Intervention Timing:** Could the authors elaborate on the rationale for the timing of the FPE intervention (e.g., applying it after 25 epochs)? How sensitive is the method's effectiveness to this choice? For instance, does applying it much earlier in training or only after the baseline model has fully converged significantly alter the results?\n    * **Q3.2 Rationale for Training Duration:** Similarly, what was the rationale for the extensive subsequent training period (e.g., 1000 epochs for the symbolic task)? Does FPE require this lengthy fine-tuning to realize its benefits, or could comparable gains be achieved with a much shorter training schedule?\n    * **Q3.3 Conceptual Interpretation of the Method:** Should FPE be interpreted as a method for structured re-initialization that provides a better starting point for a long training run, or as a method to apply after the training that does not require further fine-tuning?\n    * **Q3.4 Suggestion on Terminology:** I found the overlapping terminology to be a primary source of confusion. To improve the paper's clarity, the authors could consider explicitly distinguishing in the text between the use of the \"pre-trained\" CLIP model for feature extraction and the \"initial training phase\" of their own classifier.\n\n* **Q4. Ablation on the Classifier:** For the vision tasks, the paper mentions using a 1-layer network for all the datasets except for Imagenet1K where a 5-layer network was used. Was there any ablation study on how the architecture of the classifier itself interacts with the FPE procedure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VptM6BUkKu", "forum": "J9cwjzmiEK", "replyto": "J9cwjzmiEK", "signatures": ["ICLR.cc/2026/Conference/Submission23416/Reviewer_VKCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23416/Reviewer_VKCg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758364022, "cdate": 1761758364022, "tmdate": 1762942652319, "mdate": 1762942652319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Fixed Parameter Expansion (FPE), a method for improving neural network performance by increasing the number of neurons while keeping the total number of non-zero parameters constant. The core idea is to reduce \"polysemanticity,\" where a single neuron represents multiple features and causes interference. FPE handles this by partitioning the incoming connections of each neurons into non-overlapping components. This disentangles feature representations without increasing the model's non-zero parameter count. The authors show that on both symbolic reasoning tasks and real-world image classification datasets (like CIFAR-100 and ImageNet), FPE improves accuracy. The performance gains are most significant in small networks where feature interference is a major bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well-written and easy to follow.\n\n2.\tThe idea of increasing the number of neurons by constructing sparse input weight matrix is interesting. This approach keeps the non-zero parameter count constant.\n\n3.\tThe paper conducts abundant ablation studies, which help readers understand the effect of each quantity and the limitation of the proposed method.\n\n4.\tThe examples and illustrations are intuitive."}, "weaknesses": {"value": "1.\tThe paper is motivated by previous research on interpretability, particularly studies focused on the semantic interpretability of neurons, such as the superposition hypothesis and the polysemanticity of neurons. However, the paper does not include experiments that directly address semantic interpretability. Most of the experiments focus on performance improvements, and there are no analyses or visualizations that interpret the semantics of the “disentangled” features resulting from neuron expansion. As a result, it remains unclear whether neuron expansion actually enhances interpretability.\n\n2.\tThe widths of the models evaluated in this study are significantly smaller than those used in practical applications. For instance, the paper sets the hidden layer width to a maximum of 16 for models trained on ImageNet, whereas typical image recognition models commonly have widths exceeding 512. This discrepancy limits the practical relevance of the experimental findings. Although the authors state that “the models are intentionally underparameterized […] to study the emergence of superposition and interference,” it is important to note that real-world models are usually overparameterized, and the superposition phenomenon is still observed in such settings.\n\n3.\tThere is no Related Work section. Although the authors mention a number of related papers in the Introduction, I still recommend adding a Related Work section to systematically survey previous studies and compare them with this paper."}, "questions": {"value": "1.\tThis question is a continuation of Weaknesses 1. Is there a way to interpret the semantics of the “disentangled” features after neuron expansion? \n\n2.\tQuestion about the training procedure. In Line 198, the paper states that models on the Boolean DNF task is trained for three stages: (1) warm-up training for 1000 epochs, (2) neuron expansion, (3) continued training for 1000 epochs. Are the models on image recognition tasks also trained with the same three-stage scheme? It seems that the paper does not mention the continued training stage for these image models. In addition, I wonder whether the weight masks are fixed during the continued training stage.\n\n3.\tCould the authors provide a comparison between FPE and Sparse Autoencoders (SAEs)? Since SAEs are also based on the superposition hypothesis, it would be valuable to see a comprehensive comparison between these two approaches.\n\n4.\tMinor. In Algorithm A3, Line 8, what does the variable $r$ mean? Is it used in later part of the algorithm?\n\n5.\tMinor. In Line 187, the sentence “The full justification can be found in Section 2.5” seems to have a typo. I guess the authors are referring to some section in the Appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jtx7V8Qdyn", "forum": "J9cwjzmiEK", "replyto": "J9cwjzmiEK", "signatures": ["ICLR.cc/2026/Conference/Submission23416/Reviewer_1vPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23416/Reviewer_1vPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896436961, "cdate": 1761896436961, "tmdate": 1762942651792, "mdate": 1762942651792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Fixed-Parameter Expansion, which splits each neuron into several sub-neurons and divides its connections. This means the total number of weights stays the same, while we have more width. This is motivated by the view that polysemanticity can cause feature intereference which degrades performance. On synthetic Boolean tasks and real computer vision tasks, this improves accuracy by reducing collisions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Very simple and practical idea motivated by clear theory in understanding feature interference\n2. Works beyond simple toy data to real CV tasks\n3. Validation of theoretical prediction regarding collisions"}, "weaknesses": {"value": "1. Could be extended to modern architectural choices to see if gains persist\n2. Success of random splits casts doubt on the idea of structure-awareness \n3. Interference could be measured by mechanistic methods instead of just proxies"}, "questions": {"value": "1. Have you tried to implement this ablation on more realistic model architectures?\n2. Is there evidence of less entanglement by doing circuit analysis?\n3. Where do returns diminish?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0WatB7MVU", "forum": "J9cwjzmiEK", "replyto": "J9cwjzmiEK", "signatures": ["ICLR.cc/2026/Conference/Submission23416/Reviewer_gXok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23416/Reviewer_gXok"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986320847, "cdate": 1761986320847, "tmdate": 1762942651418, "mdate": 1762942651418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors conduct an empirical study to support the so-called superposition hypothesis, which states that neurons in small neural networks represent different features simultaneously. Unfortunately, due to this sharing arrangement, there is interference between the features represented by the same neurons, which in turn handicaps model performance. \n\nTherefore, the authors argue that, assuming the hypothesis is true, we should be able to split these \"polysemantic\" neurons into multiple neurons, each representing a single feature. Furthermore, the authors propose a method for splitting polysemantic neurons that doesn't increase the model's non-zero parameter count.\n\nThe authors demonstrate this effect on small neural networks, where they indeed observe an improvement in performance after splitting up to a point. However, if neurons are split too much, performance decreases; hence, in practice, one must find the \"sweet spot\" for how much to expand neurons."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Prima facie, the authors' work is fascinating. The fact that even the randomised masking approach can improve model performance significantly was quite surprising.\n\nThe writing is also quite clear."}, "weaknesses": {"value": "My main concerns with the authors' work are regarding impact.\n\nThe experiments in the paper are very small-scale. For their pilot study, they use single hidden layer MLPs with 8-16 neurons. This is fine to verify the initial effect, but even for their \"larger scale\" experiments in Section 3.4, they study single hidden layer MLPs with up to 128 neurons. As such, the author's claim that \"The results demonstrate that random splitting provides substantial interference reduction even at scale\" is not borne out by their experiments. To support this statement, the authors would need to conduct experiments on much larger models.\n\nI also found the starting sentence of Section 3.4 quite strange: \"While FPE is motivated primarily by theory ...\" - what theory are the authors referring to here? My understanding up to that point is that FPE is based on the superposition hypothesis, which is essentially a collection of purely empirical observations.\n\nFinally, the paper lacks an explicit \"Related Works\" section, so it is not entirely clear where the authors' work fits into the literature.\n\nMiscellaneous:\n - we duplicate $w_i$ across $\\alpha$ sub-neurons: shouldn't it be $n_{\\alpha * i: \\alpha * (i + 1)}$?\n - \"The full justification can be found in Section 2.5.\" - erroneous reference"}, "questions": {"value": "The random masking variant of FPE reminds me of (indeed, it essentially is) weight dropout, where we insist that exactly a fixed proportion of the weights for each activation are dropped. Firstly, I believe this connection should be noted as a related work, and second, this also suggests several other baselines to compare:\n - How does FPE compare with the suggested weight duplication scheme after which standard weight dropout is applied?\n - How does the following extended, \"bootstrapped\" model perform: imagine we perform FPE on the same model, but with several different seeds, after which we average the different model predictions. Could this be an even cheaper way to perform MC Dropout?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2VlkuGY3CY", "forum": "J9cwjzmiEK", "replyto": "J9cwjzmiEK", "signatures": ["ICLR.cc/2026/Conference/Submission23416/Reviewer_4ehG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23416/Reviewer_4ehG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360654513, "cdate": 1762360654513, "tmdate": 1762942650779, "mdate": 1762942650779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}