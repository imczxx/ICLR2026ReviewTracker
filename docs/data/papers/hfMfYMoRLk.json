{"id": "hfMfYMoRLk", "number": 6998, "cdate": 1758004410179, "mdate": 1759897878742, "content": {"title": "A Training-Free Framework for Long Video Understanding via Video-Query-Options Similarity", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in image and short video understanding tasks, but their performance on hour-long videos remains limited due to constraint of input token capacity. Existing approaches often require costly training procedures, hindering their adaptability to rapidly evolving MLLM architectures. In this paper, we propose a training-free framework for long video understanding, integrating three key innovations: Adaptive Frame Sampling (AFS), Dynamic Resolution Allocation (DRA), and Video-Query-Options Similarity (VQOS). AFS adaptively increases frame sampling density in highly relevant video segments to preserve critical temporal details, while DRA reduces spatial resolution in less relevant segments to suppress redundant information. VQOS enhances similarity calculation by prompting MLLMs to generate candidate answer options, fusing queries with options to refine relevance estimation. Mirroring human cognitive processes (hypothesis generation → focused verification → irrelevance filtering), our framework effectively improve model accuracy without fine-tuning. The method is implemented on LLaVA-Video and Qwen2.5-VL respectively, and experimental results show our method could achieve state-of-the-art performances over 5 mainstream benchmarks. More visualization results and code are available in the Appendix.", "tldr": "We propose a training-free framework for long video understanding that boosts MLLM performance via adaptive frame sampling, dynamic resolution allocation, and video-query-option similarity.", "keywords": ["long video understanding", "multimodal large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35139a2d567b5a2e641105a414220e89e916779b.pdf", "supplementary_material": "/attachment/a780754f9447cfd3076a78dcd6fbc774ab5836f0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a training-free framework aimed at improving long video understanding in MLLMs by integrating Adaptive Frame Sampling (AFS), Dynamic Resolution Allocation (DRA), and Video-Query-Options Similarity (VQOS). The approach leverages a video-text retrieval backbone to guide query-conditioned frame sampling and resolution strategies. VQOS uses the MLLM to generate candidate answer options and fuses them with queries to perform more targeted retrieval, akin to a hypothesis-verification cognitive process. The system is implemented on LLaVA-Video and Qwen2.5-VL (both 7B and 72B sizes) and tested across five mainstream long video benchmarks, showing measurable improvements over SOTA training-free and training-based methods. Extensive ablation and component analyses are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Empirical Evidence:** \nTable 1 (Page 6) and the associated ablations (Table 2, Table 3, Table 4, Table 6) demonstrate consistent and sizable gains on several long video understanding tasks, especially on LVBench and VideoEval-Pro. Improvements are especially marked for models applied to hour-long videos. Ablations quantify the effect of each component (AFS, DRA, option generation), while Figure 3 and Figure 4 provide insight into the trade-offs and behavior of the system—e.g., the diminishing returns in OCA versus MPCO as more option rounds are added (Page 9), as well as the visualization of similarity-guided sampling over time across the video timeline (Figure 4, Page 15).\n\n**2. Generalization:**\n The model's robust generalization is further highlighted as the results show that method can be seamlessly integrated with various mainstream MLLM backbones (such as LLaVA-Video and Qwen2.5-VL), yielding consistent performance improvements across the board. This adaptability underscores its powerful generalization and transfer capabilities."}, "weaknesses": {"value": "**1. Marginal originality:**\nThe core idea, using a retrieval model to select relevant video segments and adjusting sampling density, is essentially a combination of AKS (adaptive keyframe sampling) and AdaReTake (token-level redundancy reduction). The “Video-Query-Options Similarity” (VQOS) module is a minor twist: prompting the base MLLM to generate options and then fusing similarity across them. This is not a fundamental innovation but a straightforward heuristic that adds inference-time complexity.\n\n**2. Option generation bias:**\nThe method lets the same MLLM that will later answer the question generate “candidate options.” This risks information leakage and circular reasoning—if the model already encodes the answer distribution, the retrieval pipeline simply re-weights what the model already “believes.” The ablations (Table 2) show small gains from generated options (+0.7%, +0.4%), suggesting this stage adds little actual reasoning power.\n\n**3. VQOS dependence on retrieval quality.**\nPerformance gains (3–5%) largely stem from better retrieval models (PE-G/14). When using weaker CLIP backbones, the improvement vanishes. Thus, the method’s robustness across retrieval encoders or unseen domains is unproven.\n\n**4. Figures are cluttered:**\nFigure 2’s diagram is overly dense and still fails to clearly show the interaction between VQOS, AFS, and DRA. The legends mix fonts and arrows in ways that reduce readability."}, "questions": {"value": "1. How do you ensure that this “option generation” does not leak answer information or overfit to the model’s own prior? \n2. Would cross-model option generation (e.g., generate options with LLaVA, use Qwen2.5-VL for retrieval) yield similar improvements?\n\nMore questions please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ysQ3cq3DQw", "forum": "hfMfYMoRLk", "replyto": "hfMfYMoRLk", "signatures": ["ICLR.cc/2026/Conference/Submission6998/Reviewer_AQQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6998/Reviewer_AQQ4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761026719313, "cdate": 1761026719313, "tmdate": 1762919214803, "mdate": 1762919214803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free framework for long video understanding that integrates three components—Adaptive Frame Sampling (AFS), Dynamic Resolution Allocation (DRA), and Video-Query-Options Similarity (VQOS)—to efficiently manage the token budget by focusing computational resources on relevant video segments, reportedly achieving state-of-the-art results on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed framework is training-free and model-agnostic, making it a highly practical solution that can be readily applied to various off-the-shelf MLLMs (as demonstrated on LLaVA-Video and Qwen2.5-VL), enhancing its accessibility and potential for broad adoption."}, "weaknesses": {"value": "1. I observed that the model's performance initially improves but then declines as the number of candidate options increases. Could the authors explain this non-monotonic behavior? Is this phenomenon related to the concept of pass@k?\n\n​2. Could segmenting a long video into multiple short clips risk scattering information relevant to a long-term temporal question across different segments? If so, might this lead to incorrect segment localization when answering questions that require integrating evidence from across the entire video timeline?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsPwxiuElt", "forum": "hfMfYMoRLk", "replyto": "hfMfYMoRLk", "signatures": ["ICLR.cc/2026/Conference/Submission6998/Reviewer_1kjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6998/Reviewer_1kjF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744590337, "cdate": 1761744590337, "tmdate": 1762919214490, "mdate": 1762919214490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes three techniques to improve video prefilling without fine-tuning the MLLM. Inspired by human cognition, the framework first generates multiple candidate answers and uses their text embeddings (via a PE-G encoder) to retrieve relevant video clips. Given a computational budget, it then applies adaptive frame and spatial-resolution sampling to prefill the selected clips into the MLLM. Experiments on five benchmarks demonstrate consistent improvements over the baseline by a clear margin."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of generating candidate answers to guide retrieval is intuitive yet underexplored. Using answer-level text embeddings instead of query embeddings aligns well with how humans reason backward from possible outcomes, and the paper provides clear experimental validation of its effectiveness.\n\n\n2. The ablations are well-structured and provide good insight into how each component contributes to the overall improvement."}, "weaknesses": {"value": "1. The proposed two-pass pipeline introduces additional computation compared to single-pass or efficient prefilling methods. It would be helpful if the paper discussed the runtime overhead and its trade-off with accuracy.\n\n2. Since only the retrieved clips are passed to the MLLM, global contextual information might be lost. Have the authors considered combining local retrieval with a global summary embedding or a context fusion?"}, "questions": {"value": "It would be interesting to understand how the quality of the generated answer options affects final performance. For example, what happens if the options are produced by an MLLM without seeing the video (i.e., based purely on a textual question)? In addition, would using a stronger model (e.g., GPT-5) to generate the options lead to further gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RcZ5ynZIkr", "forum": "hfMfYMoRLk", "replyto": "hfMfYMoRLk", "signatures": ["ICLR.cc/2026/Conference/Submission6998/Reviewer_9bhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6998/Reviewer_9bhz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775051164, "cdate": 1761775051164, "tmdate": 1762919214127, "mdate": 1762919214127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}