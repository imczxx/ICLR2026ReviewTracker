{"id": "krGpQzo8Mz", "number": 22526, "cdate": 1758332263773, "mdate": 1759896861395, "content": {"title": "Latent Speech-Text Transformer", "abstract": "Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.", "tldr": "We introduce Latent Speech-Text Transformerwhich patches long speech token sequences into latent units, improving text–speech transfer while cutting pre-training and inference compute, and significantly outperforming existing speech-text LLMs.", "keywords": ["Speech–Text Models", "Latent Patching", "Multimodal Alignment", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0191bd7f8742669315e66e757bb537295afaf922.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the Latent Speech‑Text Transformer (LST), which models patches of speech tokens instead of individual tokens to reduce the speech‑vs‑text compute/sequence‑length imbalance in interleaved speech‑text LMs. A lightweight local encoder/decoder forms and reconstructs speech patches, while a global transformer models interleaved text tokens and speech patches; alignment‑based patching (Wav2Vec2+CTC word boundaries) and curriculum patching (aligned→static) are introduced to better synchronize content while enabling simple, static‑only inference. Across compute‑controlled and data‑controlled protocols, LST improves both S→S and T→T on HellaSwag, StoryCloze, TopicStoryCloze, with clearer gains from curriculum patching and competitive compute savings; the method also scales from 1B→7B parameters with consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "LST is a practical, well‑motivated way to mitigate speech‑text length and compute imbalance: it pairs a local encoder/decoder (restricted windows) with a global transformer to operate on information‑dense speech patches; alignment‑based patching improves semantic synchrony while curriculum avoids reliance on aligners at test time. The paper executes careful compute‑controlled and data‑controlled comparisons showing consistent S→S and T→T gains, compute savings under a fixed speech/text token budget, and robust scaling from 1B→7B. The patching‑strategy ablation clarifies why curriculum starting from Align (sil sep.) works well."}, "weaknesses": {"value": "- Originality\n    - The novelty is largely an application of BLT‑style patching to the speech‑text setting; stronger baselines (e.g., late cross‑attn fusion, gating/MoE variants) are not contrasted head‑to‑head under the same compute.\n    - Alignment‑aware and curriculum schedules are natural extensions; their conceptual leap is moderate relative to prior patching ideas in other modalities.\n- Quality\n    - Aligner choice is fixed to Wav2Vec2+CTC; there is no comparison to Whisper forced alignment or MFA‑style alternatives, so the robustness of the alignment dependency is unclear.\n    - The BPE SpeechLLM baseline uses 1k SentencePiece trained on 100k speech sequences, with a note that 5k/10k didn’t help; however, the paper doesn’t show tokenizer sufficiency checks (e.g., stability/perplexity/segmentation quality) that would rule out an under‑trained BPE baseline.\n    - Curriculum vs. Align (sil merged): Table 6 shows Align (sil sep.) generally stronger than sil merged on S→S, but it’s not shown whether a curriculum initialized from “sil merged” could match or exceed “sil sep.” for specific patch sizes.\n    - Compute savings are reported, but a concise methodological paragraph clarifying how savings are measured versus baseline token→patch conversion would aid interpretation.\n- Clarity\n    - Figure 2 terminology: “Patch Encoder/Decoder” appear to correspond to §3’s Local Encoder/Decoder; making this mapping explicit in the caption/text would reduce confusion (Fig. 2; §3).\n    - Local Encoder cross‑attention: clarify what queries what. Do latent patch embeddings query a local window of speech token embeddings (keys/values)?\n    - Dataset licensing/availability: Spotify Podcast (55k hrs) is listed (Table 1); please state license and present‑day access status for reproducibility/ethics.\n- Significance\n    - The gains are solid on story completion tasks synthesized with Kokoro TTS, but the generalization to other speech understanding tasks is not explored; a short discussion of when patching helps most would strengthen the takeaways."}, "questions": {"value": "- Can you confirm that Patch Encoder/Decoder in Fig. 2 are exactly the Local Encoder/Decoder of §3 and make that explicit in the text/caption? Also, in the local cross‑attention, what is the query and what are the keys/values?\n- Did you compare Wav2Vec2+CTC to Whisper alignment or MFA for word/BPE boundaries in terms of accuracy, speed, and downstream impact on LST vs. baseline? If not, any evidence that W2V2+CTC is near‑optimal for your training/inference regimen?\n- For BPE SpeechLLM, how did you determine the SentencePiece tokenizer is sufficiently trained (beyond vocab size sweeps)? Could a better BPE (more data, longer training, different seed, or unigram LM) close the gap to LST? Please provide tokenizer diagnostics or an expanded ablation.\n- You start curriculum from Align (sil sep.). Did you try starting from Align (sil merged) (arguably closer to static evaluation) and, if so, how do results compare across patch sizes?.\n- Given Spotify (55k hrs) is a substantial portion of training, can you clarify license, current accessibility, and whether research‑only use is still permissible? A short line in §4.1/Ethics would help replicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SX8VH3Cxq5", "forum": "krGpQzo8Mz", "replyto": "krGpQzo8Mz", "signatures": ["ICLR.cc/2026/Conference/Submission22526/Reviewer_Xr9g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22526/Reviewer_Xr9g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461115077, "cdate": 1761461115077, "tmdate": 1762942262188, "mdate": 1762942262188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a significant and well-known challenge in auto-regressive speech-text models: the information density mismatch between modalities. Speech, when tokenized (e.g., using HuBERT), results in disproportionately long sequences compared to the equivalent text tokens. The authors hypothesize this mismatch hinders speech-text alignment and leads to poor computational efficiency and scaling laws. To solve this, the paper introduces the Latent Speech-Text Transformer (LST), an architecture inspired by the Byte Latent Transformer (BLT). The core idea is to aggregate sequences of speech tokens into \"latent speech patches\" using a patch encoder. A global transformer then processes a shorter, more balanced sequence of interleaved text tokens and these latent speech patches. A patch decoder maps the latent representations back to speech tokens for generation. Moreover, the proposed LST architecture, and especially the curriculum patching method, is an innovative, practical, and effective solution. It demonstrates state-of-the-art performance gains over strong baselines in a rigorously controlled and very convincing experimental setup. Moreover, the work provides a practical and scalable architectural solution that improves the efficiency and alignment of multimodal speech models, which is of broad interest to the ICLR community."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper's primary strength is its direct and effective approach to a clear and significant problem in speech-text modeling. \n- The motivation is well-articulated, and the proposed LST architecture is a logical adaptation of patching techniques from other domains. The experimental design is rigorous, particularly the use of both compute-controlled and data-controlled settings, which provides a robust validation of the method's efficiency gains. \n- The most substantial contribution is the curriculum patching strategy. This is an innovative and highly pragmatic solution that achieves the \"best of both worlds\": it leverages the rich semantic guidance of an external aligner during training to produce robust representations, but transitions to a simple, dependency-free static patching method for inference. \n- The strong and consistent performance gains of this curriculum-based model over all baselines, combined with promising scaling results up to 7B parameters, makes it very effective."}, "weaknesses": {"value": "- The evaluation is focused on high-level narrative and commonsense reasoning tasks, with fine-grained lexical and syntactic benchmarks like sWUGGY and sBLIMP explicitly omitted. \n- This leaves open the question of how LST's token aggregation might affect performance on tasks requiring very fine-grained acoustic-phonetic or syntactic judgments. \n- Furthermore, the provided ablation on LST (Static), which is the \"pure\" architecture without aligner supervision, shows mixed results: it demonstrates a clear and strong advantage over both baselines on HellaSwag but underperforms the base model on TopicStoryCloze. This inconsistency, however, serves to strengthen the paper's main claim by justifying why the novel curriculum patching approach is necessary and superior, as it successfully resolves this instability and delivers robust performance across all tasks."}, "questions": {"value": "I would appreciate clarification on the following points:\n- The authors mention omitting sWUGGY and sBLIMP. While I understand the focus on narrative reasoning, could you speculate on how LST's aggregation mechanism might perform on these tasks? Would the patch decoder be sufficient to reconstruct the necessary fine-grained information, or do you expect a trade-off?\n- How sensitive is the curriculum patching method to the quality of the Wav2Vec2+CTC aligner? Would the benefits be held if a simpler, less accurate, or different-style aligner were used during training?\n- The t-SNE plots in Figure 4 are a nice qualitative illustration of LST's alignment. This claim would be significantly strengthened by including a parallel t-SNE plot showing the speech token embeddings from the baseline model for the same words. A direct visual comparison of cluster separation and tightness would make the \"improved alignment\" point much more immediate and convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jMX42JeWQy", "forum": "krGpQzo8Mz", "replyto": "krGpQzo8Mz", "signatures": ["ICLR.cc/2026/Conference/Submission22526/Reviewer_cWAg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22526/Reviewer_cWAg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560848064, "cdate": 1761560848064, "tmdate": 1762942261824, "mdate": 1762942261824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the latent speech-text transformer, a pre-training method for speech-text language models, with the goal of reducing the computation required for long speech sequences and the sequence length mismatch between text and speech. Specifically, the model works by aggregating discrete speech tokens into latent patches.The authors train LST models on interleaved speech-text data and ablate the different patching strategies used. Overall, LST models perform better than simple speech-text LMs on textual and spoken versions of commonsense and narrative coherence benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- The latent speech-text transformer (LST) is a novel architecture that attempts to address the long sequence length of speech tokens and the length mismatch with text. This is a step towards solving a significant problem that makes scaling speech models difficult\n- The authors propose and benchmark a variety of patching techniques that aggregate speech tokens together, and show how they can be combined into a stronger and faster model through curriculum learning\n- They analyze several factors related to the proposed method, such as scalability and compute equivalence."}, "weaknesses": {"value": "- Unclear evaluation procedure: the chosen evaluation sets rely on multiple choice QA. This means that in the speech setting, the model outputs HuBERT tokens. How are the output tokens converted to the actual multiple choice answer? Its unclear if ASR or some other method is used to map the model output to the actual answer choice. Without such information, reproduction and fair comparisons against this work are challenging.\n- Small test coverage: the evaluation only covers the aforementioned mQA tests. While this is a good evaluation of the model's \"intelligence\"-related capabilities, I am surprised there was no evaluation on more traditional speech tasks like ASR or TTS that measure the model's phonetic and cross-modal capabilities. I believe that it would be vital to test such abilities, since they may be affected by the compressed representations, which would lower the impact of the proposed method. Such experiments are done in the SpiritLM paper, which can be compared to.\n- While fine for understanding tasks, the proposed technique would be non-trivial to extend the multi-stream speech LMs that use neural codecs, which are the SOTA for generation tasks, limiting its impact.\n- While the scaling figure is nice, I think the data points appear too close together to be very meaningful (only from 10K to 25K iterations) for models that are trained for 200K iterations."}, "questions": {"value": "- How are the output hubert tokens converted to the actual multiple choice answer? Its unclear if ASR or some other method is used to map the model output to the actual answer choice.\n- Do they spoken Hellaswag / SC / TC datasets have multiple speakers or only use a single speaker?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ylAPrGM0S8", "forum": "krGpQzo8Mz", "replyto": "krGpQzo8Mz", "signatures": ["ICLR.cc/2026/Conference/Submission22526/Reviewer_SwjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22526/Reviewer_SwjZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958700068, "cdate": 1761958700068, "tmdate": 1762942261474, "mdate": 1762942261474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes _Latent Speech-Text Transformer_ (LST) for improving speech LLMs by using a pair of local encoder/decoder similar to the [Byte Latent Transformer](https://aclanthology.org/2025.acl-long.453.pdf):\n1.  The local encoder reduces the speech token rate via a cross attention layer turning speech tokens into _patches_.\n2.  The speech patches along with text tokens are processed by the global transformer. \n3.  The output from the global transformer can be used to directly predict the next text tokens. In order to predict the speech tokens, the corresponding global transformer output tokens are fed to the local decoder, which restores the tokens into the original speech token rate.\n\nThree patching schemes are explored:\n-   Static patching: Each patch consists of a fixed number of speech tokens without any overlap.\n-   Alignment patching: Each patch consists of a word / silence obtained from forced alignment using Wav2Vec2-CTC.\n-   Curriculumn patching: Training starts with alignment patching, then gradually transitions to static patching.\n\nModels trained from scratch using text and speech datasets are evaluated against a baseline transformer model that directly processes speech tokens in the same manner as text tokens. Evaluations are conducted on HellaSwag, StoryCloze, and Topic StoryCloze. For evaluating the speech processing capability of the proposed model, these test sets are also TTS'd by the authors using Koroko TTS. Evaluation results show LST leads to a noticeable improvement in both text and speech versions of these test sets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: This paper applies Byte Latent Transformer, originally designed for better byte based language modelling, to a new problem of speech language modelling. This is a reasonably novel approach compared to other approaches such as low bitrate speech codecs (e.g. SpeechTokenizer or encodec). The curriculum learning scheme that transitions from alignment patching to static patching is also novel.\n- Quality: The use of latent transformer is a simple and well motivated method for reducing the speech token rate, eventually resulting in a better language model for both speech and text.\n- Clarity: Most part of the paper is well organized and clearly written.\n- Significance: The proposed method compares favorably against the baseline model in both the text and TTS versions of HellaSwag, StoryCloze, and TopicStoryCloze evaluations."}, "weaknesses": {"value": "-   Clarity: To help readers not already familiar with Byte Latent Transformer, the description of the local encoder/decoder (161-185) could use some expansion. This is central to the main idea of this paper, and thus it would be great if the readers do not have to refer to another paper to understand the core ideas of this paper.\n-   Quality: Several changes in evaluation & experiments would be necessary to better support the claim of this paper.\n    -   In terms of speech modelling, all the evaluation test sets in this paper are produced by TTS. There is no evaluation on actual human speech, which often differ greatly from clean synthesized speech. For comparison, [the Spirit LM paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00728/127457), which is this paper's baseline model, included results on ASR (Table 5) and a comparison against a cascade system.\n    -   It is not clear why the authors chose to evaluate on their version of synthesized StoryCloze/TopicStoryCloze while there exists [a widely used version from the original authors](https://github.com/slp-rl/SpokenStoryCloze). This makes comparison against results from other papers very difficult.\n    -   The model is supposedly capable of both understanding and generating speech (line 269), however this is only evaluation of speech understanding in the current draft. In constrast, there is TTS evaluation in the Spirit LM paper."}, "questions": {"value": "Could you clarify what \"compute-controlled\" means in line 321? I'd expect the baseline model to need much more compute to process the same speech input due to the higher number of HuBERT tokens and the quadratic time complexity of transformer. But Table 3 seems to suggest that the number of interleaved tokens is only slightly lower different in the baseline compared against LST. In a setting where the baseline uses roughly the same flops, shouldn't it see far fewer speech tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8I6MvEqCS", "forum": "krGpQzo8Mz", "replyto": "krGpQzo8Mz", "signatures": ["ICLR.cc/2026/Conference/Submission22526/Reviewer_fuu2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22526/Reviewer_fuu2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973765651, "cdate": 1761973765651, "tmdate": 1762942261029, "mdate": 1762942261029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}