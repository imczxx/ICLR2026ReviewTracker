{"id": "QTh3aRcbTt", "number": 11508, "cdate": 1758200544612, "mdate": 1759897571294, "content": {"title": "Improved Regret for Decentralized Online Convex Optimization with Compressed Communication", "abstract": "We investigate decentralized online convex optimization with compressed communication, where $n$ learners collaboratively minimize a sequence of global loss functions using only local information and compressed data from their neighbors. Prior  work has established regret bounds of $O(\\max\\\\{\\omega^{-2}\\rho^{-4}n^{1/2},\\omega^{-4}\\rho^{-8}\\\\}n\\sqrt{T})$ and $O(\\max\\\\{\\omega^{-2}\\rho^{-4}n^{1/2},\\omega^{-4}\\rho^{-8}\\\\}n\\ln{T})$ for convex and strongly convex functions, respectively, where  $\\omega\\in(0,1]$ is the compression quality factor ($\\omega=1$ means no compression) and $\\rho<1$ is the spectral gap of the communication matrix. However, these regret bounds suffer from a quadratic or even quartic dependence on $\\omega^{-1}$. Moreover, the super-linear dependence on $n$ is also undesirable. To overcome these limitations, we propose a novel algorithm that achieves improved regret bounds of $\\tilde{O}(\\omega^{-1/2}\\rho^{-1}n\\sqrt{T})$ and $\\tilde{O}(\\omega^{-1}\\rho^{-2}n\\ln{T})$ for convex and strongly convex functions, respectively. The primary idea is to design a \\emph{two-level blocking update framework} incorporating two novel ingredients: an online gossip strategy and an error compensation scheme, which collaborate to achieve a better consensus among local learners. Furthermore, we establish the first lower bounds for this problem, justifying the optimality of our results with respect to both $\\omega$ and $T$. Additionally, we consider the  bandit feedback scenario, and extend our method with the classic gradient estimators to enhance existing regret bounds.", "tldr": "", "keywords": ["Decentralized Online Convex Optimization", "Compressed Communication"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd90029540e2b568224373bc1051d1eb976ab796.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the problem of decentralized online convex optimization (D-OCO) where communication between $n$ learners is compressed. The paper proposes a new algorithm, based on a two-level blocking framework designed to mitigate the three primary sources of error: consensus error, compression error, and projection error. The authors claim their algorithm achieves improved regret bounds of $\\tilde{O}(\\omega^{-1/2}\\rho^{-1}n\\sqrt{T})$ for convex functions and $\\tilde{O}(\\omega^{-1}\\rho^{-2}n\\ln T)$ for strongly convex functions. Furthermore, the paper establishes lower bounds for this problem, which the authors claim justify the optimality of their results. The framework is also extended to the bandit feedback setting, where similar improvements are claimed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper identifies a weakness in the prior art, specifically the D-OCO algorithm from Tu et al. (2022), which suffers from a high-order polynomial dependence on the inverse compression ratio ($\\omega^{-1}$) and the inverse spectral gap ($\\rho^{-1}$). The paper proposes a new algorithm, Top-DOGD, which achieves an exponential improvement in these dependencies, reducing the regret terms from $O(\\omega^{-4}\\rho^{-8})$ to $\\tilde{O}(\\omega^{-1/2}\\rho^{-1})$."}, "weaknesses": {"value": "*  While the improved bounds stated by the paper are correct, the algorithm used to achieve it is a synthetic combination of components drawn directly from the existing offline distributed optimization literature (Koloskova et al. (2019) and Huang et al. (2022)), an approach which lacks algorithmic innovation. The concept of using a \"blocking\" update to amortize multiple communication rounds within a single gradient step is also a standard, non-novel paradigm. \n\n* The paper's lower bound technique is also a direct and incremental adaptation of existing methods. The authors state their proof \"follows that of Wan et al. (2024)\". Their sole modification is to \"incorporate a dedicated compressor\", which they model as a \"randomized gossip\" protocol where communication succeeds with probability $\\omega$. This, in turn, modifies the expected communication delay by a factor of $1/\\omega$. This exact idea---constructing a lower bound by modeling the compressor as a probabilistic communication failure over a hard-to-communicate graph structure---was already introduced in Huang et al. (2022), which the authors fail to properly cite. Huang et al. constructed a zero-chain function split into two components and assigned them to different workers, forcing communication for the algorithm to make progress. They then introduced a \"rand-s\" compressor and explicitly modeled its effect as a probabilistic failure to transmit the necessary information, directly linking the convergence rate to the compression parameter.\n\n* Notwithstanding the above weaknesses, I believe the paper is not of interest and relevance to the ICLR community and is more appropriate for a more theoretical venue like ICML, NeurIPS, JMLR, or control journals like Automatica and IEEE TAC."}, "questions": {"value": "* Please compare your method with that of Cao & Basar (2023) which seem to be the closest to this work in terms of achieving the same regret."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kBsr3pR9Tt", "forum": "QTh3aRcbTt", "replyto": "QTh3aRcbTt", "signatures": ["ICLR.cc/2026/Conference/Submission11508/Reviewer_is8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11508/Reviewer_is8i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109226629, "cdate": 1762109226629, "tmdate": 1762922611077, "mdate": 1762922611077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies decentralized online convex optimization with compressed communication. A two-level blocking method—combining online gossip and error compensation—yields improved regret for convex and strongly convex losses with optimal dependence on the compression quality and spectral gap (backed by new lower bounds), and extends to bandit feedback via standard gradient estimators."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Provide improved lower bounds for convex online programming. Present a lot of results, e.g., improved upper bounds, new lower bounds, and extension to bandit feedback scenario.\n\n2. Established lower bounds for convex online programming. Proved that the upper bound matches with lower bound in terms of $T$ and $\\omega$.\n\n3. Extend the algorithm to the bandit feedback scenario."}, "weaknesses": {"value": "1. **Novelty**. I am concerned about the work’s novelty. The method largely combines two well-established components—multi-step gossip and multi-step compression—both already used to attain optimal rates in decentralized optimization and communication compression. It seems that the paper does not offer sufficiently new insights or algorithmic innovations.\n\n2. **Dependence on $\\omega$ and $\\rho$**. I am confused that the proposed bounds exhibit different dependences on $\\omega$ and $\\rho$ between the convex and strongly convex settings. Prior results [A1] and [A2] indicate a $\\rho^{-1/2}$ lower-bound dependence regardless of convexity, and [A3] suggests a uniform $\\omega^{-1}$ dependence across strongly convex, convex, and nonconvex regimes. Please clarify why the proposed bounds exhibit different dependences on $\\omega$ and $\\rho$. \n\n3. **Incomplete lower-bound characterization**. The lower-bound proofs appear to rely on restrictive assumptions—bounded gradients ($G$) and bounded domain diameter ($D$)—which are not used in decentralized stochastic optimization [A2] or compressed stochastic optimization. Please justify the necessity of these assumptions. In addition, the bounds do not make explicit the dependence on $L$ and $\\mu$; for strongly convex problems one typically expects a factor of $\\sqrt{L/\\mu}$, which is not reflected here.\n\n4. **Lower bounds for general vs. specific $\\rho$**. Theorems 3 and 4 claim lower bounds for any $\\rho\\in(0,1)$. However, to the best of my knowledge, the result in [A4] (the journal version of [A2]) for stochastic optimization—which is closely related to online optimization—holds only for the specific choice $\\rho=\\cos(\\pi/n)$. Please clarify how your argument extends to arbitrary $\\rho$.\n\n5. **Limited experiments**. The experiments are mainly focused on logistic regression. It is suggested to conduct modern experiments such as ResNet image classification and LLM fine-tuning. \n\n[A1] Optimal algorithms for smooth and strongly convex distributed optimization in networks\n\n[A2] Optimal Complexity in Decentralized Training\n\n[A3] Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression\n\n[A4] Decentralized Learning: Theoretical Optimality and Practical Improvements (The journal version of A2)"}, "questions": {"value": "1. Highlight the novelty and insights.\n\n2. Clarify why the proposed bounds exhibit different dependences on $\\omega$ and $\\rho$. \n\n3. Justify the necessity of bounded gradient and bounded domain in lower bounds.\n\n4. Clarify the dependence on $L$ and $\\mu$ in the lower bounds.\n\n5. Conduct modern experiments rather than the simple toy example on logistic regression."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r1zVGZ3Z04", "forum": "QTh3aRcbTt", "replyto": "QTh3aRcbTt", "signatures": ["ICLR.cc/2026/Conference/Submission11508/Reviewer_RFz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11508/Reviewer_RFz8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152216730, "cdate": 1762152216730, "tmdate": 1762922607611, "mdate": 1762922607611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates decentralized online convex optimization (D-OCO) with compressed communication, where $n$ learners collaboratively minimize a sequence of global loss functions using only local information and compressed data from neighbors. The authors propose Top-DOGD, a novel algorithm achieving improved regret bounds of $\\tilde{O}(\\omega^{-1/2}\\rho^{-1}n\\sqrt{T})$ and $\\tilde{O}(\\omega^{-1}\\rho^{-2}n \\ln T)$ for convex and strongly convex functions respectively, significantly improving upon prior work's quadratic/quartic dependence on $\\omega^{-1}$. The paper also establishes matching lower bounds and extends results to bandit feedback settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides substantial theoretical improvements in regret bounds for D-OCO with compressed communication, specifically reducing the dependence on the compression parameter $\\omega$ (from quadratic/quartic to linear/sublinear) and the spectral gap $\\rho$.\n\n2. The two-level blocking update framework—blending an online compressed gossip strategy with a projection error compensation scheme—demonstrates a creative approach to balancing communication constraints with optimization speed."}, "weaknesses": {"value": "1. The paper is hard to follow, and the writing should be improved  \n2. In a few places, explanation could be made crisper. For example, in Algorithm 2 (Page 6), some variable reuse (e.g., $\\hat{\\mathbf{y}}{j}^{(b_1)}(b)$, $\\hat{\\mathbf{y}}{i}^{(b_1)}(b)$) is a source of confusion, especially since projections, auxiliary variables, and block indices are deeply intertwined. Further, the definition of certain parameters (e.g., constants hidden in $O(\\cdot)$ in Theorems 1–2) is not always explicit, which could impede reproducibility or direct application.\n3. require knowing $\\rho, \\omega, \\beta$ (matrix norms), which may be unknown or drift in practice."}, "questions": {"value": "How to set $L_1, L_2, \\gamma$ without $\\rho, \\omega$ ? Can you provide adaptive or data-driven rules that don't need prior spectral/compression knowledge-e.g., doubling tricks or online estimation of $\\rho$ / compressor quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v0FXBb9FQF", "forum": "QTh3aRcbTt", "replyto": "QTh3aRcbTt", "signatures": ["ICLR.cc/2026/Conference/Submission11508/Reviewer_18Zx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11508/Reviewer_18Zx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168950623, "cdate": 1762168950623, "tmdate": 1762922607263, "mdate": 1762922607263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies decentralized online convex optimization (D-OCO) under compressed communication, where multiple agents cooperate over a network but exchange only quantized or compressed messages. Prior work (e.g., Tu et al., 2022) achieved regret bounds that scale poorly with the compression factor $\\omega$, the network spectral gap $\\rho$, and the number of agents $n$.  \n\nThe authors propose a new algorithm, called **Top-DOGD**, which employs a two-level blocking strategy that combines compressed gossip updates with a projection-based correction step. The method achieves improved regret bounds:\n$$\n\\tilde{O}(\\omega^{-1/2}\\rho^{-1}n\\sqrt{T}) \\text{ for convex losses, and } \n\\tilde{O}(\\omega^{-1}\\rho^{-2}n\\ln T) \\text{ for strongly convex losses,}\n$$\nalong with lower bounds\n$$\n\\Omega(\\omega^{-1/2}\\rho^{-1/4}n\\sqrt{T}) \\quad \\text{and} \\quad\n\\Omega(\\omega^{-1}\\rho^{-1/2}n\\ln T),\n$$\nwhich nearly match the upper rates up to logarithmic factors. The paper also extends the framework to bandit feedback and provides regret guarantees using both one-point and two-point gradient estimators."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper's writing is organized, and the results are internally consistent. It makes a solid theoretical contribution by tightening the dependence of regret on the compression factor and network parameters. The improvement in the convex setting from $\\omega^{-2}$ to $\\omega^{-1/2}$ (and similar for $\\rho$) is meaningful and clearly improves the state of the art, likewise for the strongly convex setting. The analysis is technically careful, with an intuitive exposition that demarcates consensus error, compression error, and projection error. The overall approach remains quite simple—blocking and error compensation are standard ideas, but combining them in this way for D-OCO under compression appears to be novel and is well-executed. The lower bounds are also valuable, as they lend credibility to the claim of near-optimality."}, "weaknesses": {"value": "**Missing References and the Issue with First-order Feedback**\n\nI believe that some essential references are missing, including [Patel et al.](https://proceedings.mlr.press/v202/patel23a.html) and the papers cited within. Discussing these references from the centralized setting, along with their results, will provide additional context for the paper's findings. \n\nSpecifically, Patel et al. point out that in the centralized setting, there is no benefit of collaboration when first-order information is available on each client. I believe this remains true in the context of this paper, and with non-collaborative OGD on each client, there is no need to do any compression or communication whatsoever. Could the authors clarify this issue? From what I can see, there appears to be no benefit to increasing $n$ in the regret guarantees, which makes me wonder if the first-order setting is essentially pointless to study. Unless I am missing something, this is my primary concern. \n\nAnother issue I have is about the regret minimization problem (1) itself. In principle, solving an online problem collaboratively only seems sensible if there is indeed some shared information between the clients. In the extreme case of distributed stochastic optimization, this is achieved by assuming some data similarity between the clients. However, even in the online case [Patel et al.](https://proceedings.mlr.press/v202/patel23a.html) examined the effect of having bounded gradient dissimilarity across clients, and their two-point bandit feedback algorithm does improve with lower heterogeneity. Can this benefit be seen in this paper's analysis as well? Perhaps for smooth online convex optimization? \n\nFinally, I am curious if Algorithm 2 of [Patel et al.](https://proceedings.mlr.press/v202/patel23a.html) can also be implemented using the ideas introduced in this paper. And if so, what guarantees of regret can be obtained? The projection error might need to be handled differently for this algorithm. \n\n**Lower Bounds**\n\nWhile the lower bounds seem correct, they are based on the worst possible compressors satisfying the paper's assumptions, which allows for the use of a very unstable compressor that, with some probability, outputs zero. I think this is a very pathological compressor, and I wonder if the authors could consider a more reasonable class of compressors and provide a lower bound for those. I understand that this might require changing the assumption on the compressor, and the upper bounds might improve as well (for instance, if the authors explicitly assume something like a randK compressor). \n\nWhile I currently give a score of 2, I am open to increasing it if my queries are answered, because I do believe there is technical novelty in the paper."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qyCnlHDAla", "forum": "QTh3aRcbTt", "replyto": "QTh3aRcbTt", "signatures": ["ICLR.cc/2026/Conference/Submission11508/Reviewer_kFty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11508/Reviewer_kFty"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231671503, "cdate": 1762231671503, "tmdate": 1762922606847, "mdate": 1762922606847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}