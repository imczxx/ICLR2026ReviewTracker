{"id": "TAthdtPe7k", "number": 12369, "cdate": 1758207302957, "mdate": 1763688173968, "content": {"title": "ICPO: Provable and Practical In-Context Policy Optimization for Test-Time Scaling", "abstract": "We study test-time scaling, where a model improves its answer through multi-round self-reflection at inference. We introduce In-Context Policy Optimization (ICPO), in which an agent optimizes its response in context using self-assessed or externally observed rewards without modifying its parameters. \nTo explain this ICPO process, we theoretically show that with sufficient pretraining under a novel Fisher-weighted logit-matching objective, a single-layer linear self-attention model can provably imitate policy-optimization algorithm for linear bandits. Building on this theory, we propose Minimum-Entropy ICPO (ME-ICPO), a practical algorithm that iteratively uses its response and self-assessed reward to refine its response in-context at inference time. \nBy selecting the responses and their rewards with minimum entropy, ME-ICPO ensures the robustness of the self-assessed rewards via majority voting. \nAcross standard mathematical reasoning tasks, ME-ICPO attains competitive, top-tier performance while keeping inference costs affordable compared with other inference-time algorithms. Overall, ICPO provides a principled understanding of self-reflection in LLMs and yields practical benefits for test-time scaling for mathematical reasoning.", "tldr": "We provide a provable and practical in-context policy optimization for test-time scaling", "keywords": ["in-context learning", "self-reflection", "policy optimization", "FTRL", "bandits", "large language models", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03f9661d9fa2b6b6e83a82fcae00fe7aa62f8a4b.pdf", "supplementary_material": "/attachment/343412ecdc329fdaf485093524f79f4d767a734d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the In-Context Policy Optimization (ICPO) framework for understanding the phenomenon of test-time scaling in LLMs, i.e., when models improve their responses through multi-round self-reflection without parameter updates. The main idea is to model this process as an agent that optimizes its response (action $x$), based on a history of in-context action-reward pairs $(x_t, r_t)$, effectively performing policy optimization within the context window. The main theoretical contribution proves that a single-layer Linear Self-Attention (LSA) transformer can imitate a specific policy optimization algorithm: a variant of FTRL for linear bandits. This imitation is achieved when the LSA model is pre-trained using a Fisher-weighted logit-matching objective. The authors present this as a foundational proof of how an attention-based architecture can learn to perform in-context optimization. Building on this theoretical result, the authors propose a practical, gradient-free inference-time algorithm called Minimum-Entropy ICPO (ME-ICPO), which they leverage for complex mathematical reasoning tasks. At each round, ME-ICPO generates multiple candidate solutions, assigns self-assessed rewards using majority voting on the final answers, summarizes the reasoning paths (Chain-of-Thought) to manage context length, and selects the next reasoning step to add to the context based on a minimum-entropy criterion. The experiments show that ME-ICPO achieves competitive, and in some cases state-of-the-art, performance on benchmarks like AIME, AMC, and MATH."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. To best of my knowledge, the paper introduces a first framework to formally model in-context self-improvement as a policy optimization problem. The authors provide a new and principled perspective on the mechanisms underlying test-time scaling.\n\n2. The main theorems establish population-level equivalence to an FTRL-like algorithm and provide finite-sample guarantees for learning this algorithm from data. They also analyze the stability of the learned policy to reward perturbations. \n\n3. The proposed ME-ICPO algorithm is an effective method. It demonstrates substantial and consistent performance improvements over specialized base models on multiple mathematical reasoning benchmarks.\n\n4. ME-ICPO is a gradient-free, inference-time-only algorithm. This makes it significantly more computationally efficient (particularly in terms of VRAM) than methods that require test-time backpropagation, such as TTRL. This practicality makes it a more accessible method for improving LLM performance."}, "weaknesses": {"value": "Please respond to weaknesses, I will consider raising my score from 6 to 8 if all weaknesses are addressed -- this is a good paper!\n\n1. The most significant weakness is the large abstraction gap between the theoretical model and the practical application. The theory is built on a single-layer Linear Self-Attention model solving a linear bandit problem, whereas the experiments are run on deep, multi-layer, non-linear transformers performing complex, structured reasoning. The paper's claim to \"explain\" the mechanism of self-reflection is an overstatement. The theory provides an elegant proof-of-concept that the attention mechanism can implement a form of optimization, but it does not and cannot prove that this specific linear mechanism is what underlies the sophisticated self-correction abilities observed in models like Qwen2.5-Math. The paper would be stronger if it framed the theory more cautiously as an inspirational, minimal model that demonstrates a core computational capability, rather than a direct explanation of an emergent phenomenon.   \n\n2. The ablation study shows that the minimum-entropy selection criterion is the most critical component of ME-ICPO. However, the paper's justification for this heuristic is purely intuitive, suggesting it avoids \"corrupted\" responses and encourages \"diversified\" ones. This justification is somewhat vague and potentially self-contradictory (low entropy implies low diversity). The success of this heuristic may be domain-specific. For mathematical problems with a single correct reasoning path, low entropy (high agreement among future sampled paths) is likely a strong proxy for correctness. However, for more open-ended or creative tasks, the optimal path might be one that leads to a rich and diverse set of possibilities (high entropy). The paper lacks a more formal justification for this algorithmic choice and does not compare it against more standard selection criteria from RL, such as simply selecting the candidate with the highest self-assessed reward.   \n\n3. Modeling a multi-round reasoning process as a sequence of K-armed bandit pulls is a major simplification. This abstraction ignores the stateful and compositional nature of logical deduction. Each step in a mathematical proof is not an independent choice from a fixed set of K options; rather, it generates a new logical state that constrains all subsequent steps. A more faithful, albeit likely intractable, model would involve a contextual bandit or a full Markov Decision Process (MDP). The paper should explicitly acknowledge and discuss the limitations of this memoryless abstraction and how it impacts the interpretation of the theoretical results.\n\n4. (minor) There is clear over-abuse of spacing in the paper in terms of vspaces. While I realize all authors use this, the authors should not abuse it. Please remove these if your paper is accepted.\n\nTypos and grammatical errors:\n- ...the model's ability to digest the in-context information to improve their response. $\\rightarrow$ ...to improve its response.\n- Such an in-context information can be... $\\rightarrow$ Such in-context information can be...\n- ...without answering why these ability emerge... $\\rightarrow$ ...without answering why this ability emerges... (or ...why these abilities emerge...)\n- ...learn to optimize it's behavior x by optimizing it's policy... $\\rightarrow$ ...learn to optimize its behavior x by optimizing its policy...\n- ...how LLM leverage the in-context actions... $\\rightarrow$ ...how LLMs leverage the in-context actions...\n- ...to improve it's response \\$x\\_\\{t+1\\}\\$... $\\rightarrow$ ...to improve its response \\$x\\_\\{t+1\\}\\$...\n- ...generating it's response \\$x\\_t\\$ and receives... and then improve it's response... $\\rightarrow$ ...generating its response \\$x\\_t\\$ and receives... and then improves its response...\n- ...into it's policy optimization process and to gradually improves its response. $\\rightarrow$ ...into its policy optimization process and to gradually improve its response.\n-  ...where the agent generates and improve it's response... $\\rightarrow$ ...where the agent generates and improves its response...\n- ...denotes its norm \\$l\\_2\\$ For a matrix A. $\\rightarrow$ ...denotes its norm \\$l\\_2\\$. For a matrix A.\n-...during the test-time can improve... $\\rightarrow$ ...during test-time can improve...\n- ...including the Monte-Carol Tree Search... $\\rightarrow$ ...including the Monte-Carlo Tree Search...\n- ...where the LLM evaluate their own response... $\\rightarrow$ ...where the LLM evaluates its own response... (or ...LLMs evaluate their own...)\n-  ...by directly assume the LLM's ability... $\\rightarrow$ ...by directly assuming the LLM's ability...\n-  ...trained linear self attention can implement... $\\rightarrow$ ...trained linear self-attention can implement...\n- ...multi head constructions... $\\rightarrow$ ...multi-head constructions...\n-  ...in which first layer heads preprocess... $\\rightarrow$ ...in which first-layer heads preprocess...\n-  ...rare recent literature have covered... $\\rightarrow$ ...rare recent literature has covered...\n- ...optimize it's policy \\$x\\_t\\$... $\\rightarrow$ ...optimize its policy \\$x\\_t\\$...\n- ...dataset is generating from the policy... $\\rightarrow$ ...dataset is generated from the policy...\n- ...similar with the Follow-the-Regularized Leader... $\\rightarrow$ ...similar to the Follow-the-Regularized Leader...\n- ...defined by \\$s\\propto log~p\\$ In the following... $\\rightarrow$ ...defined by \\$s\\propto log~p\\$. In the following...\n-  ...prefix of trajectory up to... $\\rightarrow$ ...prefix of trajectory \\$\\tau\\$ up to...\n-...exploration parametery is wide... $\\rightarrow$ ...exploration parameter \\$\\gamma\\$ is wide...\n-...and p is a normalization factor... $\\rightarrow$ ...and \\$\\rho\\$ is a normalization factor...\n-  The LSA model parameterized by starts with... $\\rightarrow$ The LSA model parameterized by \\$\\theta\\$ starts with...\n- ...the LSA model updates it's policy... $\\rightarrow$ ...the LSA model updates its policy...\n-  ...corresponding K dimension... $\\rightarrow$ ...corresponding \\$K\\$ dimensions...\n- The expected matrix I is inspired by... $\\rightarrow$ The expected matrix \\$\\Gamma\\$ is inspired by...\n- The Fisher-weighted loss provide new loss... $\\rightarrow$ The Fisher-weighted loss provides a new loss...\n- ...that common KL loss between... $\\rightarrow$ ...that the common KL loss between...\n- ...using the KL loss enable the transformers... $\\rightarrow$ ...using the KL loss enables the transformers..."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7Y8OCkaPT7", "forum": "TAthdtPe7k", "replyto": "TAthdtPe7k", "signatures": ["ICLR.cc/2026/Conference/Submission12369/Reviewer_jkwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12369/Reviewer_jkwj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442368467, "cdate": 1761442368467, "tmdate": 1762923278477, "mdate": 1762923278477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers."}, "comment": {"value": "We sincerely thank all reviewers for their insightful and constructive feedback. We appreciate the recognition of our mathematically grounded explanation (Reviewer gZmq, XA9N, 4bex, jkmj), practical algorithm (Reviewer gZmq, 4bex, jkmj). According to these comments, we have improved the paper (new pdf uploaded) and highlighted the main changes with red text. Below, we summarize all changes besides carefully correcting the typos. \n\n1. Table 1 now reports *mean ± std over 5 seeds* to address variance reporting. (Sec. 6.3, [L418–L419]; addresses **Reviewer gZmq Q1**)\n2. we highlight in Section 4 that the theoretical results serve as an inspirational analysis for in-context self-reflection via a minimal model (linear self-attention).(Sec. 4, [L196–L199]; addresses **Reviewer jkwj Q1**)\n3. As requested by Reviewer jkwj, we note in the Conclusion that modeling multi-round reasoning as an MDP and analyzing multi-layer, nonlinear Transformer structure are left for future work. (Sec. 7, Conclusion, [L482–L485]; addresses **Reviewer jkwj Q3**)\n4. Added compute comparisons vs. ToT, MCTR, and TTRL on AIME-2024/AMC/MATH, including time per question. ( Appx. B.4.1, [L1359–L1371]; addresses **Reviewer XA9N Q2**)\n5. Added frontier/long-CoT models: Qwen3-4B-Instruct, Gemini-2.5-Pro, Gemini-2.5-Flash on AIME (Mean@16 & Acc). (Appx. B.4.2, [L1373–L1377]; addresses **Reviewer XA9N Q4**)\n6. Added harder benchmarks HMMT and APEX-shortlist with Qwen2.5-Math-7B and Gemini-2.5-Flash. (Appx. B.4.3, [L1378–L1382]; addresses **Reviewer XA9N Q5**)"}}, "id": "Cbf01OnL8w", "forum": "TAthdtPe7k", "replyto": "TAthdtPe7k", "signatures": ["ICLR.cc/2026/Conference/Submission12369/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12369/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12369/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763686931971, "cdate": 1763686931971, "tmdate": 1763686931971, "mdate": 1763686931971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper introduces ICPO, framing multi‑round self‑reflection at inference as **in‑context policy optimization** that uses self‑assessed or external rewards without parameter updates. The authors prove that, under **Fisher‑weighted logit‑matching** pretraining, a **single‑layer linear self‑attention (LSA)** model can imitate a policy‑optimization algorithm for linear bandits. Building on this, they propose **Minimum‑Entropy ICPO (ME‑ICPO)**, a practical test‑time algorithm that iteratively samples candidates, assigns self‑assessed rewards, and selects low‑entropy, high‑confidence responses; **majority voting** is used to robustify the reward signal.\n- Experiments use **Qwen2.5‑Math‑7B** and **Qwen2.5‑Math‑1.5B** across **AIME‑2024, AMC, and MATH L1–L5**, etc., demonstrating the power of ME-ICPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) **Clear mechanistic link:** a theoretically grounded account connecting pretraining under a Fisher‑weighted objective to in‑context policy‑optimization behavior in an LSA. \n2) **Practicality:** ME‑ICPO yields strong math‑reasoning gains with gradient‑free test‑time optimization; **Mean@16 can surpass the base model’s majority‑vote upper bound**, and adding majority vote on ME‑ICPO output brings further gains."}, "weaknesses": {"value": "- **No variability reported in Table 1.** Table 1 reports only point estimates (Accuracy and Mean@16) with no variability across multiple runs; please add mean±std over, e.g., 5 seeds. \n- **Theory scope.** Guarantees apply to a **single‑layer LSA** and **linear bandits**; practical models may not be LSA, so the theoretical guarantees do not directly cover the standard non-LSA archetictures."}, "questions": {"value": "1) Although the proofs target single‑layer LSA, **can ME‑ICPO be safely applied to general (non‑LSA, multi‑layer) Transformers in practice**?\n2) The paper should clearly articulate the scope and the required setup for ICPO vs ICPO with LSA. In Section 4, it seems ICPO is only defined with LSA, is this correct? If so, is ME-ICPO only defined for LSA models?\n3) \"that with sufficient pretraining under a novel Fisher-weighted logit-matching objective, a single-layer linear self-attention model can provably\nimitate policy-optimization algorithm for linear bandits\", does ME-ICPO described in Algorithm 1 **require** such pretraining to work? \nIt is not clear whether ME-ICPO can be used as a test-time only method OR it has to be bundled with the specific pretraining procedures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mkkCEXnuKY", "forum": "TAthdtPe7k", "replyto": "TAthdtPe7k", "signatures": ["ICLR.cc/2026/Conference/Submission12369/Reviewer_4bex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12369/Reviewer_4bex"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605385626, "cdate": 1761605385626, "tmdate": 1762923278222, "mdate": 1762923278222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ICPO (In-Context Policy Optimization), a framework showing that a one-layer linear self-attention model can imitate a policy-optimization algorithm under a Fisher-weighted training objective. Motivated by insights from the theoretical results, the paper also proposes a practical algorithm, ME-ICPO, which performs multi-round generation, self-assessment with majority voting, chain-of-thought summarization, and minimum-entropy response selection to enable reward-aware prompting and principled feedback selection without further training. The empirical results show improvements on mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is interesting to formulate ICPO as a bandit-style policy optimization approach. The theoretical grounding for in-context self-refinement is potentially impactful if the claims hold in more realistic settings. \n\n2. The framework and algorithm diagrams are well-organized, and the writing is mostly easy to follow."}, "weaknesses": {"value": "1. The theoretical framework in Section 4 uses a linear bandit abstraction and a simplified linear self-attention model, whereas ME-ICPO is demonstrated with models like Qwen2.5-Math-7B. It is not clear how these theoretical assumptions connect to the practical model choices.\n\n2. ICPO requires iterative sampling, which implicitly increases inference compute. The paper only compares with the base model; since this is technically a prompting technique, it is unclear how this improvement differs from test-time scaling methods such as Tree-of-Thoughts, ReAct, and Monte-Carlo Tree Refinement, or from lightweight training methods such as GRPO and TTRL.\n\n3. The ME-ICPO also seems limited. Majority voting requires that (1) the model has sufficient capability to solve the task, (2) reasoning verification is cheap and easier than generation, and (3) the majority answer correlates with correctness. These appear to be strong assumptions that many real tasks may not satisfy."}, "questions": {"value": "1. How does the method perform on recent long-CoT models, for example Qwen3-4B-Instruct? And since this is a training-free method, how does it perform even on frontier models, such as GPT-5 or Gemini-2.5-Pro?\n\n2. How does ICPO extend to harder tasks—for example HMMT, APEX-shortlist tasks—or tasks without final-answer executability, or where the final answer is not discrete?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iExplonOUP", "forum": "TAthdtPe7k", "replyto": "TAthdtPe7k", "signatures": ["ICLR.cc/2026/Conference/Submission12369/Reviewer_XA9N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12369/Reviewer_XA9N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957807775, "cdate": 1761957807775, "tmdate": 1762923277963, "mdate": 1762923277963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces In-Context Policy Optimization (ICPO), a theoretical framework that explains how large language models can self-improve during test-time by iteratively refining their responses without parameter updates.The paper formulates multi-round self-refinement in LLMs as a form of in-context policy optimization, where the model treats its previous responses and associated rewards as contextual experience to adjust future outputs. This extends existing in-context learning theory from supervised prediction to policy optimization with bandit feedback. One the theoretical side, the authors prove that a single-layer linear self-attention transformer, when pretrained using a Fisher-weighted logit-matching objective, can provably imitate a policy optimization algorithm for linear bandits, thereby establishing a mechanistic explanation for the emergence of self-reflection in LLMs. Based on the theory, the paper proposes ME-ICPO, a practical inference-time algorithm that performs iterative response refinement using self-assessed rewards and entropy-based selection to ensure robustness to reward noise. Across standard mathematical reasoning benchmarks, ME-ICPO achieves competitive and often state-of-the-art test-time performance while maintaining affordable inference cost, demonstrating that test-time scaling can be improved without parameter fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors derive provable guarantees showing that a linear self-attention transformer, when trained under a Fisher-weighted objective, can imitate the behavior of a policy optimization algorithm in a linear bandit setting. This is a novel result from the theoretical perspective. \n\nThe paper proposed Minimum-Entropy ICPO (ME-ICPO) algorithm which demonstrates a practical and implementable version of in-context policy optimization. It integrates entropy-regularized response selection and self-assessed rewards, leading to consistent empirical improvements in mathematical reasoning tasks. The experimental results are strong and align with the theoretical insights. \n\nBy modeling the self-reflection and iterative response refinement as In-Context policy optimization problem, the paper offers a clear mechanistic and mathematically grounded explanation for self-improvement phenomena observed in LLMs."}, "weaknesses": {"value": "The effectiveness of ME-ICPO depends on choices such as number of refinement rounds, sample count per round, and entropy thresholds. Tuning those hyperparameters are non-trivial and might heavily depend on model sizes and datasets."}, "questions": {"value": "How can we handle the situation that the model itself cannot score or rank its own responses? How can we handle mis-aligned reward heuristics that the incorrect reasonings are being rewarded or reinforced? How to mitigate such caveats?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W8QLsFBkyu", "forum": "TAthdtPe7k", "replyto": "TAthdtPe7k", "signatures": ["ICLR.cc/2026/Conference/Submission12369/Reviewer_gZmq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12369/Reviewer_gZmq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993315734, "cdate": 1761993315734, "tmdate": 1762923277720, "mdate": 1762923277720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}