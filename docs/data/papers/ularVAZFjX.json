{"id": "ularVAZFjX", "number": 7272, "cdate": 1758013757641, "mdate": 1759897862715, "content": {"title": "A Pitfall in Conformal Prediction:  When Shorter Intervals Are Not Better", "abstract": "Conformal prediction has become a cornerstone of distribution-free uncertainty\nquantification, conventionally evaluated by its coverage and interval length. This\nwork critically examines the sufficiency of these standard metrics. We demon-\nstrate that the interval length might be deceptively improved through a counter-\nintuitive approach termed Prejudicial Trick (PT), while the coverage remains\nvalid. Specifically, for any given test sample, PT probabilistically returns an inter-\nval, which is either null or constructed using an adjusted confidence level, thereby\npreserving marginal coverage. While PT potentially yields a deceptively lower\ninterval length, it introduces practical vulnerabilities: the same input can yield\ncompletely different prediction intervals across repeated runs of the algorithm.\nWe formally derive the conditions under which PT achieves these misleading improvements and provide extensive empirical evidence across various regression\nand classification tasks. Furthermore, we introduce a new metric interval stability which helps detect whether a new conformal prediction method implicitly\nimproves the length based on such PT-like techniques.", "tldr": "We find that the interval length might be improved through a invaild conformal prediction approach while introducing instability and unfairness", "keywords": ["Conformal Prediction", "Length Hacking"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1997ac2adfd32dad6a97dad769f3addb9ff6efe1.pdf", "supplementary_material": "/attachment/9dda57bcc08e63fde70322ddfc3783dc85b9ef38.zip"}, "replies": [{"content": {"summary": {"value": "The paper is clearly written and easy to follow. \n\nProblem context: Conventionally, CP methods are evaluated by its coverage and interval length. This work argues the sufficiency of these standard metrics.\n\nPaper's proposal: This paper introduces a mechanism called the \"Prejudicial Trick\" (PT) to demonstrate a supposed \"pitfall\" in the standard evaluation of conformal prediction (CP) methods. The authors claim that PT can \"hack\" the conventional coverage-length metric by probabilistically returning a null set (length 0) or a wider interval, thereby preserving marginal coverage while deceptively reducing the average interval length. The authors then argue this reveals a flaw in the standard metrics, as PT introduces practical instability (a random output for a fixed input). They propose a new metric, \"Interval Stability\", defined as the expected variance of the interval length, to detect this \"vacuous randomness.\" \n\nEmpirical results across numerous regression and classification datasets, using different base CP algorithms (VCP and CQR) , confirm that PT can deceptively improve interval length while the proposed \"Interval Stability\" metric successfully identifies the trick."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper makes a conceptual contribution. The CP community relies heavily on the coverage-length trade-off as the primary method for evaluation. This work demonstrates that these two metrics are insufficient for capturing the practical utility of a CP method. \n\n2. The Prejudicial Trick (PT) is simple and elegant. \n\n3. The theoretical analysis, such as the proofs that PT preserves marginal coverage (Theorem 4) and the conditions under which it reduces length (Theorems 7, 8, 11), appear to be mathematically sound.\n\n4. The proposed Interval Stability metric is intuitive, simple to compute, and (as shown empirically) diagnoses the issue of \"vacuous randomness\" introduced by PT."}, "weaknesses": {"value": "1. The title “A Pitfall in Conformal Prediction” strongly implies that the authors have identified a fundamental weakness in the conformal prediction framework. In reality, the pitfall lies entirely in the choice of evaluation metric, not in conformal prediction itself. The proposed “PT”  predictor is not a conformal method; it is an external randomization layer applied after conformal intervals have been constructed. Therefore, the phenomenon described is not a failure of conformal prediction, but a property of an artificially randomized post-processing step.\n\n2. The randomization mechanism used in PT is well-known probability argument. \n\n3. For all standard, deterministic CP methods (VCP, CQR, etc.), the metric \"Interval Stability\" will be identically zero. Thus, the metric is only useful in detecting authors' PT trick. The work does not identify a scenario where such behavior might arise in standard CP outputs.\n\n4. The paper mentions (in Remark 6) that \"as methods become increasingly complex, they may implicitly utilize similar randomness to improve the length\". The authors do not provide any legitimate, complex, published CP method suffers from this supposed \"implicit randomness\". \n\nWhile the paper is clearly written and motivated by an interesting observation about the variability of conformal intervals, the contribution remains conceptually and practically limited. The reported instability arises entirely from an artificial, externally randomized post-processing step (the “PT” predictor), which is not itself conformal. The randomization mechanism is a standard probability trick and does not constitute methodological innovation. Moreover, the proposed “interval stability” metric is only nontrivial for such randomized constructions and is identically zero for all standard deterministic CP methods (e.g., VCP, CQR, split CP). Therefore, the metric lacks general practical value. These limitations of the paper lead to a low score."}, "questions": {"value": "1. The title suggests a fundamental ``pitfall'' in conformal prediction, yet the instability arises only from an externally randomized post-processing (PT). Can you clarify why this should be viewed as a limitation of conformal prediction rather than of the PT randomization itself?\n\n2. For standard deterministic conformal methods (e.g., split CP, CQR, VCP), the interval stability metric is identically zero. In what realistic settings do you expect nonzero instability to occur without deliberately injecting randomness?\n\n3. With additional interval stability metric, what should practitioners aim for: target coverage, with small length, and zero interval stability? What is the advantage of this new metric in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8e3Qq6JK3V", "forum": "ularVAZFjX", "replyto": "ularVAZFjX", "signatures": ["ICLR.cc/2026/Conference/Submission7272/Reviewer_mhUR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7272/Reviewer_mhUR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622906705, "cdate": 1761622906705, "tmdate": 1762919399829, "mdate": 1762919399829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear Area Chair and all the reviewers,\n\nThank you for managing our submission. After a careful reading of all reviews, we see a clear divide. Our most negative \"Reject\" score are based on **two fundamental, factual misunderstandings of our paper's core thesis and contributions.**\n\nWe hope that you consider our response, as the negative reviews are not critiques of our actual paper, but of a different, \"strawman\" paper they thought we wrote.\nOur paper's thesis is simple:\n1. **The Premise:** The CP community universally evaluates methods on two metrics: Marginal/Conditional Coverage and Average Interval Length.\n2. **The Pitfall (Our \"Attack\"):** We demonstrate this {Coverage, Length} paradigm is flawed because the length metric can be \"hacked.\" We introduce a \"red team\" attack, the **Prejudicial Trick (PT)**, to prove this. PT is a simple, pathological procedure that uses **unprincipled randomness** to deceptively shorten the average interval length while preserving marginal coverage.\n3. **The Solution (Our \"Defense\"):** We then propose a concrete, practical solution to defend against this pitfall: a new diagnostic metric called **\"Interval Stability\"** (Definition 1, Section 4).\n\n# The negative reviews are based entirely on missing these points.\n\n## The Core Misunderstanding: \"This paper is about Conditional Coverage\"\nReviewer cviF's \"Strong Reject\" is based entirely on the factually incorrect premise that our paper is about the (well-known) limitations of marginal vs. conditional coverage.\n- Reviewer cviF's Claim: \n> \"The paper's primary conclusion, that marginal coverage is insufficient and conditional coverage is the more desirable property, is not a new insight.\"\n- The Fact: This is **verifiably false**. Our paper is **not** about conditional coverage. As Algorithm 1 clearly shows, PT's mechanism is **explicitly stochastic** (based on a $U \\sim \\text{Unif}([0,1])$ draw) and is **completely independent of any data features or properties.** Our \"pitfall\" is about the **length metric's vulnerability to randomness**, a topic entirely orthogonal to the conditional coverage debate. We refer to Theorem 5 and Theorem 6 that PT also preserves conditional coverage. \n\n## The Second Misunderstanding: \"The paper proposes PT as a method\" / \"Fails to propose an alternative\"\nReviewers QqNu, cviF, and mhUR all critique our paper from a second flawed perspective.\n- **Reviewer QqNu's Claim**:  PT \"artificial\" and \"not grounded in practice\" with \"limited practical utility.\"\n- **The Fact:** We explicitly agree. We state PT is a \"cautionary example\" and \"poorly suited for practical deployment\". The artificiality is the point. PT is not our contribution; it is the diagnostic tool we use to prove the evaluation metric is vulnerable.\n- **Reviewers cviF & mhUR's Claim:** They claim our paper \"fails to propose a concrete alternative\" or a \"new, practical evaluation metric.\"\n- **The Fact:** This is **demonstrably false.** They have **missed the entire constructive contribution of our paper: Section 4, \"INTERVAL STABILITY\"**. This metric is the concrete, practical, and novel solution we propose to fix the very flaw we identify.\n\n## Why Our \"Interval Stability\" Metric is NOT \"Useless\"\nReviewers RdVR and mhUR argue our new metric is not valuable because it is \"identically zero\" for \"standard deterministic CP methods.\"\n\n**This is a feature, not a bug**\n\n1. **A \"Pass\" Grade:** For deterministic methods (VCP, CQR), our metric correctly outputs 0. This is a \"pass\" grade, confirming these methods are stable and their reported length is genuine. This adds trust.\n2. **A \"Fail\" Grade:** For unstable methods like PT, it outputs a large non-zero value, flagging them as \"vacuous\" and unreliable.\n3. **The Real Danger (Remark 6):** The field is moving beyond simple deterministic methods to complex, stochastic ones (ensembles, MC-dropout, sampling). The danger is that these new methods will \"implicitly utilize similar randomness\" to achieve deceptive SOTA results. Our metric is the only tool to distinguish genuine length improvements from these deceptive ones.\n\n\n**In summary:** The negative reviews are based on a misunderstanding of our paper's core thesis. Our paper successfully **(1)** identifies a new, genuine flaw in the standard CP evaluation paradigm, **(2)** proves it with a simple, elegant attack (PT), and **(3)** provides the concrete, necessary solution (Interval Stability). We urge the AC to evaluate our work based on its actual contributions."}}, "id": "Ei3195Kjdm", "forum": "ularVAZFjX", "replyto": "ularVAZFjX", "signatures": ["ICLR.cc/2026/Conference/Submission7272/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7272/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7272/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763265486585, "cdate": 1763265486585, "tmdate": 1763265594181, "mdate": 1763265594181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a weakness in the standard evaluation of CP, arguing that the two most common metrics (marginal coverage and average interval length) are insufficient for a robust evaluation. \nThe authors introduce a pathological algorithm PT, which, by construction, maintains (or exceeds) the desired marginal coverage. \nPT strategically assigns either a null set or a wider-than-necessary interval to different data points based on their underlying properties, allowing the algorithm to achieve deceptively short interval lengths, even though it fails to provide meaningful uncertainty quantification for a subset of the data. \nThe authors argue that this pitfall underscores the need to move beyond marginal coverage and evaluate methods based on conditional coverage."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clear and easy to follow, and PT is a simple, well-designed, and intuitive counterexample. \n2. The authors identify a weakness in the common practice of optimizing for average interval length, demonstrating how this objective can be tricked."}, "weaknesses": {"value": "1. The paper's primary conclusion, that marginal coverage is insufficient and conditional coverage is the more desirable property, is not a new insight. Conditional coverage has been a big area of research in CP for many years. Prior work has extensively discussed the limitations of marginal coverage and proposed numerous methods towards better conditional coverage.\n2. The paper does not offer a practical, novel method or a new solution to a practical problem. Specifically, the paper argues against using common CP evaluation metrics, but it fails to propose a concrete alternative. It neither offers a new method to achieve it nor proposes a new, practical evaluation metric that could replace average length for comparing methods.\n3. The authors do not provide any evidence that widely used methods and datasets suffer from this pitfall - the paper's significance is primarily pedagogical. The experiments are essentially synthetic. An empirical study where conventional CP methods have this pitfall would significantly enhance the persuasiveness of the work.\n4. The efficiency gains from using PT-VCP are quite minimal and not convincing (table 2)"}, "questions": {"value": "1. Can you elaborate on your core contribution in the context of the existing literature that already advocates for conditional coverage? What does your paper add for someone who is already convinced that marginal coverage is insufficient and that we should move towards conditional coverage?\n2. Are there any existing, non-adversarial CP methods (like CQR or split) or other adaptive methods that fall into this trap on any real datasets?\n\nAddressing these concerns in-text will significantly enhance the paper's impact and quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I05tG1xxuT", "forum": "ularVAZFjX", "replyto": "ularVAZFjX", "signatures": ["ICLR.cc/2026/Conference/Submission7272/Reviewer_cviF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7272/Reviewer_cviF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866581788, "cdate": 1761866581788, "tmdate": 1762919399338, "mdate": 1762919399338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretically and empirically supported perspective that interval width and coverage alone are insufficient criteria for comparing prediction intervals. The authors argue that an additional metric---\\emph{interval stability}, defined as the variance of the interval length---should also be considered, as it captures how much the interval fluctuates across repeated samples. This is an important point, given that interval width and coverage are typically the primary metrics used to evaluate predictive intervals.\n\nTo motivate this claim, the authors introduce a simple construction---the ``Prejudicial Trick''---which can be applied to any conformal predictor to produce intervals that are, in many cases, narrower while still preserving marginal coverage. The method randomizes the output: with probability $p$, it returns the empty (null) set, and with probability $1-p$, it returns an enlarged conformal interval constructed at a higher coverage level than usual. The authors show, under mild assumptions, that the expected interval width of this randomized procedure---averaged over draws of the calibration data---is strictly smaller than that of the standard conformal interval. Intuitively, the zero width of the null set reduces the average more than the enlarged interval increases it.\n\nThe paper further provides several sets of sufficient conditions under which the proposed trick yields smaller expected interval widths, as well as a counterexample illustrating when the method fails to improve width.\n\nThe theoretical results are supported by several experimental studies"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The Prejudicial Trick is a simple, deliberately pathological construction that reduces the average length of any prediction interval without changing its marginal coverage (under suitable conditions). It demonstrates that interval length can be made artificially smaller in a misleading way: the resulting intervals are unstable and, with a fixed probability, collapse to a degenerate null set that provides no information. Although mathematically valid, the construction yields intervals that are clearly undesirable in practice. The authors substantiate this point with both theoretical analysis and empirical results.\n\n\nThe paper is clearly written and well structured, and the exposition effectively conveys the construction and its implications."}, "weaknesses": {"value": "1. Full conformal prediction and split conformal prediction are non-randomized procedures: once the calibration data, the conformity score, and (in the split setting) the train--calibration split are fixed, the resulting prediction set is fully deterministic. By contrast, the Prejudicial Trick (PT) proposed in the paper is a \\emph{randomized} construction: with probability $1-p$ it outputs a valid conformal interval, and with probability $p$ it outputs a degenerate null interval. As a result, rerunning the procedure on the same data may produce different outputs. In this sense, PT is not a conformal prediction method in the usual sense. (While there exist randomized variants of conformal prediction, the standard framework and the vast majority of methods are deterministic.)\n\nThe authors argue that PT demonstrates a fundamental limitation of evaluating intervals solely by coverage and average length. However, this claim only holds if one allows \\emph{randomized} algorithms. If we restrict attention to conformal prediction methods---or, more generally, deterministic procedures---PT no longer serves as a counterexample. In that setting, it is not clear that coverage and interval length are insufficient metrics. This distinction matters, because the motivating question on page~1 asks:\n\n \n``Can a conformal prediction method maintain valid coverage and deceptively improve interval length metrics through counter-intuitive constructions, while introducing practical risks?''\n \n\nPT is then presented as evidence that the answer is ``yes,'' but PT is not actually a conformal prediction method, so it does not address the stated question.\n\n \n\n2. More broadly, the paper would benefit from a decision-theoretic perspective. The situation is reminiscent of Hodges' superefficient estimator: it improves a standard performance metric in a pathological way, yet is inadmissible under any reasonable risk criterion. The paper argues that PT is undesirable because it introduces ``instability,'' but randomization is not inherently problematic. What is missing is a principled notion under which PT is formally suboptimal---for example, a proper scoring rule for set-valued predictions, a loss function that penalizes degenerate intervals, or a stability constraint that prevents algorithms from exploiting randomness to game marginal performance metrics. For instance, Section~6.2 of Gneiting and Raftery (2007) introduces the interval score as a proper scoring rule for prediction intervals. PT would perform very poorly under this score, since the null interval incurs a large penalty whenever the true value lies outside it.\n\nhttps://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\n\n\n3. On this note, the paper would benefit from a broader discussion of existing evaluation frameworks for predictive intervals. The forecasting literature is extensive, and similar issues regarding interval quality, proper scoring rules, and pathologies of evaluation metrics have been studied in depth. In particular, see  Gneiting and Raftery (2007) and references therein.\n\n\n4. Finally, the proposed stability metric is always zero for deterministic methods, and therefore functions only as a measure of randomization. Since most conformal prediction algorithms are deterministic, it is unclear how actionable this metric is in practice. Moreover, if a method appears unstable solely because it is randomized, how should this be interpreted? Randomization does not automatically imply deficiency, so a low stability score does not itself diagnose a problem. The paper would be stronger if it provided guidance on how such a metric should inform methodological choice: when does instability constitute a meaningful failure, and when is it merely a benign algorithmic feature?"}, "questions": {"value": "The guarantees in the paper are derived marginally over draws of the calibration data. This raises a natural question: does the Prejudicial Trick retain its properties when we condition on a fixed calibration set? In particular, does it still reduce the calibration-conditional average width while preserving calibration-conditional coverage? \n\nAs discussed in the weaknesses, can PT be shown to be suboptimal from a decision-theoretic perspective? E.g., in a minimax sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SKxpgLsde", "forum": "ularVAZFjX", "replyto": "ularVAZFjX", "signatures": ["ICLR.cc/2026/Conference/Submission7272/Reviewer_RdVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7272/Reviewer_RdVR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953565283, "cdate": 1761953565283, "tmdate": 1762919398890, "mdate": 1762919398890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the sufficiency of the standard approach in evaluating conformal prediction (CP) intervals involving two metrics: coverage and interval length. The authors introduce an adversarial approach called the \"Prejudicial Trick\" (PT), which yields CP intervals with deceptively lower interval lengths, but for which one input can yield significantly different prediction intervals across repeated runs of the algorithm. The main idea behind PT is to return a null prediction interval with some fixed probability and return confidence intervals with lower miscoverage rates in remaining cases. The paper derives the conditions under which PT achieves these misleading improvements and provide experimental evaluations for both regression and classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is quite well-written and easy to understand.\n\n- I like the simplicity of the PT adversarial device, which is easy to understand and explore theoretically. After demonstrating the construction of the PT trick, the authors present several necessary theoretical directions including coverage guarantees and sufficient conditions under which PT improves interval lengths. \n\n- In addition to devising PT, the paper also suggests a metric to counteract PT (\"interval stability\") which can flag the type of vacuous randomness of PT in proposed CP methods."}, "weaknesses": {"value": "- The contribution here (PT) has limited practical utility nor does it seem to expose a fundamental insight about CP pushing the development of CP methods forward. Perhaps the practical implication of PT is as a warning to researchers constructing CP methods on the perils of only relying on interval length and coverage. However, if this is the case, I find the construction of a CP approach with a probabilistic component assigning null intervals to be an artificial one not grounded in practice. \n\n- The current experiments seem to be essentially synthetic, demonstrating a potential corner-case that in theory could happen. The paper would benefit from showing even one convincing real application where an already proposed CP approach in the literature may have to deal with this PT danger."}, "questions": {"value": "1. Can you please answer how PT has practical relevance to researchers in CP?\n\n2. Is there an actual instance (using existing CP methods) where this problem arises, or could reasonably potentially arise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yqsAirQFEN", "forum": "ularVAZFjX", "replyto": "ularVAZFjX", "signatures": ["ICLR.cc/2026/Conference/Submission7272/Reviewer_QqNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7272/Reviewer_QqNu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120639495, "cdate": 1762120639495, "tmdate": 1762919398554, "mdate": 1762919398554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}