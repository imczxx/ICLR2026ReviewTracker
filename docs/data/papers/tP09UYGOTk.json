{"id": "tP09UYGOTk", "number": 13145, "cdate": 1758214077648, "mdate": 1763648324690, "content": {"title": "Emergent Alignment Via Competition", "abstract": "Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the user’s utility lies approximately within the convex hull of the agents’ utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition; (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria; and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two forms of empirical evidence: First, we perform simulations of the best-AI selection game using best response dynamics, which show that competition among individually misaligned agents reliably improves user utility when the approximate convex hull assumption is satisfied, but does not always when it fails. Second, we show that synthetically generated AI utility functions (produced via perturbations of the same prompt to evaluate instances on a movie recommendation (MovieLens) and ethical judgement (ETHICS) dataset) quickly produce a convex hull that contains a good approximation of a given utility function even when none of the individual LLM utility functions is well aligned.", "tldr": "", "keywords": ["alignment", "bayesian persuasion", "learning agents"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98f63a5384984dc60b50e5629674c2a492b55e96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new approach for AI alignment in the presence of multiple AI agents, treating the problem as a Stackelberg game and providing well-thought definitions of equilibria. The proposed approach could have a great positive impact in terms of preventing a human user from being exploited by any one particular AI agent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The modeling and definition using Stackelberg games and utility theory are constructed well. \n- I'm not super familiar with this field, but the definition of what we hope to see in the metrics and the goal of having an utility lower bound is expressed clearly. \n- The key idea of utility being contained within convex hull and its relation to bayes-optimal actions is very interesting."}, "weaknesses": {"value": "- The experimental results are convincing but rather limited in scope.The proposed approaches and game-theoretic formalisms are very interesting; having stronger sets of experiments can make the ideas more convincing. I would be willing to raise my score if this concern is addressed. This is also not my home field, so I am not sure what additional sets of experiments I could suggest to improve the experiment section..\n- The method of constructing \"aligned AIs with noisy approximations\" by using perturbed prompts seems like a rather weak way of producing such agents. Are there any reward-guided or reduced-rationality LLM sampling approaches that could be done in place of this?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zY4S3FV5aP", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_B3Yk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_B3Yk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673533857, "cdate": 1761673533857, "tmdate": 1762923862534, "mdate": 1762923862534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Overview"}, "comment": {"value": "We thank the reviewers for their careful and encouraging feedback. We have uploaded a new revision that addresses issues raised by each reviewer, and we respond to individual reviewer questions in comments under each review.\n\nHere we want to first emphasize what we think is (and is not) the main message of the paper, and then outline the new experiments that we have included in the revision.\n\n**First:** Our goal is to identify and analyze an important new “market alignment” condition that is significantly weaker and easier to satisfy than the condition that a single model provider be perfectly aligned. It is not to provide air-tight empirical evidence that this market alignment assumption is actually satisfied by current LLM providers and users; this is a great empirical question that arises from our work, but outside the scope of our paper.\n\nInstead, the goal of our experiments is to provide stylized (but robust) evidence that our market alignment condition is substantially easier to satisfy than individual model alignment.\n\nOur initial submission contained three experiments, in which “Alice” and “Bob” utilities were derived in different ways:\n* **ETHICS:** Here “Alice” utilities were elicited from an LLM using a prompt-defined persona, and the “Bob” utilities were elicited from LLM personas derived from “perturbations” of the Alice prompt which attempted to keep the semantic meaning fixed.\n* **MovieLens:** Here “Alice” utilities were based on human annotations, and the “Bob” utilities were elicited from LLM personas derived from perturbations of the same base prompt.\n* **Strategic Equilibrium:** Here both Alice and Bob utility functions are given by explicitly defined (and misaligned) tables—no prompt perturbation is involved.\n\nSeveral reviewers raised a concern that our empirical results might be specific to “prompt perturbation” that intuitively mimics zero-bias noise. This criticism is most relevant to the ETHICS experiment, where indeed the Bobs’ utilities are “perturbations” of Alice’s. We note that the criticism applies less well to the MovieLens experiment, in which Alice’s utilities are human-elicited, and even less well to the Strategic Equilibrium experiment, in which no perturbation is involved.\n\nStill, it is a good question, and to demonstrate the robustness of our findings, we have revised the draft to include three new experiments: two that are variants of our existing ETHICS and MovieLens experiments, and one that is based on an entirely different dataset based on annotations by humans (which we use for Alice) from existing off-the-shelf LLMs (which we use for the Bobs).\n\nWe summarize these below:\n\n**1. Semantic personas (App. J.3):**\nWe re-ran our ETHICS and MovieLens experiments using LLM personas for the Bobs corresponding to distinct ethical frameworks (for ETHICS) and genre tastes (for MovieLens), not just prompt noise. The Bob prompts now encode different *goals*, but we demonstrate that their convex hull still approximates a target utility much better than any single persona.\n\nFor example in the ETHICS experiment, an example of a semantically distinct persona is the “utilitarian” defined by the prompt:\n> “You are a utilitarian philosopher. Your sole focus is on the consequences of an action. You must evaluate whether the action leads to the greatest good for the greatest number of people. A good action maximizes overall happiness and well-being. A bad action causes net harm or suffering. Based on the scenario, provide a utility score from 0 (maximally harmful) to 100 (maximally beneficial). Respond with only the integer score.”\n\nWe have 6 different base personas, each semantically distinct from “Alice”. We then scale this in our plot to 100 Bob personas by using perturbations of these 6 base prompts. We do the same thing in the MovieLens experiments by using personas that correspond to fans of different genres of movies. In both cases, the Bob personas are semantically distinct from the Alice personas (and in the MovieLens case, the Alice utility function continues to be derived from human data).\n\n**2. Polling experiment (App. J.4):**\nWe use an existing human-LLM polling dataset containing responses from public opinion surveys [Santurkar et al.]. We treat each human’s opinion vector as $u_A$ and each model’s answer distribution as $U_i$, and fit a non-negative combination of the $U_i$. We find here as well that the non-linear convex combination improves substantially over the best-LLM. Here Alice’s utility is derived from real human responses, and the Bob’s utilities are derived from existing commercial LLMs—there are no artificial personas and no “prompt perturbations”.\n\nIn all cases, we find that our results are consistent with our earlier experiments—there is much better alignment in the non-negative linear span of the Bob utilities than with individual Bobs, showing that our contention that market alignment is substantially easier to satisfy than individual alignment is robust."}}, "id": "fRUV52Wnug", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763646987588, "cdate": 1763646987588, "tmdate": 1763646987588, "mdate": 1763646987588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks whether competition among misaligned AIs can deliver outcomes close to a perfectly aligned assistant. It models a user interacting with multiple agents as a multi-leader Stackelberg game (multi-round Bayesian persuasion) and proves three guarantees under an approximate convex-hull condition on utilities. The authors ran Simulations and synthetic-utility experiments  based on MovieLens, ETHICS setting and the reuslts suggest that with sufficient model diversity, user utility approaches the aligned benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The multi-leader Stackelberg framing in the problem of alignment is novel and the author provided an equilibrium guarantees which is conceptually interesting.\n2. The author did a good job in writing Clearly. And good formalization adds weight. The including of nice tie-in to quantal response and information-substitutes is very good.\n3. The simulations in the paper are extensive and well organized."}, "weaknesses": {"value": "I have some questions about the assumptions and the simulations the authors made in this paper:\n\n1. Pluralistic alignment: the paper centers on a single representative user. In the real world, users have conflicting utilities. A hull that’s “good” for group A may be bad for group B. Without a multi-user treatment (or distributional guarantees), the policy relevance is limited.\n\n2. Misalignment is modeled as noise, it's hard to justify that they are real divergence. Experiment 1 creates ~100 “AI personas” by rephrasing the prompt, then shows a convex-hull combination can approximate the “human” utility. This mostly injects random variation, not strategic or goal-level misalignment. It shows “averaging over small perturbations beats any single noisy agent,” which is unsurprising and not strong evidence for robustness under real incentive conflicts (e.g., sales-maximizing pharma models). I would reconsider my score if the author can make a reasonable argument on this point or making alternative simulation of goal-level misalignment LLMs.\n\n3. I think core assumption of Approximate Weighted Alignment is very strong. The theory needs the user’s utility to lie (approximately) in the convex hull of provider utilities. That implies misalignments diversify and cancel. In practice, providers may share systematic biases (similar data, incentives, or redteaming), so the user’s utility can sit outside the hull. Strategic misalignment may not wash out by averaging in this case.\n\n4. I think my biggest worry about the claims in this paper is that the follower rationality assumption is too demanding. The main results still rely on a user who can parse strategies and best-respond (or commit to stylized bounded-rational rules). Real users are limited, distractible, and manipulable. Even the quantal response model remains idealized and presumes well-formed posteriors. The guarantees may not work under, say realistic cognition limits or pure lazyness which in most of practical cases, little users will compare multiple LLMs."}, "questions": {"value": "Can you include experiments with strategic misalignment (objective functions that explicitly pull users to provider goals), not paraphrase noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hrE4DrOOTG", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_U3Nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_U3Nj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798269653, "cdate": 1761798269653, "tmdate": 1762923862075, "mdate": 1762923862075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem where a user might interact with misaligned AI systems. Through the framework of Bayesian persuasion in a multi-leader Stackelberg game, it shows one overarching finding in three different variants and settings of the game: A user can determine an (approximately) optimal output (with regards to its own preferences) from the outputs of multiple misaligned AIs _as long as_ the users utility function lies in the convex hull of the AIs utility functions. With more AI systems, they argue, this condition will be more likely to be satisfied, and they show some experiments along those lines in a simple stylized setup with two datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- It studies the problem in a fitting formal framework, and is able to derive results with a clear interpretation\n- It is well-written with a great overview of results in Section 1"}, "weaknesses": {"value": "- The authors admit this themselves, and this is a theory paper first and foremost, but I have to reiterate that the experiments are very simplistic. As of now, your experiments have misalignment occur from paraphrasing, which to me resembles more like Gaussian noise-like misalignment. Are there stylized experiments you could run that cover strategic misalignment, resembling one of the two motivational applications you discuss , for example, about companies that are misaligned to drive their profits?"}, "questions": {"value": "In Proposition 1, you discuss a central condition that allows Alice to recover approximately optimal value in your later results, namely, that a perfectly aligned can make Alice learn her best Bayes action. Can you discuss this a bit further, specifically on how realistic it is for this condition to be satisfied? (Remark 2 already shows that it holds in settings where the message space is very rich.) \nBecause in the small sample of Bayesian Persuasion work I've seen, the Stackelberg leader usually forces the follower to randomize. But that is also precisely because the leader is not perfectly aligned with the follower."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sodpkeFS4m", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_JAcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_JAcD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926509466, "cdate": 1761926509466, "tmdate": 1762923861798, "mdate": 1762923861798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Τhe authors examine a setting where a user (human) interacts with multiple AI agents (AI models) with possibly different utilities (misallignment). In this setting there exists an underlying state **y** which both the user and the agents do not fully know but rather posses some information $x_A$ and $x_B$ respectively. \nThe user converses with the models to decide on an action **a**. All the utilities depend on the chosen action **a** of the user and the true underline state **y**. The goal of the user is to maximize their utility $u_A = u_A(y, a)$. \nThe authors model this interaction as a Stalckberg game with **k** multiple leaders (the AI agents) who first commit to some \"conversation rule $C_{B_i}$, $i \\in [k]$ and the follower (the user) who chooses a conversation rule $C_A$ then best-responds by choosing some deterministic decision rule $D_A$ (an action that maximizes their utility). Because $C_A$ and $D_A$ depend on the k rules $C_{B_i}$ the user observed, the authors focus their analysis on the Nash equilibrium $C_B^*$.   \nUnder the assumption that the AI models are interchangeable in the sense that different AI models induce the same joint distribution and that the agent's utility can be written approximately as a weighted sum of the utilities of the AI agents, they show that the user can achieve approximately optimal utility in the Nash equilibrium."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is generally well-motivated \n- The paper includes a deep theoretical analysis with game-theoretic principles. \n- The presentation of the model and assumptions is clear.\n- I really appreciated the experimental section especially since this is a non-trivial setup to do experiments and because the analysis had these two assumptions that at first sight do not seem to hold trivially.  It is interesting to see how indeed for real datasets the convex hull of agents utilities can incur an alignment with the user's utility, as the number of participating agents grows."}, "weaknesses": {"value": "Minors\n1. It is a little bit hard to read the Identical Induced Distribution Condition in the sense that $\\cal{I}$ was defined with three inputs, and here it has one input which is denoted as a tuple (although I understand that it is just $C_B^*$ without the i-th Bob strategy and so on). \n2. Beyond empirical evaluation is there another intuition behind how the weighted alignment condition occurs in real-world settings? Maybe the authors have in mind some literature that they came up with this idea."}, "questions": {"value": "In the communication protocol it is mentioned that Alice observes the conversation rules of the agents (and that she also chooses one). Can you provide examples of what is this quantity in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-  In the communication protocol, we have that Alice observes the rules of the agent's so I am wondering how such a rule looks like in practice.Can you provide some examples of conversation rules ? I can imagine in practice such rules for the AI models (e.g imposed by their companies privacy policy), is that correct? If so, what would be a conversation rule in practice for the user (Alice)?"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YE70C4RUu", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_GXyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_GXyA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957731482, "cdate": 1761957731482, "tmdate": 1762923861494, "mdate": 1762923861494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}