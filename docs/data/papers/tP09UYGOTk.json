{"id": "tP09UYGOTk", "number": 13145, "cdate": 1758214077648, "mdate": 1759897461031, "content": {"title": "Emergent Alignment Via Competition", "abstract": "Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the user’s utility lies approximately within the convex hull of the agents’ utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition; (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria; and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two forms of empirical evidence: First, we perform simulations of the best-AI selection game using best response dynamics, which show that competition among individually misaligned agents reliably improves user utility when the approximate convex hull assumption is satisfied, but does not always when it fails. Second, we show that synthetically generated AI utility functions (produced via perturbations of the same prompt to evaluate instances on a movie recommendation (MovieLens) and ethical judgement (ETHICS) dataset) quickly produce a convex hull that contains a good approximation of a given utility function even when none of the individual LLM utility functions is well aligned.", "tldr": "", "keywords": ["alignment", "bayesian persuasion", "learning agents"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e31bbf68d2492a0fe2d39190299db62468f49fb8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new approach for AI alignment in the presence of multiple AI agents, treating the problem as a Stackelberg game and providing well-thought definitions of equilibria. The proposed approach could have a great positive impact in terms of preventing a human user from being exploited by any one particular AI agent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The modeling and definition using Stackelberg games and utility theory are constructed well. \n- I'm not super familiar with this field, but the definition of what we hope to see in the metrics and the goal of having an utility lower bound is expressed clearly. \n- The key idea of utility being contained within convex hull and its relation to bayes-optimal actions is very interesting."}, "weaknesses": {"value": "- The experimental results are convincing but rather limited in scope.The proposed approaches and game-theoretic formalisms are very interesting; having stronger sets of experiments can make the ideas more convincing. I would be willing to raise my score if this concern is addressed. This is also not my home field, so I am not sure what additional sets of experiments I could suggest to improve the experiment section..\n- The method of constructing \"aligned AIs with noisy approximations\" by using perturbed prompts seems like a rather weak way of producing such agents. Are there any reward-guided or reduced-rationality LLM sampling approaches that could be done in place of this?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zY4S3FV5aP", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_B3Yk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_B3Yk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673533857, "cdate": 1761673533857, "tmdate": 1762923862534, "mdate": 1762923862534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks whether competition among misaligned AIs can deliver outcomes close to a perfectly aligned assistant. It models a user interacting with multiple agents as a multi-leader Stackelberg game (multi-round Bayesian persuasion) and proves three guarantees under an approximate convex-hull condition on utilities. The authors ran Simulations and synthetic-utility experiments  based on MovieLens, ETHICS setting and the reuslts suggest that with sufficient model diversity, user utility approaches the aligned benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The multi-leader Stackelberg framing in the problem of alignment is novel and the author provided an equilibrium guarantees which is conceptually interesting.\n2. The author did a good job in writing Clearly. And good formalization adds weight. The including of nice tie-in to quantal response and information-substitutes is very good.\n3. The simulations in the paper are extensive and well organized."}, "weaknesses": {"value": "I have some questions about the assumptions and the simulations the authors made in this paper:\n\n1. Pluralistic alignment: the paper centers on a single representative user. In the real world, users have conflicting utilities. A hull that’s “good” for group A may be bad for group B. Without a multi-user treatment (or distributional guarantees), the policy relevance is limited.\n\n2. Misalignment is modeled as noise, it's hard to justify that they are real divergence. Experiment 1 creates ~100 “AI personas” by rephrasing the prompt, then shows a convex-hull combination can approximate the “human” utility. This mostly injects random variation, not strategic or goal-level misalignment. It shows “averaging over small perturbations beats any single noisy agent,” which is unsurprising and not strong evidence for robustness under real incentive conflicts (e.g., sales-maximizing pharma models). I would reconsider my score if the author can make a reasonable argument on this point or making alternative simulation of goal-level misalignment LLMs.\n\n3. I think core assumption of Approximate Weighted Alignment is very strong. The theory needs the user’s utility to lie (approximately) in the convex hull of provider utilities. That implies misalignments diversify and cancel. In practice, providers may share systematic biases (similar data, incentives, or redteaming), so the user’s utility can sit outside the hull. Strategic misalignment may not wash out by averaging in this case.\n\n4. I think my biggest worry about the claims in this paper is that the follower rationality assumption is too demanding. The main results still rely on a user who can parse strategies and best-respond (or commit to stylized bounded-rational rules). Real users are limited, distractible, and manipulable. Even the quantal response model remains idealized and presumes well-formed posteriors. The guarantees may not work under, say realistic cognition limits or pure lazyness which in most of practical cases, little users will compare multiple LLMs."}, "questions": {"value": "Can you include experiments with strategic misalignment (objective functions that explicitly pull users to provider goals), not paraphrase noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hrE4DrOOTG", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_U3Nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_U3Nj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798269653, "cdate": 1761798269653, "tmdate": 1762923862075, "mdate": 1762923862075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem where a user might interact with misaligned AI systems. Through the framework of Bayesian persuasion in a multi-leader Stackelberg game, it shows one overarching finding in three different variants and settings of the game: A user can determine an (approximately) optimal output (with regards to its own preferences) from the outputs of multiple misaligned AIs _as long as_ the users utility function lies in the convex hull of the AIs utility functions. With more AI systems, they argue, this condition will be more likely to be satisfied, and they show some experiments along those lines in a simple stylized setup with two datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- It studies the problem in a fitting formal framework, and is able to derive results with a clear interpretation\n- It is well-written with a great overview of results in Section 1"}, "weaknesses": {"value": "- The authors admit this themselves, and this is a theory paper first and foremost, but I have to reiterate that the experiments are very simplistic. As of now, your experiments have misalignment occur from paraphrasing, which to me resembles more like Gaussian noise-like misalignment. Are there stylized experiments you could run that cover strategic misalignment, resembling one of the two motivational applications you discuss , for example, about companies that are misaligned to drive their profits?"}, "questions": {"value": "In Proposition 1, you discuss a central condition that allows Alice to recover approximately optimal value in your later results, namely, that a perfectly aligned can make Alice learn her best Bayes action. Can you discuss this a bit further, specifically on how realistic it is for this condition to be satisfied? (Remark 2 already shows that it holds in settings where the message space is very rich.) \nBecause in the small sample of Bayesian Persuasion work I've seen, the Stackelberg leader usually forces the follower to randomize. But that is also precisely because the leader is not perfectly aligned with the follower."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sodpkeFS4m", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_JAcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_JAcD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926509466, "cdate": 1761926509466, "tmdate": 1762923861798, "mdate": 1762923861798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Τhe authors examine a setting where a user (human) interacts with multiple AI agents (AI models) with possibly different utilities (misallignment). In this setting there exists an underlying state **y** which both the user and the agents do not fully know but rather posses some information $x_A$ and $x_B$ respectively. \nThe user converses with the models to decide on an action **a**. All the utilities depend on the chosen action **a** of the user and the true underline state **y**. The goal of the user is to maximize their utility $u_A = u_A(y, a)$. \nThe authors model this interaction as a Stalckberg game with **k** multiple leaders (the AI agents) who first commit to some \"conversation rule $C_{B_i}$, $i \\in [k]$ and the follower (the user) who chooses a conversation rule $C_A$ then best-responds by choosing some deterministic decision rule $D_A$ (an action that maximizes their utility). Because $C_A$ and $D_A$ depend on the k rules $C_{B_i}$ the user observed, the authors focus their analysis on the Nash equilibrium $C_B^*$.   \nUnder the assumption that the AI models are interchangeable in the sense that different AI models induce the same joint distribution and that the agent's utility can be written approximately as a weighted sum of the utilities of the AI agents, they show that the user can achieve approximately optimal utility in the Nash equilibrium."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is generally well-motivated \n- The paper includes a deep theoretical analysis with game-theoretic principles. \n- The presentation of the model and assumptions is clear.\n- I really appreciated the experimental section especially since this is a non-trivial setup to do experiments and because the analysis had these two assumptions that at first sight do not seem to hold trivially.  It is interesting to see how indeed for real datasets the convex hull of agents utilities can incur an alignment with the user's utility, as the number of participating agents grows."}, "weaknesses": {"value": "Minors\n1. It is a little bit hard to read the Identical Induced Distribution Condition in the sense that $\\cal{I}$ was defined with three inputs, and here it has one input which is denoted as a tuple (although I understand that it is just $C_B^*$ without the i-th Bob strategy and so on). \n2. Beyond empirical evaluation is there another intuition behind how the weighted alignment condition occurs in real-world settings? Maybe the authors have in mind some literature that they came up with this idea."}, "questions": {"value": "In the communication protocol it is mentioned that Alice observes the conversation rules of the agents (and that she also chooses one). Can you provide examples of what is this quantity in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-  In the communication protocol, we have that Alice observes the rules of the agent's so I am wondering how such a rule looks like in practice.Can you provide some examples of conversation rules ? I can imagine in practice such rules for the AI models (e.g imposed by their companies privacy policy), is that correct? If so, what would be a conversation rule in practice for the user (Alice)?"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YE70C4RUu", "forum": "tP09UYGOTk", "replyto": "tP09UYGOTk", "signatures": ["ICLR.cc/2026/Conference/Submission13145/Reviewer_GXyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13145/Reviewer_GXyA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957731482, "cdate": 1761957731482, "tmdate": 1762923861494, "mdate": 1762923861494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}