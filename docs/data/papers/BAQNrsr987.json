{"id": "BAQNrsr987", "number": 11407, "cdate": 1758198378826, "mdate": 1763673142679, "content": {"title": "Tractability via Low Dimensionality: The Parameterized Complexity of Training Quantized Neural Networks", "abstract": "The training of neural networks has been extensively studied from both algorithmic and complexity-theoretic perspectives, yet recent results in this direction almost exclusively concern real-valued networks. In contrast, advances in machine learning practice highlight the benefits of quantization, where network parameters and data are restricted to finite integer domains, yielding significant improvements in speed and energy efficiency. Motivated by this gap, we initiate a systematic complexity-theoretic study of ReLU Neural Network Training in the full quantization mode. We establish strong lower bounds by showing that hardness already arises in the binary setting and under highly restrictive structural assumptions on the architecture, thereby excluding parameterized tractability for natural measures such as depth and width. On the positive side, we identify nontrivial fixed-parameter tractable cases when parameterizing by input dimensionality in combination with width and either output dimensionality or error bound, and further strengthen these results by replacing width with the more general treewidth.", "tldr": "We study the classical and parameterized complexity of training quantized neural networks and obtain new upper as well as lower bounds for the problem.", "keywords": ["treewidth", "parameterized complexity", "quantized neural networks", "ReLU networks"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9691746db57bbea462cc29ae84760e66d86478ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a foundational study of the computational and parameterized complexity of quantized neural network training (QNNT). They prove that training remains NP-hard in general but identify tractable cases when parameterized by input dimension combined with width, error bound, or treewidth. Central to their approach is a structural lemma derived from Steinitz’ Lemma, which limits the number of relevant input connections per neuron, enabling dynamic programming–based fixed-parameter tractable algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is the first rigorous parameterized complexity analysis of quantized neural network training.\n\n- The authors establish NP-hardness and W[2]-hardness for even extremely constrained architectures (binary weights, single hidden layer), demonstrating the robustness of the hardness landscape.\n\n- It bridges machine learning and complexity theory, setting a precedent for similar analyses in quantized deep learning settings."}, "weaknesses": {"value": "- The connection to practical quantized training algorithms such as quantization-aware training (QAT), could be discussed more deeply. The current presentation is fully theoretical."}, "questions": {"value": "- To what extent can the results in this paper be leveraged for actual algorithmic design in quantized training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2lkysOqTUC", "forum": "BAQNrsr987", "replyto": "BAQNrsr987", "signatures": ["ICLR.cc/2026/Conference/Submission11407/Reviewer_D5xX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11407/Reviewer_D5xX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922604144, "cdate": 1761922604144, "tmdate": 1762922524083, "mdate": 1762922524083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies theory for quantized neural networks, i.e., whose weights are stricted to be integers.\nThey show various hardness results for the problem of determining whether there exists weights that achieves performance better than some threshold.\nThey also show tractability in terms of the tree-width of the underlying graph of a neural network architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "They consider feed-forward neural net architectures that is not just the fully-connected nets. rather, they allow \"sparser\" connections.\nFor many of the hardness results, simple architectures (i.e., low depth) already already lead to hard problems.\nDiscovery of tree-width and connection to tractability for QNNs is also interesting."}, "weaknesses": {"value": "There isn't a lot of intuition for why small tree-width neural networks. what do those graphs look like? It'd be useful if there is more small scale examples e.g., like the one from Figure 4, but for small tree-width architectures.\n\nSince small tree-width architectures are tractable, it'd be nice to see some experiments on say the two-moon dataset. this will help answer: are small tree-width neural networks useful?"}, "questions": {"value": "I don't understand the sentence on Line 468-469. How does the tree width becomes replaced with (ordinary) width?\n\nAs ell becomes larger, the problem should become easier. Is this correct? I'm not understanding how ell is used alongside with other parameters like the width which makes the problem harder when the width grows larger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOak3FjNMF", "forum": "BAQNrsr987", "replyto": "BAQNrsr987", "signatures": ["ICLR.cc/2026/Conference/Submission11407/Reviewer_hBha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11407/Reviewer_hBha"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027511207, "cdate": 1762027511207, "tmdate": 1762922523631, "mdate": 1762922523631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the fixed-parameter tractability (FPT) of training quantized neural networks (namely networks with all weights discretized to a certain grid). The basic question is as follows: given a certain neural network architecture (with ReLU activations), discretization level, and dataset, is it feasible to check whether there is a neural network (of that architecture and discretization) achieving low error on that dataset? The fixed-parameter setting is obtained when we \"fix\" certain parameters (such as depth, width, input dimension, output dimension, etc, and combinations thereof) to be bounded by $k$ and ask whether there is at least an algorithm running in time $f(k) n^{O(1)}$ for some function $f$. As there are many ways to parameterize a neural network, there are many variants of this question. This paper makes significant progress towards answering most natural variants. For example, one of the main results is that the problem is indeed FPT with respect to width + input dimension + output dimension. However, it is _not_ FPT with respect to just width + input dimension. Many other combinations are considered in the paper, and it gives a fairly comprehensive set of results detailing this complexity-theoretic landscape. It is worth noting that the landscape for the real-valued version of this problem (i.e., not quantized) is not quite as rich; the real-valued version is known to be complete for the large class $\\exists \\mathbb{R}$, whereas the quantized version necessarily lies in NP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Understanding the complexity of training quantized neural networks is a natural and timely theoretical question. The authors have made a commendable effort to map out the FPT landscape of this problem fairly comprehensively. The techniques in the work may be of technical interest to others in this area, especially the use of Steinitz's Lemma to enable some key simplifications. (However, note that I was not able to verify all the proofs carefully.)"}, "weaknesses": {"value": "My concern with this paper is that the core problem, while natural in a complexity-theoretic sense, is also quite theoretical and stylized. It is subject to all the usual criticisms of worst-case complexity theory, and as such feels unlikely to have much bearing on training quantized neural networks in practice. This is fine --- I do think the contributions are worthwhile from a complexity-theoretic point of view, and I do understand that such topics have a place in the machine learning literature. I just wonder if a better audience for this particular set of results might be found at say COLT or CCC rather than ICLR."}, "questions": {"value": "1. It would be really useful to have a more detailed discussion of what makes this problem different from two closely related ones: (a) training real-valued networks, and (b) Boolean satisfiability problems (esp circuits or CSPs). It feels like this problem (training quantized neural networks) has flavors of both, and it would be good to understand exactly what techniques do and do not carry over, and where novel ideas are really needed.\n2. Probably the most common way in which quantized neural networks are actually obtained is through distillation, i.e. quantizing a higher-precision model. I am curious if the authors have thoughts about this and whether this might make the problem different. For example, consider the problem where the dataset is assumed to be realizable by a network of quantization level poly(d) (or even real-valued), and the question is to check whether there is a network of quantization level d that realizes it. Basically, trying to model the distillation setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4XVOadrHDQ", "forum": "BAQNrsr987", "replyto": "BAQNrsr987", "signatures": ["ICLR.cc/2026/Conference/Submission11407/Reviewer_VvcW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11407/Reviewer_VvcW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762281943939, "cdate": 1762281943939, "tmdate": 1762922523177, "mdate": 1762922523177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}