{"id": "N1v0kQwy92", "number": 8735, "cdate": 1758096517935, "mdate": 1759897766853, "content": {"title": "CaCuTe: Casual Cubic-Model Technique for Faster Optimization", "abstract": "We establish a local $\\mathcal{O}(k^{-2})$ rate for the gradient update $x^{k+1}=x^k-\\nabla f(x^k)/\\sqrt{H\\|\\nabla f(x^k)\\|}$ under a $2H$-Hessian--Lipschitz assumption. Regime detection relies on Hessian--vector products, avoiding Hessian formation or factorization.\nIncorporating this certificate into cubic-regularized Newton (CRN) and an accelerated variant enables per-iterate switching between the cubic and gradient steps while preserving CRN’s global guarantees. The technique achieves the lowest wall-clock time among compared baselines in our experiments.\nIn the first-order setting, the technique yields a monotone, adaptive, parameter-free method that inherits the local $\\mathcal{O}(k^{-2})$ rate. Despite backtracking, the method shows superior wall-clock performance. Additionally, we cover smoothness relaxations beyond classical gradient--Lipschitzness, enabling tighter bounds, including global $\\mathcal{O}(k^{-2})$ rates. \nFinally, we generalize the technique to the stochastic setting.", "tldr": "", "keywords": ["convex optimization", "adaptive method", "Hessian-vector product"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba7166f1d5507185e4c128b3b1ebffacf4c35ac9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considered unconstrained minimization problem $\\min f(x)$ where $f$ is twice-differentiable and the Hession of $f$ is $2H$-Liptchitz. The paper studies first-order methods in the form of \n$$\nx^{k+1} = x^k - \\frac{1}{\\sqrt{H\\|\\nabla f(x^k)\\|}} \\nabla f(x^k).\n$$\nCombining with cubic regularized Newton steps, the casual cubic Newton method and its acclerated version are proposed. With specific pamameter choices, the casual cubic Newton method recovered the results of the cubic regularized Newton methods. Then under assumptions on bounded Hession eigenvalue in the graident direction, the paper poposed the casual cubic adaptive gradient descent method and extend to directional $L_0, L_1$- smoothness cases. Finally, a variant of methods under stochastic regime is studied."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper showed a potential framework to analyze cubid regularized Newton methods and gradient methods under various smoothness regimes."}, "weaknesses": {"value": "While the core idea presented in this manuscript is interesting and has potential, the current presentation significantly limits its impact. The contribution of the work is not clearly articulated, and the results are not framed with the necessary rigor to be fully convincing. As a result, the contribution of the work remains unclear; see questions below regrading writing, paper structure and the correctness."}, "questions": {"value": "- $\\mathbf I$ is not used in the line 34.\n\n\n- In Line 59, '...in the convex' seems complete; 'enhances' --> 'is enhanced'?\n\n- In line 69, 'However  ... can be avoided while keeping ... without... under...' this sentence is strange. \n\n- What are the convergence results of the adaptive gradient methods in the related work? \n\n\n- The sentences and ambiguous words in Section 1.3 are confusing. \n\n    - The first long sentence in Line 89-91 is difficult to understand. I suggest to separate to several sentences. What does the 'corresponds coincides aligns' in the parenthese mean? The terminology 'certificate' is frequently used but I am not sure what method/technique they refer to.  \n\n    - In the AccCaCuN paragraph, what does the 'same argument' refer to?\n\n    - What does 'a method is monotone' stand for?\n\n- In section 3,  $O(k^{-2})$ convergence is claimed for Cases I (cubic-model decrease) and $O(k^{-1})$ convergence is claimed for Case II (quadratic-model decrease). Are these claims existing results? If so, proper references should be listed; otherwise, rigorous statements and proofs should be given. \n\n- Why $L$-smoothness is mentioned in Line 186? Is it related to the smoothness assumption in the previous sentence. \n\n- **There is no clear direction linked the proofs in the appendix to the results (theorem, claims, or examples) in the main paper, making it difficult to verify the correctness of the results.**\n\n\n- Below are questions regarding the results in section 4: \n    \n    - Why the inequality in Line 203 holds? It seems inequality (6) is used but $M_k = (3/4)H$ may not satisfied (5).\n\n    - What is $y^0$ in Line 208?\n\n    - What is the gradient step that yields the inequality Line 208? I suggest to write out the first step explicitly.\n\n    -  Section 4 seems to obtain convergence results by relating Algorithm 1 to the results in [Nesterov & Polyak, 2006]. But I am not sure what the 'original proof' refers to and most importantly why Algorithm 1 achieves global $O(k^{-2})$ convergence. The choice of $M_k$ in (5) switches to $M_k = (3/4)H$ and in Algorithm 1 there is no $M_k$. If Algorithm 1 and the proofs are fully covered by the results in [Nesterov & Polyak, 2006]. What is the point of Section 3 since (5) may not hold? Even though the proof is not new, I suggest to write a formal theorem as this is the first main result. The proof can be postponed to appendix. \n\n\n   - What do the 'same technique' and 'the original converge guarantees' in Section 4.2 refer to? \n   \n\n   - In Algorithm 2, '$M_k \\leq H, i.e. M_K = H$' does not make sense.\n\n- The vanishing directional curvature is used in Section 1 and the title of Section 5.1 but never formally introduced. Is it defined as (10)?\n\n- The proof for Theorem B.4 is not self-contained.\n\n- The condition in Theorem 6.3 is inconsistent for the method and the result. \n\n- Can any global convergence rate be derived using the result of Theorem 6.3? How does it compared to other SGD methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V0a9b4dlBj", "forum": "N1v0kQwy92", "replyto": "N1v0kQwy92", "signatures": ["ICLR.cc/2026/Conference/Submission8735/Reviewer_soLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8735/Reviewer_soLp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917563775, "cdate": 1761917563775, "tmdate": 1762920530173, "mdate": 1762920530173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **CaCuTe**, a family of algorithms that leverage **cubic regularization ideas** while avoiding the heavy cost of full cubic‐model solves.  \nStarting from the **Lipschitz–Hessian (2H–smooth)** assumption used in classical cubic‐regularized Newton (CRN) methods, the authors derive a **Hessian–vector–based condition** that certifies when a **simple gradient step** will achieve the same decrease as a cubic step.  \nThis leads to the **CaCuN algorithm**, which dynamically switches between a cheap gradient step and a standard CRN step depending on the local curvature.  \nThe approach extends to an **accelerated variant (AccCaCuN)** that retains the accelerated \\(O(k^{-3})\\) global rate, as well as a **first-order adaptive method (CaCuAdGD)** that uses Hessian–vector products and backtracking to adapt \\(M_k\\) without explicit Hessians.  \nA **stochastic version (CaCuSGD)** further generalizes the idea to noisy gradients.  \nOverall, the paper provides a unified view of cubic-model techniques that reduce computational cost while maintaining theoretical guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation:** Classical CRN guarantees are strong but expensive; CaCuTe identifies when cheaper first-order steps suffice.  \n- **Theoretical soundness:** The paper rigorously derives one-step decrease bounds, global \\(O(k^{-2})\\) and accelerated \\(O(k^{-3})\\) rates, and expected stochastic analogues.  \n- **Low overhead:** The curvature certificate requires only **one Hessian–vector product**, a modest cost compared to full second-order solves.  \n- **Comprehensive framework:** Covers deterministic, accelerated, adaptive, and stochastic regimes under a single analytic template.  \n- **Practical relevance:** Algorithms can exploit automatic-differentiation HVPs available in modern ML libraries."}, "weaknesses": {"value": "- **Complex exposition:** The notation is dense and the main ideas are sometimes buried in algebraic detail; a higher-level intuition for why the curvature test works would help readers.  \n- **Dependence on convexity:** The theoretical results assume convex objectives; it is unclear how the approach behaves on **non-convex** losses where Hessians may be indefinite.  \n- **Parameter sensitivity:** The constants used in the curvature condition (e.g., the choice of \\(H\\) and \\(\\alpha\\)) could affect practical performance, but this dependence is not discussed.    \n- **Weak CaCuSGD results:** The empirical performance of **CaCuSGD** appears weak compared to standard **SGD**. It would be interesting to see whether adding **momentum or variance reduction** could mitigate stochastic noise.  \n- **Reliance on differing assumptions:** The analysis invokes different assumptions (e.g., Eq. (10), Eq. (14)), making it difficult to compare results across variants. A **summary table** of assumptions and corresponding guarantees would greatly clarify the contributions.  \n- **Quantification missing:** The paper does not quantify **how often the full cubic Newton step is avoided** in practice. Reporting this ratio would concretely demonstrate the computational advantage claimed."}, "questions": {"value": "1. Could the same idea extend to **non-convex** settings?  \n2. In practice, how often does CaCuN actually switch to the cubic step versus taking the cheaper gradient step?  \n3. What is the **computational overhead** of the backtracking procedure in CaCuAdGD relative to standard gradient descent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KnzlHn7Brm", "forum": "N1v0kQwy92", "replyto": "N1v0kQwy92", "signatures": ["ICLR.cc/2026/Conference/Submission8735/Reviewer_w1tX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8735/Reviewer_w1tX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941403486, "cdate": 1761941403486, "tmdate": 1762920529777, "mdate": 1762920529777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThis paper establishes the convergence rate of a special stepsize rule for gradient descent under different settings. The stepsize rule is motivated by the optimality condition of the subproblem arising from the cubic regularized Newton's method. Some experiments validate the efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Strength**\n\nThe paper is overall easy to follow."}, "weaknesses": {"value": "**Weaknesses**\n\nI have several concerns regarding both the theoretical and practical aspects of the paper.\n\n1. Theoretical complexity\n\n   In terms of the theoretical complexity, while the paper claims $O(1/K^2)$ result is achievable using first-order information, the analysis ends up relying on strong assumptions such as equation (10). In general, the first-order methods developed in the paper only guarantee an $O(1/K)$ convergence rate. I find the result relatively weak since it uses extra operations such as the Hessian vector product. Besides, some results (**Corollary 5.7**) in the paper contain strong assumptions on the trajectory, which I find unacceptable; some results (**Theorem 6.3**) are incomplete and do not give a convergence rate.\n\n2. Practical implementation \n\n   The proposed algorithm is claimed to be parameter-free. However, in most cases, it still requires knowledge of the Hessian Lipschitz constant $H$. Even the adaptive variant still requires a line-search procedure and introduces an additional hyperparameter $\\alpha$. Given that the algorithm essentially adopts the gradient direction, I don't think the algorithm design is justified.\n\n3. Weak experiment evaluation\n\n   The experiments in **Section 7**mseem cherry-picked. The results from the appendix suggest the proposed stepsize typically underperforms standard accelerated gradient descent.\n\nFinally, I noticed a number of typos and notation inconsistencies throughout the paper. Overall, I don't think the paper can be published at ICLR in its current form."}, "questions": {"value": "**Questions**\n\n1. I don't feel I understand what \"casual\" in the title means. Does it come from the motivation on line 41?\n2. Except for (AccCaCuN), the algorithms developed in the paper search along the gradient direction. Is there any intuition that the resulting algorithm can outperform vanilla gradient descent?\n\n**Minor issues**\n\n1. Line 11\n\n   You mention \"local\" rate here, but the contributions claim global convergence.\n\n2. Line 93\n\n   \"i.e. logistic regression\" is unclear.\n\n3. Line 130\n\n   \"is convex with, i.e.\" is unclear.\n\n4. Line 132\n\n   I don't think continuity is necessary here.\n\n5. Line 160\n\n   \"Inequality (5) employs...\" is unclear.\n\n6. Line 186, 254, 284\n\n   Gradient smoothness corresponds to Lipschitz Hessian; Hessian $\\nabla^2 f$  corresponds to the Lipschitzness of the third-order derivative.\n\n7. Line 356\n\n   The assumption is not correct"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WrYiRfoALP", "forum": "N1v0kQwy92", "replyto": "N1v0kQwy92", "signatures": ["ICLR.cc/2026/Conference/Submission8735/Reviewer_LXQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8735/Reviewer_LXQv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980521695, "cdate": 1761980521695, "tmdate": 1762920529417, "mdate": 1762920529417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a ‘casual cubic Newton’ framework, where the algorithm decides whether to use second-order updates or cheaper normalized gradient updates based on a Hessian-vector-product (HVP) certificate, which allows algorithms inspired by the framework (CaCuN and AccCaCun) to enjoy similar convergence rates with cubic-regularized Newton, yet without forming or factorizing Hessians as often. Moreover, under a specific assumption, it is possible to construct an entirely HVP-based algorithm (CaCuAdGD), where the algorithm adapts to the local geometry via the HVP in the ‘certificate inequality’ and the backtracking of the constant $H$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and easy to read overall.\n- The paper includes numerical experiments that support the theoretical results."}, "weaknesses": {"value": "See **Questions.**"}, "questions": {"value": "- For CaCuN (and AccCaCuN) in Section 4, is it possible to quantify how much CRN steps we might expect to need? I think this should be a bit clearer that the algorithm will stay at the regime (with constants simplified to $c'$),\n$$\nf \\left( y^k - \\frac{\\nabla f(y^k)}{\\sqrt{c' \\cdot H \\| \\nabla f(y^k) \\|}} \\right) \\le f(y^k) - \\frac{1}{c' \\cdot \\sqrt{H}} \\| \\nabla f(y^k) \\|^{3/2}\n$$\nat least quite often to conclude that using CRN steps only outside this regime will really be computationally beneficial than vanilla methods and/or comparable with other efficient second-order methods like lazy Hessians (Doikov et al., 2023). Is it possible for the authors to quantify the Hessian (and gradient) oracle costs, at least on a high-level? Or could there be some specific cases where we can clearly expect fewer CRN steps?\n    - In particular, when taking iterations other than CRN steps (i.e., excluding the Hessian computation steps), my intuition is that methods like lazy Hessians, which both preserves the Hessian structure itself but uses matrix-vector products for most steps, might be better than using a first-order method based on a upper-bound-ish scalar $H$, which is typically a bit more conservative. I didn’t check this in detail, could the authors elaborate on this?\n    - I might be wrong, but isn’t this ‘HVP-based certificate’ is actually fully first-order computable if we take $H$ as constants, not matrices (which could actually be slightly better than Hessian-vector products)? This is a minor thing, but the term is a bit confusing to me as I can only see HVPs in Algorithm 3 (CaCuAdGD).\n- For CaCuAdGD in Section 5, while it is nice that we can enjoy convergence only with first-order oracles and Hessian-vector products, it feels like that the assumption in $(10)$ is essentially just tailored to make the above regime to be true. As there are a few examples (cubic and logistic functions) mentioned that fall into this category, is there a better reason to consider this assumption other than merely a necessary condition for similar ideas as in the previous section to work? (This could also possibly be related to the first question, as we might be able to assert that CaCuN type algorithms will require very few Hessian oracles for the cubic and logistic functions.)\n\nDoikov et al., 2023. Second-order optimization with lazy Hessians."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XmaMyVMTFC", "forum": "N1v0kQwy92", "replyto": "N1v0kQwy92", "signatures": ["ICLR.cc/2026/Conference/Submission8735/Reviewer_Esyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8735/Reviewer_Esyd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984782419, "cdate": 1761984782419, "tmdate": 1762920528880, "mdate": 1762920528880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}