{"id": "6eSNG1VNkl", "number": 33, "cdate": 1756728171839, "mdate": 1759898278049, "content": {"title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks", "abstract": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes.", "tldr": "", "keywords": ["jailbreak", "attack", "multi-turn", "reinforcement learning", "large language model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/689aa1dbf5ca139920b52f3c93fd1376cf21b832.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a framework for training multi-turn jailbreak attackers that addresses exploration complexity and intent drift without relying on predefined strategies, external datasets, or victim feedback. The method uses two stages: (1) prefilling self-tuning to fine-tune the attacker on self-generated, non-refusal multi-turn prompts, and (2) reinforcement learning with intent-drift-aware rewards using GRPO and a composite reward function to maintain harmful objectives while exploring diverse strategies. By adopting an open-loop, response-agnostic approach, SEMA decouples prompt planning from victim responses, reducing computational costs. The framework achieves state-of-the-art results (80.1% ASR on AdvBench, 33.9% above prior best), outperforms all existing baselines, and demonstrates strong transferability across models and datasets, providing a realistic, scalable, and reproducible stress test for LLM safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces an intent-drift-aware reward function that prevents conversation drift—the key failure mode of multi-turn attacks—enabling attacks to maintain harmful intent across 5–7 turns while appearing benign.\n\nIt employs online reinforcement learning to automatically discover diverse multi-turn jailbreak strategies without any predefined templates or external attack datasets."}, "weaknesses": {"value": "Though the paper shows generalization of the attacker on smaller models, it provides limited evaluation on frontier models such as GPT-4o/5, Claude 3.5/4 (Sonnet/Opus), and Gemini 1.5/2.0 Pro. Additionally, small open-source models do not undergo extensive safety training and are relatively easy to jailbreak. Demonstrating whether the method generalizes to frontier, highly safety-tuned models would better showcase its effectiveness.\n\nAlthough the authors compare their method with other multi-turn attacks like Crescendo, GOAT, FITD, etc., they omit comparison or discussion with more recent state-of-the-art multi-turn methods such as X-Teaming (https://arxiv.org/abs/2504.13203) and ActorAttack (https://arxiv.org/abs/2410.10700\n), which demonstrate the effectiveness of open-source attacker models in jailbreaking nearly all frontier models.\n\nIt primarily focuses on the attack side and does not explore the defense side."}, "questions": {"value": "1. As the authors use GPT-4.1-mini as their evaluation model for reward computation, does the evaluator maintain consistent evaluation performance as training progresses, or is any reward hacking or exploitation pattern observed?\n\n2. Why does performance degrade beyond 7 turns (Figure 3)?\n\n3. Is there any quantitative analysis and comparison of attack diversity?\n\n4. What responsible disclosure and access control practices will be implemented to prevent the malicious use of SEMA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gP7heVBpf3", "forum": "6eSNG1VNkl", "replyto": "6eSNG1VNkl", "signatures": ["ICLR.cc/2026/Conference/Submission33/Reviewer_Y3NC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission33/Reviewer_Y3NC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission33/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808412857, "cdate": 1761808412857, "tmdate": 1762915440611, "mdate": 1762915440611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for training multi-turn jailbreak attackers via reinforcement learning. The approach has two stages: 1) prefilling self-tuning to generate parseable, non-refusal multi-turn attack sequences, basically it's a finetuning to get an attacker model to generate paraphrases or different ways of asking for something 'bad. \n\nthis is followed by 2) GRPO training with an intent-drift-aware reward, which makes sure the conversation content doesnt change during multi-turn. \n\nBasically their method decouples multi-turn from any independence on prior turns or the victim, reducing complexity and fanning degree when doing the 'search' over prompt space. The method operates in an open-loop manner, generating complete multi-turn attack plans without conditioning on victim responses. Experiments across AdvBench and HarmBench show high attack success rates against multiple victim models (Qwen2.5-3B, Llama-3.1-8B, GPT-4.1-mini, and GPT-oss-20B), outperforming single-turn and template-driven multi-turn baselines across multiple judges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong transferability: The method demonstrates high transfer rates across different victim models, suggesting the learned attacks capture generalizable vulnerabilities rather than model-specific artifacts.\n\nSimplified threat model: The open-loop generation approach reduces computational requirements by avoiding the need for iterative victim interaction during attack generation. This also removes dependencies on predefined strategy templates or branching assumptions that constrain template-driven methods.\n\nIndependent prompt generation: The finding that response-agnostic, independently generated prompts can achieve effective multi-turn jailbreaks is useful for understanding attack mechanics and may inform future defense strategies."}, "weaknesses": {"value": "*Missing cost analysis*: Despite frequent mentions of reduced cost as a key advantage, the paper lacks quantitative analysis of computational requirements. Specifically:\n\n1. How many prompts need to be generated on average during training and inference?\n2. What are the API costs for the evaluation model (GPT-4.1-mini) during training?\n3. How does the total cost compare to baseline methods like Crescendo or GOAT?\n4. What is the cost breakdown between prefilling self-tuning and RL stages?\n\n\n*Incomplete ablation studies*: Several design choices lack sufficient justification:\n\nThe contribution of multi-turn structure versus the prefilling optimization is not isolated\nThe impact of prompt ordering is not analyzed\nThe relative importance of turn position versus prompt content is unclear\n\n\n*No discussion of defenses or mitigations*: The paper focuses entirely on the attack side without discussing potential countermeasures, detection methods, or mitigation strategies. This limits the utility for practitioners trying to defend against such attacks."}, "questions": {"value": "**Turn and prompt statistics**: Do you have statistics on how many prompts/turns are needed on average to achieve successful jailbreaks? Are there patterns where certain types of harmful intents require more or fewer turns? Any clustering analysis on this?\nCommon attack patterns: Did you observe common themes or strategies across prompts that successfully jailbreak models? This could provide insights into failure modes.\n\n**Order sensitivity**: If you shuffle the order of the independently generated prompts for a given intent, does it significantly change the outcome? What is the standard deviation or error over different orderings?\nTurn position vs. content: Is the turn at which a prompt appears more important than the prompt itself? For instance, if prompt 7 achieves a jailbreak, would the same prompt work at turn 3?\n\n**Component contributions**: How much of the performance gain comes from the multi-turn structure versus the prefilling self-tuning optimization? Can you provide an ablation that isolates these factors?\n\n**Computational costs**: Can you provide a detailed cost analysis including:\n\nNumber of API calls during training\nTotal token costs for evaluation model\nTraining time comparisons with baselines\nInference cost per attack generation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V8cgYo4Fhd", "forum": "6eSNG1VNkl", "replyto": "6eSNG1VNkl", "signatures": ["ICLR.cc/2026/Conference/Submission33/Reviewer_aujp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission33/Reviewer_aujp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission33/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958526451, "cdate": 1761958526451, "tmdate": 1762915440316, "mdate": 1762915440316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SEMA is a compact, reproducible framework for training open-loop, response-agnostic multi-turn jailbreak attackers that avoids hand-authored scripts, templates, or external corpora by combining a prefilling self-tuning stage (to produce non-refusal, well-formatted multi-turn rollouts) with reinforcement learning using an intent-drift-aware reward (which balances intent alignment, compliance risk, and level of detail) so the attacker preserves harmful intent across turns while exploring broadly; this approach reduces exploration complexity via one-shot multi-turn prompt generation, achieves state-of-the-art attack success rates and strong transferability on AdvBench and HarmBench (≈80% ASR on average, large gains over prior single- and multi-turn baselines), scales with attempt budget, and is offered as an automated red-teaming tool intended to surface vulnerabilities and improve LLM safety under responsible use guidelines"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes a decent multi-turn jailbreak framework that achieves higher ASR compared to reported single-turn and multi-turn jailbreak methods.\n\n- The evaluation is relatively thorough, testing many open and closed models across two solid benchmarks.\n\n- Results show that SEMA achieves higher ASR compared to compared single-turn and multi-turn methods.\n\n- The visual presentations of this paper are effective for conveying the mechanism of the framework as well as delivering core takeaways of the results."}, "weaknesses": {"value": "- The paper’s scope is limited by its exclusive focus on developing attackers without accompanying defensive methods. While SEMA advances the study of multi-turn jailbreaks, it offers no systematic exploration of countermeasures or co-evolving defenses. As a result, the work demonstrates how to break safety mechanisms effectively but provides little insight into how to strengthen or adapt them, narrowing its overall contribution to LLM safety research.\n\n- This works claims to achieve SOTA attacker performance but it lacks comparisons to more recent/performant advances in multi-turn jailbreaks, e.g., https://arxiv.org/abs/2504.13203 and https://arxiv.org/abs/2410.10700 which are shown to be substantially better than Crescendo, CoA, and FITD, the baselines included in this paper.\n\n- The method largely builds on GRPO with modified reward components, which limits its degree of methodological novelty."}, "questions": {"value": "In addition to the weakness:\n\n- To serve realistic red-teaming needs for broadly revealing LLM vulnerability, it's crucial that an automatic jailbreak or red-team method to be able to discover a wide range of successful attacks. Is SEMA capable of identifying multiple diverse attacks given the same seed harmful query? Could you quantify such ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B1Aaw1tJYT", "forum": "6eSNG1VNkl", "replyto": "6eSNG1VNkl", "signatures": ["ICLR.cc/2026/Conference/Submission33/Reviewer_9KdW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission33/Reviewer_9KdW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission33/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985948113, "cdate": 1761985948113, "tmdate": 1762915440119, "mdate": 1762915440119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SEMA, which is a simple yet effective framework for multi-turn jailbreaks. It uses prefilling self-tuning to produce non-refusal, well-structured rollouts and an intent-drift-aware reward to keep the harmful objective anchored across turns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clearly written and provides a well-defined explanation of the proposed approach.\n2. The proposed intent-drift-aware reward and GRPO-based jailbreaking method is simple yet effective and novel.\n3. The experiments are comprehensive, and the experiments involving various baselines and models sufficiently demonstrate the superiority of the proposed methodology."}, "weaknesses": {"value": "Major\n1. The open-loop assumption side-steps the real feedback dynamics where victim replies steer the attacker (including deflections). While this is computationally attractive, it may overestimate transferability to real attackers who adapt turn-by-turn. A head-to-head closed-loop variant of SEMA (same reward and intent anchor, but conditioned on last victim response) would clarify the realism/efficiency trade-off.\n\n2. The intent-drift-aware reward is central but depends on an evaluation model (GPT-4.1-mini) and prompt design. The paper would benefit from: (i) prompt release, (ii) cross-evaluator robustness (swap the evaluator LLM family/size).\n\n3. Claims of efficiency (vs. interactive templates) are not quantified. Please report absolute compute for training (SFT+RL) and per-attempt inference costs vs. baselines.\n\nMinor:\nIn Table 3, HarmBench's “No Refusal” performance seems better for Crescendo than for SEMA, but the bold highlighting appears reversed.\n\nTypos:\nL66 : prefilling self turning - > prefilling self tuning,\nL938 : Qwen2.5-3B-Intrust - > Qwen2.5-3B-Instruct\nL1322 C.3 MORE ABALATION STUDIES -> MORE ABLATION STUDIES"}, "questions": {"value": "1. What happens if only rewards are used with basic RL methods like PPO? While experiments were conducted on DPO/SFT, I would like to see the potential for combining the proposed reward with methods other than GRPO.\n\n2. Why does performance decrease when T_max increases from 7 to 10 in Fig.3(right) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VA0SC51iqq", "forum": "6eSNG1VNkl", "replyto": "6eSNG1VNkl", "signatures": ["ICLR.cc/2026/Conference/Submission33/Reviewer_eAud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission33/Reviewer_eAud"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission33/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990095828, "cdate": 1761990095828, "tmdate": 1762915439935, "mdate": 1762915439935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}