{"id": "cbDEAfoilt", "number": 6836, "cdate": 1757997378914, "mdate": 1763289182004, "content": {"title": "Vector Quantization using Gaussian Variational Autoencoder", "abstract": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed __Gaussian Quant (GQ)__, that converts a Gaussian VAE into a VQ-VAE without any additional training. GQ generates random Gaussian noise as a codebook and finds the closest noise vector to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAE, such as VQ, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in supplementary materials.", "tldr": "Training-free converion of a Gaussian VAE into VQ-VAE can be achieved with provably similar rate distortion performance.", "keywords": ["Vector Quantization", "VQ-VAE"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8eb98cceabb8a61b75588831aad1bff46d10a1a8.pdf", "supplementary_material": "/attachment/4a45deca4c8b359547702cdea9fe7254024c0437.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a latent discretization method that converts a (continuous latent) Gaussian VAE into a VQ-VAE without requiring additional codebook training. The authors introduce a simple codebook generation and code selection mechanism, supported by theoretical analysis regarding the appropriate codebook size. Additionally, they propose an adaptive KL regularization for VAE training, which encourages the Gaussian VAE to have a latent capacity closer to a pre-defined bits-back coding bitrate."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The theorems provide valuable insights for estimating the appropriate codebook size.\n- The introduction of the Target Divergence Constraint is novel and interesting.\n- The reconstruction performance improvement of GQ over existing methods is impressive."}, "weaknesses": {"value": "**Motivation for conversion is not clearly presented**\n- The motivation for the proposed conversion is not sufficiently clear. Since the training framework begins with a pre-trained Gaussian VAE, the benefits of using the resulting VQ-VAE over the original Gaussian VAE should be discussed in greater detail.\n\n**Some aspects of the methodology are unclear**\n- It is not clear whether the codebook depends on $i$ throughout the manuscript.\n- The timing of codebook $c_{1:K}$ sampling is ambiguous. Providing pseudo-code or an algorithmic description would help clarify this point.\n- Section 3.4 does not clearly explain the necessity of grouping. Even after multiple readings, the rationale remains unclear. If $\\log_2 K = 0$ for some $i$, wouldn’t preparing a single code for that dimension suffice?\n\n**Comparison with TokenBridge**\n- TokenBridge should be described in more detail, as it is the most relevant related work.\n- The proposed method should be compared with TokenBridge more thoroughly, not just in terms of generation performance. Section 3.5 is not accessible to readers unfamiliar with TokenBridge.\n- What are the key factors that contribute to the improved performance of the proposed method compared to TokenBridge?\n\n**Experimental verification**\n- The experiments are somewhat limited regarding generation performance, which is only reported in Table 7. In particular, a direct comparison of the VQ-VAE learned by the proposed method with the original Gaussian VAE in terms of generation performance would be valuable.\n- The performance of LFQ is significantly worse than the other baselines. It is unclear whether the experiments are fair. Were the main experiments conducted using existing codebases, or did you implement the code yourselves?\n\n**Minor weaknesses**\n- Equation (1) is not a standard formulation, as it does not include commitment and codebook losses.\n- $\\sigma_j$ in Theorem 1 should be $\\sigma_i$?\n- The reference for TokenBridge is incorrect in line 219."}, "questions": {"value": "- From Theorems 1 and 2, I understand that $\\log K$ is expected to be larger than the KL divergence, and that a larger $\\log K$ is beneficial for improving generation performance. However, line 154 states that $\\log K$ should be close to the KL divergence. Is there any drawback to simply increasing the codebook size?\n- According to the manuscript, the codebook is generated by random sampling from a Gaussian distribution, which seems to make the results highly dependent on the random seed. Is this correct?\n- While Gaussian VAEs typically suffer from posterior–prior mismatch, the codebook is sampled from the prior (normal) distribution. Do you have any insights on whether sampling the codebook from this distribution is sufficiently effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Potential ethic concerns are discussed in a section."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8VDC0HKpqS", "forum": "cbDEAfoilt", "replyto": "cbDEAfoilt", "signatures": ["ICLR.cc/2026/Conference/Submission6836/Reviewer_MSjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6836/Reviewer_MSjv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792257307, "cdate": 1760792257307, "tmdate": 1762919099168, "mdate": 1762919099168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for obtaining a VQ-VAE from a Gaussian VAE.  \nThe proposed Gaussian Quantization (GQ) method generates a random Gaussian noise codebook and utilizes it to quantize the posterior mean of the Gaussian VAE encoder.  \nThe authors present theoretical analyses that clarify the relationship between the bits-back coding rate of Gaussian VAEs and the required codebook size for achieving small quantization error.\nBased on these analyses, they also propose the Target Divergence Constraint (TDC) to train Gaussian VAEs suitable for GQ, as well as grouping techniques that facilitate GQ by organizing the dimensions of the latent variables.  \nExperimental results demonstrate that the proposed framework outperforms baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well structured and is easy to follow.\n2. The idea of discussing the relationship between quantization error and bits-back encoding is interesting.\n3. The effectiveness of the proposed method is consistently demonstrated in the experiments."}, "weaknesses": {"value": "1. The method for determining the $\\lambda$ parameters in TDC seems questionable. According to Eq. (6) and (7), when there is no dimension to which $\\lambda_\\mathrm{min}$ and $\\lambda_\\mathrm{max}$ are applied, $\\lambda_\\mathrm{min}$ and $\\lambda_\\mathrm{max}$ are multiplied by 0.99 at each step. Is there a risk that these parameters become extremely small when needed again?\n2. Further explanation is needed for why Eq. (8) encourages greater codebook usage and entropy, and why Eq. (8) takes its specific form. For example, if the first term were a squared L2 norm, it would resemble the form of $\\log p(c_j|x)$ for a Gaussian, but this is not the case. While some details seem to be described in Appendix C.2, a clearer explanation in the main text would be helpful.\n3. The connection between IsoKL, MIRACLE, and the proposed method at line 246 is not sufficiently clear. Although there is some explanation around line 401, a more explicit discussion and guidance earlier in the text would improve readability.\n4. Although Theorems 1 and 2 provide conditions for large and small quantization errors, the quantization error is measured in latent space and distortion caused by quantization in data space is not discussed. When the Lipschitz constant of decoder is large, one concern is that small quantization errors can be magnified to large distortions in data space.\n\nOther minor comments:\n* The definition of distortion in Eq. (1) is unclear, especially regarding the commitment loss [Van Den Oord et al., 2017]. If the commitment loss is intentionally excluded, it should be explicitly stated. If the commitment loss is included in the distortion term, then $\\Delta$ should be expressed as a function of $z$ as well as $X$ and $g(z)$.\n* In the conditions of Theorem 1, $\\mu_i \\sigma_j \\leq c_1$ and $|\\mu_i| + |\\sigma_j| \\leq c_2$, both $i$ and $j$ are used. Isn't this a typo? Please clarify whether this is a typo or if different variables are intended.\n\n[Van Den Oord et al., 2017] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017."}, "questions": {"value": "1. Could you give an answer to the question in Weakness 1? Or is there any ablation study regarding this algorithm design choice of the adaptive weights $\\lambda_i ~ (i=1,\\dots,d)$?\n2. Could you give further explanations regarding Weaknesses 2 and 3?\n3. Regarding Weakenss 4: Have you considered evaluating the distortion caused by quantization directly in the data space, rather than only in the latent space? If not, could you comment on the potential concern or provide insights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zdYSHB9LJ8", "forum": "cbDEAfoilt", "replyto": "cbDEAfoilt", "signatures": ["ICLR.cc/2026/Conference/Submission6836/Reviewer_ym9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6836/Reviewer_ym9y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836792102, "cdate": 1761836792102, "tmdate": 1762919098639, "mdate": 1762919098639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Gaussian Quant (GQ), a method that converts a Gaussian VAE into a VQ-VAE without additional training. GQ achieves training-free conversion by using noise sampled from a Gaussian distribution as the codebook and selecting the codebook entry closest to the posterior mean of the Gaussian VAE. Theoretically, the authors prove that when the logarithm of the codebook size exceeds the bits-back coding rate, the quantization error becomes small. Furthermore, the paper proposes Target Divergence Constraint (TDC) as a technique to train Gaussian VAEs for effective GQ conversion, demonstrating that this is a simple yet powerful quantization method by matching the KL divergence of each dimension to the bits-back coding rate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- While the idea of converting a Gaussian VAE trained in continuous space into a VQ-VAE is simple, the paper clearly motivates the story by first showing that direct conversion from a vanilla Gaussian VAE does not perform well, and then demonstrating that TDC enables training of Gaussian VAEs suitable for quantization.\n\n- The paper establishes a theoretical relationship between codebook size and bits-back coding rate. By proving theorems showing that quantization error decreases doubly exponentially when $\\log K$ matches or exceeds the bits-back coding rate (Eq. 4), and increases exponentially when it falls below (Eq. 5), the work provides principled guidelines for \"how to choose $K$\".\n\n- The paper compares against existing discrete tokenizers (VQGAN, FSQ, LFQ, BSQ) on ImageNet at identical bpp settings, demonstrating superior performance across all metrics: PSNR, LPIPS, SSIM, and rFID. Moreover, the effectiveness is confirmed on both UNet and ViT architectures, demonstrating generality that does not depend on specific backbones."}, "weaknesses": {"value": "- The core claim of the method is that \"a pre-trained Gaussian VAE can be converted to VQ without additional training,\" but in practice, applying GQ to a Gaussian VAE trained without TDC constraint results in significant performance degradation (PSNR: 26.43 dB vs 32.11 dB in Table 6). In other words, \"GQ is training-free\" only applies to the conversion step itself, and there is a prerequisite of \"training a Gaussian VAE with TDC\" in the preceding stage. While the paper mentions conversion from a \"pre-trained Gaussian VAE\" and indicates that pre-training of the Gaussian VAE is necessary, the critical importance of the TDC constraint should have been emphasized more strongly.\n\n- From the perspective of experimental scope diversity, there are the following limitations:\n    - Image generation task evaluation (gFID, IS) is limited to ImageNet, with only reconstruction performance evaluated on COCO. Validation of generation performance on more diverse datasets is desired.\n    - The evaluation is mainly conducted in the 0.22-1.00 bpp range, as acknowledged in the paper: \"We limit our evaluation... to 0.22–1.00 bpp, which extends the BSQ's original range of 0.25–0.50 bpp.\" Performance in lower bpp regions ($\\leq 0.2$) and at higher resolutions remains as future work.\n\n- TDC depends on multiple hyperparameters ($\\lambda\\_{\\min}$, $\\lambda\\_{\\text{mean}}$, $\\lambda\\_{\\max}$) and dynamic update heuristics (update rate of 1.01, threshold of ±0.5 bits as shown in Eq. 7), but there is insufficient sensitivity analysis or theoretical justification for these parameter choices. In particular, robustness across different architectures and datasets has not been sufficiently validated, which remains an important challenge for practical deployment."}, "questions": {"value": "- What is the rationale for choosing the TDC hyperparameters (especially the ±0.5 bits threshold and the 1.01 update rate)? Is there any sensitivity analysis or performance comparison with different settings for these values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aM1Ew7mqO9", "forum": "cbDEAfoilt", "replyto": "cbDEAfoilt", "signatures": ["ICLR.cc/2026/Conference/Submission6836/Reviewer_uBPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6836/Reviewer_uBPL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892360423, "cdate": 1761892360423, "tmdate": 1762919097543, "mdate": 1762919097543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Gaussian Quant (GQ), a training-free procedure that converts a pretrained Gaussian VAE into a VQ-VAE. GQ generates random Gaussian noise as a codebook and quantizes each dimension of the posterior mean to the closest codeword. The authors prove that when the codebook capacity exceeds the VAE's bits-back rate, the induced quantization error is small. They further introduce a Target Divergence Constraint (TDC) to train VAEs that convert more effectively. Experimental results demonstrate that GQ outperforms previous VQ-VAE variants and Gaussian VAE discretization methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is interesting and theoretically grounded.\n\n- Practical method TDC is a lightweight tweak that improves the downstream discretization quality. \n\n- Empirical results and ablations are reported across architectures."}, "weaknesses": {"value": "- The paper discusses “grouping to multiple dimensions” for very low‑bitrate regimes, but it’s unclear whether grouping is evaluated as a separate ablation in the main experiments. Clarifying this would help.\n\n- As GQ quantizes each dimension of the posterior mean to the closest codeword, this results in much longer discrete token sequences compared to VQ-VAE variants that quantize sub-vectors, making the generation step more challenging and computationally expensive."}, "questions": {"value": "I would like to clarify: \n- Do the pretrained baselines in Table 2 use different architectures? And in Table 1, do all baselines and the proposed GQ method use the same architecture?\n- Does GQ generate a single set of K codewords from a unit Gaussian N(0,1) and use it for all posterior-mean dimensions? It would be interesting to see some geometric analysis of the quantized latents, i.e., t-SNE visualizations of some classes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zl9ULbR0po", "forum": "cbDEAfoilt", "replyto": "cbDEAfoilt", "signatures": ["ICLR.cc/2026/Conference/Submission6836/Reviewer_TXyD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6836/Reviewer_TXyD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952796468, "cdate": 1761952796468, "tmdate": 1762919097271, "mdate": 1762919097271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revisions"}, "comment": {"value": "Thank you for your detailed review. We have uploaded the revised paper, with all the revisions marked in blue. Below is a summary of revisions:\n* We emphasis that our Gaussian VAE requires specific constraint, as suggested by uBPL.\n* We clarify the definition of VQ-VAE loss in Eq.1, and GQ codebook is dimension independent as suggested by ym9y and MSjv.\n* We include ablation study on TDC parameters in Table 12, as suggesed by uBPL and ym9y.\n* We clarify grouping related questions, as suggested by ym9y and MSjv.\n* We explain IsoKL, MIRACLE and TokenBridge in main text, as suggested by ym9y and MSjv.\n* We include model parameter in Table 2, TokenBridge generation in Table 4, and low bpp result in Table 11 as suggested by uBPL, TXyD and MSjv.\n* We include results and discussion on the quantization loss in pixel space in Table 16, as suggested by ym9y.\n* We include result of codebook with different seed in Table 17, result of simply increase codebook size in Table 18, and t-SNE visualization in Figure 4 as suggested by MSjv and TXyD.\n* We fix the typos as suggested by ym9y and MSjv."}}, "id": "gZxPcIVIPb", "forum": "cbDEAfoilt", "replyto": "cbDEAfoilt", "signatures": ["ICLR.cc/2026/Conference/Submission6836/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6836/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission6836/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763289066940, "cdate": 1763289066940, "tmdate": 1763289066940, "mdate": 1763289066940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}