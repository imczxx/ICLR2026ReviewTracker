{"id": "Rh72R0VXPS", "number": 11684, "cdate": 1758203063153, "mdate": 1759897561290, "content": {"title": "From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning", "abstract": "The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science.With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation.Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20–35\\% gains over domain-specific baselines and outperforms general-purpose LLMs by 10–15\\% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.", "tldr": "", "keywords": ["Multi-Agent System，Large Language Model，Evidence-Based Reasoning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75ec53f2fe1d414f5352c0c1de913b1dc110ea1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ChemMAS, a multi-agent system that reframes chemical reaction condition recommendation from \"predicting what\" to \"evidence-based reasoning why.\" The system uses a \"General Chemist\" agent to analyze reactions, recalls a large pool of candidates, and then uses a \"tournament-style debate\" among specialized agents to select the Top-50 recommendations, providing a detailed rationale for each. ChemMAS was trained on a large, private dataset using a two-stage \"SFT+RL\" framework, and its prediction accuracy (\"what\") surpassed top-tier LLMs like GPT-5."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Novety: The paper's greatest strength is its re-definition of the problem. Shifting reaction recommendation from a black-box prediction task to a transparent, evidence-based reasoning task is critical for the adoption of AI in serious scientific research11. The formal problem definition in Section 2.1 (requiring each output $c$ to be accompanied by a verifiable rationale $\\rho(c)$ 12) is excellent and directly targets a key pain point in modern AI for Science.\n\n2. Good Design. The architecture of ChemMAS is well-thought-out. It doesn't just stack agents; it builds a logically-sound workflow. From the \"General Chemist\" building a shared \"Memory\", to \"Multi-Channel Recall\" building the candidate pool , to the \"Tournament-Style Debate\" for ranking, each step is purposeful. The use of a tournament-style pair-wise debate instead of brittle global scoring is a particularly robust mechanism for ranking, and its efficacy is confirmed in the ablations.\n\n3. Paper writing and presentation are really good for me. The idea is easy to follow, and the workflow shown in the Figures is clear."}, "weaknesses": {"value": "1. Private Dataset: The entire evaluation hinges on a private dataset of 544,591 reactions. This is a major weakness, as it severely hinders the reproducibility and external validation of the results. Furthermore, how was the \"Reaction Base\" for \"Multi-Channel Recall\" constructed? How much overlap does it have with the test set? If the system relies heavily on near-duplicate examples from the recall database, its \"reasoning\" capabilities might be overestimated, functioning more like a sophisticated k-NN lookup.\n\n2. Lack of Evaluation for the \"Rationale\": The paper's core claim is to provide \"falsifiable, human-trustable rationales\"  (the \"why\"). However, all quantitative evaluations (Tables 1, 2, 3) focus exclusively on Top-k prediction accuracy (the \"what\"). The paper provides no evaluation of the quality of the generated rationales themselves. Are these explanations chemically sound? Do they offer genuine insight, or are they just templated statements? This lack of human-in-the-loop evaluation (e.g., by expert chemists) leaves the central claim of the paper (\"From What to Why\") unsubstantiated.\n\n3. The ChemMAS system is extremely complex. To predict a single reaction, it must run $\\mathcal{A}_{Gen}$ (with 3 tool calls) 23, perform a large-scale multi-channel recall (generating 5k candidates)24, and then execute a multi-round tournament (approx. 7 rounds to get from 5k to 50). In each tournament round, a panel of agents (e.g., 4 agents) must adjudicate each pair via \"Multi-Step Reasoning\" (which is itself an iterative process)25252525252525. This implies massive computational overhead. The authors provide no analysis of inference cost (e.g., total tokens, API costs, or end-to-end latency). How does this compare in practicality to a single prompt sent to GPT-5?"}, "questions": {"value": "The paper's central thesis is the shift from \"what\" (prediction) to \"why\" (explanation), claiming to provide \"falsifiable, human-trustable rationales\". Yet, all quantitative results (Tables 1-3) only measure Top-k prediction accuracy (\"what\"). Without any human-in-the-loop evaluation or expert validation of the generated rationales, how can the authors substantiate their claim that the system delivers on the \"why\" by producing high-quality, scientifically sound explanations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JZnMfmwdeR", "forum": "Rh72R0VXPS", "replyto": "Rh72R0VXPS", "signatures": ["ICLR.cc/2026/Conference/Submission11684/Reviewer_bFZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11684/Reviewer_bFZ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004319181, "cdate": 1762004319181, "tmdate": 1762922735256, "mdate": 1762922735256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-agent system for the problem of reaction condition recommendation. The system operates through several stages including reasoning and tool-use LLMs, all of which are augmented by tools and retrieval, as well as a memory mechanism.\nA strong point in the exposed reasoning is that explainability, and knowing why decisions are made, is important for the use of these type of tools.\nThe authors employ several advanced techniques to improve their models, including SFT, RL, and some specific reward formulations to incentivize specific behaviours.\nThe authors show that this system outperforms any other system out there, including state-of-the-art LLMs like GPT5 and gemini-2.5-pro.\nLittle to no details are given with respect to the datasets used for training and evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes several techniques that work together towards making the accuracy quite high and improving over several other baselines.\nThis tackles an important challenge in organic chemistry by enabling prediction of reaction conditions."}, "weaknesses": {"value": "The authors make it a strong point in the introduction that explainability and knowledge about what factors drive reactivity is important for experts using systems like this, however no analysis or examples are shown that demonstrate that the predicted conditions are indeed connected to insights in the reasoning, and thus fail to establish causation between the applied reasoning and debate mechanisms and improvement in performance.\n\nThere are some ablations, however the authors should work on more ablations, to understand the role of \"peer deliberation\" in the results. Arguably this is a type of inference-time scaling, and as such it should be ablated against other types of inference-time scaling techniques, like simple aggregation or majority voting of unrelated reasoning threads.\n\nThe work claims state of the art across all the test categories in a benchmark, beating every other baseline including pretrained models and other LLMs in zero-shot mode. The evaluation is very unclear and concerning as:\n- there is no mention as to what is the source of the datasets used for testing, other than \">500k private samples\". There is barely any mention of the source, curation process, etc. The code only shows some data loading.\n- It is not clear how results are standardized for comparison with ground truths. Given the many ways that are used to write the same reagent (e.g. AcOH or H3CCOOH) it is highly unlikely that any system will achieve such high accuracies unless there is some reagent standardization method, which is by no means mentioned in the paper.\n- The role of RL training or of all the rewards is not at all discussed, and it is not clear if they actually provide any value. There should be more ablations in this direction, to really determine if all these mini contributions do indeed drive performance.\n- The comparison against zero-shot LLMs is unfair, in that it is not clear what prompts were used for the evaluation. In addition, these were not connected to memory, tools, or any other of the additional aids that were given to ChemMAS. It would be very good for transparency to also provide the benchmark results on the same framework, but substituting the LLM for one of the commercial LLMs, or other open-weights.\n\nIt is not clear why the \"functional group tagger\" tool needs to be a tool. As I understand it, it could simply be a pre-processing step that is hardwired to be done for every smiles string that is given in the input. There should also be ablations of this.\n\nNo discussion about the latency of the different systems is provided. It looks like there is a lot of different processes going on and several stages that make this approach notably expensive, and this has not been mentioned anywhere."}, "questions": {"value": "- Can you elaborate on the data collection and processing pipeline? it is very unclear on what data the evaluation was performed. Add descriptions of the resulting dataset such as distributions of reaction types, molecule size, etc.\n- Add more thorough ablations and evaluations of your system. As described above, it's not clear if all the contributions in terms of tools, memory, design of rewards, etc, indeed contribute significantly to the final performance. Otherwise they look like ad-hoc decisions that make the work very non-transparent.\n- Provide more details into the evaluation: Have the chemicals been standardized or preprocessed in any way? Also see the weaknesses section for details on this\n- Can you elaborate on the chemical explainations, or the reasoning traces given by your LLMs during inference? This can clarify how specifically the system produces results, and it would be very illustrative to analyze manually some successful trace that can show a key insight that led to the correct prediction of reaction conditions.\n- Connected to this, can you discuss the different failure modes that can be found upon inference with this system? An analysis of the failure modes would be great."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "70zCZTc52D", "forum": "Rh72R0VXPS", "replyto": "Rh72R0VXPS", "signatures": ["ICLR.cc/2026/Conference/Submission11684/Reviewer_nePm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11684/Reviewer_nePm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107229431, "cdate": 1762107229431, "tmdate": 1762922734540, "mdate": 1762922734540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChemMAS, a novel multi-agent system designed to move chemical reaction condition recommendation from simple \"what\" predictions to \"why\" evidence-based reasoning. The system uses a specialized team of LLM agents (e.g., General Chemist, Catalyst Agent) that collaboratively debate and reason using a Chemical Knowledge Base to select and justify reaction conditions. Empirically, ChemMAS claims to demonstrate substantial improvements in accuracy and sets a new standard for interpretability in this high-stakes domain."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Paradigm-Shifting Research Focus: The core idea of requiring interpretable, falsifiable justifications for condition recommendations is highly valuable to chemists, addressing the main bottleneck in adopting black-box AI models for synthesis planning.\n\n2. Sophisticated Architecture: The ChemMAS multi-agent system is a well-structured solution to the problem. Decomposing the task into a series of specialized steps (parsing, multi-channel recall, tournament selection, and multi-agent debate) mirrors a more realistic, collaborative scientific process.\n\n3. Innovative Training Framework: The two-stage training (Supervised Fine-Tuning for \"Chemical Teaching\" followed by Reinforcement Learning for \"Tool Incentivization\") is a compelling method for ensuring agents not only know the correct answer but also effectively utilize their available chemical tools during the collaborative reasoning process.\n\n4. Exceptional Empirical Performance: The reported accuracy gains are highly significant. Outperforming specialized chemical models (RCR, Reagent Transformer) by 20–35% and general-purpose LLMs (like GPT-5, Gemini 2.5) by 10–15% validates the design philosophy and the efficacy of the multi-agent approach.\n\n5. Achieving Explainability: The system successfully produces human-trustable rationales, which is the primary stated goal and a major step forward for explainable AI in chemical discovery."}, "weaknesses": {"value": "1. Out-of-Distribution (OOD) Performance and Generalizability: While performance on the benchmark dataset is excellent, a critical review question revolves around performance on novel, out-of-distribution reactions. Since the system relies on a \"Multi-Channel Recall\" from a structured database, performance on genuinely new reaction types not represented in the knowledge base or training data needs detailed analysis.\n\n2. Ablation of Agent Debate: The paper demonstrates that the full system is best. However, a deeper analysis of the multi-agent collaboration would be beneficial—specifically, how often the \"debate\" process actually changes a consensus decision established by the initial \"Tournament Selection.\" Understanding the cost-benefit of the full, computationally intensive debate is key.\n\n3. Computational Cost Analysis: Multi-agent systems with tool-use and multi-step reasoning often incur high inference latency compared to simpler end-to-end models. The paper should ideally provide a quantitative comparison of the wall-clock inference time and resource demands (e.g., number of agent calls, token count) compared to the baseline LLMs.\n\n4. Falsifiability of Rationales: While the rationales are deemed \"trustable,\" the paper could strengthen its claim by detailing a systematic human evaluation of the rationales not just for correctness, but for their adherence to known chemical principles and their degree of falsifiability (i.e., whether they rely on concrete, verifiable chemical concepts)."}, "questions": {"value": "1. Computational Resources and Latency\n- Inference Cost: The proposed system involves complex multi-stage pipelines: recall, tournament pairing, and multi-round, multi-agent debate for each pair. What is the average inference time (wall-clock) and token cost per reaction compared to the single-pass baselines? Is this latency acceptable for real-time library screening applications?\n\n- Scalability of Debate: The tournament selection narrows 5,000 candidates down to 50 via pairwise debate. How does the computational cost scale? Did you explore more efficient ranking methods before the final expensive debate stage?\n\n2. Generalization and Out-of-Distribution Performance\n- Novel Reactions: The \"Multi-Channel Recall\" relies heavily on retrieving similar reactions (by type, reactant, or product). How does the system perform on truly novel reaction types that are chemically valid but have very poor coverage in the Reaction Base? Does the debate mechanism still function effectively when retrieval quality is low?\n\n3. Mechanism Analysis\n- Debate Impact: In the ablation study, removing \"Multi-Agent Debate\" hurt performance. Can you quantify how often the debate process actually changed the outcome compared to a simpler majority vote of the agents' initial, individual assessments? This would help justify the high computational cost of the debate rounds.\n\n- Agent Specialization: You used specialized agents (Catalyst, Solvent, Reagent). Did you observe instances where these agents conflicted in unhelpful ways (e.g., a solvent agent insisting on a solvent incompatible with the best catalyst), and how robust is the \"facilitator\" or voting mechanism in resolving strictly chemical incompatibilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X5Apc2lUNv", "forum": "Rh72R0VXPS", "replyto": "Rh72R0VXPS", "signatures": ["ICLR.cc/2026/Conference/Submission11684/Reviewer_H2cV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11684/Reviewer_H2cV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762758926085, "cdate": 1762758926085, "tmdate": 1762922734078, "mdate": 1762922734078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ChemMAS, a multi-agent system for evidence-based reaction condition reasoning. The system decomposes condition recommendation into four collaborative modules: mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Through this process, each decision is supported by interpretable, falsifiable justifications grounded in chemical knowledge and retrieved precedents. Extensive experiments show that ChemMAS achieves 20–35% gains over domain-specific baselines and 10–15% improvement over general-purpose LLMs on Top-1 accuracy, while producing human-trustable rationales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.Evidence-based reasoning: This paper introduces evidence-based reasoning for chemical reaction, which is intuitive and aligns well with how human chemists rationalize reaction design. Empirical experiments show that the proposed method outperforms baselines by a large margin and is interpretable via evidence-based reasoning.\n\n2.Chemical-grounded interpretability: The system employs general chemist agents equipped with specialized tools to extract key chemical features, such as functional groups, stoichiometric balance, and potential by-products, from molecular formulas. These extracted elements then serve as the foundation for multi-step reasoning, forming an innovative and interpretable workflow that adheres to fundamental chemical principles, rather than treating the problem as a purely linguistic translation task as in many existing LLM-based approaches.\nExtension of tool-based and multi-agent RL in Chemical domains: Multi-agent/tool-based  reinforcement learning has previously been applied to domains such as deep search, mathematical reasoning, and code generation. The authors naturally extend this paradigm to the multi-agent setting for chemical reaction condition reasoning, enabling more effective and collaborative evidence-based deliberation."}, "weaknesses": {"value": "1.The paper briefly mentions broader applications. A short discussion on how ChemMAS could transfer to other reasoning-heavy scientific domains (e.g., physics, materials science) would improve the positioning.\n2.While the benchmark results are convincing, a discussion or demonstration of ChemMAS’s utility in closed-loop or experimental lab pipelines would further solidify its practical value.\nEven simulated lab feedback could strengthen the argument."}, "questions": {"value": "1. Is there any expert evaluation of the interpretability or practical utility of ChemMAS’s rationales compared to baselines or human chemists?\n2. How does ChemMAS handle conflicting evidence retrieved from different sources during the agentic debate stage?\n3. Is there a measurable trade-off between interpretability (longer rationales) and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eNM5PAshtU", "forum": "Rh72R0VXPS", "replyto": "Rh72R0VXPS", "signatures": ["ICLR.cc/2026/Conference/Submission11684/Reviewer_DYA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11684/Reviewer_DYA5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762813179192, "cdate": 1762813179192, "tmdate": 1762922733690, "mdate": 1762922733690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}