{"id": "3u2L0GVern", "number": 25120, "cdate": 1758364372148, "mdate": 1759896733559, "content": {"title": "SelfMask: Cross-modal Self-Masking for Multimodal Representation Learning in Missing Modality Scenarios", "abstract": "Multimodal learning promises to harness complementary information across diverse modalities, yet real-world deployments often face missing modalities due to acquisition costs, privacy constraints, or data corruption, leading to substantial performance degradation. We present \\ours, a framework for learning robust representations in the presence of incomplete multimodal data. During training, \\ours\\ imputes missing modality representations through a masked representation learning scheme with adaptive masking, where informative masks are learned from data rather than sampled at random. To guide the imputation without relying on unavailable ground-truth for missing modalities, we introduce a cross-modal consistency loss: predicted representations of missing modalities are required not only to align with semantic content but also to support the reconstruction of observed ones. This consistency-based objective encourages robust, semantically grounded representations. Experiments on MIMIC-IV and CMU-MOSEI demonstrate that \\ours\\ consistently improves resilience and predictive accuracy under diverse missing-modality scenarios. Ablation studies further show that our learned masks outperform conventional random masking, yielding more reliable cross-modal representations. Our framework is broadly applicable across multimodal domains, offering a practical solution for real-world settings where incomplete modalities are the norm.", "tldr": "SelfMask improves robustness under missing-modality inputs by learning representation-level imputation and a context-aware masking policy, trained with cycle-consistent self-supervision.", "keywords": ["Multimodal learning", "Missing modality", "Self-supervised learning", "Representation-level imputation", "Cross-modal masking"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f6f8256a7a2fc0635fb4f22b9ff09b514c05686.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SELFMASK, a framework for learning robust multimodal representations when modalities are missing during deployment. The goal is to move beyond random masking by learning adaptive masking patterns that are informed by cross-modal relationships, while imputing missing modality representations through a cross-modal consistency objective. The approach consists of three main components:​\n\n- Adaptive Mask Prediction: A mask predictor learns which parts of observed modalities to mask based on an exponential moving average (EMA) of the representation predictor​ weights and leverages the reconstruction of the learned mask in the loss (L_obs)\n- Cross-Modal Representation Imputation: Missing modality representations are predicted from observed modalities and trained via reconstruction of the observed modality representations​ from the predicted via a cross-modal consistency loss (L_cross)\n- Multimodal Fusion and Task Prediction: The predicted and observed modalities are agregated to predict the final result (L_task)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Excellent Presentation and Figures: The paper is well written, easy to understand, and follow, and the Figures and equations are informative and convey the ideas of the paper well.\n\n2.  Rigorous Experimentation: Experiments were averaged over independent runs with standard deviations reported. The tables are clear to understand, and the model shows consistent improvements over baselines.\n\n3. Elegant Method: The model combines various ideas well, including masked representation reconstruction for intramodal and cross-modal feature learning, as well as missing modality feature imputation."}, "weaknesses": {"value": "1. Limited Novelty: The core components of SelfMask closely resemble existing approaches in missing modality learning. Specifically: \n   - The representation-level imputation approach is similar to ActionMAE’s training framework, CMAT's cross-modal translation framework\n   - Masked autoencoder approaches for multimodal learning (M3AE, MultiMAE) already explore similar masking strategies. There is not enough evidence or support to claim learned masking over random masking is a significant contribution (e.g, ablations, motivating examples, visualizations, etc.)\n   - The combination of learning each modality's unique features as well as imputing missing ones through a reconstructed feature is also present in Robult\nThe authors should clearly differentiate their contributions from these existing works and provide a comparative analysis to justify the claimed novelty.\n\nCMAT: Park et al  \"Cross-modal alignment and translation for missing modality action recognition\" (ACM Computer Vision and Image Understanding, 2023)\n\nActionMAE: Park et al. “Towards Good Practices for Missing Modality Robust Action Recognition” (AAAI 2023)\n\nM3AE: Geng et al. \"Multimodal Masked Autoencoders Learn Transferable Representations\" (2022)\n\nMultiMAE: Bachmann et al MultiMAE: Multi-modal Multi-task Masked Autoencoders (ECCV 2022)\n\nRobult: Nguyen et al: Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning (IJCAI 2025)\n\n2.  Limited Theoretical Analysis: The paper lacks a theoretical justification for why the proposed EMA-based mask predictor should learn informative masks​. The connection between mask informativeness and cross-modal consistency is relatively intuitive but not rigorously established\n\n3.  Hyperparameter Sensitivity and Complexity: The framework introduces multiple hyperparameters (α=0.01, β=0.1, τ=0.996, different masking ratios) without comprehensive sensitivity analysis​ or intuitive justification for them. The dual masking ratios (25% for Lobs, 50% for Lcross) appear somewhat arbitrary and may require task-specific tuning​.\n\n4. Computational Overhead Analysis: Some discussion of the comptuational overhead of this method would be beneficial. The EMA mask predictor and dual forward passes increase computational cost during training, but this overhead is not quantified​. No analysis of inference time complexity when handling missing modalities in deployment\n\n5. Evaluations and Visualizations: The analysis of performance across different missing-modality scenarios is limited. In particular, it’s unclear how the model behaves when certain modality combinations are consistently unavailable or when the proportion of missing data varies. Figure 3 lacks clarity—why did the model choose to mask those specific parts? Were those regions highly informative during training (e.g., indicative of disease) or relatively uninformative? The paper would benefit from clearer and more interpretable visualizations, as well as a systematic failure mode analysis. For example, which test instances did the model handle well despite missing modalities where previous approaches failed? How did the self-mask design contribute to this success? Conversely, on what types of instances does the model still struggle?"}, "questions": {"value": "*Clarification Questions:*\n\n1. Are the learned missing tokens (the pink section in Figure 2) shared across all modalities, or are they learned separately for each modality? Additionally, is V^m used to represent missing modalities, or is it only employed to fill in the masks for observed modalities that were intentionally masked during training?\n\n2. Is the sequence length fixed for each modality and input sample, or does it vary depending on the sample or modality? For instance, would a 10-second video and a 30-second video both be represented by the same number of tokens, or would their token counts differ? How is this handled in the model across different samples and modalities?\n\n*Thought Experiments/Extensions:*\n\n3. Mask Informativeness: How do you ensure that the learned masks are indeed more informative than random masks? Can you provide a quantitative or qualitative analysis of mask quality beyond downstream performance, such as an ablation with random masks instead of learned masks.\n4. Cross-Modal Dependence: How does the method perform when modalities have minimal shared information or when cross-modal correlations are weak? Are there failure modes you can characterize?\n5. Scalability: How does the approach scale to scenarios with more modalities (>3) or when different extreme percentages of the test distribution have missing modalities (5% or 95% missing modalities)? Is this method better under certain instances compared to others"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5wW1gATTPg", "forum": "3u2L0GVern", "replyto": "3u2L0GVern", "signatures": ["ICLR.cc/2026/Conference/Submission25120/Reviewer_pQCE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25120/Reviewer_pQCE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932363424, "cdate": 1761932363424, "tmdate": 1762943330362, "mdate": 1762943330362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SELFMASK, a cross-modal self-masking framework for multimodal learning under missing-modality conditions. The method learns adaptive (data-driven) masks rather than random masks and uses a cross-modal consistency objective: predicted representations for missing modalities should be semantically aligned and also help reconstruct observed modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Problem relevance**. Handling missing modalities is practically important in clinical and multimedia settings, and the paper targets recognized gaps in robustness. \n- **Method intuition**. Learning informative masks echoes advances in learned/curriculum masking for MAE-style objectives (e.g., AutoMAE, CL-MAE; also guidance via token-critic), and a consistency signal across modalities is conceptually appealing."}, "weaknesses": {"value": "- **Poor Illustration**. The overall pipeline figure (Fig.2) is not self-explained and cannot capture the idea/pipeline well enough.\n- **Selection bias from \"complete-case\" construction**. The paper extracts only fully paired samples and then simulates missingness. This is somewhat restrictive, while current literature on missing modalities target much more generic settings where missing modalities actually happen during both training and evaluation.\n- **Incremental novelty**. Learned masking has been explored in several works (AutoMAE, CL-MAE, self-guided MAE), and cross-modal masked pretraining is not new (e.g., Multimodal MAE). The paper’s technical delta over these lines is not crisply articulated [1,2,3].\n\n[1] Chen, Haijian, et al. \"Improving masked autoencoders by learning where to mask.\" Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Singapore: Springer Nature Singapore, 2023.\n\n[2] Madan, Neelu, et al. \"Cl-mae: Curriculum-learned masked autoencoders.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024.\n\n[3] Shin, Jeongwoo, et al. \"Self-guided masked autoencoder.\" Advances in Neural Information Processing Systems 37 (2024): 58929-58954."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QasXf8fpRV", "forum": "3u2L0GVern", "replyto": "3u2L0GVern", "signatures": ["ICLR.cc/2026/Conference/Submission25120/Reviewer_rsUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25120/Reviewer_rsUS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951817184, "cdate": 1761951817184, "tmdate": 1762943330125, "mdate": 1762943330125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-modal self-masking framework, SelfMask, for learning robust multimodal representations with missing modalities. The key idea is to leverage an adaptive masking strategy (learned via a mask predictor) and a cross-modal consistency loss to impute missing modality representations without ground truth. The method is evaluated on MIMIC-IV and CMU-MOSEI datasets, demonstrating improvements over several strong baselines under simulated missing-modality conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper addresses the missing modality problem, which is a significant and practical barrier to deploying multimodal systems in real-world settings \n+ The proposed framework is model-agnostic and can attach to standard encoders and a simple fusion module.\n+ The cross-modal consistency loss provides a reasonable way to supervise imputation when ground truth for the missing modality is unavailable."}, "weaknesses": {"value": "- The proposed \"adaptive masking\" strategy is repeatedly contrasted with \"conventional random masking”. However, the ablation study in Table 4 only evaluates the different loss components. There is no experiment in the paper that compares the proposed adaptive masking ($h_{\\omega}$) against a standard, computationally simpler baseline.\n- More specifically, the evaluation is missing several strong but simpler control experiments that could strengthen the paper. For example: i) A simple modality dropout or zero-imputation baseline combined with a robust fusion module (no predictor). ii) A standard random masking strategy. iii) Simpler latent-space imputation methods (e.g., k-nearest neighbors) with a comparable parameter budget.\n- The \"cross-modal consistency\" loss functions as a reconstruction constraint. However, there is limited analysis of what semantic properties are actually preserved in the imputed representations ($\\hat{Z}^{(m)}$). An analysis of class-conditional fidelity or visualization of the imputed latent space would be beneficial.\n- The complete framework, including the representation predictor ($g_{\\phi}$) and mask predictor ($h_{\\omega}$), is required during inference for missing modalities. The paper does not quantify the latency, FLOPs, or memory overhead."}, "questions": {"value": "(1) Could you please provide the ablation study that compares your adaptive \"mask prediction\" ($h_{\\omega}$) against a standard, computationally cheaper random masking strategy with the same masking ratios?\n\n(2) How do you expect the proposed framework, particularly the adaptive masking and consistency loss, to scale to high-modality scenarios (e.g., 5 or 10 modalities)?\n\n(3) Could you provide a more detailed analysis of the computational cost (parameters, training time, and inference-time FLOPs/latency) to better contextualize the performance-vs-complexity trade-off of SelfMask against the baselines?\n\n(4) Can you deepen the discussion on the learned masks (e.g., via visualization or quantification) to help us understand what patterns or features the mask predictor ($h_{\\omega}$) is learning to identify as \"informative\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YR4S9DYiYE", "forum": "3u2L0GVern", "replyto": "3u2L0GVern", "signatures": ["ICLR.cc/2026/Conference/Submission25120/Reviewer_EWBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25120/Reviewer_EWBh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972273231, "cdate": 1761972273231, "tmdate": 1762943329751, "mdate": 1762943329751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework designed to learn robust multimodal representations in the presence of missing modalities. The method employs an adaptive masking strategy that learns which parts of observed modalities to mask and uses a cross-modal consistency loss to impute missing modality representations without ground truth. The approach is evaluated on two benchmarks under various missing-modality scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly and concisely structured, making it easy to follow.\n\n2. The method is systematically evaluated on two diverse datasets under various missing-modality settings. An ablation study effectively demonstrates the contribution of each loss component."}, "weaknesses": {"value": "1. Most building blocks exist (MAE/JEPA-inspired latent prediction, EMA teacher, Top-K masking). The novelty rests on how they’re assembled for missing-modality training. \n\n2. The paper claims that learned adaptive masks outperform random masking, yet no quantitative analysis is provided. For example, do the adaptive masks consistently target tokens with higher semantic or task-relevant information?\n\n3. The current evaluation uses controlled and fixed missing-modality patterns. In real-world datasets, missingness can be heterogeneous across samples (the specific missing modalities may vary). Could the authors comment on how well SelfMask is expected to perform under such more realistic and heterogeneous missingness patterns?"}, "questions": {"value": "1. The paper introduces zero-fill and random-fill as baselines but provides minimal implementation details. Moreover, some methods (e.g., SMIL, MedFuse) perform comparably or even worse than these simple imputations on certain metrics. Could the authors clarify why this might occur and what it implies about the inherent challenges of the task?\n\n2. In Figure 3, the learned mask appears visually random. Could the authors provide a quantitative comparison or ablation showing that the learned mask indeed outperforms random masking? Can the authors elaborate on the semantic meaning or provide qualitative insights from Figure 3?\n\n3. Do the authors think that strong pretrained encoders (e.g., SigLIP2) may overshadow the contribution of the masking mechanism?\n\n4. There are minor typos (e.g., in Section 3.6: “our method achieves only a 7.9 percentage point drop…”). It is unclear where the “7.9” figure comes from. Please verify or clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rrISi2W79m", "forum": "3u2L0GVern", "replyto": "3u2L0GVern", "signatures": ["ICLR.cc/2026/Conference/Submission25120/Reviewer_V93W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25120/Reviewer_V93W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156545822, "cdate": 1762156545822, "tmdate": 1762943329349, "mdate": 1762943329349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}