{"id": "8NdXdmWqgD", "number": 13178, "cdate": 1758214707277, "mdate": 1763095833080, "content": {"title": "Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential", "abstract": "Reinforcement learning with verifiable rewards (RLVR) can elicit strong reasoning in large language models (LLMs), while their performance after RLVR varies dramatically across different base models. This raises a fundamental question: what microscopic property of pre-trained models leads to this variation? To investigate, we formalize reasoning as chains of Horn clauses (\"if-then\" rules) built from features extracted from the LLM's latent space via cross-layer sparse autoencoders (SAEs). We estimate the transition probabilities between its features, and further categorize each rule by its semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key discovery is that high-potential models are inherently soundness-aware: their internal probability distributions systematically shift across rules' soundness levels, becoming highly distinct for \"strict\" versus \"noisy\" rules. In contrast, weaker models are soundness-agnostic, collapsing to one distribution regardless of soundness levels. To quantify this, we introduce the Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon Divergence to measure the separation between these distributions. We show that SAL's predictions of post-RLVR reasoning performance follow a precise empirical law ($R^2=0.87$) across diverse model families (Qwen, Mistral, Llama, DeepSeek) and scales (0.5B–14B). This reveals that a model's reasoning potential is tied to its intrinsic, pre-trained ability to distinguish sound knowledge from unsound ones. These findings underscore the critical role of model pre-training in shaping reasoning and offer a practical metric grounded in the model's internal mechanisms for selecting/designing stronger base models.", "tldr": "", "keywords": ["Interpretability", "Large Language Models", "Reasoning", "Sparse Autoencoders"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7db73abccad862ba7df3b3493d890307b45ba40e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Soundness-Aware Level (SAL), a metric designed to predict a large language model’s reasoning ability after reinforcement learning with verifiable rewards (RLVR). Experiments across multiple model families show that SAL strongly correlates with post-RLVR reasoning performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Logical rules (Horn clauses) are extracted from sparse representations of model activations, and their soundness is assessed based on whether each rule “makes sense” intuitively. The model’s reasoning potential is then evaluated by measuring how distinctly it separates sound from unsound logical rules in its internal feature space. It is an appealingly simple and intuitive idea.\n2. The paper carefully explains each stage of the proposed pipeline — training sparse autoencoders on hidden activations, extracting co-activation rules, labeling their logical soundness, and computing the Soundness-Aware Level (SAL) via Jensen–Shannon divergence. Each step is well-motivated and supported with sufficient detail, making it easy to follow and pleasant to read.\n3. The experiments align well with the authors’ hypothesis. The results consistently show that higher SAL values correlate strongly with better post-RLVR reasoning performance across multiple model families."}, "weaknesses": {"value": "The approach relies on LLMs to label and categorize logical rules. This may introduce circularity or bias."}, "questions": {"value": "Why do the authors use the term “Mechanism interpretation” rather than mechanistic interpretability or mechanistic interpretation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SCQsbkukHW", "forum": "8NdXdmWqgD", "replyto": "8NdXdmWqgD", "signatures": ["ICLR.cc/2026/Conference/Submission13178/Reviewer_jKNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13178/Reviewer_jKNw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865253708, "cdate": 1761865253708, "tmdate": 1762923879159, "mdate": 1762923879159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to find a score for selecting a suitable pretrained model for RLVR. They find that the soundness-aware level (SAL) is indicative of the post-RL performance, which roughly measures whether the model can tell if a logical implication is sound vs not.\n\nThe method takes the following steps:\n- Extract features from hidden representations using SAE.\n- Construct logical implication rules from the features by checking co-occurrences patterns, and use a LLM (DeepSeek-R1) to label each rule as Strict / Plausible / Noise.\n- Compare how the distributions over Strict / Plausible / Noise rules are different, which is quantified by the JSD on histograms.\n\nIntuitively, the more different these distributions are, the higher the score, suggesting the model is more \"soundness-aware\".\n\nEmpirically, SAL correlates well with post-RL performance on 7 models. 4 of them are from the Qwen family with different sizes, and 3 models are around 7B scale, from Mistral, Llama, and DeepSeek."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This is one of the first papers that consider the problem of choosing base model for RL, which is timely.\n- The experiments consider different model families and tasks."}, "weaknesses": {"value": "I'm mainly concerned about limited empirical evidence.\n- There are only 7 models, so the correlation number is not convincing.\n- The trend is especially unclear with 7B-scale models across families, i.e. no clear ordering at the lower left corner of Fig 4 (left).\n- The paper fits a power law curve between SAL and the post-RL performance, but the empirical data only supports the near-linear regime of the curve, so it's unclear why a power law is justified.\n- There's no quantification of uncertainty/variation. For example, if we train different SAEs on the same model, how much difference will there be in SAL scores?"}, "questions": {"value": "- It seems that the bigger model is better based on Qwen results. Is this always true?\n  - If yes, this (i.e. \"bigger is better\") is not providing new information unless SAL can quantify the amount of improvement -- though I'm doubtful about the accuracy of the power law; please see my comment in Weakness.\n  - If no, please comment on when & how SAL can identify the proper size. \n- Quantifying uncertainty/variations due to randomness: If we train different SAEs on the same model, how much difference will there be in SAL scores?\n- Please report the computational cost of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yC1SOFYDv4", "forum": "8NdXdmWqgD", "replyto": "8NdXdmWqgD", "signatures": ["ICLR.cc/2026/Conference/Submission13178/Reviewer_gLaR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13178/Reviewer_gLaR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928319474, "cdate": 1761928319474, "tmdate": 1762923878751, "mdate": 1762923878751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is the updated review, incorporating a critical assessment of the statistical validity of the paper's central claims.SummaryThe paper investigates why different base LLMs exhibit varied reasoning performance after Reinforcement Learning with Verifiable Rewards (RLVR). The authors hypothesize that this potential is tied to a model's pre-trained ability to internally distinguish sound from unsound knowledge. They introduce a \"microscopic signature,\" the Soundness-Aware Level (SAL), to quantify this.\n\nThe methodology for calculating SAL is complex. It involves: (1) training cross-layer Sparse Autoencoders (SAEs) to extract features; (2) identifying implicit logic rules (Horn clauses) by analyzing feature co-occurrence probabilities; (3) using an external LLM judge (DeepSeek-R1) to categorize the soundness of these rules (Strict, Plausible, Noise); and (4) calculating SAL as the Jensen-Shannon Divergence (JSD) between the probability distributions of these categories.The central claim is that SAL strongly predicts post-RLVR performance. Based on an analysis of **only** 7 models (4 Qwen, Llama, Mistral, DeepSeek), the authors claim the relationship between SAL and post-RLVR error rate follows a precise \"empirical law\" ($\\epsilon=exp(-\\alpha\\cdot SAL^{\\beta})$), reporting high R-squared values."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The premise of the paper (understanding what makes a model RL-able) is an important problem. The proposed approach is new and interesting, using mechanistic interpretability.\n\n- The framework, combining SAEs and logic, provides a novel and interesting method for analyzing the internal logic of LLMs. And the findings are \n\n- SAL is proposed as an intrinsic signature that does not require labeled data from the downstream task (only an unlabeled corpus and the LLM judge), which makes it more usable than other model selection methods."}, "weaknesses": {"value": "- The paper's assertion that the relationship between SAL and error rate follows a precise \"empirical law\" is a stretch! The law is derived from only 7 data points (N=7) with a heavily biased dataset of 4 out of the 7 models belong to the same Qwen family. It is too few points to claim anything about a law, in particular a piecewise linear function would fit this perfectly too.\n\n- The SAL metric hinges entirely on an external LLM (DeepSeek-R1) categorizing the soundness of extracted rules. Appendix D reveals that the agreement between this LLM judge and human annotators is extremely low (0.566). If the foundational categorization of rules is unreliable, the SAL metric derived from these labels is inherently unstable and noisy. \n\n- SAL is exceedingly complex and computationally intensive. It involves specialized SAE training, feature interpretation, and massive co-occurrence counting (Appendix D mentions $5.8\\times10^{12}$ combinations). The computational resources make the approach infeasible."}, "questions": {"value": "Address weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVlgCsPW2L", "forum": "8NdXdmWqgD", "replyto": "8NdXdmWqgD", "signatures": ["ICLR.cc/2026/Conference/Submission13178/Reviewer_9NuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13178/Reviewer_9NuL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951452650, "cdate": 1761951452650, "tmdate": 1762923878347, "mdate": 1762923878347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response to Reviewers"}, "comment": {"value": "We sincerely thank the reviewers for their detailed and insightful feedback. Below, we summarize the key strengths of our manuscript as noted by the reviewers:\n\n- The research problem of choosing base models for RL training is important and timely. (Reviewer 9NuL, gLaR)\n- The proposed method that utilizes features from SAEs to study logic reasoning is novel and intuitive. (Reviewer gH88, 9NuL, jKNw) \n- The experiment considers the generalizability of the proposed method across different model families. (Reviewer gLaR, jKNw)\n- The presentation of the manuscript is good. (Reviewer gH88, 9NuL, gLaR, jKNw)\n\nWe respond to each reviewer's concerns in their respective comment sections."}}, "id": "6UnwpgYHK5", "forum": "8NdXdmWqgD", "replyto": "8NdXdmWqgD", "signatures": ["ICLR.cc/2026/Conference/Submission13178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13178/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13178/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763095815662, "cdate": 1763095815662, "tmdate": 1763095815662, "mdate": 1763095815662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new method for probing the internal logic of a pre-trained LLM. The method consists of three steps. In step 1, a cross-layer sparse autoencoder produces semantically meaningful features from the LLM’s hidden activations. In step 2, the method extracts implicit logical rules by computing feature co-occurrences and estimating conditional probabilities for entailment in if-then rules. In step 3, an LLM judge evaluates the quality of the extracted rules, and the method computes the Jensen-Shannon (JS) divergence between three soundness distributions: noise, plausible, and strict. The higher the JS divergence, the more effective the pre-trained model is at distinguishing sound statements from unsound ones. The paper presents experiments showcasing the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is interesting: use features from sparse autoencoders to extract logical rules with co-occurrent probabilities, then separate the probability distributions."}, "weaknesses": {"value": "-The claim that their method \"successfully predicts the downstream reasoning potential of pre-trained language models after RL training\" (Line 472) is overstated. The paper addresses correlation, not causation (see Line 484). Also, there may be unknown confounding factors. The paper does not have intervention or ablation studies.\n\n-The sparse autoencoders are not perfect. Their error should be propagated forward, but this is not done.\n\n- The paper assumes that the features extracted from the sparse autoencoder are monosemantic, which is usually not the case.\n\n- Using an LLM to judge the soundness of rules extracted from other LLMs seems circular."}, "questions": {"value": "- Why did you choose Jensen-Shannon divergence instead of Wasserstein distance?\n\n- Why did you not use human validation instead of an LLM as a judge?\n\n- I assume you did not run controlled experiments with the same architecture on different pre-training corpora, or vice versa, because you did not have the compute resources. Is that correct?\n\n- How does your method generalize to out-of-distribution reasoning?\n\n- Why does soundness-awareness correlate with reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ux8u5jABnE", "forum": "8NdXdmWqgD", "replyto": "8NdXdmWqgD", "signatures": ["ICLR.cc/2026/Conference/Submission13178/Reviewer_gH88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13178/Reviewer_gH88"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054716656, "cdate": 1762054716656, "tmdate": 1762923878070, "mdate": 1762923878070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}