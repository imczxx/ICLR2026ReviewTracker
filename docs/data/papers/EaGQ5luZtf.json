{"id": "EaGQ5luZtf", "number": 16689, "cdate": 1758267683223, "mdate": 1759897224804, "content": {"title": "Light Differentiable Logic Gate Networks", "abstract": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency at inference while sustaining competitive accuracy.\nBut vanishing gradients, discretization errors, and high training cost impede scaling these networks.\nEven with dedicated parameter initialization schemes from subsequent works, increasing depth still harms accuracy.\nWe show that the root cause of these issues lies in the underlying parametrization of logic gate neurons themselves.\nTo overcome this issue, we propose a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate.\nFor binary inputs, this already reduces the model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps.\nOn top of that, we show that the accuracy on CIFAR-100 remains stable and sometimes superior to the original parametrization.", "tldr": "", "keywords": ["reparameterization", "logic gate networks", "vanishing gradients"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbbdb612bb1fed4013892e6d23ad70035a316b3d.pdf", "supplementary_material": "/attachment/461736c47143023c7e8807ad65edc9213d2d2efc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new parameterization called input-wise parameterization (IWP) for Differentiable Logic Gate Networks (DLGNs), which converts the original parameterization (OP), where mixture-over-Boolean-functions parameterization is learned. IWP directly learns the numerical outcome of four input cases, i.e., (0,0), (0,1), (1,1), (1,0), leading to a 4x parameters reduction per gate. The authors claim that the new proposed parameterization can alleviate gradient vanishing and less computational costs. Experiments on CIFAR-100 show superior performance and faster computation of IWP compared to the original parameterization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis in discretization error, redundancy, and gradient stability of OP in Section 3 is insightful.\n2. The manuscript is well-organized and easy to follow."}, "weaknesses": {"value": "1. My major concern of the paper is the experiment. The authors only evaluate IWP on single CIFAR-100 datasets. I would expect the authors to include more diverse datasets, for example, MNIST, CIFAR-10, WMT’14, and possibly TinyImageNet. Experiments on more diverse datasets are needed to evaluate the proposed parameterization robustly. \n2. My second concern is that the improvement over OP is marginal. For example, in CIFAR-100 the authors show 2% improvement with 8x faster convergence. However, there is no breakthrough in representation power, i.e., the depth scaling behaviors are unchanged compared to the OP as seen in Figure 4. However, note that the overall test accuracy is 29% while very small CNNs like resnet-20 can have 70%+ accuracy on CIFAR-100. Hence, results would be more compelling if deeper models with IWP closed the gap to standard architectures, not just DLGNs with OP."}, "questions": {"value": "1. In line 196-197, should the argmax is the pass-through gate G4 rather than G3?\n2. The expression of Equation 9 is a bit confusing to me. The equation would be more self-contained with more details like $G(k,l) = \\alpha_{0,0} E_{k,l,0,0} + ... $ and $E_{k,l,i,j} = \\textbf{1}_{\\\\{(k,l)=(i,j)\\\\}}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsVzW17kl6", "forum": "EaGQ5luZtf", "replyto": "EaGQ5luZtf", "signatures": ["ICLR.cc/2026/Conference/Submission16689/Reviewer_Pghb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16689/Reviewer_Pghb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529518239, "cdate": 1761529518239, "tmdate": 1762926743134, "mdate": 1762926743134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Performance results on other datasets"}, "comment": {"value": "We thank the reviewers for their thoughtful remarks. We will respond to each of them in detail later. \nTo not put off the discussion until then, we already want to provide some preliminary performance results on other datasets than CIFAR-100, since multiple reviewers asked for that.\nAdditional experiments were conducted on the RGB datasets ImageNet32 and CIFAR-10, and the grayscale datasets MNIST and Fashion-MNIST.\nFor the RGB datasets, we use the baseline CIFAR-10 M LGN architecture from Petersen et al. (2022) with 4 layers of 128k neurons each, just as we have done for CIFAR-100. \n\nFor the RGB datasets we use the following baseline model:\n```\nSequential(\n  (0): Encoding(thermometer, resolution=3)\n  (1): LogicLayerIWP(9216, 128000)\n  (2): LogicLayerIWP(128000, 128000)\n  (3): LogicLayerIWP(128000, 128000)\n  (4): LogicLayerIWP(128000, 128000)\n  (5): GroupSum(k=10, tau=...)\n)\n```\nFor CIFAR-10 we set tau to 100 as was done by Petersen et al. (2022), and to accommodate the 100-fold increase of classes in ImageNet32, we adopt the heuristic that was proposed in Petersen et al. (2022) and shrink the softmax temperature by sqrt(100).\n\n\nFor the simpler gray-scale datasets, we use 1-threshold thermometer encoding and a smaller width of 32k neurons to again align with Petersen et al. (2022) resulting in the model below.\n```\nSequential(\n  (0): Encoding(thermometer, resolution=1)\n  (1): LogicLayerIWP(784, 32000)\n  (2): LogicLayerIWP(32000, 32000)\n  (3): LogicLayerIWP(32000, 32000)\n  (4): LogicLayerIWP(32000, 32000)\n  (5): GroupSum(k=10, tau=100.0)\n)\n```\nAll of the networks were trained for 250k steps. Due to time constraints, the results are not yet averaged over multiple seeds.\n\nWe have gathered the test accuracies of the discretized logic gate networks (in %) into the following table, along with the performance of a vanilla two-layer CNN. The latter is included to give a baseline performance to expect with the encoding in place. Note that, as shown in Figure 11 in the paper, the CNN is very sensitive to the number of thresholds.\n\n| Test accuracy (disc.) | ImageNet32 | Cifar-100 | Cifar-10 | Fashion-MNIST | MNIST |\n|--------|------------|-----------|----------|---------------|-------|\n|DLGN OP | 3.48*      |   27.2    |   55.9   |     80.5      |  91.0 |\n|DLGN IWP| 3.53*      |   29.3    |   56.8   |     81.5      |  93.3 |\n|CNN     | 5.00       |   20.6    |   58.2   |     75.7      |  90.3 |\n\n*These numbers were obtained for models with twice as many layers as the baseline models.\n\n\nFor ImageNet32, the following tables depict the continuous training and discretized test accuracies of the IWP DLGN across depth scales 1 to 5.\nWe observe a monotonic improvement in both train and test accuracies for deeper models, even with the same number of training iterations. However, the improvement from depth scale 4 to 5 is small.\n\n| ImageNet32 - Train accuracy (cont.) | DLGN IWP |\n|-------------------------------------|----------|\n| depth scale 1                       |   5.37   | \n| depth scale 2                       |   6.84   | \n| depth scale 3                       |   7.62   | \n| depth scale 4                       |   8.98   | \n| depth scale 5                       |   9.38   | \n\n| ImageNet32 - Test accuracy (disc.) | DLGN IWP |\n|------------------------------------|----------|\n| depth scale 1                      |   2.90   | \n| depth scale 2                      |   3.53   | \n| depth scale 3                      |   3.59   | \n| depth scale 4                      |   3.81   | \n| depth scale 5                      |   3.82   | \n\nWhen comparing the IWP to the OP, we do observe marginal but consistent improvements across depth scales. Here, the discretized test accuracy is displayed for depth scales 2, 4, and 5.\n\n| ImageNet32 - Test accuracy | DLGN OP  | DLGN IWP |\n|----------------------------|----------|----------|\n| depth scale 2              |   3.48   |   3.53   | \n| depth scale 4              |   3.48   |   3.81   | \n| depth scale 5              |   3.56   |   3.82   | \n\nWe will conduct more experiments over the following days and include these in the paper."}}, "id": "eDctQiqSyR", "forum": "EaGQ5luZtf", "replyto": "EaGQ5luZtf", "signatures": ["ICLR.cc/2026/Conference/Submission16689/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16689/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16689/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763195839605, "cdate": 1763195839605, "tmdate": 1763195941206, "mdate": 1763195941206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an input-wise parametrization (IWP) for differentiable logic gate networks that removes redundancy, stabilizes gradients, and reduces discretization error. It cuts parameters by 4×, speeds up backward pass up to 1.86×, and converges 8.5× faster while preserving or improving accuracy on CIFAR benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a highly efficient reparametrization that eliminates redundancy in logic gate networks and significantly stabilizes gradients. It is grounded in a strong theoretical analysis of vanishing gradients and discretization error. The method delivers substantial improvements in training speed and resource efficiency. It also enables deeper DLGNs to train reliably without performance collapse. Finally, the approach maintains hardware compatibility, making it practical for edge and FPGA deployment."}, "weaknesses": {"value": "See Question Below"}, "questions": {"value": "1) Missing ablation studies on hyperparameters such as learning rate and optimizer choices. Since logic-gate networks can be highly sensitive to gradient flow and gating stability, how do different learning rates and optimizers affect the gate sharpening process and convergence behavior in IWP-based DLGNs? Additionally, how robust is the model to hyperparameter variation, and does IWP mitigate instability compared to the original parametrization (**OP**)?\n\n2) The experiments are conducted only on CIFAR-10/100, which are datasets from the same image family and share similar structure. To test generalization of the proposed IWP method to diverse problem domains, would including simpler symbolic datasets (e.g., **MNIST**) or non-vision benchmarks provide stronger evidence that the approach generalizes beyond natural images and is not over-specialized to CIFAR distributions?\n\n3) Results show that IWP scales effectively with depth under residual initialization (**RI**). However, the study appears limited to thermometer encoding and nearest-rounding. Could the authors kindly clarify whether the stability and discretization improvements of IWP are expected to extend to alternative binary encodings or rounding strategies, and whether similar behavior holds without residual biasing?\n\n4) Since IWP's parameter count scales as $2^n$ for $n$-input gates, did the authors examine performance and memory implications for $n>2$? Additionally, are there any observed or expected stability issues or computational constraints as the logic arity increases, particularly in terms of gradient behavior, training efficiency, or numerical sensitivity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKngKXO7XM", "forum": "EaGQ5luZtf", "replyto": "EaGQ5luZtf", "signatures": ["ICLR.cc/2026/Conference/Submission16689/Reviewer_Nt7w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16689/Reviewer_Nt7w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915450336, "cdate": 1761915450336, "tmdate": 1762926742779, "mdate": 1762926742779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an input-wise parameterization (IWP) of logic gate networks with tailored initializations that mitigate vanishing gradient issues of  DLGNs when increasing depth. Specifically, the paper points out that in previous DLGN, since there are negation pairs of logic gate in each layer, initializing the weight of each gate independently will cause gradient cancellation during back propagation. To address this issue, the paper reparameterize the logic function with fewer independent components. Experimental results show that the proposed method achieve better performance than existing DLGNs when increasing network depth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow\n2. The paper is well motivated, as addressing the vanishing gradient issue of deep DLGNs will improve their applicability for more challenging tasks\n3. The analysis of the root cause of vanishing gradient issue in DLGN , and the solution proposed by the paper make intuitive sense and are all backed by solid proof.\n4. Experimental results and ablation study are comprehensive and positively support the proposed design."}, "weaknesses": {"value": "1. The paper conducts experiment only on CIFAR 100."}, "questions": {"value": "1. It would be better if the paper add comparison experiment on more complex dataset like ImageNet 32"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lqnsJaAQd0", "forum": "EaGQ5luZtf", "replyto": "EaGQ5luZtf", "signatures": ["ICLR.cc/2026/Conference/Submission16689/Reviewer_H3Lt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16689/Reviewer_H3Lt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234837354, "cdate": 1762234837354, "tmdate": 1762926742391, "mdate": 1762926742391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of vanishing gradients, discretization errors, and high training cost of Differentiable logic gate networks (DLGNs). The authors claimed to root cause the issues and proposed a reparametrization solution to resolve it. Redundant parameters of input grates are replaced while maintaining the representability. With binary inputs, it can achieve 4x smaller model size, 1.86x backward pass speedup and 8.5x fewer training steps to converge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Less weight parameters compared w/ the original DLGN paper."}, "weaknesses": {"value": "DLGN seems to have a low test accuracy, which makes it less appealing as a practically useful solution."}, "questions": {"value": "It makes sense that light weight gate parameters help resolving the vanishing gradient issue. How is this  solution working on a larger data set like imagenet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "o93seFzVXM", "forum": "EaGQ5luZtf", "replyto": "EaGQ5luZtf", "signatures": ["ICLR.cc/2026/Conference/Submission16689/Reviewer_fw6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16689/Reviewer_fw6a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285147496, "cdate": 1762285147496, "tmdate": 1762926741706, "mdate": 1762926741706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}