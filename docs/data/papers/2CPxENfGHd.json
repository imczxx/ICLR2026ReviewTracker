{"id": "2CPxENfGHd", "number": 19155, "cdate": 1758293930091, "mdate": 1759897056153, "content": {"title": "Mitigating Discretization Bias in Neural Stochastic Differential Equations via Inference-Time Dropout", "abstract": "Neural stochastic differential equations (Neural SDEs) provide a principled framework for modeling complex, continuous-time dynamics by combining deep learning with Itô calculus. However, when the drift and diffusion functions are nonlinear, the common practice of substituting the noise process with a Gaussian distribution introduces non-negligible approximation errors. We show that these errors are not merely technical but fundamental, as they can directly cause failures in fitting certain classes of stochastic distributions. To address this issue, we propose the Uncertainty-Aware Neural SDE (UA-NSDE) framework, which leverages inference-time dropout to approximate an implicit uncertainty distribution. By maintaining dropout during both training and inference, UA-NSDE reduces discretization bias without imposing restrictive parametric assumptions such as Gaussianity. Empirical evaluations across synthetic and real-world benchmarks demonstrate that our method achieves more accurate and robust modeling", "tldr": "", "keywords": ["Neural stochastic differential equations", "discretization error", "inference-time dropout", "uncertainty estimation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5ab6af7002e4419e37753dfe7243a651eb74555.pdf", "supplementary_material": "/attachment/226248c8c507bb94b4993315ebebd6260863b8d4.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses discretization bias in Neural Stochastic Differential Equations (NSDEs), particularly those using the Euler-Maruyama (EM) scheme. The authors argue this bias accumulates and harms inference performance. The paper proposes Inference-Time Dropout (ITD), a method that activates dropout during the inference phase."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly articulates the problem of discretization bias in NSDEs, an important and often-overlooked practical issue in the field. The proposed ITD method is lightweight, and requires no model retraining or architectural changes, giving it high potential for adoption by practitioners.\n\n- The method demonstrates consistent and significant improvements on end-user metrics (e.g., MMD, NLL, MSE) across a variety of synthetic and real-world datasets.\n\n- The paper provides a formal theorem (Theorem 3.4) establishing that dropout-induced randomness can universally approximate conditional output distributions, which is a non-trivial theoretical insight."}, "weaknesses": {"value": "- The central claim that ITD mitigates discretization bias is not rigorously demonstrated. The provided theory (Theorem 3.4) concerns distributional expressivity, not SDE weak or strong convergence error. The paper must provide a direct theoretical link.\n\n- The empirical results, while showing large improvements (e.g., in MMD), are presented without confidence intervals, error bars, or significance testing. Given the stochastic nature of dropout, this omission makes it difficult to assess the robustness and reproducibility of the results.\n\n- Critical details of the ITD mechanism remain under-specified. The paper must clarify precisely how dropout masks are sampled (e.g., i.i.d. per step, fixed per path) and whether or how the injected noise is scaled to match diffusion scaling (e.g., proportional to $\\sqrt{\\Delta t}$)."}, "questions": {"value": "- Can you provide direct empirical evidence, such as a plot of weak error versus step size $\\Delta t$, that demonstrates ITD reduces SDE discretization bias, rather than just improving downstream metrics?\n\n- Can you provide a more formal derivation linking the variance from dropout to the reduction of EM bias, beyond the qualitative argument about expressivity (Theorem 3.4)?\n\n- How is the injected noise $\\epsilon_\\theta(\\Delta t)$ scaled with the time step $\\Delta t$? Is its variance proportional to $\\sqrt{\\Delta t}$ to match the scaling of Brownian motion increments?\n\n- What is the computational overhead of ITD? How many forward passes (samples) are required at inference to achieve the reported results, and how sensitive is performance to this number?\n\n- The paper claims to improve uncertainty modeling but provides no standard calibration metrics. Can you provide results for Expected Calibration Error (ECE) or other uncertainty quantification measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MY48Xfk9CO", "forum": "2CPxENfGHd", "replyto": "2CPxENfGHd", "signatures": ["ICLR.cc/2026/Conference/Submission19155/Reviewer_drzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19155/Reviewer_drzD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762510292, "cdate": 1761762510292, "tmdate": 1762931166289, "mdate": 1762931166289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of discretization bias in Neural Stochastic Differential Equations (NSDEs) — a systematic error introduced when continuous-time SDEs are numerically approximated via the Euler–Maruyama (EM) scheme. To address this issue, the paper proposes Inference-Time Dropout (ITD) — a simple yet effective mechanism that reinterprets dropout as a stochastic correction layer at inference. Instead of using Gaussian noise increments, the model replaces them with neural-network-generated random vectors with fixed dropout masks. This approach effectively allows the network to learn and sample more complex, non-Gaussian noise structures, potentially compensating for EM-induced bias."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "ITD is lightweight, easy to integrate into existing NSDE frameworks, and does not significantly alter model complexity or training pipelines. Empirical evaluation on synthetic and real-world datasets shows consistent improvements in metrics."}, "weaknesses": {"value": "1. Limited theoretical novelty.\n  * Theorem 3.1 restates a classical result on EM convergence (see [1]) and should be properly cited rather than presented as new.\n  * Theorem 3.4 follows directly from the universal approximation theorem for ReLU networks; its contribution to the novelty is limited.\n2. Lack of analysis of improvement.\nAlthough empirical results show strong gains, the paper does not isolate why ITD helps:\n  * Is the improvement due to mitigating discretization bias specifically?\n  * Or does the additional noise simply improve optimization or model expressivity?\n3. Insufficient exploration of related directions.\nThe connection to Mixture-of-Experts (MoE) and Gaussian Mixtures is conceptually intriguing but lacks empirical substantiation. No experiments explicitly test these analogies.\n4. Incomplete baselines.\nMany comparisons are made against earlier NSDE and ODE models, while recent developments in neural SDEs and diffusion processes are missing.\nRecommended additional baselines:\n  * Higher-order or adaptive discretization methods from Table 7.\n  * Modern neural architectures: ContiFormer (2024), Stable Neural SDEs [2, 3].\n  * Non-Brownian or fractional noise models [4–6].\n\n  ITD introduces additional trainable components; therefore, comparisons should include the number of parameters and training and inference time to properly estimate the tradeoff between quality and complexity of the approach.\n\n5. Ambiguity in discretization bias mitigation.\nDespite claims that ITD reduces discretization bias, no quantitative metric of bias reduction (e.g., error vs. step size) is presented. Improvements in prediction and uncertainty metrics might reflect better fitting capacity rather than bias correction per se.\n6. Details on the MNIST experiment (Figure 3) are missing.\n\n[1] https://urbain.vaes.uk/static/teaching/lectures/build/lectures-w6.pdf\n\n[2] ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling\n\n[3] Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data\n\n[4] Learning Fractional White Noises in Neural Stochastic Differential Equations\n\n[5] Fractional SDE-Net: Generation of Time Series Data with Long-term Memory\n\n[6] Variational Inference for SDEs Driven by Fractional Noise"}, "questions": {"value": "1. Does the improvement persist when drift and diffusion are fixed to ground-truth dynamics? \n2. How does ITD behave under very small time steps (Δ → 0)? Does the noise distribution converge to Brownian increments, or does the model maintain a persistent bias?\n3. What is the computational overhead of ITD during training and inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d3G1Q5yyZb", "forum": "2CPxENfGHd", "replyto": "2CPxENfGHd", "signatures": ["ICLR.cc/2026/Conference/Submission19155/Reviewer_tWgP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19155/Reviewer_tWgP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817409465, "cdate": 1761817409465, "tmdate": 1762931165987, "mdate": 1762931165987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents an alternative approach to simulate from a Neural Stochastic Differential Equation. Instead of an Euler simulation, the proposed approach replaces the iid Gaussian increment with an increment output by neural networks with dropout enabled at inference time. An analysis of the error of the Euler simulation is presented, along with a distribution-wise approximation theorem for neural networks with dropout."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Theorem 3.4, if true, is an interesting observation that neural networks can not only approximate a deterministic map but also approximate a conditional distribution by allowing some components to be random. This is a novel contribution to me."}, "weaknesses": {"value": "While the central theorem (Thm 3.4) does seem to be interesting, I don't think its impact and applicability are properly conveyed by applying it to NSDE. \n\n1. This work does not offer any guarantee that the incremental distribution output by the IDT will lead to a Brownian motion $W_t$.  Hence, the processes generated by NSDE with IDT will potentially not have the law of an Ito's SDE. If the authors would like to show that IDT can outperform Euler, I'd say a bound similar to Theorem 3.1, but for IDT with a better rate, would be necessary. Overall, the process driven by IDT should be $$X_t = f(X_t, t) d t + g(X_t, t) d Y_t$$ where $Y_t$ is another process directly approximated by IDT. Yet, this is beyond the scope of Ito's SDE, and I believe it has the name \"Rough Stochastic Differential Equations\".\n\n\n\n\n2. This work also misses a lot of critical citations. Here, I provide only a few examples, rather than an exhaustive list. For example, in the proof \"the universal approximation theorem for ReLU networks\" is used but never stated, proved, or cited. There are also no citations provided for the datasets. There is a question mark around line 165 indicating a missing citation. Terms like DGP, maximum mean discrepancy (MMD), and jump processes are not defined or cited. Moreover, some citations seem to be mismatched with the paper. For example, around line 253, \"...GAN-SDE (Li et al., 2020),\", but as far as I can understand, Li et al., 2020 is not on GAN-SDE but proposes a generalization of the adjoint method to NSDE to reduce the memory footprint of NSDE backpropagation. \n\n\n\n\n3. Some recent works in NSDE are not covered in the literature review. On top of my head, at least [1, 2, 3, 4] (not an exhaustive list) are missing.\n\n\n\n\n4. The methods applied in experiments are too old. As far as I'm aware, the [2, 3, 5, 6] are considered SOTA (either in performance or in efficiency) and open-sourced, and there is an implementation of [4] provided in [6]. Also, given that the paper is trying to reduce the numerical error, an interesting baseline would be higher-order solvers for SDEs like Milstein.\n\n\n\n\n5. There is no information on the real-world datasets used in experiments. From my perspective, Beijing Air Quality is not an appropriate choice for NSDE benchmark because it only has a length of 24 according to [5]. The horizon is too short to test the capacity of modern deep learning models. (In case the authors are looking for better datasets, the stock prices are publicly available, can have a very long horizon if you sample frequently, and they're known to be best modeled by SDEs.)\n\n\n\n\n6. It is not clear to me how NSDE, presented in the form of this paper, applies to reconstruction and prediction (while neither  task is precisely defined in the paper), and why MSE and NLL are reasonable measures of performance. NSDE is a generative model for temporal data and matches the distribution of the training samples. Thus, I think it would be more reasonable to make distribution-wise comparisons than pathwise comparisons, $i.e.$, it would be more reasonable to compare the law of samples generated by NSDE against the law of real data, or at least the marginal distribution of generated samples vs real data. \n\n\n\n\n\n\n## References:\n[1]  Liu, Y.-J., Lu, M., Nock, M. K., & Yacoby, Y. (2025). Neural stochastic differential equations on compact state-spaces. arXiv preprint arXiv:2508.17090. https://arxiv.org/abs/2508.17090\n\n[2] Zhang, J., Viktorov, J., Jung, D., & Pitler, E. (2025). Efficient training of neural stochastic differential equations by matching finite dimensional distributions. arXiv preprint arXiv:2410.03973. https://arxiv.org/abs/2410.03973\n\n[3] Snow, L., & Krishnamurthy, V. (2025). Efficient neural SDE training using Wiener-space cubature. arXiv preprint arXiv:2502.12395. https://arxiv.org/abs/2502.12395\n\n[4]  Bonnier, P., & Oberhauser, H. (2024). Proper scoring rules, gradients, divergences, and entropies for paths and time series. Bayesian Analysis, 1-32. https://doi.org/10.1214/24-BA1435\n\n[5]  Kidger, P., Foster, J., Li, X., Oberhauser, H., & Lyons, T. (2021). Neural SDEs as infinite-dimensional GANs. arXiv preprint arXiv:2102.03657. https://arxiv.org/abs/2102.03657\n\n[6] Issa et al. (2023) Issa, Z., Horvath, B., Lemercier, M., & Salvi, C. (2023). Non-adversarial training of neural SDEs with signature kernel scores. In Advances in neural information processing systems (Vol. 36, pp. 11102–11126)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LTDMgdZsVV", "forum": "2CPxENfGHd", "replyto": "2CPxENfGHd", "signatures": ["ICLR.cc/2026/Conference/Submission19155/Reviewer_tFtT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19155/Reviewer_tFtT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874237938, "cdate": 1761874237938, "tmdate": 1762931165516, "mdate": 1762931165516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel inference-time mechanism, Inference-Time Dropout (ITD), designed as a data-driven stochastic correction to reduce discretization bias. Instead of relying on the standard Gaussian increment, ITD replaces it with a learned noise process parameterized by a neural network with fixed dropout masks. Theoretical analysis (Theorem 3.4) claims ITD can act as a universal conditional distribution approximator, while empirical studies across synthetic and real-world datasets suggest performance improvements compared to standard EM-based Neural SDEs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Relevance**: The paper highlights an important and under-discussed issue, discretization bias, within Neural SDE frameworks. Although this bias is well-known in numerical analysis, its practical impact in Neural SDEs has been rarely studied.\n\n2. **Empirical performance**: Empirical results show that ITD-based models outperform baselines in certain scenarios. This suggests that the learned noise can capture distributional properties that conventional EM solvers miss.\n\n3. **Conceptual novelty**: It seems that the idea of learning a non-Gaussian, structured noise process at inference time is novely."}, "weaknesses": {"value": "1. **Unclear mechanism of ITD**: \n\n- The paper immediately introduces Inference-Time Dropout (ITD) but does not clearly explain its mechanism. It states that dropout masks are maintained during inference, yet it remains unclear how dropout is actually applied whether it is implemented within the drift and/or diffusion networks of the Neural SDE.\n- The definition of the key component $\\epsilon_\\theta$ is vague. \n\n\n2. **Core conceptual disconnect**: \nThe core message of the paper is ambiguous. While ITD claims to mitigate discretization bias, the main content does not provide a clear explanation or theoretical basis for such an effect.  The fact that the learned noise process $\\epsilon_\\theta$ can model an arbitrary noise distribution is not equivalent to reducing discretization bias. In particular, Theorem 3.4 merely discusses the universal approximation property of ReLU networks, which is conceptually disconnected from the claimed contribution of ITD.\n\n3. **Thereotical irrelevance of Theorem 3.4**: Theorem 3.4 only shows that dropout-based networks can approximate conditional distributions under Wasserstein distance, which is unrelated to discretization bias or convergence rates in SDE solvers. I believe what is required is a theorem showing that ITD yields a better weak/strong order of accuracy compared to EM.\n\n4. **Inappropriate experimenta ldesign**: \n- To validate “bias mitigation,” experiments should show how error scales with step size $\\Delta$. The paper uses a fixed $\\Delta$ compares model performance, making it impossible to separate bias reduction from mere overparameterization or regularization effects.\n\n- It is unlear why the MNIST generate experiment is related to the purpose of this work. \n\n5. Thereom 3.1 is a well-established standard fact. I think citing a proper reference is enough."}, "questions": {"value": "Please see the weaknesses and the following questions:\n\n1. How dropout is actually applied? Is it implemented within the drift and/or dfifusion networks of the Neura SDE?\n\n2. Could you specify how $\\epsilon_\\theta$ is formulated and how the parameters $\\theta$ are trained\n\n3. Please report the performance with respect to varying step size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qSywQ6VdKY", "forum": "2CPxENfGHd", "replyto": "2CPxENfGHd", "signatures": ["ICLR.cc/2026/Conference/Submission19155/Reviewer_K1EV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19155/Reviewer_K1EV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883161471, "cdate": 1761883161471, "tmdate": 1762931164795, "mdate": 1762931164795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}