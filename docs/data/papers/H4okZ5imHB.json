{"id": "H4okZ5imHB", "number": 21524, "cdate": 1758318513597, "mdate": 1763696535578, "content": {"title": "Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated Learning", "abstract": "Split Federated Learning (SFL) enables collaborative training between resource-constrained edge devices and a compute-rich server by partitioning deep neural networks. Communication overhead is a central issue in SFL and is well mitigated with auxiliary networks; yet the core client-side computation challenge remains, as back-propagation requires substantial memory and computation costs, severely limiting the scale of models that edge devices can support. To make the client side more resource-efficient, we propose HERON-SFL, a novel hybrid optimization framework that integrates zeroth-order (ZO) optimization for local client training while retaining first-order (FO) optimization on the server. With the assistance of auxiliary networks, ZO updates enable clients to approximate local gradients using perturbed forward-only evaluations per step, eliminating memory-intensive activation caching and avoiding explicit gradient computation in the traditional training process. Leveraging the low effective rank assumption, we theoretically prove that HERON-SFL's convergence rate is independent of model dimensionality, addressing a key scalability concern common to ZO algorithms. Empirically, on ResNet training and large language model (LLM) fine-tuning tasks, HERON-SFL matches benchmark accuracy while reducing client peak memory by up to 64\\% and client-side compute cost by up to 33\\% per step, substantially expanding the range of models that can be trained or adapted on resource-limited devices.", "tldr": "", "keywords": ["federated split learning", "resource-efficient split learning", "zeroth-order optimization", "low rank", "fine-tuning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d99f10d6c3abfc6eeb02105daaea5d7c5bb95f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework that enhances Split Federated Learning (SFL) by combining zeroth-order (ZO) and first-order (FO) optimization methods. Traditional SFL suffers from high computational and memory demands on client devices due to backpropagation. HERON-SFL addresses this by using lightweight ZO optimization on the client side, while retaining FO optimization on the server. This hybrid approach eliminates costly backpropagation on edge devices, substantially reducing client-side memory and computation without sacrificing model accuracy. Theoretical analysis shows that HERON-SFL achieves convergence rates independent of model dimensionality under a low effective-rank assumption. Experiments on ResNet training and GPT-2 fine-tuning tasks confirm that HERON-SFL matches benchmark accuracy while greatly improving efficiency, enabling large-scale model training on resource-constrained devices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Introduces HERON-SFL, the hybrid zeroth-order (ZO) and first-order (FO) optimization framework for Split Federated Learning (SFL). This hybridization smartly leverages ZO for clients (forward-only computation) and FO for servers (precise gradient updates), balancing computational efficiency and accuracy.\n\n+ Provides a rigorous convergence analysis for HERON-SFL. Shows that under a low effective-rank assumption, the convergence rate becomes independent of model dimensionality, overcoming a major limitation of traditional ZO methods.\n\n+ Validated on both vision (ResNet on CIFAR-10) and language (GPT-2 fine-tuning on E2E dataset) tasks. Matches or surpasses the accuracy of SFL baselines (e.g., FSL-SAGE, CSE-FSL) with substantially lower resource use."}, "weaknesses": {"value": "- The experiments focus on ResNet-18 (CIFAR-10) and GPT-2 (E2E dataset) — both relatively moderate-scale tasks. There is no evaluation on truly large foundation models (e.g., GPT-3-scale or ViT-level networks) where the claimed scalability advantages would be more convincing.\n- Client-device heterogeneity (e.g., varying compute or network speeds) is not experimentally explored, which is critical in federated settings.\n- Although communication costs are analyzed, real-world communication latency, bandwidth constraints, and asynchrony are not simulated. No analysis of packet loss, network delay, or intermittent connectivity, which are common in federated edge systems.\n- While the paper claims that ZO variance is mitigated by server-side FO refinement, it lacks a quantitative analysis of gradient noise or bias across training. ZO methods are inherently noisy and can be sensitive to perturbation size, but the paper does not offer sensitivity or ablation studies on this hyperparameter. Also, from Fig. 2, the convergence curve of HERON-SFL under non-IID case is not stable, indicating that ZO variance still has negative impact on the convergence of HERON-SFL. High-variance updates could lead to instability, particularly in non-IID or large-scale client distributions, but this is not deeply examined.\n- Theoretical convergence independence from model dimensionality hinges on the “low effective rank” assumption, which may not hold universally — especially in deep or overparameterized models. The paper does not empirically validate whether this assumption is satisfied in practice for models like GPT-2 or ResNet-18.\n- The paper does not compare with recent SFL paper [R1], which proposes a dynamic tiering approach for SFL to address the computation and communication challenges in SFL. Also, the paper compares only with SFL-based baselines, and comparisons with other related FL methods such as those cited in the paper are not provided.\n\n[R1] Mohammadabadi, Seyed Mahmoud Sajjadi, Syed Zawad, Feng Yan, and Lei Yang. \"Speed up federated learning in heterogeneous environments: a dynamic tiering approach.\" IEEE Internet of Things Journal (2024).\n\n- The paper does not clearly specify client compute capabilities, memory constraints, or network parameters, making it difficult to gauge real-world feasibility. This lack of transparency weakens the empirical section’s reproducibility and external validity.\n- Although some non-IID experiments are reported, there is no study on extreme heterogeneity, client dropouts, or adversarial behaviors in real SFL deployments. Security and privacy implications of zeroth-order updates (which could leak model outputs) are not discussed."}, "questions": {"value": "1. Can HERON-SFL scale to large models like GPT-3 or ViT while maintaining efficiency?\n2. How does HERON-SFL handle clients with different compute power or network speeds?\n3. How does the system perform under real-world communication network (with latency, bandwidth limits, packet loss)?\n4. What quantitative evidence supports that FO refinement reduces ZO variance? How sensitive is HERON-SFL to perturbation size ($\\mu$)?\n5. The experiments use only 3 to 5 clients, which is far smaller than typical federated setups involving tens or hundreds of participants. How does HERON-SFL scale as the number of clients increases?\n6. How does HERON-SFL perform under various non-IID, dropout, or adversarial conditions? Could ZO updates leak information through activation outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wCXl2YQwwz", "forum": "H4okZ5imHB", "replyto": "H4okZ5imHB", "signatures": ["ICLR.cc/2026/Conference/Submission21524/Reviewer_7LFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21524/Reviewer_7LFG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504229899, "cdate": 1761504229899, "tmdate": 1762941818569, "mdate": 1762941818569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hybrid approach for split federated learning, in which the server performs first-order optimization while the clients employ zeroth-order optimization to reduce peak memory usage and computational cost. The paper theoretically proves the convergence of the proposed method and empirically demonstrates that it achieves comparable performance to the baselines while offering significant savings in peak memory usage and computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of reducing memory and computation costs through zeroth-order optimization on the client side, while compensating for it with precise first-order optimization on the server side, is convincing.\n* The paper provides evidence, both experimental and theoretical, that the proposed hybrid approach can effectively reduce memory and computation costs without causing performance degradation.\n* The paper is well written and easy to follow."}, "weaknesses": {"value": "**The importance of client-side training.**  I believe that the main reason the proposed hybrid optimization does not lead to a notable performance drop is that most of the learning still happens on the server using first-order optimization, while the client side primarily serves to “smash” the data for privacy. Because of this, it is difficult to clearly separate whether the benefit comes from the effectiveness of zeroth-order optimization itself, or simply from the fact that training the client-side model is not that important. For example, it would be helpful to include a comparison experiment between (i) freezing the client-side (encoder) part of a pretrained model and training only the server side, and (ii) applying zeroth-order optimization on the client side. This would clarify the role and necessity of client-side optimization.\n\n**Communication may still remain the primary bottleneck.**  Based on the comparison of communication cost and FLOPs in Table 2, it appears that the main bottleneck lies in communication rather than computation. Although zeroth-order optimization indeed reduces the burden of client-side updates, it is questionable whether client-side updates can truly be considered the bottleneck in the context of split federated learning.\n\n**The experimental setting is too limited.**  Although the paper aims to reduce memory and computation costs for resource-constrained edge devices, using only 3 or 5 clients in a cross-silo setting is insufficient to validate this objective. Please refer to the detailed questions provided in the Questions below."}, "questions": {"value": "* Would there be any difference in performance when the number of clients increases, each holding a smaller amount of data, or when the batch size becomes smaller?\n* What is the effect of partial participation compared to full participation?\n* What happens when the data distribution becomes more non-IID, for example, when the α value in the Dirichlet distribution is reduced below 1?\n* If the client-side model becomes larger, would the disadvantages of zeroth-order optimization become more significant?\n* As I understand it, the performance of first-order optimization should serve as the upper bound for that of zeroth-order optimization. However, in Figure 3 and similar results, zeroth-order optimization sometimes performs even better. Why might that be the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E49MLsH9EZ", "forum": "H4okZ5imHB", "replyto": "H4okZ5imHB", "signatures": ["ICLR.cc/2026/Conference/Submission21524/Reviewer_22nq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21524/Reviewer_22nq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635523556, "cdate": 1761635523556, "tmdate": 1762941818320, "mdate": 1762941818320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the issue of limited system resources at the client-side by developing a new split learning + federated learning framework which combines first-order and zeroth-order optimizations. Specifically, the clients update local models using zeroth-order method while the server updates the assigned output-side layers using the first-order method. In addition, the authors employ auxiliary models at the client-side so that the local models can proceed to the backpropagation without waiting for the server to propagate the error back to the individual clients. The convergence properties were analyzed based on relatively strong assumptions. Then, extensive empirical study demonstrates its superior FLOPS-to-target accuracy compared to other FSL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical analysis covers iid and non-iid together and also consider the specific split + federated learning settings. This thorough analysis framework will provide readership with a useful starting point of analyzing other algorithms.\n\n2. Client-side resource consumptions are thoroughly analyzed (section 4.2), which clearly shows the benefits from the proposed method.\n\n3. Experiments are fairly extensive covering training from scratch as well as fine-tuning."}, "weaknesses": {"value": "While I see mostly valueable contributions, still I find some limitations as follows.\n\n1. [**Relative performance gain against conventional FL and conventional SplitLearning**] While the theoretical analysis and empirical results demonstrate the efficacy of the proposed method, I am not sure whether it consumes less resources than conventional FL to achieve the same target accuracy. What if the zeroth-order method is applied to a few input-side layers and the first-order method to the rest of the layers? I recommend more thoroughly justifying the proposed framework because it is basically a combination of two existing distributed learning paradigms.\n\n2. [**Latency analysis**] In general, split learning methods suffer from frequent communications (latency cost). Even though the proposed method has fewer communications than the typical split learning methods, thanks to the combination of FL and the proposed auxiliary model, I guess it will still have some latency cost issues. Table 2 only considers bandwidth consumptions. I recommend providing an additional analysis of communication counts.\n\n3. [**Unrealistic number of clients**] The authors use 3~5 clients in the experiments. I believe this is seriously misaligned with the concept of federated learning which is a large-scale distributed learning paradigm. Most federated learning studies use at least 32$\\sim$64 clients and many papers use even larger numbers of clients like 128 to 1024. Only with 3 or 5 clients, the whole empirical results do not well support the efficacy of the proposed method. Will the proposed method work well with 128 clients?\n\n4. [**Strong assumptions**] I appreciate the extensive and thorough theoretical analysis provided in Section 4. However, the assumptions are relatively stronger than recent FL studies. E.g., many studies rely on either assumption 2 or 3, not on both of them. In addition, assumption 6 implicitly indicates that the high effective rank of model parameters potentially harms the convergence rate. This is a worrisome property because the higher the effective rank, the stronger the representation capacity of the model. Could authors provide more detailed discussions regarding the meaning of this assumption?\n\nOverall, I think this study provides meaningful and useful insights to Split Learning or Federated Learning researchers. However, due to the above critical limitations, I cannot give a positive score for now. I will re-evaluate the paper after the authors' rebuttal."}, "questions": {"value": "My questions are provided in the above weakness section. Please carefully address them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "MZWHIy0m6u", "forum": "H4okZ5imHB", "replyto": "H4okZ5imHB", "signatures": ["ICLR.cc/2026/Conference/Submission21524/Reviewer_FesG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21524/Reviewer_FesG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679348668, "cdate": 1761679348668, "tmdate": 1762941818047, "mdate": 1762941818047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for reducing computation and communication overhead in split learning while preserving model accuracy. The key idea is to enable clients to train local models with new auxiliary components so that the clients can keep doing the local training without waiting for the backpropagation results from the server. Server does receive the smashed activations from clients, but not for every forward pass in clients."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets a highly relevant challenge of how to scale federated training to resource-limited clients without sacrificing global model quality. The proposed method preserves split learning’s efficiency benefits (clients train partial models) while avoiding its synchronization and overheads. The experiments are broad."}, "weaknesses": {"value": "1. The idea is very similar to FedGKT [1], which has not been compared with. There are many works that are follow ups of FedGKT, the authors need to cover some of the recent ones in their comparisons.\n2. The smashed activations can leak data privacy. It has to be experimentally demonstrated how the proposed scheme is robust to model inversion and other attacks.\n3. While not directly FL, there is another recent work that seeks to train small models at clients with support from server by offloading some intermediate activations with guaranteed DP privacy [2]. It seems the current paper is an extension of [2]. Instead of auxiliary network, [2] directly creates a smaller model for the user. I think that is a more principled approach for extension to FL. \n\n[1] Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge (NIPS 2020).\n[2] All Rivers Run to the Sea: Private Learning with Asymmetric Flows (CVPR 2024)."}, "questions": {"value": "Please addresses the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YK00mGLsbx", "forum": "H4okZ5imHB", "replyto": "H4okZ5imHB", "signatures": ["ICLR.cc/2026/Conference/Submission21524/Reviewer_n2hD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21524/Reviewer_n2hD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907876365, "cdate": 1761907876365, "tmdate": 1762941817595, "mdate": 1762941817595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HERON-SFL, a Split Federated Learning framework that uses zeroth-order (ZO) optimization for client-side updates and first-order (FO) optimization on the server side. This hybrid approach reduces client-side memory and computational costs by eliminating backpropagation and activation caching. Empirical results show significant reductions in client memory and computational cost while maintaining comparable accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Using ZO optimization on the client side show significant resource savings (up to 64% memory and 65% computation) with comparable accuracy, making it suitable for resource-constrained devices."}, "weaknesses": {"value": "1.\tThe main contribution of HERON-SFL is an incremental update to Han et al.'s local-loss-based split learning. In Han et al., clients perform local updates using their own auxiliary model eliminating global backprop. HERON-SFL simply swaps the FO updates on the client-side with ZO updates while keeping the server-side optimization FO-based.\n2.\tThe core idea, that ZO optimization can replace FO optimization for memory reduction, is not new, as ZO optimization has been explored extensively before.\n3.\tWhile ZO optimization can reduce memory usage, it introduces several new hyperparameters (perturbation size µ, number of probes etc), which can make tuning more complex. The sensitivity of ZO to these hyperparameters is a critical issue which is not addressed or ablated in this paper.\n4.\tZO optimization is typically slow and suffers from higher variance which suggest client-side may face unstable updates. While the FO server-side optimization may mitigate this to some extent in the current experiments, the instability inherent to ZO methods is a known issue which is not clarified.\n5.\tThe paper provides theoretical convergence guarantees, but relies on low effective rank assumption, which may not always hold, especially for high-dimensional models.\n6.\tThe empirical experiments are limited to ResNet-18 (CIFAR-10) and GPT-2 fine-tuning with relatively small numbers of clients (N=5 for CIFAR-10 and N=3 for GPT-2). These do not demonstrate the scalability of HERON-SFL to larger models, larger datasets and more clients."}, "questions": {"value": "1.\tCould you clarify how many ZO probes per mini-batch/step were used in the experiments, and how the communication cost was accounted for when these probes are performed?\n2.\tIn Table 2, the peak memory claims for HERON-SFL seem to suggest an O(1) memory model—can you provide a more detailed breakdown of how memory is measured (e.g., parameters, activations, optimizer states)? Can you compared this cost to Han et. al.’s method?\n3.\tThe resource cost analysis in Table 1 does not correspond well with the consumptions reported in Table 2 and Table 3. The flops count of HERON-SFL is expected to be 2/3rd to that of FSL_SAGE but is 3 times less in Table 2. Can you discuss why this discrepancy exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fy172nDAVm", "forum": "H4okZ5imHB", "replyto": "H4okZ5imHB", "signatures": ["ICLR.cc/2026/Conference/Submission21524/Reviewer_EnDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21524/Reviewer_EnDt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982298423, "cdate": 1761982298423, "tmdate": 1762941817129, "mdate": 1762941817129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}