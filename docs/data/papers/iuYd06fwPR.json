{"id": "iuYd06fwPR", "number": 11654, "cdate": 1758202832496, "mdate": 1759897562484, "content": {"title": "SelfCAD: Protecting Your Efficient Reasoning Capabilities via Self Cautious Insertion", "abstract": "Large reasoning models (LRMs) are increasingly deployed in modern AI systems due to their accuracy, efficiency, and transparency, as their reasoning traces enable users and auditors to interpret model outputs. \nHowever, publishing these traces introduces new risks. \nAdversaries may distill them to replicate efficient LRMs for their own purpose or build proxy models for malicious attacks, raising both copyright and security concerns that threaten the sustainability of the LLM ecosystem.\nExisting defenses mainly detect distillation after violations occur or suppress transparency by masking or rewriting reasoning traces, which are impractical in real-world deployments. \nIn this work, we propose a defense framework that preserves reasoning traces while preventing effective distillation. \nWe begin with a systematic analysis of how different reasoning components affect model efficiency and accuracy. \nOur results reveal that the number of self-cautious sentences plays a crucial role: excessive self-cautious sentences lead to redundant outputs, while insufficient ones harm accuracy. \nBuilding on this insight, we propose $\\textbf{SelfCAD (Self-Cautious Anti-Distillation)}$, a lightweight anti-distillation method that strategically manipulates self-cautious parts after models generate their reasoning traces. \nSelfCAD maintains the semantic clarity of reasoning traces for human users and LLM auditors, but significantly degrades the efficiency and accuracy of the downstream distilled models. \nExperiments on Llama and Qwen show that distilled models incur higher inference cost and lower accuracy, especially for Qwen-1.5B, whose token length is $4.8\\times$ longer on GSM8K after distillation with our processed responses compared with distillation with vanilla responses. \nThe results highlight a new efficiency-based perspective on safeguarding reasoning models from distillation while preserving interpretability.", "tldr": "", "keywords": ["Anti-distillation", "Reasoning model", "Copyright protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44fd19cad53d797ed1f6872eb563c86405993440.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SelfCAD (Self-Cautious Anti-Distillation), a method to protect large reasoning models (LRMs) from unauthorized distillation. The key insight is that self-cautious sentences in reasoning traces significantly affect both efficiency and accuracy of distilled models. The authors propose inserting additional self-cautious sentences after generation to make distilled models inefficient (producing 1.2-4.8× longer outputs) while preserving semantic clarity for legitimate users."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel perspective: Identifying self-cautious sentences as a key factor in reasoning efficiency is an interesting observation\n2. Lightweight implementation: Inference-time processing without model modification is practical\n3.Comprehensive experiments: Testing across multiple models and datasets shows consistency\n4. Theoretical grounding: Provides mathematical analysis explaining the mechanism\n5. Timely problem: Addresses important concerns about LLM intellectual property"}, "weaknesses": {"value": "1. No adaptive attack evaluation: Doesn't test against adversaries who might detect the pattern\n2. Strong assumptions: Theoretical analysis assumes distributions remain stable (Eq. 4) without justification\n3. Presentation issues: Poor writing quality, structural problems, missing implementation details"}, "questions": {"value": "1. Robustness to preprocessing: How does SelfCAD perform when adversaries use simple regex or pattern matching to remove the inserted sentences before distillation?\n\n2. Template variations: Have you tested different self-cautious templates or randomized insertions to make detection harder?\n\n3. Adaptive attacks: Can you evaluate against adversaries who train classifiers to detect artificially inserted vs. natural self-cautious sentences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0rKun7yQh6", "forum": "iuYd06fwPR", "replyto": "iuYd06fwPR", "signatures": ["ICLR.cc/2026/Conference/Submission11654/Reviewer_x2de"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11654/Reviewer_x2de"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611470549, "cdate": 1761611470549, "tmdate": 1762922716242, "mdate": 1762922716242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SelfCAD, a defense mechanism against unauthorized reasoning distillation. The key idea is to inject self-cautious sentences into chain-of-thought explanations—phrases expressing doubt or self-verification. These interventions increase the verbosity and reduce the efficiency of student models trained on the generated traces, while preserving the original model’s transparency and performance. Experiments on mathematical reasoning show that distillation from SelfCAD-protected data leads to significantly longer reasoning chains and modestly reduced accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple and low-cost deployment — does not require modifying the trainer or model architecture.\n\n2. Maintains transparency — unlike encryption or CoT suppression, users still see the reasoning process.\n\n3. Applies to real-world API scenarios — defense could plausibly be adopted by service providers immediately."}, "weaknesses": {"value": "The most important thing is that I think defense seems trivial to bypass. While the idea is intuitive, the core mechanism—adding self-cautious phrases into reasoning traces—appears easily removable. A defender with basic text-processing capability (e.g., filtering, paraphrasing, style normalization) could simply strip or rewrite these caution sentences before distillation. This raises a fundamental question:\n\nIf the protection can be removed by a simple text post-processing step, is the defense truly effective?\n\nAdditionally, because the defense operates purely at the surface-form level, an attacker could bypass it entirely by:\n\n• distilling from logits / token probabilities instead of CoT text\n\n• accessing hidden states directly (a common research practice)\n\n• supervised fine-tuning student confidence calibration back to normal\n\nThe paper would benefit from a more rigorous robustness evaluation, including adversarial distillation settings rather than only naïve student training pipelines. So far the method appears fragile under even basic threat models."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DctwJF64OK", "forum": "iuYd06fwPR", "replyto": "iuYd06fwPR", "signatures": ["ICLR.cc/2026/Conference/Submission11654/Reviewer_NMMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11654/Reviewer_NMMS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684190049, "cdate": 1761684190049, "tmdate": 1762922715697, "mdate": 1762922715697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **SelfCAD (Self-Cautious Anti-Distillation)**, a lightweight defense mechanism designed to protect reasoning-capable LLMs from unauthorized distillation while preserving transparency.  \nThe authors first perform a fine-grained analysis of reasoning trajectories, decomposing them into *statement*, *reasoning*, *self-cautious*, and *conclusion* sentences. They discover that self-cautious sentences—phrases like “wait” or “let me double-check”—strongly influence both the *efficiency* (output length) and *accuracy* of reasoning models.  \nLeveraging this insight, SelfCAD strategically inserts additional self-cautious sentences into reasoning traces. This manipulation keeps human readability intact but leads student models trained on these traces to produce **redundant and inefficient reasoning**, thereby degrading the effectiveness of model distillation.  \nExperiments across Llama-3.2-1B/3B and Qwen2.5-1.5B/7B show that distillation with SelfCAD-processed traces increases output length by 1.3–4.8× and reduces accuracy by 2–8% while maintaining over 99% semantic equivalence for humans.  \nThe paper offers a practical and creative perspective on proactive anti-distillation for reasoning models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Conceptual novelty:** Introduces a unique efficiency-oriented defense mechanism distinct from watermarking or audit-based approaches.  \n- **Lightweight and practical:** Can be applied at inference time without model retraining or architecture modification.  \n- **Empirical clarity:** Includes ablations and trajectory analyses that clearly illustrate the “self-cautious effect.”  \n- **Transparency preserved:** Maintains semantic fidelity while effectively degrading distillation efficiency.  \n- **Reproducibility:** Implementation details and evaluation settings are well documented."}, "weaknesses": {"value": "- **Shallow methodological contribution:** The proposed method essentially inserts fixed *self-cautious* sentences (e.g., “*Wait, let me check again…*”) after each reasoning step. There is no adaptive component, learning mechanism, or optimization objective. As a result, the core technique is heuristic and lacks algorithmic or theoretical depth.  \n- **Overstated novelty:** While the problem of protecting reasoning traces from distillation is timely and relevant, the proposed solution is minimal and does not introduce new principles beyond simple text augmentation. Most of the originality lies in the *problem framing* rather than the *technical approach*.  \n- **Lack of rigorous baselines:** The paper does not compare SelfCAD against simpler or more intuitive baselines—such as random phrase insertion, noise injection, or partial reasoning truncation—that could yield similar degradation effects.  \n- **Limited generalization:** All experiments focus solely on mathematical reasoning tasks. It remains unclear whether the proposed mechanism would transfer to other domains such as commonsense reasoning, logical inference, or code generation.  \n- **Weak theoretical justification:** The included theorem is more descriptive than analytical—it essentially formalizes an intuitive observation that adding self-cautious sentences encourages longer reasoning. No provable guarantees or quantitative bounds are provided.  \n- **Formatting and polish issues:** Several sections are overly bolded or inconsistently formatted, which negatively impacts readability. These should be fixed for the camera-ready version."}, "questions": {"value": "1. Since the core mechanism is inserting templated *“wait”* phrases, how does SelfCAD differ from random or semantically neutral text insertion? Could a trivial baseline achieve similar anti-distillation effects?  \n2. Have you evaluated whether paraphrasing or filtering the outputs before student training can remove these self-cautious tokens and thus bypass the defense?  \n3. Does the anti-distillation effect persist when the student model is fine-tuned for more epochs or trained with RLHF-based objectives instead of simple supervised distillation?  \n4. Beyond math reasoning, do the authors have evidence that SelfCAD generalizes to domains such as commonsense QA, coding, or multimodal reasoning?  \n5. Could a future version of SelfCAD learn *where* and *when* to insert self-cautious sentences adaptively, rather than applying them uniformly across all reasoning steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FA7je5vto7", "forum": "iuYd06fwPR", "replyto": "iuYd06fwPR", "signatures": ["ICLR.cc/2026/Conference/Submission11654/Reviewer_KNkw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11654/Reviewer_KNkw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969103666, "cdate": 1761969103666, "tmdate": 1762922715163, "mdate": 1762922715163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SelfCAD, a lightweight, inference-time defense that preserves transparent reasoning traces while degrading the effectiveness of unauthorized distillation. The key insight is that the number of self-cautious sentences in reasoning trajectories critically impacts both efficiency and accuracy in distilled models. SelfCAD post-processes teacher outputs by inserting self-cautious sentences after each reasoning step, keeping the original reasoning intact for human and LLM auditors but causing student models trained on these traces to become less confident and over-verbose. Analyses show self-cautious sentences strongly reduce sequence termination probability and lengthen trajectories; training-time studies confirm that removing self-cautious sentences shortens outputs while adding them increases length and can reduce accuracy. A simple theoretical model explains how repeated self-cautiousness induces excessive reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper identifies and isolates the role of self-cautious sentences in driving reasoning length and student confidence, providing both empirical and theoretical support for an efficiency-oriented protection mechanism.\n\n(2) SelfCAD is practical and minimally invasive: it operates as post-processing at inference time, requires no teacher fine-tuning, preserves original reasoning content, and can be run on CPU in minutes.\n\n(3) The methodology is clearly described, including sentence-type categorization, termination-probability analysis, a simple but principled theorem, and an explicit algorithm for insertion.\n\n(4) Experiments span multiple student sizes and two distillation sources, with consistent increases in inference cost and modest accuracy reductions; the stealthiness evaluation suggests preserved semantic transparency for users and auditors."}, "weaknesses": {"value": "(1) The insertion strategy is uniform and naive, applying the same self-cautious sentence after every step; more targeted placement tuned to correctness or step importance could strengthen effect or reduce accuracy loss, but is not explored.\n\n(2) The defense primarily targets text-reasoning math datasets; generalization to other domains and modalities, longer-context tasks, or instruction-heavy settings is not demonstrated.\n\n(3) Adversary adaptivity is not studied: a distiller could filter or downweight self-cautious spans, apply truncation, use RLHF to penalize over-caution, or use contrastive objectives to recover efficiency.\n\n(4) The semantic-equivalence “stealth” check uses LLM judges on a binary yes/no criterion; human studies or more granular equivalence metrics would better validate transparency preservation.\n\n(5) Theoretical assumptions (mixture model, stability to prior steps) simplify dynamics; empirical tests that vary $\\lambda$ and distributional separability would strengthen the causal link to observed length inflation.\n\n(6) Potential collateral effects on downstream evaluators or audit tools that rely on reasoning brevity or structure are not assessed."}, "questions": {"value": "(1) How robust is SelfCAD to adaptive distillers that strip or downweight self-cautious sentences, truncate intermediate steps, or apply RLHF to penalize excessive caution; can you report results under such countermeasures?\n\n(2) Can you design and evaluate a selective insertion policy that targets steps likely to be correct or pivotal, using lightweight heuristics or a small classifier, to maximize length inflation while minimizing accuracy degradation?\n\n(3) How does SelfCAD perform beyond math, for example, in symbolic logic, code reasoning, tool-use chains, or long-context QA, and does the effect size persist with longer contexts and different tokenizers?\n\n(4) Could you report human evaluation of transparency and usefulness, beyond LLM-judge equivalence, including perceived clarity, redundancy, and auditability of the modified traces?\n\n(5) What are the impacts on student training stability and compute cost during distillation, such as gradient variance, convergence speed, and GPU hours; can you quantify the additional cost imposed on the distiller?\n\n(6) Can you provide ablations on the content and style of self-cautious sentences, frequency of insertion, and position relative to sub-steps, to identify minimally invasive yet maximally effective variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3w0iowV2I1", "forum": "iuYd06fwPR", "replyto": "iuYd06fwPR", "signatures": ["ICLR.cc/2026/Conference/Submission11654/Reviewer_qVvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11654/Reviewer_qVvB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985858913, "cdate": 1761985858913, "tmdate": 1762922714717, "mdate": 1762922714717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}