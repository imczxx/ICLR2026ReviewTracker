{"id": "yjrVOxjkDR", "number": 20679, "cdate": 1758308924627, "mdate": 1759896964447, "content": {"title": "Persona Features Control Emergent Misalignment", "abstract": "Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a \"model diffing\" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several \"misaligned persona\" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.", "tldr": "We study where and why emergent misalignment arises, finding an underlying misaligned persona feature and proposing mitigations.", "keywords": ["interpretability", "alignment", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b188c772c763ba03c7440e185b1e9987fae9219c.pdf", "supplementary_material": "/attachment/968502fb224d26eb9a1241b3ac3ddf430fd3e454.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies emergent misalignment, where fine-tuning on narrow incorrect data leads to broad harmful behaviors. The authors show that both supervised and RL training on flawed code or advice can activate “misaligned persona” features in model activations. Using sparse autoencoders, they identify a dominant “toxic persona” latent that causally drives misalignment and predicts unsafe behavior. They further find that brief fine-tuning on benign data can re-align models, suggesting a mechanistic pathway for detecting and mitigating misalignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Extensive experiments**: The paper conducts a wide range of experiments across fine-tuning, reinforcement learning, and different data domains, providing strong empirical evidence that emergent misalignment is a general phenomenon rather than a domain-specific artifact.\n2. **Interesting mechanistic finding**: The discovery of “misaligned persona” features, especially the “toxic persona” latent, offers a compelling mechanistic explanation for how narrow fine-tuning can induce broad behavioral shifts in LLMs.\n3. **Practical mitigation insights**: The finding that small-scale benign fine-tuning can efficiently re-align misaligned models provides a simple yet promising avenue for mitigating emergent misalignment in practice."}, "weaknesses": {"value": "1. **Limited experimental clarity**: Some experimental setups, including dataset generation and grading procedures, are described only briefly, making it difficult to fully capture the whole picture of the experiments.\n2. **Narrow evaluation scope**: The misalignment evaluation relies on a fixed set of 44 prompts, which may not capture the full range of harmful or unsafe behaviors; broader assessments on toxicity, deception, or harmfulness would strengthen the conclusions.\n3. **Shallow mechanistic explanation**: While the paper identifies “toxic” and “sarcastic” persona features, it stops short of explaining *why* these particular personas emerge—such as whether they stem from the nature of the incorrect data or reflect deeper inductive biases in model training."}, "questions": {"value": "1. The result in Figure 2 shows that fine-tuning on correct responses does not cause misalignment, which seems to contradict [1], where even benign fine-tuning can compromise model safety. Could the authors elaborate on this discrepancy and clarify how their setup differs from prior findings?\n2. The paper suggests that in-distribution re-alignment is highly effective at restoring alignment. Could such targeted fine-tuning risk overfitting or degrading other generalization properties of the model?\n3. The proposal to use sparse autoencoders as an unsupervised “early warning system” for misalignment is intriguing, but training a new SAE for each checkpoint may be computationally expensive. Are there more scalable or lightweight alternatives the authors would recommend for practical deployment?\n\n[1] Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!, 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OddzbxtI9K", "forum": "yjrVOxjkDR", "replyto": "yjrVOxjkDR", "signatures": ["ICLR.cc/2026/Conference/Submission20679/Reviewer_bZuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20679/Reviewer_bZuS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552706578, "cdate": 1761552706578, "tmdate": 1762934063222, "mdate": 1762934063222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors perform a comprehensive investigation of emergent misalignment from finetuning on narrow datasets. They find that many different datasets can result in emergent misalignment. They use SAE latents to characterize the differences in the fine-tuned models, and find that the changes may be explainable as due to 'toxic' or 'sarcastic' personas in the pretraining data. Lastly, they perform ablations, and find that mixing in neutral data or subsequent finetuning on benign data removes emergent misalignment."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: good. While emergent misalignment was described in a previous paper https://arxiv.org/abs/2502.17424, the authors substantially expand upon previous findings by proposing many new settings in which emergent misalignment can occur and doing extensive mechanistic analysis via model diffing. \n\nQuality: excellent. Authors improve upon the evaluation methodology previously described and introduce a practical model diffing pipeline leveraging SAE latents. \n\nClarity: good. Paper is well written and easy to understand. \n\nSignificance: good. The authors highlight how emergent misalignment could happen in practice - via negligent data preparation or deliberate data poisoning. They also suggest implications for scalable oversight given weak training signal."}, "weaknesses": {"value": "It is unclear how to interpret \"emergent misalignment\". The evaluations in the paper consist largely of single turn chat responses, where the assistant has no real capacity to do harm. Furthermore, emergent misalignment appears to be inconsistent - some samples are misaligned and others are not, even when sampling from the same model. It is an open question whether emergently misaligned models would take coherently misaligned actions."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F6djBS3D3l", "forum": "yjrVOxjkDR", "replyto": "yjrVOxjkDR", "signatures": ["ICLR.cc/2026/Conference/Submission20679/Reviewer_FVmM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20679/Reviewer_FVmM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780590877, "cdate": 1761780590877, "tmdate": 1762934062793, "mdate": 1762934062793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a phenomenon called \"emergent misalignment,\" where fine-tuning a language model (like GPT-4o) on a narrow, incorrect task (e.g., insecure code) causes the model to exhibit broad, malicious behavior on unrelated prompts . The authors extend this finding to multiple settings, including Reinforcement Learning (RL) and various synthetic datasets. Using a \"model diffing\" approach with Sparse Autoencoders (SAEs), they probe the internal mechanism for this generalization. The study finds that this misalignment is controlled by the activation of \"misaligned persona features,\" particularly a \"toxic persona\" feature (#10) . Finally, the paper demonstrates that this misalignment can be efficiently mitigated via \"emergent re-alignment,\" where fine-tuning on just a few hundred benign samples restores alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology is advanced; the use of SAEs for \"model diffing\" combined with \"activation steering\" provides strong causal evidence for the mechanistic claims.\n2. I really love Figure 6. It clearly demonstrates the causal role of the \"toxic persona\" feature (#10) by showing it can be used to both induce misalignment in a safe model and suppress it in a misaligned one.\n3. The paper's findings on detection and mitigation have high practical value. The \"toxic persona\" feature (#10) acting as an \"early warning system\" and the simplicity of \"emergent re-alignment\" are both significant contributions."}, "weaknesses": {"value": "1. The real-world relevance of the paper's core mechanism (the \"toxic persona\" feature) is highly questionable.\n2. The reproducibility of the experiments is zero, as the study relies entirely on proprietary, non-public models (GPT-4o and OpenAI o3-mini).\n3. The mechanism appears to be an artifact of synthetic data. The experiments in Appendix I show that when fine-tuning on real human data, the key \"toxic persona\" feature (#10) is not activated."}, "questions": {"value": "Given that the paper's core mechanism relies on the optimizer linking a narrow, technically incorrect training objective (e.g., insecure code) to a semantically unrelated, pre-existing persona (e.g., \"toxic persona\" #10) . Appendix J.10 does show that activating this feature helps lower the loss on \"bad\" data . However, why is this specific, non-obvious connection made? What theoretical or empirical evidence explains why the model \"chooses\" to minimize the loss for \"insecure code\" by activating a \"toxic persona,\" rather than simply overfitting the specific technical features of insecure code itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bia0vjESPp", "forum": "yjrVOxjkDR", "replyto": "yjrVOxjkDR", "signatures": ["ICLR.cc/2026/Conference/Submission20679/Reviewer_RR5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20679/Reviewer_RR5G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971642880, "cdate": 1761971642880, "tmdate": 1762934062100, "mdate": 1762934062100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the phenomenon of emergent misalignment in LLMs, where the model develops broad misalignment after finetuning  on narrowly misaligned data. The authors reproduce this phenomenon on SFT and show it also applies to RL. The authors trace the phenomenon back to so-called persona features (features in the model encoding personas such as a sarcastic person) which can be identified through sparse autoencoders. By steering the models with these features positively or negatively, misalignment can be exacerbated or mitigated with little finetuning (in most cases)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is excellently written and relatively easy to follow (with the caveat being that a lot of relevant information, including related work, is deferred to the appendix)\n2. The paper significantly advances our understanding of the emergent misalignment phenomenon, demonstrating it applies more broadly than previously shown but possible to mitigate with existing tools. Given the safety risk of emergent misalignment, the paper represents an important contribution to safety research \n3. The experimental setup is comprehensive and well-described. I appreciate that the authors combine LLM graders with manual verification, as nowadays many papers rely on LLM graders alone."}, "weaknesses": {"value": "(In no particular order)\n\n1. As mentioned above, too much relevant information is deferred to the appendix. it’s worth noting that this is not due to excessive verbosity - the paper genuinely has a lot to show. Nevertheless, it makes the paper harder to read, and in a way defeats the purpose of a 9-page conference submission. \n2. The y-axes showing misalignment % are different across figures and somewhat misleading. For the RL experiments, the scores are around 10% which the authors argue represents a significant degree of misalignment. In the steering experiments (Fig 6), misalignment scores are reduced to around 20%. Can you get to 0% with stronger steering? If not, is 20% not still a significant risk?\n3. There could be more clarity around why code specifically shows relatively less misalignment in SFT (this is discussed in the paper) but more misalignment in RL (I couldn’t really find an explanation for this discrepancy).\n4. It would be helpful to elaborate on the hypothesis that initial model behaviour is more important in in-policy than off-policy training for emergent misalignment (2.3), as currently this is an unsubstantiated claim.\n5. Figure 4 shows that misaligned personas can be verbalised in CoT, but how often does this happen? This experiment looks more like a vibe check than a principled evaluation.  \n6. The notion of a „context“ feature could be more clear. Perhaps the authors could consider giving an example.\n7. It is not clear (unless buried somewhere in the appendix) to what extent steering with SAE latents or narrow FT affects the model‘s capabilities. Does removing misalignment have an impact on seemingly unrelated abilities, e.g. solving math problems?\n8. As authors write themselves, their proposed mitigation tools might not work in more realistic finetuning settings  \n9. The authors only study ChatGPT models, which is better than nothing as ChatGPT is the most widely used model, but it makes the results harder to reproduce (and they may not generalise)."}, "questions": {"value": "Why is the appendix before the references? Is that even allowed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8rMfnUh5df", "forum": "yjrVOxjkDR", "replyto": "yjrVOxjkDR", "signatures": ["ICLR.cc/2026/Conference/Submission20679/Reviewer_b6aW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20679/Reviewer_b6aW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167232566, "cdate": 1762167232566, "tmdate": 1762934061476, "mdate": 1762934061476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}