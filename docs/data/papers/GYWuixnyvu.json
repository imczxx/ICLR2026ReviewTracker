{"id": "GYWuixnyvu", "number": 673, "cdate": 1756765354405, "mdate": 1763122902795, "content": {"title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens", "abstract": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery—the internal construction and manipulation of visual cues—we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as \\textbf{\\Model}, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to “think visually”, it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that \\Model unlocks stronger multimodal reasoning without explicit image generation.", "tldr": "", "keywords": ["VLM", "latent reasoning", "spatial reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d9de0752738fd537b395ffc719659c70b98d8dab.pdf", "supplementary_material": "/attachment/e07940d353ee45fefc078bfbb924b33f16e8c6cc.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents Mirage, a framework designed to enhance visual reasoning in vision-language models (VLMs) by enabling them to form 'mental images' of the tasks they solve. Rather than generating explicit images, Mirage operates entirely within the model’s latent representation space, using compact visual tokens. These are first trained through supervision from real image embeddings, followed by text-only finetuning and reinforcement learning to strengthen textual reasoning. The study evaluates several Mirage configurations across a range of multimodal understanding benchmarks, demonstrating consistent improvements over existing approaches such as supervised finetuning and chain-of-thought finetuning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and generally accessible, with the main design choices well-motivated and logically presented.\n- The proposed idea of enhancing visual reasoning through latent visual tokens is both interesting and, to the best of my knowledge, novel."}, "weaknesses": {"value": "- My primary concern is the limited generalizability of the proposed approach. As currently presented, each task appears to require separate fine-tuning on a specifically synthesized dataset. It remains unclear whether the capabilities learned on one task can transfer to other domains. Moreover, some visual reasoning tasks may not admit a straightforward procedure for generating the synthetic 'helper image' used during training. Finally, results are reported exclusively on the Qwen2.5-VL family; validating the method on at least one additional VLM backbone would considerably strengthen the empirical claims.\n\n- The presentation and interpretation of results are somewhat confusing:\n    - Most experiments use Qwen2.5-VL-7B, but the Jigsaw and SAT benchmarks switch to the 3B variant. While it is valuable to show that Mirage can benefit smaller models, it would help to report results more coherently or provide explicit reasoning for the change.\n    - The set of baselines varies across experiments. For example, the spatial planning tasks compare Mirage to Aurora, Anole, and MVoT, whereas the Jigsaw and SAT benchmarks use ViGoRL and MindJourney instead. Clarifying whether these models can be applied uniformly across benchmarks (or explaining their selection) would improve comparability.\n    - The training setup for baselines is insufficiently detailed. Specifically, it is unclear which baselines (if any) were granted access to the helper images during fine-tuning. If competing models did not receive equivalent supervision, the observed performance gains might largely arise from the additional visual supervision rather than the proposed latent-token mechanism itself. This concern is reinforced by Table 4, which shows that the first training stage (latent grounding) has substantially less impact than the second (text-only fine-tuning).\n\n- Table 1 presents multiple variants of Mirage (without CoT, with CoT, and with CoT+RL), with inconsistent trends across spatial reasoning and planning tasks. The paper never clearly specifies which variant should be considered the final or recommended version, complicating the interpretation of subsequent results that only report one configuration.\n\n- The evidence provided for the 'visual nature' of the latent tokens is not fully convincing. The t-SNE visualization shows that latent tokens cluster near image embeddings, but t-SNE preserves only local distances while distorting inter-cluster relations; thus, these tokens could in fact overlap with text embeddings in the original space. Moreover, in the third panel, textual clusters already separate the latent cluster from the image embeddings, further weakening the visual-alignment claim.\n\nMinor:\n\n- Figure 2 denotes an L2 loss for latent visual tokens, whereas the text consistently refers to cosine similarity. Although the two are related, the paper should clarify which metric is actually used.\n\n- Equations (2) and (4) are identical; this redundancy could be removed for brevity.\n\n- At line 456, the reference to 'previous findings' should be supported with an explicit citation."}, "questions": {"value": "- How are baseline models trained? Specifically, line 308 mentions that Anole is finetuned 'with the same multimodal supervision': does this imply that it gets access to the helper images $I$ during training?\n- The logical connection between lines 369-370 and 371-372 is quite opaque. Could you clarify this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vlfbbORXR1", "forum": "GYWuixnyvu", "replyto": "GYWuixnyvu", "signatures": ["ICLR.cc/2026/Conference/Submission673/Reviewer_qvR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission673/Reviewer_qvR7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758078891, "cdate": 1761758078891, "tmdate": 1762915578881, "mdate": 1762915578881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QLU3piwkBI", "forum": "GYWuixnyvu", "replyto": "GYWuixnyvu", "signatures": ["ICLR.cc/2026/Conference/Submission673/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission673/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763122901980, "cdate": 1763122901980, "tmdate": 1763122901980, "mdate": 1763122901980, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Mirage, a method that enables multimodal language models to interleave latent visual tokens within text sequences, allowing them to “reason visually” without generating images. The approach involves a two-stage training procedure: The method is evaluated on several visual–spatial reasoning benchmarks, showing consistent improvements over text-only and existing interleaved baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Relevance of the problem: The paper tackles an important limitation of current vision–language models—their predominantly text-centric reasoning, which prevents them from integrating visual and textual information as humans naturally do. This is a timely and significant research direction."}, "weaknesses": {"value": "- The paper is not explicit about which baselines have access to helper images during training, making it difficult to interpret the reported gains. Although the authors mention that “unified models (Anole, MVoT) use the same multimodal supervision” (l. 308), it remains unclear whether these baselines (or the others in Tab 1) were trained with helper images or only with text-based reasoning traces that reference them. \nMy understanding is that only Mirage directly used the helper images, but this should be clearly stated. If this interpretation is correct, a central limitation lies in the evaluation itself: Mirage is fine-tuned using a privileged training signal: helper images synthesized from ground-truth solutions (e.g., annotated maps showing the correct path). This gives Mirage access to explicit answer-informing visual cues, whereas other baselines are trained only on textual reasoning. Such asymmetry makes the comparison potentially unbalanced, leaving the true source of Mirage’s improvement uncertain.\n\n- The approach appears difficult to generalize because it relies on fine-tuning with artificially constructed visual reasoning traces (helper images), which are not readily available at scale. Consequently, the method’s practicality is confined to cases where such synthetic supervision can be engineered, limiting its broader applicability.\n\n- The paper's core claim of enabling \"Machine Mental Imagery\" is questionable given the architecture of the base model, Qwen2.5-VL. This model maps image features from its visual encoder into the same embedding space as text. Prior work on VLM interpretability has demonstrated that such projected embeddings often lose low-level perceptual structure and function more as textual representations of visual clues rather than as genuine, analog visual scenes (Neo et al., ICLR 2025).\nThis context makes the claim that Mirage's latent tokens are a form of \"mental imagery\" a very strong one that requires substantial proof. However, the primary evidence provided, the t-SNE visualization in Section 5 (Figure 5), is insufficient to support it. While the analysis shows that the latent embeddings cluster near the model's image embeddings and away from text embeddings, this only confirms that the training mechanism is working as intended (i.e., grounding the latents to the VLM's internal visual features). It does not prove the nature of those features. The clusters could still represent high-level linguistic abstractions for visual concepts (e.g., \"circular object,\" \"left of the box\") rather than a pictorial, scene-like representation.\n\n## Minor Weaknesses and Suggestions\n1- Table 1 presents detailed variants of Mirage (“Direct,” “CoT,” and “w/ GRPO”), whereas Tables 2 and 3 collapse them under a single “Ours” label. This inconsistency reduces readability and makes it difficult to trace which variant is being compared across tasks.\n2-  There is a duplicated citation for Chameleon (2024/2025)."}, "questions": {"value": "- Could the authors give the details on which representations were extracted for producing Fig. 5? Which layer and tokens were selected and used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ORamh0KY7z", "forum": "GYWuixnyvu", "replyto": "GYWuixnyvu", "signatures": ["ICLR.cc/2026/Conference/Submission673/Reviewer_ahkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission673/Reviewer_ahkJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832422978, "cdate": 1761832422978, "tmdate": 1762915578738, "mdate": 1762915578738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces Mirage, a framework that enables Vision-Language Models (VLMs) to perform multimodal reasoning by interleaving latent visual tokens with text tokens during decoding—without generating explicit images. Mirage involves a two-stage training process: Joint Supervision, where the model is trained to predict both text and latent visual tokens (derived from compressed image embeddings), grounding the latent tokens in the visual space. 2. Text-Only Supervision in which the model generates latent tokens autoregressively without direct supervision, allowing them to adapt flexibly to the reasoning task. Subsequently, a reinforcement learning (RL) stage is then applied to further refine the reasoning trajectory. Experiments on spatial reasoning benchmarks (VSP, Jigsaw, SAT, COMT) show that Mirage outperforms text-only and image-interleaving baselines, demonstrating stronger reasoning without explicit image generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of latent visual tokens as a condition for Chain-of-Thought sounds interesting, which avoids the pitfalls of explicit image generation.\n\n\n2. The two-stage training is simple and easy to implement, which can effectively balance visual grounding and reasoning flexibility. Besides, the use of RL for further refinement is a modern and justified addition.\n\n3. Experiments conducted on spatial planning tasks present consistent improvements across multiple challenging benchmarks (e.g., VSP, Jigsaw, SAT), outperforming both text-only and image-generating baselines, including strong models like Aurora and MVoT.\n\n4. Sufficient ablation studies validate the design choices. Besides, the introduced visual tokens result in no extra computational cost during inference compared to text-only approaches, which balances the inference effectiveness and efficiency."}, "weaknesses": {"value": "1. The idea of generating latent visual tokens for Chain-of-Thought is not new, as it has already been used in many autonomous driving applications[1, 2, 3]. Therefore, the authors need to discuss the distinctions between their approach and these existing works.\n\n2. There is a lack of a clear description regarding how to effectively encode the helper image. The approach for compressing an image into $k$=8 tokens requires a clear explanation.\n\n\n3. It is necessary to analyze the attention maps between the $k$ visual tokens from the helper image  and the input images.\n\n**Minor Weakness**:\n\n1. It would be beneficial for this work to include a framework flowchart of the newly designed VLM architecture to facilitate readers' in-depth understanding of its structural design.\n\n2. A thorough analysis is suggested to explain the essential reasons behind introducing latent visual tokens can enhance the performance of VLMs.\n\n**Reference**\n\n[1] FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving, Zeng Shuang, et al\n\n[2] CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models, Zhao Qingqing etal\n\n[3] DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge, Zhang Wenyao"}, "questions": {"value": "Please see the problems listed in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CXel8tbp3G", "forum": "GYWuixnyvu", "replyto": "GYWuixnyvu", "signatures": ["ICLR.cc/2026/Conference/Submission673/Reviewer_7jyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission673/Reviewer_7jyo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903714496, "cdate": 1761903714496, "tmdate": 1762915578114, "mdate": 1762915578114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mirage, a framework that interleaves latent visual tokens with text, allowing vision–language models to perform multimodal reasoning without generating pixel-level images. The authors introduce a two-stage fine-tuning pipeline: (1) a joint supervision stage that grounds latent embeddings to compressed image features, and (2) a text-only relaxation stage where the model learns to autoregressively generate and evolve these latent tokens. The approach is implemented by fine-tuning Qwen2.5-VL 3B and 7B models. Combined with GRPO reinforcement learning, Mirage achieves considerable improvements over text-only and image-generation baselines across four spatial-reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel multimodal reasoning method that replaces full pixel-level decoding with compressed latent embeddings, achieving considerable performance gains.\n\n- Clear experimental and qualitative results.\n\n- Valid ablation study validating the necessity and effectiveness of the proposed two-stage training framework."}, "weaknesses": {"value": "- The process for compressing latent tokens (line 215) is not clearly explained. It is unclear how these features are divided and compressed with uniform pooling, and similarly how random pooling is applied.\n\n- There are several minor writing issues, such as line 133: urther -> further and inconsistent citation formatting (e.g., Anole (Chern et al., 2024) vs. Aurora Bigverdi et al. (2025) in lines 306–312). The authors are advised to carefully proofread the manuscript and standardize the citation style.\n\n- It is unclear how the authors handled multimodal rationales in synthetic data when training text-only models. Since the synthetic reasoning chains explicitly mention helper images, were these references removed before fine-tuning? If helper image placeholders were simply deleted while keeping text referring to them, the text-only baseline might contain incoherent pseudo-visual descriptions, leading to unfair or ambiguous comparisons."}, "questions": {"value": "- How are latent reasoning steps detected in the generated trajectories? Is there a sentinel token or any special marker that explicitly indicates the start of latent tokens during inference?\n\n- It would be interesting to examine how the compression ratio affects performance. For instance, what number of latent tokens yields the best results when representing compressed visual embeddings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MyypefbVeK", "forum": "GYWuixnyvu", "replyto": "GYWuixnyvu", "signatures": ["ICLR.cc/2026/Conference/Submission673/Reviewer_SZCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission673/Reviewer_SZCB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970311792, "cdate": 1761970311792, "tmdate": 1762915577975, "mdate": 1762915577975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}