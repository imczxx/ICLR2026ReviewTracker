{"id": "8MLGZgOJus", "number": 9187, "cdate": 1758114517123, "mdate": 1759897738794, "content": {"title": "Hide and Find: A Distributed Adversarial Attack on Federated Graph Learning", "abstract": "Federated Graph Learning (FedGL) is vulnerable to malicious attacks, yet developing a truly effective and stealthy attack method remains a significant challenge. Existing attack methods suffer from low attack success rates, high computational costs, and are easily identified and smoothed by defense algorithms. To address these challenges, we propose FedShift, a novel two-stage ``Hide and Find\" distributed adversarial attack. In the first stage, before FedGL begins, we inject a learnable and hidden “shifter” into part of the training data, which subtly pushes poisoned graph representations toward a target class's decision boundary without crossing it, ensuring attack stealthiness during training. In the second stage, after FedGL is complete, we leverage the global model information and use the hidden shifter as an optimization starting point to efficiently find the adversarial perturbations. During the final attack, we aggregate these perturbations from multiple malicious clients to form the final effective adversarial sample and trigger the attack. Extensive experiments on six large-scale datasets demonstrate that our method achieves the highest attack effectiveness compared to existing advanced attack methods. In particular, our attack can effectively evade 3 mainstream robust federated learning defense algorithms and converges with a time cost reduction of over 90\\%, highlighting its exceptional stealthiness, robustness, and efficiency.", "tldr": "", "keywords": ["Federated Graph Learning; Adversarial Attacks; Backdoor Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0340645f9b15b6d508be8dbc6008380c7176adb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FedShift, a two-stage distributed adversarial attack on Federated Graph Learning (FedGL). The attack framework aims to overcome key limitations of existing attack paradigms such as signal smoothing during aggregation, high computational cost, and detectability by integrating concepts from both backdoor and adversarial attacks. The proposed “Hide and Find” mechanism includes two stages in which the first stage introduces a learnable shifter generator that injects hidden, low-intensity perturbations into training data to cause subtle feature-space drifts toward a target class boundary. The second stage leverages the trained shifter and global model information post-training to efficiently fine-tune the perturbation for final attack execution. Extensive experiments on six large-scale datasets demonstrate that FedShift improves attack efficiency compared to other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical and emerging security issue in federated graph learning, a field gaining significant attention in federated and distributed AI research. The problem is well-motivated and clearly articulated.\n\n- The formulation, notably Algorithm 1 for adaptive shifter generator training, and loss design are mathematically reasoned.\n\n- The significance of this work is notable. FedShift reveals a new and realistic attack surface in distributed graph learning, which can have important implications for designing future defenses in federated systems. By proposing an attack that is both effective and stealthy, the paper advances the understanding of adversarial vulnerabilities in a rapidly growing field."}, "weaknesses": {"value": "- The paper primarily provides an empirical demonstration of the proposed attack’s effectiveness but lacks theoretical grounding. For example, there is no formal analysis of complexity and why the two-stage design ensures convergence or how the “gentle shift” quantitatively balances stealth and effectiveness. Providing theoretical intuition, even in simplified settings (e.g., convergence bounds or an analysis of the distributional shift dynamics), would greatly strengthen the paper’s technical depth theoretically.\n\n- Although the experiments cover six datasets, all are standard benchmarks under IID client data splits. Federated environments are typically non-IID, with varying client graph distributions and participation rates. Testing FedShift in such settings (e.g., heterogeneous feature or label distributions) would better demonstrate its robustness and practical viability.\n\n- The related work section is relatively brief and does not sufficiently discuss existing defense mechanisms in Federated Graph Learning (FedGL). A more comprehensive review of defenses would help clarify the broader research landscape and better articulate the motivation for introducing this new type of attack. In its current form, the section focuses primarily on prior attack methods but lacks a balanced discussion that positions FedShift in relation to both attack and defense strategies. Additionally, including a comparative summary table (e.g., in the appendix) outlining the main characteristics, assumptions, and differentiating factors between FedShift and prior attacks would make the paper’s contributions and novelty clearer and more compelling.\n\n- The paper would benefit from a brief limitations section outlining the current method’s drawbacks, such as scalability or attacker assumptions, and suggesting future extensions. Including remarks on potential defense strategies would also provide a more balanced and forward-looking perspective.\n\n- The defense evaluation is limited to three well-known algorithms. Incorporating more recent adaptive or certified defense frameworks (e.g., anomaly-based or graph-structure defense) would enhance the comprehensiveness of the results.\n\n- Although the authors highlight a 90% reduction in optimization time, it remains unclear how costly Stage 1 shifter training is in large-scale or resource-limited environments. Reporting per-client overhead or wall-clock time would provide practical insights.\n\n- [Minor] Some figures (Figs. 2-4) could be visually clearer with larger fonts and consistent legends for better clarity and understanding"}, "questions": {"value": "1. All experiments appear to assume IID data partitions. How would the method behave under non-IID distributions. For example, heterogeneous node features or label skews among clients? Have the authors explored or plan to explore this case to verify robustness in more practical scenarios?\n\n2. Could the authors comment on how FedShift might perform against newer structure-aware or representation-based defenses that analyze graph topology anomalies rather than gradient statistics? Would the “hidden shifter” still remain undetected?\n\n3. How sensitive is the proposed FedShift to the lower number of malicious clients? Does FedShift maintain stealth and success under extremely small malicious proportions (for example <5%)?\n\n4. Is the convex combination of loss terms (L_dist, L_homo, L_ce in Eq.7) tuned per dataset or fixed globally? Some discussion on hyperparameter sensitivity is required to understand the effectiveness of FedShift. \n\n5. **Questions on Threat Model**\n- Could the authors clarify whether the threat model is **white-box, grey-box, or black-box**?  \n- Since attackers can access global model parameters ($\\theta_t$), why is a **two-stage attack** needed instead of directly poisoning local updates or model parameters?  \n- Are attackers restricted from modifying gradients or model updates, if so, why?  \n- Please provide references or justification showing that these assumptions reflect **practical FedGL threat model scenarios**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dvlA2Z4m5t", "forum": "8MLGZgOJus", "replyto": "8MLGZgOJus", "signatures": ["ICLR.cc/2026/Conference/Submission9187/Reviewer_GoYE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9187/Reviewer_GoYE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568924044, "cdate": 1761568924044, "tmdate": 1762920859646, "mdate": 1762920859646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedShift, a novel two-stage distributed adversarial attack for Federated Graph Learning (FedGL). The key idea is to first implant a “shifter” during pre-training to subtly push graph representations toward the target decision boundary (“Hide” stage), and then refine these perturbations post-training (“Find” stage) to generate effective adversarial samples. The authors claim that this two-phase design improves attack stealthiness, resistance to aggregation smoothing, and efficiency, achieving up to 90% reduction in time cost and strong evasion of several defense mechanisms (Foolsgold, Krum, Bulyan). Experiments on six graph datasets (DD, NCI109, Mutagenicity, FRANKENSTEIN, Eth-Phish&Hack, Gossipcop) show high Attack Success Rate (ASR) and stability across settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Comprehensive Experiments: The paper evaluates on six datasets across multiple domains, with ablations and sensitivity analyses that show consistent performance gains.\n+ Strong Empirical Results: FedShift achieves notably higher attack success and efficiency compared to baselines, even under standard defense mechanisms."}, "weaknesses": {"value": "- Limited Novelty: The approach mainly combines known techniques (backdoor + adversarial optimization) without substantial theoretical or algorithmic innovation.\n- Unrealistic Threat Model: It assumes attackers can fully control local data and observe global model updates continuously, which may not hold in real federated settings.\n- Weak and Outdated Defenses: Evaluation against older baselines (e.g., Krum, Bulyan) limits the credibility of claimed robustness.\n- Lack of Theoretical or Interpretive Insight: The paper does not analyze why or when the method works, nor provide visualization or formal justification for stealthiness."}, "questions": {"value": "1. How realistic is the assumption that malicious clients can directly control and continuously optimize shifters during federated training, especially when secure aggregation or differential privacy is applied?\n2. What happens if the global model is not shared in every round (e.g., in partial sharing or privacy-preserving FedGL)? Does the “Find” stage still work?\n3. How sensitive is the performance to the number of malicious clients? Most results fix |CM| = 4 — what if only one attacker participates?\n4. Is there any measurable trade-off between stealthiness and attack success when using more aggressive shifters?\n5. Could the same effect be achieved with a simpler single-stage attack if initialization is carefully chosen?\n6. Have the authors attempted to visualize the feature-space movement of poisoned graphs to justify the “shift toward decision boundary” claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cy9GEfT5dQ", "forum": "8MLGZgOJus", "replyto": "8MLGZgOJus", "signatures": ["ICLR.cc/2026/Conference/Submission9187/Reviewer_UVgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9187/Reviewer_UVgg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807119117, "cdate": 1761807119117, "tmdate": 1762920859270, "mdate": 1762920859270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedShift, a novel two-stage “Hide and Find” distributed adversarial attack. It first embeds a hidden \"shifter\" during training to steer the model, and subsequently uses it as a basis for the rapid generation of adversarial examples. Experimental results show that the proposed method significantly outperforms existing techniques in terms of attack success rate, robustness against defenses, and efficiency"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method is innovative, introducing a \"distributional shift\" strategy in federated graph learning that enhances stealthiness. \n\n2. The experimental evaluation is comprehensive, utilizing six large-scale datasets that span multiple real-world scenarios"}, "weaknesses": {"value": "1. Only the AAS metric is used. It lacks comparison with other commonly used attack evaluation metrics and does not provide a theoretical justification. \n\n2. The evaluated defense methods do not appear to be state-of-the-art. Using simple practical defenses as a reference does not sufficiently demonstrate the true effectiveness of the attack. It is recommended that the authors survey relevant prior work, such as [1].\n\n3. This paper lacks explicit discussion on preventing malicious use of the attack or proposing appropriate defensive measures.\n\n[1] Nguyen, Thien Duc, et al. \"{FLAME}: Taming backdoors in federated learning.\" 31st USENIX Security Symposium (USENIX Security 22). 2022.\n\nMinor concern: Figure 1 does not explicitly show that the attacker poisons only part of the data, which could potentially mislead readers"}, "questions": {"value": "1. Are there any effective defenses or corresponding mitigation strategies against FedShift?\n\n2.It is not very clear how the authors compare their method with GTA. As far as I understand, GTA targets GNNs rather than FedGL.\n\n3.Adversarial attacks and backdoor attacks are distinct types of attacks. Adversarial attacks aim to cause incorrect predictions by adding small perturbations to inputs during the testing phase. However, the \"Attacker’s objective\" described in the paper seems to correspond to a backdoor attack. Could the authors have confused these concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JcuIPRLZE5", "forum": "8MLGZgOJus", "replyto": "8MLGZgOJus", "signatures": ["ICLR.cc/2026/Conference/Submission9187/Reviewer_Jq1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9187/Reviewer_Jq1e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835614940, "cdate": 1761835614940, "tmdate": 1762920858744, "mdate": 1762920858744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedShift, a two-stage distributed adversarial attack on Federated Graph Learning (FedGL). Stage 1 injects \"hidden shifters\" that gently push poisoned graphs toward target class boundaries without crossing them. Stage 2 uses these shifters as initialization to efficiently find adversarial perturbations after federated training completes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The combination of backdoor and adversarial attack paradigms is creative, addressing limitations of each method when used independently.\n2. Overall, the paper is well written and clearly explained.\n3.  The method shows significant improvements in attack success rate and efficiency."}, "weaknesses": {"value": "1. The assumption that attackers can \"continuously optimize shifters throughout the whole federated process\" is strong and may not reflect realistic scenarios.\n2. The number of clusters k shows high sensitivity on some datasets (Figure 5).\n3. No sensitivity analysis on loss weights.\n4. No comparison with spectral defense techniques or pruning based defenses."}, "questions": {"value": "1. Can you provide a mathematical characterization of how shifting graphs toward target class boundary without crossing boundary ensures stealthiness?\n2. Can you elaborate more on the high sensitivity of cluster k on some datasets?\n3. Are you considering both stage 1 and stage 2 for the computational comparison with other attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q83O9gruGN", "forum": "8MLGZgOJus", "replyto": "8MLGZgOJus", "signatures": ["ICLR.cc/2026/Conference/Submission9187/Reviewer_JZox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9187/Reviewer_JZox"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976884605, "cdate": 1761976884605, "tmdate": 1762920858400, "mdate": 1762920858400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}