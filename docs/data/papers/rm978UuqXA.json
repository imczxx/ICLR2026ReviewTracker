{"id": "rm978UuqXA", "number": 14237, "cdate": 1758230955838, "mdate": 1759897382002, "content": {"title": "Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs", "abstract": "We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation, that addresses key limitations of recent averaging-based optimizers such as DiLoCo and Schedule-Free (SF). These two recent algorithmic approaches improve the performance of base optimizers such as AdamW through distinct averaging strategies. Schedule-Free explicitly averages iterates at every step, while DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. This periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters to tune. To address these limitations, GPA smoothens DiLoCo in the non-distributed setting by averaging iterates at every iteration using two interpolation constants. When applied to language model pre-training, GPA consistently outperforms DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning and reducing memory overhead to just a single additional buffer. Furthermore, we prove that for any base optimizer with regret bounded by $\\mathcal{O}(\\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of the interpolation constants.", "tldr": "We introduce a new primal averaging scheme called Generalized Primal Averaging that smooths DiLoCo and generalizes Nesterov acceleration and Schedule-Free learning.", "keywords": ["distributed optimization", "stochastic optimization", "distributed learning", "momentum", "iterate averaging", "primal averaging"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a90fecffb5674be201f697e690b1d80963e6cf4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Generalized Primal Averaging (GPA), an extension of Nesterov’s method under a primal averaging framework for optimizer design. GPA is proposed as a smoothing and generalization of DiLoCo, addressing its memory and hyperparameter tuning complexity by averaging updates at every iteration via two decoupled interpolation constants. Theoretical convergence analysis has been presented. Experiments have been conducted on C4 dataset with Llama 3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper identifies major limitations of DiLoCo, notably its two-loop structure and hyperparameter coupling, then constructs GPA as a principled, general alternative that captures the positive effects of iterate averaging with smooth, decoupled hyperparameters.\n\n2) Theoretical results are presented with careful derivations and proofs (notably Theorem 1 and the equivalency propositions in Appendix B and C).\n\n3) Experiments have demonstrated tangible improvement over baselines."}, "weaknesses": {"value": "1) Limited Experimental Breadth: The empirical evaluation is confined mainly to the Llama-160M model on the C4 dataset in a non-distributed setup. All results rely on a single model and dataset, leaving claims of improved generality, scalability, and ease of deployment (e.g., in distributed environments or with other optimizer families) unsupported by experiments, especially considering that DiLoCo is primarily for distributed setting. \n\n2) Baselines and Missing Comparisons: The experiments omit direct comparisons with Schedule-Free (SF), despite its close theoretical and practical relevance. Although SF is discussed in the text, it is absent from the main comparison tables (e.g., Table 1) and figures. Considering the prominence of SF and related averaging-based optimizers in prior work, this omission substantially weakens the empirical support for the proposed method. Also, comparison to (Lan, 2012) is missed/not adequate in experiments, when their main difference is the decoupled interpolation. Although Figure 4 provides a comparison between coupled and decoupled interpolation, the considered range of free parameters is very limited.\n\n3) Probably weak and non-novel theoretical analysis: The analysis is for convex setting only while deep learning are essentially nonconvex. Also, since the algorithm is similar to (Lan, 2012), is there any novelty in analysis compared to (Lan, 2012)?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KgLfiNYihj", "forum": "rm978UuqXA", "replyto": "rm978UuqXA", "signatures": ["ICLR.cc/2026/Conference/Submission14237/Reviewer_sJRe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14237/Reviewer_sJRe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687108645, "cdate": 1761687108645, "tmdate": 1762924691725, "mdate": 1762924691725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a iterative averaging scheme called generalized primal averaging (GPA), which is utilized to simplify and smooth DiLoCo.  GPA removes DiLoCo’s two-loop structure by averaging iterates at every step using two interpolation parameters. It decouples smoothing and recency control. This design reduces memory use, eliminates the “inner step” hyperparameter. The authors prove that GPA achieves the same convergence rate as its base optimizer and can improve upon it by selecting the interpolation parameters. Experiments on Llama-160M pretraining show GPA matches or outperforms DiLoCo, achieving up to 38% faster convergence compared to AdamW with smoother, more stable training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposed GPA scheme aims to simplify the DiLoCo optimizer and get rid of the inner optimization loop. The author also provide theoretical guarantees on its convergence."}, "weaknesses": {"value": "I think the presentation of this paper should be improved. After introducing the GPA, its analysis and connections to other methods are hidden in the text, and sometimes make claims without revealing the logic behind it. For example, line 259-260, why we have to use learning rate scheduler? Therefore, I have to admit that I do not fully understand every details of this paper."}, "questions": {"value": "1. Generally why GPA can serve as the smoothed version of DiLoCo? Is it because the connections of primal averaging to Nestrov momentum? And then based on the analysis, you introduce two interpolation parameter for x and y? If so, this message can be much clearer than the current format. \n\n2. For the 160m model, the ppl seems to be quite high compared to standard benchmarks. For example, [1-2] report the ppl for AdamW as 25.08 for 130M LLaMA model with C4 dataset. But your method (GPA) reports 26.03?\n\n3. Do you have sensitivity analysis on the interpolation parameters? In paper, you only mention that you sweep over different values of $\\mu_x$ and $\\mu_y$. \n\n4. 130M model is still too small for practical usage, and 12K steps/compute optimal setup are often not adopted by actual industrial LLM model training. Is it possible to run a larger model with over-train setup? Like over 1B model with around 100B data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wRX77xbizx", "forum": "rm978UuqXA", "replyto": "rm978UuqXA", "signatures": ["ICLR.cc/2026/Conference/Submission14237/Reviewer_NDzb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14237/Reviewer_NDzb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905284432, "cdate": 1761905284432, "tmdate": 1762924691357, "mdate": 1762924691357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Generalized Primal Averaging (GPA), a two-parameter variant of the primal-averaging view of Nesterov that decouples the averaging used for  the evaluation iterate $x_t$ and  the gradient point $y_t$. The stated goal is to smooth non-distributed DiLoCo by replacing its two-loop, periodic pseudo-gradient aggregation with per-step iterate averaging, reducing memory and hyperparameters while preserving (or improving) wall-clock efficiency. The authors claim an $O(1/\\sqrt{T})$ convergence guarantee for the average iterate under a regret-bounded base optimizer, and empirical speedups on Llama-160M pre-training on C4."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- GPA is precisely specified with both direct and memory-efficient forms; implementation notes (extra buffer, reconstruction) are helpful.\n\n- The smoothing view and the $H\\leftrightarrow \\mu_x$ heuristic connect two families (Lookahead/DiLoCo vs iterate-averaging).\n\n- On Llama-160M, GPA improves final loss over AdamW and DiLoCo at matched effective inner steps, with reported peak step-speedup ~38%."}, "weaknesses": {"value": "- Theory does not justify the central empirical claims. The bound is (i) convex-only, (ii) on the average iterate, while the method uses a schedule and returns the last iterate; (iii) does not quantify when GPA strictly improves over the base beyond informal remarks about negative Bregman terms. Also, theoretical novelty is incremental. GPA’s decoupling of $\\mu_x$ and $\\mu_y$ is a straightforward extension of primal averaging and Schedule-Free/EMA-style iterate averaging, but Theorem 1 does not establish the last iterate guarantee as in [1]\n\n- The experimental scope is rather limited. The paper evaluates only a single model size, one dataset, and one base optimizer. Moreover, it lacks comparisons with recently prominent optimizers such as Shampoo, Muon, and SOAP, as well as distributed experiments or results on larger-scale LLMs (≥1B parameters).\n\n- Memory/compute claims lack measurement. The paper argues only one extra buffer and simpler state than DiLoCo, but provides no peak-memory or step-time measurements to substantiate practical savings.\n\n- The results are based on single runs without reporting multiple seeds, and no variance or error bars are provided.\n\n\n[1] Defazio, Aaron, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky. \"The road less scheduled.\" Advances in Neural Information Processing Systems 37 (2024): 9974-10007."}, "questions": {"value": "- How should practitioners choose $(\\mu_x, \\mu_y)$ without first running DiLoCo to obtain $H$?\n\n- Regarding compute and memory, what are the per-step wall-clock overheads and peak memory of GPA vs AdamW and DiLoCo? Report tokens/sec and GB, not only steps-to-loss.\n\n- Could you provide results with Shampoo/Muon/SOAP/Schedule-Free AdamW as base optimizers and at ≥1B parameters, do the claimed speedups persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2PrFr0ZQz1", "forum": "rm978UuqXA", "replyto": "rm978UuqXA", "signatures": ["ICLR.cc/2026/Conference/Submission14237/Reviewer_Th9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14237/Reviewer_Th9M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938686504, "cdate": 1761938686504, "tmdate": 1762924690421, "mdate": 1762924690421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a method named GPA, which generalize primal averaging formulation of momentum method into arbitrary base optimizers rather than SGD. The method is shown to be convergent under the convex case. Numerical results in pre-training Llama 3-130M on C4 dataset show that GPA consistently outperforms DiLoCo."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The method accelerates the base optimizer AdamW by a large margin and does not require the storage of a \"slow weight\" as in DiLoCo.\n* The method is shown to be convergent under the convex case."}, "weaknesses": {"value": "* If I understand it correctly, the formulation (3) is the classical Polyak momentum rather than Nesterov momentum. The Pytorch's implementation of Nesterov accelerated gradient is inconsistent with the document it refers to ([1, equation 3]), where the momentum aggregates the gradient evaluated at $\\theta_t + \\mu v_t$ instead of $\\theta_t$. These two momentum algorithms have different theoretical convergence rate in convex case, and usually exhibit substantial convergence gap in practice. I suggest the authors to correct the corresponding claims.\n* Follow my previous comment, the Proposition 1 only shows the equivalence of classical momentum and the primal average formulation.\n* From the perspective of algorithmic design, the difference between GPA and Schedule-Free method is the strategy of weight average, where GPA applies exponential moving average instead of $t/t+1$. Could the authors illustrate more on the importance of the design? There is no numerical comparison between these two methods in the paper.\n\n[1] On the importance of initialization and momentum in deep learning"}, "questions": {"value": "Please see the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EJoPqG36O9", "forum": "rm978UuqXA", "replyto": "rm978UuqXA", "signatures": ["ICLR.cc/2026/Conference/Submission14237/Reviewer_PLNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14237/Reviewer_PLNw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257178027, "cdate": 1762257178027, "tmdate": 1762924689097, "mdate": 1762924689097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}