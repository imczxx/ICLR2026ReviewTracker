{"id": "ly09ZBzSOc", "number": 4721, "cdate": 1757753142179, "mdate": 1759898017832, "content": {"title": "ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning", "abstract": "Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce ADORA (Advantage Dynamics via Online Rollout Adaptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations on various tasks demonstrate that ADORA significantly enhances long reasoning in both geometric and mathematical tasks across large vision–language models and large language models, achieving notable performance gains.", "tldr": "We introduce ADORA (Advantage Dynamics via Online Rollout Adaptation), a novel RL framework designed to dynamically calibrate advantage estimation.", "keywords": ["reinforcement learning", "optimization methods", "generalization", "continual learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b38e0e8b31ac38a1148d0a5cd729dab32b0856a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ADORA, a reinforcement learning framework that dynamically updates the advantage estimation weights during training. \nThe basic idea is to categorize samples into temporarily advantageous and temporarily disadvantageous based on their evolving utility, and ADORA enables more efficient policy optimization. Experiments across both large language models and vision–language models shows consistent gains over baselines like GRPO and DAPO, especially in reasoning-intensive tasks such as mathematical and geometric problem solving."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  Feasible approach to dynamic weighting: The paper introduces a conceptually clean yet empirical effective way to dynamically calibrate the advantage function during reinforcement learning. It targets solving a key problem with static estimations.\n-  Strong empirical validation: The experiments are relative extensive, covering both LLMs and VLMs. It shows clear and consistent performance improvements even with limited data. This gives confidence in the robustness of ADORA’s approach. However, in terms of ablation, there remain several concerns, which will be detailed next. (See Weakness)"}, "weaknesses": {"value": "- Generalization not deeply explored: Although ADORA performs well on tested benchmarks, the discussion on transferability and performance on out-of-distribution or unseen domains feels somewhat limited. Also, the weighting is based on heuristic rule and there lacks some theoretical insights.\n- Dependence on rollout quality: The authors themselves note that ADORA’s success depends on the quality of generated rollouts (Appendix D). However, the paper does not clearly propose methods to mitigate low-quality samples or noise in the rollout process.\n- Missing some prompt difficulty related works: There are related works that can be discussed in the manuscript. For example, MoPPS [1] actively infers the prompt difficulty and selects the subset to improve both efficiency and performance. SPO does a similar thing from policy optimization perspective.\n- Besides the above, I have raised some questions below, which should be well addressed in either added discussions or experiments.\n\nReferences: \n\n[1] Qu, Yun, et al. \"Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models?.\" arXiv preprint arXiv:2507.04632 (2025).\n\n[2] Xu, Zhongwen, and Zihan Ding. \"Single-stream policy optimization.\" arXiv preprint arXiv:2509.13232 (2025)."}, "questions": {"value": "- How sensitive is ADORA to the hyperparameters like τ and ws? Would small changes significantly affect convergence?\n- How does ADORA behave with other family of base models such as Qwen and other sizes such as 1.5B base models?\n- It seems reweighting mechanism adjusts the learning rate, so is the VLA result’s advantage from the learning rate adjustment.\n\nOther suggestions: \n\n- (1) It would be better to include an illustration Figure to show the detailed reweighting implementation steps in rollout and policy optimization. \n- (2) Learning curves for all results should be reported if necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VMmNQ8aG3T", "forum": "ly09ZBzSOc", "replyto": "ly09ZBzSOc", "signatures": ["ICLR.cc/2026/Conference/Submission4721/Reviewer_nDXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4721/Reviewer_nDXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666477924, "cdate": 1761666477924, "tmdate": 1762917534111, "mdate": 1762917534111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADORA (Advantage Dynamics via Online Rollout Adaptation), a framework to improve the sample efficiency of policy gradient methods for training VLMs and LLMs. It works by dynamically calibrate advantage estimation based on two heuristics that assess the model's current capacity for the given samples. The paper demonstrates the proposed framework in mathematical and geometric tasks improving over the baseline on several benchmarks for the Qwen2.5-VL-7B and Qwen2.5-7B"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper investigate the important topic of sample efficiency for LLMs for hard reasoning tasks.\n\nThe proposed heuristics show promising results to improve the sample efficiency of GRPO without any additiona significant computational cost.\n\nThe benchmark results are supported with more qualitative analysis."}, "weaknesses": {"value": "The method introduces several key hyperparameters ($\\tau=0.5$, $w_s=0.1$, $w_s=2.0$) that are presented without any justification or sensitivity analysis. These values are likely to heavily influence performance and would almost certainly require re-tuning for new models or tasks thus undermining the paper's central claim of being a general and lightweight approach. This weakness is compounded by the fact that all experiments are limited to a single model family (Qwen).\n\nThe heuristics are calculated using a very small number of rollouts ($G=5$ for LLMs, $G=8$ for VLMs). Basing the TAS/TDS classification on such a small sample size means the signal is statistically noisy and high-variance. A sample's classification could easily \"flicker\" between advantageous and disadvantageous from one step to the next due to simple sampling luck, not a true change in the model's capability.\n\nThe length advantage criterion actively punishes correct, concise reasoning and creates a strong incentive for verbosity. While this may not have compromised performance on the chosen benchmarks, it raises serious questions about the method's generalizability. The paper attempts to investigate this overthinking issue in Section 5.3, but its analysis relies on an \"Overthinking Score\" that is never properly introduced, making the results in Table 3 difficult to interpret.\n\n\nThe \"Difficulty Advantage\" heuristic ($R_{succ}^s \\le 0.5$) [cite: 340] uses a sharp, binary threshold that encourages proficiency rather than mastery. As soon as the model achieves >50% success on a sample, its learning signal is halved (from $w_s=2.0$ to $w_s=1.0$). This may prematurely de-prioritize the sample and *prevent* the model from achieving true robustness (e.g., 90-100% success). Furthermore, this rule conflates \"instructive\" ($R^s=50\\%$) with \"impossibly hard\" ($R^s=0\\%$), treating them both as equally advantageous."}, "questions": {"value": "The paper's claim of a \"general\" framework is a significant one, but it's evaluated exclusively on the Qwen model family. Can you provide any results on ADORA's performance when applied to other model families (e.g., Llama, Mistral)? Relatedly, how were the key hyperparameters ($\\tau=0.5$, $w_s=0.1$, $w_s=2.0$) selected? A sensitivity analysis would be critical to understand how much these \"magic numbers\" must be re-tuned for new models.\n\nRegarding the heuristics themselves, the TAS/TDS classification relies on a very small number of rollouts ($G=5$ or $G=8$). How do you ensure this signal is statistically stable and not just high-variance noise? A sample's classification could flicker from step to step based on sampling luck alone.\n\nThe reflection frequency analysis in Section 5.2 is also a concern. Since ADORA produces longer responses (Figure 7), how did you disentangle *true reflection* from *mere verbosity*? A more verbose model will naturally use more reflective words. Could you provide a length-normalized analysis (e.g., \"reflection keywords per 100 tokens\")? On that topic, your investigation in Section 5.3 relies on an \"Overthinking Score\" that is never introduced. Could you please provide a clear definition of this score and how it is calculated?\n\nFinally, on the heuristic design: Why was a sharp $\\tau=0.5$ threshold chosen for the \"Difficulty Advantage\"? This rule seems to punish mastery by de-prioritizing samples once proficiency exceeds 50%. Have you considered a \"softer\" weighting? And critically, why does the VLM strategy only use the \"Length Advantage\"? We are missing the ablation study, similar to Figure 2, that justifies this specific design choice for VLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cZ4MWUN77u", "forum": "ly09ZBzSOc", "replyto": "ly09ZBzSOc", "signatures": ["ICLR.cc/2026/Conference/Submission4721/Reviewer_T6y3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4721/Reviewer_T6y3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817063910, "cdate": 1761817063910, "tmdate": 1762917533877, "mdate": 1762917533877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that static advantage estimation, as traditionally used, leads to inefficient credit assignment due to ignoring the dynamic utility of training samples over time. To address this, they propose ADORA (Advantage Dynamics via Online Rollout Adaptation) that dynamically tune the importance of the advantage function based on the utility of the samples. They perform considerable number of experiments with LLMs and VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Strengths:\n\n1. ADORA categorizes the training data into temporarily advantageous and disadvantageous samples and adaptively assigns sample-wise weights based on predefined criteria to estimate the ultimate advantage.\n\n2. Validation and ablations are conducted across different domains and datasets, especially on both LLMs and VLMs. Further, ADORA achieves a consistent performance gain to an extent."}, "weaknesses": {"value": "Weaknesses:\n\n1. The paper only consider length and success rate to measure the utility. It does not consider more complex evaluations such as step consistency. I believe there is opportunity to define the criteria more comprehensively. \n\n2. The authors mention \"How to assign a corresponding weight $w_s$ that reflects its training utility?\". However, I don't see an appropriate answer to this question. The rationale behind choosing the specific values is not discussed. I would consider them as hyperparameters, and the sensitivity to those hyperparameters are yet to explore.\n\n3. Same concern goes for $\\tau$ in Eqn. 6 which is set to 0.5. How the model reacts with the changes to $\\tau$?"}, "questions": {"value": "### Questions:\n\n1. In Eqn. 5, why do you need the **longest** successful rollout? Shouldn't every successful rollout which has a length $> L_{fail}$ be considered?\n\n2. The paper mentions that a direct indicator of explicit reasoning is the frequency of reflective vocabulary usage. How you come up with that vocabulary? Can you provide any reference in favor of this?\n\n3. Can you simply describe the interpretation of Figure 4.? \n\nAlso, look at the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2yfYg996Af", "forum": "ly09ZBzSOc", "replyto": "ly09ZBzSOc", "signatures": ["ICLR.cc/2026/Conference/Submission4721/Reviewer_H1YL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4721/Reviewer_H1YL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013589197, "cdate": 1762013589197, "tmdate": 1762917533664, "mdate": 1762917533664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ADORA (Advantage Dynamics via Online Rollout Adaptation), a framework that dynamically reweights advantages in reinforcement learning for reasoning models. \n\nThe core idea is to classify training samples into Temporarily Advantageous Samples (TAS) and Temporarily Disadvantageous Samples (TDS) based on rollout statistics, then amplify or attenuate their learning signals accordingly. The method uses Length Advantage and Difficulty Advantage as criteria. \n\nExperiments on Qwen2.5-VL and Qwen2.5 show improvements over GRPO baseline across mathematical and geometric reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The dynamic reweighting of advantages based on online rollout statistics is a practical contribution. \n\nQuality: Strong experimental validation across both VLMs (MathVista: 73.5%) and LLMs (3.5% average improvement over GRPO).\n\nClarity: Well-structured presentation with clear motivation. And figures effectively illustrate training dynamics and sample evolution.\n\nSignificance: Addresses a real problem in RL-based reasoning training."}, "weaknesses": {"value": "1.  Limited experiment scope:\n* Only tested on Qwen family models. Generalization to other model family (Gemma, Llama, Phi, etc.) is unknown.\n* VLM experiments use only 2K samples—unclear if benefits persist at larger scales.\n\n2. Incomplete ablations:\n* No ablation on weight values (see Question#1 and #2).\n* Figure 2 shows ablations on advantage criteria but only for LLMs—missing VLM ablations.\n* The threshold τ=0.5 for difficulty appears arbitrary—no ablation on this critical hyperparameter.\n* No formal analysis of why ws=0.1 (VLM) or ws=2.0 (LLM) are optimal choices.\n\n3. Questionable claims:\n* \"No cold-start\" (Table 6) is misleading—they start from Qwen2.5-VL-7B-Instruct, which is already instruction-tuned, while Vision-R1 starts from base model.\n* Section 5.3: Lower overthinking on AIME24 (40.1 vs 44.8) is attributed to ADORA, but ADORA also achieves higher accuracy—is this confounded?\n* Length Advantage conflates token count with reasoning quality:\n```\nEquation 5 assumes longer responses indicate deeper reasoning, but consider:\n\nSolution A: 15×4 = 15×2×2 = 60 (20 tokens, uses factorization)\nSolution B: 15+15=30, 30+15=45, 45+15=60, verify: 60÷4=15✓ (100 tokens, brute-force)\n\nADORA prefers B over A despite A showing better insight. The paper provides no evidence that:\n\n- Short correct solutions (<50 tokens) only solve trivial problems\n- Length correlates with reasoning depth rather than redundant verification\n```\n4. Statistical rigor:\n* While 3 runs are reported, no significance tests or confidence intervals are provided.\n* Table 1 shows ADORA (73.5%) matches Vision-R1 (73.5%) on MathVista—is this difference statistically significant vs GRPO (70.2%)?\n\n\n\n### Minor Issues\n\n* Equation 4: ws is sample-level but notation doesn't clearly distinguish from trajectory-level weights.\n* Figure 4 visualization is cluttered—consider simplifying or providing clearer legends."}, "questions": {"value": "1. Threshold sensitivity: How sensitive is performance to τ? Have you tried τ ∈ {0.3, 0.5, 0.7}? What happens at extremes (τ=0.1 or τ=0.9)?\n2. Weight justification: Why ws=2 for LLM amplification and ws=0.1 for VLM attenuation? Have you ablated ws ∈ {0.05, 0.1, 0.2} and ws ∈ {1.5, 2, 3}?\n3. Can you analyze solution length vs. quality (weakness#3)? Please provide evidence that Length Advantage captures reasoning depth rather than just token count.\n4. Cold-start claim: Table 6 compares against Vision-R1 with 200K cold-start data, but your base model is already instruction-tuned. Can you clarify whether this is a fair comparison?\n5. Overthinking confound: Lower overthinking scores on AIME24 (Table 3) correlate with higher accuracy. Is the reduction in overthinking due to ADORA, or simply a byproduct of better performance?\n6. Cross-architecture validation: Have you tested ADORA on non-Qwen models (Gemma, Llama, Phi, etc.) ? This is critical for establishing generalizability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QojnLldxlw", "forum": "ly09ZBzSOc", "replyto": "ly09ZBzSOc", "signatures": ["ICLR.cc/2026/Conference/Submission4721/Reviewer_WhN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4721/Reviewer_WhN1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027358619, "cdate": 1762027358619, "tmdate": 1762917533365, "mdate": 1762917533365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}