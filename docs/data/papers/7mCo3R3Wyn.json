{"id": "7mCo3R3Wyn", "number": 212, "cdate": 1756731272789, "mdate": 1763696296483, "content": {"title": "TEMPFLOW-GRPO: WHEN TIMING MATTERS FOR GRPO IN FLOW MODELS", "abstract": "Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow-GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces three key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases; and (iii) a seed group strategy that controls for initialization effects to isolate exploration contributions. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and text-to-image benchmarks.", "tldr": "", "keywords": ["GRPO; Flow Matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59b5d165bf8aa9f940bd82e58e888211a9cf9c98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed TempFlow-GRPO, a framework that makes the optimization process temporally aware to address the key limitation of temporal uniformity in previous RLHF works. The paper introduces a mixture of ODE and SDE sampling, along with a noise-aware policy weighting scheme, to balance exploration and reward exploitation. Experiments demonstrate that TempFlow-GRPO achieves state-of-the-art performance, yielding higher rewards than standard GRPO approaches."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper pinpoints temporal uniformity as the primary limitation of existing flow-based GRPO methods and proposes TempFlow-GRPO to solve it with precise credit assignment and noise-aware optimization. The authors demonstrate this non-uniformity well with empirical evidence from rewards, supporting the need for temporal information.\n- The paper introduces the core mechanisms of trajectory branching and noise-aware reweighting to create temporally-structured policies that respect the dynamics of the generative process. The authors also provide a theoretical justification from the policy gradient perspective, further supporting the use of noise-aware reweighting.\n- The proposed TempFlow-GRPO achieves state-of-the-art performance compared to the existing vanilla GRPO approach, demonstrating the effectiveness of the method. The authors also include comprehensive ablation studies to better understand the dynamics of this model."}, "weaknesses": {"value": "- The computational cost, as thoroughly analyzed in Appendix A.6, will be higher than the vanilla GRPO models due to the branching process. Nonetheless, this is more like a trade-off between quality and time, given the superior quality metrics."}, "questions": {"value": "- How is the performance affected by the number of branches (K) at each step, the specific timesteps chosen for branching, or the exact function used for noise-aware weighting? The ablation study (Fig. 8) shows that the 4x6 (seed x branch) configuration was chosen, but it's unclear how much tuning is required to find the optimal setup for a new model or dataset. A discussion on how to choose these hyperparameters will be useful for general applications of the proposed framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bwnN4Dpw2N", "forum": "7mCo3R3Wyn", "replyto": "7mCo3R3Wyn", "signatures": ["ICLR.cc/2026/Conference/Submission212/Reviewer_rGTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission212/Reviewer_rGTd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408585508, "cdate": 1761408585508, "tmdate": 1762915470636, "mdate": 1762915470636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TempFlow-GRPO, a new reinforcement learning framework that addresses the limitation of uniform credit assignment across timesteps. The method introduces trajectory branching, which switches from ODE to SDE sampling at selected timesteps to generate exploratory branches and assign their rewards to intermediate states. This paper further proposes noise-aware policy weighting, prioritizing optimization at high-noise early stages over low-noise refinement phases. Experiments show that TempFlow-GRPO achieves substantially improved efficiency and final performance compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is overall well-written and easy to follow.\n- The motivation and the proposed method are clear and straightforward: addresses the temporal inhomogeneity and credit assignment problems through intermediate resampling for intermediate value estimation and noise-aware reweighting.\n- The proposed method shows strong empirical performance in both efficiency and end-level performance, with comparisons that include GPU time."}, "weaknesses": {"value": "- Theorem 1 is intuitively reasonable, but labeling it as a Theorem feels overstated since the underlying assumptions and proof sketch are insufficiently formalized. The analytical depth is also somewhat limited.\n- The explanation around line 847 (regarding why the average number of branches is 4.5Ã— when K = 10) is unclear. It is not obvious how this factor arises or how the branching schedule operates, and the paper does not explicitly describe it.\n- Adding more algorithmic details or pseudocode would improve readability and make the proposed procedure easier to follow."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ll3uG2W6Wk", "forum": "7mCo3R3Wyn", "replyto": "7mCo3R3Wyn", "signatures": ["ICLR.cc/2026/Conference/Submission212/Reviewer_wmxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission212/Reviewer_wmxk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461719922, "cdate": 1761461719922, "tmdate": 1762915470095, "mdate": 1762915470095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the sparse terminal reward and uniform credit assignment problem in GRPO training of flow models. The authors propose TempFlowGRPO, which includes: (1) Trajectory Branching, where only one step of SDE is used at timestep k; (2) Noise-Aware Policy Weighting by reweighting according to noise level; and (3) a seed group strategy. The method achieves state-of-the-art performance in human preference alignment and text-to-image benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe authors astutely identify that the FLOW-GRPO algorithm treats all timesteps equally, and tackle this issue via single-timestep SDE optimization.\n2.\tThe noise reweighting method is shown to be effective through both soild theoretical analysis and experiment results.\n3.\tThe paper is generally well written with a clear logical structure."}, "weaknesses": {"value": "1.\tThe contribution of seed group strategy is relatively small to other parts of the work, and the paper should provide additional details of the seed group strategy.\n2.\tSimilarly, MixGRPO [1] proposes a training window of SDE time steps that also tackles the issue of treating all timesteps equally. However, there is limited discussion comparing with MixGRPO.\n3.\tThe paper does not discuss the phenomenon of reward hacking, which is an inevitable problem for the GRPO method.\n\n[1] Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde"}, "questions": {"value": "1.\tThe trajectory branching mechanism appears similar to MixGRPO limited with a single-timestep window. How do their efficiency and effectiveness compare?\n2.\tThe paper claims that Flow-GRPO (Prompt) is an improved baseline with group standard deviation stabilization, but does not provide much detail. Could the authors elaborate on this improved method?\n3.\tWhy are the Pickscore curve trends by steps and GPU hours on the left of Figure 3 inconsistent?\n4.\tCompare to FlowGRPO, the experiment of Visual Text Rendering is not addressed. How well does TempFlow-GRPO perform on this particular task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XJXdsO9lSo", "forum": "7mCo3R3Wyn", "replyto": "7mCo3R3Wyn", "signatures": ["ICLR.cc/2026/Conference/Submission212/Reviewer_zj9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission212/Reviewer_zj9Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891360244, "cdate": 1761891360244, "tmdate": 1762915469900, "mdate": 1762915469900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TempFlow-GRPO is a temporally-aware reinforcement learning framework for flow matching models that improves human preference alignment by introducing trajectory branching, noise-aware weighting, and seed grouping to achieve precise credit assignment and efficient optimization across timesteps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "For reinforcement learning tasks, dense rewards are crucial for effective credit assignment. The proposed Trajectory Branching mechanism provides an elegant and effective way to obtain dense rewards along the denoising trajectory.\n\nThe introduced reweighting mechanism offers a valuable analysis of how gradients evolve across steps in baseline algorithms and presents a solution to mitigate the identified issues."}, "weaknesses": {"value": "The proposed method involves numerous ODE denoising steps, which substantially increase computational overhead. However, the paper lacks a comparison against the baseline method using training time as the horizontal axis to illustrate efficiency trade-offs.\n\nThe authors should evaluate the performance of the reweighting mechanism under different $\\sigma_t$ schedulers rather than relying solely on the one used in Flow-GRPO, to examine how the choice of scheduler influences its effectiveness. It remains unclear whether simply reweighting the coefficients in the earlier part to 1 would yield good results under different schedulers."}, "questions": {"value": "The comparison between batch std and global std is only evaluated on PickScore. How does this observation generalize to other tasks?\n\nCan the proposed reweighting mechanism be applied to hybrid variants (FlowGRPO-Fast/MixGRPO) where only a subset of steps follows an SDE formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7kHXzXm0N", "forum": "7mCo3R3Wyn", "replyto": "7mCo3R3Wyn", "signatures": ["ICLR.cc/2026/Conference/Submission212/Reviewer_RKyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission212/Reviewer_RKyA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898391897, "cdate": 1761898391897, "tmdate": 1762915469597, "mdate": 1762915469597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reviewer Comments Summary"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and constructive reviews. We are greatly encouraged by the overwhelmingly positive feedback on our work, including the recognition of our trajectory branching mechanism and reweighting method. In the revised manuscript, we have carefully addressed all concerns and suggestions raised by the reviewers. The key updates include:\n\n**Grouping Strategy Details (Appendix A.10):** We provide additional ablation studies on the grouping mechanism.\n\n**Visual Text Rendering Results (Appendix A.11):** We include performance evaluations on the visual text rendering task.\n\n**Qwen-Image Results (Appendix A.12):** We include performance evaluations on Qwen-Image to demonstrate the effectiveness of TempFlow-GRPO.\n\n**Flow-GRPO-Fast and TempFlow-GRPO-Fast (Appendix A.13):** We add an extensive discussion and empirical comparison with the SDE window approach, including an analysis under different window sizes and the hybrid variant, TempFlow-GRPO-Fast.\n\n**Diversity and Reward Hacking (Appendix A.14):** We evaluate the diversity of TempFlow-GRPO and provide an analysis regarding reward hacking.\n\n**Algorithm (Appendix A.15):** We provide the full algorithm for TempFlow-GRPO to ensure a clear understanding.\n\n**Generalization of Reweighting Mechanism (Appendix A.16):** We evaluate the noise-aware weighting under different $\\sigma_t$ schedulers besides Flow-GRPO's optimal setting, demonstrating robustness across various scheduling strategies.\n\n**Moreover, we highlight several key points:**\n\n**The novelty and effectiveness of trajectory branching.** Unlike existing methods that treat all timesteps uniformly, our trajectory branching mechanism introduces single-timestep SDE optimization at selected points along the denoising trajectory. This provides dense intermediate rewards for precise credit assignment, addressing a limitation in flow-based GRPO methods. As our experiments demonstrate, this design achieves superior performance while maintaining reasonable computational overhead.\n\n**The theoretical foundation and practical impact of noise-aware weighting.** We provide both theoretical justification from a policy gradient perspective and empirical validation showing that our reweighting mechanism effectively addresses gradient imbalance across timesteps. The analysis generalizes across different schedulers.\n\nWe provide detailed responses to each reviewer below and welcome any further questions or discussions."}}, "id": "Pqz8M5uOF3", "forum": "7mCo3R3Wyn", "replyto": "7mCo3R3Wyn", "signatures": ["ICLR.cc/2026/Conference/Submission212/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission212/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission212/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763627847509, "cdate": 1763627847509, "tmdate": 1763629881399, "mdate": 1763629881399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}