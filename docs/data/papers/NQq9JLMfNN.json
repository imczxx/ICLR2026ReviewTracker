{"id": "NQq9JLMfNN", "number": 22018, "cdate": 1758324923975, "mdate": 1759896890869, "content": {"title": "Unified 3D Scene Understanding Through Physical World Modeling", "abstract": "Understanding 3D scenes requires flexible combinations of visual reasoning tasks, including depth estimation, novel view synthesis, and object manipulation, all of which are essential for perception and interaction. Existing approaches have typically addressed these tasks in isolation, preventing them from sharing a common representation or transferring knowledge across tasks. A conceptually simpler but practically non-trivial alternative is to unify these diverse tasks into a single model, reducing different tasks from separate training objectives to merely different prompts and allowing for joint training across all datasets. In this work, we present a physical world model for unified 3D understanding and interaction 3WM, formulated as a probabilistic graphical model in which nodes represent multimodal scene elements such as RGB, optical flow, and camera pose. Diverse tasks emerge from different inference pathways through the graph: novel view synthesis from RGB and dense flow prompts, object manipulation from RGB and sparse flow prompts, and depth estimation from RGB and camera conditioning, all zero-shot without task-specific training. 3WM outperforms specialized baselines without the need for finetuning by offering precise controllability, strong geometric consistency, and robustness in real-world scenarios, achieving state-of-the-art performance on NVS and 3D object manipulation. Beyond predefined tasks, the model supports composable inference pathways, such as moving objects aside while navigating a 3D environment, enabling complex geometric reasoning. This demonstrates that a unified model can serve as a practical alternative to fragmented task-specific systems, taking a step towards a general-purpose visual world model.", "tldr": "", "keywords": ["3D Scene Undertstanding", "Visual World Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ff1da5ca926ae7bf1aac507937969d8b315e21e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes 3WM (3D World Model), a unified framework for 3D scene understanding and interaction based on a probabilistic graphical model (PGM) formulation. Unlike task-specific 3D models, 3WM encodes multimodal scene elements (RGB, optical flow, camera pose) as nodes and performs diverse tasks—such as novel view synthesis, depth estimation, and 3D object manipulation—as different inference pathways through the same graph."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a conceptually elegant unification of multiple 3D vision tasks under one probabilistic and autoregressive modeling framework.\n2. The local random-access sequence modeling design and pointer-token formulation are interesting and potentially valuable for scalable multimodal 3D reasoning.\n3. Both quantitative and qualitative experiments have been conducted in sufficient detail."}, "weaknesses": {"value": "1. The presentation quality is inconsistent. The Introduction section fails to clearly and explicitly highlight the main contributions of the paper, making it difficult for readers to grasp the motivation and significance of the work.\n2. A teaser figure should be included in the Introduction to provide an intuitive overview of the proposed framework and help readers quickly understand the key idea and workflow.\n3. The paper lacks comprehensive ablation studies, which are necessary to fully evaluate the generalization ability and effectiveness of the proposed method."}, "questions": {"value": "I am not very familiar with the research areas of Novel View Synthesis and World Model, so my current evaluation mainly focuses on the presentation quality and writing clarity of the paper. I will update my assessment after considering feedback and technical insights from other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4PDNfv2SlR", "forum": "NQq9JLMfNN", "replyto": "NQq9JLMfNN", "signatures": ["ICLR.cc/2026/Conference/Submission22018/Reviewer_WZBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22018/Reviewer_WZBe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486035433, "cdate": 1761486035433, "tmdate": 1762942020767, "mdate": 1762942020767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to propose a method to unify several important tasks of 3D scene understanding. To this end, the authors propose a probabilistic graphical model, where nodes represent multimodal scene elements such as RGB, optical flow, and camera pose. The combinations of these nodes therefore enables different tasks, including novel view synthesis, 3D object manipulation and depth estimation without task-specific training. Experiments show that the unified model outperforms the respective task-specific baselines. The authors also discover that the model supports composable inference pathways, therefore can help other tasks such as navigation and amodal completion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty of the problem. I like the story that the authors want to develop a unified model for different 3D understanding tasks.\n\n2. Novelty of the method. The method to unify the different 3D understanding tasks is also novel and interesting to me.\n\n3. Good evaluation results. Experiment results show the developed unified model generally outperforms the task specific models for novel view synthesis, 3D object manipulation and depth estimation, although it has not been trained for the specific tasks. The results are impressive to me."}, "weaknesses": {"value": "1. Efficiency. It is good to see the authors have developed such a unified model with good performance. It would be better if the authors can also show the efficiency of their model for different tasks compared with the respective task-specific baselines. If the model are going to be applied to embodied AI systems for navigation, for example, the efficiency does matter.\n\n2. Application scenarios. In section 5 of the paper, the authors discuss about the emergent geometric reasoning abilities of the model, and have provided several qualitative examples. However, it increases the paper's contribution and significance if the authors can provide quantitative results on the tasks, such as navigation, and amodal completion, as they mentioned."}, "questions": {"value": "Generally I like the paper. I would recommend the authors to address my concerns in the weakness section during the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mI6YCwLNw8", "forum": "NQq9JLMfNN", "replyto": "NQq9JLMfNN", "signatures": ["ICLR.cc/2026/Conference/Submission22018/Reviewer_Dr8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22018/Reviewer_Dr8R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559058895, "cdate": 1761559058895, "tmdate": 1762942020539, "mdate": 1762942020539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed physical world model aims to enable 3D scene understanding enabling improvements in visual reasoning tasks. \nThe method proposes a probabilistic graphical model operating on scene information such as RGB patches, optical flow, and depth maps, implemented as a 7B autoregressive transformer performing next-token prediction. The model is trained on Big video dataset and 3D vision benchmarks such as Co3D, ScanNet++, and others. The model enables novel-view synthesis, 3D object manipulation, and depth estimation. The paper presents strong results for each of the tasks on standard test benchmarks demonstrating improvements over existing methods. The authors also contribute a new dataset of 100 image pairs for 3D object manipulation which could be useful for future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated and proposes a simple yet effective method enabling strong results for Novel-view synthesis, 3D object manipulation, and depth estimation tasks. The paper expresses the ideas with enough detail for understanding each component and its implementation. The evaluation is comprehensive with existing methods on standard benchmarks for the respective tasks. Composable inference pathways for 3D scene understanding as shown in Figure 6 is a strong result and could be useful for downstream robotics tasks. This can be used for complex reasoning and planning in 3D scenes."}, "weaknesses": {"value": "- The results do not show strong understanding of lighting and appearance understanding based on the results in Figure 3 (the specular highlights on the objects) and 4 (the macbook), as the objects seem to have baked in shading.\n- The object manipulations are limited to rigid transformations and do not show compositional understanding of objects such as stacked objects. The demonstrated examples are limited to the new dataset and would require more evidence to support the claim of improved 3D object manipulation capabilities.\n- The paper should include a discussion on the limitations of the model to better inform the reader about the scope of the contributions.\nSuggestions for improvement of the presentation:\n- I recommend the authors to include a short introduction to each section as an overview of the content.\n- Introduction can include a concise summary of the contributions of the paper.\n\nMinor:\n- L115: an video --> a video"}, "questions": {"value": "- Can the model perform transformation of non-rigid objects such as cloth? If so, how can the transformations be performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KMFLgOOLzY", "forum": "NQq9JLMfNN", "replyto": "NQq9JLMfNN", "signatures": ["ICLR.cc/2026/Conference/Submission22018/Reviewer_AqTG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22018/Reviewer_AqTG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762074643683, "cdate": 1762074643683, "tmdate": 1762942020118, "mdate": 1762942020118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an autoregressive generative model (3WM) for RGB images and optical flow based on an interesting \"pointer-content representation\" that allows a single autoregressive model to generate image or optimal flow output from sparse or dense image or optical flow input. Effectively, the paper trains a single model to learn to approximate a diversity of types of conditional distributions on image and optical flow tokens, via randomization of the training set. The paper demonstrates the resulting model on several tasks including novel-view synthesis, 3D object manipulation, and depth extraction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The pointer-content representation that allows for a single model to approximate a number of conditional generation tasks on the joint space of (RGB image, optical flow, RGB image) is significant. The approach of training a model to approximate diverse types of conditional distributions via randomly selecting and re-reorder \"pointer-content\" pairs is a useful contribution. The qualitative comparisons to baselines shown in the paper are compelling."}, "weaknesses": {"value": "The use of \"graphical model\" as the formal framework for describing the work seems somewhat ill-suited, since there are no conditional independences in the model (and not surprisingly, the the paper includes no graphical models visualized). See \"Questions\" for connections to other formal frameworks for generative modeling that seem more aligned with the \"pointer-content\" representation."}, "questions": {"value": "1. The specific way that training examples were generated from the video dataset seems central to the model. It would help to provide more detail on the type of (latent, observed) pairs that were generated synthetically from the video dataset, and the distribution of these different types of examples at training time?\n2. Since the autoregressive model represents in principle all possible conditional distributions, it would be interesting to study how self-coherent it is. It would be interesting to compare (i) the approximate amortized inferences obtained by sampling from the trained autoregressive model, with (ii) the idealized conditional distribution that is defined by sampling from the unconditioned joint distribution and performing rejection sampling. One way to do this is via estimating expected symmetrized KL divergence on synthetic data generated from the model itself as in \"AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms\" to compare rejection sampling versus the variational approximation learned by the model (this essentially boils down to sampling from the joint model, then evaluating the conditional probability of the latent tokens given the observed tokens under the autoregressive model). Are there certain classes of queries where it is more self-consistent than others? Can you use the joint model to do a little model-based Monte Carlo (importance sampling) to improve upon proposals sampled from the autoregressive amortized approximation of the conditional distribution?\n3.  Interesting connection: The \"pointer-content\" representation doesn't fit well with standard graphical models, but is more closely related to the address-value representation used in the Gen probabilistic programming language (see e.g. section 4.1 of Gen: A General-Purpose Probabilistic Programming System with Programmable Inference) where random choices are assigned labels (\"addresses\") and generative models are probability distribution on dictionaries that map addresses to values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "F42ndFB7B7", "forum": "NQq9JLMfNN", "replyto": "NQq9JLMfNN", "signatures": ["ICLR.cc/2026/Conference/Submission22018/Reviewer_eRy7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22018/Reviewer_eRy7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762203957349, "cdate": 1762203957349, "tmdate": 1762942019827, "mdate": 1762942019827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}