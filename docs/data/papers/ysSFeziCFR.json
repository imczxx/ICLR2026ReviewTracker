{"id": "ysSFeziCFR", "number": 24519, "cdate": 1758357567627, "mdate": 1759896762101, "content": {"title": "Constraint-Aware Federated Learning: Multi-Resource Optimization via Dual Ascent", "abstract": "We present CAFL (Constraint-Aware Federated Learning), a principled approach for multi-resource optimization in federated learning that simultaneously manages energy, communication, memory, and thermal constraints through dual ascent. Unlike existing methods that optimize primarily for convergence, CAFL formulates federated learning as a constrained optimization problem and employs Lagrangian dual methods to dynamically adapt layer freezing, compression levels, and batch sizing. We provide theoretical convergence guarantees for our dual ascent controller and demonstrate that CAFL preserves training effectiveness through token-budget preservation while achieving significant resource savings. Experimental results on character-level language modeling demonstrate a 70.5\\% reduction in energy consumption, 95.3\\% lower communication cost, and 23\\% memory savings compared to FedAvg, while maintaining comparable convergence in training loss.", "tldr": "We present CAFL (Constraint-Aware Federated Learning), a principled approach for multi-resource optimization in federated learning that simultaneously manages energy, communication, memory, and thermal constraints through dual ascent.", "keywords": ["Federated Learning; Constraints-Aware;  Constrained optimization; Lagrangian dual methods; On-Device; Language Models; Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0cc3b32153869ef242d8c275638bc2c17350170.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents CAFL (Constraint-Aware Federated Learning), which formulates federated learning as a constrained optimization problem considering multiple resource constraints (energy, communication, memory, and thermal). The authors employ Lagrangian dual ascent methods to dynamically adapt training parameters including layer freezing depth, batch size, and local training steps. Furthermore, the authors provide a theoretical convergence guarantee for the dual ascent controller. Their experiments on character-level Shakespeare text modeling demonstrate significant resource savings compared to FedAvg, notably 70.5% in energy and 95.3% in communication."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1）The paper tackles resource constraints, a crucial and highly practical challenge for the deployment of federated learning. It takes a systematic approach to federated learning under explicit, multi-dimensional resource constraints, providing a formal constrained optimization framework that is absent in much of the prior literature.\n\n2）The paper is grounded in a mathematically well-motivated dual ascent method that provides clear convergence guarantees for the controller (as shown in Theorem 1 and 2) . Moreover, the introduction of adaptive policy mechanisms controlled by the dual variables shows a careful design that reflects the interplay between resource usage and learning dynamics.\n\n3）The introduction of \"token-budget preservation\" is an interesting idea. By employing gradient accumulation to offset a reduced effective batch size or fewer local steps , the method intelligently decouples physical resource limitations from the algorithm's need to process a consistent number of samples per round."}, "weaknesses": {"value": "1）The literature review is notably outdated and misses a significant body of recent, relevant research. The paper fails to position itself against modern state-of-the-art methods in both resource-constrained FL (e.g., FedRolex [1], DepthFL [2]) and advanced communication-efficient techniques [3, 4]. This lack of engagement makes it difficult to accurately assess the paper's novelty and contribution relative to the current landscape.\n\n2）The models for energy and thermal management are overly simplistic. For instance, Equation 4's claim that energy consumption scales linearly with the number of trainable parameters requires much stronger theoretical or empirical justification, as energy use has complex dependencies on clock frequency, memory access patterns, and other non-trivial factors. Similarly, the thermal model (Eq. 7) is trivial and ignores key dynamics like heat dissipation and ambient temperature.\n\n3）More critically, the framework models aggregate resource budgets for the entire round rather than on a per-client basis. This design choice fundamentally violates the premise of resource-constrained FL, where the bottleneck is the heterogeneous capability of individual devices. For example, if a single device's available memory is insufficient to afford local training, it cannot participate, a critical issue this aggregate model overlooks.\n\n4）The experimental evaluation is not comprehensive enough to support the paper's broad claims. The only baseline used for comparison is FedAvg. The paper fails to benchmark against any other resource-aware, communication-efficient, or adaptive FL methods (as mentioned in point 1), making it impossible to judge its relative performance. The evaluation is restricted to a single, small-scale character-level language model (Shakespeare) using a 2-layer transformer. This is a traditional, simple task, and it is highly unclear if the proposed resource models, control mechanisms, and observed trade-offs would generalize to the larger, more modern architectures (e.g., Llama, ViT) and diverse modalities (e.g., vision) where resource constraints are most pressing.\n\n\n\n[1] Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction, NeurIPS, 2022\n\n[2] Depthfl: Depthwise federated learning for heterogeneous clients, ICLR, 2023\n\n[3] Communication-efficient adaptive federated learning, ICML, 2022\n\n[4] Feddm: Iterative distribution matching for communication-efficient federated learning, CVPR, 2023"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TFMgWMI5eN", "forum": "ysSFeziCFR", "replyto": "ysSFeziCFR", "signatures": ["ICLR.cc/2026/Conference/Submission24519/Reviewer_utLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24519/Reviewer_utLZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729702606, "cdate": 1761729702606, "tmdate": 1762943110928, "mdate": 1762943110928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAFL (Constraint-Aware Federated Learning), a framework that formulates federated learning (FL) as a multi-resource constrained optimization problem. The approach introduces a dual ascent controller that dynamically adjusts training parameters to satisfy multiple resource limits.\n\nThe authors also propose a token-budget preservation technique to maintain training progress under these adaptive constraints and provide theoretical convergence guarantees for the dual variables."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addresses an important practical challenge in FL: real-world resource constraints across multiple modalities.\nThe dual ascent formulation is theoretically principled and integrates smoothly into the FL pipeline.\nThe paper is clearly written and logically structured."}, "weaknesses": {"value": "Narrow experimental validation: Only one small-scale task with simulated clients. The setup lacks diversity and scale to demonstrate generality.\n\nLimited baselines: Comparison only against FedAvg; misses other strong adaptive or efficiency-oriented FL algorithms.\n\nSimplified resource modeling: Linear models for energy, thermal, and memory may not hold for real devices.\n\nTheory–practice gap: Dual convergence is shown, but implications for federated convergence or model accuracy are not quantified.\n\nScalability and heterogeneity are untested, leaving unclear whether CAFL can operate effectively in realistic federated environments.\n\nWeak contextualization: The paper doesn’t situate CAFL against key adaptive/resource-aware FL families (e.g., FedProx, FedAvgM/FedAdam, SCAFFOLD, compression/quantization, device-aware scheduling). A brief compare table (assumptions, constraints handled, objectives, comm/compute cost) would clarify what’s new.\n\nLimited empirical breadth: Evaluation is confined to Shakespeare + one model, with no non-IID partitions, client/device heterogeneity (slow/fast, dropout), or scale variation. Add at least one more dataset/architecture and standard heterogeneity tests to support generality."}, "questions": {"value": "Client heterogeneity:\nHow does CAFL handle heterogeneous clients with differing energy, memory, or communication budgets? Is the dual ascent mechanism still valid when resource limits vary per client, and can the dual variables be defined client-specifically without destabilizing training?\n\nBaseline coverage:\nWhy were only FedAvg results reported? Methods such as FedProx, FedDyn, Scaffold, or FedAdapt target complementary aspects like system heterogeneity or communication efficiency. Even if these are not “unified,” why were they excluded as comparative baselines?\n\nResource-constrained methods:\nHow does CAFL compare against existing resource- or communication-efficient FL techniques (e.g., QSGD, Deep Gradient Compression, or energy-aware FL)? Without such comparisons, it is difficult to judge the effectiveness of the proposed framework relative to prior art.\n\nResource modeling assumptions:\nThe paper assumes linear resource functions for energy, temperature, and memory. What justifies this simplification, and how would the framework behave if more realistic nonlinear or empirically profiled models were used? Would convergence guarantees still hold?\n\nStability and sensitivity:\nHow sensitive is the algorithm to the choice of dual learning rate η and the dead-zone threshold? Were oscillations or instability observed when constraints were tight or when parameters were mis-specified?\n\nScalability and dataset scope:\nWhy was the evaluation limited to a single dataset and small-scale setup? Could CAFL be applied to larger or more standard FL benchmarks (e.g., CIFAR-10, FEMNIST, TinyImageNet) without significant modification? What prevents this extension in the current version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "txrJ3ADQRm", "forum": "ysSFeziCFR", "replyto": "ysSFeziCFR", "signatures": ["ICLR.cc/2026/Conference/Submission24519/Reviewer_jxqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24519/Reviewer_jxqw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909734214, "cdate": 1761909734214, "tmdate": 1762943110630, "mdate": 1762943110630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAFL, a dual-ascent controller that adapts layer freezing depth, local steps, batch size, and compression level to keep energy, communication, memory, and temperature within user-specified budgets during federated training. The method defines simple linear resource models (for energy/communication/memory/thermal), updates dual variables with a dead-zone, and maps them to training knobs via hand-tuned policy functions. Experiments on a simple Shakespeare with 16 clinets claim large energy/communication savings vs. FedAvg at modest performance degradation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Considering multiple constraints - energy, communication, memory, and thermal budgets - is important in FL."}, "weaknesses": {"value": "* Baselines are oudated. The paper compares exclusively to FedAvg; it omits established baselines that already target some of these constraints (e.g., communication-efficient training/quantization/sparsification, system-heterogeneity methods, proximal/variance-reduction approaches).\n* Resource modeling is overly simplistic and tuned. Energy, memory, and thermal equations are linear surrogates with global scaling constants and policy weights that are grid-searched; no device-specific profiling or validation against ground truth.\n* Experimental setup is far from ICLR-level. Only a small, synthetic Shakespeare char-LM with 2 layers, 16 clients, and random participation; no realistic cross-device benchmarks (e.g., FedScale/LEAF, non-IID vision or mobile NLP), no on-device evaluation, no wall-clock/latency or user-experience metrics. Claims of energy/thermal benefits are therefore simulation-based, not measured.\n* In addition, experimental results has no confidence intervals/error bars; no client-level fairness metrics; no heterogeneity scenarios (varying budgets or device classes); no ablations for η, dead-zone size, or policy weights."}, "questions": {"value": "* Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3PKGs1EBtf", "forum": "ysSFeziCFR", "replyto": "ysSFeziCFR", "signatures": ["ICLR.cc/2026/Conference/Submission24519/Reviewer_z3TJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24519/Reviewer_z3TJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083525602, "cdate": 1762083525602, "tmdate": 1762943110366, "mdate": 1762943110366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates energy, communication, memory, and thermal constraints in a federated setting as a lagrangian equations, and attempts to limit the resource usage under the budget while conducting federated learning. The paper is only less than 6 pages, not well written, missing significant details on methodology and evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ Lagrangian formulation of constraints in FL setting"}, "weaknesses": {"value": "The description of the dual ascent optimization seems incomplete. Line 118-128 seems truncated, missing punctuations, and could use rounds of polishing\n\nThere is a significant lack of details on how the Lagrangian formulation relates to the training process, and how that connects to resource usage reductions. The paper is only less than 6 pages long, and could use the 3-page spaces to fill those in.\n\n1) Energy Consumption. Energy usage is proportional to computational workload. 2) Communication overhead depends on model size and compression. 3) Memory usage scales with batch size and model size. 4) Thermal load depends on computational intensity.\n\nClearly, the constraints are not orthogonal but correlated. It would be nice to rationalize the design to give each an independent lambda in the Lagrangian formulation. How does the covariance factor in? \n\nHow does the proposed method compare to existing related work? It would be helpful to compare with a wide spectrum of existing methods and conduct an ablation study."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EM8AxAKRcC", "forum": "ysSFeziCFR", "replyto": "ysSFeziCFR", "signatures": ["ICLR.cc/2026/Conference/Submission24519/Reviewer_bdu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24519/Reviewer_bdu6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121002626, "cdate": 1762121002626, "tmdate": 1762943109970, "mdate": 1762943109970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}