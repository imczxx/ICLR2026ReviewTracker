{"id": "7iFZ6uzILL", "number": 8002, "cdate": 1758050466113, "mdate": 1759897816075, "content": {"title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding", "abstract": "Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval.", "tldr": "VisR-Bench is a comprehensive benchmark dataset for question-driven, multilingual, and multimodal document retrieval in long documents.", "keywords": ["Document Retrieval", "Vision Question Answering", "Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b0f402c5847bdc0765adb37a920006c13b89664.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents **VISR-BENCH**, a benchmark for evaluating Visual Retrieval-Augmented Generation (RAG) in multilingual long-document understanding. It addresses gaps in existing benchmarks by integrating tables, figures and *16 languages* (including low-resource ones like Swahili). Key components:  \n- A dataset of long documents with QA pairs targeting textual/visual information;  \n- An evaluation framework adopting **PNLS (Partial Normalized Levenshtein Similarity, proposed by Chen et al., 2024a)** for character-level partial matching, plus **GAcc** (GPT-based semantic consistency);  \n- Experiments on 13 models (e.g., GPT-4o, Llama 3) showing closed-source models outperform open-source ones (especially on visual-related tasks), but multilingual low-resource performance remains a bottleneck.  \n\nContributions: \n1) Introduce VisR-Bench, a benchmark for evaluating MLLMs’ retrieval capabilities in multilingual/multimodal settings (16 languages, diverse docs); \n2) Evaluate diverse retrieval models (text-based, multimodal encoders, MLLMs) to quantify performance across evidence types/languages; \n3) Reveal MLLMs’ advantages over other models, as well as their challenges with structured docs/low-resource languages, providing insights for improvement."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Originality**: Combines long documents, visual content, and multilingualism (rare in prior benchmarks like DocVQA/XOR-Retrieval); thoughtfully selects PNLS to solve length bias/partial answer issues of EM/ROUGE-L in long-document tasks.  \n2. **Quality**: Transparent dataset curation (source, annotation logic); comprehensive experiments (with/without visual retrieval) using dual metrics for robust evaluation.  \n3. **Clarity**: Logical structure (problem→design→results); PNLS application details (e.g., alignment/normalization steps) explained clearly; intuitive visualizations (e.g., language-accuracy heatmaps).  \n4. **Significance**: Standardizes retrieval-focused evaluation for MLLMs in two high-impact, underaddressed areas—low-resource language retrieval and multimodal structured layout (e.g., table) QA；the empirical findings (e.g., MLLMs’ struggles with low-resource languages and table layouts) directly highlight critical pain points, guiding future research to prioritize optimization for these scenarios."}, "weaknesses": {"value": "1. **Lack of Data Quality Assessment**: The paper does not provide systematic evaluation of the VisR-Bench dataset’s quality—for example, there is no analysis of annotation consistency (e.g., inter-annotator agreement for QA pairs targeting tables or low-resource languages), nor verification of the accuracy of \"explicit/implicit answer\" labeling. This omission raises uncertainty about whether dataset noise (e.g., incorrect answer annotations) might skew the experimental results of retrieval model evaluations.\n2. **Unvalidated Claim About Queries Without Explicit Answers**: The abstract mentions \"we include queries without explicit answers, preventing models from relying on superficial keyword matching,\" but the main text and appendix lack corresponding experiments to validate this design. There is no comparison (e.g., PNLS/GAcc differences) between models’ performance on \"queries with explicit answers\" and \"queries without explicit answers,\" nor evidence that the latter truly reduces models’ reliance on superficial keyword matching—making this key design feature unsubstantiated.\n3. **Insufficient Error Analysis:**: The paper states that error analysis is included in the appendix, but the appendix only provides two error examples without in-depth analysis (e.g., categorization of error types like \"layout misinterpretation\" or \"low-resource language ambiguity,\" or statistical distribution of errors across models/languages)."}, "questions": {"value": "1. What is VISR-BENCH’s document length distribution? Have you tested performance scalability with longer documents?  \n2. Will you add multilingual transfer experiments (English-trained → low-resource evaluation)? What metrics will quantify transfer efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c4N1Bkv6xc", "forum": "7iFZ6uzILL", "replyto": "7iFZ6uzILL", "signatures": ["ICLR.cc/2026/Conference/Submission8002/Reviewer_pw8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8002/Reviewer_pw8V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760947276132, "cdate": 1760947276132, "tmdate": 1762920004665, "mdate": 1762920004665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisR-Bench, a comprehensive multilingual benchmark for multimodal retrieval in long documents. The dataset covers sixteen languages, featuring over 53K high-quality synthetic QA pairs and 1,286 documents, with detailed documentation of its construction process. VisR-Bench supports question-driven retrieval across text, figures, and tables, and uniquely includes unanswerable queries to prevent models from exploiting keyword overlaps. The authors evaluate three categories of models, including text-based retrievers, multimodal encoders, and MLLMs, on both retrieval and VQA tasks. Results indicate that MLLMs outperform the other two categories overall, but all models exhibit substantial performance degradation in low-resource languages and structured table understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed benchmark is the first multilingual long-document retrieval benchmark, making it a valuable contribution to the study of multilingual multimodal evaluation.\n2. The experiments and analyses are comprehensive, covering different categories of models and clearly demonstrating the necessity of MLLMs for multilingual long-document understanding."}, "weaknesses": {"value": "1. While the benchmark covers multiple languages, it includes only phonographic languages and lacks logographic ones such as Chinese, which limits its linguistic diversity and generalization scope.\n2. Given that Gemini is well known for its strong long-context and multilingual capabilities, its absence in the evaluation is notable."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "giQWfqdEN5", "forum": "7iFZ6uzILL", "replyto": "7iFZ6uzILL", "signatures": ["ICLR.cc/2026/Conference/Submission8002/Reviewer_3A7H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8002/Reviewer_3A7H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656918868, "cdate": 1761656918868, "tmdate": 1762920004138, "mdate": 1762920004138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisR-Bench, a synthetic benchmark dataset comprising 35K QA pairs across 1.2K documents in 16 languages for evaluating multilingual visual retrieval. Each PDF document is parsed using the Adobe Document Extract API, which extracts text, figures, and tables; GPT-4o then generates corresponding QA pairs. The dataset is designed to test retrieval and visual understanding across languages and modalities (text, tables, figures). The authors benchmark a wide range of retrieval methods — text-based, multimodal encoders, and multimodal large language models (MLLMs). Results show that retrieval performance degrades for low-resource languages and non-text modalities (figures and tables) relative to English text documents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive model coverage: The authors evaluate a diverse set of retrieval models, including text-only, multimodal encoders, and MLLM-based approaches.\n\n2. Strong clarity and structure: The paper is well written, logically organized, and easy to follow, making its experimental design and contributions accessible.\n\n3. Data curation: The paper notes that all documents underwent human validation to ensure exclusion of harmful content and PII, enhancing dataset safety and reliability.\n\n4. Dataset Diversity: The benchmark includes many QA pairs and documents, spanning 16 languages and incorporating figures, tables, and multilingual text, making it comprehensive."}, "weaknesses": {"value": "1. Limited evaluation of reasoning-capable MLLMs.\nIt would strengthen the analysis to include recent reasoning-optimized MLLMs (e.g., OpenAI o3), even on a small, hard subset. These models could provide deeper insights into reasoning gaps and multimodal generalization.\n\n2. Over-reliance on Top-1 accuracy.\nThe use of Top-1 retrieval accuracy as the primary metric may overstate model weaknesses. While the paper reports a Top-1 accuracy of ~75.2%, the Top-5 accuracy reaches 94.1%, suggesting retrieval systems can already surface the correct document in most cases. Given that RAG pipelines typically include rerankers or pass multiple candidates to the generator, the claimed “large room for improvement” may be somewhat overstated.\n\n3. Benchmark Novelty: While multilinguality and visual context are both valuable, it’s unclear how the dataset offers fundamentally new insights over existing multilingual or visual-only benchmarks.\n\n4. Dataset balance and comparability.\nIt is unclear whether the proportions of document types (text / table / figure) are consistent across languages, which could affect the fairness of multilingual performance comparisons.\n\n5. Lack of Human QA Verification: I did not see the mention of human validation of the QA pairs beyond filtering for safety. This raises concerns about factual correctness and answerability.\n\n6. Limited Dataset Analysis and Insights:\nThe paper introduces a large dataset, but offers minimal exploratory analysis that could help the community understand its properties and challenges. Additional insights about Document-Type Composition in multilingual subsets, Question Type Taxonomy, Visual Density, Domain Diversity, and more would strengthen the paper."}, "questions": {"value": "1. On the multilingual gap:\nIf performance degradation in low-resource languages mirrors existing text-based multilingual retrieval gaps, what additional insights does VisR-Bench provide? Do results show cases where models are strong in text retrieval for a language but significantly weaker in multimodal (VQA) retrieval for the same language?\n\n2. On ColQwen2 variants:\nHow do you explain ColQwen2-M’s lower performance compared to the base ColQwen2-v0.1?\n\n3. Text vs. MLLM retrieval:\nIn multilingual settings, why do text-based retrievers sometimes outperform MLLMs?\n\n4. Dataset balance:\nDo all languages contain comparable proportions of text-, table-, and figure-based QA pairs?\n\n5. GPT-4o QA Accuracy: Since GPT-4o generated the QA, why can’t it achieve perfect accuracy?\n\n6. Table QA Filtering: Did the table-based QA undergo the same heuristic filtering as figure-based QA to ensure the question requires table understanding and can’t be answered from the surrounding text?\n\n7. Human QA Validation: Beyond filtering for harmful content and PII, was there any human validation to confirm that the synthetic questions are answerable and the answers are correct?\n\nL289:  English English split -->  English split\nL481: VisR-Benchpaves --> VisR-Bench paves"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Rsby4ErPph", "forum": "7iFZ6uzILL", "replyto": "7iFZ6uzILL", "signatures": ["ICLR.cc/2026/Conference/Submission8002/Reviewer_1xa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8002/Reviewer_1xa4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695798882, "cdate": 1761695798882, "tmdate": 1762920003819, "mdate": 1762920003819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark VisR-Bench, focusing on evaluating the multilingual multimodal retrieval in long documents. This benchmark is for going beyond surface-level similarities and incorporating deeper semantic and layout understanding. \nIt is constructed by collecting PDF files, leveraging Adobe document parser, designing heuristics, and prompting LLMs.\nBy evaluating existing retrieval methods, the paper demonstrates a series of findings on retrieval methods, multilingual, and modality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explores a significant challenge of multimodal retrieval for multimodal RAG.\n\n- The benchmark includes not only English, but also 15 non-English test samples.\n\n- The paper is clear and well presented, which is easy to understand."}, "weaknesses": {"value": "The benchmark was constructed using several strong assumptions, which could lead to biases and inaccuracies in the evaluation results.\n\n- When feeding the documents into LLMs to derive the corresponding QA pairs, no matter figure-, table-, or text-based QA pairs, the input documents are assumed as the oracle. This seems reasonable, but there might be other documents (not the input documents) can lead to the correct answer.\n\n- In terms of the heuristics that enforce figure-, table-, or text-related QA pairs, e.g., for table-related QA, extract pages that contain tables but no figures to ensure that the generated questions are not influenced by visual elements, It's unclear such assumption is reasonable without any verification. It would be helpful to enhance the paper by adding the error analysis in this process.\n\n- The whole construction process is entirely based on LLMs and heuristics. It should be OK if the process is trustworthy enough. However, there is no such analysis provided, e.g., human correlation analysis on a subset, which is strongly necessary.\n\n- The evaluation findings are not surprising. For example, Retrieval of table content is still challenging, Struggle on Low-Resource Languages. Clearly demonstrating and emphasizing the distinct findings can enhance the novelty of this paper."}, "questions": {"value": "Just curious, Chinese data is also common. Why Chinese is not included in the non-English class?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P4pgT7upV1", "forum": "7iFZ6uzILL", "replyto": "7iFZ6uzILL", "signatures": ["ICLR.cc/2026/Conference/Submission8002/Reviewer_2yjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8002/Reviewer_2yjC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763110002, "cdate": 1761763110002, "tmdate": 1762920003458, "mdate": 1762920003458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}