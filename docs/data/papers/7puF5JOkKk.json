{"id": "7puF5JOkKk", "number": 12130, "cdate": 1758205827948, "mdate": 1759897529952, "content": {"title": "Conformal Regression under Distribution Shift: A Reinforcement Learning Method for Adaptive Uncertainty Quantification", "abstract": "Conformal prediction (CP) offers distribution-free uncertainty quantification with formal coverage guarantees, and has been widely applied to regression tasks, including time-series forecasting. However, in time-series settings, the exchangeability assumption underlying CP is often violated due to temporal dependencies. To address this, recent adaptive CP methods mitigate distributional shifts by dynamically calibrating intervals based on recent residuals and adaptive weighting strategies. However, these methods remain limited by their sensitivity to outliers, inability to detect systematic prediction bias, and the decoupling of calibration from model learning. In this work, we introduce CORE that establishes a mutual feedback loop between reinforcement learning (RL) and conformal prediction for adaptive uncertainty quantification. The method leverages RL's exploration capability to better cover uncertain or outlier regions, adapts calibration through exploration feedback, and designs uncertainty-guided rewards, enabling dynamically improved prediction and interval quality through policy interaction and feedback. We conduct extensive experiments to validate its effectiveness across 8 time-series standard datasets. The results demonstrate that our approach achieves superior accuracy and calibration, consistently outperforming 6 state-of-the-art baselines, with an average improvement of 1.36% in coverage rate and 5.03% in interval length.", "tldr": "", "keywords": ["Conformal Prediction", "Reinforcement Learning", "Uncertainty Quantification", "Time-series Forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/415fe2600ee685c79b22dc73c2d2714bea583512.pdf", "supplementary_material": "/attachment/75a8b869b4f5eb3fcd9550711523aff6dae1f51a.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents CORE, a conformal prediction method that can adapt to distribution shifts by adjust its prediction interval output using Reinforcement learning (RL). The policy predicts quantiles (of a set $\\mathcal{Q}$ ) of the output variable, and is trained to maximize reward (a combination of coverage, accuracy, and tightness objectives) via the PPO algorith. The authors present theoretical and empirical support for the method."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is novel and interesting to use reinforcement learning to optimize for coverage - this is inline with recent works that treats coverage as a controllable variable such as in [1] or [2]. The RL setting is well thought of, the 3-part reward that is robust to $\\lambda$ choices is a nice design. \n\n- The paper, other than the theoretical proofs section, is written clearly and easy to follow. \n\n- The experiments cover a lot of ground, spanning 8 datasets and 3 tasks (UQ, prediction, and anomaly detection). Clear analysis in 5.3 as well."}, "weaknesses": {"value": "1. Missing some key baselines. There is almost an explosion of  time series conformal prediction work in 2024 and 2025, but the literature review on CP stops at 2022 (if we take out SPCI) and most recent baseline that the authors compared to is SPCI. Some important work in this lineage that comes to mind are \n\n[1] Auer, Andreas, et al. \"Conformal prediction for time series with modern hopfield networks.\" Advances in neural information processing systems 36 (2023): 56027-56074.\n\n[2] Angelopoulos, Anastasios, Emmanuel Candes, and Ryan J. Tibshirani. \"Conformal pid control for time series prediction.\" Advances in neural information processing systems 36 (2023): 23047-23074.\n\n[3] Lee, Jonghyeok, Chen Xu, and Yao Xie. \"Kernel-based optimally weighted conformal time-series prediction.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[4] Li, Ruipu, and Alexander Rodríguez. \"Neural Conformal Control for Time Series Forecasting.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 17. 2025.\n\nand for the anomaly detection task\n\n[5] Lekeufack, Jordan, et al. \"Conformal decision theory: Safe autonomous decisions from imperfect predictions.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\n\n[6] Zecchin, Matteo, and Osvaldo Simeone. \"Localized adaptive risk control.\" Advances in Neural Information Processing Systems 37 (2024): 8165-8192.\n\n[7] Zhang, Shuai, et al. \"Conformal anomaly detection in event sequences.\" Forty-second International Conference on Machine Learning. 2025.\n\nThere are just so, so many. I do not expect the authors to compare to all of them in experiment, but at least include them in discussion and compare to a few more recent baselines. In particular, the core message of this paper is treating the prediction interval as a target optimizable by RL - this is similar in spirit to seeing ti as control target in the Conformal PID paper. Given how sample-inefficient and unstable RL is, I do not see why RL can outperform gradient based methods such as PID. \n\nSimilarly, the \"extension to anomaly detection\" is not new. Maybe we should compare to conformal risk control as well as a more sophisticated frequentist AD algorithm? \n\n2. Unclear ablation studies. I would like to see the full results of section 5.3 / figure 3 as well. There are many parts in the algorithm contributing to the adaptation mechanism -  the ACI adjustment, the RL quantile regression, and the conformal quantiles used to build the intervals. Can you do an ablation-study style experiment to show why all the parts are needed?   \n\n3. Misleading experiment presentation. What we care about for UQ is *calibration*, not coverage itself? if your target coverage is 90%, getting 93% is bad (not as bad as under-covering, but uncalibrated nonetheless), no?  Claiming \"improvement of 1.36% in coverage rate\" is misleading. \n\n4. Some inconsistencies in the theory. See questions section."}, "questions": {"value": "1. What is the base prediction model for ACI / SPCI / EnbPI? The result from figure 2 seems like the prediction is almost flat. What happens to the interval width if they use a strong predictor, such as TimeXer?\n\n2. Why is the interval constructed as in eq. (3)? This is different than the CQR setup, where the $\\hat{q}$ is not the absolute error as in this paper, but the error of the interval. Doesn't the resulting interval become ```[0.05 percentile of preds - 0.9 percentile of errors, 0.95 percentile of preds + 0.9 percentile of errors]```, doubling the width of a theoretically-just interval? \n(for example, for quantile regression alone the interval is  ``` [0.05 percentile of preds , 0.95 percentile of preds] ```, and for conformal prediction alone it is ```[mean - 0.9 percentile of errors, mean + 0.9 percentile of errors]```. \n\n[1] Romano, Yaniv, Evan Patterson, and Emmanuel Candes. \"Conformalized quantile regression.\" Advances in neural information processing systems 32 (2019).\n\n\n3. The above point is also the inconsistency in the theory. In D.1, you say \n\nBy construction of the interval, the event \n\n$y_t \\in \\hat{C}^{(w_c ,t)}_{1−\\alpha_t}$\n\nis equivalent to the event that the absolute residual at time $t$ is at most $\\hat{q}^{(w_c ,t)}_{1−\\alpha_t}$. \n\nThis is not correct, it should be something like: the absolute residual at time $t$ is at most $\\hat{q}^{(w_c ,t)}_{1−\\alpha_t} + \\max{ (\\alpha_t^{0.5} - \\alpha_t^{(low)}, \\alpha_t^{(up)}-\\alpha_t^{0.5} ) }$ ?\n\n\n- The definition of $\\hat{\\epsilon}_t$ is inconsistent through the paper. in D.2 it is the absolute error, in D.3 it is the  empirical miscoverage estimate, and in Proposition 2 it is the instantaneous coverage error at time step $t$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1L5EpJAYYr", "forum": "7puF5JOkKk", "replyto": "7puF5JOkKk", "signatures": ["ICLR.cc/2026/Conference/Submission12130/Reviewer_RYgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12130/Reviewer_RYgc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761104023379, "cdate": 1761104023379, "tmdate": 1762923093675, "mdate": 1762923093675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **CORE (Conformal Reinforcement Learning)**, an adaptive online conformal prediction framework that integrates reinforcement learning (RL) into conformal calibration for time-series forecasting. The method establishes a feedback loop where an RL agent produces quantile-based predictions (trained via pinball loss), and an adaptive mechanism updates the coverage level $\\alpha$ over time. CORE leverages RL’s exploration to improve calibration under distributional shifts and outliers. The authors provide coverage guarantees under $\\beta$-mixing assumptions and report consistent gains over adaptive CP baselines such as ACI, SPCI and EnbPI on multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of **reinforcement learning and conformal prediction** is an interesting and novel idea, potentially bridging two communities (distribution-free uncertainty quantification and sequential decision-making).\n\n- The algorithm combines quantile regression, adaptive calibration, and reinforcement learning in a coherent computational pipeline. The adaptive α-update resembles ACI but is coupled to the RL agent’s training, which is conceptually elegant.\n\n- Experiments show promising improvements over existing online or adaptive CP methods such as ACI, SPCI and EnbPI."}, "weaknesses": {"value": "1. **Unclear problem formulation.**  \n   The paper never explicitly formulates the *statistical setting* it considers. It remains ambiguous whether the data sequence $(X_t, Y_t)$ is assumed i.i.d., $\\beta$-mixing, or piecewise stationary. Without a formal description of the environment and objective (what is optimized by the agent, what “policy” means in this context), it is difficult to judge the validity of the method and theory.\n\n2. **Severe notational inconsistency and missing definitions.**  \n   The text contains numerous undefined or inconsistently defined symbols, which makes the paper extremely difficult to follow. Examples include:\n   - Line 157: $a_t$ is defined as *quantile-based predictions*, yet line 189 introduces $S(x_t, y_t) = |\\hat{f} (a_t^{(0.5)}) - y_t|$ with an unexplained $\\hat{f}$.  \n   - The symbol $\\varepsilon_t$, used throughout Section 4 and in the theoretical results, is defined only in the appendix (around line 1041).  \n   - Symbols $\\tilde{a}_t$ and $a_t^\\star$ appear in the definitions of $\\hat{\\varepsilon}_t,\\varepsilon_t$ but are never formally defined.  \n   - $\\delta_T^2$ is defined twice, inconsistently: empirical squared error (around line 300) and population expectation (around line 310). The so-called “median prediction error” in Proposition 1 seems to correspond to the overall mean squared error of the predictor rather than any median-specific quantity.\n\n   These inconsistencies make it difficult to verify the theoretical claims.\n\n3. **Disconnection between method and theory.**  \n   The theoretical analysis operates on abstract quantities ($a_t^\\star, \\delta_T^2, \\varepsilon_t$) without linking them concretely to the implemented algorithm (which trains via quantile regression and ACI-like $\\alpha$ updates). The theory thus reads as an independent construction that does not clearly justify the empirical method.\n\n4. **Limited ablation and interpretability.**  \n   Although the model outputs multiple quantiles, the evaluation uses only the median prediction. The paper does not empirically justify why learning the entire quantile function is necessary. An ablation comparing (i) median-only training vs. (ii) full quantile-based actor would be essential to substantiate the methodological contribution.\n\n\n5. **Ambiguity and weak motivation in the use of quantile levels.**  \n   Equation (3) constructs the interval using only the lowest and highest quantile outputs $a_t^{(\\mathrm{low})}$ and $a_t^{(\\mathrm{up})}$, yet the paper never explains how the quantile levels $q^{(1)},\\ldots,q^{(K)}$ are chosen or what benefit this design provides. The intuition behind using multiple quantiles is unclear, especially since the paper illustrates the example with $K=3$, effectively relying only on the low, median, and high quantiles. For $K>3$, the additional quantile outputs are never utilized in interval construction, leaving their role and training benefit unexplained. Clarifying the rationale and providing intuition or empirical evidence for how these quantiles improve performance would significantly strengthen the paper.\n\n\n6. **Lack of PPO implementation details.**  \nThe paper claims to train the quantile-based predictor using PPO but provides no concrete implementation details. Since PPO is central to the approach, at least a brief pseudo-code or appendix description is necessary."}, "questions": {"value": "Please see the issues raised in the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fCZEsryIns", "forum": "7puF5JOkKk", "replyto": "7puF5JOkKk", "signatures": ["ICLR.cc/2026/Conference/Submission12130/Reviewer_39Se"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12130/Reviewer_39Se"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746104258, "cdate": 1761746104258, "tmdate": 1762923093260, "mdate": 1762923093260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CORE, a framework integrating adaptive conformal prediction and reinforcement learning for valid, efficient uncertainty estimates under distribution shift . It models adaptive calibration as an MDP, an RL agent outputs quantile-based predictions, with a composite reward encouraging accuracy, short intervals and proper coverage. The authors provide theoretical guarantees for CORE’s near-nominal coverage and efficiency under weak dependence and bounded drift. Experiments on time-series and anomaly detection datasets show CORE outperforms some existing adaptive CP methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel integration of conformal prediction and reinforcement learning through the CORE framework, enabling adaptive uncertainty calibration under distribution shift.\n2. It provides solid theoretical guarantees for coverage validity and efficiency, supported by comprehensive experiments on diverse time-series datasets showing consistent performance gains.\n3. The writing is clear and well-structured, with intuitive explanations and effective visualizations that enhance readability and understanding."}, "weaknesses": {"value": "1. The proposed reward design, while intuitively appealing, relies on heuristic balancing among accuracy, length, and coverage without deeper analysis of the reward landscape or convergence behavior. It remains unclear how these competing objectives interact or whether they can lead to suboptimal equilibria.\n2. The baseline selection is relatively limited, omitting more recent and competitive approaches like [1] and [2] that are highly relevant to adaptive uncertainty calibration. Including these would provide a fairer and more up-to-date comparison.\n\n[1]: Angelopoulos A, Candes E, Tibshirani R J. Conformal pid control for time series prediction[J]. Advances in neural information processing systems, 2023, 36: 23047-23074.\n\n[2]: Wu J, Hu D, Bao Y, et al. Error-quantified Conformal Inference for Time Series[C]//The Thirteenth International Conference on Learning Representations.  \n\n3. The Markov state design, defined as a sliding window of length w, appears to be a crucial modeling choice. However, the paper provides little discussion or ablation analysis regarding how different window sizes influence the policy’s performance or stability. A systematic exploration of w’s impact would strengthen the empirical credibility of the framework."}, "questions": {"value": "1. Cold-start and training stability are noted but not deeply studied. What failure modes occur early in training, and can lightweight safeguards (e.g., fallback classical CP, conservative prior on $\\alpha_t$) mitigate them?\n2. What is the computational overhead of running CORE (training and online inference) compared to lightweight adaptive CP methods, and is the method feasible for real-time applications with tight latency constraints?\n3. The composite reward mixes accuracy, interval length, and coverage; do the authors observe multimodal or unstable reward landscapes in practice, and how were the reward weights chosen to avoid pathological equilibria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J2EfohkQyK", "forum": "7puF5JOkKk", "replyto": "7puF5JOkKk", "signatures": ["ICLR.cc/2026/Conference/Submission12130/Reviewer_fuGq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12130/Reviewer_fuGq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810246344, "cdate": 1761810246344, "tmdate": 1762923092883, "mdate": 1762923092883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CORE, a novel framework for adaptive uncertainty quantification in time-series forecasting. The authors identify key failures in existing adaptive CP methods, which are often post-hoc, sensitive to outliers, and unable to correct for systematic model bias due to the decoupling of model training and calibration.\nThe central contribution is to unify reinforcement learning and conformal prediction into a mutual feedback loop. The problem is framed as a sequential decision task where an RL agent  learns to output quantile predictions. These predictions are then fed into an \"adaptation-aware calibration\" module that uses a sliding window of recent residuals to produce a final, calibrated prediction interval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n1. Integrating Reinforcement Learning in adaptive conformal prediction is novel, which let the prediction model aware of the time-rolling coverage and thus lead to potentially better coverage over time\n2. Comprehensive theory with marginal coverage guarantee\n3. Extensive experiment showing the benefit of the method in uncertainty quantification and prediction accuracy"}, "weaknesses": {"value": "Weakness:\n1. The RL training typically requires much data and the method may perform poorly with limited data\n2. The prediction model's awareness of coverage might sacrifice the prediction accuracy"}, "questions": {"value": "1. In the second paragraph of section 3.3, the author computed a conformity score |\\hat{f}(a^{0.5})-y_t|, where a is a median prediction of y. What is \\hat{f} then?\n\n2. In the experiment, what are the base predictors used by the other CP methods?\n\n3. It seems to me that the RL model can be replaced by a prediction model that keeps updating on new data, and the confidence level update is fulfilled by the Adaptive CP method. The difference only lies in that the RL reward makes the prediction recognize of the coverage. Is this true? If it is true, will this sacrifice the prediction accuracy?\n\n4. Considering the cold-start instability and that  RL requires much data for training, how many data would the method need to show stable performance?\n\n5. Is it possible to use RL learn the adaptive confidence level instead of use a constant update like Adaptive CP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jIV3ocPUCa", "forum": "7puF5JOkKk", "replyto": "7puF5JOkKk", "signatures": ["ICLR.cc/2026/Conference/Submission12130/Reviewer_29Gc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12130/Reviewer_29Gc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12130/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871558974, "cdate": 1761871558974, "tmdate": 1762923092445, "mdate": 1762923092445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}