{"id": "jtrhwfgseW", "number": 11155, "cdate": 1758191357847, "mdate": 1759897603810, "content": {"title": "VLAs Are Confined yet Capable of Generalizing to Novel Tasks", "abstract": "Vision-language-action models (VLAs) often achieve high performance on demon-\nstrated tasks but struggle significantly when required to extrapolate, recombining\nskills used in different tasks in novel ways. For instance, VLAs might successfully\nput the cream cheese in the bowl and put the bowl on top of the cabinet, yet still\nfail to put the cream cheese on top of the cabinet. This motivates us to investigate\nwhether VLAs merely overfit to demonstrated tasks or still hold the potential to\nextrapolate. Our study uses text latent as the ingredient; it is a task-specific vector\nderived from the models’ hidden states. It thus encodes semantics necessary for\ncompleting a task and can be used to reconstruct the associated task behavior by\nwriting it to the model’s residual stream. Furthermore, we find that skills used\nin distinct tasks can be combined to produce novel behaviors by blending their\nrespective text latent. Applying this to π0, we increase its success rate from 9% to\n83% on the proposed libero-ood benchmark, which features 20 tasks extrapolated\nfrom standard LIBERO tasks. This reveals that the skill representations encoded in\ntext-latent are individual yet composable, while π0 fails to autonomously combine\nthese representations for extrapolation. This also validates the design of libero-ood;\nit comprises tasks that the model fails, yet should be able to complete. We then\ntested other VLAs on libero-ood, and none of them achieved a success rate higher\nthan 21%. Further analysis reveals VLAs share a common pattern to exhibit spatial\noverfitting, associating object names with where the object is spatially located in\nthe demonstrated scene rather than achieving true object and goal understanding.", "tldr": "VLAs fail to generalize, while it holds the potential to do so.", "keywords": ["Vision Language Action Models", "Generalization", "Interpretability", "Benchmark"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a959fdda3f49f4bf09cfe6e81b9ee1f82a16c5f.pdf", "supplementary_material": "/attachment/5cfab070e8e1d5a07d1877097a796185574fa964.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides an insightful diagnosis of why Vision-Language-Action (VLA) models fail at compositional generalization. It demonstrates that SOTA models (like $\\pi_0$) do learn the necessary sub-skills but cannot autonomously compose them for novel tasks. Using a novel inference-time technique called Text Latent Interpolation (TLI), the authors prove this latent capability exists (boosting success from 9% to 83%) and identify Spatial Overfitting—where the model memorizes locations instead of understanding objects—as a root cause for this failure."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the critical and well-defined problem of compositional generalization in robotics.\n2. Offers a deep diagnosis of why SOTA models fail, rather than just proposing a new model.\n3. The TLI method is a clever experimental tool that compellingly proves the model's \"locked\" potential (9% $\\rightarrow$ 83% success).\n4. Contributes a valuable and challenging new benchmark (libero-ood) that effectively exposes this common VLA failure mode.\n5. Identifies \"Spatial Overfitting\" as a specific root cause, revealing models are memorizing locations, not learning object semantics."}, "weaknesses": {"value": "1. The proposed TLI method is an inference-time \"trick,\" not a fundamental solution. The model itself remains incapable of autonomous composition.\n2. TLI is impractical for real-world use, as it requires a priori knowledge of the task decomposition (e.g., knowing \"A $\\rightarrow$ C\" = \"A $\\rightarrow$ B\" + \"B $\\rightarrow$ C\").\n3. The method relies on manually-tuned hyperparameters (like interpolation speed), limiting its generality.\n4. All experiments are in simulation, making it unclear if the observed flaws are fundamental or an artifact of the training environment."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nope."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f1Q0Nzj0TH", "forum": "jtrhwfgseW", "replyto": "jtrhwfgseW", "signatures": ["ICLR.cc/2026/Conference/Submission11155/Reviewer_KYX7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11155/Reviewer_KYX7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915047193, "cdate": 1761915047193, "tmdate": 1762922319998, "mdate": 1762922319998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates skill composability in VLAs via text latent interpolation and introduces an OOD benchmark, revealing limited generalization despite fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method demonstrates that latent skill representations can be combined to solve novel tasks, improving OOD performance significantly."}, "weaknesses": {"value": "1. The motivation of this paper is unclear in the abstract. The example given, \"VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet,\" is incorrect. Why? Existing VLA models can do this; why can't they?\n2. The abstract's conclusion, \"Further analysis reveals that VLAs share a common pattern to exhibit spatial overfitting, associating object names with where the object is spatially located in the demonstrated scene rather than achieving true object and goal understanding,\" is also problematic. Many papers have already proposed and proven that large models only possess associative abilities but lack logical reasoning capabilities. This paper's innovation is weak."}, "questions": {"value": "1. The motivation of this paper is unclear in the abstract. The example given, \"VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet,\" is incorrect. Why? Existing VLA models can do this; why can't they?\n2. The abstract's conclusion, \"Further analysis reveals that VLAs share a common pattern to exhibit data overfitting, associating object names with where the object is spatially located in the demonstrated scene rather than achieving true object and goal understanding,\" is also problematic. Many papers have already proposed and proven that large models only possess associative abilities but lack logical reasoning capabilities. This paper's innovation is weak.\n3. How was the libero-ood benchmark validated to ensure it fairly assesses generalization?\n4. Were hyperparameters like λ tuned systematically, and how do they impact the robustness of TLI across tasks?\n5. What measures were taken to ensure that the compared VLAs were evaluated under identical conditions and fine-tuning protocols?\n6. Does the VLA model truly have an OOD problem? If the amount of data is large enough, does it still have an OOD problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YqTx8F4jxv", "forum": "jtrhwfgseW", "replyto": "jtrhwfgseW", "signatures": ["ICLR.cc/2026/Conference/Submission11155/Reviewer_7KJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11155/Reviewer_7KJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915831822, "cdate": 1761915831822, "tmdate": 1762922319508, "mdate": 1762922319508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a mechanistic interpretability approach to probe what VLAs have learned internally. The authors also release LIBERO-OOD as a new benchmark to test compositional generalization in VLAs. It demonstrates large (9×) improvement using a simple, interpretable mechanism\n\nHowever, there are also some concerns and limtations about the experiments. First and also most important: All experiments are in simulation (LIBERO) — no real-robot validation. The VLA model are usurally pretrained on real world data. The LIBERO itself lack of realistic rendering, making spatial undersanding and reasoning harder than real. The benchmark might not be fair. Also TLI is manua: human selects which tasks to blend; not an autonomous reasoning process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First to identify and manipulate task-specific text latents inside VLAs.\n2. Demonstrates large (9×) improvement using a simple, interpretable mechanism.\n3. Introduces LIBERO-OOD, a principled OOD suite targeting skill recomposition.\n4. Reveals potential for “encrypted prompts” and backdoor control via latent injection.\n\nOverall the idea and method are interesting and might be useful to understand VLA models better and help improve VLA models."}, "weaknesses": {"value": "1. All experiments are in simulation (LIBERO) and no real-robot validation. The VLA model are usurally pretrained on real world data. The LIBERO itself lack of realistic rendering, making spatial undersanding and reasoning harder than real. The benchmark might not be fair. \n2. TLI is manual: human selects which tasks to blend; not an autonomous reasoning process.\n3. Due to the scope of LIBERO, the paper focused mainly on pick-and-place tasks — limited coverage of complex manipulation dynamics."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HWTyyX7GsW", "forum": "jtrhwfgseW", "replyto": "jtrhwfgseW", "signatures": ["ICLR.cc/2026/Conference/Submission11155/Reviewer_hfJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11155/Reviewer_hfJt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085268409, "cdate": 1762085268409, "tmdate": 1762922319089, "mdate": 1762922319089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}