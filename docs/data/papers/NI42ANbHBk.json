{"id": "NI42ANbHBk", "number": 15993, "cdate": 1758258246934, "mdate": 1759897268368, "content": {"title": "Adam or Gauss-Newton? — A Comparative Study In Terms of Basis Alignment and SGD Noise", "abstract": "Approximate second-order optimizers are increasingly showing promise in accelerating training of deep learning models, yet their practical performance depends critically on how preconditioning is applied. Two predominant approaches to preconditioning are based on (1) Adam, which leverages statistics of the current gradient, and (2) Gauss-Newton (GN) methods, which use approximations to the Fisher information matrix (often raised to a power). This work compares these approaches through the lens of two key factors: the choice of basis in the preconditioner and the impact of gradient noise from mini-batching. To gain insights, we analyze these optimizers on quadratic objectives and logistic regression under all four quadrants.\nWe show that regardless of the basis, there exist instances where Adam outperforms both $\\text{GN}^{-1}$ and $\\text{GN}^{-1/2}$ in full-batch settings.\n  Conversely, in the stochastic regime, Adam behaves similarly to $\\text{GN}^{-1/2}$ under a Gaussian data assumption.\n  These theoretical results are supported by empirical studies on both convex and non-convex objectives.", "tldr": "We compare Adam and Gauss-Newton based diagonal optimizers for linear regression and various synthetic tasks in the identity and the Hessian basis and provide theoretical results under certain assumptions.", "keywords": ["Optimization", "Adam", "Gauss-Newton", "Second-order Optimizers"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfdae8c412c6e3975c35aeddee7fac48d26380a2.pdf", "supplementary_material": "/attachment/88e65bd5fb668c4d647b7d7fc3ad1f1ab44eb24e.zip"}, "replies": [{"content": {"summary": {"value": "This paper compares two optimization methods, Adam and Gauss Newton (GN) method, decomposes the parameter update formula in the preconditioned optimizer, and establishes an analytical framework based on the following two aspects: the choice of basis in the preconditioner, and the impact of gradient noise from mini-batching. This helps to gain a deeper understanding of optimization algorithms such as Adam and Gauss Newton methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides some observations and  conclusions, such as:    \n\t1.GN is optimal under the ideal eigenbasis. In the correct basis, $GN^{-1}$ is the optimal preconditioner, in both full-batch and the stochastic regime.   \n\t2. GN is sensitive to the basis choice, when the basis is misaligned, Adam can outperform both , $GN^{-1}$ and , $GN^{-1/2}$.  \nThis paper provides theoretical analysis and proof of the above conclusion based on two examples: linear regression and logistic regression."}, "weaknesses": {"value": "1.The examples of linear regression and logistic regression are too simplistic, and the optimization algorithm is well studied for the simple linear models. This paper contains relatively little theoretical analysis on the more commonly used MLP and attention, and is far from the recent architectures of neural network.   \n2. The analyses of this paper are based on several assumptions which is not well/further justified. E.g., It assumes the input is Gasussian, which may obtained by linear model, but it is difficult to obtain layer-wise Gaussian input in DNNs. This assumption limits the analyses extending to DNNs. Besides, this paper assume that “the gradient norms are the same for coordinate within the same block”, why is that? Can provide more illustration?  \n3.This paper does not analyze whether any optimizer can analyze from the perspectives of \"the choice of basis and the impact of grade noise\", nor does it provide proof.  \n4.The description of hyperparameter tuning (such as learning rate and regularization coefficient) in the experiment is relatively brief. Suggest providing a more detailed hyperparameter table or search scope in the appendix."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ojpUYmvoWz", "forum": "NI42ANbHBk", "replyto": "NI42ANbHBk", "signatures": ["ICLR.cc/2026/Conference/Submission15993/Reviewer_sdk6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15993/Reviewer_sdk6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927580657, "cdate": 1761927580657, "tmdate": 1762926203045, "mdate": 1762926203045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comparative analysis of Adam and Gauss-Newton (GN) optimizers by evaluating them along two key axes: (1) the choice of preconditioning basis (identity vs. the GN eigenbasis) (2) the level of gradient noise (full-batch vs. small-batch) .\n\nThe work's main findings are twofold. First, in the full-batch (low-noise) setting, Adam can surprisingly outperform GN. The authors provide a quadratic example where Adam \"auto-tunes\" to the curvature in the identity basis, while GN fails and performs like simple gradient descent. They also present a logistic regression example where Adam converges faster than $GN^{-1}$ even in the ideal eigenbasis. Second, in the stochastic (high-noise) setting, the authors demonstrate a strong connection between Adam and $GN^{-1/2}$. They theoretically show for linear regression that Adam's preconditioner becomes approximately equivalent to $GN^{-1/2}$, regardless of the basis. This suggests that Adam's empirical design is well-suited for the high-noise regime, effectively mirroring a square-root curvature preconditioner. These theoretical insights are supported by experiments on both convex and non-convex problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a a critical and relevant problem: understanding the precise relationship and connection between first-order adaptive methods (like Adam) and approximate second-order methods (like Gauss-Newton). This investigation is highly relevant, especially as the choice of the preconditioning exponent (\\$p\\$) remains an open and poorly understood question in popular modern optimizers such as Shampoo.\n\n2. The central contribution of proposing a \\$2 \\times 2\\$ analytical grid—decoupling the optimizer's behavior along the axes of Basis Choice and Gradient Noise (batch size)—is excellent . This \"disentanglement\" is a novel and powerful lens, offering a much more granular and insightful model for comparing optimizer performance than prior work."}, "weaknesses": {"value": "1. The paper's analysis does not fully deliver on its promise of \"disentanglement\". According to Algorithm 1, the authors compare Adam, which is based on a diagonal empirical Fisher preconditioner , against GN methods, which are defined using the true Gauss-Newton(GN) matrix (i.e., true Fisher). This compares two things that differ in more ways than just \"basis\" and \"noise\"—it also compares a diagonal approximation to a full-matrix AdaGrad method and an empirical statistic to a true curvature matrix. A much cleaner analysis to isolate the effect of noise would have been to compare a full-matrix empirical Fisher method (e.g., full-matrix Adagrad/Adam) against the full-matrix GN method.\n\n2. The key theoretical argument that Adam can outperform GN rests on a single, specific counter-example: a quadratic problem where GN is forced to use the identity basis . This is a extreme argument, as no practical application of GN would use the identity matrix as its preconditioning basis. A far more common and realistic \"misaligned basis\" scenario would be using the basis of the empirical Fisher Information Matrix(FIM) to approximate the true GN matrix. Therefore, the conclusion that \"GN converges slowly\"  is not sufficiently proven; it is only shown to be true in an unrealistic, extreme case.\n\n3. The empirical support for the paper's broad claims is very limited. The experiments are confined to simple 1-hidden-layer MLPs and a 1-layer Transformer. Whether these findings can be generalized to deep, complex, and high-dimensional architectures (e.g., modern LLMs, large CNNs) is uncertain, because the differences between these optimizers are most critical in these architectures."}, "questions": {"value": "1. Could you please clarify the practical and theoretical motivation for studying \\$GN^{-1/2}\\$? In Algorithm 1, \\$H^{(GN)}\\$ is defined as the true Fisher matrix (approximated with a separate batch, \\$X_H\\$). While \\$GN^{-1}\\$ is the well-understood Natural Gradient Descent(NGD), the justification for applying an inverse square root (\\$p=-1/2\\$) to the true Fisher is unclear. Why is this specific variant a meaningful object of study?\n\n2. Following on Weakness 2, could you extend your analysis of basis misalignment to a more practical scenario? Specifically, what would your theory predict about the performance of \\$GN^{-1}\\$ if it were applied in the (misaligned) eigenbasis of the empirical Fisher information matrix, rather than the identity matrix?\n\n3. There appears to be a contradiction between your theoretical claims and your empirical results. In Figure 2(c) (Identity | Small), your plot shows \\$GN^{-1}\\$ (solid blue line) converging to a lower loss than Adam (orange line). This seems to contradict the claim from that Adam's \"auto-tuning\" allows it to outperform GN in the identity basis. Could you please explain this discrepancy?\n\n4. Given the simplicity of the models used, what is your perspective on how these findings would apply to much deeper, state-of-the-art architectures? Do you have preliminary evidence or a theoretical argument to suggest that these conclusions will hold in more complex, practical settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuLoS2wJ5T", "forum": "NI42ANbHBk", "replyto": "NI42ANbHBk", "signatures": ["ICLR.cc/2026/Conference/Submission15993/Reviewer_UWDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15993/Reviewer_UWDe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940516347, "cdate": 1761940516347, "tmdate": 1762926202667, "mdate": 1762926202667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the relative benefit and shortcomings of the related methods of Adam-style preconditioning versus Gauss-Newton preconditioning. In particular, two additional design axes are considered: 1. it is observed that given an orthogonal basis other than the standard one, one can compare \"diagonal\" preconditioning with respect to differing bases, for example the basis described by the Gauss-Newton preconditioner, 2. as Adam-preconditioning takes the \"square-root\" power, both the inverse Gauss-Newton and inverse-square-root Gauss-Newton are taken into consideration. It is then demonstrated in two exemplar simple models of Linear Regression and Logistic Regression that the hierarchy of Adam, inverse Gauss-Newton, and inverse-sqrt Gauss-Newton shifts around significantly depending on setting, for example: preconditioning in {standard, GN eigenbasis}, and {full-batch, stochastic} updates. This clarifies the understanding that, though Adam (or Adagrad) preconditioning is related to Gauss-Newton preconditioning, the latter does not subsume the benefits of the former. Numerical experiments are provided to back up the theoretical claims, and some extrapolations are demonstrated in non-convex settings with shallow neural networks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this is a well-written paper with clearly delineated messages and claims, and thus is worthy of acceptance. I think the overall message that there is a lot more subtlety to (Adam/Gauss-Newton) preconditioning than one might expect is well-appreciated and timely, given the many recent research efforts focused on deriving or explaining different preconditioning methods. Some theory results worth highlighting are: \n\n1. Demonstrating how Adam \"auto-tunes\" to the curvature when both Adam and Gauss-Newton are provided poor preconditioning bases in a linear regression setting. This was a bit surprising, as this shows when diagonal Gauss-Newton trivially does not outcompete gradient descent, Adam when provided the same (ill-specified) basis can achieve faster convergence rates by adapting to the gradient norm. The stark difference between Adam and Gauss-Newton in this setting is surprising at first glance.\n\n2. The local convergence vs. step-size tradeoff of Gauss-Newton in logistic regression. This result demonstrates that full Gauss-Newton, even on the correct eigenbasis, experiences tension between larger learning rates and local convergence speed, leading to overall suboptimal descent compared to even vanilla Adam. This is also surprising at first glance, considering full Gauss-Newton is a p.s.d. approximant to the Hessian."}, "weaknesses": {"value": "Thee are no glaring deficiencies of the paper that I found. Overall, the theoretical constructions are not necessarily novel, but I think they clearly serve the purpose of identifying mismatches between Adam and Gauss-Newton. More relevantly, the theory results, while providing some suggestive intuition, do not extend to neural network settings, where it is not as clear to me how approximations to Gauss-Newton experience pathologies compared to Adam, beyond numerical demonstrations. The authors might also find the following theory paper [1] interesting, where a Kronecker-Factored approximation to the Gauss-Newton matrix seems to be a demonstrably good thing to do in some square-loss two-layer network settings.\n\n[1] Zhang et al. \"On The Concurrence of Layer-wise Preconditioning Methods and Provable Feature Learning\""}, "questions": {"value": "I have no pressing questions currently. As a curiosity, I'm wondering if the authors have intuition for whether there are generic tasks with deep neural networks such that a layer-wise factored basis (e.g. standard->diagonal, or Kronecker-Factored) is preferable to the full Gauss-Newton one. I ask because empirical evidence seems to repeatedly show the \"off-diagonal blocks\" have non-negligible magnitude, which layer-wise methods throw away, which suggests the factored vs full eigenbases are appreciably different."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zf3AwNCeNl", "forum": "NI42ANbHBk", "replyto": "NI42ANbHBk", "signatures": ["ICLR.cc/2026/Conference/Submission15993/Reviewer_mAGy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15993/Reviewer_mAGy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948879898, "cdate": 1761948879898, "tmdate": 1762926202176, "mdate": 1762926202176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the comparison between the Gauss-Newton algorithm and Adam from some theoretical perspective. The paper considers some examples and analysis in linear regression and logistic regression settings to demonstrate the when GN or Adam may perform better, together with empirical simulations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an interesting topic, considering the effectiveness of recent preconditioning optimizers like Shampoo and SOAP. It will be nice to find out which way of doing preconditioning is the best, i.e., using GN or Adam, using $ p = 1/2 $ or $ 1 $.\n2. The paper starts from simple but important settings, i.e., linear regression and logistic regression, aiming to provide understanding through theoretical analysis in these relatively simple settings."}, "weaknesses": {"value": "1. My major concern is that the results are basically not strong enough. For the major contributions and claims, it seems not clear what conclusions we can draw from the paper. Could the authors provide some clear conclusions from the paper on maybe when we should use Adam and when we should use GN, or maybe when we should use $ p=1/2 $ and when we should use $ p = 1 $? To me, the theoretical part of the paper majorly provides some extreme cases as examples for that Adam may outperform GN and GN may out perform Adam, without a proper explanation and analysis for how these examples can be reflected in the real applications of Adam and GN.\n2. The experiments don't seem to be valid for me. Simulations for the examples are good, but 2-layer MLP and 1-layer transformer are definitely not enough for validation of real nonconvex settings.\n3. I suggest the authors better introduce and organize the notations in the paper, which may not be easy to follow currently. For example, the notation $ g(x) $ and even $ x $ itself lacks a proper definition, but appears very often and plays an important role."}, "questions": {"value": "1. I am curious about why comparing GN with Adam? And what can we learn from this comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GJjicvBwC3", "forum": "NI42ANbHBk", "replyto": "NI42ANbHBk", "signatures": ["ICLR.cc/2026/Conference/Submission15993/Reviewer_VcjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15993/Reviewer_VcjS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980856574, "cdate": 1761980856574, "tmdate": 1762926201749, "mdate": 1762926201749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}