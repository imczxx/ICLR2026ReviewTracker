{"id": "31N19VSZUM", "number": 11594, "cdate": 1758202250459, "mdate": 1759897565777, "content": {"title": "Mitigating Over-Refusal in Adversarial Tuning via Subspace-guided Sample Selection", "abstract": "As the adoption of large language models (LLMs) increases, their vulnerability to jailbreaks poses a significant concern. Adversarial tuning offers an effective means of enabling LLMs to resist jailbreak prompts, but it inevitably introduces the problem of over-refusal, where benign queries are mistakenly rejected, thereby comprising the model utility. To address the limitation, we propose the Soft Adversarial Tuning (SAT) framework, which selects “soft samples” that balance robustness and over-refusal for adversarial tuning. Specifically, SAT decomposes the model’s hidden states into two behavioral subspaces via representation engineering: one for producing robust responses to malicious queries and another for avoiding over-refusal on benign queries. By projecting the gradients of candidate adversarial-tuning samples onto these subspaces, we quantify each sample’s influence on jailbreak defense and over-refusal. We then select ”soft samples” that exert strong influence in the robustness subspace while having minimal effect in the over-refusal subspace for soft adversarial tuning. We evaluate SAT with six existing defense methods across different settings. Experimental results show that SAT consistently outperforms these methods, reducing the over-refusal rate by more than 22%, while maintaining an attack success rate below 2.8% against five representative jailbreak attacks.", "tldr": "", "keywords": ["LLM safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42671dd20c5c9e2ef1f9e5227ba926d53d949401.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses over-refusal in LLM adversarial tuning, which degrades utility. It proposes SAT, a subspace-guided soft sample selection framework. Key results: It reduces over-refusal by over 22%, maintains ASR below 2.8% across 5 attacks, and preserves utility (e.g., AlpacaEval winrate 54.61% vs SafeLoRA’s 47.37%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Precise subspace decomposition: It splits model activation space into robustness and over-refusal subspaces, using gradient projection to quantify sample impact (e.g., projecting candidate gradients to select samples with strong robustness and weak over-refusal influence), enabling targeted tuning. \n2. Extensive baseline comparisons: It compares 6 SOTA defenses (PPL, Self-Reminder, etc.) across 3 models (Vicuna, Llama2, Dolphin), showing SAT’s superiority (e.g., Vicuna’s SAT reduces ASR to 2.8% vs Self-Examination’s 10%).\n 3. Utility preservation: Unlike Goal Priority (which harms GSM8K accuracy), SAT maintains or improves utility (e.g., Llama2’s GSM8K accuracy 53.20% vs SafeLoRA’s 52.20%)."}, "weaknesses": {"value": "1. Limited sample generation sources: It relies on GCG to generate candidate adversarial samples; other attack methods (e.g., AutoDAN, TAP) are untested—using diverse attack-generated samples could enhance sample selection generality. \n2. Lack of long-term evaluation: It does not test over-refusal drift after extended fine-tuning (e.g., 10+ epochs); evaluating durability would confirm long-term effectiveness. \n3. Narrow domain coverage: It only tests semantic QA and math reasoning; high-stakes domains (e.g., healthcare, finance) are unexamined—adding domain-specific tests would improve real-world relevance."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kRjLbabYo6", "forum": "31N19VSZUM", "replyto": "31N19VSZUM", "signatures": ["ICLR.cc/2026/Conference/Submission11594/Reviewer_VSnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11594/Reviewer_VSnu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831971333, "cdate": 1761831971333, "tmdate": 1762922676517, "mdate": 1762922676517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a common side-effect of adversarial fine-tuning for LLM safety: over-refusal of benign queries. The authors propose Soft Adversarial Tuning (SAT), a data-selection framework that: (i) constructs two behavior subspaces from hidden activations, one for jailbreak robustness and one for over-refusal avoidance, using difference-in-means “steering” vectors; (ii) projects per-sample gradients onto these subspaces to estimate how each candidate training example would shift behavior; and (iii) scores and selects “soft” samples that push strongly along the robustness direction while minimally affecting the over-refusal direction. The selected set is then used for LoRA fine-tuning. Evaluated on Vicuna-7B, Llama-2-7B-Chat, and Dolphin-7B against five jailbreak attacks, SAT is reported to lower attack success rates and reduce refusal on safe inputs relative to several baselines, with an ablation indicating the explicit over-refusal penalty in the scoring term is important."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper centers on over-refusal on LLM, then proposes a specific mechanism (dual subspaces, gradient projections, sample scoring) rather than an open-ended recipe. The subspace construction via contrastive positive/negative pairs and difference-in-means vectors is well-defined and easy to implement. \n2.\tSAT is a pre-fine-tuning data curation step that could be bolted onto diverse PEFT schemes; the actual model update uses standard LoRA, keeping the method pragmatic for practitioners. \n3.\tOn three 7B chat models and five representative jailbreaks, SAT often shows lower ASR and lower harmful scores than baselines such as Self-Examination, ICD, Retokenization, and Paraphrase; safe-set refusal is also reduced on XSTest. The tables make these cross-method comparisons explicit."}, "weaknesses": {"value": "1.\tThe approach assumes a linear correspondence between activation-space projections at a single layer and subsequent parameter-space updates, then uses one-dimensional directions per behavior to rank samples. There is no sensitivity analysis across layers, pooling choices, or multi-dimensional subspaces, and no theoretical or empirical justification that a single direction captures over-refusal vs robustness without leakage between them. At minimum, a study varying layer index, using multi-basis PCA/LDA subspaces, and checking orthogonality would be needed. \n2.\tSince SAT is a sample selection method, it should be compared against strong selection baselines: e.g., gradient-norm filtering, loss-based filtering, influence-function/DShapley-style data valuation, or even simple intermediate-iteration only heuristics. Current baselines are mostly defense mechanisms at inference or full training, not data curation approaches, so the incremental value of subspace-guided selection is unclear. \n3.\tThe pipeline generates 500 optimized candidates per seed over 400 seeds, then projects gradients for scoring yet finally keeps only 500 samples. There is no accounting of attack-generation time, gradient-projection overhead, or overall wall-clock vs baselines. For a method pitched as “efficient pre-selection,” this omission is important."}, "questions": {"value": "1.\tTable 1 mixes ASR and harmful scores per cell but has a few oddities/typos (e.g., ICA, duplicated averages formatting) and the narrative highlights dramatic gains without confidence intervals, seed variability, or per-category breakdowns. The claim reduce over-refusal by more than 22% while keeping ASR below 2.8% would benefit from statistical tests across runs and categories. \n2.\tOne figure panel includes an error message (index out of bounds / lda (Failed)), which undermines polish and raises questions about the robustness of the dimensionality-reduction diagnostics. Implementation details crucial to reproducing subspaces are not fully specified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A1AgjK8iAB", "forum": "31N19VSZUM", "replyto": "31N19VSZUM", "signatures": ["ICLR.cc/2026/Conference/Submission11594/Reviewer_dshb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11594/Reviewer_dshb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840002823, "cdate": 1761840002823, "tmdate": 1762922675667, "mdate": 1762922675667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the \"over-refusal\" problem in LLMs. This is when models reject benign queries after adversarial tuning. The authors propose the Soft Adversarial Tuning (SAT) framework. SAT uses representation engineering to define two subspaces: a \"robustness subspace\" and an \"over-refusal subspace\". It then uses gradient projections to select \"soft samples\". These samples have a strong influence on robustness but a minimal effect on over-refusal. The main contribution is this automatic sample selection mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to understand. The figures are also clear and well-done.\n2. The paper's method is highly novel. The experimental results also show that the proposed method is effective."}, "weaknesses": {"value": "1. The quotation marks for \"soft samples\" in the abstract are not correct.\n\n2. The paper uses mean-pooled hidden states from layer $l$. Many methods use the hidden state of the last token. The authors should explain why they chose mean-pooling and discuss how this choice impacts the results.\n\n3. Please provide a detailed explanation for the rationale behind Equation (8). Specifically, why is the absolute value of $p_2$ used? The paper describes $v_2$ as the direction vector pointing from the over-refusal space to the normal response space. Does this imply that more robust samples also tend to cause over-refusal, thereby resulting in a negative $p_2$ value?\n\n4. Equation (5) is unclear. Please specify what the gradient is taken with respect to.\n\n5. The paper has small formatting problems. For example, lines 316 and 297 are missing spaces after the colons. Many citation formats are also incorrect.\n\n6. A formatting error: The caption for a table should be placed above it.\n\n7. It would be beneficial to add results on the latest Qwen models."}, "questions": {"value": "These are all in the 'Weaknesses' section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pWqCvqt1bi", "forum": "31N19VSZUM", "replyto": "31N19VSZUM", "signatures": ["ICLR.cc/2026/Conference/Submission11594/Reviewer_mb6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11594/Reviewer_mb6v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889485643, "cdate": 1761889485643, "tmdate": 1762922675226, "mdate": 1762922675226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}