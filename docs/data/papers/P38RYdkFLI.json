{"id": "P38RYdkFLI", "number": 464, "cdate": 1756741192796, "mdate": 1759898259450, "content": {"title": "REAL: Reading Out Transformer Activations for Precise Localization in Language Model Steering", "abstract": "Inference-time steering aims to alter an LLM’s responses without changing its parameters. A key challenge lies in selecting internal modules that most strongly govern the target behavior; existing approaches often rely on simplistic cues or ad hoc heuristics, leading to suboptimal or unintended effects. In this work, we introduce \\modelname{}, a novel framework for identifying behavior-relevant modules (heads or layers) in Transformers. For each module, we train a vector-quantized autoencoder (VQ-AE) on its hidden activations, partitioning the latent space into behavior-relevant and behavior-irrelevant subspaces via a shared, learnable codebook. We quantify each module’s behavioral relevance by evaluating how effectively the VQ-AE encodings distinguish between behavior-aligned and behavior-violating responses using a binary classification metric. This relevance score informs both module selection and steering strength. We evaluate \\modelname{} across eight LLMs from two model families (\\textsc{Llama} and \\textsc{Qwen}) and nine datasets spanning truthfulness enhancement, open-domain question answering under knowledge conflicts, and general alignment tasks. \\modelname{} enables more effective inference-time interventions, yielding significant improvements on these steering tasks. Notably, it achieves an average relative improvement of 20\\% (up to 81.5\\%) over the seminal ITI method~\\citep{DBLP:conf/nips/0002PVPW23} on truthfulness steering. Moreover, the modules selected by our method exhibit strong zero-shot generalization in cross-domain truthfulness-steering scenarios.", "tldr": "", "keywords": ["language modeling; representation engineering"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d46f38d6ee8f6d4a60f1f7f08d309adfde7f224f.pdf", "supplementary_material": "/attachment/640cb858b1112a21d71752b7382a8698994e849f.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of localizing modules (heads or layers) in language models responsible for a particular behavior (like truthfulness), so as to achieve more effective steering. The paper proposes a method, called REAL, that localizes model behavior in the following steps. REAL first  learns vector-quantized autoencoders (VQ-AE) to encode representations into a latent space, and uses additional contrastive loss to discriminate behavior relevant representations behavior irrelevant representations. Next, REAL trains an autoregressive model to identify important modules.  The paper evaluates on LMs from two model families across multiple datasets spanning truthfulness enhancement, open-domain QA with knowledge conflicts, and general alignment tasks. The paper compares against recent activation steering baselines including ITI and LoFiT, which shows some improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental setting is comprehensive, coveromg multiple tasks (truthfulness, knowledge seeking, contextual/parametric knowledge selection, general alignment), two model families (LLAMA and QWEN).\n\nThe method shows consistent improvements over the classic ITI baseline,\n\nThe paper provides good empirical analysis including ablation studies on hyperparameters, visualization of learned representations showing disentanglement, analysis of codebook usage patterns.\n\nThe paper is mostly well written and easy to understand."}, "weaknesses": {"value": "Some important claims in the experiments need more support. In particular, the paper claims (in Table 3) about zero-shot cross-domain generalization. The paper claims that steering heads selected from TruthfulQA transfer to MQUAKE and CLUTRR, but seems to only compare against no steering baseline. A more proper baseline is to steer using LoFiT-selected heads on the same transfer tasks.\n\nThe paper could benefit from including more latest baselines, like JOLA (Lai et al., 2025), which also studies how to localize specific modules.\n\nThe approach seems to be much more complex than existing methods, while the absolute improvements over LoFit are only 1-2% on TruthfulQA. REAL requires training a VQ-AE with contrastive loss plus an autoregressive scoring function. This probably adds substantial complexity compared to simple tuning methods (like in LoFIT), as well as introducing many hyperparameters: number of semantic units, codebook size, contrastive loss weight, temperature, and all the standard VQ-AE hyperparameters."}, "questions": {"value": "In Table 3, what is the performance when using LoFiT-selected heads for the same transfer experiments?\n\nDo authors consider adding more recent baselines like JoLA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l7SfMseyck", "forum": "P38RYdkFLI", "replyto": "P38RYdkFLI", "signatures": ["ICLR.cc/2026/Conference/Submission464/Reviewer_2qsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission464/Reviewer_2qsE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410541841, "cdate": 1761410541841, "tmdate": 1762915526005, "mdate": 1762915526005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel representation steering method, called REAL, which aims to address two limitations in existing literature: 1) use linear probing to locate intervention location (layer/head selection), and 2) use constant intervention strength. \nSpecifically, since behavior-related features are often entangled and may not be linearly separable, they train a VQ-AE to map head/layer activations to a disentangled, quantized latent space. That is, an activation is mapped to a vector of k indexes. An autoregressive scoring function is then trained to map this quantized vector to a score that measures the level of association with the target behavior. The heads/layers with large scores are selected for intervention. For each selected head/layer, the intervention strength is multiplied with the normalized score.\nExtensive experiments across different behavioral datasets demonstrate the effectiveness of the proposed method. Specifically, compared with ITI, the proposed method can select more effective heads and generate text that is more faithful."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. They did experiments across different behavioral datasets to validate the effectiveness of the proposed method on steering different kinds of behavior.\n2. The proposed method is novel in the sense that they adopt a VQ-AE to map activations to a disentangled, quantized latent space.\n3. The writing of preliminaries is very clear."}, "weaknesses": {"value": "1. The computation of the behavior-discriminative scores is depicted in Line 249 in text, which remains a bit unclear to me. It would be appreciated if the authors could formulate this computation.\n\n2. Truthfulness is one of the testbeds selected in this paper. The authors show the effectiveness of the proposed method on the TruthfulQA benchmark using the MC tasks. There is also another task, i.e., generation, on TruthfulQA, where there are metrics like Truth and Info measuring the truthfulness and informativeness of the generated text. As this is also a common task for assessing truthfulness, it would be appreciated if the authors could validate the proposed method on this task. \n\n3. The spellings of some words could be consistent for better reading experience—e.g., quantized vs. quantised.\n\n4. The main concern I have is that it seems the margins of the paper are modified (correct me if I am wrong), making it apparently different with ICLR template. The reduced margins create more space for writing, which would make it unfair for other submissions."}, "questions": {"value": "1. If the VQ-AE an architecture proposed in this paper, or it is an existing architecture? There seems to be no related work about this, even in appendices (correct me if I am wrong).\n2. The encoder in the VQ-AE maps an activation to various semantic units. Is there any way to show which specific semantics are captured?\n\n3. The proposed method significantly improves the performance compared with other representation engineering methods, which is great. But the trade-off is that the proposed method requires heavy training for VQ-AE and the autoregressive model. I wonder how many samples are used to train them in the experiments?\n\n4. The paper proposes intervention strengths in Eq. (6), rather than using constant intervention strengths. I wonder if there is any comparison for the performance of using the proposed strengths vs. using constant strengths in the REAL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fU5jUiSRyn", "forum": "P38RYdkFLI", "replyto": "P38RYdkFLI", "signatures": ["ICLR.cc/2026/Conference/Submission464/Reviewer_nyJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission464/Reviewer_nyJZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595041231, "cdate": 1761595041231, "tmdate": 1762915525881, "mdate": 1762915525881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a large language model steering approach that focuses on identifying optimal intervention locations and intervention strengths. The proposed REAL framework learns behaviorally relevant representations from attention head activations by training a VQ-AE with a latent contrastive loss to obtain discrete, disentangled encodings. These encodings are then used to train a scoring function that predicts their association with target behaviors, and the heads are ranked using metrics like AUC-ROC to measure behavioral relevance. Experiments on eight LLMs from two leading open-source families—LLaMA and Qwen—show that REAL effectively steers model behavior across various tasks, including truthfulness steering, open-domain QA involving knowledge conflicts, and general alignment objectives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is interesting—LLM steering is a promising research direction, especially since inference-time steering can modify model behavior toward desirable outcomes without altering model weights.\n2. The method is approach-agnostic and can be integrated with different steering techniques such as ITI and LoFIT.\n3. The experiments cover diverse scenarios and tasks, including truthfulness steering, open-domain QA involving knowledge conflicts, and general alignment objectives, etc.\n4. The proposed approach is novel, introducing a perspective and methodology that have not been explored in prior work.\n5. The paper is overall well-written and easy to follow."}, "weaknesses": {"value": "1. The performance improvement is relatively marginal, and several results are missing or incomplete, making it difficult to fully assess the effectiveness and potential of the proposed approach.\n2. The evaluated models are relatively small in scale, raising concerns about the approach’s applicability and scalability to larger LLMs.\n3. The paper lacks theoretical analysis to support or explain the observed empirical results. The proposed approach is also not well-motivated.\n4. As shown in Figure 6(a) and (b), even with very few semantic units (e.g., 1) and a small codebook size (e.g., 2), the results remain competitive, and increasing these parameters does not yield significant gains. This raises questions about the necessity and contribution of the VQ-AE component in the proposed framework."}, "questions": {"value": "1. Are there any studies on steering large language models with Mixture-of-Experts (MoE) architectures?\n2. I’m not sure I understand the claim: “Note that in-context learning (ICL) is not an inference-time strategy, as it requires modifying the original input to elicit the desired behavior.” Why is ICL not considered an inference-time method?\n3. How does this approach perform or apply to “thinking” models that involve multi-step reasoning or chain-of-thought generation?\n4. How are the hyperparameters determined—for example, how many heads or layers should be steered?\n5. Why are the interventions scaled by the maximum behavior-discriminative score? What benefits does this provide, and is there any ablation study supporting it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r0a0MndglO", "forum": "P38RYdkFLI", "replyto": "P38RYdkFLI", "signatures": ["ICLR.cc/2026/Conference/Submission464/Reviewer_wRmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission464/Reviewer_wRmW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892237614, "cdate": 1761892237614, "tmdate": 1762915525686, "mdate": 1762915525686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel localization method for activation steering of LLMs. The proposed method, REAL, trains an autoencoder to project activations of LLMs into a quantized latent space where the polysemantic activations can be better disentangled into relevant and irrelevant features based on the contrastive data. The trained autoencoder is then used to select the most relevant module for intervention as well as the strength of the intervention. Across multiple tasks and models, the paper demonstrates improvements of activation steering methods from prior work by using the localized module from REAL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel module localization method for activation steering of LLMs. The method can be generally applied to many activation steering methods that operate on different modules of LLMs.\n\n2. The paper conducts extensive experiments across multiple tasks and models, and demonstrates the empirical strength of the method over multiple baselines.\n\n3. The paper performs in-depth ablation analyses of the proposed method. Particularly, Figure 4 and Figure 5 show that the proposed method is able to disentangle polysemantic activations into features that are separated based on the desired behaviors."}, "weaknesses": {"value": "1. The proposed localization method does not show a strong empirical gain over other finetuning-based localization methods. The main empirical strength of the method is demonstrated in comparison with methods that are based on simple linear probes (e.g., ITI for truthfulness and spare for NQSwap and Macnoise), but the proposed method only leads to 1-2% absolute performance increase over other localization methods that also require fine-tuning (lofit for TruthfulQA). This makes sense because linear-probing-based methods are naturally less expressive with fewer learnable parameters. These results undermine the argument that the proposed VAE-style learning objectives to disentangle activations are necessarily better than other learning objectives in the prior work.\n\n2. There is a missing ablation experiment on the effectiveness of the head selection part of REAL and the intervention strength part. It is unclear to me which one of these two factors of REAL contributes most to the empirical improvements."}, "questions": {"value": "1. Table 3: Why is the CLUTRR result for Llama3.1-8B missing?\n\n2. Line 312: Why ITI, and potentially other linear-probing-based localization methods, fail with grouped-query attention? And which design of REAL happens to address it?\n\n3. Lines 313 - 319: The dataset is called MQUAKE, not QUAKE.\n\n4. Section 4.2: SPARE was not originally used to steer the attention heads, as the SAEs are not trained on the head activations. Thus, training REAL on head activations and then selecting layers for SPARE based upon that might lead to potential distribution shifts. \n\n5. Another general question is: Since REAL already learns the disentangled clusters of representations for the desired behavior during training, is it possible to extend the method beyond module selection so that it can jointly learn the steering vector as well? In this way, the method can be standalone without being combined with other steering methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z6SA80LKoR", "forum": "P38RYdkFLI", "replyto": "P38RYdkFLI", "signatures": ["ICLR.cc/2026/Conference/Submission464/Reviewer_3mok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission464/Reviewer_3mok"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142959763, "cdate": 1762142959763, "tmdate": 1762915525570, "mdate": 1762915525570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}