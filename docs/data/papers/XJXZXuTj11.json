{"id": "XJXZXuTj11", "number": 11845, "cdate": 1758204219330, "mdate": 1763567187746, "content": {"title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective.  In this paper, we present *QVGen*, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (*e.g.*, $4$-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\\Phi$, we propose a *rank-decay* strategy that progressively eliminates $\\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\\mathbf{\\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out additional inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3\\text{B}\\sim14\\text{B}$, show that QVGen is *the first* to reach full-precision comparable quality under $4$-bit settings. Moreover, it significantly outperforms existing methods. For instance, our $3$-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. *Code and videos are available in the supplementary material.*", "tldr": "", "keywords": ["quantization-aware training", "video diffusion models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/807d16ce6be6350082c093b0e4a444af98a1149d.pdf", "supplementary_material": "/attachment/69f98c70f0ddf5a9d5d003a04d902bd52f22a0be.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces QVGen, a novel Quantization-Aware Training (QAT) framework designed to enable high-quality video generation under extremely low-bit (3/4-bit) quantization, a task where previous methods fail. The key innovation is a two-stage process: first, it stabilizes training and improves convergence by adding lightweight auxiliary modules to mitigate quantization error, which is theoretically and empirically shown to reduce the gradient norm; second, it progressively eliminates these modules during training via a \"rank-decay\" strategy that uses Singular Value Decomposition (SVD) and a rank-based regularization to identify and shrink low-impact components to zero, resulting in a final quantized model with no inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-Motivated Contribution: This paper addresses the challenge of ultra-low-bit QAT for large-scale video diffusion models, filling a significant gap in the literature which has primarily focused on image models.\n\n2. Strong Theoretical and Empirical Foundation: The paper provides a solid theoretical analysis linking gradient norm reduction to improved QAT convergence, and then designs a method (the auxiliary module Φ) specifically to achieve this. The empirical results consistently show lower gradient norms and training loss.\n\n3. Impressive and Extensive Experiments: The evaluation is comprehensive, testing on four state-of-the-art models ranging from 1.3B to 14B parameters. The results are compelling, showing that QVGen is the first method to achieve full-precision comparable quality with 4-bit quantization and significantly outperforms all baselines in 3-bit settings.\n\n4. Practical and Efficient Solution: The proposed \"rank-decay\" strategy is a clever way to gain the training benefits of the auxiliary modules without incurring any inference cost, making the final model directly deployable with standard low-bit kernels. The reported ~4x memory reduction and up to 1.7x speedup are substantial."}, "weaknesses": {"value": "1. Computational Cost of QAT: While the final model is efficient, the QAT process itself is expensive, involving iterative SVD operations and training on up to 32 H100 GPUs for large models. The paper does not deeply discuss the trade-offs between this training cost and the resulting inference savings.\n\n2. Limited Analysis of \"Rank-Decay\": The strategy is shown to work, but the analysis of why the singular values of W_Φ evolve to have an increasing number of small components is somewhat surface-level. A deeper investigation into the dynamics between the quantized model and the auxiliary module during training would be valuable.\n\n3. Ablation on Simpler Alternatives: The paper compares against other fine-grained decay strategies (Sparse, Residual Quantization), but it would be strengthened by also ablating against a simple scheduled decay (e.g., linearly reducing the magnitude of all parameters in Φ to zero) to more clearly isolate the benefit of the SVD-based, rank-aware approach."}, "questions": {"value": "Hyperparameter Sensitivity: How sensitive is the final performance to the key hyperparameters, such as the initial rank r=32 and the shrinking ratio λ=1/2? Was there a systematic process for selecting these values across different model architectures and sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1AiazpvTLo", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Reviewer_uekc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Reviewer_uekc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794883251, "cdate": 1761794883251, "tmdate": 1762922863160, "mdate": 1762922863160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces QVGen, a quantization-aware training (QAT) method designed to enable high-quality video diffusion models under extremely low-bit quantization (4-bit or 3-bit). The paper begins with a theoretical analysis showing that reducing the gradient norm is key to stabilizing and improving convergence in training. Based on this insight, the paper introduces auxiliary modules $\\Phi$ during QAT to compensate for weight quantization errors and smooth optimization. To avoid extra inference overhead from these modules, the paper proposes a rank-decay mechanism, progressively eliminating $\\Phi$ by decomposing its weights via singular value decomposition (SVD) and decaying low-contributing components using a rank-based regularization schedule. Extensive experiments on state-of-the-art models demonstrate that QVGen achieves full-precision comparable performance in 4-bit settings and sets new records under 3-bit quantization, outperforming existing QAT and PTQ baselines such as Q-DM, EfficientDM, and SVDQuant. The method also improves memory efficiency and inference speed while maintaining compatibility with standard low-bit kernels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated methodology**: The proposed approach is grounded in a clear motivation. The authors observe that the auxiliary module $\\Phi$ exhibits rank decay during quantization-aware training and effectively leverage this property through their rank-decay mechanism. This insight is both intuitive and novel.\n2. **Comprehensive experimental validation**: The paper provides extensive experimental results across multiple large-scale video diffusion models, supported by detailed ablation studies and qualitative visual comparisons.\n3. **Strong empirical performance**: QVGen achieves consistently superior results compared to existing QAT and PTQ baselines, showing that the proposed framework is highly effective in maintaining video generation quality under ultra-low-bit quantization while improving efficiency."}, "weaknesses": {"value": "1. **Theory disconnected from the main method**: The theoretical analysis in the early part of the paper appears largely disconnected from the core contributions. Although the authors claim that the auxiliary module $\\Phi$ is motivated by this theory, the linkage is tenuous, and the theoretical result itself is rather weak. As a result, the theory does not substantially contribute to the understanding or justification of the proposed framework (although I think it is totally fine to be motivated empirically).\n\n2. **Limited acceleration gains**: The acceleration gains reported by QVGen are not particularly large, which somewhat limits its practical impact on efficiency. However, the authors explicitly acknowledge this limitation and attribute it to the absence of kernel fusion optimizations, which is reasonable."}, "questions": {"value": "1. The proposed QVGen framework focuses on compensating quantization errors in the weights through the auxiliary module $\\Phi$. However, the paper does not analyze the effect of activation quantization separately. Could the authors provide results or discussion on how the model performs when only activations are quantized to low-bit precision while keeping the weights in full precision? This would help clarify whether the main difficulty in quantizing video diffusion models arises more from weights or activations.\n\n2. The paper mentions that the current acceleration is limited because kernel fusion is not applied. To better understand the efficiency aspect, could the authors provide additional profiling results—such as the achieved TFLOPs of the INT4 GEMM kernel used—and an end-to-end inference time breakdown? It would be very helpful if the authors could use a profiling tool (e.g., nsys profile) with `torch.cuda.nvtx.range_push` and `torch.cuda.nvtx.range_pop` tags to visualize where most time is spent and estimate how much of the overhead could be mitigated by kernel fusion, even without implementing it. The author is encouraged to provide a table that describe how much time the qkv projection, o projection, ffn up projection, ffn down projection, self attention, and all other memory bound modules take in a single dit block."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xz4RNuq1D7", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Reviewer_eVaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Reviewer_eVaC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977226047, "cdate": 1761977226047, "tmdate": 1762922862725, "mdate": 1762922862725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QVGen, a novel Quantization-Aware Training (QAT) framework designed specifically for very low-bit (≤4-bit) video quantization. The core of this method is the introduction of an auxiliary module ($\\Phi$) to stabilize the QAT process. To eliminate the inference overhead from this auxiliary module, the authors propose a \"rank-decay\" strategy, which uses Singular Value Decomposition (SVD) and rank regularization to progressively remove these modules during training. Experiments on video DMs ranging from 1.3B to 14B parameters show that QVGen achieves quality comparable to full-precision at 4-bit settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The first full QAT method for video generation models I'm aware of.\n\n2. Experiments are conducted on four SOTA open-source video DMs (CogVideoX and Wan), with parameter scales from 1.3B to 14B, providing broad coverage.\n\n3. It validates practical efficiency gains and demonstrates orthogonality with other acceleration techniques like SVG.\n\n4. The provided experimental materials are comprehensive, and the ablation studies are extensive."}, "weaknesses": {"value": "1. Since some other QAT methods are trained using only LoRA, a comparison of training time and memory (GPU VRAM) requirements against these methods should be provided for a comprehensive assessment of algorithm efficiency.\n\n2. Quantization-related initialization settings should be specified, such as the choice of quantizer (e.g., granularity, symmetric/asymmetric) and which layers, if any, are not quantized.\n\n3. In Fig.3, why the inital training loss of the proposed method is bettern than Q-DM? Did the proposed method use a better initialization strategy? Since the authors reproduce other QAT baselines, a detailed training loss curve comparison would greatly enchance the soundness of the paper."}, "questions": {"value": "Please see the weaknesses above, and:\n\n1. Can the proposed method be combined with SVDQuant? If so, could this combination be trained efficiently by updating only LoRA parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zSOsCwbYHN", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Reviewer_FHRA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Reviewer_FHRA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986991732, "cdate": 1761986991732, "tmdate": 1762922862298, "mdate": 1762922862298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a quantization method for video diffusion transformers. The key idea is to introduce additional modules containing full-precision lora parameters to mitigate aggressive training losses, whose effectiveness of reducing gradient norm and quantization error has been demonstrated by the authors. Then, the authors devise a rank shrinking strategy, which reduces the rank to 0 to avoid additional stroage of full-precision weights. Extensive experiments demonstrate the effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The work could be highly impactful for the community of quantized video generation models due to its state-of-the-art performance.\n2. The analysis of the importance of reducing the gradient norm is valid and motivating for the proposed method.\n3. Although the method first introduces full-precision parameters, the authors devise effective solutions to reduce the rank to even 0, which means eliminating the need for additional full-precision storage. From the results, such a two-stage pipeline is highly effective and superior to one-stage methods.\n4. The experiments are very extensive and sufficient to demonstrate the effectiveness of quantizing video diffusion models."}, "weaknesses": {"value": "I don't find so many weaknesses, but would like to list some minor points below:\n1. It seems that the method is not tailored for video diffusion models and has potential for other models, like image generation and image backbone. The authors are encouraged to conduct experiments on these widely adopted benchmarks.\n2. It is encouraged to include another baseline of fine-tuning the model using the same data under full precision, which is useful to reflect the effect introduced by additional data and fine-tuning."}, "questions": {"value": "* Is it possible to directly regulate gradient norm by gradient normalization, clip, or direct optimization? These methods may yield inferior results, but can they outperform the original baseline introduced in Sec. 2?\nPlease refer to the weaknesses part above for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TFCh9DsfSi", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Reviewer_RsTT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Reviewer_RsTT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998372108, "cdate": 1761998372108, "tmdate": 1762922861848, "mdate": 1762922861848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QVGen, a quantization-aware training (QAT) framework specifically designed to enable high-performance, low-bit (≤4-bit) quantization of large-scale video diffusion models (DMs) based on the diffusion transformer (DiT) architecture. QVGen incorporates auxiliary modules ($\\Phi$) to mitigate the quantization error during training, supported by a theoretical analysis linking reduced gradient norm to improved convergence. To remove the inference burden of these modules, the paper proposes a rank-decay strategy using singular value decomposition (SVD) and a rank-based regularization for progressively pruning $\\Phi$ without significant loss in quality. Extensive evaluations across multiple state-of-the-art video DMs show QVGen achieves performance on par with full-precision models at 4-bit, and significantly outperforms competing quantization methods, with both quantitative metrics (VBench) and qualitative visualizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work addresses the challenging and critical task of efficient, high-fidelity video generation under ultra-low-bit quantization, an area with clear importance for practical deployment.\n\nProvides a regret-based convergence analysis (see Theorem 3.1, Page 4) linking gradient norm to QAT performance, justifying the introduction of $\\Phi$.\n\nThe auxiliary module ($\\Phi$) is elegantly conceived and is integrated with a flexible, theoretically justified rank-decay scheme, allowing benefits during training while incurring no inference cost.\n\nExperiments cover a wide range of SOTA video DMs (from 1.3B to 14B parameters) with comprehensive ablations (Tables 3, 4, 5; Figs. 3–6). Quantitative results (Tab. 1, 2, H, K; Figs. 5, 6) convincingly show QVGen’s superiority over PTQ and QAT baselines in essentially all relevant metrics."}, "weaknesses": {"value": "While the use of singular value decomposition is effective (Fig. 4, Section 3.2), the alternative strategies (Sparse, Residual Quantization) examined in Table 6 are somewhat strawman/naive and do not fully explore more sophisticated structured pruning or adaptive fading that could yield competitive trade-offs. There is little discussion of possible pathological cases where the SVD approach might fail, particularly if singular spectrum decays slowly.\n\nThe key result (Theorem 3.1, Page 4) relies on convexity of $f_t$, which is non-standard for deep networks, and the analysis lacks formal proof linking reduced regret to generalization in nonconvex settings. No concrete evidence connects gradient norm changes to video-specific generative performance beyond empirical plots.\n\nWhile the main approach is clearly stated, critical aspects of $\\Phi$ require deeper explanation, such as its initialization scheme across different models, architecture, and potential sensitivity to scale or data distribution shifts. Section J.2 offers two initialization methods, but does not thoroughly evaluate edge cases or sensitivity to poor initialization. There is also no discussion of stability if $\\Phi$'s rank is diminished too rapidly relative to the learning rate schedule."}, "questions": {"value": "While the use of singular value decomposition is effective (Fig. 4, Section 3.2), the alternative strategies (Sparse, Residual Quantization) examined in Table 6 are somewhat strawman/naive and do not fully explore more sophisticated structured pruning or adaptive fading that could yield competitive trade-offs. There is little discussion of possible pathological cases where the SVD approach might fail, particularly if singular spectrum decays slowly.\n\nThe key result (Theorem 3.1, Page 4) relies on convexity of $f_t$, which is non-standard for deep networks, and the analysis lacks formal proof linking reduced regret to generalization in nonconvex settings. No concrete evidence connects gradient norm changes to video-specific generative performance beyond empirical plots.\n\nWhile the main approach is clearly stated, critical aspects of $\\Phi$ require deeper explanation, such as its initialization scheme across different models, architecture, and potential sensitivity to scale or data distribution shifts. Section J.2 offers two initialization methods, but does not thoroughly evaluate edge cases or sensitivity to poor initialization. There is also no discussion of stability if $\\Phi$'s rank is diminished too rapidly relative to the learning rate schedule."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ynaw1d8aye", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Reviewer_U5JF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Reviewer_U5JF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762423063543, "cdate": 1762423063543, "tmdate": 1762922861281, "mdate": 1762922861281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear AC and Reviewers\n\nWe sincerely thank all reviewers for their time and effort in evaluating our paper. We are pleased that the reviewers acknowledged the following contributions:\n\n- **Strong motivation.** We provide novel and clear theoretical and empirical analyses of the gradient norm and the rank dynamics of $\\Phi$. *[Noted by Reviewers U5JF, RsTT, eVaC, and uekc]*\n- **Practical solutions.** To the best of our knowledge, we are the first to design a QAT approach for effective low-bit ($\\leq$4-bit) quantization of video diffusion models. *[Noted by Reviewers U5JF, RsTT, FHRA, eVaC, and uekc]*\n    - An auxiliary module that significantly improves convergence, as shown in Fig. 2.\n    - A rank-decay schedule that removes the inference overhead of the auxiliary module with negligible performance drop, as shown in Tab. 3.\n- **Extensive experiments.** In Tables 1–7, Tables A–U, and Figs. 5–6, we present comprehensive ablation studies and evaluations on four advanced video diffusion models and one image diffusion model, covering parameter scales from 1.3B to 14B. *[Noted by Reviewers U5JF, RsTT, FHRA, eVaC, and uekc]*\n- **Superior performance.** For video diffusion models, our method is the first to reach near full-precision quality under 4-bit quantization and substantially outperforms all baselines in 3-bit settings. *[Noted by Reviewers U5JF, RsTT, FHRA, eVaC, and uekc]*\n\nWe also thank all reviewers for their insightful and constructive suggestions, which helped further improve our paper. The major revisions are summarized below:\n\n- Added more fine-grained decay baselines. *[Suggested by Reviewers U5JF and uekc]*\n- Extended the theoretical analysis from a convex assumption to a nonconvex setting. *[Suggested by Reviewer U5JF]*\n- Added additional ablations and discussion on the initialization of $\\Phi$. *[Suggested by Reviewers U5JF and uekc]*\n- Included more image generation results, full-precision fine-tuning results, and naive gradient clipping ablation. *[Suggested by Reviewer RsTT]*\n- Added further discussion and analysis on training-inference efficiency. *[Suggested by Reviewers FHRA, eVaC, and uekc]*\n- Added detailed loss curve comparisons and results combining SVDQuant with QVGen. *[Suggested by Reviewer FHRA]*\n- Added discussion comparing activation-only and weight-only quantization. *[Suggested by Reviewer eVaC]*\n\nIn addition, we provide point-by-point responses for each reviewer and have incorporated all changes into the revised manuscript, highlighted in red.\n\nBest regards,\n\nAuthors of Paper #11845"}}, "id": "n8KnsHuhE7", "forum": "XJXZXuTj11", "replyto": "XJXZXuTj11", "signatures": ["ICLR.cc/2026/Conference/Submission11845/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11845/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission11845/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763771746829, "cdate": 1763771746829, "tmdate": 1763771746829, "mdate": 1763771746829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}