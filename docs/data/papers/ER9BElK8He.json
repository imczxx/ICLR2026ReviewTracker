{"id": "ER9BElK8He", "number": 8680, "cdate": 1758094873120, "mdate": 1763629926998, "content": {"title": "HiddenEcho: Mitigating Noise Amplification in Differentially Private LLMs with Hidden-State Correction", "abstract": "The rise of large language models (LLMs) has driven the adoption of Model-as-a-Service (MaaS). However, transmitting raw text to servers raises critical privacy concerns. Existing approaches employ deep neural networks (DNNs) or differential privacy (DP) to perturb inputs. Yet, these approaches suffer notable limitations: DNN-based methods often require task-specific pre-training, and conventional DP techniques, though privacy-preserving, suffer from noise amplification as perturbed inputs propagate through the deep transformer layer, leading to significant degradation in downstream task performance. To alleviate this, we propose HIDDENECHO, an end-to-end framework with client noise correction, where hidden states are sent from the server to the client and refined by a lightweight module using both embeddings and intermediate representations. HIDDENECHO suppresses inter-layer noise amplification without pretraining, effectively preserving task-relevant signals under DP constraints. To further reduce communication, HIDDENECHO incorporates gradient-based hidden layer selection and information bottleneck compression, reducing communication cost while preserving essential task information. Experiments across text classification and generation tasks demonstrate that HIDDENECHO achieves up to 46.89\\% performance improvement over DP baselines, over 85\\% communication reduction, and up to 72.52\\% faster training compared to existing denoising approaches, establishing a new privacy-utility trade-off for privatized LLMs. Codes are available at https://anonymous.4open.science/r/hidden-echo.", "tldr": "We introduce a server-guided client correction mechanism HiddenEcho that suppresses inter-layer noise amplification under differential privacy, achieving a superior privacy–utility trade-off for LLMs.", "keywords": ["LLM", "Privacy Preservation", "Denoise"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2162202a33bb622a6b07642c6dd37e6b6aae13a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents HiddenEcho, an end-to-end framework that introduces client noise correction to address the issue of noise amplification in LLMs under differential private input embeddings. The main motivation for this paper is the usage of LLMs as Model-asa-Service (MaaS). In these setups, DP noise is added to text embeddings for preserve user privacy. However, this noise gets progressively amplified as it passes through the transformer layers, significantly degrading model performance. HiddenEcho works by having the server send hidden states back to the client, where a lightweight denoising module uses both the original embeddings and intermediate representations to correct the noise. The framework includes gradient-based hidden layer selection and dimensionality reduction of hidden embeddings to reduce communication overhead. Experiments on selected classification and generation tasks show that HiddenEcho achieves up to 46.89% performance over DP baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper address an import problem of user (input) privacy when using LLMs as a Service. The paper presents an end-to-end framework to which addresses the noise amplification through the model when DP noise is added to the token embeddings before leaving the device. The proposed method learns a denoising module that takes compressed clean embeddings and a few selected & compressed hidden states from the base LLM as inputs and outputs the final predictions.\n2. The paper presents experimental results on classification and generation tasks showing the benefits of the proposed method.\n3. The paper also studies Embedding Inversion Attack (EIA) and Attribute Inference Attack (AIA) to measure the empirical privacy of the proposed method.\n4. Paper presents an ablation study to show the benefit of each design choice ie the residual connections, hidden layer filtering and dimensionality reduction with a linear layer.\n5. The paper also analysis communication overhead introduced by the proposed method."}, "weaknesses": {"value": "1. The framework mentions training the model on the client but it is unclear if the model is trained in a federated fashion (federated split learning in particular) across a set of clients or if it is trained locally on each client. \n2. The proposed method introduces additional communication cost between the service and client."}, "questions": {"value": "1. What is the memory overhead on the client side? Usually, the client devices can't support models larger than 1-2 GB (this is after considering the high end devices) and usually embedding layers of an large scale LLM is significantly large. \n2. In Model-asa-Service (MaaS) setups, we will not know the clients data distribution apriori. In such scenarios, how can we get the client model adapt to new inputs? how would we precompute layer contributions? Or do the authors think their model is robust to shift in data distribution?\n3. \"However, when compared to SnD, which also includes a denoising module, HiddenEcho-Full demonstrates faster training speeds.\" -- SnD trains the denoising module on server where one could utilize latest GPUs and larger clusters to train these models. However, HiddenEcho aims to train the denoising model on-device where resources will be the bottleneck. In my opinion, inference time comparison is more reasonable here. How does HiddenEcho compare with other baselines in terms of inference latency and inference compute FLOPs?\n4. The eval loss shown in Figure 4a doesn't seem to be converging but AUC improved. Can authors provide more insights on this?\n5. Can you add two more baselines to Table 2: 1) the performance on a small on-device model which only uses \"Dimensionally Reduced Clean Embeds\" for the given task, vs 2) the performance of the server-side model without any denoising? This will basically help us understand the contribution of noisy server-side embeddings vs reduced clean on-device embeddings to the correctness of predicted outcome."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kyNykoje0q", "forum": "ER9BElK8He", "replyto": "ER9BElK8He", "signatures": ["ICLR.cc/2026/Conference/Submission8680/Reviewer_WawF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8680/Reviewer_WawF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760926674303, "cdate": 1760926674303, "tmdate": 1762920494656, "mdate": 1762920494656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the issue of privately sharing text data in a split learning framework where a client and a server own different parts of an LLM. The client own the first embedding layer and the final fine-tuning layers. The server owns the main attention layer blocks. Directly sharing embeddings can lead to privacy leakage through embedding inversion and attribute inference attacks, thus a solution is to share noisy embeddings. The noise however propagates through the network leading to low utility. \n\nThis paper tackles the issue of noise amplification by introducing a denoising strategy, where the server sends the paramters of the hidden layers to the client and the client denoises those layers with access to the original text. The client then uses the denoised hidden layers together with the last few layers to complete one training step. To mitigate the communication cost of the hidden layer paramters, the authors propose using dimensionality reduction on the layers and selecting only the most relevant layers. \n\nWith this approach, the authors achieve between 10-40% increase in AUC compared to other differential privacy approaches for classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Authors show substanative improvement over prior methods. \n\n- Authors provide comprehensive experimental results."}, "weaknesses": {"value": "- I belive the approach has limited applications due to the high communication costs of (1) split learning, where client and server communicate for each forward and backward pass and (2) the additional cost introduced by this paper of sharing the hidden state parameters. \n\n- Prior work and baselines are not sufficiently well described: please explain DP-GAN and SnD in more detail. LDP, while simple, also needs some explanation.\n\n- Please add explanation for the d_X-DP notation where it is first used, or better to replace with \"metric-DP\"."}, "questions": {"value": "The client sends gradients from the denoised hidden layers to the server. The denoising of the hidden layers uses the original embeddings as input. Is it correct that there could be some privacy leakage through the backprops from the denoised hidden layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gSTgg8saOY", "forum": "ER9BElK8He", "replyto": "ER9BElK8He", "signatures": ["ICLR.cc/2026/Conference/Submission8680/Reviewer_p3hP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8680/Reviewer_p3hP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791625242, "cdate": 1761791625242, "tmdate": 1762920494293, "mdate": 1762920494293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiddenEcho, a split-learning framework designed to mitigate noise amplification in large language models (LLMs) under differential privacy (DP) constraints. The method employs a lightweight client-side denoising module that refines selectively transmitted hidden states from the server by leveraging both clean embeddings and intermediate representations. To minimize communication overhead, HiddenEcho utilizes an integral-gradient-based hidden layer selection mechanism and an information bottleneck–based dimension reducer to retain task-relevant information. Experimental results on text classification and generation tasks across multiple LLM architectures demonstrate that HiddenEcho achieves substantial performance gains over existing DP baselines while significantly reducing communication cost."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper explicitly characterize and address the intermediate layer noise amplification problem in DP-based LLMs. The framework is comprehensive, including the gradient-based layer selection, information bottleneck compression and denoising module. Experimental results consistently suppress baseline methods over diverse datasets and backbone models. This paper also provide theoretical proofs and analyses which strength the technical credibility."}, "weaknesses": {"value": "1. The experiments are conducted on 1B or 1.5B parameter models, while scalability to lager model is not shown, which may limit the scalability of this work. \n2. While three ablations are presented, there is no evaluation isolating the impact of the information bottleneck parameter β.\n3. Selection uses a Riemann-sum gradient proxy (Eq. 11) but the paper doesn’t show that high-contribution layers are the ones that improve downstream utility, nor how sensitive selection is to the step count m.\n4. Experiments assume a white-box attacker on embeddings/embedding-matrix, but not on returned hidden states or repeated releases across epochs."}, "questions": {"value": "1. What is the impact of varying the information bottleneck coefficient β on privacy–utility trade-offs? As well as the layer-selection hyperparameter k.\n2. Does sending intermediate hidden states to the client alter the formal DP guarantee? Is the overall protocol still (ε,δ)-DP or only empirically private?\n3. Because in this method, the filter is precomputed on a subset before fine-tuning. I’m wondering does distribution shift make selections stale? Did you try dynamic re-selection during training?\n4. Are selected layers typically contiguous or scattered? Any patterns shown over different depth or dataset?\n5. Since HiddenEcho transmits and corrects only a subset of hidden layers, gradient flow during training becomes sparse across the full transformer. How does this partial backpropagation affect optimization stability and representation alignment between the selected and unselected layers? Have you observed any degradation or convergence issues compared to full-layer backpropagation, and how does the method mitigate this potential imbalance?\n6. What are the formal guarantees and empirical attack outcomes when an eavesdropper can observe both E’ (embedding with noise) and the returned hidden states (server→client) across training/inference, with proper DP composition and channel-security assumptions made explicit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QiQlXvvC6q", "forum": "ER9BElK8He", "replyto": "ER9BElK8He", "signatures": ["ICLR.cc/2026/Conference/Submission8680/Reviewer_v3aX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8680/Reviewer_v3aX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942090404, "cdate": 1761942090404, "tmdate": 1762920493877, "mdate": 1762920493877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method, HiddenEcho, which splits an LLM where the client has the embedding layer and the rest of the model is on the server side. The main insight is that adding privacy at the token embedding level amplifies errors at each layer of the transformer, so denoising intermediate hidden states can help reduce the noisiness of the outputs. \n\nSpecifically in HiddenEcho the client sends noised client embeddings to the server, where the server calculates hidden layers and send the most important ones back to the client. The client then uses the original noise-free token embeddings plus the (noisy) intermediate hidden states and inputs them into a denoiser which then uses a combination of the clean embeddings and the noisy intermediate hidden states to produce an output hidden state for downstream task usage. \n\nThe contributions as claimed by the paper are as follows:\n1. Analyzes noise amplification in LLMs coming from token embedding perturbation\n2. Introduces HiddenEcho (as discussed above)\n3. Shows large performance gains and efficiency improvements"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper makes progress on a difficult and important problem: how do we ensure DP in the MaaS setting?\n- The method is fairly novel and interesting. In particular I find the formulation of adding hidden states together in equation 2 pretty new and thought it was interesting to learn that it works\n- The authors consider a range of text classification baselines and they seem fairly comprehensive\n- Thorough ablations give insight into specific design choices."}, "weaknesses": {"value": "- One concern I have is in the generality of the method. Specifically, I think this method would not support the next-token-prediction task well. From what i understand, you would need to do this 'Echo' for each new token you generate, which makes this quite expensive considering clients typically have slow upload speeds. Not being able to support next-token-prediction is pretty big drawback I think, as it limits us to encoder-based tasks like summarization and classification (which are largely getting done by autoregressive models anyways!). Furthermore each 'Echo' would cost more privacy.\n- As far as I can tell, it is not clear how small the denoising network is. It is a transformer, so understanding how small it is compared to the full LLM that is stored on the server side would be nice to understand how scalable this solution is. Most clients in this setting are small which is the motivation of this splitting approach.\n- What needs to be communicated to the server side to enable backprop on the server LLM? I think in general it would be good to be more explicit about what is being communicated, and how it compares in communication cost to other baselines (not just HE-full). One thing to compare against would be just not running any denoising.\n- Comparison and discussion with \"federated learning\" methods would be ideal. For example you could train a small model on a downstream task and have it perform inference on the client side. Is HiddenEcho better than that? Some representative modern work in FL (some of which have adapted to incorporate LLMs):\n\nKairouz, Peter, et al. \"Practical and private (deep) learning without sampling or shuffling.\" International Conference on Machine Learning. PMLR, 2021.\n\nChoquette-Choo, Christopher A., et al. \"Privacy amplification for matrix mechanisms.\" arXiv preprint arXiv:2310.15526 (2023).\n\nHou, Charlie, et al. \"Private federated learning using preference-optimized synthetic data.\" arXiv preprint arXiv:2504.16438 (2025).\n\nTan, Bowen, et al. \"Synthesizing privacy-preserving text data via finetuning without finetuning billion-scale llms.\" arXiv preprint arXiv:2503.12347 (2025).\n\nWu, Shanshan, et al. \"Prompt public large language models to synthesize data for private on-device applications.\" arXiv preprint arXiv:2404.04360 (2024)."}, "questions": {"value": "Questions listed above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rOuSpGnxEx", "forum": "ER9BElK8He", "replyto": "ER9BElK8He", "signatures": ["ICLR.cc/2026/Conference/Submission8680/Reviewer_UaRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8680/Reviewer_UaRs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958537407, "cdate": 1761958537407, "tmdate": 1762920493383, "mdate": 1762920493383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}