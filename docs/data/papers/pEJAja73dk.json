{"id": "pEJAja73dk", "number": 14424, "cdate": 1758235042765, "mdate": 1759897371169, "content": {"title": "$\\nabla$-Reasoner: LLM Reasoning via Test-Time Gradient Descent in Textual Space", "abstract": "Scaling inference-time compute for Large Language Models (LLMs) has unlocked unprecedented reasoning capabilities.\nHowever, existing inference-time scaling methods typically rely on inefficient and suboptimal discrete search algorithms or trial-and-error prompting to improve the online policy. In this paper, we propose $\\nabla$-reasoner, an iterative generation framework that integrates differentiable optimization over token logits into the decoding loop to refine the policy on the fly. Our core component, Differentiable Textual Optimization (DTO), leverages gradient signals from both the LLM’s likelihood and a reward model to refine textual representations. $\\nabla$-reasoner further incorporates rejection sampling and acceleration design to robustify and speed up decoding. Theoretically, we show that aligning an LLM with a reward function is equivalent to performing inference-time gradient descent in the sample space. Empirically, $\\nabla$-reasoner achieves over 20% accuracy improvement on a challenging mathematical reasoning benchmark, while reducing computation by approximately 40% compared to strong baselines. Overall, our work introduces a paradigm shift from zeroth-order search to first-order optimization at test time, offering a cost-effective path to amplify LLM reasoning.", "tldr": "We propose $\\nabla$-reasoner, an iterative decoding approach with policy refinement by test-time gradient descent on textual representations to improve LLM reasoning.", "keywords": ["LLM Reasoning", "Test-Time Scaling;Textual Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a1c27b7b9c0007f1f0bf67eed18596f4e60858a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces $\\nabla$-Reasoner, a test-time differentiable reasoning framework for large language models (LLMs). It proposed Differentiable Textual Optimization (DTO) that performs gradient descent on logits. The paper provides some empirical experiments and theoretical analysis."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea sounds interesting. It is interesting to perform training on logits during test time since it has some challenges. The authors also proposed methods to reduce the heavy computational overhead, showing their awareness of practical utility."}, "weaknesses": {"value": "My major concern is that **the experiment setting is considerably vague, incomplete and flawed** in many aspects.\n\n1. **Lack of efficiency analysis**. This is a strong concern I have on this paper. Although the authors proposed methods to reduce the heavy overhead, the detailed analysis and experiments are missing. In Sec 5.1, “Efficiency” is measured solely by model calls, ignoring **wall-clock latency, FLOPs, and GPU memory**, which are the core components of efficiency. Intuitively it is very likely that the overhead would be significant since getting gradients would require at least 2x GPU memory, and the latency is likely substantial since it requires performing gradient descent back of forth. In addition, it is important to show whether the efficiency is strongly dependent on the choice of policy models and reward models, and the output sequence length. Lack of such analysis clearly undermines the soundness of the paper.\n\n2. **Unfair comparison with test-time methods**. It is unfair to compare your methods with vanilla SC and BoN using both 8 rollouts. With tools like vLLM library, vanilla SC and BoN can be done flashly in one go, which is much more efficient than your proposed framework if the number of rollouts are the same. I suggest the authors align the budgets (wall-clock latency) with these methods to ensure fairness.\n\n3. **Training-based baselines are not convincing**. In practice, we often apply SFT then RL, not to perform them separately. It's unfair to compare SFT and GRPO separately since they should be incorporated as a whole framework. The paper also did not demonstrate how the budget of the training-based baselines aligns with $\\nabla$-reasoner.\n\n4. **No comparison with other test-time training / inference-time adaptation baselines**. This is a serious issue since there are already so many test-time training methods, including but not limited to [1][2]. **None of these are compared with $\\nabla$-reasoner.**\n\n5. **Limited domain coverage**. All datasets are about math reasoning. It is unclear how the method performs on other domains like coding, general reasoning, open-ended QA, helpfulness and safety.\n\n6. **Missing analysis of dependence on reward model quality**. The proposed method’s effectiveness appears highly sensitive to the quality and compatibility of the reward model (RM) used for gradient guidance. Appendix D shows that different RMs are applied for different policy LLMs. Why do you do that? Can your framework still work well when the quality of RM is not that good or the output of policy model is OOD for the RM? I suggest incorporating detailed experiments on the effect of applying different RMs across different tasks to show that whether your method is sensitive with the choice of RM or not.\n\n7. **Missing details for reproducibility**. The paper incorporates many experimental details in Appendix D, which is good. However some details are still unclear, including hardware (which is crucial for efficiency analysis) and evaluation methods (rule-based evaluation or strict string matching?). The authors also did not provide their code.\n\n\nIn addition to the experiment settings, I also have concern on the theoretical analysis. Theorem 4.1 is not sound enough since there is a **clear gap between the theorem and actual methodology**. The Wasserstein gradient-flow theorem assumes continuous time, continuous space, infinitesimal steps, and Gaussian diffusion noise. However, actual DTO uses finite discrete steps and deterministic updates. Hence, the theory is **intuitive but not rigorous**, that could act as a motivational study but not a rigorous proof of the equivalence.\n\n[1] Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback  (ICML 2025)\n\n[2] Learning to Reason from Feedback at Test-Time  (ACL 2025 main)"}, "questions": {"value": "1. What do you mean in \"it reduces costs by up to 40.2%\" in Sec 5.1's cost analysis? How did you get the figure? Which baseline are you comparing with? Is it about wall-clock latency or FLOPs or something else? This claim is not supported by any concrete experiments in the paper.\n\n2. As mentioned above, why do you use different RMs for different policy LMs? Would the performance degrade when the quality of RM is poor or the output of policy model is OOD for the RM?\n\n3. Is the overhead significant especially when RMs are large or output sequence is long?\n\n4. Can your theory be more aligned with the actual methodology? \n\n5. Could you provide some case studies, that is, concrete examples on what your framework is doing on some certain questions and models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ziO0xytuHj", "forum": "pEJAja73dk", "replyto": "pEJAja73dk", "signatures": ["ICLR.cc/2026/Conference/Submission14424/Reviewer_qoEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14424/Reviewer_qoEv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985430763, "cdate": 1761985430763, "tmdate": 1762924832466, "mdate": 1762924832466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates expanding computation during inference. Existing methods tend to saturate in long-chain reasoning due to the influence of sparse and noisy rewards. The authors propose performing gradient descent on token logits in the sample space during decoding. Theoretically, DTO supports bidirectional gradient propagation along the sequence and establishes an equivalence to reinforcement learning–based training. Empirically, it outperforms baselines across multiple benchmarks, achieving comparable or superior reasoning accuracy at a lower computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The method demonstrates outstanding performance across multiple benchmarks. \n2.The authors made practical considerations, enabling the proposed method to integrate well with existing LLM inference acceleration infrastructures.\n3.The authors derived gradients over discrete text and attempted to provide theoretical guarantees for DTO"}, "weaknesses": {"value": "1. The authors introduced a hyperparameter in the objective function but did not conduct experiments to analyze its impact. \n2.The proposed method heavily relies on the performance of the reward model, yet only one reward model was used in the experiments. We hope to see results under multiple reward models."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JIUlXg9sim", "forum": "pEJAja73dk", "replyto": "pEJAja73dk", "signatures": ["ICLR.cc/2026/Conference/Submission14424/Reviewer_wy2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14424/Reviewer_wy2L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987550077, "cdate": 1761987550077, "tmdate": 1762924831854, "mdate": 1762924831854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focues on test-time scaling to improve LLM's performance for reasoning task. Commonly, we can use a reward model to choose the best sampling from a pool of candidates (BoN). Such a method can be considered as zeroth-order search. In this paper, the authors propose a new iterative generation framework, ∇-Reasoner, that applies differentiable optimization over token logits into the decoding loop to redfine the policy with the signal from reward model. The Differentiable Textual Optimization (DTO) can be considered as a first-order optimization, used at test time. It utilizes the gradient signals from the LLM's likelyhood and reward model to refine the textual representations. \n\nThe proposed method is theoretically justified, and empirically shows significantly better accuracy and efficiency than various strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, with a clear motivation. \n2. The proposed method, DTO, is interesting and novel, offering a new way based on gradient optimization to test-time scaling. And DTO is supported with theoretical justification.\n3. Extensive experiments show a significant accuracy and efficiency improvement."}, "weaknesses": {"value": "1. Lack of reward models. In Appendix D, I notice you apply different reward models for differetn policy models. May I ask why? Could you offer an ablation study for the choice of different reward models for the same policy model.\n2. The improvement statement in the abstract is overstated.  The 20% accuracy improvement and 40% less computation are for different baselines, incuring some confusion."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hrBonXjMB0", "forum": "pEJAja73dk", "replyto": "pEJAja73dk", "signatures": ["ICLR.cc/2026/Conference/Submission14424/Reviewer_gxvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14424/Reviewer_gxvJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000085317, "cdate": 1762000085317, "tmdate": 1762924831336, "mdate": 1762924831336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ∇-Reasoner, an inference-time reasoning framework that applies gradient-based optimization to refine LLM outputs during decoding. The core innovation is Differentiable Textual Optimization (DTO), which optimizes token logits via gradient descent using signals from both the LLM's likelihood and a reward model. The approach combines an objective that balances reward maximization with log-likelihood regularization (Eq. 2), employs the straight-through estimator for differentiability, and integrates rejection sampling to accept only improved tokens. To enhance efficiency, the authors introduce gradient caching, rollout trajectory reusing, and confidence/gradient-guided token selection. Theoretically, they establish that sampling from an RL-optimized LLM is equivalent to drawing from the reference LLM and refining via DTO's gradient flow (Theorem 4.1). Evaluated on mathematical reasoning benchmarks (MATH-500, AMC, AIME), ∇-Reasoner achieves over 20% accuracy improvement while reducing computation by approximately 40% compared to Best-of-N and Self-Consistency baselines, demonstrating superior test-time scaling efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n- The shift from zeroth-order search to first-order gradient-based optimization for test-time reasoning is conceptually appealing and well-motivated by Figure 1\n- The theoretical connection between DTO and PPO via Wasserstein gradient flow (Theorem 4.1) provides an elegant unification of parametric and non-parametric inference perspectives\n- The gradient decomposition (δ_prefix, δ_postfix, δ_reward) offers clear intuition about how DTO enables bidirectional information flow along sequences\n\nQuality:\n- The experimental evaluation spans multiple model families (Qwen-2.5, Llama-3.1) and benchmarks with consistent improvements\n- The ablation study in Table 2 effectively demonstrates DTO's impact on rejection rates (reducing from 66% to ~30-40%)\n- The cost-accuracy trade-off analysis (Figure 3, Figure 4) convincingly shows efficiency gains over sampling-based baselines\n- The comparison includes both test-time methods (SC, BoN, ToT, RAP) and training-based methods (SFT, GRPO), providing comprehensive context\n\nClarity:\n- The motivation is clearly articulated—gradient information provides richer directional guidance than scalar rewards\n- Figure 2 and Algorithms 1-2 effectively communicate the overall framework\n- The progression from basic formulation to acceleration techniques is logical and well-structured\n- The gradient decomposition in Section 4 provides valuable interpretability\n\nSignificance:\n- Addresses the fundamental limitation of sparse reward signals in test-time scaling\n- The 40% cost reduction while maintaining or improving performance has practical value\n- The theoretical framework connecting test-time and training-time optimization could inspire future work\n- The method appears general enough to work with different reward models and base LLMs"}, "weaknesses": {"value": "Experimental Analysis:\n- While the cost comparison uses \"number of calls,\" actual wall-clock time could differ due to backward passes. Could you provide runtime measurements to complement the theoretical cost analysis?\n- The comparison with RAP and ToT might not be entirely fair if those methods weren't given comparable computational budgets. Could you ensure all baselines use similar total compute?\n- Table 1 shows ∇-Reasoner sometimes underperforms training-based GRPO (e.g., Qwen-2.5-7B on AMC: 51.5% vs 52.8%). This suggests potential limitations—could you characterize when training-time methods remain superior?\n\nImplementation Details:\n- The method requires modifying the decoding loop and maintaining gradients through generation, which could complicate integration into existing serving infrastructure. Have you explored compatibility with standard inference optimization (e.g., KV caching, speculative decoding)?\n- The acceleration techniques are crucial for practicality but add complexity. It would be valuable to quantify each technique's individual contribution to overall speedup\n- For the Llama-3.1 experiments, you note inability to evaluate on AIME due to model incapability. This suggests potential brittleness—how does the method degrade with weaker base models?\n\nComparison Fairness:\n- The SFT baseline uses only 10K examples while GRPO uses 35K—this asymmetry makes it difficult to assess whether ∇-Reasoner truly matches training-based methods or is being compared against undertrained baselines\n- For the \"comparable performance with training-based methods\" claim, some results show ∇-Reasoner trailing GRPO (e.g., AIME25 on Qwen-2.5-7B: 15.0% vs 16.7%)"}, "questions": {"value": "- Gradient quality with straight-through estimator: The straight-through trick provides biased gradients. Have you investigated whether this bias significantly impacts convergence, or explored alternative differentiable relaxations (e.g., Gumbel-softmax)?\n- Reward model dependency: How does performance degrade with reward models of varying quality? Have you experimented with different reward model architectures or training strategies? Could process rewards provide better gradients than outcome rewards?\n- Optimization landscape: Can you characterize the reward landscape's properties (smoothness, multimodality, local optima)? Does DTO sometimes get stuck in local optima, and if so, how might this be addressed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d9nlDgQJCI", "forum": "pEJAja73dk", "replyto": "pEJAja73dk", "signatures": ["ICLR.cc/2026/Conference/Submission14424/Reviewer_sgwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14424/Reviewer_sgwy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072918889, "cdate": 1762072918889, "tmdate": 1762924830931, "mdate": 1762924830931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}