{"id": "N9UkhfgF4Y", "number": 13890, "cdate": 1758224435738, "mdate": 1759897406083, "content": {"title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "abstract": "Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.", "tldr": "We propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff.", "keywords": ["Retrieval-augmented generation", "in-context learning", "generalization bound", "large language model"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40004a9996c32a5ddea8d575803937a06c5fc494.pdf", "supplementary_material": "/attachment/1fa5efa9b5878b8c569bc95a70fa2a200e7b794e.pdf"}, "replies": [{"content": {"summary": {"value": "The paper systematically investigates generalization bound for retrieval-augmented generation (RAG) in in-context linear regression and derive an exact bias-variance tradeoff. \nThe proposed framework recovers classical in-context learning (ICL) and standard RAG as limiting cases by viewing the retrieved texts as query-dependent noisy in-context examples.\nThe paper presents an interesting finding: an intrinsic ceiling on generalization error exists in RAG, as opposed to ICL.\nExperiments across two downstream datasets valiate the sample efficiency of ICL and RAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Important Problem: The paper identifies and systematically investigates an important problem.\n\n2. Clear Organization: The paper is well-structured, with clear problem formulation, and visualizations.\n\n3. Solid theoretical background: The proposed framework for RAG in in-context linear regression appears to be built upon a solid theoretical foundation.\n\n4. Comprehensive Experimental Evaluation: Extensive and detailed experimental results provide clear evidence and detailed analysis."}, "weaknesses": {"value": "1. The paper assumes that all RAG or ICL examples follow independent Gaussian distributions. However, some studies have shown that ICL or RAG use similarity-based retrievers to select demonstrations [1]. It would be useful for the authors to discuss the scenario where RAG or ICL examples are retrieved based on similarity-based retrieval methods.\n\n2. The paper only considers a single-layer linear self-attention (LSA) model. It would be useful to extend the proposed framework to nonlinear or multi-layer self-attention architectures.\n\n3. The paper validate the proposed framework only on two simple QA datasets. The authors should add more complex downstream tasks.\n\n4. The author needs to correct the page margins of the whole paper. \n\n5. Missing the analysis of different model sizes and architectures. \n\n6. Missing the analysis of different prompt length. \n\n7. Missing the details of used dataset, such as dataset sizes, prompts.\n\n[1] What makes good in-context examples for GPT-3"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RPy1OpGxUT", "forum": "N9UkhfgF4Y", "replyto": "N9UkhfgF4Y", "signatures": ["ICLR.cc/2026/Conference/Submission13890/Reviewer_rY2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13890/Reviewer_rY2P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761026969514, "cdate": 1761026969514, "tmdate": 1762924401306, "mdate": 1762924401306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper models RAG as noisy in-context learning and presents the first finite-sample generalization bounds with an exact bias‚Äìvariance decomposition for in-context linear regression, revealing an intrinsic performance ceiling. The framework recovers vanilla ICL and standard RAG as limiting cases and formalizes query-dependent retrieval via Gaussian offsets around the query. Under uniform retrieval noise, variance error decreases with more retrieved examples while bias error does not, producing diminishing returns and a plateau."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper offers a rigorous, unified view by casting RAG as noisy ICL and deriving finite-sample bounds plus an explicit bias‚Äìvariance split.\n\n* The query-dependent retrieval offset and the two noise regimes (uniform and distance-dependent) align well with practical retrieval behavior.\n\n* The theory isolates why extra retrieval beyond a point stops helping by showing variance reduction without bias reduction under uniform noise."}, "weaknesses": {"value": "* The theoretical results rely on a single-layer linear self-attention model with Gaussian inputs and isotropic noises, which may limit transfer to modern nonlinear transformer stacks.\n\n* The evaluation metric in theory is MSE while the experiments report EM on QA, and the paper does not quantify how this metric mismatch affects conclusions. \n\n\n* The Gaussian retrieval-offset assumption simplifies real retrieval distributions and ignores indexing heuristics and filtering used in practice\n\nThe experimental scope covers two QA datasets and two model families without sensitivity to retriever choice, retrieval-pool size, or fitted exponents q and \\hat_q"}, "questions": {"value": "Can the authors provide an online diagnostic that separates variance-driven gains from bias-driven plateaus to indicate when to stop adding documents. \n\nHow robust are the conclusions when the retrieval-offset distribution deviates from Gaussian due to ANN index structures or filtered negative mining."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UCDuuRNwk0", "forum": "N9UkhfgF4Y", "replyto": "N9UkhfgF4Y", "signatures": ["ICLR.cc/2026/Conference/Submission13890/Reviewer_CWz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13890/Reviewer_CWz6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802119713, "cdate": 1761802119713, "tmdate": 1762924400830, "mdate": 1762924400830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper (1) formalizes RAG as noisy in-context learning for in-context linear regression with a single-layer LSA predictor, (2) derives finite-sample generalization bounds and an exact bias‚Äìvariance decomposition under query-dependent retrieval, and (3) analyzes both uniform and non-uniform noise regimes to explain when retrieved context helps, plateaus, or harms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The most tangible contribution is a finite-sample, closed-form risk analysis that cleanly separates variance and bias effects of adding retrieved examples. Partition-depth-like roles are played by the number and distance of retrieved items, producing an optimal ùëõ* with diminishing returns O(1/m¬≤). This provides decision-relevant guidance for when to stop retrieving and how to trade off close versus far items."}, "weaknesses": {"value": "1. The theory rests on stylized assumptions, e.g., Gaussian linear data, LSA proxy, no RAG finetuning, and power-law retrieval distance distributions. These assumptions enable tractability, and the conclusions should be read as qualitative guidance.\n2. The empirical section is still preliminary in scope, and does not benchmark against strong retrieval policies or compression strategies, such as error‚Äìtime‚Äìmemory Pareto. Consequently, claims about optimal budgets and mixing ratios remain suggestive rather than decisive for deployment."}, "questions": {"value": "How sensitive are your bounds and the optimal ùëõ* to deviations from Gaussian linear assumptions, and to adding RAG fine-tuning or context compression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBQGmMcvGY", "forum": "N9UkhfgF4Y", "replyto": "N9UkhfgF4Y", "signatures": ["ICLR.cc/2026/Conference/Submission13890/Reviewer_SkT8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13890/Reviewer_SkT8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901183604, "cdate": 1761901183604, "tmdate": 1762924400242, "mdate": 1762924400242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework that views RAG as a form of noisy ICL. The authors derive the first finite-sample generalization bounds for RAG in a linear regression setting, analyzing how the number and quality of retrieved examples impact model performance. They introduce two noise models‚Äîuniform and non-uniform‚Äîto capture different retrieval scenarios (e.g., retrieving from a generic corpus vs. a labeled training set). Their theoretical findings suggest that while RAG can initially reduce variance-induced error, there's a ceiling to its effectiveness, and adding too many noisy examples can hurt performance. These theoretical insights are then backed up by experiments on common QA datasets like Natural Questions and TriviaQA, where the results align well with their proposed models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It's great to see a paper that finally puts some solid theory behind RAG, which has been a mostly empirical field until now.\n\n2. The idea of modeling RAG as noisy ICL is quite clever and provides a unified lens to understand its connection to standard in-context learning.\n\n3. The experimental results do a good job of backing up the theoretical claims, especially in showing how performance can drop when you add too many retrieved examples."}, "weaknesses": {"value": "1. The analysis is limited to a linear regression setting, which feels a bit disconnected from the complex, non-linear reality of modern language models.\n\n2. The paper doesn't touch on how RAG fine-tuning might change the dynamics, which is a pretty common way people use RAG in practice.\n\n3. While the noise models are a good start, they might be too simple to capture all the different ways retrieval can be \"noisy\" in the real world."}, "questions": {"value": "1. I'm curious if you have any thoughts on how your theoretical framework might extend to more complex, non-linear models like Transformers. Do you think the fundamental trade-offs you identified would still hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jlOrql6QrD", "forum": "N9UkhfgF4Y", "replyto": "N9UkhfgF4Y", "signatures": ["ICLR.cc/2026/Conference/Submission13890/Reviewer_t2Gz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13890/Reviewer_t2Gz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987524465, "cdate": 1761987524465, "tmdate": 1762924399693, "mdate": 1762924399693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}