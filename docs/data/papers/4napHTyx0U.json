{"id": "4napHTyx0U", "number": 2331, "cdate": 1757059062106, "mdate": 1759898155373, "content": {"title": "EDIF: Editing via Dynamic Interactive Tuning with Feedback", "abstract": "Although text-guided image editing (TIE) has advanced rapidly, most prior works remain object-centric and rely on attention maps or masks to localize and modify specific objects. In this paper, we propose a method of Editing via Dynamic Interactive Tuning (EDIF) that adaptively trades off source-image structure and instruction fidelity in difficult scene-centric editing settings. Unlike object editing, scene-centric editing is challenging because the target cannot be clearly localized, and edits need to preserve global structure. To cope with the limitation of TIE systems that typically use a unified conditioning signal and ignore the block-wise variation in the internal behavior of the model, we show that inside the model, the source-image condition and the text-prompt embedding act with layer-dependent directions and strengths. We also demonstrate both empirically and the oretically that the editing state can be diagnosed using the source image signal-to-noise ratio and VLM logits, which indicate whether the edited image faithfully reflects the intended editing prompt. By constructing a Pareto line between these two objectives, EDIF adaptively modulates the source-image and editing-text conditions, guiding each denoising step to stay close to this line for balanced optimization. Extensive experiments on ImgEdit, EmuEdit-Bench, and Places365 show that EDIF achieves state-of-the-art performance in various scene-editing scenarios, including indoor and outdoor environments.", "tldr": "A feedback-driven diffusion framework that adaptively adjusts conditioning layer by layer to balance structural preservation and semantic alignment.", "keywords": ["diffusion models", "image editing", "scene-centric editing", "feedback", "Pareto optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631bc8c52eb228b709641d54f5c32ed73492aac3.pdf", "supplementary_material": "/attachment/032830be8c85fb19c8f5fbf7f60313bb7cdfacef.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents EDIF, a method for scene-centric text-guided image editing that dynamically balances the influence of the source image and the textual editing instruction during diffusion-based generation. Unlike traditional object-centric editing, EDIF targets more complex scene-level edits where global coherence and spatial structure must be preserved. The authors observe that image and text conditions have layer-dependent effects within the diffusion model, and propose to diagnose the editing state via source-image SNR and VLM logits. EDIF constructs a Pareto trade-off curve between fidelity to the original image and adherence to the textual edit, adaptively modulating the denoising process to stay close to this optimal balance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Introduces an adaptive modulation mechanism using SNR and VLM feedback to control the editing strength during diffusion.\n\n* Provides both empirical and theoretical analysis on layer-wise condition influence and editing diagnostics.\n\n* Experimental results on multiple benchmarks indicate reasonable quantitative and qualitative improvements.\n\n* Conceptually clear in balancing fidelity vs. edit strength via a Pareto trade-off framework."}, "weaknesses": {"value": "* The idea of adaptive conditioning or feedback tuning in diffusion-based editing is conceptually similar to prior guidance or attention-control methods.\n\n* The paper’s focus on “scene-centric editing” is somewhat narrow and not well contextualized with real-world applications.\n\n* Poor presentation quality: Figures are low-resolution, making qualitative evaluation difficult. In addition, formatting inconsistencies and citation errors (e.g., line 53) deviate from the ICLR template, giving the impression of a hastily prepared submission.\n\n* Overall contribution-to-effort ratio is moderate; the framework is incremental rather than groundbreaking."}, "questions": {"value": "* Further clarification on the novelty is appreciated.\n* The paper claims that source-image and text conditions have layer-dependent effects. Can the authors provide quantitative or visual evidence (e.g., attention maps, activation statistics) to substantiate this observation?\n* The concept of “scene-centric editing” is not clearly defined. What specific criteria distinguish it from conventional object-centric editing, and how does EDIF handle transitions between the two types?\n* Since the paper targets practical scene editing, could the authors include comparisons with strong or commercial baselines (e.g., DALLE-3, Firefly, EmuEdit) to demonstrate the competitiveness and real-world applicability of EDIF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqnqLImAfc", "forum": "4napHTyx0U", "replyto": "4napHTyx0U", "signatures": ["ICLR.cc/2026/Conference/Submission2331/Reviewer_Pefd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2331/Reviewer_Pefd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760695462808, "cdate": 1760695462808, "tmdate": 1762916196176, "mdate": 1762916196176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel text-guided image editing method for scene-centric settings, called Editing via Dynamic Interactive Tuning (EDIF). Unlike existing methods, EDIF constructs a Pareto line between the source image to edited image ratio ($SNR_{src}$) and VLM logits, aiming to strike a balance between preserving the structure of the source image and accurately reflecting the intended edits. The editing process of EDIF is informed by an ablation study that analyzes the block-wise influence of conditions by zeroing out either the image condition or the text condition in specific blocks. This work provides new insights into training-free image editing, and the experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is novel and interesting, offering new insights into training-free image editing.\n\n2. The results demonstrate strong performance in scene-centric editing."}, "weaknesses": {"value": "1. Some parts of the presentation are confusing. In line 267, it states, \"we first obtain the predicted clean image.\" Does this imply that EDIF requires one complete editing iteration before adjusting the edits in subsequent iterations? However, other descriptions suggest that EDIF can be completed during denoising iterations. Could the authors clarify this point?\n\n2. The number of adjustments needed during the editing process is not discussed. What is the typical editing time for each image?\n\n3. Some content requires further clarification. The process for constructing an effective Pareto line and determining when the editing is satisfactorily completed is unclear. Additionally, it is not specified which blocks should be adjusted during the editing process.\n\n4. There are typos present, such as a citation error in line 053 and \"Pareto frint\" in line 249. Some citations also appear incorrect. For example, the citation \"Xu et al., 2023a\" in line 307 cannot be found in the reference list."}, "questions": {"value": "1. The prompt decomposition transforms free-form prompts into key concept prompts by incorporating keywords such as \"add\" or \"make.\" However, what about cases where the scene editing only requires the removal of elements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UR5Wo1jxHB", "forum": "4napHTyx0U", "replyto": "4napHTyx0U", "signatures": ["ICLR.cc/2026/Conference/Submission2331/Reviewer_Tq5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2331/Reviewer_Tq5x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746779746, "cdate": 1761746779746, "tmdate": 1762916196030, "mdate": 1762916196030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper does not follow ICLR's official template. I suggest a desk rejection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "/"}, "weaknesses": {"value": "/"}, "questions": {"value": "/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VRTj38GL5F", "forum": "4napHTyx0U", "replyto": "4napHTyx0U", "signatures": ["ICLR.cc/2026/Conference/Submission2331/Reviewer_WoaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2331/Reviewer_WoaN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892598013, "cdate": 1761892598013, "tmdate": 1762916195742, "mdate": 1762916195742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focused on scene-centric editing setting, and proposed Editing via Dynamic Interactive Tuning (EDIT) to achieve adaptive trade-off between source-image structure and instruction fidelity. This paper pointed out the block-wise variation inside the diffusion models, i.e., both the image condition and text condition functions independently. This paper used the signal-to-noise ratio and VLM logits to diagnose the editing state, and then using them to adaptively modulate the source-image and editing-text condition to achieve balanced editing results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focused on the scene-centric editing, which is more challenge compared with the object-centric editing in existing works and is an important research direction.\n2. This work proposes a reasonable method to achieve dual optimization of source preservation and prompt fidelity"}, "weaknesses": {"value": "1. The writing and organization of this paper are inadequate and require substantial revision. There’re many typos such as the “?” citation in line 53.\n2. The method is plug-and-play, but the fact that it was only tested on Kontext weakens the generalizability of the study. Can this method be used on other image editing base models besides Kontext?\n3. The quantitative experimental results of this work did not show significant improvement.\n4. User studies need to include more users and explain the specific rating criteria."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Et8U2aZyg8", "forum": "4napHTyx0U", "replyto": "4napHTyx0U", "signatures": ["ICLR.cc/2026/Conference/Submission2331/Reviewer_5cbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2331/Reviewer_5cbW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401149560, "cdate": 1762401149560, "tmdate": 1762916195558, "mdate": 1762916195558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EDIF to address the trade-off between source-image structure preservation and instruction fidelity in scene-centric text-guided image editing. The author shows that the source image condition and the embedding act with layer-dependent directions. Therefore, this paper uses source SNR and VLM logits to diagnose the editing state. Extensive experiments on three benchmarks demonstrate the effectiveness of EDIF."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work is clearly expressed and easy to understand.\n2. This work provides extensive experimental results and comprehensive comparisons with multiple baselines."}, "weaknesses": {"value": "1. The increased time consumption of EDIF compared to baseline was not mentioned.\n2. The function differences between different blocks are largely based on empirical observations, which lack theoretical support. Can these function differences be extended to other image editing methods?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CMzYPB1mFb", "forum": "4napHTyx0U", "replyto": "4napHTyx0U", "signatures": ["ICLR.cc/2026/Conference/Submission2331/Reviewer_W2As"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2331/Reviewer_W2As"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401239825, "cdate": 1762401239825, "tmdate": 1762916195376, "mdate": 1762916195376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}