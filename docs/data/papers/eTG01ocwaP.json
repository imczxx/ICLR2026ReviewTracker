{"id": "eTG01ocwaP", "number": 4591, "cdate": 1757718338996, "mdate": 1763218062476, "content": {"title": "BRIM: Block-wise Return Induction Method for Sequence Knowledge Distillation", "abstract": "Reinforcement Learning (RL)-based knowledge distillation (KD) is increasingly used to train language models for text generation. However, existing methods suffer from high variance caused by long action chains during sampling. To address this, we propose a novel block-wise return induction approach (called BRIM) that mitigates the high variance issue and stabilizes the training process. \nOur idea is to apply the Bellman Optimality Equation inversely to each $K$-step block segmented student's explored trajectories, and thus induce a total reward for all blocks from the teacher model, serving as the policy-gradient training signal.\nTheoretical analysis shows that our BRIM reduces the variance of the gradient estimates, thus leading to improved RL optimization, especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation, which suggests that our BRIM offers a promising direction for enhancing RL-based KD in LLM research.", "tldr": "We propose a novel block-wise return induction method for RL-based knowledge distillation, which mitigate the high variance issue in RL and stabilize the training process.", "keywords": ["Reinforcement Learning", "Knowledge distillation", "Text Generation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea7ea59f3ef8f021d57578ac7b1f9f03cc22c082.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets the high-variance problem in RL-based sequence KD. It proposes BRIM, which applies an inverse Bellman optimality expansion over K-step blocks along student rollouts to construct an approximate return used in policy-gradient updates. The paper positions BRIM as a REINFORCE-with-baseline variant whose baseline arises from teacher Q-values, with a variance-reduction argument. In the experiment section, it shows empirical gains across datasets including benchmarks of summarization, translation, and math reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a novel approach to reduce variance induced in teacher-student distillation, with clear intuition. \n* The method shows consistent empirical gains on T5 models compared to previous baselines across evaluation benchmarks."}, "weaknesses": {"value": "* The presentation lacks clarity and could make the algorithm hard to follow. For example, the paper does not mention how Q value function is implemented in the teacher model. Please consider adding more clarifications about this part. \n* All experiments use T5 for both teacher and student, which limits conclusions about model-family generality. Results on decoder-only families (e.g., Llama, Qwen, Mistral) and cross-family teacher-to-student settings are needed, along with a sensitivity study to weaker/miscalibrated teachers.\n* The current evaluation tasks (seq2seq tasks + GSM8K) do not include open-ended instruction following or multi-turn settings, where sequence-level RL variance is often most problematic. Evaluations on mainstream instruction-following benchmarks(e.g., MT-Bench [1], AlpacaEval 2.0 [2]) would better validate the method’s breadth.\n* The method is motivated as a low-variance alternative to REINFORCE-style sequence RL, yet there are no results for (i) REINFORCE on task reward and (ii) PPO+GAE on task reward under identical budgets. Adding REINFORCE based RL and PPO as baseline could better validate the method's effectiveness \n\n[1] Zheng, Lianmin, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin et al. \"Judging llm-as-a-judge with mt-bench and chatbot arena.\" Advances in neural information processing systems 36 (2023): 46595-46623.\n\n[2] Dubois, Yann, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. \"Length-controlled alpacaeval: A simple way to debias automatic evaluators.\" arXiv preprint arXiv:2404.04475 (2024)."}, "questions": {"value": "* In line 154-160, How's $q(s, a)$ defined for teacher model ? Do you train any separate critic or value function, please clarify for the implementation for the $Q$ value function of teacher model. \n* The current K-step return estimate might be closely related to the classical multi-step/bootstrapped estimators such as n-step return [1], TD($\\lambda$) [2], could you elaborate on the difference between your method and existing methods to reduce the variance? \n\n[1] Mnih, V., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning (A3C). ICML 2016.\n\n[2] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning 3(1): 9–44."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BL4PSMJJXX", "forum": "eTG01ocwaP", "replyto": "eTG01ocwaP", "signatures": ["ICLR.cc/2026/Conference/Submission4591/Reviewer_kvMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4591/Reviewer_kvMT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550243729, "cdate": 1761550243729, "tmdate": 1762917461067, "mdate": 1762917461067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method called BRIM (Block-wise Return Induction Method) to address the high-variance issue in Reinforcement Learning (RL)-based Knowledge Distillation (KD) for text generation, which is caused by long action sequences during sampling. The method segments the student model's generated trajectory into blocks of length K. By applying the inverse Bellman Optimality Equation to each block, it induces a block-wise cumulative reward from the teacher model, which serves as the training signal for policy gradient. Theoretical analysis demonstrates that this approach effectively reduces the variance of gradient estimates. Extensive experiments on three text generation tasks (summarization, machine translation, and arithmetic reasoning) validate its superior performance in both standard task metrics and LLM-based evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: BRIM applies the inverse Bellman equation for block-wise reward induction, effectively mitigating the high variance problem in long-sequence RL training, which is novel and well-grounded in theory.\nS2: The paper proposes the method with a theoretical proof that it reduces gradient variance (Theorem 1) and analyzes the bias-variance trade-off, which significantly enhances the credibility of the approach.\nS3: The method is systematically evaluated on three text generation tasks from different domains. The evaluation includes not only traditional metrics (e.g., ROUGE, BLEU) but also introduces an LLM-based assessment, verifying the method's generality and effectiveness.\nS4: The varianc-bias trade-off trends observed experimentally match theoretical predictions."}, "weaknesses": {"value": "W1: It seems that the main content of this work is a simplified estimation of the calculation of G_t in [1], so I hope to see more insights from the author about this work and further explanation of possible optimizations.\nW2: Theorem 1 relies on the assumption that the (state, action, reward) tuples are independent and identically distributed across timesteps. However, in autoregressive text generation, such tuples are inherently correlated since each token depends on its preceding context. The paper only briefly mentions this issue; therefore, a more thorough discussion is needed to justify when this i.i.d. assumption can be approximately valid. For example, when the student policy closely matches the teacher’s distribution or when large-batch sampling mitigates correlation effects.\nW3: The experimental results show that the optimal value of K is inconsistent across different tasks and datasets (e.g., 2, 4, 8, 16). I hope the author could provide a strategy for automatically selecting K, which could increase tuning costs in practical applications.\nW4: Estimation bias in the derivation process of Eq 5: In addition to the bias pointed in line 134, the decoding sampling strategy also has an impact, unless greedy decoding is used. Moreover, as K increases, the bias accumulates further. What kind of impact does this produce? From Fig. 1, there does not seem to be a consistent effect.\n\n\n[1] LLMR: Knowledge Distillation with a Large Language Model-Induced Reward"}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H8z0WxDEzh", "forum": "eTG01ocwaP", "replyto": "eTG01ocwaP", "signatures": ["ICLR.cc/2026/Conference/Submission4591/Reviewer_Qefc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4591/Reviewer_Qefc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656688880, "cdate": 1761656688880, "tmdate": 1762917460615, "mdate": 1762917460615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved RL approach called BRIM, which mitigates the high variance issue in knowledge distillation (KD). Specifically, the authors define the sum of rewards over consecutive steps by approximating that an optimal action is taken by a student policy. Based on this, they propose a K-step reward formulation for RL-based generation KD and update the model following the policy gradient formula. Experiments are conducted on various tasks, including XSum Summarization, Europarl EN-NL Translation, and GSM8K. The results demonstrate that BRIM with a larger K leads to a more stable training process and achieves better performance across these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-organized and clearly presented. \n* The proposed approach is simple yet effective in mitigating the variance issue in RL-based knowledge distillation, and is supported by theoretical analysis.\n* BRIM demonstrates consistent improvements over the baseline methods across different tasks presented in Table 1."}, "weaknesses": {"value": "* The evaluation metrics in Table 1 are primarily based on n-gram matching. Incorporating semantic-centric metrics, such as G-Eval, could further validate the effectiveness of the proposed approach.\n* The study lacks human evaluation. Although LLM-as-a-judge was used as a surrogate, LLM judges are prone to various biases and may not accurately reflect genuine human preferences.\n* The experiments were conducted exclusively on T5 models with fewer than 3B parameters. Extending the evaluation to other model families and a wider range of teacher-student sizes is necessary to assess the robustness and generalizability of BRIM."}, "questions": {"value": "1. It would be valuable to include an analysis of BRIM in knowledge distillation scenarios where there is a growing capability gap between the teacher and student models.\n2. It is recommended to add experiments with other backbone language models, along with an analysis of their scaling trends.\n3. Human evaluation should be incorporated to complement the automated metrics.\n4. The addition of semantic-based evaluation metrics and the 95% confidence intervals for the results in Table 1 is suggested. While the authors state that their results are statistically significant compared to each baseline, the improvements on the Europarl dataset appear quite marginal, and some values seem to be bolded incorrectly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BAQxD9S9Wr", "forum": "eTG01ocwaP", "replyto": "eTG01ocwaP", "signatures": ["ICLR.cc/2026/Conference/Submission4591/Reviewer_eUcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4591/Reviewer_eUcQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880540902, "cdate": 1761880540902, "tmdate": 1762917460002, "mdate": 1762917460002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Block-wise Return Induction Method (BRIM) for reinforcement learning (RL)-based knowledge distillation. BRIM mainly based on LLMR (Li et al. 2024). BRIM further introduces a K-step reward estimation. This paper evaluates BRIM on three benchmarks including XSum (for summarization tasks), Europarl (for translation tasks), and GSM8K (for mathematical reasoning tasks)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1. [Idea] This paper aims to advance RL-based knowledge distillation methods, and these approaches seem promising in training small language models."}, "weaknesses": {"value": "- W1. [Novelty] BRIM mainly based on LLMR (Li et al. 2024). LLMR is a knowledge distillation method based on a reward function induced from large language models. Based on LLMR, BRIM further introduces a K-step reward estimation. However, the extension seems rather limited. Furthermore, it is unclear what the advantages of K-step reward estimation are. \n\n- W2. [Performance] According to Table 1, BRIM (26.38 on GSM8K) does not seem to have a significant difference in performance from LLMR (25.39 on GSM8K).\n\n- W3. [Evaluation] This paper evaluates BRIM on three benchmarks including XSum, Europarl, and GSM8K. These are rather easy benchmarks in each task. I am not sure that BRIM works well on more complex benchmarks such as AIME2024 instead of GSM8K."}, "questions": {"value": "- Q1. What are the advantages of K-step reward estimation of BRIM, compared to LLMR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MAe9r2b5ky", "forum": "eTG01ocwaP", "replyto": "eTG01ocwaP", "signatures": ["ICLR.cc/2026/Conference/Submission4591/Reviewer_ZqJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4591/Reviewer_ZqJx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972781382, "cdate": 1761972781382, "tmdate": 1762917459654, "mdate": 1762917459654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}