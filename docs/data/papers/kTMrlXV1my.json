{"id": "kTMrlXV1my", "number": 25038, "cdate": 1758363421892, "mdate": 1763119957216, "content": {"title": "Style Decomposition and  Content Preservation for Artistic Style Transfer", "abstract": "Artistic style transfer is a crucial task that aims to transfer the artistic style of a style image to a content image, generating a new image with preserved content and a distinct style. With the advancement of image generation methods, significant progress has been made in artistic style transfer. However, the existing methods face two key challenges: i) style ambiguity, due to inadequate definition of style, making it difficult to transfer certain style attributes; ii) content nonrestraint, the lack of effective constraint information causes stylistic features of the content, such as color and texture, to seriously influence content preservation effectiveness.\nTo address this challenges, improving the quality of style transfer while ensuring effective content preservation, we propose SDCP, Style Decomposition and  Content Preservation for Artistic Style Transfer, to achieve effective style transfer through style decomposition and content preservation. First, distinguishing from previous work, we propose a style decomposing module that effectively represents style based on three basic attributes (brushstrokes, color, and texture) enabling clear style definition. Second, we design a content preserving module that employs line drawings as constraints to discard style elements while preserving content, utilizing cross-modal alignment to preserving semantic. Finally, all representations are injected into the denoising U-Net through a conditional injection mechanism. Quantitative and qualitative experiments are conducted to demonstrate that SDCP outperforms the current state-of-the-art models.", "tldr": "", "keywords": ["Style Transfer", "Decompsing", "Diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9542a3238556d8ef077a48564b583d9cfd954c77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address two key challenges in existing style transfer approaches: style ambiguity and content non-restraint. To this end, the authors propose two core modules: first, a style decomposition module that effectively represents style using three attributes—brushstrokes, color, and texture; second, a content preservation module that leverages line drawings as constraints to filter out style elements while retaining content, with cross-modal alignment employed to preserve semantics. These two representations are then injected into a denoising U-Net via a conditional injection mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Using white-box definitions to decompose style is interesting and encouraging.\n\n+ Comprehensive experiments and ablation studies."}, "weaknesses": {"value": "- The proposed method is quite complex, with numerous detailed operations. Notably, the definitions of style components are largely inherited from existing literature, yet there is a lack of clear evidence to substantiate whether these operations are optimal choices. This concern is further underscored by the ablation results in Figure 12, where removing certain representations (e.g., Line drawing, Q-former, textures) appears to exert little impact on the results.\n\n- The authors propose a LineDraw method based on CycleGAN for extracting content line drawings, but the rationale for adopting this relatively traditional approach remains unclear. What advantages does this method offer in comparison to current SOTA line drawing extraction techniques? Additionally, the motivation for using line drawings to enhance content preservation requires further elaboration. Would substituting line drawings with Canny edge maps or grayscale images yield comparable effects?"}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cWY2a1hruB", "forum": "kTMrlXV1my", "replyto": "kTMrlXV1my", "signatures": ["ICLR.cc/2026/Conference/Submission25038/Reviewer_suBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25038/Reviewer_suBr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760706589481, "cdate": 1760706589481, "tmdate": 1762943293136, "mdate": 1762943293136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "uhWZ7DEX4M", "forum": "kTMrlXV1my", "replyto": "kTMrlXV1my", "signatures": ["ICLR.cc/2026/Conference/Submission25038/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25038/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119955881, "cdate": 1763119955881, "tmdate": 1763119955881, "mdate": 1763119955881, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new style transfer framework called SDCP to improve the quality of style transfer while ensuring effective content preservation. The proposed method consists of two key components: a style decomposing module that captures the key features of style through three style properties (brushstroke, color, and texture), and a content preserving module that effectively preserves content through structural and semantic constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper aims to solve the problems of style ambiguity and content nonrestraint, which are both challenging and meaningful.\n2. This paper proposes representing style based on three basic attributes (brushstrokes, color, and texture), which could offer a new perspective for style transfer.\n3. Extensive experiments are conducted to evaluate the performance of the proposed method."}, "weaknesses": {"value": "1. Currently, there are many methods that can generate line drawings from a given image. Why not directly use existing methods? What advantages does the newly designed method, LineDraw, have compared to the current methods?\n\n2. This paper proposes extracting style based on three basic attributes: brushstrokes, color, and texture. However, the results in Figure 12 show that the impact of 'w/o brushstrokes,' 'w/o color,' and 'w/o texture' on the final outcome is not significant. Does this imply that the proposed strategy may not be very effective?\n\n3. The qualitative comparison examples with other methods provided in Figure 9 are too few (only 4 examples), and it seems that the method proposed in this paper does not exhibit significantly superior performance. It is hard to find one example that the proposed approach is significantly better.\n\n4. In the ablation experiment shown in Figure 10, the difference between using and not using content preservation is not significant.\n\n5. What is the time efficiency of different methods? Some evaluations regarding this could be conducted."}, "questions": {"value": "Please see **Weaknesses**.\n\nOthers:\n\n1. The experiments in the paper are based on Stable Diffusion 1.5, which is no longer considered state-of-the-art in the community. In addition to newer U-Net architecture models like SDXL 1.0, I am also more interested in whether the proposed method can be applied to models based on the DiT architecture, such as Stable Diffusion 3 and FLUX.1.\n\n2. The effectiveness of the loss functions proposed in Section 3.3 should be validated through ablation studies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "USs2Otwnjt", "forum": "kTMrlXV1my", "replyto": "kTMrlXV1my", "signatures": ["ICLR.cc/2026/Conference/Submission25038/Reviewer_6Kfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25038/Reviewer_6Kfx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226640238, "cdate": 1761226640238, "tmdate": 1762943292909, "mdate": 1762943292909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SDCP , a framework for artistic style transfer. The main idea is to achieve more effective and interpretable style transfer by separately modeling style and content. The authors introduce a style decomposition module that represents artistic style in terms of three fundamental attributes, namely brushstrokes, color, and texture, which enables clearer style definition and control. In addition, they design a content preservation module that uses line drawings as structural constraints to remove stylistic artifacts while maintaining semantic and spatial consistency through cross-modal alignment. Overall, the method aims to achieve a good balance between stylistic expression and faithful content preservation in artistic style transfer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、The paper is overall easy to follow."}, "weaknesses": {"value": "1、The authors appear to devote considerable effort to the design of the content preserving module for extracting a line drawing from an image, but this seems unnecessary given that there are already numerous existing methods for extracting line drawings from images.\n\n2、Although I understand that the use of line drawings is intended to inject less content information and encourage the model to generate high-quality images, it also inevitably leads to the loss of fine content details.\n\n3、For U-Net-based architectures, the design appears somewhat outdated. It is recommended that the authors explore more recent approaches based on DiT (Diffusion Transformer) structures.\n\n4、As the paper emphasizes style decomposition and content preservation, it would be more convincing if the evaluation focused on metrics that directly reflect these two aspects, such as style consistency and content preservation, and demonstrated clear improvements over existing methods.\n\n5、There have already been a number of studies exploring style transfer based on DiT architectures, which seems to be clearly lacking in this paper, such as Omnistyle."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bantbps7tV", "forum": "kTMrlXV1my", "replyto": "kTMrlXV1my", "signatures": ["ICLR.cc/2026/Conference/Submission25038/Reviewer_RyEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25038/Reviewer_RyEq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393240880, "cdate": 1761393240880, "tmdate": 1762943292659, "mdate": 1762943292659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SDCP (Style Decomposition and Content Preservation), a diffusion-based framework for artistic style transfer. The authors argue that prior diffusion-based style transfer methods suffer from two main issues:\n(1) Style ambiguity — unclear definition of “style” leading to incomplete transfer, and\n(2) Content non-restraint — lack of constraints that preserve structural and semantic content."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of separating style into brushstrokes, color, and texture is conceptually interesting and aligns with artistic analysis theory.\n\nThe paper is well-organized and provides many illustrations and examples (e.g., brushstroke distribution across artists, qualitative comparisons, and ablation visualizations)."}, "weaknesses": {"value": "Only 40 style images and 20 content images are used — far too small to support quantitative claims.\nThe improvements on FID/ArtFID are modest and within possible noise margins.\nUser study design is unclear (only 30 participants, limited task description).\n\nThe proposed framework still relies on Stable Diffusion v1.5, which has become outdated compared to modern DiT-based architectures (e.g., Flux, UViT, or PixArt). Since DiT variants offer better efficiency and spatial fidelity, it is unclear why the authors did not adopt or compare against a transformer-based diffusion backbone.\n\nThe system contains too many hand-designed modules (style decomposition, line drawing, Q-former alignment, conditional injection, etc.), making it extremely difficult to isolate which component truly contributes to the improvement. The design feels more like a pipeline of heuristics rather than a coherent, principled model.\n\nThe assumption that all styles can be decomposed into brushstrokes, color, and texture is restrictive. Many modern or abstract art styles (e.g., minimalism, photography-based stylization) do not contain clear brushstrokes or texture cues, raising doubts about the generality of this representation.\n\nThe paper only compares against older diffusion methods such as StyleID and StyleSSP, but omits newer and stronger baselines like DiffStyler, Paint-by-Example, or other training-free diffusion editing techniques. Without these comparisons, it’s difficult to judge real progress.\n\nThe motivation for standalone style transfer is becoming weaker with the rise of general-purpose image editing models (e.g., Qwen-Image-Edit), which can also take an image prompt as a “style input.” The paper does not discuss how SDCP fits into this new paradigm or whether its advantages persist when compared to these modern, prompt-based editing frameworks."}, "questions": {"value": "Have the authors verified whether the claimed improvements still hold if the same design is implemented on a DiT-based architecture such as Flux, PixArt?\n\nGiven the large number of modules (e.g., style decomposition, LineDraw, Q-former alignment, conditional injection), how can the reader assess which part contributes most to performance?\n\nHave you conducted any module-level ablation or sensitivity analysis beyond simple removal experiments to isolate the actual effect of each component?\n\nThe decomposition assumes that all styles can be described by brushstroke, color, and texture. How does this approach generalize to styles that lack explicit brushstrokes or textures (e.g., minimalist, digital, or photographic styles)?\n\nWould the model fail or degrade gracefully when such attributes are not present?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vajnwdRwRY", "forum": "kTMrlXV1my", "replyto": "kTMrlXV1my", "signatures": ["ICLR.cc/2026/Conference/Submission25038/Reviewer_VHvW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25038/Reviewer_VHvW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964466450, "cdate": 1761964466450, "tmdate": 1762943292143, "mdate": 1762943292143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}