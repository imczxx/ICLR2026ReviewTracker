{"id": "shywrceKra", "number": 4167, "cdate": 1757617852010, "mdate": 1759898049352, "content": {"title": "SeBA: Semi-supervised few-shot learning via Separated-at-Birth Alignment for tabular data", "abstract": "Learning from scarce labeled data with a larger pool of unlabeled samples, known as semi-supervised few-shot learning (SS-FSL), remains critical for applications involving tabular data in domains like medicine, finance, and science.\nThe existing SS-FSL methods often rely on self-supervised learning (SSL) frameworks developed for vision or language, which assume the availability of a natural form of data augmentations. For tabular data, defining meaningful augmentations is non-trivial and can easily distort semantics, limiting the effectiveness of conventional SSL. In this work, we rethink SSL for tabular data and propose Separated-at-Birth Alignment (SeBA), a joint-embedding framework for SS-FSL that eliminates the dependence on augmentations. \nOur core idea is to separate the data into two independent, but complementary views and align the representations of one view to mirror the nearest-neighbor correspondence of the data in the second view. A type-aware separation scheme ensures robust handling of mixed categorical and numerical attributes, while a lightweight architecture with ensemble aggregation improves generalization and reduces sensitivity to misselection of model parameters. An experimental study conducted in various benchmark datasets demonstrates that SeBA often achieves state-of-the-art performance in tabular SS-FSL, opening a new avenue for SSL paradigm in the domain of tabular data.", "tldr": "A novel augmentation-free joint-embedding self-supervised pretraining algorithm for tabular data.", "keywords": ["Self-supervised learning", "tabular data", "few-shot learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/614d37e394884e1e709d4d4094d6d3cc0e7a1dd6.pdf", "supplementary_material": "/attachment/807ac2ffaa03a4b061dd6fd557529298abbb7a6f.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents SeBA, a novel augmentation-free SSL framework for tabular data. It splits each sample into two complementary views ‚Äî feature and target ‚Äî and learns to align the nearest neighbor relationships identified in the target view within the representation space of the feature view, under the assumption that tabular features are redundant and semantically correlated, allowing partial views to preserve the same underlying structure."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow.\n- The paper is well motivated, addressing the challenge of meaningful augmentation in tabular learning.\n- The paper introduces a novel augmentation-free SSL framework, which based on JEAs, for tabular data. \n- The framework is conceptually simple yet generalizable."}, "weaknesses": {"value": "- The paper lacks evaluation fairness across datasets. While SeBA adds new datasets, it remains unclear under what criteria these datasets were added and how representative they are of broader tabular domains. Several entries in Tables 1‚Äì2 remain blank, leaving unclear whether SeBA‚Äôs advantage stems from better representation learning or from dataset selection bias.\n- The paper lacks organization and comprehensive baseline coverage. The evaluation does not clearly separate supervised, semi-supervised, and self-supervised baselines, resulting in a fragmented comparison. Several widely used methods are omitted: classical semi-supervised algorithms such as Pseudo-Labeling [1], Mean Teacher [2], and ICT [3], as well as strong tabular SSL frameworks like SAINT [4]. Moreover, recent contrastive and range-limited augmentation approaches (e.g., FESTA [5]) directly address the same challenge of constructing semantically valid augmentations for tabular data. Including and organizing these baselines would provide a fairer and more interpretable evaluation of SeBA‚Äôs contribution within the broader SSL landscape.\n- The paper shows an unreliable alignment assumption on certain datasets, as the core premise‚Äîthat samples close in the target-view space are semantically similar‚Äîdoes not hold for datasets such as CMC and GES, where feature redundancy is low. This suggests that the alignment mechanism is not reliable and becomes unstable when the underlying feature correlation is weak or when the domain structure is heterogeneous.\n- The paper shows limited performance without ratio ensemble, as training with a single separation ratio ùëá leads to highly variable results across datasets. This suggests that the learned representations are mask-dependent rather than semantically invariant. The ratio ensemble compensates for this instability but requires training multiple encoders (‚âà5√ó compute), reducing scalability and reproducibility. More efficient alternatives‚Äîsuch as Dynamic Ratio Sampling or a ratio-conditioned encoder‚Äîcould replace the multi-encoder ensemble by learning to handle varying separation ratios within a single network.\n\n*Reference*\n\n[1] Dong-Hyun Lee, Pseudo-label: The simple and efficient semi-supervised learning method\nfor deep neural networks. (ICML 13)\n\n[2] Antti Tarvainen and Harri Valpola, Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (NeurIPS 17)\n\n[3] Vikas Verma et al., Interpolation consistency training for semi-supervised learning. (Neural Networks 22)\n\n[4] Gowthami Somepalli et al., Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. (TRL workshop at NeurIPS 22)\n\n[5] K Lee et al., Range-limited Augmentation for Few-shot Learning in Tabular Data with Comprehensive Benchmark. (KDD 25)"}, "questions": {"value": "[Q1] Dataset selection criteria\n\nWhat were the criteria for adding the new datasets from OpenML-CC18 (e.g., domain diversity, feature-correlation profiles, class balance)?\nAdditionally, could the authors clarify why the results (‚Äú‚Äì‚Äù entries in Tables 1‚Äì2) for these datasets were left unreported, and whether they could be provided to ensure evaluation fairness and reproducibility?\n\n[Q2] Baseline coverage\n\nWould additional comparisons with well-known semi-supervised methods (Pseudo-Labeling [1], Mean Teacher [2], ICT [3]), strong tabular SSL models such as SAINT [4], and recent contrastive or range-limited augmentation approaches like FESTA [5] provide further insight into SeBA‚Äôs strengths? Such comparisons could better demonstrate how SeBA differs from and potentially improves upon existing paradigms that also address the challenge of defining semantically valid augmentations in tabular data.\n\n[Q3] Alignment stability and assumption validity\n\nIn datasets like CMC and GES, where feature redundancy is low, the alignment assumption seems unreliable.\nHave the authors analyzed whether the instability arises from the neighbor selection process or from dataset-specific feature sparsity?\nWould a feature-group-aware masking strategy mitigate this issue?\n\n[Q4] Dependence on ratio ensemble\n\nCould the authors clarify whether the ratio ensemble is a core design choice that contributes to SeBA‚Äôs effectiveness, or merely a compensatory mechanism to mitigate instability in single-ratio training?\nHave the authors explored a single-model variant, for example by sampling different separation ratios ùëá  for each batch (Dynamic Ratio Sampling) or by explicitly conditioning the encoder on the current ratio (ratio-conditioned encoder), so that one model learns to handle multiple ratio distributions without ensembling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PaSguBRX5g", "forum": "shywrceKra", "replyto": "shywrceKra", "signatures": ["ICLR.cc/2026/Conference/Submission4167/Reviewer_LvJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4167/Reviewer_LvJK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761193991126, "cdate": 1761193991126, "tmdate": 1762917211823, "mdate": 1762917211823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Separated-at-Birth Alignment (SeBA), a semi-supervised few-shot learning (SS-FSL) framework specifically designed for tabular data. The SeBA addresses the challenge of learning representations from scarce labeled samples with access to many unlabeled ones.\n\nThe key innovation is to remove the augmentation dependency that is typical in self-supervised learning (SSL). Instead of generating positive pairs through data augmentations, SeBA divides the input features into two complementary ‚Äúviews‚Äù (a feature view and a target view) and aligns the learned representations of the feature view with the nearest-neighbor graph derived from the target view.\nThe method introduces a type-aware separation scheme to handle mixed categorical and numerical attributes, ensuring semantic consistency when creating the two views.\nSeBA uses a lightweight multi-layer perceptron (MLP) encoder and a conditioned projector, combined with an ensemble strategy across multiple separation ratios to improve generalization and avoid overfitting on small datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important gap in the literature. Few-shot and semi-supervised learning have been well explored in vision and NLP, but the tabular domain remains underdeveloped. SeBA offers a principled approach to this problem.\n\n2. The method eliminates the need for artificial augmentations. This is a substantial conceptual improvement, as data augmentations are ill-defined or even harmful in tabular data, and SeBA provides a clear alternative.\n\n3. The design is elegant and simple. By constructing ‚Äúseparated-at-birth‚Äù views and aligning representations via nearest neighbors, the framework remains lightweight yet effective."}, "weaknesses": {"value": "1. The paper‚Äôs scope is somewhat narrow. Although the title and framing emphasize ‚Äúsemi-supervised few-shot learning,‚Äù all experiments are limited to tabular data. The contribution might not generalize to other modalities such as vision or multimodal data.\n\n2. The novelty is incremental within the self-supervised learning paradigm. The core idea is constructing paired views without augmentation. This idea is conceptually related to existing joint-embedding methods (e.g., BYOL, SimCLR) with different pairing mechanisms.\n\n3. The method relies on the assumption of meaningful nearest neighbors. The nearest-neighbor graph in the target view may not always reflect true semantic relationships, especially in high-noise or high-dimensional data.\n\n4. The evaluation lacks direct comparison to transformer-based tabular models. Models like TabPFN and UniTabE are mentioned but not deeply analyzed in the few-shot regime, which could provide a more rigorous benchmark.\n\n5. In recent years, Multi-modal Large Language Models (MLLMs) have demonstrated strong capabilities, featuring large parameter sizes and excellent generalization. In contrast, the model proposed in this paper appears to have a relatively small number of parameters. I believe that the proposed method could potentially be integrated into MLLM frameworks, but the authors have not explored this direction."}, "questions": {"value": "Please address the concerns I raised in the Weaknesses section.\n\nIn addition, could the authors include qualitative examples of the datasets and model outputs in the main text (or in the supplementary material)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bkDUO11vzG", "forum": "shywrceKra", "replyto": "shywrceKra", "signatures": ["ICLR.cc/2026/Conference/Submission4167/Reviewer_Jx13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4167/Reviewer_Jx13"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442025778, "cdate": 1761442025778, "tmdate": 1762917210164, "mdate": 1762917210164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the challenge of semi-supervised few-shot learning (SS-FSL) for tabular data, where models must classify data with very limited labeled examples while leveraging a large pool of unlabeled data. This problem is particularly relevant in domains like medical diagnosis, credit risk prediction, and cognitive sciences where obtaining labeled data is expensive but unlabeled data is readily available. The research aims to develop an effective pretraining method specifically tailored for tabular data that can learn meaningful representations from unlabeled data and then be fine-tuned with just a few labeled examples.\n\nThe paper identifies critical limitations of existing self-supervised learning (SSL) methods when applied to tabular data. Traditional SSL approaches rely heavily on data augmentations to create semantically similar positive pairs for contrastive learning. While augmentations like cropping, rotation, or color jittering work naturally for images, defining meaningful augmentations for tabular data is problematic. Poorly chosen transformations such as zero masking, Gaussian noise, or sampling from marginal distributions can distort semantic meaning, generate out-of-distribution samples, or create invalid data points. For instance, decreasing a car's age while increasing its mileage would be semantically inconsistent, or assigning non-integer values to discrete features like number of car seats would be invalid. This fundamental challenge has led recent work to largely abandon SSL for tabular data in favor of alternative approaches like cluster detection or diffusion-based methods.\n\nSeBA introduces a novel approach that eliminates the need for augmentations entirely. The core idea involves separating tabular data \"at birth\" into two complementary and independent views: feature views and target views. For each minibatch, a random binary mask determines which columns belong to each view. The method then identifies nearest-neighbor relationships in the target view space and trains an encoder to align the representations of feature views according to these nearest-neighbor correspondences.\nThe authors argue this works well for several reasons. First, it avoids the problematic task of designing augmentations for tabular data. Second, the nearest-neighbor relationships provide semantically meaningful positive pairs based on actual data similarity rather than artificial transformations. Third, the method employs a conditioned projector that takes both the encoder representation and the separation mask as inputs, allowing the model to adapt to different separation schemes. Fourth, type-aware separation ensures categorical variables are handled properly without splitting their one-hot encodings. Finally, an ensemble strategy using multiple separation ratios eliminates the need for careful hyperparameter tuning.\n\nThe authors conduct extensive experiments across twelve tabular datasets in 1-shot, 5-shot, and 10-shot classification settings. They compare SeBA against multiple baseline categories including supervised methods like CatBoost and k-NN, self-supervised methods like VIME and SCARF, meta-learning approaches, and state-of-the-art SS-FSL methods STUNT and D2R2. Performance is measured through classification accuracy with multiple random seeds and support/query set selections to ensure statistical reliability.\nAdditionally, the authors provide detailed ablation studies examining the impact of data normalization, missing data imputation strategies, separation ratios, and classifier choices. They also analyze the alignment between the pretraining objective and downstream tasks by measuring the proportion of nearest neighbors that share the same class label and examining the stability of neighbor assignments across different random separations.\n\nThe authors conclude that SeBA successfully demonstrates that self-supervised learning can be effective for tabular data when properly designed. The method achieves state-of-the-art performance on tabular few-shot learning benchmarks, obtaining the best accuracy in 29 out of 36 experimental instances. The main contributions include introducing the Separated-at-Birth Alignment framework that eliminates augmentation requirements, instantiating it as a lightweight model with ensemble strategies to prevent overfitting, providing thorough empirical validation, and demonstrating consistent generalization across diverse tabular datasets. The work opens new avenues for SSL paradigms in tabular data and provides a practical foundation for data-constrained applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "SeBA addresses the fundamental incompatibility between traditional SSL and tabular data by completely reimagining how positive pairs are constructed. Rather than forcing unnatural augmentations onto tabular data, it leverages the inherent structure of the data itself through nearest-neighbor relationships. This approach is particularly well-suited because tabular data naturally contains meaningful similarity structures that can be discovered through partial feature comparisons.\n\nThe key contribution lies in demonstrating that SSL principles can be successfully adapted to tabular data without relying on augmentations. By introducing the separated-at-birth concept, the authors provide a principled alternative to augmentation that maintains the benefits of contrastive learning while respecting the unique characteristics of tabular data. The type-aware separation scheme for handling mixed categorical and numerical features represents an important technical innovation that ensures semantic validity.\n\nThe method builds on solid theoretical foundations from contrastive learning while addressing specific tabular data challenges systematically. The use of InfoNCE loss provides a well-understood optimization objective, while the conditioned projector allows the model to handle varying separation schemes coherently. The ensemble approach addresses the practical challenge of hyperparameter selection in few-shot scenarios where validation data is scarce. Each design choice, from zero imputation to type-aware separation, is motivated by specific tabular data characteristics and supported by ablation studies."}, "weaknesses": {"value": "The paper lacks theoretical justification for why nearest-neighbor relationships in partial feature spaces should consistently produce semantically meaningful positive pairs. While empirical results show high same-class neighbor rates, the conditions under which this assumption holds or might fail are not thoroughly analyzed. The relationship between separation ratio and dataset characteristics remains underexplored, leaving practitioners without clear guidance on when certain ratios might be preferred.\n\nWhile the experiments cover multiple datasets and shot settings, certain aspects lack depth. The comparison with D2R2 uses only the inductive variant rather than the full transductive version that achieves better performance. The paper does not explore performance on datasets with very high dimensionality or extreme class imbalance, both common in real-world tabular applications. Additionally, computational efficiency comparisons are absent, which is important given the ensemble strategy requires training multiple models.\n\nThe ensemble approach, while eliminating hyperparameter tuning, significantly increases computational cost during both training and inference. The method's reliance on nearest-neighbor relationships may be problematic for datasets where local similarity doesn't align well with class structure, such as data with multimodal class distributions. The fixed separation ratios used in the ensemble might not be optimal for all datasets, particularly those with very few or very many features. Finally, the approach assumes that partial views contain sufficient information for meaningful nearest-neighbor matching, which might not hold for datasets with complex feature dependencies."}, "questions": {"value": "Regarding the architectural choice, what is the theoretical advantage of using a lightweight MLP encoder over a transformer-based model for representation learning in the tabular SS-FSL setting, aside from the general benefit of avoiding overfitting on small datasets?\n\nCan the authors clarify the underlying logical reason why zero imputation for masked features performs best in the SeBA framework compared to previous methods like sampling from empirical distribution, especially considering how this choice impacts the subsequent nearest-neighbor calculation in the target view?\n\nGiven the acknowledged weakness of SeBA's performance on the CMC and GES datasets, what specific characteristics of the data in these two datasets, such as dimensionality, feature distribution, or type complexity, might be hypothesized as the cause of the reduced efficacy of the Separated-at-Birth Alignment mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YY9FcacZA", "forum": "shywrceKra", "replyto": "shywrceKra", "signatures": ["ICLR.cc/2026/Conference/Submission4167/Reviewer_nyXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4167/Reviewer_nyXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790134844, "cdate": 1761790134844, "tmdate": 1762917209783, "mdate": 1762917209783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the Tabular Semi-supervised Few-shot Learning (SS-FSL) setting, to address the fundamental difficulty that conventional Self-Supervised Learning (SSL) methods face in defining data augmentations, the authors propose a new joint-embedding architecture called Separated-at-Birth Alignment (SeBA). The method splits the data into a complementary Feature View and Target View, and achieves augmentation-free construction of positive pairs by aligning the representation of the Feature View to the k-nearest-neighbor relationships defined in the Target View. Combining a type-aware separation scheme with an ensemble strategy over various separation ratios ($T$), SeBA is reported to outperform existing state-of-the-art methods across diverse benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Augmentation-Free SSL Paradigm**: By sidestepping the long-standing challenge of designing semantically meaningful augmentations for tabular data, the authors introduce a new SSL pretext task that combines feature separation with nearest-neighbor matching. This has clear potential to steer research directions in the area.\n\n**Robustness-Oriented Design**: Instead of manually tuning the optimal feature separation ratio ($T$), the approach ensembles encoders trained with multiple $T$ values to secure generalization and reliability. This is a practical strategy under few-shot constraints.\n\n**Effective Ablations**: The analyses show that, for missing-data imputation, zero imputation outperforms marginal sampling by about 3‚Äì5 percentage points. They also confirm that linear probing is the most effective few-shot classifier, providing concrete justification for key design choices."}, "weaknesses": {"value": "(Novelty)\n\n\nThe proposed idea is theoretically very similar to existing augmentation-free, feature-separation-based‚Äìbased SSL methods (e.g., T-JEPA, Thimonier et al., 2025). In particular, generating subsets via random masking of tabular data and learning structure based on another subset overlaps with the core concept of T-JEPA.\n\n\nThe theoretical and empirical distinctions from the most closely related prior work, T-JEPA, are not clearly articulated .\n\n\n(Technical Quality)\n\n\nLack of reproducibility and stability verification: Standard deviations are missing for all methods in the key results (Tables 1 and 2). Without them, one cannot assess statistical significance‚Äîcrucial for evaluating reproducibility and performance stability in few-shot settings‚Äîrepresenting a serious deficiency in technical rigor.\n\n\nIncomplete and potentially unfair baseline comparisons: Results on datasets where important baselines were added (MAR, SAT, TEX, etc.) omit strong methods such as TabPFN, SCARF, and UMTRA, making it difficult to judge whether the proposed method generalizes broadly against up-to-date baselines (Tables 1 and 2). No clear technical rationale is provided for these omissions.\n\nQuestionable justification for random masking / NN alignment: In tabular data, random masking can discard information from critical features, and defining positive pairs via nearest neighbors assumes that local manifold similarity in high-dimensional/sparse spaces reflects global semantic similarity. This strong assumption (Section 3.1) lacks rigorous theoretical or empirical support.\n\n\nInsufficient justification for the Conditional Projector: The design of the conditional projector œÄ(h,m)\\pi(h, m)œÄ(h,m), which re-conditions the Feature-View encoding on the mask (Equation 5), appears logically unnecessary, and the paper provides limited analysis of its added value.\n\n\n(Significance)\n\n\nWhile the work aims to offer methodological insights toward addressing long-standing challenges in tabular SSL, the absence of statistical stability (missing STDs) and incomplete baseline coverage prevent an objective assessment of whether the paper meaningfully advances the field. (Insufficient evidence; reason: no statistical significance testing possible.)\n\n\n(Writing & Presentation)\n\n\nThe overall structure and methodological exposition are clear. However, the omission of standard deviation information for the key experimental results (Tables 1 and 2) substantially limits readers‚Äô ability to judge the reliability of the findings. Details necessary for reproducibility‚Äîespecially those concerning statistical stability‚Äîare insufficient."}, "questions": {"value": "Request for sensitivity analysis of NN alignment: To validate the methodology of using Nearest Neighbors (NN) as surrogates for positive pairs, please present an analysis of the final performance and the structural changes in the embedding space as the value of \nùëò varies. In particular, could you quantitatively analyze how frequently semantic mismatch occurs when the definition of NN does not align semantically?\n\nRequest for explanation of missing baseline experiments: Please explain the specific reasons why the results for certain datasets (e.g., MAR, SAT, TEX) are missing for major baselines such as TabPFN, SCARF, and UMTRA in Tables 1 and 2. If possible, please provide additional experimental results on the missing datasets using the codebases of those baselines to ensure fairness in comparison.\n\nStrengthening the distinction from T-JEPA: Please provide a detailed analysis of the theoretical and experimental differences between T-JEPA (Thimonier et al., 2025) and SeBA‚Äôs nearest-neighbor alignment‚Äìbased approach. Clearly explain whether the two approaches pursue fundamentally different learning objectives rather than being simple variations of each other."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bkCR02jNUa", "forum": "shywrceKra", "replyto": "shywrceKra", "signatures": ["ICLR.cc/2026/Conference/Submission4167/Reviewer_iSr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4167/Reviewer_iSr3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977391978, "cdate": 1761977391978, "tmdate": 1762917209335, "mdate": 1762917209335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}