{"id": "D7i6BIbCz0", "number": 22565, "cdate": 1758332915347, "mdate": 1759896859150, "content": {"title": "UniBP: Toward Universal Backdoor Purification via Fine-Tuning", "abstract": "Deep neural networks (DNNs) remain vulnerable to backdoor attacks, perpetuating an arms race between attacks and defenses. Despite their efficacy against classical threats, mainstream defenses often fail under more advanced, defense-aware attacks, particularly clean-label variants that can evade decision-boundary shifting and neuron-pruning defenses. We present UniBP, a universal post-training defense that operates with only 1\\% of the original training data and unveils the relationship between batch normalization (BN) behavior and backdoor effects. \nAt a high level, UniBP scrutinizes BN layers’ affine parameters and statistics using a small clean subset (i.e., as small as 1\\% of the training data) to find the most impactful affine parameters for reactivating the backdoor, then prunes them and applies masked fine-tuning to remove the backdoor effects. We compare our method against 5 SOTA defenses, 5 backdoor attacks, and various attack/defense conditions, and show that UNBP consistently reduces the attack success rate from more than 90\\% to less than 5\\% while preserving clean performance, whereas other baselines degrade under smaller fine-tuning sets or stronger poisoning techniques.", "tldr": "UniBP is a fine-tuning–based, data-efficient, attack-agnostic defense that uses ~1% of data to cut backdoor ASR from >90% to <5% across diverse attacks and benchmarks.", "keywords": ["backdoor attack", "backdoor defense", "adversarial ML"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/408c9c051b9a3425cf76c8bc63cab2382ab7d2d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper adjusts the parameters of batch normalization layers to mitigate backdoor poisoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Backdoors pose a challenging problem."}, "weaknesses": {"value": "The following uncited paper adjusts batch-normalization parameters\nfor backdoor defense.  So, it should have been cited and\ncompared against.\n[1] X. Li et al.  Backdoor Mitigation by Correcting the Distribution of Neural Activations. Elsevier Neurocomputing  614, 21 January 2025. http://arxiv.org/abs/2308.09850\n\nRe. line 145: Though some papers do assume a strong adversary (insider) who controls the training _process_, typically backdoor poisoning can be effectively accomplished by just inserting poisoned examples into the training dataset. \n\nRe. line 148,149: The statement is odd because a very large \nnumber of prior papers on backdoor defense, particularly\ninversion/reverse-engineering approaches,  make exactly this\n\"post training\" assumption.\n\nIs the (cited) I-BAU method compared against in Table 1?\n\nThe fonts in the figures and tables are too small."}, "questions": {"value": "See the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i4lvmeTDGd", "forum": "D7i6BIbCz0", "replyto": "D7i6BIbCz0", "signatures": ["ICLR.cc/2026/Conference/Submission22565/Reviewer_nswQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22565/Reviewer_nswQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761089089446, "cdate": 1761089089446, "tmdate": 1762942282632, "mdate": 1762942282632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniBP, a post-training defense framework to remove backdoors from deep neural networks using only a small clean subset (as little as 1% of data). The method exploits the relationship between batch normalization (BN) statistics and backdoor behavior, pruning the most backdoor-sensitive BN affine parameters and applying masked fine-tuning. Experiments across multiple architectures, datasets, and attack/defense settings show that UniBP achieves a substantial reduction in attack success rate (ASR <5%) while maintaining high clean accuracy, outperforming existing defenses such as NAD, ANP, FST, and TSBD."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The finding and utilization of BN affine parameters as the key mechanism for backdoor activation is novel. It provides new insights related to backdoors.\n- The performance comparisons are based on some newly-proposed methods, e.g., COMBAT, SBL in attacks, and FST, TSBD in defenses, making the results more trustworthy. And the method is simple and effective.\n- The methodology is clearly structured into four well-explained stages, and visualizations (e.g., t-SNE, BN statistics) effectively illustrate the underlying intuition."}, "weaknesses": {"value": "- Some typos exist, e.g., \"??\" in line 211 and \"Batch-norm affine reset\" in line 250 is not consistent with the other title (The first letter of each word is not capitalized).\n- For the results presentation, it is unfair to color only UniBP in blue for the comparable performance. The baseline performance should also be highlighted and fairly show the comparison.\n- Some scalable experiments may help better illustrate the effectiveness, e.g., performance in the ViT model or the ImageNet dataset. The CIFAR-10 and GTSRB are too small, making the generalizability of UniBP unclear."}, "questions": {"value": "- How to UniBP on transformer-based or normalization-free architectures?\n- Can the method be combined with data-free defense approaches to further relax the clean data requirement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ypdEpEDp1", "forum": "D7i6BIbCz0", "replyto": "D7i6BIbCz0", "signatures": ["ICLR.cc/2026/Conference/Submission22565/Reviewer_88MC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22565/Reviewer_88MC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661789682, "cdate": 1761661789682, "tmdate": 1762942282330, "mdate": 1762942282330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UniBP, a universal post-training defense method designed to remove backdoors from deep neural networks. Unlike existing defenses that need large clean datasets or fail under adaptive clean-label attacks, UniBP uses only 1% of the original clean data. It works by exploiting a key insight: Batch Normalization layers capture backdoor-related distributional shifts.\nUniBP identifies a small subset of BN affine parameters responsible for trigger activation, prunes them, and applies masked fine-tuning to purify the model. Experiments across multiple attacks and architectures show UniBP consistently lowers ASR from over 90% to below 5% while maintaining clean accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and clearly presents the motivation, methodology, and results.\n- UniBP is a novel approach that effectively leverages Batch Normalization layers to identify and mitigate backdoors.\n- The evalution showcases strong performance across various datasets, architectures, and attack types."}, "weaknesses": {"value": "- The observation and the technique are not new, which are already explored in prior works.\n- The reliance on BN may limit the applicability to models that do not use BN.\n- The evaluated attacks and baselines are really limited.\n- Further discussion is needed for adaptive attacks given the knowledge of UniBP."}, "questions": {"value": "This paper is well-written and easy to follow. The entire flow is sound and the experiments are well-designed. However, I have several concerns:\n\n(1) **Novelty**:\n\nThe key observation of this paper is that BN layers capture backdoor-related distributional shifts, and can be used to identify and mitigate backdoors. However, this observation has been made in prior works such as [8, 11]. The idea of pruning a set of neurons and then doing fine-tuning to remove backdoors is also similar to [9]. The authors should clarify the novelty of their findings and approach compared to these prior works. What are the key differences and contributions of UniBP that set it apart from existing methods?\n\n(2) **Limited Evaluation**:\n\nThe evaluation is limited to a small set of attacks (5) and baselines (5). It would be helpful to see results on a wider range of attacks[1,2,3,4,5], including more recent adaptive attacks that may specifically target embedding distribution[6,7]. Additionally, evaluating on recent defense baselines are important to showcase the superiority of UniBP[8,9,10].\n\n(3) **Dependence on Batch Normalization**:\nUniBP relies heavily on the presence of Batch Normalization layers to identify and mitigate backdoors. However, many modern architectures, such as Vision Transformers (ViT) and ConvNeXt, do not use BN layers. This limits the applicability of UniBP to a narrower set of models. The authors should discuss how UniBP could be adapted or extended to work with models that do not use BN, or provide empirical results on such architectures.\n\n(4) **Adaptive Attacks**:\nThe paper does not sufficiently address the potential for adaptive attacks that could specifically target the UniBP defense. If an attacker is aware of the UniBP method, they may design triggers or training strategies that evade detection by BN parameter pruning. The authors should discuss potential adaptive attack scenarios and evaluate the robustness of UniBP against such attacks.\n\n---\n**Reference**:\n\n[1] Turner, Alexander, Dimitris Tsipras, and Aleksander Madry. \"Clean-label backdoor attacks.\" Preprint 2018.\n\n[2] Salem, Ahmed, et al. \"Dynamic backdoor attacks against machine learning models.\" EuroS&P 2022.\n\n[3] Nguyen, Tuan Anh, and Anh Tran. \"Input-aware dynamic backdoor attack.\" NeurIPS 2020.\n\n[4] Liu, Yunfei, et al. \"Reflection backdoor: A natural backdoor attack on deep neural networks.\" ECCV 2020.\n\n[5] Barni, Mauro, Kassem Kallas, and Benedetta Tondi. \"A new backdoor attack in cnns by training set corruption without label poisoning.\" ICIP 2019.\n\n[6] Qi, Xiangyu, et al. \"Revisiting the assumption of latent separability for backdoor defenses.\" ICLR 2023.\n\n[7] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\" CCS 2023.\n\n[8] Cheng, Siyuan, et al. \"Unit: Backdoor mitigation via automated neural distribution tightening.\" ECCV 2024.\n\n[9] Li, Yige, et al. \"Reconstructive neuron pruning for backdoor defense.\" ICML 2023.\n\n[10] Zhu, Rui, et al. \"Selective amnesia: On efficient, high-fidelity and blind suppression of backdoor effects in trojaned machine learning models.\" IEEE S&P 2023.\n\n[11] Zheng, Runkai, et al. \"Pre-activation distributions expose backdoor neurons.\" NeurIPS 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G01V9KRcad", "forum": "D7i6BIbCz0", "replyto": "D7i6BIbCz0", "signatures": ["ICLR.cc/2026/Conference/Submission22565/Reviewer_uAhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22565/Reviewer_uAhn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942435967, "cdate": 1761942435967, "tmdate": 1762942281779, "mdate": 1762942281779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniBP, a four-stage post-training defense against conventional backdoor attacks. Built upon the observation that backdoor attacks usually change the BN layer's statistics, UniBP identifies and prunes BN affine parameters and channels with high backdoor sensitivity, followed by masked fine-tuning to restore clean performance. Extensive experiments demonstrate that UniBP achieves a larger reduction in ASR compared with existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a fine-grained method that precisely identifies the BN affine parameters most strongly correlated with backdoor behavior.\n2. Extensive experiments show that UniBP achieves consistently better backdoor defense performance than all evaluated baselines.\n3. Empirical results demonstrate the universal effectiveness of the proposed approach against multiple traditional backdoor attacks."}, "weaknesses": {"value": "1. The proposed UniBP builds upon a well-established observation that backdoor training perturbs BN layer's statistics, as also discussed in papers like [1]. The authors should explicitly discuss how the proposed method differs and surpasses those existing line of work.\n2. UniBP relies on the BN layers. As a result, it cannot be easily applied to modern generative models that do not have such design. This will limit the potential and generalizability of the proposed method.\n3. The paper lacks any discussion or empirical results of UniBP's computational overhead compared with baseline defenses.\n4. UniBP exhibits higher clean-accuracy degradation compared to several baselines. The authors should provide a more detailed analysis of this trade-off and present potential mitigation strategies.\n5. The paper lacks comparison with several SOTA baselines that also rely on parameter masking or activation tightening, such as [2-3].\n\n\n[1] Zheng, Runkai, et al. \"Pre-activation distributions expose backdoor neurons.\" Advances in Neural Information Processing Systems 35 (2022): 18667-18680.\n\n[2] Li, Yige, et al. \"Reconstructive neuron pruning for backdoor defense.\" International Conference on Machine Learning. 2023.\n\n[3] Cheng, Siyuan, et al. \"Unit: Backdoor mitigation via automated neural distribution tightening.\" European Conference on Computer Vision. 2024."}, "questions": {"value": "1. Could the attackers try to regularize the attack to not influence BN layer's parameters? If so, the proposed method may fail easily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3c56Hq1seJ", "forum": "D7i6BIbCz0", "replyto": "D7i6BIbCz0", "signatures": ["ICLR.cc/2026/Conference/Submission22565/Reviewer_Yqvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22565/Reviewer_Yqvj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973402725, "cdate": 1761973402725, "tmdate": 1762942281568, "mdate": 1762942281568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}