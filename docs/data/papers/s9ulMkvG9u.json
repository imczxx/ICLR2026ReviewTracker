{"id": "s9ulMkvG9u", "number": 9757, "cdate": 1758138756467, "mdate": 1762946738557, "content": {"title": "BEYOND SYNTAX: ACTION SEMANTICS LEARNING FOR APP AGENTS", "abstract": "The recent development of Large Language Models (LLMs) enables the rise of App agents that interpret user intent and operate smartphone Apps through actions such as clicking and scrolling. While prompt-based solutions with proprietary LLM APIs show promising ability, they incur heavy compute costs and external API dependency. Fine-tuning smaller open-source LLMs solves these limitations. However, current supervised fine-tuning methods use a syntax learning paradigm that forces agents to reproduce exactly the ground truth action strings, leading to out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action Semantics Learning (ASL), a novel learning framework, where the learning objective is capturing the semantics of the ground truth actions. Specifically, inspired by the programming language theory, we define the action semantics for App agents as the state transition induced by the action in the user interface. Building on this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a semantic similarity to train the App agents in generating actions aligned with the semantics of ground truth actions, even when their syntactic forms differ. SEE is a flexible module that can be applied in both supervised and reinforcement fine-tuning paradigms. To support the effectiveness of ASL, we theoretically demonstrate the superior robustness of ASL for the OOD problem compared with the existing syntax learning paradigm. Extensive experiments across multiple offline and online benchmarks demonstrate that ASL significantly improves the accuracy and generalisation of App agents compared to existing methods.", "tldr": "This work introduces Action Semantics Learning (ASL), a novel framework that trains App agents to capture the semantics of actions, defined as the state transition in the UI induced by the actions.", "keywords": ["App agent", "semantic learning", "World Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b3311757ed2b2b4b2f41d2c5875f2685d63b5d9e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Action Semantics Learning (ASL), a novel learning framework for training smartphone app agents that focuses on understanding the semantic effects of actions rather than their syntactic forms. The framework includes a Semantic Estimator (SEE) that uses a world model to predict UI state changes and BERT to compute semantic similarity between predicted and ground truth actions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed SEE module demonstrates elegant design in being training-free and computationally efficient, avoiding the need for additional GPU memory during deployment while still providing meaningful semantic rewards. The experimental validation is comprehensive, covering both online environments (AndroidWorld, AndroidLab) and offline benchmarks (AndroidControl), with consistent improvements across difficulty levels."}, "weaknesses": {"value": "The reliance on proprietary APIs (Gemini-Flash-2.0) for the world model component undermines the claimed advantages over prompt-based methods that depend on external APIs. This creates a fundamental contradiction: while criticizing existing approaches for API dependency, ASL itself requires API access for its core semantic estimation functionality."}, "questions": {"value": "How does the world model handle dynamic UI elements, time-dependent states, or stochastic UI behaviors that are common in real applications? The paper assumes deterministic state transitions, but many app interactions involve randomness or external factors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "piWLgf1Wgb", "forum": "s9ulMkvG9u", "replyto": "s9ulMkvG9u", "signatures": ["ICLR.cc/2026/Conference/Submission9757/Reviewer_hjH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9757/Reviewer_hjH7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761009762082, "cdate": 1761009762082, "tmdate": 1762921249958, "mdate": 1762921249958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WT3dhyZCFd", "forum": "s9ulMkvG9u", "replyto": "s9ulMkvG9u", "signatures": ["ICLR.cc/2026/Conference/Submission9757/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9757/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762946737234, "cdate": 1762946737234, "tmdate": 1762946737234, "mdate": 1762946737234, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the training of smartphone application agents. It points out that existing methods largely follow a syntax learning paradigm, where models are required to exactly reproduce the *ground-truth action strings* during supervised fine-tuning, which makes them vulnerable to out-of-distribution UI changes.\n\nTo address this, the authors propose Action Semantics Learning (ASL) — a new framework that shifts the learning objective from syntactic replication to semantic reproduction of UI state transitions.\nIts core component, the Semantic Estimator (SEE), uses an LLM-based world model (Gemini-Flash-2.0) to simulate the UI state changes caused by an action, and then computes the semantic similarity between the predicted and ground-truth transitions using a BERT encoder. The resulting score serves as a reward signal.\nASL can be applied to both supervised fine-tuning and reinforcement learning stages. The authors also provide theoretical analysis suggesting ASL’s robustness under OOD conditions and evaluate it on multiple App-Agent benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies that existing App-Agent fine-tuning paradigms overfit to syntactic action forms and generalize poorly to unseen UIs.\n2. Drawing inspiration from programming language semantics, the paper reframes action learning as learning the resulting state transitions—a novel and thought-provoking perspective.\n3. ASL can be integrated into both SFT and RL pipelines, showing potential for broader applicability."}, "weaknesses": {"value": "The overall effectiveness of ASL hinges on an unverified key assumption: that Gemini-Flash-2.0 (or similar LLMs) can accurately simulate the UI state transition for any given action. However, the paper provides no evaluation of this world model’s prediction accuracy, nor reports its failure rate or bias. If the world model predicts incorrect transitions, SEE will generate misleading semantic rewards—propagating wrong signals to the downstream agent. Such reward noise may reduce robustness rather than improve it. The authors should quantify or analyze how closely the world model’s predicted transitions align with the actual UI state changes; otherwise, the validity of ASL rests on a fragile assumption.\n\nSEE uses BERT cosine similarity between textual descriptions of predicted and ground-truth state transitions. This design is both linguistically sensitive and conceptually imprecise. LLMs may describe the same state change using very different wording, leading to low similarity despite true equivalence. Semantic equivalence is inherently a discrete yes/no relation, but the model treats it as a continuous reward. The paper applies a threshold only in the RL stage but not during SFT, where the continuous score directly modulates the gradient. This could produce non-zero rewards for incorrect actions, potentially destabilizing training. The authors should clarify the normalization and thresholding mechanisms of the reward, otherwise ASL might not be learning true semantic equivalence in practice.\n\nThe main experiments compare Qwen-2.5-7B-Instruct-ASL only with its SFT counterpart. This resembles an internal ablation rather than a comparison with state-of-the-art methods. Although Appendix G reports some results against AGUVIS and GUI-R1, this evaluation is narrow—restricted to a small 500-sample subset of AndroidControl. More importantly, for the key online benchmarks (AndroidWorld and AndroidLab), there is no direct comparison with advanced fine-tuning methods. As a result, the empirical evidence remains weak, making it hard to judge whether ASL truly surpasses mainstream approaches under realistic settings."}, "questions": {"value": "1. The validity of ASL depends entirely on Gemini-Flash-2.0’s ability to accurately simulate UI transitions.\n  Could you provide quantitative evaluation or qualitative analysis verifying how well these predicted transitions match real ones?\n2. How does SEE perform if another LLM (e.g., GPT-4o, Qwen, Claude) is used as the world model? Is the framework robust to such substitutions?\n3. If the world model mispredicts transitions, how does this affect the reward signal and convergence of ASL? How is the continuous reward signal handled during SFT? Why is the threshold only introduced in the RL stage but not in SFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KEzid8rwAL", "forum": "s9ulMkvG9u", "replyto": "s9ulMkvG9u", "signatures": ["ICLR.cc/2026/Conference/Submission9757/Reviewer_nxxF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9757/Reviewer_nxxF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453872804, "cdate": 1761453872804, "tmdate": 1762921249599, "mdate": 1762921249599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces something called Action Semantics Learning (ASL) for app agents. The main idea is to focus on what the actions mean in terms of changing the app state, instead of just copying the exact words of the actions. They use a semantic estimator to reward actions that have the same effect, even if they look different. They say it's better for handling new situations and show some experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a good idea about making agents understand the meaning behind actions, not just the syntax. This could help with generalization. The semantic estimator part is clever because it uses existing models like LLM and BERT without needing extra training, which saves time. The experiments cover different setups and models."}, "weaknesses": {"value": "The paper says current methods force agents to copy action strings exactly, but that's not totally true. For example, there's earlier work like AutoDroid (https://arxiv.org/pdf/2308.15272) that already lets models choose from a list of actions instead of generating text, so the criticism is a bit off. Maybe the authors didn't look enough into related work.\n\nThe big claim is about handling out-of-distribution cases, but the experiments don't really prove it. The training and test data seem too similar, like from the same apps. There's no real tough OOD test, like with completely new interfaces or big changes, so it's hard to say if the semantic reward really helps in wild scenarios.\n\nUsing the semantic reward might be too heavy on computation. Since it relies on LLM predictions and BERT similarities for every training step, it could slow things down a lot, especially for bigger models or datasets. The paper doesn't talk about how long training takes or if it's practical for large-scale use.\n\nThe connection to programming language theory feels weak. They mention denotational semantics but don't really use it in a deep way, like for formal proofs or anything. It's more like a fancy comparison without solid backing, which makes the theory part seem vague.\n\nThe performance numbers are pretty low compared to other recent methods, like those on the AndroidWorld leaderboard where small models do better. The paper doesn't explain why their results are worse or how to improve them. For instance, success rates around 70% might not be competitive, and there's no discussion on limitations or next steps."}, "questions": {"value": "- Can you discuss the fundamental advantages of ASL over action-selection methods like AutoDroid?\n- Can you demonstrate OOD robustness on apps with entirely unseen UI?\n- Can you discuss the training time overhead of the semantic reward mechanism?\n- Why does ASL underperform compared to recent AndroidWorld leaderboard results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AJlCBgXOfD", "forum": "s9ulMkvG9u", "replyto": "s9ulMkvG9u", "signatures": ["ICLR.cc/2026/Conference/Submission9757/Reviewer_7Fu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9757/Reviewer_7Fu5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815400918, "cdate": 1761815400918, "tmdate": 1762921249275, "mdate": 1762921249275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Action Semantics Learning (ASL), a framework designed to train models to grasp the semantics of actions rather than simply reproduce the exact ground-truth strings. ASL includes a Semantic Estimator (SEE) module to measure how similar two actions are in terms of the state changes they produce. To compute the semantic reward, ASL adopts Gemini-2.5-Flash to generate textual descriptions of the next GUI state for both the ground-truth and the predicted actions. Then, ASL is evaluated on both online and offline benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe paper identifies a real and often overlooked issue in GUI agent training: different actions can lead to the same next interface state, yet most current methods treat them as distinct due to strict string matching. Framing this as a semantic rather than syntactic learning problem is both intuitive and meaningful.\n-\tThe adoption of world models to simulate state transitions and compute a semantic reward through cosine similarity is neat idea, which combines conceptual clarity with practical feasibility.\n-\tThe authors evaluate across both online and offline benchmarks, and include analyses under SFT and RL settings."}, "weaknesses": {"value": "-\tWhile ASL point out the mismatch between syntax and semantics at the single-action level, it does not extend to cases where multiple action sequences can achieve the same task outcome. This multi-step semantic equivalence is more central to how real GUI agents should reason, and remains outside the current formulation.\n-\tFor both the AndroidLab and WebArena-Lite, task success is judged by another LLM. It inevitably raises questions about the reliability and consistency of the reported results. A small validation study with human raters, or a discussion of the LLM evaluator’s bias, would help strengthen the paper’s credibility.\n-\tAcross most benchmarks, improvements are within a few percentage points, and comparisons are limited to relatively weak baselines. It would be more convincing to include more existing works.\n\nMinor issues:\nIn line 362, the sentence “The 2K human-generated samples are curated to ensure no overlap with the test set in AndroidControl.” appears inconsistent with the preceding description, which indicates that the 2K samples come from AndroidWorld rather than AndroidControl."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D8bmqcMc6E", "forum": "s9ulMkvG9u", "replyto": "s9ulMkvG9u", "signatures": ["ICLR.cc/2026/Conference/Submission9757/Reviewer_zEB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9757/Reviewer_zEB7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880017093, "cdate": 1761880017093, "tmdate": 1762921248856, "mdate": 1762921248856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}