{"id": "38Ey1FrSDt", "number": 9079, "cdate": 1758109658108, "mdate": 1763714995221, "content": {"title": "Adaptive Destruction Processes for Diffusion Samplers", "abstract": "This paper explores the challenges and benefits of a trainable destruction process in diffusion samplers -- diffusion-based generative models trained to sample an unnormalised density without access to data samples. Contrary to the majority of work that views diffusion samplers as approximations to an underlying continuous-time model, we view diffusion models as discrete-time policies trained to produce samples in very few generation steps. We propose to trade some of the elegance of the underlying theory for flexibility in the definition of the generative and destruction policies. In particular, we decouple the generation and destruction variances, enabling both transition kernels to be learnt as unconstrained Gaussian densities. We show that, when the number of steps is limited, training both generation and destruction processes results in faster convergence and improved sampling quality on various benchmarks. Through a robust ablation study, we investigate the design choices necessary to facilitate stable training. Finally, we show the scalability of our approach through experiments on GAN latent space sampling for conditional image generation.", "tldr": "Training the destruction (noising) process with state-dependent means and variances in diffusion-based samplers of unnormalised densities improves few-step modelling on standard benchmarks and high-dim applications.", "keywords": ["diffusion samplers", "diffusion models", "sampling", "amortized inference", "GFlowNets", "reinforcement learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd492bb74232049ced6c7f0084501c8680dc665d.pdf", "supplementary_material": "/attachment/e76d57b2c7ed43ceef394524275f013e4d38be39.zip"}, "replies": [{"content": {"summary": {"value": "The paper jointly learns generation and destruction in a discrete-time sampler, enabling state-dependent, learnable variance in both directions. This sidesteps continuous-time path-KL issues when variances differ and is supported by bounded parameterizations plus stabilization tactics (shared backbone, separate optimizers, target nets, replay). Empirically, it helps most in few-step regimes and narrow-mode targets; second-moment objectives (TB/VarGrad) are competitive with or better than PIS while being more memory-friendly. A style-transfer/latent-space demo shows the approach scales. Main trade-offs: weaker ties to continuous-time bridges, hyperparameter sensitivity (especially destruction LR), occasional instability (e.g., TLM at large steps), limited breadth beyond faces, and extra compute for some conditional setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation; concrete contribution: learnable, state-dependent variance for both directions in discrete time.\n\n2. Thoughtful engineering for stability with solid ablations.\n\n3. Good theoretical positioning vs. IPF/SB/CMCD/GFlowNet."}, "weaknesses": {"value": "1. Continuous-time connection & guarantees. Because the method allows different variances for generation vs. destruction, the link to continuous-time bridge formalisms seems less direct. Could you clarify what guarantees still hold in discrete time (e.g., well-posedness, stability, convergence/consistency), and how you ensure or demonstrate them in practice? Any safeguards to prevent pathological transitions when the two variances diverge?\n\n2. Hyperparameter sensitivity & tuning. The approach appears sensitive to learning-rate ratios—especially for the destruction model—which may affect stability and reproducibility across datasets/seeds.  How to guarantee the stability of training across settings?"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbyVUSBKvT", "forum": "38Ey1FrSDt", "replyto": "38Ey1FrSDt", "signatures": ["ICLR.cc/2026/Conference/Submission9079/Reviewer_83Ld"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9079/Reviewer_83Ld"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581895738, "cdate": 1761581895738, "tmdate": 1762920786397, "mdate": 1762920786397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a trainable destruction process technique during discrete diffusion model's sampling process. In detail, the method proposed a novel view for diffusion sampling. The experimental results showed that the deconstruction process yielded in fastering convergence, and higher ELBO. And the ablation studies show the effectiveness of the decomposition process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Framework novelty. The method firstly extend the traditional diffusion process into learnable variances in an unified theoretical framework. \n\n2. Integration of stability mechanism. The paper involved reinforcement-learning stabilization tools inspired by reinforcement learning's view. And Table 2 systematically evaluate the performance of each tool. \n\n3. Scalability to high-dimensional tasks. Section 4.4 demonstrated the capability of the method to higher dimension image generation tasks, which leads to boarder applications."}, "weaknesses": {"value": "1. Insufficient theoretical analysis. Although there is unified framework and well-defined processes, no analysis of the convergence or gradient bias of KL divergence is provided. \n\n2. Lack of continuous-time analysis. There is no proof for the equivalence between the generation and th destruction processes as T goes to infinity. \n\n3. Limited evaluation to TLM. The paper proposed TB and TLM, but the main experiments were conducted by TB."}, "questions": {"value": "1. In equation 14, does the choice of proposal distribution tied to the off-policy exploration in Section 3.3?\n\n2. Is there analysis showing the connection between the learnable variances between two processes?\n\n3. How will the method behaive if the energy landscape violates smoothness assumptions (non-Lipschitz E(x))? Does the KL objective in Eq. (13) remain well-defined under this situation?\n\n4. Were target networks and PER ablated individually or only in combination (Table 2)? How sensitive is performance to the PER replay ratio?\n\n5. The paper alternates between KL-based and second-moment objectives. Is there a principled reason not to use hybrid losses (weighted KL + TB)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gvkEnLRHoJ", "forum": "38Ey1FrSDt", "replyto": "38Ey1FrSDt", "signatures": ["ICLR.cc/2026/Conference/Submission9079/Reviewer_YXt4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9079/Reviewer_YXt4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709883250, "cdate": 1761709883250, "tmdate": 1762920785969, "mdate": 1762920785969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for training diffusion samplers by jointly optimizing both the generative and destruction processes in discrete time. Unlike prior approaches that fix the destruction process or assume continuous-time SDE dynamics, the authors propose learning both forward and reverse transitions, with decoupled, state-dependent variances parameterized via neural networks. This flexibility enables better adaptation to limited-step sampling regimes and complex energy landscapes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel joint training of generation and destruction processes in diffusion samplers, enabling improved convergence and sampling quality, especially in few-step regimes.\n\n2. Flexible design with state-dependent, decoupled variances for both processes—only possible in discrete-time formulation—leading to enhanced adaptability to complex energy landscapes."}, "weaknesses": {"value": "1. Limited visual results: The paper presents few qualitative or visual examples (only human faces in Fig. 4), making it difficult to fully assess sampling quality, especially in image-related tasks.\n\n2. No discussion of limitations: The paper lacks a section acknowledging potential limitations (e.g., scalability to more complex distributions, sensitivity to architecture choices), which raises concerns about generalizability."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mMnIBX2mI3", "forum": "38Ey1FrSDt", "replyto": "38Ey1FrSDt", "signatures": ["ICLR.cc/2026/Conference/Submission9079/Reviewer_sTsU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9079/Reviewer_sTsU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016395523, "cdate": 1762016395523, "tmdate": 1762920785682, "mdate": 1762920785682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for training diffusion samplers, which are models designed to sample from an unnormalized density (defined by an energy function) without access to data samples. Specifically, it introduces the joint training of both the generation and destruction processes by viewing them as discrete-time policies. The core novelty is decoupling their variances, which enables the means and variances of both processes to be learned as state-dependent neural networks, rather than being fixed or constrained by continuous-time theory. The experiments demonstrate that this joint training approach results in faster convergence and improved sampling quality across various small scale benchmarks, especially when the number of generation steps is limited or the energy landscape has narrow modes. This scalability is validated on a high-dimensional GAN latent space sampling task, showing quantitative and qualitative benefits for conditional image generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem studies is novel and interesting\n2. The proposed training objective, Second-Moment Divergence, deviates from the standard KL formulation, which is an interesting direction to explore. \n3. The design space is meticulously swept over, with the key ingredients for stable training reported in this paper. \n4. The advantage is most pronounced when the number of sampling steps is small. The paper shows that on some tasks, its method with as few as 5 steps can outperform 20-step samplers that use fixed variances. \n5. Their experiments on synthetic dataset, though limited by their scale, are very explanatory."}, "weaknesses": {"value": "1. Despite the non-trivial efforts to stabilize the training, the joint training process is inherently unstable. The Trajectory Likelihood Maximization (TLM) objective, one of the main candidates for generically training the destruction process, is \"unstable and often leads to divergent training\" when the number of steps is large. \n2. The method also seems to be highly sensitive to hyperparameters. L265-266, \"tuning relative learning rates is critical for stable training\". \n3. The results on GAN, which is regarded as scalability test of the method, has mixed results."}, "questions": {"value": "Theoretically speaking, would the trained sampler guaranteed to approximate the target distribution as the number of sampling steps goes to infinity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jIcWcYS58y", "forum": "38Ey1FrSDt", "replyto": "38Ey1FrSDt", "signatures": ["ICLR.cc/2026/Conference/Submission9079/Reviewer_CUPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9079/Reviewer_CUPi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116638373, "cdate": 1762116638373, "tmdate": 1762920785215, "mdate": 1762920785215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}