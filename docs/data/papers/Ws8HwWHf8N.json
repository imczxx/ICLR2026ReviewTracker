{"id": "Ws8HwWHf8N", "number": 16574, "cdate": 1758266256208, "mdate": 1759897231795, "content": {"title": "VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics", "abstract": "Current video generation techniques mainly focus on creating under text or image conditioning. However, real-world applications often require seamlessly connecting two independent video clips. To address this, we introduce **Video Connecting**, an innovative task that aims to generate smooth intermediate video content between given start and end clips.\nHowever, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed **VC-Bench**, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: **Video Quality Score** *VQS*, **Start-End Consistency Score** *SECS*, and **Transition Smoothness Score** *TSS*. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics.\nWe evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, with notable performance gaps among models. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.", "tldr": "", "keywords": ["Video Generation", "Video Connecting", "VC-Bench"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0e75ec104669c428e19a8a6ff12d7546b59b116.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the novel task of Video Connecting, which aims to generate transitional video content that seamlessly links a given start clip and end clip. The authors propose VC-Bench, a new benchmark to evaluate this task, consisting of a 1579 video dataset and a three part evaluation framework (Video Quality Score, Start-End Consistency Score, and Transition Smoothness Score). The paper provides a comprehensive evaluation of several video generation models on this benchmark, identifying current limitations in start-end consistency and transition smoothness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formalizes the Video Connecting task, which, while related to existing video generation problems, presents a non-trivial challenge. The paper provides a valuable comparison by adapting and evaluating several recent state-of-the-art video generation models for this new task.\n\n- The paper offers a detailed pipeline for the VC-Bench dataset construction and the calculation of the proposed evaluation metrics."}, "weaknesses": {"value": "- The long-term impact of the benchmark may be limited, as Video Connecting could be viewed as a niche or minor task rather than a foundational problem. There is significant overlap with existing video generation, extension, or interpolation tasks, and few works are specifically dedicated to this problem, which may limit the benchmark's adoption.\n\n- The dataset construction pipeline (e.g., scene detection, clip filtering, captioning) and core evaluation metrics (particularly the Video Quality Score components) are largely adopted from existing benchmarks for text-to-video and image-to-video generation. The newly proposed task-specific metrics, such as the Start-End Consistency Score and Transition Smoothness Score, appear to be straightforward implementations and lack significant novelty.\n\n- The proposed SECS and TSS metrics rely on a direct comparison against the ground-truth video. For a generative task, there are potentially many plausible ways to connect two clips. Relying on ground-truth similarity may unfairly penalize novel or creative, thus making the metrics less reliable for evaluating the true generative capabilities of a model.\n\n- Related to the point above, while the paper distinguishes the VC task from First-Last Frame to Video generation, the evaluation metrics do not seem to fully capture the complexity of ensuring content consistency with the entirety of the start and end clips, instead focusing on pixel-level and optical flow comparisons which are still largely frame-based."}, "questions": {"value": "- Regarding the human alignment evaluation (Section 5.3): Did the authors just check the correlation with human scores, or did they actively try to make the metrics match human preferences? For instance, how were the weights for the sub-metrics (in VQS, SECS, TSS) decided? Were they tuned to match human scores, or just set by a simple rule, like averaging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IwO3f2AeYV", "forum": "Ws8HwWHf8N", "replyto": "Ws8HwWHf8N", "signatures": ["ICLR.cc/2026/Conference/Submission16574/Reviewer_u86t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16574/Reviewer_u86t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890379474, "cdate": 1761890379474, "tmdate": 1762926653605, "mdate": 1762926653605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VC-Bench, a novel benchmark for evaluating models on the emerging Video Connecting (VC) task — generating smooth, temporally coherent transitions between given start and end video clips. The benchmark includes a curated dataset of 1,579 high-quality videos spanning 15 major and 72 subcategories, along with 9 quantitative metrics that evaluate three key dimensions: Video Quality, Start-End Consistency, and Transition Smoothness. Results highlight the open-source models’ limitations in maintaining continuity and temporal smoothness, while human evaluation shows strong alignment with the proposed metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Task Definition: Clear formulation of the Video Connecting task as a distinct challenge, bridging isolated generation and temporal continuity\n- Comprehensive Benchmark Design: A well-curated dataset with rigorous filtering, aesthetic scoring, and scene detection ensures quality and diversity."}, "weaknesses": {"value": "- Model Diversity: Evaluation excludes closed-source systems (e.g., Sora, Runway Gen-3) that might exhibit different performance trends.\n- Metric Interpretability: Some metrics (e.g., Video Connecting Distance) could benefit from additional qualitative examples to illustrate their perceptual meaning.\n- Minor Writing Artifacts: Occasional typographical spacing and minor stylistic inconsistencies could be refined."}, "questions": {"value": "- How sensitive are the evaluation metrics (especially TSS and SECS) to different video lengths and resolutions?\n- Could VC-Bench be extended to evaluate multi-clip or looped video transitions beyond simple start-end pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DSfc3fsJ4U", "forum": "Ws8HwWHf8N", "replyto": "Ws8HwWHf8N", "signatures": ["ICLR.cc/2026/Conference/Submission16574/Reviewer_WfZb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16574/Reviewer_WfZb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898516580, "cdate": 1761898516580, "tmdate": 1762926653110, "mdate": 1762926653110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark for the video connecting task, which requires video generation models to synthesize the missing intermediate frames between the start and end video clips. Specifically, this paper apply scene detection, periodic motion detection, and video clips extraction to construct the datasets, which provides the diversity on open-domains. This paper further proposes 9 metics based on video quality, start-end consistency and transition smoothness score. Comprehensive experiments are conducted on various video genernation models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed task is highly valuable and meaningful for future research.\n2. This paper is well-organized and easy to read."}, "weaknesses": {"value": "1. The paper needs a more in-depth comparative analysis beyond the task definition to clarify what specific innovations have been made in constructing this benchmark, especially compared with the existing First-Last Frame to Video task.\n2. Compared with the existing First-Last Frame to Video task, what different requirements does the video connecting task impose on the generation model? Are there corresponding experiment results to support this in the evaluation?"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EWFxnwy3Ff", "forum": "Ws8HwWHf8N", "replyto": "Ws8HwWHf8N", "signatures": ["ICLR.cc/2026/Conference/Submission16574/Reviewer_Qn9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16574/Reviewer_Qn9h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195081720, "cdate": 1762195081720, "tmdate": 1762926652638, "mdate": 1762926652638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}