{"id": "EzHOmjg3R6", "number": 10260, "cdate": 1758165378622, "mdate": 1759897662459, "content": {"title": "DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching", "abstract": "Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation. To address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction. In addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss. Experimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints.", "tldr": "a semi-autoregressive framework that enables fast, high-fidelity song generation, overcoming long-sequence lyric alignment and multi-preference optimization challenges", "keywords": ["song generation", "block flow matching", "direct preference optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e9d1160f8e5c052289a45dc5e2a9b724bb70783.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This technical paper proposes DiffRhythm 2, a chunk-based diffusion autoregressive model for full song generation.\nThe main design choices of the presented approach are:\n- using autoregressive chunk-wise generation, where each chunk is modeled using Flow matching, conditioned on the previous clean samples. Not novel, very similar to \"Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models\" in the text domain.\n- leveraging Representation alignment loss (REPA) as an extra regularization. To allow the use of pretrained feature extractors trained on longer audio chunks (here MuQ), noisy features are concatenated with clean features to produce sequences of suitable size. Since it's too costly to regularize all such subsequences, only a few are kept, leading to the proposed \"stochastic block REPA loss\"\n- \"cross-pair preference optimization strategy\", which only consists in slight modification of how samples are ranked during the DPO phase: \"we pair (musicality, lyric alignment accuracy) and (style similarity, audio quality). During DPO training, the winning sample must satisfy both preferences within a pair, whereas the losing sample is required to satisfy at least one\".\n- training over 1.7million songs\n\nIf not necessarily novel, these design choices are carefully evaluated using objective and subjective evaluations. These reveal consistent improvements over the previous iteration DiffRhythm+ and other open source music generation models, while still staying below commercial models like SUNO v4.5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The plan to release code and checkpoints is an important point but needs to effectively done, as this would be the main contribution of this paper. \nThe design choices are relevant and proved to be effective enhancing the quality over previously released open source models.\nIf effective and if the accompanying website features interesting generation, this paper is still rather incremental."}, "weaknesses": {"value": "The writing is rather general and we can regret that some technical points are not referenced.\ne.g. \"SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts.\"\n\"Existing approaches typically rely on merging separately optimized models.\"\n\nSome repetitions between intro and related works.\nLack of references beyond music generation. Many building blocks already exists in the literature and should be properly cited like\n- Block Diffusion\n- Diffusion DPO\nsince they are used in the presented approach.\nOverall, the paper is not really self-contained: a more detailed paragraph on REPA could help. The reader needs to check in the overview section 3.1 to know what SSL backbone was used (there's no mention of it in the main 3.4 STOCHASTIC BLOCK REPA LOSS section).\nIt is also hard to understand the DPO procedure from the paper alone."}, "questions": {"value": "Can you detail the DPO training? it seems that it should be some sort of DiffusionDPO, but the paper is not cited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Unclear regarding data sources:\n\"DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with\na total duration of about 70,000 hours\""}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GpVtEcM69p", "forum": "EzHOmjg3R6", "replyto": "EzHOmjg3R6", "signatures": ["ICLR.cc/2026/Conference/Submission10260/Reviewer_Ph3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10260/Reviewer_Ph3U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580518415, "cdate": 1761580518415, "tmdate": 1762921617022, "mdate": 1762921617022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffRhythm 2, a semi-autoregressive song generation framework based on block flow matching to improve alignment between lyrics and vocals without relying on timestamp labels. The model integrates a Music VAE with a 5 Hz low-frame-rate compression, which enables efficient long-sequence modeling, and introduces stochastic block REPA loss to enhance structural coherence and musicality. Furthermore, a cross-pair preference optimization strategy is proposed to mitigate conflicts among multiple human-preference dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The block flow matching addresses lyric–vocal alignment without timestamp labels, reducing data preprocessing requirements and improving usability.\n2. The cross-pair preference optimization takes into account the interdependence among different optimization dimensions in song generation.\n3. The work involves substantial engineering effort and system implementation."}, "weaknesses": {"value": "1. The definition of “lyrics alignment” is insufficiently precise. It is unclear whether this term refers purely to lyric accuracy (i.e., no omissions or mispronunciations of words) or also encompasses prosodic naturalness—such as rhythm, phrasing, and pauses. In general, compared with AR models, non-autoregressive (NAR) models tend to excel in lyric accuracy but struggle with natural rhythmic expressiveness.\n2. Block flow matching is not a new idea; it has been applied in video generation [1] and text-to-speech synthesis [2]. Please include proper citations in the method section and clarify what methodological novelty this paper introduces beyond these prior works.\n3. Could you provide the comparison results of generation quality and speed with Yue? In the appendix, the paper states, “We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2,” but the results for Yue are not included.\n4. The presentation quality could be improved. Figures 1 and 2 convey limited information, and Sections 3.4 and 3.5 would benefit from additional diagrams to support the textual descriptions.\n\n[1] Zhang, Yuan, et al. Generative Pre-trained Autoregressive Diffusion Transformer. arXiv:2505.07344 (2025).\n[2] Liu, Zhijun, et al. Autoregressive Diffusion Transformer for Text-to-Speech Synthesis. arXiv:2406.05551 (2024)."}, "questions": {"value": "1. What are the specific innovations when applying block flow matching to the text-to-song generation task?\n2. How was the block size determined, and how does it affect model performance and stability?\n3. The paper mentions 70,000 hours of music data and 100,000 hours of speech data. How are these two datasets specifically used during training? In the first example, compared with LeVo, the generated songs show less natural rhythm, weaker vocal techniques, and a relatively thin accompaniment. Does this mean that the speech data had a stronger influence on the model?\n4. Are there controlled ablation experiments verifying the contribution of block flow matching to lyric accuracy and rhythmic naturalness, while excluding the effects of different training datasets?\n5. The paper introduces a 5 Hz Music VAE. What enables such a low bitrate? You report reconstruction metrics, but how does this low frame rate affect generation quality? What would happen if a higher-bitrate VAE were used?\n6. How was the evaluation set constructed? What are its data sources and style distributions? How do you ensure fairness across systems, and will this dataset be released publicly?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper presents a full-song generation model that can synthesize realistic human singing voices. Such capability raises potential misuse concerns, including impersonation, deepfake generation, and copyright infringement. Clear ethical guidelines and data usage disclosures are recommended."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ax7muhyRnG", "forum": "EzHOmjg3R6", "replyto": "EzHOmjg3R6", "signatures": ["ICLR.cc/2026/Conference/Submission10260/Reviewer_pj94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10260/Reviewer_pj94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795404320, "cdate": 1761795404320, "tmdate": 1762921616674, "mdate": 1762921616674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffRhythm 2, a semi-autoregressive song generation framework built on block flow matching. The system (i) compresses audio with a 5 Hz Music VAE to keep long sequences tractable; (ii) achieves lyrics–vocal alignment without external timestamps via block-wise flow matching and an EOP (end-of-prediction) mechanism; (iii) improves musical structure using a stochastic block REPA loss; and (iv) addresses multi-preference post-training with cross-pair preference optimization (CPPO) rather than model merging. Experiments on ~300 prompts report better objective/subjective scores than prior open-source systems (DiffRhythm+, ACE-Step, LeVo), competitive generation speed, and up to 210 s coherent songs. The paper positions itself as a fast, high-fidelity, controllable text/audio-to-song generator."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear technical framing & originality. The block flow matching formulation that is non-AR within block yet AR across blocks is a neat, minimalistic way to get alignment while keeping efficiency. The timestep trick (S/L set to −1, clean=1, noisy∈[0,1]) to disambiguate streams is simple and effective.\n\n2. Practical long-sequence engineering. Using a 5 Hz VAE plus block-level KV cache gives a realistic path to multi-minute songs with stable inference time; the paper also discusses the EOP design choices and their failure modes.\n\n3. PPO avoids the usual degradation from weight-space interpolation of multiple DPO heads.\n\n4. Good empirical results. Objective metrics (PER, Mulan-T/A, Audiobox-Aesthetics, SongEval) and ablation (w/o DPO, w/o CPPO, w/o REPA) are comprehensive; results are consistently strong vs. open-source baselines with a reasonable gap to top commercial systems."}, "weaknesses": {"value": "1. My main concern is that this paper reads more like a technical report or system description rather than a research paper with focused insights.\n\nAlthough the engineering contributions like block flow matching, low-frame-rate VAE, stochastic REPA loss, and cross-pair preference optimization are each well-motivated and empirically validated, the work overall feels like a composition of effective engineering tricks rather than a cohesive theoretical or methodological advancement.\n\n2. Missing baselines: SongGen, Yue, and SongBloom."}, "questions": {"value": "1.  While PESQ (perceptual evaluation of speech quality) and STOI are widely used for speech quality,  is it reasonable to use the same metrics for music ?\n\n2.  Another issue is that the choice of the Music VAE is not sufficiently justified. Because it is well known that good reconstruction does not necessarily imply good generation quality [1].\n\n[1] Yao J, Yang B, Wang X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 15703-15712."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xyBgjcbquA", "forum": "EzHOmjg3R6", "replyto": "EzHOmjg3R6", "signatures": ["ICLR.cc/2026/Conference/Submission10260/Reviewer_VLPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10260/Reviewer_VLPj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976590810, "cdate": 1761976590810, "tmdate": 1762921615436, "mdate": 1762921615436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffRhythm2, a method for full-song generation based on block flow matching. The approach models music signals using a variational autoencoder (VAE) that operates at a low frame rate of 5 Hz. The authors further propose a cross-pair preference optimization strategy to enhance robustness across diverse human preferences, complemented by a musicality and structural coherence alignment loss. Experimental results show that DiffRhythm2 is capable of generating complete songs of up to 210 seconds, outperforming existing open-source models in both subjective and objective evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is simple, elegant, and easy to follow\n2. The paper is well written and easy to follow\n3. Results and provided samples are impressive"}, "weaknesses": {"value": "1.\tThe experimental setup could be strengthened, particularly in relation to the paper’s primary contribution.\n2.\tSeveral methodological details and results are missing—for example, information regarding the VAE configuration, hyperparameters, and related implementation choices.\n3.\tThe overall contribution of the proposed approach could be better articulated and justified. At the moment, some components appear to be loosely integrated ideas aimed primarily at improving generation performance, rather than forming a cohesive methodological framework."}, "questions": {"value": "1.\tThe authors describe training a VAE with various reconstruction losses and discriminators. However, it is unclear whether any KL regularization or other latent constraints were applied. In other words, what specifically distinguishes this model as a variational autoencoder rather than a standard autoencoder? Based on the current description, it appears closer to an AE than a true VAE.\n2.\tIn Table 2, the authors report subjective evaluation results. It would be helpful to include measures of variability—such as standard deviation or 95% confidence intervals—to better assess the statistical significance of these findings.\n3.\tTable 3 presents ablation studies over different loss functions, which is valuable. However, additional ablations on the model architecture would further strengthen the analysis—especially since architectural design is highlighted as a primary contribution and emphasized in the title. For example, what are the advantages of using block diffusion? how does the block size influence performance? etc.\n4.\tThe related work section should include appropriate citations. For instance, the statement “Conditional flow matching (CFM) further conditions the vector field on external information, enabling guided generation. CFM is widely used in the field of image generation and has also been applied to TTS tasks in recent years.” requires supporting references to prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pvoWSnY5ct", "forum": "EzHOmjg3R6", "replyto": "EzHOmjg3R6", "signatures": ["ICLR.cc/2026/Conference/Submission10260/Reviewer_UTAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10260/Reviewer_UTAb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155049374, "cdate": 1762155049374, "tmdate": 1762921614405, "mdate": 1762921614405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}