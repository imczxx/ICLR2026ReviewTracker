{"id": "RtCrDs9xQg", "number": 10280, "cdate": 1758165827787, "mdate": 1759897661279, "content": {"title": "Multi-Token Policy Gradient Optimization", "abstract": "Policy-gradient optimization methods like PPO typically operate at the token level, estimating action probabilities for each next-token prediction. While effective, this formulation often mismatches the sequence-level nature of rewards in reasoning tasks, such as math equations and multi-step derivations. To address this issue, we propose Multi-Token Policy Gradient Optimization (MPO), a framework that treats blocks of K future tokens as atomic actions, directly aligning the optimization granularity with semantic segments of text. This block-level view better captures the structure of reasoning trajectories and provides a more faithful connection between policy optimization and sequence-level objectives. To stabilize training, we additionally adopt a log-space approximation of importance ratios, which mitigates numerical instability during optimization. Experiments on GSM8K and MATH show that MPO consistently outperforms standard token-level policy gradient baselines, demonstrating the effectiveness of modeling multi-token actions for large-scale language model post-training.", "tldr": "", "keywords": ["reinforcement learning", "large language model", "reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e7d4fb758d1f1f3857aaceb288f29a35ce3dc79.pdf", "supplementary_material": "/attachment/5c51e60f08082a89c530891bd989d61c75d8ca9b.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the challenge of granularity mismatch between sequence-level rewards and token-level actions in auto-regressive language models. The authors propose Multi-token Policy Gradient Optimization (MPO), a method that addresses this by aggregating contiguous blocks of future tokens into unified action units, rather than treating each token as an isolated action. The method is evaluated on the GSM8K and MATH benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is generally well-written and easy to follow.\n- The idea of treating semantically continuous tokens as a group for optimization is novel and intuitively sound: disadvantageous token spans should be suppressed collectively, while advantageous ones should be encouraged as a whole.\n- This paper tries to address the important challenge of the granularity mismatch between token-level optimization and sequence-level reward signals in current RLVR scenario."}, "weaknesses": {"value": "- **The motivation behind this paper is somewhat confusing** \n\nWhile MPO is proposed to address the granularity mismatch between token-level generation and sequence-level rewards by aggregating tokens into blocks, a mismatch still remains between these block-level units and the sequence-level reward. The authors should clarify how this approach truly resolves the core issue.\n\n- **Lack of Justification for hyperparameter K**\n\nThe paper's motivation for MPO is based on grouping semantically coherent segments, as illustrated in Figure 1(a) in the paper, where segments like equations vary in length. For MPO to be most effective, one would expect it to identify the natural boundaries of these semantic units. However, as described in Section 4, the method does not appear to identify such boundaries and instead relies on a fixed hyperparameter K to evenly divide the token sequence into contiguous blocks.\n\nA related question concerns the optimal value of K. Table 2(a) indicates that K=5 yields the best performance among the values {2,3,5}. This seems to contradict the intuitive example in Figure 1(a), where the authors state that the \"model's decision-making process needs to switch constantly between long token segments with more than 10 tokens.\" Could the authors explain this apparent mismatch? Specifically, why does a relatively small K=5 achieve optimal performance when the semantic segments it is meant to approximate are typically much longer?\n\n- **Limited experimental evaluation**\n\nThe experimental evaluation is limited to two mathematical reasoning benchmarks, GSM8K and MATH. This raises concerns about the generalizability of MPO's effectiveness. To robustly support the claim that MPO is effective for \"large language model post-training,\" additional evaluations on diverse tasks (e.g., code generation and instruction following) are necessary."}, "questions": {"value": "In section 6.2, the authors state, \"(clip fraction) reduction becomes more pronounced as the proportion of future information increases. This suggests that future-aware training stabilizes policy updates\" (lines 405-407). However, on lines 417-419, they state, \"This highlights that excessive incorporation of future information may lead to training instability.\" Both experiments were conducted using the same settings (MPO-10%/20%/30%/40%) but appear to have contradictory conclusions. I hope the authors can clarify this for me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UoE8RsOyPa", "forum": "RtCrDs9xQg", "replyto": "RtCrDs9xQg", "signatures": ["ICLR.cc/2026/Conference/Submission10280/Reviewer_fG3C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10280/Reviewer_fG3C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760521244676, "cdate": 1760521244676, "tmdate": 1762921633676, "mdate": 1762921633676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multi-Token Policy Gradient Optimization (MPO), a framework that extends token-level policy gradient methods by treating blocks of $K$ future tokens as atomic actions. The key motivations are: (1) to stabilize advantage estimation by aggregating future knowledge through multi-token prediction, and (2) to encourage higher-level planning over coherent text segments rather than individual tokens.\nTo realize this, MPO replaces the standard per-token importance sampling ratio with a block-level ratio computed over $K$ consecutive tokens. Because directly multiplying per-token ratios can cause high variance, the authors approximate this product using a weighted log-sum of log-ratios, inspired by the Log-COP-TD method.\nEmpirically, MPO consistently outperforms PPO and GRPO baselines on mathematical reasoning benchmarks (GSM8K and MATH), demonstrating improved stability, lower clip fraction, and higher accuracy in sequence-level reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "MPO outperforms GRPO and PPO on mathematical reasoning benchmarks (GSM8K and MATH)"}, "weaknesses": {"value": "Although this paper is practical, it lacks sufficient experiments.\n- The experimental results are based on relatively small language models (1B or 1.5B).\n- The GSM8K and MATH benchmarks are not sufficient to demonstrate the performance of LLMs. Additional tasks should be included (at least those used in the GRPO paper)."}, "questions": {"value": "I have following questions about this paper:\n\n1. Additional Experiments (related to Weaknesses)\n\n    The experimental results are based on relatively small language models (1B and 1.5B), which may not be sufficient to evaluate the alignment effect. Are there any additional results on larger models? In addition, the GSM8K and MATH benchmarks may not adequately demonstrate the performance of LLMs. Do the authors have results on other tasks, such as those used in the GRPO paper?\n\n2. About $\\beta\\_k$\n\n    The authors state that $\\beta\\_k$ are normalized to $\\sum\\_{n=1}^K \\beta\\_n=1$. I assume this means $\\beta\\_k=\\frac{\\tilde\\beta\\_k}{\\sum\\_{n=1}^K \\tilde\\beta\\_n}$ for arbitrary $\\{\\tilde\\beta\\_k\\}$. However, the paper seems to adopt a different kind of normalization. Could the authors clarify what “normalization” specifically means in this context?\n\n3. Approximation of $\\widetilde{R}$\n\n    Equations (11) and (12) are equivalent when $\\beta\\_n=1$ for all $n$. However, enforcing $\\sum\\_{n=1}^K \\beta\\_n=1$ may introduce additional bias, even if it helps reduce variance, as authors mention in line 289 - 290. How do the authors justify this design choice? Moreover, given the potential bias, is the comparison in Figure 4 fair?\n\n4. Trade-off between variance and bias\n\n    In this paper, both the choice of $K$ and the set of $\\{\\beta\\_n\\}$ involve a trade-off between variance and bias. Could the authors clarify how these trade-offs are characterized or managed in the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concern"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7cwPNO2MqF", "forum": "RtCrDs9xQg", "replyto": "RtCrDs9xQg", "signatures": ["ICLR.cc/2026/Conference/Submission10280/Reviewer_xY8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10280/Reviewer_xY8Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493844782, "cdate": 1761493844782, "tmdate": 1762921633143, "mdate": 1762921633143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Multi-Token Policy Gradient Optimization (MPO)**, a novel framework for post-training large language models (LLMs) using reinforcement learning. The central claim is that standard policy-gradient methods like PPO operate at a token-level granularity, which is a poor match for sequence-level rewards in complex reasoning tasks like mathematics. To address this, MPO redefines the policy gradient \"action\" as a **block of $K$ future tokens**. It computes the importance sampling ratio over this entire multi-token block rather than a single token.\n\nTo implement this, the method incorporates auxiliary **Multi-Token Prediction (MTP) modules**, which are first \"warmed up\" to predict future token probabilities. To manage the high variance that arises from multiplying multiple probability ratios, MPO introduces a **log-space approximation** (a weighted log-sum) to stabilize training. Experiments conducted on the GSM8K and MATH benchmarks show that MPO consistently outperforms token-level baselines, PPO and GRPO, in mathematical reasoning accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  **Important Problem:** The paper addresses a significant and timely problem: the granularity mismatch between the token-level optimization of standard policy gradient methods and the sequence-level, holistic nature of rewards in complex reasoning tasks. Improving RL alignment for multi-step reasoning is a critical research direction.\n\n- **Sufficient Literature Review:** The paper provides a comprehensive review of related work. It clearly positions MPO relative to both existing policy gradient methods (PPO, GRPO, DAPO) and the separate field of multi-token prediction (MTP) techniques.\n\n- **Novel Method:** The core contribution—modifying the policy gradient objective to compute importance sampling ratios over multi-token blocks—is a novel and interesting approach for LLM alignment.\n\n- **Clarity and Presentation:** The paper is generally well-written and easy to follow. The method is explained clearly, and the inclusion of diagrams like Figure 1 and Figure 2 is helpful for understanding the MTP module's role and the overall MPO training process."}, "weaknesses": {"value": "- **Insufficient Motivation:** The paper is not adequately motivated. While it claims MPO \"better captures the structure of reasoning\", it doesn't provide a strong intuitive or theoretical explanation for *why* computing the importance ratio over $K$ future tokens leads to a more effective or stable policy update compared to the standard single-token ratio.\n\n  \n\n- **Methodological Soundness and Cost:** The proposed method's reliance on *new* MTP modules raises significant concerns about soundness and efficiency:\n\n  - **Redundancy:** The authors do not explain why the original, backbone LLM cannot be used to auto-regressively compute the probabilities for the $K$ future tokens. The introduction of separate MTP modules seems redundant and adds significant complexity.\n\n  - **Parameter Overhead:** These new modules add a substantial number of parameters, increasing the model size significantly (e.g., 1.52x to 2.05x for $K=4$ on a Llama3.2-1B model).\n\n  - **Training Complexity:** The MTP modules require a separate \"warm-up\" phase before MPO training can begin, complicating the overall training pipeline.\n\n  - **Computational Cost:** When computing the MPO loss, the objective (Eq. 12) appears to require $K$ forward passes (or at least $K$ probability lookups) *for each token* $t$ in the trajectory to calculate the multi-token ratio. This represents a potential $K$-fold increase in computational cost for the loss computation, which is a major drawback.\n\n- **Limited Empirical Evaluation:** The empirical results could be improved. The method is only tested on two math benchmarks (GSM8K, MATH) and compared against only two baselines (PPO, GRPO). A key omission is a comparison against **DAPO**, which is cited as an advanced method for reasoning tasks and would serve as a much stronger baseline.\n\n- **Insignificant Performance vs. Cost:** The reported performance gains are not very significant, especially when weighed against the massive increase in computational and parameter costs. For example, on GSM8K (Deepseek-Qwen2.5-1.5B), MPO ($K=5$) improves over PPO by only 1.6 percentage points (0.882 vs. 0.866), while on MATH the gain is also 1.6 points (0.779 vs. 0.763). This marginal improvement does not seem to justify the 26-48% slowdown in training iteration speed and the 1.2x-2.05x increase in model size."}, "questions": {"value": "1. Could the authors clarify the precise reason for introducing separate MTP modules? Why not use the backbone LLM's own decoder to compute the future token probabilities $\\pi_{\\theta}(o_{i,t+n}|o_{i,1:t+n-1})$ for the importance ratio? What advantages do the specialized MTP module architectures (as shown in Figure 1b)  provide over the standard backbone?\n\n2. The paper motivates MPO as \"encouraging higher-level planning\" Could the authors elaborate on this mechanism? How does aggregating $K$ ratios via a weighted log-sum (Eq. 12) translate to a better policy gradient for long-term reasoning, beyond the variance reduction shown in Figure 4a?\n\n3. The best results are reported for $K=5$. However, the cost analysis in Table 3 only provides data for $K=2$ and $K=4$. What are the specific model size and training speed costs for the $K=5$ setting, which yielded the best performance?\n\n4. Given that DAPO is also a policy gradient method designed to improve upon PPO/GRPO for reasoning tasks, why was it omitted from the experimental comparison? How would the authors hypothesize MPO compares to DAPO in terms of both performance and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n9cahEQChq", "forum": "RtCrDs9xQg", "replyto": "RtCrDs9xQg", "signatures": ["ICLR.cc/2026/Conference/Submission10280/Reviewer_14o5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10280/Reviewer_14o5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928863632, "cdate": 1761928863632, "tmdate": 1762921632652, "mdate": 1762921632652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multi-Token Policy Gradient Optimization (MPO) that extends token-level policy optimization to multi-token policy optimization for matching the sequence-level nature of rewards in reasoning tasks. More specifically, MPO calculates importance sampling ratios (of PPO) over K future tokens. Then, MPO proposes the MPO objective in Eq. 13. This paper evaluates MPO on two representative mathematical reasoning benchmarks including GSM8K and MATH by using two language models including DeepSeek-Distilled-Qwen2.5-1.5B and Llama3.2-1B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1. [Presentation] First of all, this paper is very well written and organized.\n\n- S2. [Motivation] It is clearly motivated why multi-token policy optimization of LLMs is required for mathematical reasoning. \n\n- S3. [Analysis] Besides the main results, this paper provides a comprehensive empirical analysis on the effectiveness of MPO. It includes (1) variance of importance sampling ratios, (2) effect of future information injection, (3) effect of MTP hyper-parameters, and (4) reliability of incorporated future information. Also, this paper provides an analysis of time and memory cost, comparing MPO with PPO."}, "weaknesses": {"value": "- W1. [Performance] One of main weaknesses of this paper may be the performance gain, compared to the training cost. According to Figure 3, in case of DeepSeek-Qwen2.5-1.5B, MPO achieves the accuracy of 0.882, while PPO provides 0.866. According to Table 3, MPO takes 30% more training time and 40% more memory."}, "questions": {"value": "- Q1. This paper uses MPO to train relatively small language models such as Deepseek-Qwen2.5-1.5B and Llama3.2-1B. Is the performance improvements increased, if larger language models are used?\n\n- Q2. Current implementation of MPO is based on PPO. Is it possible to apply MTP to GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g4ZtVFZUQV", "forum": "RtCrDs9xQg", "replyto": "RtCrDs9xQg", "signatures": ["ICLR.cc/2026/Conference/Submission10280/Reviewer_7sKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10280/Reviewer_7sKG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977489105, "cdate": 1761977489105, "tmdate": 1762921632278, "mdate": 1762921632278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}