{"id": "BmkOKYfbmV", "number": 12526, "cdate": 1758208413902, "mdate": 1763666056403, "content": {"title": "The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimization", "abstract": "The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models.\nDespite its success in single generation problem solving, \nthe reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of\ngenerations and a resulting degradation of performance during Best-of-N sampling for large N values.\nIn this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k.\nWe extend on-policy gradient estimate to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency.\nEmpirically, we show that our objective effectively optimizes max@k metric in off-policy\nscenarios, aligning the model with the Best-of-N inference strategy.", "tldr": "", "keywords": ["RLVR", "Code Generation", "pass@k"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ae24427d87c9e0d7ee787f7b25db2a007f7d945.pdf", "supplementary_material": "/attachment/fa81e3a6bedff0dd0551d5a69aa0cb1c0ce54cc7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to improve performance of models fine-tuned with RLVR on mathematical reasoning tasks. As many models are used with best-of-N sampling at inference time, the paper proposes to explicitly optimize the max @ k metric which is compatible with the best-of-N sampling technique. It builds on existing estimators for the policy gradient of max @ k reward, and extends them to the off-policy setting prevalent in RLVR methods. The paper presents a few experiments to support the usage of this optimization target."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents its ideas and proposed contributions clearly and simply, the reasoning and motivation are easy to follow into the proposed method and experiments.\n\nThe derivation of the policy gradient estimator seems sound and correct. \n\nUsing the proposed estimator, it seems like the proposed method outperforms baselines on the targeted metric of max @ k for large k."}, "weaknesses": {"value": "My main concern with this paper is that the proposed method and results seem incremental and slightly insignificant.  \n\nAs the authors themselves note, the unbiased estimator for the max @ k policy gradient was developed by Walder & Karkhanis. The motivation for deriving this gradient has also been described in previous papers. In addition, while many modern RLVR algorithms are indeed effectively off-policy, most are designed as on-policy algorithms and assumptions are only relaxed due to necessity of implementation. The same holds for the max @ k objective estimator for the on-policy case. Given all of this, it seems like the novelty of this paper is very limited. \n\nThis compounds with the results being unimpressive - while found significant by the Wilcoxon test, it seems like there is very little gain over baselines. It is also lacking that results are not reported for k values other.than 128 (eg. similarly to the plot in Figure 1). Clarity in the way the baselines were run and measured would be a significant addition - see some questions below. \n\nFinally, it is worth wondering whether optimizing for best-of-N sampling, which in itself is necessary only because our models are imperfect, is good algorithmic design. It may border on the realm of Goodhart’s law - if we optimize for best-of-N, do we give up on improving top-1 performance?"}, "questions": {"value": "1. Figures 1/2: the effects do not seem very pronounced. How has the statistical significance of the motivating results been measured?\n2. In the Off-Policy BoN approach (as presented in the experiments), is the policy gradient presented at the end of section 3 evaluated directly, of is it incorporated into GRPO? If it is the latter, how is it incorporated? \n3. Following up on Q2, is any variance reduction performed? Does it follow mean or LOO-1 (comparing to the on policy version)?\n4. Does the derived policy gradient estimator incur higher computational complexity than standard reward? How does it compare to standard methods in terms of runtime/extra computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9tZj80QOBi", "forum": "BmkOKYfbmV", "replyto": "BmkOKYfbmV", "signatures": ["ICLR.cc/2026/Conference/Submission12526/Reviewer_wYXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12526/Reviewer_wYXd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844129027, "cdate": 1761844129027, "tmdate": 1762923391180, "mdate": 1762923391180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of current Reinforcement Learning with Verifiable Rewards (RLVR) methods, which optimize for single-sample accuracy but often reduce generation diversity, leading to suboptimal performance when applying Best-of-N (BoN) sampling strategies at inference time. Focusing on the code generation task, the authors propose a novel optimization objective based on max@k, a continuous generalization of the pass@k metric. They derive an unbiased gradient estimator for both on-policy and off-policy cases, enabling direct optimization of max@k. Empirical results on several code benchmarks demonstrate that the proposed off-policy BoN objective improves max@k performance over strong inference-aware baselines, showing its effectiveness in aligning RL optimization with BoN inference in code generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a novel optimization objective based on max@k which is a continuous generalization of the pass@k metric for aligning RL optimization with BoN inference in continuous reward settings like code generation tasks.\n- This paper proposes an unbiased gradient estimator for both on-policy and off-policy cases, enabling direct optimization of max@k.\n- The empirical results on several code benchmarks demonstrate that the proposed off-policy BoN objective improves max@k performance over strong inference-aware baselines, showing its effectiveness in aligning RL optimization with BoN inference in code generation tasks."}, "weaknesses": {"value": "- This continuous max@k optimization method is a direct extension of the previous works on binary reward setting. The novelty is limited.\n- The empirical results mainly focus on code generation tasks, and with only the Qwen2.5-coder-7B model. It is hard to conclude that the proposed method can generalize to other tasks and models.\n- The max@1 score compared with the baselines is not significantly improved which raises the question of whether the proposed method can improve the single-sample accuracy.\n- The results only demonstrate the effectiveness of the off-policy BoN objective in max@k while k=128. It lacks the understanding of the performance with different k values."}, "questions": {"value": "- The experiments are limited to code generation tasks using only the Qwen2.5-Coder-7B model. Can you provide results on other domains (e.g., mathematical reasoning) or with different model architectures to validate its generalization？\n- The improvement on the max@1 metric compared to baselines is not significant. Does your method inherently trade off single-sample accuracy for better BoN performance, or can it be tuned to improve both?\n- The experiments focus on k=128 for evaluating the off-policy BoN objective. Could you provide results or analysis for different values of k (e.g., 256, 512) to better understand how the method scales with the number of samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2y0eHi3sdc", "forum": "BmkOKYfbmV", "replyto": "BmkOKYfbmV", "signatures": ["ICLR.cc/2026/Conference/Submission12526/Reviewer_THfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12526/Reviewer_THfK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981311949, "cdate": 1761981311949, "tmdate": 1762923390896, "mdate": 1762923390896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Prior work derived an on-policy gradient estimator for max@k. This work derives an off-policy version, and gives experimental evidence that this can improve max@1 scores (and can remain competitive for max@k) across five RL with Verifiable Rewards (RLVR) coding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Simple\n\nClear with few exceptions (noted in weaknesses)\n\nI am not familiar with the related works, but I believe the off-policy gradient estimator is novel\n\nFour reasonable baseline algorithms in main results\n\nIncludes both code snippets in the paper, and full code in the supplementary materials"}, "weaknesses": {"value": "The abstract says \"We derive an unbiased on-policy gradient estimate for direct optimization of this metric.\" But my understanding is that exact (on-policy) estimator was already derived in prior work (as noted at the start of section 3.2).\n\nThe bulleted list of contributions at the end of Section 1 also repeats that claim, but again I think it maybe shouldn't.\n\nThat same list of contributions includes \"We show that optimization of continuous reward is crucial for successful RLVR application\", but I think that claim should be worded a lot less strongly.\n\nA big weakness is the `clamp_delta` hyperparameter. It's great that the code snippet is included, but it's bad that this hyperparameter is not mentioned anywhere else in the paper, and it also seems a potentially big downside of the algorithm.\n\nDoesn't explain why those particular hyperparameters in Section 4.4 were chosen. I searched quickly just now, and a learning rate of 5e-6 is a common default, and it looks like the other hyperparameters might likewise be common defaults, but I think this should probably be mentioned in the paper."}, "questions": {"value": "> Yue et al. (2025) investigate the effect of RLVR finetuning on math, coding, and visual reasoning benchmarks. They show that while RLVR improves pass@1 performance, the base model achieves higher pass@k scores for larger k.\n\nIf I understand this right, Yue et al. find more intuitive results for pass@1 than the maybe counterintuitive binary reward results found in this submission's Table 1? Why might that be? It might be helpful to note this contrast.\n\n&nbsp;\n\n> The policy gradient family of RL algorithms became standard in LLM post-training. Introduced by Stiennon et al. (2020), the key idea of these approaches is to consider the language model as a policy and train it with an RL algorithm.\n\nWould it be fair to say Ranzato et al. ([https://arxiv.org/abs/1511.06732](https://arxiv.org/abs/1511.06732)) or Ziegler et al. ([https://arxiv.org/abs/1909.08593](https://arxiv.org/abs/1909.08593)) introduced policy gradient training of LLMs, rather than Stiennon et al.?\n\n&nbsp;\n\n> However, BoN can be too computationally expensive, and several works addressed this issue. A range of works address this limitation by fine-tuning the model to directly mimic the BoN distribution: [...]. This line of work is orthogonal to the present study since their inference strategy assumes generation of a single completion, as opposed to the generation of k completions employed here.\n\nJust how orthogonal are they? Could any of those works be modified to generate $k$ completions in an optimized way?\n\n&nbsp;\n\n> Another line of work explores model’s optimization to maximize performance with an account for inference-time techniques, like BoN.\n\nA bit ungrammatical\n\n&nbsp;\n\n> Applying the log-derivative trick\n\nThe $\\theta$ after this part runs into $\\pi$, it needs better spacing\n\nA similar issue happens for $\\theta$ and $\\rho$, later, in Section 3.3, after \"Similarly to on-policy scenario, we can calculate gradient of the objective\"\n\n&nbsp;\n\n> All other deltas appear in the sum $\\binom{i-3}{k-3}$\n\nShould that be $\\binom{j-3}{k-3}$?\n\n&nbsp;\n\n> Adam optimizer(Kingma & Ba, 2014)\n\nmissing space\n\n&nbsp;\n\n> with learning rate $5e – 6$\n\nAs rendered in the paper, the 5e-6 part looks like 5 times Euler's number minus 6\n\n&nbsp;\n\n$k$ vs k (math font vs plain text font) are used inconsistently to mean the same thing (throughout the paper)\n\n&nbsp;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2janH3ibRY", "forum": "BmkOKYfbmV", "replyto": "BmkOKYfbmV", "signatures": ["ICLR.cc/2026/Conference/Submission12526/Reviewer_5AKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12526/Reviewer_5AKJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135876277, "cdate": 1762135876277, "tmdate": 1762923390574, "mdate": 1762923390574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates fine tuning Large Language Models (LLM) with Reinforcement Learning to perform well on coding tasks. In particular, the paper focuses on best of N sampling at inference time, where N solutions are generated and a verifier selects the best performing solution to be returned.\n\nThe authors show that fine tuning an LLM with a policy gradient algorithm (GRPO) reduces the diversity of its generations, where performance of a single generation improves. But if we generate N solutions and evaluate the best one, fine tuning with RL hurts performance.\n\nThe paper uses a continuous reward function defined as the ratio of unit tests passed on a generated program by the LLM instead of a binary signal to check if all tests have passed. This leads to the max@K objective as the expected value of highest reward among the generations. The authors continue to derive an estimate of this objective in on-policy and off-policy settings, to be used with GRPO. Finally, experiments on 5 coding domains show that fine tuning an LLM with the proposed algorithm outperforms other baselines in 4 out of 5 domains under max@K scheme but does not improve performance under max@1 scheme."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper’s writing and organization is clear and coherent, I could mostly follow the motivation for the research and how this work fits with the covered prior work.\n2. The experiment section contains sufficient details regarding the methodology and evaluation scheme, the choice of hyperparameters, baselines, and statistical tests performed to evaluate the significance of the results.\n3. The derivations of the GRPO objective adapted to max@K in section 3 is clear and easy to follow.\n\n(I should point out that I am not an LLM expert, and therefore I do not say anything strong regarding the novelty or significance of this work to the LLM community but do raise a concern regarding the applicability of best of N sampling in the Weaknesses section.)"}, "weaknesses": {"value": "1. The authors interchangeably use K and N in the text. This should be unified and cleaned up.\n2. I did not understand the first experiment (Table 1). The authors claim optimization of continuous reward function is crucial for fine tuning LLMs with RL. While this intuitively makes sense to me as an RL expert, I missed how this experiment suggests such a claim.\n3. Perhaps the biggest shortcoming of this work, in my opinion, is conceptual:\n  - Optimizing max@K performance of the model does improve its performance under best of N sampling, since the optimization objective better matches the evaluation scheme but it may also increase the variance of generations making each output less reliable.\n - While this issue may not show up in benchmark performance under best of N sampling, it does raise concerns for the applicability of such model in the real world. Not every coding problem in the world comes with a suite of verifiable tests. And the additional cost of computation and verification would limit the usability of such a model.\n- Optimizing the performance of best of N generations changes the distribution of outputs such that given sufficiently large N, a very good solution is generated. This is an effort to produce a large set of generations that contain a good answer (like finding a needle in a haystack), rather than producing reliable confident answers (planning and reasoning to complete a difficult task).\n- It would be good to include a discussion regarding the variance of generations and the distribution of quality of generated solutions to see whether optimizing the max may harm the performance of other solutions."}, "questions": {"value": "Please respond to the raised issues in points (2) and (3) of the weaknesses section of this review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8WmuQOnugY", "forum": "BmkOKYfbmV", "replyto": "BmkOKYfbmV", "signatures": ["ICLR.cc/2026/Conference/Submission12526/Reviewer_A8Cg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12526/Reviewer_A8Cg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762388164912, "cdate": 1762388164912, "tmdate": 1762923389759, "mdate": 1762923389759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment"}, "comment": {"value": "We sincerely thank the reviewers for their thorough evaluation and feedback. All concerns have been carefully considered, and corresponding changes have been made to the paper. Detailed responses to each reviewer's questions are provided below. We believe these revisions address the feedback and strengthen the contribution without altering the method or conclusions.\n\nAdditionally we want to address concerns about the novelty of our work. While [Walder & Karkhanis 2025](https://arxiv.org/abs/2505.15201) provide derivation for pass@k (binary reward) and max@k (continuous reward) objectives, they don’t provide any real data experiments with it and report only with pass@k results. In our work, we show that their exact approach (with loo-1 regularisation) is suboptimal and doesn’t improve max@k for large k compared to the base model (table 2 row 1 vs row 4). Additionally, we show that a change to the variance reduction can improve its performance (bon mean in table 2). And on top of that, switching from on-policy estimate to off-policy estimate improves it further. These results highlight meaningful extensions beyond prior work."}}, "id": "ITAAVjkPBe", "forum": "BmkOKYfbmV", "replyto": "BmkOKYfbmV", "signatures": ["ICLR.cc/2026/Conference/Submission12526/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12526/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12526/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763666184688, "cdate": 1763666184688, "tmdate": 1763666184688, "mdate": 1763666184688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}