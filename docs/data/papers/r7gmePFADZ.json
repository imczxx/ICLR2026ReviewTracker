{"id": "r7gmePFADZ", "number": 2372, "cdate": 1757065296626, "mdate": 1759898152655, "content": {"title": "SmartDS-Solver: Agentic AI for Vertical Domain Problem Solving in Data Science", "abstract": "We present SmartDS-Solver, a hierarchical multi-agent framework for adaptive and cost-efficient automation of data science workflows. While LLM-based agents show potential, current multi-agent systems suffer from fragile task coherence, excessive reliance on prompt-based interactions, and a tendency toward task silos. Pursuing improved performance through multi-turn exchanges is often misguided, as accuracy gains are uncertain and frequent API calls further drive up token costs. SmartDS-Solver addresses these challenges through: (i) a domain-specific reasoning LLM, trained via structured methodological distillation and two-stage GRPO fine-tuning, to enable robust end-to-end code generation; and (ii) a hierarchical Agentic AI architecture with task-decoupled multi-agent collaboration, in which a meta-learning agent dynamically adjusts expert model decoding strategies based on feedback from the code agent—thereby unlocking the full potential of the domain reasoning LLM and improving both the success rate and quality of data-science code generation. Evaluated on 11 MLE-Bench tasks, SmartDS-Solver attains an 81.8% win rate over the AIDE+o1-preview baseline while substantially reducing computational overhead. In additional real-world task evaluations, our method—using only a 7B model—successfully generated 100% executable code, whereas AIDE+o1-preview failed on one task.", "tldr": "", "keywords": ["Agentic AI", "Auto ML", "Hyperparameter Tuning", "NLP", "Code Generation from Natural Language", "Planning with Language Models"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bf25cf5f7e55cb50832df87887cec664dab5c4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to improve the capabilities of automated data science for LLM agents. Specifically, this paper proposes SmartDS-Solver, which consists of (1) domain-specific finetuning; two-stage GRPO finetuning; (3) Hierarchical agent framework. Extensive experiments on 11 MLE-Bench tasks demonstrate the effectiveness of the proposed SmartDS-Solver."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The investigated research problem is interesting and of significance."}, "weaknesses": {"value": "- The writing is poor. It is hard for me to follow. I suggest the authors carefully revising the paper to meet the basic bar of academic writing. Also, Introduction is important. The current manuscript is lack of this part. The Background and Motivation section of this paper is plain and ambiguous. \n\n- How to compose the data used for finetuning? What does the data look like? What loss function is utilized for finetuning. For the RLFT, what is the interactive environment? I cannot figure out the techniques in this paper.\n\n- Lack of comparison of SOTA data science agents, such as [1].\n\n[1] MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement, NeurIPS 2025."}, "questions": {"value": "I think this paper is poor in writing quality. The revision for this paper should undergo a new round of review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nG1igRYsRV", "forum": "r7gmePFADZ", "replyto": "r7gmePFADZ", "signatures": ["ICLR.cc/2026/Conference/Submission2372/Reviewer_aEad"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2372/Reviewer_aEad"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714895318, "cdate": 1760714895318, "tmdate": 1762916212742, "mdate": 1762916212742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a hierarchical multi-agent framework (called SmartDS-Solver) , which efficiently automates data science workflows. SmartDS-Solver proposes a specialized reasoning LLM and a task-decoupled agent architecture to tackle with three key challenges: fragile task coherence, excessive reliance on prompt-based interactions, and a tendency toward task silos. Experiments on 11 MLE-Bench tasks show an 81.8% win rate over baselines while reducing overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• Developing LLM-driven agents for automating end-to-end data science pipelines is interesting and can augment human analysts."}, "weaknesses": {"value": "• The proposed method is complicated, which involves a multi-stage pipeline with multiple complex components. This paper lacks sufficient implementation details to reimplement the SmartDS-Solver. \n\n• The proposed SmartDS-Solver architecture is incremental, which combines multiple existing techniques (e.g., meta-learning, data augmentation, SFT, RL with GRPO).\n\n• As the SmartDS-Solver architecture consists of multiple complex components, it would be better to analyse the computational cost compared to baselines. \n\n• To thoroughly evaluate the performance of the proposed method, more advanced automated data science systems should be included as comparison baselines, such as “Data Interpreter: An LLM Agent for Data Science”."}, "questions": {"value": "• The proposed method is complicated, which involves a multi-stage pipeline with multiple complex components. This paper lacks sufficient implementation details to reimplement the SmartDS-Solver. \n• As the SmartDS-Solver architecture consists of multiple complex components. It would be better to analyse the computational cost compared to baselines. \n• To thoroughly evaluate the performance of the proposed method, more advanced automated data science systems should be included as comparison baselines, such as “Data Interpreter: An LLM Agent for Data Science”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OqUgFXIUlM", "forum": "r7gmePFADZ", "replyto": "r7gmePFADZ", "signatures": ["ICLR.cc/2026/Conference/Submission2372/Reviewer_2DKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2372/Reviewer_2DKn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375328769, "cdate": 1761375328769, "tmdate": 1762916212562, "mdate": 1762916212562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SmartDS-Solver, a hierarchical multi-agent framework for automating data science workflows. The system combines a domain-specific reasoning LLM (trained via structured distillation and GRPO fine-tuning) with a meta-learning agent (SARTE) that dynamically adjusts decoding parameters. Evaluated on MLE-Bench tasks, the system achieves an 81.8% win rate over AIDE+o1-preview while reducing computational costs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses real limitations in current AutoML agents - high costs, fragile multi-agent interactions, and excessive reliance on expensive models.\n2. Comprehensive training methodology. The three-stage training pipeline (SFT → GRPO1 → GRPO2) with carefully designed reward functions is well-documented and appears reproducible.\n3. SARTE's dynamic temperature adjustment based on execution feedback is creative and shows meaningful performance gains (+3.9% accuracy, -12% error rate).\n4. Testing across 11 MLE-Bench tasks, 3 real-world tasks, and 18 AutoML-Agent benchmark tasks demonstrates broad applicability.\n5. Significant reduction in token consumption compared to AIDE+o1-preview (e.g., ~90% reduction in inference tokens) while maintaining competitive performance.\n6. 81.8% win rate with Qwen-32B-GRPO2 and 100% executable code generation on real-world tasks."}, "weaknesses": {"value": "1. Limited baseline comparisons: The paper primarily compares against AIDE+o1-preview and AutoML-Agent. Missing comparisons with other recent systems like Agent-K, SELA (only shown in AutoML-Agent table), or AutoGluon on the primary benchmark would strengthen claims.\nIncomplete reproducibility details:\n\n2. Hardware requirements not fully specified (only \"2 NVIDIA H100 GPUs\" mentioned)\nTraining time and convergence details missing\nHyperparameter selection methodology for SARTE not clearly explained\nHow were the 11 MLE-Bench tasks selected from the 75 available?\n\n3. Sample size is relatively small (11-14 tasks for main comparisons). Statistical significance testing only appears in appendix (Table A11). Some results show marginal significance (p=0.0995 for Qwen-7B).\n\n4. All experiments are in data science domain - claims about \"vertical domain\" applicability need validation. Temperature sensitivity analysis (Table 3) shows high variance across tasks - unclear how SARTE would perform in completely new domains. The 7B model shows notably lower performance, questioning scalability to resource-constrained settings.\n\n5. No ablation on individual GRPO stages (SFT+GRPO1 vs SFT+GRPO2). Limited analysis of reward function components ($\\alpha, \\beta, \\gama$ weights). Code Agent's \"minimally invasive patching\" not empirically validated separately\n\nPresentation issues:\n\n1. Figure 1 is dense and difficult to parse. Some notation inconsistencies (e.g., \"RLM\" vs \"reasoning LLM\"). The distinction between GRPO1 and GRPO2 training objectives could be clearer\n\n2. The composite reward function (Equation 2) has fixed weights - no justification or sensitivity analysis provided. SARTE's boundary-aware step-size control has multiple hyperparameters (line 16-18 in Algorithm 1) with unclear tuning process. The \"semantic similarity\" threshold for early stopping not specified.\n\nExperimental design:\n\n1. Different models trained to different stages (72B/70B only SFT) makes fair comparison difficult. AIDE configuration uses 20 steps uniformly - no exploration of whether fewer steps would be sufficient. Real-world tasks (Section 4, Table 1) show one failure for AIDE but unclear if this is representative.\n\nData concerns:\n\nTraining data construction relies heavily on DeepSeek R1 for augmentation - potential bias inheritance\nQuality filtering uses Gemma3-27B scoring - criteria not validated\nCode4ML and cell2doc datasets are relatively old (2023-2024)\n\nSpecific Technical Issues\n\n1. SARTE algorithm: The control factor computation (Algorithm 1, lines 4-11) uses different formulas for success/failure/no-code cases, but the rationale for these specific functional forms is not provided. Why piecewise nonlinear for success but linear penalty for failure?\n2. Reward function design: Equation 3 uses \"Aggregate\" function that's not defined until Appendix (Table A3). The weighting scheme between feature/algorithm/metric dimensions is not justified."}, "questions": {"value": "1. How does SmartDS-Solver perform on tasks outside data science? Even a preliminary experiment in one other domain would strengthen generalization claims.\n2. Can authors provide ablation studies isolating the contribution of each training stage (SFT, GRPO1, GRPO2)?\n3. What is the sensitivity of performance to the reward function weights (α=0.5, β=0.25, γ=0.25)?\n4. How were the specific functional forms in SARTE's control factor (Algorithm 1, Equations on lines 4-11) derived?\n5. Can authors clarify the task selection process for the 11 MLE-Bench tasks used in primary evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fQyaFZEw0L", "forum": "r7gmePFADZ", "replyto": "r7gmePFADZ", "signatures": ["ICLR.cc/2026/Conference/Submission2372/Reviewer_J3ei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2372/Reviewer_J3ei"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773078681, "cdate": 1761773078681, "tmdate": 1762916212193, "mdate": 1762916212193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a solid and well-executed system for automated data science with impressive empirical results (81.8% win rate, 93% token reduction) and exceptional implementation details rarely seen in current LLM research. However, the writing quality is surprisingly poor for ICLR. The paper structure is severely imbalanced: Introduction (1 page) is too brief, Methodology (5 pages) is overly detailed, and Experiments (2 pages) lacks depth, with most critical results buried in the appendix. The presentation reads more like a technical report than an academic paper, with confusing organization and crude figures. If the authors can substantially restructure the paper—expanding the introduction and experimental analysis while condensing the methodology—I would be willing to raise my score. The core contributions are valuable, but they are currently obscured by poor presentation."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong empirical results with solid methodology.\nThe paper achieves an 81.8% win rate against AIDE+o1-preview on MLE-Bench while reducing token consumption by 93%, demonstrating an excellent cost-performance trade-off. Extensive experiments across 32 tasks and 20 configurations validate the method's effectiveness and robustness.\n2. Exceptional implementation details and reproducibility.\nThe 27-page appendix provides complete prompt templates, pseudocode, data construction pipelines, and hyperparameter settings, which is rare in current LLM research. The authors demonstrate strong engineering commitment, facilitating reproduction and future improvements.\n3. Methodological innovation in training data construction.\nThe proposed three-component framework (Full Workflow + Decision Logic + Adjustment Trail) encodes agentic reasoning capabilities into training samples, going beyond existing work that only provides code+comments. Quality control uses a dual-layer mechanism (format checking + semantic alignment + Gemma3 scoring) to ensure data reliability.\n4. Clever and practical SARTE mechanism design.\nThe approach models hyperparameter tuning as an online learning problem with O(1) space complexity (depending only on previous-step feedback) without requiring model retraining. The boundary-aware update strategy incorporates physical intuition, and Table 3 demonstrates that optimal temperatures vary dramatically across tasks/models (range up to 0.58), validating the necessity of dynamic adjustment."}, "weaknesses": {"value": "1. Severely imbalanced paper structure, failing to meet academic standards.\nThe Introduction spans only 1 page with insufficient background and motivation—readers cannot understand why existing methods are inadequate. The methodology occupies 5 pages (54% of content) while experiments and conclusions take only 2 pages, lacking in-depth analysis and insights. The overall presentation reads like a technical report rather than an academic paper.\n2. Confusing organization in the methodology section, poor readability.\nSection 3 mixes training (3.2), inference (3.3), and code execution (3.4) into a single section when these should be separate chapters for clarity. Key innovations (e.g., the SARTE algorithm) are buried in implementation details, making it difficult for readers to quickly grasp the core contributions.\n3. Insufficient experimental content in the main text; most experiments relegated to appendix with minimal analysis.\nThe main text contains only Figure 3 and Tables 1-2, while critical detailed results (Table A5 with full configuration comparisons, Table A8 with token consumption breakdown, Table A10 with temperature analysis) are all in the appendix. The main text completely lacks error analysis, failure case discussions, or deep investigation into why the method works—it merely stops at \"proving the method works.\"\n4. Poor figure quality with crude presentation.\nFigure 3(b) appears as an unfinished draft with box plots missing legend explanations. Figure 1 is overly complex with too much text, making the system architecture hard to grasp quickly."}, "questions": {"value": "The paper's detail is enough, the most important problem is the writing is terrible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7zO6COCTmO", "forum": "r7gmePFADZ", "replyto": "r7gmePFADZ", "signatures": ["ICLR.cc/2026/Conference/Submission2372/Reviewer_qpDP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2372/Reviewer_qpDP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883702124, "cdate": 1761883702124, "tmdate": 1762916211509, "mdate": 1762916211509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}