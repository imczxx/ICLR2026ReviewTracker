{"id": "1tLzCkxVfC", "number": 17408, "cdate": 1758275676519, "mdate": 1759897177089, "content": {"title": "Learning Quantifiable Visual Explanations Without Ground-Truth", "abstract": "Explainable AI (XAI) techniques are increasingly important for the validation and responsible use of modern deep learning models, but are difficult to evaluate due to the lack of good ground-truth to compare against. We propose a framework that serves as a quantifiable metric for the quality of XAI methods, based on continuous input perturbation. Our metric formally considers the sufficiency and necessity of the attributed information to the model's decision-making, and we illustrate a range of cases where it aligns better with human intuitions of explanation quality than do existing metrics.\n\nTo exploit the properties of this metric, we also propose a novel XAI method, considering the case where we fine-tune a model using a differentiable approximation of the metric as a supervision signal. The result is an adapter module that can be trained on top of any black-box model to output causal explanations of the model's decision process, without degrading model performance. We show that the explanations generated by this method outperform those of competing XAI techniques according to a number of quantifiable metrics.", "tldr": "We propose a novel metric for evaluating the quality of explainable AI visualisations, and derive a novel explainable AI method to learn annotation-free explanations.", "keywords": ["Explainable AI", "Computer Vision", "XAI Metrics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2fad604aaac6144d88314d90bde6641ce4a6f2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Minimality-Sufficiency Integration (MSI), a new metric for evaluating visual explanations without ground-truth saliency annotations, and Learnable Adapter eXplanation (LAX), a self-supervised module trained to generate compact, sufficient saliency maps that optimize MSI directly.\nMSI integrates sufficiency (the explanation retains information needed for correct prediction) and minimality (it avoids redundant regions), and tolerates multiple valid explanations. \nLAX, inspired by the Information Bottleneck framework, learns an explanation mask on top of a frozen backbone without altering predictive performance.\nExperiments on Synthetic-MNIST, CUB-200, and CIFAR-10 show that LAX outperforms Grad-CAM variants on both traditional fidelity metrics and the proposed MSI score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper directly tackles a major challenge in explainability (evaluating without ground truth) by introducing a metric grounded in sufficiency and minimality.\n\n- The combination of a differentiable evaluation metric (MSI) and a compatible self-supervised explanation module (LAX) is conceptually elegant and generally applicable to pretrained models.\n\n- The paper is easy to follow, with thorough tables and visualizations demonstrating improved localization and focus over CAM-based baselines."}, "weaknesses": {"value": "- The paper only evaluates on image classification and only against CAM-style methods (Grad-CAM, Grad-CAM++, Layer-CAM, etc.). This omits important region-based attribution work such as XRAI (Kapishnikov et al., ICCV 2019), which also emphasizes compact, region-level explanations and introduces similar perturbation-based evaluation curves (AIC/SIC). The lack of direct comparison or discussion of XRAI weakens claims of novelty.\n\n- MSI’s integration of minimality and sufficiency is a thoughtful extension, but conceptually similar to XRAI’s “region-based sufficiency curves” and fidelity metrics. The theoretical innovation is modest, and the proposed IB connection remains heuristic.\n\n- LAX is trained to optimize MSI, and performance is then evaluated primarily with MSI. Without external or human evaluation, it’s unclear whether the explanations are genuinely better or merely optimized for the metric.\n\n- No experiments beyond vision classification, no transformer-based or multimodal examples, and no human-alignment studies. This limits the claim that MSI captures intuitive explanation quality."}, "questions": {"value": "- How does MSI relate quantitatively to XRAI’s AIC/SIC metrics?\n\n- Would LAX or MSI generalize to non-visual or transformer-based models?\n\n- Does MSI correlate with human perception of explanation quality or just model fidelity?\n\n- How does MSI behave under adversarial perturbations or for explanations of incorrect predictions?\n\n- How sensitive is MSI to the choice of α_min and integration bounds?\n\n- Would human raters’ preferences correlate with MSI scores?\n\n- How stable is the LAX module across different random seeds or model checkpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vaiKnZ1lRd", "forum": "1tLzCkxVfC", "replyto": "1tLzCkxVfC", "signatures": ["ICLR.cc/2026/Conference/Submission17408/Reviewer_Gsqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17408/Reviewer_Gsqk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574349856, "cdate": 1761574349856, "tmdate": 1762927306588, "mdate": 1762927306588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Minimality-Sufficiency Integration (MSI), a perturbation-based evaluation metric for saliency or relevance maps. MSI aims to unify some existing faithfulness metrics, combining different aspects of previous perturbation metrics such as prediction performance comparison between high and low importance regions, thresholded MoRF-Insertion and Deletion, and a penalty term for less-sparse explanations. The paper also introduces Learnable Adapter eXplanation (LAX), an auxiliary “explanation network” trained on top of a frozen classifier to generate saliency maps that optimize MSI scores. They evaluate the LAX method on Synthetic MNIST, CIFAR-10, and a CUB-200 subset, comparing LAX to Grad-CAM variants on a ResNet-18 architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Important Topic**: The evaluation of explanation methods in quantitative as well as theoretical terms is an important topic and helps improve trust in interpretability methods.\n- **Reasonable Goal**: Combining sufficiency and minimality in a single metric is a reasonable goal for a metric and addresses some shortcomings in existing metrics.\n- **Simple Metric Implementation**: The MSI appears to be simple to implement."}, "weaknesses": {"value": "- **Weak motivation and limited validation of MSI**: The paper does not convincingly demonstrate that MSI better captures attribution quality than existing metrics. MSI is essentially a composition of prior perturbation metrics with an additional sparsity penalty and a tunable selection threshold. To highlight its usefulness, the paper would need to provide more extensive empirical validation across multiple models, attribution methods, datasets, and hyperparameter regimes. The presented quantitative evidence (Table 2) is based on only four samples from a single ResNet-18 model using Grad-CAM and LAX, which is insufficient to draw general conclusions. Methods such as DeepLIFT, LRP or LIME are cited but never tested. The dependence of MSI on the hyperparameter alpha_min (the importance threshold) is especially underexplored. This parameter directly affects which regions are considered relevant and can behave inconsistently across attribution methods with different sparsity levels.\n- **Insufficient LAX Experiments**: The LAX adapter is evaluated only on Grad-CAM variants with a single ResNet-18 architecture. The hyperparameter alpha_min changes across datasets without justification, and it is unclear whether any train/test split is used. As a result, it is not possible to assess whether the explanations generalize beyond the training data. The reported results are insufficient to draw meaningful conclusions about the proposed method.\n- **Missing discussion of LAX Limitations**: Conceptual limitations of the LAX module are not addressed sufficiently. LAX introduces another model that learns to produce explanations for a frozen model optimizing for targets that are related to maximizing their introduced MSI metric. The paper does not analyze whether these explanations remain faithful or robust, especially for off-manifold or out-of-distribution inputs. Without such analysis, it is unclear whether LAX produces genuinely interpretable or merely overfitted saliency maps.\n- **Missing computational analysis**: The paper provides no discussion of the computational cost of computing MSI or training LAX. Without runtime or complexity analysis, it is difficult to assess whether the proposed method is practical compared to existing explanation methods."}, "questions": {"value": "- **Sensitivity of alpha_min**: How sensitive is MSI to the choice of this hyperparameter? Why is it varied across datasets? How stable are results across different attribution methods and model architectures?\n- **Attribution baselines**: Why are only Grad-CAM variants (Grad-CAM++, Layer-CAM, KPCA-CAM, Finer-CAM) considered?\n- **Model variety:** Why is the evaluation limited to a single ResNet-18 architecture? Have MSI or LAX been tested on other architectures such as ViTs?\n- **LAX generalization and off-manifold behavior**: Have you tested the adapter on out-of-distribution data or with synthetic ground-truth datasets such as FunnyBirds? Was a train/test split used? How does adapter size or architecture affect behavior?\n- **Computational cost**: How does the cost of computing MSI and training LAX compare to existing metrics or attribution methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YHd1cCdym4", "forum": "1tLzCkxVfC", "replyto": "1tLzCkxVfC", "signatures": ["ICLR.cc/2026/Conference/Submission17408/Reviewer_gg79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17408/Reviewer_gg79"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901055771, "cdate": 1761901055771, "tmdate": 1762927306097, "mdate": 1762927306097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles how to quantify the quality of visual explanations when there’s no ground-truth saliency to compare against. It introduces MSI (Minimality–Sufficiency Integration)—a perturbation-based metric inspired by the information bottleneck—that rewards explanations that are both specific (sufficient) and compact (minimal), by contrasting show/hide perturbation curves and integrating a mask-size penalty. In tandem, it proposes LAX (Learnable Adapter eXplanation), a lightweight adapter trained on top of a frozen model to generate saliency maps without ground-truth explanation labels. Experiments on Synthetic MNIST, CIFAR-10, and CUB-200 illustrate the metric and method working together, with tables and examples reported across these datasets. Finally, the paper outlines extensions to multimodal data and large transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Proposes MSI, a clean metric that combines sufficiency and compactness into a single criterion. By integrating the show/hide perturbation curves with a mask-size penalty, it’s better suited to handling cases with multiple plausible supporting regions.\n\nThe metric is more interesting than the method: MSI has real discussion value and, if adopted, could help reduce the inconsistencies often seen across insertion/deletion-style evaluation metrics.\n\nOn the training side, LAX is relatively lightweight: it does not require saliency annotations, can be attached to a frozen backbone, and is straightforward to deploy in practice.\n\nExperiments on synthetic MNIST, CIFAR, and CUB subsets are largely consistent with the paper’s claims: in multi-solution scenarios, MSI rankings align better with intuition."}, "weaknesses": {"value": "Hyperparameter sensitivity: MSI depends quite heavily on the choice of α_min and the mask penalty (with different values used for different datasets). Method rankings may change with these settings, and the paper does not provide a robustness analysis over these choices.\n\nPerturbation strategy under-specified: the choice of insertion/deletion “fill-in” or reconstruction strategy is not clearly fixed or ablated, even though it can significantly affect the curves and introduce distribution shift, which in turn affects MSI.\n\nLimited baselines: important and commonly used methods such as RISE, Score-CAM, Integrated Gradients, LRP, and SmoothGrad are missing, which weakens the generality of the empirical conclusions.\n\n“Black-box” claim is overstated: LAX relies on intermediate feature maps and additional forward passes, which is closer to a frozen white-box than a true API-only black-box setting."}, "questions": {"value": "Please see Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nYtSC3gDfh", "forum": "1tLzCkxVfC", "replyto": "1tLzCkxVfC", "signatures": ["ICLR.cc/2026/Conference/Submission17408/Reviewer_dzRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17408/Reviewer_dzRG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762422829449, "cdate": 1762422829449, "tmdate": 1762927305731, "mdate": 1762927305731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}