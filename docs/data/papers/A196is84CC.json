{"id": "A196is84CC", "number": 20615, "cdate": 1758308229420, "mdate": 1759896967935, "content": {"title": "KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution", "abstract": "The rapid progress of multimodal large language models (MLLMs) calls for more reliable evaluation protocols. Existing static benchmarks suffer from the potential risk of data contamination and saturation, leading to inflated or misleading performance evaluations. To address these issues, we first apply Graph formulation to represent a static or dynamic VQA sample. With the formulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic multimodal evaluation framework. KBE first analyzes the original static benchmark, then expands it by integrating multimodal knowledge, transforming the static benchmark into a controllable, dynamic evolving version. Crucially, KBE can both reconstruct questions by Re-selecting visual information in the original image and expand existing questions with external textual knowledge. It enables difficulty-controllable evaluation by adjusting the degree of question exploration. Extensive experiments demonstrate that KBE alleviates the risk of data contamination, data saturation, and provides a more comprehensive assessment of MLLM capabilities.", "tldr": "We propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic multimodal evaluation framework.", "keywords": ["Multimodal", "Dynamic Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50001c3fa0e8037d477aabea1258b2418ae81244.pdf", "supplementary_material": "/attachment/2de2b112051e20a1be17bd90c6d4ef889f440c77.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes KBE-DME, a dynamic multimodal evaluation framework that transforms static VQA benchmarks into evolving benchmarks through knowledge-enhanced graph formulations. The method addresses data contamination and saturation issues by dynamically generating new test questions via two strategies: re-selecting key knowledge triplets from existing multimodal knowledge and exploring external textual knowledge. Experiments on OK-VQA and A-OKVQA demonstrate controllable difficulty and effective evaluation of five MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The graph-based representation of VQA samples using multimodal knowledge triplets provides a structured and interpretable framework for dynamic benchmark evolution, enabling clear manipulation of question difficulty.\n2. The framework successfully mitigates data contamination and saturation by generating new test samples, with experimental results showing consistent performance degradation as difficulty increases across multiple models.\n3. Human evaluation confirms the generated VQA samples are reasonable (95% score), with correct triplets (96.8%) and strong alignment between questions and triplets (97.9%), ensuring reliability."}, "weaknesses": {"value": "1. The framework relies heavily on GPT-4o for triplet extraction and question generation, which may introduce biases and limit reproducibility if the external model changes or becomes unavailable.\n2. The paper lacks discussion and comparison of relevant work and baselines, such as Dyn-VQA and OmniSearch[1], which I believe should serve as a strong baseline for this paper.\n\n[1] Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent, ICLR 2025."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CegcbnC5NT", "forum": "A196is84CC", "replyto": "A196is84CC", "signatures": ["ICLR.cc/2026/Conference/Submission20615/Reviewer_u47u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20615/Reviewer_u47u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718573236, "cdate": 1761718573236, "tmdate": 1762934017757, "mdate": 1762934017757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “KBE-DME: Dynamic Multimodal Evaluation via Knowledge-Enhanced Benchmark Evolution” proposes a novel framework for dynamic evaluation of multimodal large language models (MLLMs). The authors argue that current static benchmarks for multimodal evaluation suffer from data contamination and data saturation, leading to unreliable assessments as models improve and possibly overfit to open benchmarks. To address these issues, the paper introduces KBE-DME, a graph-based dynamic benchmark evolution system that represents each Visual Question Answering (VQA) sample as a multimodal knowledge graph of visual and textual triplets. Using this formulation, KBE-DME dynamically evolves benchmarks through two mechanisms: (1) re-selection of key triplets from existing knowledge, and (2) exploration with external textual knowledge to generate new, more challenging and knowledge-enriched questions. This framework allows fine-grained, difficulty-controllable evaluation of MLLMs. The authors conduct experiments on OK-VQA and A-OKVQA benchmarks with five major MLLMs (GPT-4o, Gemini-2.5-pro, Claude, LLaVA-OV, and Qwen-2.5-VL), showing consistent performance degradation as task difficulty increases, thereby demonstrating controllable difficulty and reduced contamination risk. Statistical and human evaluation results further confirm that the dynamically generated questions are diverse, reasonable, and well-aligned with their knowledge bases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tOriginality: The paper introduces an original and conceptually elegant reformulation of multimodal evaluation as a dynamic, knowledge-evolving process rather than relying on static datasets.\n2.\tTechnical Innovation: It proposes a novel graph-based representation of VQA tasks combined with triplet re-selection and external knowledge exploration, offering a principled framework (KBE-DME) to mitigate benchmark contamination and saturation.\n3.\tExperimental Rigor: The work includes comprehensive experiments across multiple benchmarks and MLLMs, supported by human evaluations that confirm the quality and validity of the generated data.\n4.\tClarity and Organization: The paper is clearly written and well-structured, with high-quality figures and explanations that make a complex methodology easy to follow.\n5.\tSignificance: The proposed approach addresses pressing issues in multimodal evaluation and provides a generalizable, sustainable direction for trustworthy assessment of MLLMs."}, "weaknesses": {"value": "1.\tInsufficient Baseline Comparison: Comparisons are mainly conducted against perturbation-based dynamic evaluation methods; the study would be stronger if it incorporated generation-based dynamic evaluation baselines such as DyVal, NPHardEval, and MPA (Meta Probing Agents), as well as multimodal counterparts like VLB.\n2.\tScalability and Cost: The study does not provide a thorough analysis of the framework’s computational cost, scalability, or feasibility when large LLMs are not accessible.\n3.\tReproducibility Concerns: The paper would benefit from more discussion or resources (e.g., code, datasets, or prompts) to enhance reproducibility and practical adoption by the broader research community."}, "questions": {"value": "The framework measures question difficulty by the number of edges (|Eₖ|) in the key subgraph. Could the authors provide additional quantitative evidence that this metric correlates with human-perceived or model-measured difficulty (e.g., via response latency, uncertainty, or accuracy decay rate)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C0elvXJHdt", "forum": "A196is84CC", "replyto": "A196is84CC", "signatures": ["ICLR.cc/2026/Conference/Submission20615/Reviewer_w3NL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20615/Reviewer_w3NL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906766877, "cdate": 1761906766877, "tmdate": 1762934017295, "mdate": 1762934017295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Knowledge-Enhanced Benchmark Evolution (KBE), a dynamic multimodal evaluation framework for multimodal large language models (MLLMs). Traditional static benchmarks often face data contamination and performance saturation, which lead to unreliable evaluations. KBE addresses these issues by representing each Visual Question Answering (VQA) sample as a graph of multimodal knowledge triplets and dynamically evolving the benchmark through two processes: re-selecting visual information from the original image and expanding questions with external textual knowledge. This enables controllable task difficulty and continuous benchmark evolution as models improve. Experiments show that KBE effectively reduces contamination and saturation, providing a more adaptive and comprehensive evaluation protocol for MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Structured Representation: The graph-based formalization of VQA samples into triplets (s, r, o) is clean and conceptually useful. It provides an interpretable framework that could, in principle, be extended to other modalities.\n\n2) Comprehensive Experimentation: Testing across multiple strong MLLMs (GPT-4o, Gemini, Claude, Qwen, LLaVA) demonstrates awareness of model diversity. The observed monotonic accuracy drop supports the intuition that the generated samples are more complex.\n\n3) High Quality Human Evaluation: The human study shows that the majority of generated samples are reasonable and consistent, lending some empirical support to data quality."}, "weaknesses": {"value": "1) Overreliance on Proprietary LLMs: The framework’s key processes—triplet extraction, filtering, generation, and even evaluation—depend entirely on GPT-4o. This makes the method difficult to reproduce and raises uncertainty about whether the improvements stem from the proposed framework itself or from GPT’s latent knowledge and linguistic priors.\n\n2) Limited Novelty in Practice: The idea of dynamically generating evaluation samples has already been explored in prior works (e.g., DyVal, VLB). The paper’s additions—graph formulation and triplet-based reasoning—feel more like presentation refinements than a fundamentally new evaluation paradigm.\n\n3) Evaluation Bias and Circularity: Using GPT-4o both to generate and to evaluate the benchmark introduces circularity and bias, as the system effectively evaluates itself. This undermines claims of objectivity and fairness in the reported results."}, "questions": {"value": "1) Overreliance on Proprietary LLMs: The entire KBE-DME framework depends heavily on GPT-4o for triplet extraction, filtering, question generation, and evaluation. Can the authors demonstrate that the approach is reproducible using open-source MLLMs, or explain how they ensure that the reported results are not simply a byproduct of GPT-4o’s internal knowledge and biases?\n\n2) Limited Novelty in Practice: Dynamic or adaptive evaluation frameworks, such as DyVal (Zhu et al., 2024) and VLB (Yang et al., 2025), have already addressed similar goals. Beyond the graph-based triplet formulation, what concrete innovation does KBE-DME introduce, and how does it outperform or differ meaningfully from these existing methods?\n\n3) Evaluation Bias and Circularity: Since GPT-4o is used both to generate dynamic benchmark data and to evaluate model responses, how do the authors ensure that the evaluation process is unbiased and independent? Have they validated the results using alternative evaluators or human assessments to avoid circularity and model-specific bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PWeEXzCvKj", "forum": "A196is84CC", "replyto": "A196is84CC", "signatures": ["ICLR.cc/2026/Conference/Submission20615/Reviewer_dVnQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20615/Reviewer_dVnQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961791692, "cdate": 1761961791692, "tmdate": 1762934016881, "mdate": 1762934016881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is well-regarded for its motivation in creating a benchmark for more reliable MLLM evaluation. The method is praised for the innovative modeling of queries as triplets and the ability to control difficulty, showing promise for inspiring future work. However, the notation could be defined more clearly in the first place. E.g., annotate corresponding notation in Figure 2 (and add more details if needed).\nTo further strengthen the evaluation, I suggest including results on the output token count from MLLMs, which would help quantify the effort needed to solve the generated queries."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-motivated. Such a benchmark can help evaluate MLLMs more reliably. The method is developed with the ability to control the difficulty of generated queries, showing its universality for various MLLMs. Modeling the VQA queries in the form of triplets can help inspire future work."}, "weaknesses": {"value": "1. The notation used in the paper could be more clearly defined. Could the authors please clarify the meaning of these symbols and specify what each subscript denotes?\n    - For visual knowledge triplets $M=\\{e_{m}\\}$, what exactly is $e_m$? If $e_m$ can be expressed in $(s,r,o)$, what do $s,r$ and $o$ refer to exactly? Can $s$ stand for an image, or are they all textual content? The same question for $T=\\{e_{t}\\}$.\n    - Line217. Does $m$ refer to triplet $(s,r,o)$? Why do we define $m$? Is $m$ ever used in later sections?\n\n2. The \"Extract\" operation relies on a powerful LLM where the LLM has to output certain formats, e.g., constructing a subgraph $K$, and correct visual and textual triplets."}, "questions": {"value": "1. Line216: subjects or objects -> subjects and objects? \"or\" means that the nodes can be either one of subjects and objects.\n2. Could the author add the results of how many tokens the MLLMs output to answer the newly generated queries to Table 2, to show the approximate efforts MLLMS made to solve the new queries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dq25GneasB", "forum": "A196is84CC", "replyto": "A196is84CC", "signatures": ["ICLR.cc/2026/Conference/Submission20615/Reviewer_D6Am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20615/Reviewer_D6Am"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050544680, "cdate": 1762050544680, "tmdate": 1762934016430, "mdate": 1762934016430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}