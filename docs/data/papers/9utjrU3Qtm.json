{"id": "9utjrU3Qtm", "number": 21196, "cdate": 1758314774453, "mdate": 1759896936113, "content": {"title": "D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation", "abstract": "We present a systematic exploration of the parameter-efficient fine-tuning design space under practical constraints, yielding D$^{2}$-LoRA---a method that reaches 76.4\\% average accuracy on eight QA/RC benchmarks using only 5k training samples per task and two epochs, while retaining algebraic mergeability at inference with near-exact numerical equivalence. D$^{2}$-LoRA combines a differential signed low-rank residual with a directional per-column normalization applied only during training. Specifically, given a frozen $W_0$, we learn two rank-$r$ components forming an update\n$\\Delta W=\\tfrac{\\alpha}{r}(A_+B_+-\\tau A_-B_-)$. This update is then projected onto the original column norms of\n$W_0$ to yield $W^\\star$, thereby allowing optimization to adjust directional components while preserving the original magnitude. At inference time, we merge $W^\\star$ and $\\Delta W$ into $\\widehat W$, which incurs no additional latency. Compared to baselines, D$^{2}$-LoRA achieves a +2.2pp macro improvement over LoRA (74.2\\%), and matches or exceeds DoRA. It also preserves numerical equivalence after merging (mean gap $\\approx 0.03$pp; worst $0.7$pp), while restoring $\\sim1.91\\times$ evaluation throughput. A geometric analysis explains why projection stabilizes low-rank training, and ablation studies isolate the effects of the negative branch, rank, target modules, scoring function, and fixed~$\\tau$.", "tldr": "", "keywords": ["parameter-efficient fine-tuning", "low-rank adapters", "directional normalization", "mergeability", "large language models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6293b1b1fd9cae2c5f367e832b9987863ad83ee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Based on LoRA, this paper introduces a negative branch and applies per-column normalization to the fine-tuned modules during training. The experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple and easy to implement.\n\n2. The experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. As shown in Equation (3), after normalization, the LoRA module is added one more time. The motivation for this operation is unclear.\n\n2. Although the authors provide three reasons, the rationale behind the different initializations of $A_+$ and $A_-$ remains somewhat unclear. Additional experimental or theoretical evidence would strengthen this argument. From my understanding, the concerns mentioned could potentially be addressed by properly setting $\\tau$.\n\n3. Line 193 claims that the proposed method leads to a smoother loss curve, but this claim is not elaborated on or demonstrated in the experimental section.\n\n4. The experimental comparison in this paper does not appear to be entirely fair. The proposed method introduces twice as many trainable parameters; therefore, the ranks of the baseline methods should be set to twice that of the proposed method for a fair comparison.\n\n5. The number of baseline methods used for comparison is somewhat limited, only two baselines (i.e., LoRA and DoRA).\n\n6. In the experiments, the Unmerged variant outperforms the Merged one in most cases, which makes the additional LoRA operation in Equation (3) rather confusing.\n\n7. An ablation study exploring different ranks for the $A_+$ and $A_-$."}, "questions": {"value": "1. In Table 4, it is unclear which baseline the reported speedup is compared against.\n\n2. There seems to be a typo in Table 6. The results for Minus off and Detach minus grad are exactly the same."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8GnNgoYfmS", "forum": "9utjrU3Qtm", "replyto": "9utjrU3Qtm", "signatures": ["ICLR.cc/2026/Conference/Submission21196/Reviewer_YLzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21196/Reviewer_YLzG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760693863552, "cdate": 1760693863552, "tmdate": 1762941603621, "mdate": 1762941603621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces D2-LoRA, a parameter-efficient fine-tuning (PEFT) method designed for budget-constrained scenarios. The method combines two key ideas: a signed low-rank residual, which provides both additive and subtractive update capabilities, and a train-time-only directional projection, which constrains the updated weights to maintain the column-wise magnitudes of the original pretrained model. This design aims to enhance model expressivity and training stability while preserving the crucial advantage of being fully mergeable at inference time, thereby incurring no additional latency. The authors provide theoretical motivation and empirical results on several question-answering benchmarks, demonstrating performance improvements over LoRA and DoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a well-motivated synthesis of concepts from LoRA and DoRA. The introduction of a signed residual to provide subtractive capacity is a logical extension, and the use of a directional projection only during training is a clever mechanism to gain stability benefits without sacrificing inference-time mergeability and efficiency.\n\n2. The work is explicitly framed around practical, budget-constrained fine-tuning, using a small number of training samples and epochs. The validation of near-exact merge equivalence and the measurement of post-merge throughput gains directly address the needs of real-world deployment, making the contributions highly relevant."}, "weaknesses": {"value": "1. The negative branch is modulated by a scalar α̃, which is a critical new hyperparameter. The main experiments fix α̃=0.5, but the ablation study (Table 7) shows that α̃=1.0 yields better results on one of the backbones. This suggests performance is sensitive to this choice.\n\n2. All eight evaluation benchmarks are question-answering or reading comprehension tasks that can be framed as multiple-choice classification. The method's effectiveness on more open-ended, generative tasks (e.g., summarization, dialogue, or long-form instruction following) remains unproven.\n\n3. D2-LoRA doubles the number of trainable parameters compared to LoRA and introduces additional computations (a second residual GEMM, column-norm calculation, and projection) during the training forward pass. The reported performance gains are modest in some cases.\n\n4. The ablation study on target modules (Table 5) only considers attention projections (q, k, v, o). It is common practice in PEFT to also evaluate the effect of adapting MLP layers (e.g., gate_proj, up_proj, down_proj), which is missing from the analysis.\n\n5. The paper provides a geometric intuition that the projection removes radial gradient components, which acts as an implicit regularizer. While plausible, this is not directly demonstrated with empirical evidence, such as by visualizing gradient distributions with and without the projection.\n\n6. How robust is the method to variations in this initialization? Does the training become unstable or fail to converge if a standard initialization is used for both branches?\n\n7. The paper states that setting the minus branch to zero yields a \"DoRA-like\" variant. This configuration is tested in the ablation study (Table 6, \"Minus off\"), but the connection is not made explicit in the table, which could improve clarity.\n\n8. The failure case analysis mentions that a strong negative branch (α̃=1.0) can \"over-correct\" on CommonsenseQA. This highlights a potential failure mode where the subtractive capacity could be detrimental."}, "questions": {"value": "1. How should practitioners approach tuning α̃ for new models or tasks? Is there a principled way to set it, or does it require a dedicated hyperparameter search, which would add to the tuning budget the paper aims to minimize?\n\n2. How does D2-LoRA perform on tasks that require creative or long-form generation? Does the directional constraint, which stabilizes discriminative tasks, potentially limit the model's generative diversity?\n\n3. Could you provide a more direct comparison of the training time overhead (e.g., wall-clock time per epoch) versus LoRA and DoRA? In what scenarios is the trade-off of higher training cost for the observed accuracy improvement most justified?\n\n4. Have you experimented with applying D2-LoRA to the MLP layers? How does this impact the parameter count and overall performance compared to adapting only the attention layers?\n\n5. Can you provide empirical evidence, such as plots of the norm of radial vs. tangential gradient components during training, to directly support the claim that the projection stabilizes optimization by removing radial modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Null."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ejLQ9bd6Pp", "forum": "9utjrU3Qtm", "replyto": "9utjrU3Qtm", "signatures": ["ICLR.cc/2026/Conference/Submission21196/Reviewer_qTF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21196/Reviewer_qTF7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412094017, "cdate": 1761412094017, "tmdate": 1762941603037, "mdate": 1762941603037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes D2-LoRA, a parameter-efficient fine-tuning method that combines a signed low-rank residual (positive and negative branches) with a training-time column-wise directional projection to preserve backbone weight norms. Under a strict budget (≤5k examples per task, ≤2 epochs) on eight QA/RC benchmarks and two backbones, it improves average accuracy over LoRA and often matches or exceeds DoRA. The adapter is algebraically mergeable at inference, yielding near-identical post-merge accuracy and about 2× evaluation throughput. A geometric analysis (norm preservation, Lipschitz control) and ablations over rank, targeted modules, the negative branch, and τ support the stability and effectiveness claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is straightforward to implement—combining a signed low-rank residual with a training-time directional projection—and remains mergeable at inference, imposing minimal engineering overhead within standard PEFT pipelines.\n- In low-data, small-rank settings, it consistently improves accuracy over baseline LoRA and stays competitive with related variants across multiple QA/RC tasks and backbones under tight training budgets, with ablations indicating robustness to rank and module choices."}, "weaknesses": {"value": "- Parameter-count fairness is not fully addressed: comparisons primarily match D²-LoRA at rank r against LoRA at the same r, without a parameter-matched LoRA (e.g., 2r) or equivalent-capacity baselines to isolate architectural benefits from increased parameterization.\n- The evaluation scope is limited to multiple-choice QA/RC benchmarks and does not assess open-ended generation, instruction following, code/math reasoning, or multilingual settings, which may exhibit different behaviors and trade-offs.\n- The theoretical analysis is mainly supportive of the method’s stability and expressivity and relies on established techniques, offering limited novelty beyond contextualizing the proposed projection and signed residual within known frameworks."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wndqTYyEyu", "forum": "9utjrU3Qtm", "replyto": "9utjrU3Qtm", "signatures": ["ICLR.cc/2026/Conference/Submission21196/Reviewer_BYCW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21196/Reviewer_BYCW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818005241, "cdate": 1761818005241, "tmdate": 1762941602064, "mdate": 1762941602064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a central challenge in the field of PEFT: the effective adaptation of large models under practical budget constraints, specifically in low-data and limited-compute regimes. The authors introduce D2-LoRA, a novel PEFT method built on two core architectural innovations designed to work in synergy. The first is a differential signed low-rank residual, which equips the model with both additive (feature reinforcing) and subtractive (feature suppressing) update capabilities, effectively doubling the expressivity to rank 2r. This increased expressivity is controlled by the second innovation: a train-time directional projection that normalizes updated weight columns to preserve their original magnitudes, providing crucial stability for training in low-data settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. D2-LoRA represents a novel synthesis of ideas. It intelligently combines an expressive signed residual—allowing the model to not only learn new features but also explicitly suppress pre-existing ones—with a stability-enhancing directional constraint. \n2. The authors provide detailed ablation studies that methodically dissect the architecture's performance. \n3. The evaluation is conducted with commendable rigor under a strict and realistic budget: a maximum of 5,000 training samples per task and only two epochs."}, "weaknesses": {"value": "1. While the focus on the low-data regime is well-motivated, the experiments are confined to QA/RC tasks. This is a reasonable scope, but the authors themselves note in Section 9 that evaluation on \"Broader modalities and RLHF-style pipelines are left for future work.\" \n2. The sensitivity of the τ hyperparameter, which balances the positive and negative branches, appears to be model-dependent. Data from Table 7 shows that the optimal value is 1.0 for Llama-3.2-3B-Instruct but 0.5 for Qwen2.5-7B-Instruct."}, "questions": {"value": "1. D2-LoRA introduces trade-offs that could be discussed more directly. The architecture doubles the number of trainable parameters compared to LoRA and introduces train-time computational overhead from \"a column-norm pass and one extra residual GEMM.\" The paper would benefit from a more explicit analysis of this performance-versus-cost trade-off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0hHjEWlUJM", "forum": "9utjrU3Qtm", "replyto": "9utjrU3Qtm", "signatures": ["ICLR.cc/2026/Conference/Submission21196/Reviewer_VjVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21196/Reviewer_VjVX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975979685, "cdate": 1761975979685, "tmdate": 1762941601463, "mdate": 1762941601463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}