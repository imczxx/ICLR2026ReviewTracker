{"id": "vpeBaP1hcs", "number": 14658, "cdate": 1758240901955, "mdate": 1759897356773, "content": {"title": "Towards Effective Model Editing for LLM Personalization", "abstract": "Customer-facing LLMs need personalization to reflect individual preferences and needs. However, existing personalization methods are often computationally expensive, data-intensive, prone to catastrophic forgetting, and degrade in multi-turn conversations and on implicit questions. To address these challenges, we conceptualize personalization as model editing and present Personalization Editing, a framework that applies localized edits guided by clustered preference representations, enforcing desired behavior where preferences apply while preserving other capabilities. Existing personalization datasets often use synthetic personas in role-playing dialogues, leading to indirect evaluation that does not reflect real-world user queries. We introduce UPQA, a short-answer QA dataset based on in-situ user queries, with varying levels of difficulty. Unlike prior benchmarks, UPQA directly tests whether models can recall and apply specific user preferences, enabling more accurate and efficient evaluation. Across settings, Personalization Editing improves editing accuracy, and is more computationally efficient than fine tuning, while outperforming prompting and retrieval based baselines in multi-turn conversations and on implicit preference questions.", "tldr": "", "keywords": ["Personalization", "Model Editing", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e21684b98e42c2192b33ba8e0d82becce38b352e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the use of existing model editing techniques as a method for LLM personalization. It contrasts this approach with the common paradigms of full fine-tuning (which is resource-intensive) and in-context learning/RAG (which can be unreliable, especially in multi-turn conversations).\n\nThe paper's primary contributions are:\n\n- A Benchmark: The introduction of UPQA (User Preference Question Answering), a new dataset for evaluating preference-following.\n\n- An Evaluation: A comparative study of existing model editing methods (e.g., ROME, LoRA, FT-M) on this new benchmark, showing they are more robust than prompting in multi-turn dialogues.\n\n- A Technique: A \"clustering-based preference representation\" is proposed, which acts as a form of data augmentation to help models generalize from explicit preferences (e.g., \"I like hiking\") to related implicit queries (e.g., \"What should I do this weekend?\")."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper clearly articulates a valid and important problem: the failure of prompting/RAG-based personalization in multi-turn conversations (as demonstrated in Figure 5) and the high cost of full fine-tuning.\n\n2. The paper conducts a thorough comparison of several representative model editing techniques (ROME, LoRA, FT-M) and baselines (Zero-Shot) on its new benchmark, providing a clear performance landscape.\n\n3. The problem of generalizing from stated preferences to implicit user needs is a key challenge for personalization, which is a good research direction."}, "weaknesses": {"value": "1. The paper's central concept, \"Personalization Editing,\" does not appear to be a novel algorithm. Rather, it is a re-application of existing model editing techniques (LoRA, FT-M, ROME) to the problem of personalization. The primary technical contribution, \"clustering-based preference representation,\" is a data augmentation strategy, not a new editing mechanism. This frames the paper more as an evaluation study than a significant methodological advancement.\n\n2. The paper's main contribution, the UPQA dataset, has several weaknesses that question its solidity:\n\n- \"In-Situ\" Claims are Misleading: The paper repeatedly describes the queries as \"in-situ\" (Abstract, Sec 4), which implies collection from real-world user interactions. However, the methodology (Sec 4.1, Appendix C) clearly states the dataset was synthetically generated by an LLM (Claude-Sonnet-4), which was prompted to create questions based on another synthetic dataset (Jandaghi et al., 2023). This is the opposite of \"in-situ\" and casts doubt on the benchmark's realism.\n\n- Oversimplified and \"Toy\" Queries: The examples provided (e.g., \"What's my hobby?\") appear overly simplistic and directly tied to the synthetic persona attribute. It is questionable whether these queries reflect the complexity, ambiguity, and contextual richness of real user-preference queries.\n\n- LLM-Generated Benchmark Evaluated by an LLM: There is a risk of a \"self-fullfilling prophecy\" when a benchmark generated by one LLM (Claude-Sonnet-4) is then evaluated by another LLM from the same family (Claude-4-Sonnet, as per Sec 5.2). This circular, synthetic methodology may not be a reliable measure of performance on genuine human-generated text.\n\n3. A fundamental claim of any \"model editing\" paper is locality—that the edit does not break unrelated knowledge (i.e., it avoids catastrophic forgetting). The objective function (Sec 3.4) explicitly includes this constraint. However, the experiments (Section 5) completely lack any evaluation of locality. They measure if the edit worked (Efficacy) and if it generalized (Generalization Score), but not if it damaged the model's general capabilities (e.g., performance on a general benchmark like MMLU). This is a critical omission.\n\n4. The experiments appear to test preferences one at a time. A real-world personalization system must handle hundreds of preferences added sequentially, including preferences that conflict or evolve over time (e.g., \"I am vegan\" followed later by \"I love cheese pizza\"). The paper provides no analysis of how \"Personalization Editing\" scales or handles such conflicts, which are central challenges in the field."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDqFcpxVG8", "forum": "vpeBaP1hcs", "replyto": "vpeBaP1hcs", "signatures": ["ICLR.cc/2026/Conference/Submission14658/Reviewer_Artt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14658/Reviewer_Artt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649291655, "cdate": 1761649291655, "tmdate": 1762925029840, "mdate": 1762925029840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work frames personalization as model editing, proposing Personalization Editing with clustering-based preference representations and introducing UPQA, a short-answer benchmark. It achieves efficient, robust personalization than prompting/fine-tuning across multiple LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ This work frames personalization as model editing, enabling efficient and persistent parameter-level updates that outperform prompting in multi-turn and implicit-query settings. \n+ This work introduces UPQA, a realistic short-answer benchmark with explicit, rephrased, implicit, and recommendation queries plus synonym clusters for robust evaluation. \n+ This work proposes clustering-based preference representations that generalize to paraphrases and implicit requests, improving accuracy and multi-model robustness."}, "weaknesses": {"value": "- Personalization metric relies on short-answer accuracy, failing to capture nuanced preferences, trade-offs, or long-term satisfaction across diverse tasks and interactions. \n- No human evaluation prevents measuring perceived relevance, satisfaction, or harms from personalized edits in real users. \n- Focus on short answers limits applicability to open-ended tasks, multi-turn dialogues, or creative personalization needs—unclear generalization to longer responses. \n- Parameter edits may cause unintended behavior, overfitting, or harmful side effects; robustness, rollback, and safety under adversarial preferences are underexplored."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vMOp8zt1uh", "forum": "vpeBaP1hcs", "replyto": "vpeBaP1hcs", "signatures": ["ICLR.cc/2026/Conference/Submission14658/Reviewer_Bwcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14658/Reviewer_Bwcd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839932180, "cdate": 1761839932180, "tmdate": 1762925028830, "mdate": 1762925028830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a novel Personalization Editing framework to better reflect user demand. The authors also introduced a short-answer UPQA for evaluating Personalization Editing. The proposed framework demonstrates strong and robust editing performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed Personalization Editing is an interesting and novel editing task.\n* The proposed framework demonstrated and robust strong performance on editing"}, "weaknesses": {"value": "* The formatting in Section 3 looks a bit disorganized with few sentences in a subsection, I recommend that the authors summarize the problem setups in fewer subsections and elaborate a bit more on the objective function, say, what exactly is the loss function $\\mathcal{L}$?\n\n* The overall novelty of this work is limited, the theoretical and empirical insight may not benefit broader audience beyond this task."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "smmPJZKawm", "forum": "vpeBaP1hcs", "replyto": "vpeBaP1hcs", "signatures": ["ICLR.cc/2026/Conference/Submission14658/Reviewer_G3jE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14658/Reviewer_G3jE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985854343, "cdate": 1761985854343, "tmdate": 1762925028302, "mdate": 1762925028302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Personalization Editing, which novelly frames the LLM personalization problem as a model editing task. It then proposes clustering-based preference representation and uses it to extend existing model editing methods. The paper also presents a new dataset, UPQA, which can directly test personalization methods on user queries of various difficulty levels.\n\nExperiments show that model editing methods constantly outperform the zero-shot method, where user preference is incorporated directly into the model prompt. As the number of turns increases, the model editing methods are also more robust than the zero-shot method. Besides, the clustering-based preference representation improves some of the editing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel framework that conceptualizes LLM personalization as a model-editing task, so that the existing model-editing methods can be adopted to tackle the LLM personalization problem. \n\n2. The paper presents a new dataset, UPQA, together with clearly defined metrics, which can serve as a new benchmark for LLM personalization methods. \n\n3. The paper extensively experiments with existing model-editing methods and presents comprehensive experimental results."}, "weaknesses": {"value": "1. Although the paper presents definitions and high-level formulas for the Personalization Editing framework, such as input, output, objective, etc., it lacks sufficient details about how existing model-editing methods are used to be compatible with the framework and the evaluation datasets. The constrained objective is conceptually clear, but integration details per editor (e.g., layer selection, masking strategy) are not specified. \n\n2. The paper primarily augments existing editors, and no new low-level weight-update algorithm is introduced, which limits its originality. \n\n3. The paper uses Claude as the judge. While it shares the evaluation prompt, a validation of the prompt (e.g., human validation) is missing."}, "questions": {"value": "1. For the Experiments section, you are comparing a list of model-editing methods to the zero-shot baseline. Could you clarify what changes you made to these model-editing methods? Did you integrate any innovative improvements to them, or use them off-the-shelf?\n\n2. In Figures 2 and 3, we see that a few methods (such as LoRA and FT-M) already achieve 100% on almost every model and preference type.  Does this indicate that the proposed dataset (UPQA) is not difficult / diverse enough?\n\n3. As the Weakness part mentioned, could you share more implementation-level details about per-editor integration and clustering integration?\n\n4. Figure 6 shows that increasing cluster size negatively affects FT-L's performance on Deepseek-7B and Llama3-8B. Could you elaborate more on the effectiveness and generalizability of the clustering method?\n\n5. Two anonymous repo links are provided, but one is empty (only contains a README doc), and the other one has expired. Could you re-share the anonymous repo if available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z79MLEZOLu", "forum": "vpeBaP1hcs", "replyto": "vpeBaP1hcs", "signatures": ["ICLR.cc/2026/Conference/Submission14658/Reviewer_g3aB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14658/Reviewer_g3aB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762507943006, "cdate": 1762507943006, "tmdate": 1762925027813, "mdate": 1762925027813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}