{"id": "yKsIC2emJt", "number": 11013, "cdate": 1758186900696, "mdate": 1763106826015, "content": {"title": "Thinking is Seeing: Multi-modal Large Language Models are Exceptional in Understanding Knowledge Graphs", "abstract": "The representation learning of knowledge graphs (KGs) is a longstanding research problem. While graph neural networks (GNNs) have driven recent progress, they still struggle with encoding textual features and subtle relationships of KGs, particularly in conveying key information to large language models (LLMs). The emergence of multi-modal LLMs (MLLMs), which combine linguistic and visual understanding, presents an intriguing opportunity: Could their vision capabilities inspire mental visualization, facilitating conceptual thinking and abstract reasoning akin to human cognition? To investigate this premise, we propose SeeKG , an innovative framework that transforms KGs into visually rendered representations as image inputs for MLLMs. We evaluate SeeKG under both training-free and supervised fine-tuning settings, where the experimental results show that SeeKG excels in understanding KG sub-graphs and achieves competitive performance even without training or demonstrations. Further fine-tuning on small-batch data reveals that it outperforms state-of-the-art LLM-based KG completion methods by substantial margins across multiple benchmark datasets.", "tldr": "", "keywords": ["knowledge graph", "larege language model", "multi-modal large language model", "knowledge graph completion"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1cc3e92c564cd2c9d9d40888233982fae9399292.pdf", "supplementary_material": "/attachment/02897b4df166bba35bbb398061ab4f77a3df78ac.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose to transform knowledge graphs into visually rendered representations as image, and then input into MLLM for leveraging advanced vision capabilities. Leveraging visual understanding of knowledge graphs is an important and promising direction because of the inherent visual symbols and connections within them.\nThe authors propose SeeKG, and carry out training-free and supervised fine-tuning settings to evaluate performance. They conduct experiments on two small datasets (CoDeX-S and FB15K-237N) and compare with LLM-based methods and traditional embedding-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "--The direction explored in this paper is valuable.\n\n--The ablation study and case study are sufficient."}, "weaknesses": {"value": "Representation：\n\n--Understanding KGs through MLLM is not a new direction; this paper lacks elaboration and comparison with existing work.\n\n--The authors need to polish their paper writing and carefully declare their contributions.\n\nMethod：\n\n--The technical contributions are limited. Specifically, rendering knowledge graphs into images, using training-free or SFT-based method to evaluate MLLM, are not novel.\n\n--Compared to existing work, this paper does not contribute any new ideas or technologies.\n\nExperiment：\n\n--The experiments are insufficient. The authors conduct limited experiments on only two small synthetic datasets.\n\n--The comparison is unreasonable. The authors should compare their work with similar methods in the same field to demonstrate its true contribution.\n\nSome Reference:\n[NeuIPS 2025] Gita: Graph to visual and textual integration for vision-language graph reasoning\n[CVPR 2025] Mosaic of modalities: A comprehensive benchmark for multimodal graph learning"}, "questions": {"value": "Key questions:\n\n--Distinctions from existing works;\n\n--Novel perspectives offered in this paper compared to prior research;\n\n--Comprehensive experiments, including justified dataset and baseline selections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "byestQq7BJ", "forum": "yKsIC2emJt", "replyto": "yKsIC2emJt", "signatures": ["ICLR.cc/2026/Conference/Submission11013/Reviewer_QkkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11013/Reviewer_QkkW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738008830, "cdate": 1761738008830, "tmdate": 1762922194480, "mdate": 1762922194480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "jdKyuVn8yZ", "forum": "yKsIC2emJt", "replyto": "yKsIC2emJt", "signatures": ["ICLR.cc/2026/Conference/Submission11013/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11013/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763106824937, "cdate": 1763106824937, "tmdate": 1763106824937, "mdate": 1763106824937, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SeeKG, an MLLM-based framework for knowledge graph (KG) reasoning that leverages visualized sub-KG images as input to address the triple classification task. The framework supports various MLLMs, flexible sub-KG sampling strategies, and customizable visualization settings. Experimental results on KG triple classification (Table 1), alongside ablation studies evaluating key components (Table 2), demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper explores the use of visualized knowledge graph images and MLLMs to tackle the triple classification task in KG completion. The results presented in Table 1 demonstrate that multi-modal models are capable of effectively leveraging visual KG representations for this task. In addition, the authors conduct a comparative evaluation across different families of vision-language models, including both in-house and open-source variants, to further validate the robustness of their proposed setting."}, "weaknesses": {"value": "1. **Limited and unclear contribution**. The overall contribution of this paper remains unclear and appears limited. The scope is narrow, as the method only targets the triple classification task in KG completion. In addition, several claims are overstated or inaccurate. For example, the authors state that “_rather than adapting MLLMs for multi-modal KG tasks, we explore the correlation between the visual capacity of MLLMs with abstract thinking_” (line 141–142). However, the paper strictly evaluates models on triple classification, which primarily involves symbolic relational reasoning within a predefined knowledge graph. In contrast, abstract thinking generally requires higher-level conceptualization, analogical reasoning, and commonsense inference grounded in perceptual understanding. These capabilities that are not adequately assessed by the presented experiments. Therefore, this claim is not well supported by the evidence.\n\n2. **Ambiguous and unfair comparisons in Table 1**. The comparison in Table 1 is problematic.\n(1) Most baselines are LLM-based approaches, whereas SeeKG is multi-modal, yet this critical distinction is not clearly emphasized. The visual separation in the table is subtle and may lead readers to misinterpret SeeKG as an LLM-only method.\n(2) The baselines utilize different-scale backbones: SSQR is based on LLaMA-2 7B, while SeeKG uses Qwen2.5-VL-7B, which introduces additional visual capabilities and a different LLM backbone (Qwen2.5 Transformer). The fairness of the comparison is thus questionable.\n(3) In Table 2, the notation “w/” appears to be used incorrectly; it seems intended to indicate “w/o” but is currently ambiguous.\n\n3. **Questionable results in Figure 3**. I am skeptical about the results reported in Figure 3. It is unexpected that the zero-shot performance of Gemini 2.5 Pro is reported lower than that of Qwen2.5-VL-3B. Given that Gemini 2.5 Pro typically exhibits superior multimodal reasoning ability, this counterintuitive outcome requires further explanation or ablation to ensure the correctness of the evaluation setup.\n\n4. **Missing related work on graph-based multimodal reasoning**. The related work section is incomplete. Section 2 only discusses LLMs for KG reasoning, while recent studies have demonstrated strong LMM capability on visual-graph reasoning tasks. Relevant literature, such as VisionGraph [1], GITA [2], and GraphArena [3], should be discussed to better situate this work within the broader research landscape. Omitting these works weakens the novelty claim.\n\n5. **Uninformative case study analysis (Figure 5)**. The case study provided in Figure 5 is unclear and does not offer meaningful insight. A useful case study should highlight why the proposed model succeeds (or fails), revealing strengths and limitations of the approach. However, the example merely reports an example of complicated KG image and model's results. This prevents readers from understanding how SeeKG reasons or where it may struggle.\n\n6. **Unsupported claims regarding reasoning validity (Figure 1)**. Figure 1 showcases an example reasoning chain as output by an MLLM and claims that the model “sufficiently understands the graph context and integrates visual clues with its internal knowledge.” Yet, no experiments substantiate whether the intermediate reasoning content is reliable. Given that hallucinations remain a common issue in MLLMs, these claims require empirical validation. For example, the authors could analyze:\n(1) the correctness of generated reasoning vs. ground-truth facts,\n(2) cases where the reasoning text is incorrect but the final prediction is correct.\n\n[1] Li, Yunxin, et al. \"Visiongraph: Leveraging large multimodal models for graph theory problems in visual context.\" ICML 2024.\n[2] Wei, Yanbin, et al. \"Gita: Graph to visual and textual integration for vision-language graph reasoning.\" NeurIPS 2024.\n[3] Tang, Jianheng, et al. \"Grapharena: Evaluating and exploring large language models on graph computation.\" ICLR 2025."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1v9N7w9EFY", "forum": "yKsIC2emJt", "replyto": "yKsIC2emJt", "signatures": ["ICLR.cc/2026/Conference/Submission11013/Reviewer_61Mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11013/Reviewer_61Mg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792670345, "cdate": 1761792670345, "tmdate": 1762922194048, "mdate": 1762922194048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work propose SeeKG, which convert the KG into images (i.e., graph visualization), and wish to prove the benefits of such vision-based structure perception on KG completion. The experiments show it works on 2 datasets, however, some insights of its motivation and techniques it adopted are established, make its idea and technique novelty neural."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is easy to understand\n2. The motivation make sense, adopt this on KGC is valuable.\n3. Results on 2 datasets are promising"}, "weaknesses": {"value": "Idea Novelty: While the motivation behind the paper: Leveraging graph visualization to enhance graph structural comprehension and benefit from visual perception, such as substructure sensitivity is sound, this concept is not particularly new. Prior works, such as [1] and [2], have explored similar directions. Specifically,  GITA [1] demonstrates that transforming graph structures into images and processing them through VLMs enhances fundamental graph-related tasks, highlighting the benefits of visualizing structures and substructures. Similarly, [2] investigates the efficacy of graph visualization in GNNs with dedicated experiments focusing on substructure sensitivity. Both studies substantially cover the core idea of this paper in terms of utilizing vision to better understand structure and substructure, addressing comparable problems, and employing similar tasks, such as link prediction, which overlaps conceptually with KGC. Another problem is that this paper avoid all these works in discussion. Therefore, the paper's contribution in terms of idea novelty appears somewhat overstated. \n\nTechnical Novelty: The proposed approach of integrating structural or visual inputs via special tokens is also not particularly novel, as it has been a well-established practice in numerous multimodal approaches. For instance, LLaVA [3] employs special tokens for vision input, and LLaGA [4] utilizes a similar mechanism to encode graph structure. Consequently, the technical design does not introduce distinctly novel methodologies.\n\nDomain-Specific Contributions: While adapting existing insights and techniques to KGC is a meaningful contribution, the paper could benefit from demonstrating more distinctive designs or considerations tailored specifically to Knowledge Graph (KG) visualization, e.g., how perform current KG visualization versus other visualization approach, why such visualization, how benefits? Such domain-specific contributions should be central to works of this nature but are not strongly evident in the current approach (There are some, but not enough from my point of view).\n\nExperimental Scope and Metrics: The experimental evaluation suffers from a limited scope, being conducted on only two datasets. To ensure robust results, it is important to test the approach on a broader range of datasets commonly used for KGC, such as WN18RR, UMLS, and Yelp. Additionally, the evaluation should encompass a more comprehensive set of standard metrics, including Mean Reciprocal Rank (MRR), Hits@1, Hits@3, Hits@10, and Hits@100, along with reporting standard deviations to provide a clearer picture of performance reliability. Besides, the baselines are not powerful, particular for the LLM-based training free ones, there have been many SOTA frameworks for KGC in such path. More necessary, time/memory analysis, OCR ability rely contents, are need to be exposed. \n\n[1] GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning, NeurIPS 2024\n\n[2] Open Your Eyes: Vision Enhances Message Passing Neural Networks in Link Prediction, ICML 2025\n\n[3] Visual Instruction Tuning, NeurIPS 2023\n\n[4] LLaGA: Large Language and Graph Assistant, ICML 2024"}, "questions": {"value": "1. What about the performance on small models 2B/1.5B for resource-limited setting?\n2. The time/memory comparison with other methods?\n3. How can such approach be extend to large KG, and how can current visualization handle the ambiguous/similar entities, like winner of  ICPC 2023 and winner of ICPC 2024, like The U.S. and America.  Is it depends on the OCR capabilities and how much? What if when the OCR failed or errored, like recognize the 2026 to 2025, e.g.\n5. The semantic label is massive, how to make the KG clearly presented in the canvas when visualized?\n6. The absent hyperparameters claim and sensitive studies?\n7. For the comparison baseline of LLM-based train-free group, why it only contains trivial ICL/ZS, but not other SOTA LLM-based KGC training-free frameworks in recent years?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LRYgnO9J91", "forum": "yKsIC2emJt", "replyto": "yKsIC2emJt", "signatures": ["ICLR.cc/2026/Conference/Submission11013/Reviewer_qWdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11013/Reviewer_qWdC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327524200, "cdate": 1762327524200, "tmdate": 1762922193504, "mdate": 1762922193504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SeeKG, which transforms KGs into visually rendered representation as image inputs for MLLMs to facilitate conceptual thinking and abstract reasoning of MLLMs. The authors evaluate SeeKG under both training-free and supervised fine-tuning settings and demonstrate that it achieves better performance than existing methods that combine LLM with KG."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using visualized KG as the context of MLLM seems novel and interesting."}, "weaknesses": {"value": "- Technical contribution is a bit limited. Generally the proposed method simply uses the visualized KG as the context of MLLM (possibly with some further fine-tuning). \n- Performance improvements seem not significant enough to support the proposed method."}, "questions": {"value": "- In Table 1, the classical baseline methods are restricted to embedding-based methods and ignore GNN-based methods like NBFNet (Zhu et al., 2021, mentioned in Section 1) or RED-GNN [1]. As these methods can often yield better performance than embedding-based methods, some discussion (preferably with some empirical comparison) should be necessary here. \n- While the performance improvements in Table 1 are a bit incremental, the authors may need to consider the computational cost of their proposed method, including:\n    - How does the computational cost scale with the number of parameters (e.g., from Qwen2.5-VL-3B, 7B to 32B, 72B)?\n    - How does the fine-tuning method introduce additional computational cost than its training-free counterparts? Furthermore, given that embedding-based methods easily excels training-free SeeKG, how does such computational cost compare against these embedding-based methods?\n- I am a bit uncertain if there is possible information leakage in the experiments, as the baseline without any context achieves only slightly worse performance than the complete proposed method in the ablation study. Some discussion should be welcome here, and the authors are encouraged to consider more specific KG (e.g., medical or scientific) to better avoid possible information leakage. \n- Despite the GNN-based methods (NBFNet and RED-GNN), some other related works [2,3,4] may also worth some discussion to better support the superiority of proposed method. \n\n## References\n[1] Knowledge Graph Reasoning with Relational Digraph. WWW 2022\n\n[2] Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. ICLR 2024\n\n[3] Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering. EMNLP 2024\n\n[4] Open Your Eyes: Vision Enhances Message Passing Neural Networks in Link Prediction. ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "35Fs2cn9QV", "forum": "yKsIC2emJt", "replyto": "yKsIC2emJt", "signatures": ["ICLR.cc/2026/Conference/Submission11013/Reviewer_BHn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11013/Reviewer_BHn7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762355443676, "cdate": 1762355443676, "tmdate": 1762922193056, "mdate": 1762922193056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}