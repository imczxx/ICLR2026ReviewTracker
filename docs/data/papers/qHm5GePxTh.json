{"id": "qHm5GePxTh", "number": 5166, "cdate": 1757858462686, "mdate": 1759897990935, "content": {"title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers", "abstract": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Our proposed approach outperforms prior state-of-the-art methods at all latencies on COCO and Roboflow100-VL. Notably, RF-DETR (medium) approaches performance parity with GroundingDINO (tiny) on Roboflow100-VL while running 60x as fast, and RF-DETR (nano) achieves 48.0 AP on COCO, improving upon D-FINE (nano) by 5.3 AP.", "tldr": "We present RF-DETR, a real-time object detector that achieves pareto-optimal accuracy and latency using Neural Architecture Search.", "keywords": ["Real-Time Object Detection", "Neural Architecture Search", "Transfer Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/209594904e4d70656385af644510ab8eb7613c76.pdf", "supplementary_material": "/attachment/8dd4e7d9e56ce74b3e74ccd71cfcd7703a0f6e3f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces RF-DETR, a lightweight specialist object detector that uses NAS to discover accuracy-latency Pareto curves for target datasets. The key contributions are: (1) a family of NAS-based detection and segmentation models that outperform prior real-time methods on COCO and Roboflow100-VL; (2) exploration of \"tunable knobs\" in weight-sharing NAS for end-to-end detection, improving transferability; and (3) a standardized latency evaluation protocol to address reproducibility issues."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The novelty of this work is acceptable. It applys end-to-end weight-sharing NAS to DETR-based detectors, which has been underexplored. Unlike prior NAS methods focused on image classification or backbones, RF-DETR optimizes full detection pipelines, including segmentation heads.\n+ The paper includes extensive experiments on COCO and RF100-VL. Ablations validate design choices, such as backbone replacements and NAS components. Results show consistent improvements.\n+ The authors have clarified their motivations. Some figures can help understanding, and the language is precise. Some method details have been provided.\n+ This method outperforms some famous baselines like YOLO and D-FINE, and the NAS framework allows customization for diverse hardware."}, "weaknesses": {"value": "- The generalization beyond COCO and RF100-VL cannot be confirmed.​​ The experiments focus on two benchmarks, but the claim of generalizability to \"any target dataset\" is not fully validated. Testing on diverse domains would demonstrate broader applicability. The paper notes that hyperparameters may overfit to COCO-like data and more cross-dataset results would alleviate this concern.\n- The paper lacks a theoretical analysis of why weight-sharing NAS generalizes well to unseen architectures. For example, it does not provide robustness analysis for the NAS mechanism.\n- The authors may further figure out the specific gaps in existing NAS methods for object detection in the introduction. While it mentions overfitting to COCO, it does not thoroughly explain why current NAS approaches fail in detection tasks or how RF-DETR uniquely addresses these issues, especially NAS is not a new concept or tool."}, "questions": {"value": "- The weight-sharing NAS involves sampling configurations during training. What is the total training time compared to a non-NAS baseline? \n- RF-DETR is positioned as a specialist detector, but how does it compare to fine-tuned VLMs in terms of accuracy-latency trade-offs?\n- The buffering method reduces throttling, but did the authors consider other techniques? Please explain why buffering was preferred over alternatives.\n- The authors may consider the weakness above and address these key concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GQ0JGeMaaP", "forum": "qHm5GePxTh", "replyto": "qHm5GePxTh", "signatures": ["ICLR.cc/2026/Conference/Submission5166/Reviewer_HtQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5166/Reviewer_HtQ3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897361759, "cdate": 1761897361759, "tmdate": 1762917924799, "mdate": 1762917924799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RF-DETR, a fast, closed-vocabulary DETR that uses weight-sharing NAS to pick the best accuracy–latency setup (tuning resolution, patch size, decoder depth, queries, and windowing) after a single fine-tune—no extra retraining needed. It swaps in a DINOv2 ViT backbone, adds a lightweight mask head (RF-DETR-Seg), and standardizes latency benchmarking (buffering, same FP model, and counting NMS/mask conversion). Results on COCO and Roboflow100-VL show a new real-time Pareto frontier—e.g., the nano model beats D-FINE (nano) by about 5 AP and the medium model is near GroundingDINO (tiny) while roughly 60× faster—suggesting many recent detectors are implicitly over-optimized for COCO."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Originality here is pragmatic: the paper smartly fuses weight-sharing NAS with DETR and a foundation-model backbone, plus a no-nonsense latency protocol—less theory, more removal of real deployment roadblocks. Quality is strong for an engineering paper: careful ablations, honest discussion of TensorRT variance and FP16 pitfalls, and fair apples-to-apples latency (counting NMS/mask steps) bolster credibility. Clarity is good—the “knobs” are concrete, the scheduler-free recipe is easy to replicate, and the deployment story (post-hoc grid search, decoder/query truncation) is clean. Significance is high for practice: it moves the real-time Pareto frontier, cuts the cost of retargeting to new hardware/domains, and exposes COCO-centric benchmarking biases; the results on RF100-VL make the transfer claims believable."}, "weaknesses": {"value": "1) Gains appear driven mainly by stronger pretraining (DINOv2, O365+SAM2) and a stable recipe, making NAS’s unique contribution unclear.\n2) Size taxonomy is misleading: the “nano” model (26.9M params) is far larger than baseline “nano” (3–4M), confounding size vs latency comparisons.\n3) The latency protocol (200ms buffering; TensorRT/FlashAttention) stabilizes single-shot timing but can bias rankings and does not reflect sustained throughput.\n4) YOLO baselines on RF100-VL are disadvantaged by COCO-tuned thresholds and multi-/single-class NMS mismatches, likely underestimating their performance.\n5) Mixed-precision/export inconsistencies (FP32 accuracy vs FP16 latency; modified ONNX export) undermine strict parity across methods."}, "questions": {"value": "1) Size taxonomy and fairness: Can you clarify the naming (e.g., “nano”) and provide comparisons under fixed latency/parameter/FLOP budgets or a plot at fixed latency caps? \n2) Latency and throughput: Can you report sustained throughput (QPS) under continuous load without the 200 ms buffer, include full pipeline timing (pre/post-processing, NMS/mask conversion), and show sensitivity to buffer length and FlashAttention usage? \n3) YOLO baselines on RF100-VL: Did you use multi-class NMS and dataset-specific threshold tuning consistent with original inference? Can you provide threshold/post-processing sensitivity analysis to verify robustness? \n4) Mixed-precision/export parity: Are all methods evaluated with identical artifacts (same precision, same export path)? Can you disclose the modified export scripts (e.g., ONNX opset 17) and quantify FP16 accuracy changes? \n5) NAS cost and coverage: What are the GPU-hours for training/search, how much of the search space is sampled, and how do “unseen” subnets perform statistically? \n6) Method details: How is “encoder confidence” defined for query dropping, and what criteria or default policy govern decoder truncation at inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hZVcddLfti", "forum": "qHm5GePxTh", "replyto": "qHm5GePxTh", "signatures": ["ICLR.cc/2026/Conference/Submission5166/Reviewer_FU3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5166/Reviewer_FU3r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926870503, "cdate": 1761926870503, "tmdate": 1762917924544, "mdate": 1762917924544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RF-DETR, a real-time DETR that uses weight-sharing NAS to pick the best accuracy–latency setups (tuning resolution, patch size, decoder depth, queries, and windowing) after a single fine-tune, no extra retraining needed. It swaps in a DINOv2 ViT backbone, adds a lightweight mask head (RF-DETR-Seg), and standardizes latency benchmarking (buffering, same FP model, and counting NMS/mask conversion). Results on COCO and Roboflow100-VL show a new real-time Pareto frontier, the nano model beats D-FINE (nano) by about 5 AP and the medium model is near GroundingDINO (tiny) while roughly 60× faster."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper fuses weight-sharing NAS with DETR and a foundation-model backbone, plus a latency protocol: less theory, more removal of real deployment roadblocks. Quality is strong for an engineering paper: impressive results, careful ablations, honest discussion of TensorRT variance and FP16 pitfalls, and fair latency (counting NMS/mask steps) bolster credibility. It also moves the real-time Pareto frontier, cuts the cost of retargeting to new hardware/domains, and exposes COCO-centric benchmarking biases; the results on RF100-VL make the transfer claims believable."}, "weaknesses": {"value": "1) Gains appear driven mainly by stronger pretraining (DINOv2, O365+SAM2) and a stable recipe, making NAS’s unique contribution unclear.\n2) Size taxonomy is misleading: the “nano” model (26.9M params) is far larger than baseline “nano” (3–4M), confounding size vs latency comparisons.\n3) YOLO baselines on RF100-VL are disadvantaged by COCO-tuned thresholds and multi-/single-class NMS mismatches, likely underestimating their performance."}, "questions": {"value": "1) Can you clarify the naming (nano) and provide comparisons under fixed latency/parameter/FLOP budgets or a plot at fixed latency caps? \n2) About YOLO baselines on RF100-VL, did you use multi-class NMS and dataset-specific threshold tuning consistent with original inference? Can you provide threshold/post-processing sensitivity analysis to verify robustness? \n3) What are the GPU-hours for NAS training/search, how much of the search space is sampled, and how do unseen subnets perform statistically? \n4)  How is encoder confidence defined for query dropping, and what criteria or default policy govern decoder truncation at inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZVcddLfti", "forum": "qHm5GePxTh", "replyto": "qHm5GePxTh", "signatures": ["ICLR.cc/2026/Conference/Submission5166/Reviewer_FU3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5166/Reviewer_FU3r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926870503, "cdate": 1761926870503, "tmdate": 1763351603667, "mdate": 1763351603667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a weight-sharing neural architecture search space for finding realtime DETR-style object detectors that are pareto-optimal w.r.t. accuracy and runtime. The resulting models achieve state-of-the-art real-time results on the COCO and Roboflow100-VL object detection benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and approachable. The search space is elegantly designed and evidently yields state-of-the-art results. The paper makes a set of great points about suboptimal benchmarking practices in prior work and makes an effort to perform fair comparisons."}, "weaknesses": {"value": "While the paper does provide significant contributions, it could support more ablations and experiments. For example, I would have loved to see a thorough discussion of how much each of the “tunable knobs” contributes to favorable accuracy-runtime tradeoffs, and how much pareto-optimal “knob-settings” vary between datasets. Furthermore, it would be interesting to see whether specific dataset characteristics, like the prevalence of small objects, have an impact on knobs like patch size. I furthermore find it highly interesting that DINOv2 performs much better on “small datasets”, but there are no systematic comparisons between other backbones."}, "questions": {"value": "* Instance segmentation:  \n  * How is the proposed approach different from Mask DINO?  \n  * “Our segmentation head bilinearly interpolates the output of the FPN and learns a lightweight projector to generate a pixel embedding map” \\- Where is the FPN coming from? I thought this was using a DINO backbone?  \n* There are two inconsistent definitions of latencies at which RF-DETR outperforms prior work: “for all latencies” and “all latencies ≤ 40 ms”. Which of these is correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BMmwUevFdI", "forum": "qHm5GePxTh", "replyto": "qHm5GePxTh", "signatures": ["ICLR.cc/2026/Conference/Submission5166/Reviewer_c7Lr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5166/Reviewer_c7Lr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981181918, "cdate": 1761981181918, "tmdate": 1762917924128, "mdate": 1762917924128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a weight-sharing, once-for-all style search over DETR variants (“RF-DETR”) to obtain real-time operating points along an accuracy–latency Pareto curve. A single base model is trained while exposing a compact set of architectural knobs (input resolution, patch size, decoder depth, number of queries, windowed vs. global attention). After training, sub-networks are selected by validation without per-subnet fine-tuning. Experiments on COCO and a broader multi-dataset suite show consistent gains over strong real-time baselines, and an instance-segmentation extension indicates broader utility."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. A practical blend of weight sharing, once for all selection, and DETR knob tuning that feels deployable, with latency protocols that practitioners can actually follow.\n2. Clear empirical gains against strong real time baselines on COCO and on a broader evaluation suite, and the instance segmentation extension indicates the idea transfers beyond detection.\n3. A search space whose knobs are easy to grasp, with figures and ablations that make the design choices legible.\n4. A simple path to high quality low latency detectors without fine tuning of each subnet, plus useful guidance on latency measurement such as throttling mitigation and artifact consistency."}, "weaknesses": {"value": "1. The way subnets are sampled during training and the policy or grid used for post training selection are not specified clearly, which hurts reproducibility and interpretation.\n2. The paper does not make clear when or how decoder layers and queries are dropped during training, whether losses are reweighted across depths, or how queries are ranked at test time.\n3. The contribution reads as incremental relative to once for all and weight sharing NAS in vision, and a tighter comparison to prior NAS for detection and backbones is needed to clarify what is new beyond engineering.\n4. Some comparisons mix backbones and pretraining regimes, so stronger parity baselines or controlled reruns would better isolate the benefit of the proposed recipe.\n5. It is unclear how a fixed mined subnet transfers to unseen datasets or domains without reselection, and a cross dataset test would strengthen the robustness claim.\n6. Per knob sensitivity and stability evidence are limited, for example queries versus AP at fixed FLOPs and the interaction of resolution and patch size, and reporting variance with error bars or minimum median maximum would help."}, "questions": {"value": "1. How are sub-networks sampled during training (uniform over knobs, FLOPs-aware, or constrained)? Any coupling constraints to avoid pathological combinations?\n2. Are layers/queries randomly dropped during training to mimic inference-time truncation? Is there loss re-weighting across depths? How exactly are queries ranked at test time?\n3. How many candidates are evaluated during selection, what is the wall-clock/energy budget, and is the same operating point reused across datasets/hardware or re-selected each time?\n4. Could you add controlled re-runs (or a table) where backbones and pre-training are aligned across methods to isolate the effect of your approach?\n5. Could you report results where a single subnet chosen on COCO is evaluated unchanged on other datasets to assess transfer.\n6. Could you provide per-knob sensitivity plots and report variance across random seeds and multiple TensorRT engine builds?\n7. Will you release export scripts, calibration settings, the buffering/throttling harness, and the exact list of selected sub-networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nZsjNASZTz", "forum": "qHm5GePxTh", "replyto": "qHm5GePxTh", "signatures": ["ICLR.cc/2026/Conference/Submission5166/Reviewer_fnAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5166/Reviewer_fnAm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986477328, "cdate": 1761986477328, "tmdate": 1762917923514, "mdate": 1762917923514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}