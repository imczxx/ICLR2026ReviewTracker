{"id": "GFDSGlEks2", "number": 24349, "cdate": 1758355980523, "mdate": 1759896770123, "content": {"title": "The Impact of Post-training on Data Contamination", "abstract": "We present a controlled study of how dataset contamination interacts with the post-training stages now standard in large language model training pipelines. Starting from clean checkpoints of Qwen2.5 (0.5B/1.5B) and Gemma3 (1B/4B), we inject five copies of GSM8K and MBPP test items into the first 2B tokens of an otherwise 25B token extended pre-training dataset. We then compare the contaminated and clean models both immediately after pre-training and again after two popular post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL) with group relative policy optimization (GRPO). The applied post-training steps do not have any contamination. Across math and coding benchmarks, we find three consistent patterns: (i) Contamination causes performance spikes that are gradually diminished with continued pre-training. After even 25B tokens the apparent performance inflation of contamination can become close to zero. (ii) Both SFT and GRPO resurface the leaked information, but with different external validity: SFT inflates scores only on the contaminated tasks, whereas GRPO also inflates performance on uncontaminated counterparts (GSMPlus, HumanEval). (iii) Model scale amplifies these tendencies, larger Supervised Fine Tuned models memorize more, while larger GRPO models translate leakage into more generalizable capabilities. Our results underscore the need for contamination audits \\emph{after} post-training and suggest that RL-based post-training, although not immune, can help alleviate contamination-related over-estimation problems.", "tldr": "We train a contaminated and clean base model and experiment with clean post-training and how it changes the impact of contamination.", "keywords": ["data contamination", "post-training", "supervised finetuning", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef156cac29b3a3455ea9ee530f1f30b51904d238.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines the relationship between LLMs that experience dataset contamination during the pretraining stage and its impact after undergoing post training in the form of SFT and GRPO, most notably in terms of performance inflation on contaminated benchmarks and whether it leads to generalized performance gains on related benchmarks, across LLMs tested at different scales."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Given the ubiquity of LLM post training, studying dataset contamination under this more practical scenario is useful and has more real-world value in determining whether dataset contamination is impactful. \n\n2. The paper presents a clear research goal, the experiments used support the conclusions, and the results are clear. Presence of error bars makes for more rigidity in the results."}, "weaknesses": {"value": "1. The contribution is limited and practical takeaways should be expanded. The paper's main contribution is testing SFT and GRPO on top of contaminated pretrained models which has limited technical novelty given that it is a minor expansion over previous works studying the pretraining stage such as (Kocyigit et al., 2025; Jiang et al., 2024). While the conclusion that post training leads to inflation on contamination benchmarks is interesting, it retreads that dataset contamination is a major issues in LLM evaluation. Nonetheless, I believe further analysis can help strengthen the contributions. In particular, given the inflation after post training, does it become easier to detect the contamination using popular methods such as [1]  that could allow to see whether this can be mitigated?\n\n2. As the authors mention in 146-147 and 425-426, there is also a lack of comparison of common types of real world contamination, which makes the results difficult to generalize outside of this specific setup. Even providing some results on an additional one and comparing the differences could prompt a discussion about how different setups change the outcomes, which would help real world usability. \n\n3. In line 164 it is said these models were selected based on their demonstrated capabilities in math and coding tasks, but this seems like it would increase the risk that they are already contaminated models. Although it is stated the study is on the partial incremental effect of added leakage, some comparison with models poorer or not designed for these tasks, or at least some ability to estimate the level of contamination already present within these models to draw more stringent conclusions.\n\n4. Further explanation on why contamination learns generalizable features. The paper states that GRPO gains performance on uncontaminated counterparts but are the benefits better than pretraining on an alternative set of data, such as if the training set was used as \"contaminated data\" as opposed to the test set?\n\n5. 2B tokens of contamination is a sizeable amount. Is there any discussion on whether this is a realistic amount and thus the results seen would generalize to amounts seen within off-the-shelf models? \n\n6. Some polish issues. Line 173 and 244 Appendix 7 Referred to multiple times but it doesn't exist and Appendix is letter based\n\n*References*\n\n[1] Proving Test Set Contamination in Black-Box Language Models"}, "questions": {"value": "Refer questions mentioned in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1AMQtqpi1l", "forum": "GFDSGlEks2", "replyto": "GFDSGlEks2", "signatures": ["ICLR.cc/2026/Conference/Submission24349/Reviewer_TWC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24349/Reviewer_TWC4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638079663, "cdate": 1761638079663, "tmdate": 1762943052861, "mdate": 1762943052861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a study on the effect of data contamination during pretraining on models that are then post trained with SFT and RL. The main contribution consists of a set of findings that analyze model behavior and show that data contamination analyses needs to be conducted at all stages, and that the impact of data contamination may have different symptoms on RL and SFT models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1- The topic is important to the community. Better understanding dynamics of data contamination in each stage of the model lifecycle is crucial to improving generalization.\n\nS2- The setup is easy to understand and results are clearly described."}, "weaknesses": {"value": "The main concern I have with the paper is that the study's scope is somewhat narrow and MVP.  For example:\n\n- It is somewhat common knowledge that larger models can generalize better even when data contamination is present. The models studied in the paper are quite small and the findings may only be valid for this size. While compute constraints are common these days, perhaps even scaling to say the Olmo family of models (7b, 12b, 32b) might be more informative than staying in the 1-4B range. \n\n- There could be other setups that are still interesting to study but are not covered such as shuffling the contamination data equally in the pretraining set, introducing it at the end of pretraining, using the data exclusively for SFT or even for RL."}, "questions": {"value": "- Do the authors contaminate the data with both the question and its answer or only with the question? \n\n- Have the authors considered expanding the study to more datasets and models for which the training data is known? E.g. Olmo 2, Nemotron Nano v2\n\n- How do best-of-N scores change throughout the study? Best-of-N is usually considered a good measure for the model having some knowledge but not being able to surface it in one shot reliably."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dH1kB2mpBK", "forum": "GFDSGlEks2", "replyto": "GFDSGlEks2", "signatures": ["ICLR.cc/2026/Conference/Submission24349/Reviewer_P8XN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24349/Reviewer_P8XN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774833766, "cdate": 1761774833766, "tmdate": 1762943052666, "mdate": 1762943052666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a controlled experimental study of how data contamination interacts with post-training stages SFT and RL. The authors begin with clean checkpoints of two open-weight model families (Qwen2.5 at 0.5 B and 1.5 B params; Gemma‑3 at 1 B and 4 B params). They then create a “contaminated” branch by injecting five copies of the test sets of the benchmarks GSM8K (math) and MBPP (code) into the first 2B tokens of an extended pre-training dataset (~25B tokens). They compare the clean and contaminated models immediately after pre-training, and then after post-training using either SFT or GRPO (on the same sets, but with no contamination in the SFT/GRPO data). They evaluate on contaminated tasks (GSM8K, MBPP) and “uncontaminated” counterparts (GSMPlus, HumanEval)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Realistic Setting for Data Contamination Research. This paper used a pretraining -> SFT/RL setting, which is so far the most realistic to what could actually happen in model training. The previous work in data contamination research overwhelmingly focus on SFT on the test-set. Of course it's going to be super obvious that the model is contaminated. Although the model/corpus is small, it's an important step of moving towards the right direction.\n2. Insightful findings. This paper finds out that with continued pretraining, the contamination signal becomes occluded, and will resurface again with SFT/RL. Even brings generalization benefits for RL training. I think this conclusion is counter-intuitive, and valuable to the general research community.\n3. Solid analysis and inclusion of experiment details. The authors conducted ablation studies, and the authors released the experiment details."}, "weaknesses": {"value": "1. The main claims of the paper relies on a small performance gap. Around 2-4% across the experiments. Although they are smaller models, of limited capacity, it still makes me question the generalizability of this papers findings.\n2. The difference between SFT and GRPO is a major contribution of the paper, but more depth (or hypothesised mechanism) would strengthen the claim. For example, are RL-tuned models less “local‐overfit” to contaminated items because the reward encourages broader pattern recognition? Some analysis capturing this could help."}, "questions": {"value": "Besides the main concerns, I have the following minor suggestions:\n1. Varying the dose of contamination would be an interesting thing to analyze as well. Maybe the conclusions of this paper will change at a certain portions, or maybe it will hold for all contamination levels.\n2. Figure 6 is a bit too big."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7eZAVWn0qO", "forum": "GFDSGlEks2", "replyto": "GFDSGlEks2", "signatures": ["ICLR.cc/2026/Conference/Submission24349/Reviewer_ZfoS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24349/Reviewer_ZfoS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778196236, "cdate": 1761778196236, "tmdate": 1762943052134, "mdate": 1762943052134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}