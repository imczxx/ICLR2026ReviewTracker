{"id": "aqGNdZQL9l", "number": 20559, "cdate": 1758307385697, "mdate": 1763682209770, "content": {"title": "Decoupled Q-Chunking", "abstract": "Bootstrapping bias problem is a long-standing challenge in temporal-difference (TD) methods in off-policy reinforcement learning (RL). Multi-step return backups can alleviate this issue but require delicate importance sampling to correct their off-policy bias. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, enabling unbiased multi-step backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal in environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning policies over long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned benchmarks and show that it reliably outperforms prior methods.", "tldr": "", "keywords": ["Reinforcement learning", "action chunking", "offline RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ae35aaccae04c04df7b2ac6e110dbc377b67d97.pdf", "supplementary_material": "/attachment/29f71a41347c85e80cac29dd73d2cc56f6e92cec.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the bootstrapping bias problem by proposing a novel method to decouple the chunk length of the critic from that of the policy, allowing for more efficient multi-step return backups. The proposed algorithm optimizes the policy against a distilled critic, retaining the benefits of multi-step value propagation while avoiding the challenges of open-loop sub-optimality and long action chunk learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with well-structured explanations.\n2. The authors effectively discuss the scenarios in which action chunking Q-learning is preferable over standard n-step return learning."}, "weaknesses": {"value": "1. My main concern is that, by comparing the performance and curves of DQC and QC-NS, QC-NS—without decoupling the action chunking of the Q-value from that of the policy—achieves nearly the same performance as DQC, except for the cube-quadruple-100M task.\n2. How much does the performance degrade if the expectile update in Eq. (24) is replaced with a quantile update?\n3. Why does setting $N=128$ not lead to better performance?\n4. In most algorithms, the batch size is typically set to 256, but in this paper, it is set to 4096. Could the authors conduct an ablation study to justify this choice?\n5. Could the proposed method be evaluated on standard offline RL benchmarks such as D4RL?\n6. There appear to be some minor errors, such as two consecutive \"the\"s in the first line of page 2, and potential mistakes regarding $\\tau_b$ and $\\tau_d$ in Algorithm 1. The authors should carefully proofread the manuscript."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xhh1JCfoHE", "forum": "aqGNdZQL9l", "replyto": "aqGNdZQL9l", "signatures": ["ICLR.cc/2026/Conference/Submission20559/Reviewer_c39B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20559/Reviewer_c39B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638256006, "cdate": 1761638256006, "tmdate": 1762933973839, "mdate": 1762933973839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comments"}, "comment": {"value": "We would like to thank all reviewers for the detailed and insightful reviews and area chair for facilitating the review discussion. For the updated version of PDF, we have made various improvements to our empirical evaluations and theoretical analysis which we summarize below. All changes in the PDF are highlighted in purple.\n\n## 1. Empirical evaluation improvements with additional tuning and seeds (3 seeds => **6 seeds**): \n\nIn our original submission, we used a uniform $\\kappa_b=0.9$ for all environments, which turned out to be sub-optimal for both our method and our baselines. For this rebuttal, we performed additional hyperparameter tuning on DQC, NS, QC, OS, QC-NS with two seeds over the hyperparameter ranges specified in Table 8. Then, we ran all these methods with the best hyperparameter configurations (Table 7) with 6 different seeds. As a result, DQC’s performance improves substantially. In our original submission, we also had results where we took directly from prior work (with different batch size and number of training steps). For this rebuttal, we reran all such baselines ourselves with the setting consistent with ours (e.g., 4096 bath size and 1M training steps). Overall, our main conclusion remains similar: **(1) Our method, DQC, outperforms SHARSA, the previous state-of-the-art method on 6 of the hardest OGBench GCRL tasks. (2) While sometimes DQC is slightly worse than NS on some tasks, overall it is more robust across the board compared to NS and consistently outperforms QC**. We include a summary of our new result table below (see Table 1 in the updated PDF for the confidence intervals). On top of these new results, we also included a more comprehensive sensitivity analysis in Figure 3.\n\n| Task | FBC | HFBC | IQL | HIQL | SHARSA | OS | NS | QC | DQC-naïve | DQC |\n|--|-|-|-|-|-|-|-|-|-|-|\n| cube-triple-100M | 53  | 57   | 64  | 36   | 82 | 56 | 56 | 17 | 36 | **98** |\n| cube-quadruple-100M    | 32  | 38   | 53  | 24   | 67     | 0  | 22 | 29 | 36 | **93** |\n| cube-octuple-1B  | 0   | 28   | 0   | 18   | **33** | 0  | 7  | 0  | 2 | **31** |\n| humanoidmaze-giant | 1   | 4    | 4   | 24   | 18  | 0  | **97** | 34 | 81 | 92  |\n| puzzle-4x5   | 0   | 0    | 20  | 0    | 1      | 18 | 88 | 22 | 31 | **96** |\n| puzzle-4x6-1B  | 0   | 5    | 7   | 10   | 62  | 19 | **95** | 43 | 42 | 81  |\n\n(we bold both DQC and SHARSA because SHARSA’s confidence interval overlaps with the DQC’s mean performance)\n\n## 2. New theoretical results to provide guarantees for our method, DQC, in Section 4.4. \n\nWe developed two new results regarding our DQC method. (1) Under the strong $\\varepsilon_h$-open-loop consistency condition, we showed that closed-loop execution of the action chunking policy is near-optimal (Proposition 4.9). (2) Under our new optimality variability conditions with $\\vartheta^L_h$ and $\\vartheta^G_h$, the closed-loop execution of the action chunking policy is also guaranteed to be near-optimal, independent from the degree of open-loop consistency of the data distribution (Theorem 4.11). \n\nLet $H=1/(1-\\gamma), \\bar H = 1/(1-\\gamma^h)$, our current theoretical results can be summarized as follows: \n\n| |**Value Estimation Error**|**AC Optimality (QC)**| **Closed-loop AC Optimality (DQC with $h_a=1$)** |\n|---|---|---|---|\n|  | $\\| \\hat{V}\\_{\\mathrm{ac}} - V\\_{\\mathrm{ac}} \\|$ | $V^\\star - V^+_{\\mathrm{ac}}$ | $V^\\star - V^\\bullet$|\n| **Weak $\\varepsilon_h$-OLC**      | $O(\\varepsilon_h H \\bar{H})$ | - | - |\n| **Strong $\\varepsilon_h$-OLC**    | $O(\\varepsilon_h H \\bar{H})$| $O(\\varepsilon_h H \\bar{H})$| $O(\\varepsilon_h H^2 \\bar{H})$                          |\n| **$(\\vartheta^L_h, \\vartheta^G_h)$-BOV** | -  | - | $O(\\vartheta^L_h H + \\vartheta^G_h H \\bar{H})$|\n\n(OLC: open-loop consistent, BOV: bounded optimality variability)\n\n## 3. Additional baseline: DQC-naïve. \n\nSince the core idea of our method is to decouple the chunk size of the policy from that of the critic, we also explore another common sense baseline where we take QC and simply execute the first couple of actions in the action chunk instead of the full chunk. We include this baseline as part of our main updated result table and plot (Table 1 and Figure 1). While this naïve way of decoupling alone can already provide some benefits over QC, they are generally much worse than DQC. This shows that training a separate distilled critic and a policy with smaller action chunk is crucial for performance improvements in DQC.\n\n## 4. Tight bounds for all our theoretical results in Section 4. \n\nFor Theorem 4.4, Corollary 4.5, Theorem 4.6, we improved the bounds such that they match our lower-bounds (available in Appendix F).\n\n**We again thank all reviewers and AC and please let us know if there are any additional questions or concerns regarding our paper.**"}}, "id": "K9VynNCPzi", "forum": "aqGNdZQL9l", "replyto": "aqGNdZQL9l", "signatures": ["ICLR.cc/2026/Conference/Submission20559/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20559/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20559/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763681520741, "cdate": 1763681520741, "tmdate": 1763681520741, "mdate": 1763681520741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DQC, that builds upon Q-chunking but decouples the critic’s chunk length from the policy’s chunk length. It aims to address bootstrapping bias from TD backups and open-loop policy inefficiency in off-policy TD learning. Experiments on the OGBench suite show that DQC outperforms Q-chunking and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper derives explicit bias and near-optimality bounds, offering theoretical insights of when action-chunking methods succeed or fail.\n\n2. Empirical performance of DQC is nice."}, "weaknesses": {"value": "1. Theorem 4.6 is somewhat idealized. It depends on the open-loop consistency assumption. It is hard to hold in the practice for realistic offline datasets, especially in long-horizon settings, in such cases $\\epsilon_h$ may not be small, and thus the errors scale and then the bound can become vacuous. Could the authors provide an empirical analysis on how big is $\\epsilon_h$ in the OGBench datasets?\n2. Theorem 4.6 does not seem to cover the decoupled critics. Is this bound still valid when the two critics are trained with different horizons and objectives?"}, "questions": {"value": "1. The distilled partical critic $Q_\\psi^P$ is not Bellman-consistent with the long-horizon cirtic $Q_\\phi$. $Q_\\psi^P$ is trained to approximate an optimistic projectiion of $Q_\\phi$ rather than its bellman target, thus there's an inherent objective mismatch between the two critics. Will it introduce systematic bias or training instability? Especially, when in non-markovian or sparse-reward settings, the optimistic assumption might not hold. Could the authors clarify why this training remains stable and effective in practice? It would be helpful to see a quantitive analysis showing how closely $Q_\\psi^P$ tracks $Q_\\phi$.\n\n2. How the key hyperparameters were chosen, for example, $h$ and $h_a$? How were these values selected in practice—through tuning, fixed ratios, or heuristics? And how sensitive is the method to their choice?\n\n3. What is the computational overhead of maintaining two critics with different horizons?\n\n4. In Algorithm, it introduces two expectile parameters, $\\tau_d$ and $\\tau_b$ for the two critics. However, I do not find how the two factors are selected or tuned in Table 4. Expectile regression is known to be sensitive to $\\tau$. It would amplify overestimation bias when using a large $\\tau$. Are $\\tau_b$ and $\\tau_d$ the same across envs, or task-specific? Did the authors observe significant sensitivity in performance when varing $\\tau_b$ and $\\tau_d$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MOHDhR6lIb", "forum": "aqGNdZQL9l", "replyto": "aqGNdZQL9l", "signatures": ["ICLR.cc/2026/Conference/Submission20559/Reviewer_WFcb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20559/Reviewer_WFcb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814947404, "cdate": 1761814947404, "tmdate": 1762933973501, "mdate": 1762933973501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Decoupled Q-Chunking (DQC), an offline RL algorithm that mitigates bootstrapping bias in long-horizon learning by evaluating long action chunks while predicting only short ones. By decoupling the policy and critic chunk lengths, DQC eases policy learning without sacrificing the benefits of multi-step value propagation. Theoretically, it formalizes the open-loop bias in chunked TD backups and proves when chunked critics outperform standard $n$-step returns. Empirically, DQC consistently outperforms prior state-of-the-art methods—including SHARSA—on the most challenging OGBench tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of decoupling the policy and critic chunk sizes is novel, effectively addressing a known trade-off in multi-step Q-learning to get \"the best of both worlds.\"\n2. The paper provides deep theoretical backing for Q-learning with action chunking, formally identifying and quantifying bias, and proving the conditions under which the approach is superior.\n3. It demonstrates superior, state-of-the-art results on challenging long-horizon tasks, significantly outperforming previous methods on the OGBench benchmark.\n4. The work includes comprehensive experiments, comparisons to relevant baselines, and ablation studies that strongly support the authors' claims."}, "weaknesses": {"value": "1. The approach still suffers from the inherent bias of open-loop value evaluation in action chunking and lacks a mechanism to actively correct it.\n2. Its theoretical guarantees rely on a strong \"open-loop consistency\" assumption for the offline dataset, which may not hold in many real-world scenarios, limiting the generality of the claims.\n3. The use of a fixed, global chunk size for both the policy and critic is a limitation, as the optimal action horizon might vary depending on the state.\n4. The framework introduces additional components (e.g., two Q-networks, best-of-N sampling) and hyperparameters, increasing computational overhead and the practical difficulty of tuning and deployment."}, "questions": {"value": "in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RlepEPY4D9", "forum": "aqGNdZQL9l", "replyto": "aqGNdZQL9l", "signatures": ["ICLR.cc/2026/Conference/Submission20559/Reviewer_uJSe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20559/Reviewer_uJSe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848304382, "cdate": 1761848304382, "tmdate": 1762933973023, "mdate": 1762933973023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work improves on the concept of chunked critics in the context of off-policy RL.\nThe chunk length of the critic and policy is decoupled by optimizing the policy against a distilled critic, allowing the policy to output shorter chunks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-written and detailed theoretical investigation.\n- Strong and robust results."}, "weaknesses": {"value": "- Some minor grammatical errors (e.g., l. 26, 182)\n- There is no discussion of the computation overhead compared to the baseline methods (e.g., from maintaining the additional distilled critic). Since the performance gap to the baselines is substantial, providing a short intuition should suffice."}, "questions": {"value": "- Notably, the only experiment where the proposed method is outperformed is by the $NS$ approach in the puzzle-4x6-1B environment. Can you provide an explanation/intuition of why this is the case? Are there certain environmental characteristics where you would expect your approach to perform worse than computationally simpler methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YBD8ui7m2k", "forum": "aqGNdZQL9l", "replyto": "aqGNdZQL9l", "signatures": ["ICLR.cc/2026/Conference/Submission20559/Reviewer_WP1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20559/Reviewer_WP1s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993929364, "cdate": 1761993929364, "tmdate": 1762933972557, "mdate": 1762933972557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}