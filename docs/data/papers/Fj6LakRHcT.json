{"id": "Fj6LakRHcT", "number": 3153, "cdate": 1757343796518, "mdate": 1763646849329, "content": {"title": "Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study", "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this perspective.\nWe examine whether safety-relevant behavior is concentrated in specific linear subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in activations. Across both weight and activation spaces, our findings are consistent: subspaces that amplify safe behaviors also amplify useful ones, and prompts with different safety implications activate overlapping representations. Rather than residing in distinct directions, we show that safety is highly entangled with the general learning components of the model. This suggests that subspace-based defenses face fundamental limitations and underscores the need for alternative strategies to preserve safety under continued training. We corroborate these findings with multiple experiments on five open-source LLMs from the Llama and Qwen families. Our code is available anonymously at: https://anonymous.4open.science/r/safety-subspaces-anon-E3CB.", "tldr": "", "keywords": ["Safety", "Alignment", "Harmful Fine-Tuning", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d76ba9b4e25fc3d2d7f436c084bee9505290f586.pdf", "supplementary_material": "/attachment/b22d3eba408f332c039cd37972144fdaa397382e.zip"}, "replies": [{"content": {"summary": {"value": "The paper tests whether “safety subspaces” exist as linear directions in weight or activation space that could preserve or filter safety during fine-tuning. The authors construct subspaces from SVD of alignment and safety deltas, then study how “useful” and “harmful” fine-tuning updates interact with these subspaces via parallel and orthogonal projections, and how their activation spaces overlap. Across five open-weight models, Top-K singular directions strongly affect both utility and harmfulness, orthogonal filtering reduces both together, and activation subspaces for useful and harmful prompts overlap. The paper concludes that distinct linear safety subspaces are unlikely and that projection-based defenses have limited value."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **The question is important for alignment interpretability**. The paper asks whether safety can be isolated linearly in weight or activation space, which underlies many safety direction defenses. The experimental design addresses both weights and activations across several models.\n- **Proper analyzing methods**. The projection schemes and subspace overlap measurement is theory-driven and well-formulated. The paper examine the so-called safety subspace from multiple aspects, which is solid.\n- **The paper is generally clearly written**. I can easily follow the description and understand the claims."}, "weaknesses": {"value": "My main concern of the paper is that the experiment settings in the paper are limited for the broad claims that \"Safety subspace does not exist\".\n\n- **The training settings setup does not isolate safety signal**. The Safety tuning and downstream fine‑tuning use supervised fine‑tuning on text corpora with many properties besides safety. The \"useful\" task is MetaMathQA math problems and the \"harmful\" task uses BeaverTails unsafe data. This setup makes it hard to tell whether the top directions reflect safety or general learning, since both data sources induce large distribution shifts unrelated to safety per se. A stronger test would include a preference‑based safety objective that explicitly trains the model to discriminate safe versus unsafe generation (e.g. DPO [1]), avoiding introducing too much safety-agnostic signals.\n- **Subspace extraction is unsupervised and layer‑agnostic**. Safety subspaces are extracted as the top‑k singular directions of layer-flattened weight updates. This can overweight non-safety layers and wash out safety features, as past works show that large weight shifts happen in later layers while safety mechanism might locate in mid layers [2]. I suggest the author to use layer-wise SVD. The author should also discuss supervised safety-subspace extraction (e.g. probing), which are shown to more effective in extracting safety-subspace. If none of these yield selective safety effects, the linear non‑separability claim is much more convincing.\n- **The harmfulness metric is confounded with general ability**. Harmfulness is graded by a model judge on AdvBench, which may confound general ability (e.g. instruction following and writing quality) with harmfulness of the model. Figures 3 and Tables 2/5 show that orthogonal projection and even random projections reduce both utility and harmfulness together, which can be due to general degradation rather than safety improvement. I suggest the author also report more robust metrics like probability of toxicity generation or further examine if harmful score changes are caused by changes in general ability.\n- **Interpretation of “energy kept” is unclear.** Energy-kept trends appear similar across choices, so it is hard to tell what semantic insight this quantity adds. More explanation will be helpful.\n\nTherefore, the claims in this paper need narrower wording or stronger evidence: \n- **Line 261: Alignment Directions Reflect General Learning, Not Safety**. The symmetry in Figure 2 and Table 1 shows that projecting useful and harmful updates onto top alignment directions increases both utility and harmfulness. This supports high‑impact general directions, but does not rule out safety linear-structure that is not captured by the SVD components. The paper should add the layerwise and supervised directions described above before generalizing this implication. \n- **Line 340: No Selective Removal Is Possible**: Orthogonal projection removes top‑k directions and causes utility and harmfulness to drop together at similar rates (Figure 3; Tables 2 and 5). However, the top-k components is not relevant to safety anyway (as per the first experiment suggested). The result may reflect choice of subspace rather than a fundamental limit.\n- **Line 461: Shared Subspaces Drive Behavior, Not Safety**: Section 6 compares activations from useful prompts (U) with two harmful sets (BeaverTails H1 and ToxiGen H2), then finds useful–harmful overlap often comparable to or exceeding harmful–harmful overlaps. Because H1 and H2 come from different sources and the activation analysis uses only the last token, the overlap patterns can reflect dataset and style similarities rather than safety. \n\nFrom the experiments in the paper, what I can agree on is that **when we SFT models on some aligned / toxic dataset, the dominant subspace of the weights update contains not only safety-specific features, but also utility features.** While this negative result has its own contribution, we need stronger arguments to prove that safety is inherently entangled with general learning and are not linearly separable. The author should address above concerns before I will raise my score.\n\n[1] Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. **A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity.** ICML, 2024.\n\n[2] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. **LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning.** NeurIPS, 2024."}, "questions": {"value": "- Can you address my concern about the weaknesses?\n- At line 242: \"At the same time, while energy is evenly distributed, behavioral impact is not.\". I don't quite get it. I see in Figure 2, all metrics are uniformly increasing with more fraction of SVD vector. How does this results implicate \"behavioral impact is not evenly distributed\"?\n- Most results in the paper are observable, and the paper didn't give analysis on the counterintuitive results. Could you give some reasons on why your experiments shows different results than previous papers? - The implications in line:404 and line:461 are identical. Is it a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Nl9nnZ0b7", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_M6zX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_M6zX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761021891232, "cdate": 1761021891232, "tmdate": 1762916575368, "mdate": 1762916575368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether linear subspaces derived from alignment and safety deltas can selectively control safety, and whether activation patterns for harmful prompts occupy distinct regions. The main observations are that Top-K SVD directions are strong general-impact directions for both utility and harmfulness, orthogonal filtering trades off both together, and activation subspaces of useful and harmful prompts overlap at many layers. These results argue that simple linear projection defenses have limited selectivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Evaluations span multiple Llama and Qwen variants and include analyses under contamination. This breadth supports external validity of the main pattern.\n- The paper explains the operators, datasets, and scoring choices clearly enough that others can reproduce or extend the tests.\n- Demonstrating limits of global SVD-style filtering is actionable for researchers who might otherwise rely on it as a safety control."}, "weaknesses": {"value": "- Broad-claim relative to evidence. The experiments show that global Top-K SVD directions are not safety-selective. That does not exclude the existence of linear safety structure discoverable through other methods, such as multiple independent refusal directions or concept cones[1]. The claim should be scoped accordingly.\n- Activation analysis focuses on last-token states, but refusal dynamics often appear early in the response[2] instead in the last token. I would like to see whether using earlier token windows or residual-stream features changes the overlap picture.\n- Useful and harmful prompts differ in source and style. Some of the observed overlap could reflect stylistic differences.\n    \n[1] Tom Wollschläger et, al. *The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence. 2025.\n[2] Andy Arditi et, al. _Refusal in Language Models Is Mediated by a Single Direction._ 2024"}, "questions": {"value": "- The paper introduces both the general alignment update ($\\Delta_A$) and a specific safety-tuning update ($\\Delta_S$), but the analysis often groups them. What are the difference between the subspaces derived from $\\Delta_A$ and $\\Delta_S$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tspy7zbhJa", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_QX9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_QX9G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761283368264, "cdate": 1761283368264, "tmdate": 1762916575140, "mdate": 1762916575140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work challenges the prior assumption that safety relevant features lie in a specific subspace of the weight space, hence, can be isolated and defended against jailbreaks. Using 2 metrics (Energy-kept ratio, Mode Subspace Overalp), authors find that safety subspaces are entangled with general utility subspaces hindering the applicability of the prior assumption and defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significance:** The paper addresses broadly believed hypothesis that a concept-related features are essentially contained in a specific lower dimensional subspace.\n2. **Novelty:** While mostly building upon [1], the authors take a critical standpoint to better investigate the source of the positive results in [1] and tradeoffs it poses (utility drops similarly with the removal of \"harmful subspace\"). There have been works to observe that most jailbreak defenses are also likely to hurt the general utility of the model, yet a dedicated investigation was lacking in the literature.\n3. **Writing:** The writing is mostly clear and easy to follow."}, "weaknesses": {"value": "Although the paper claims their findings for both weight space and activation space, the activation space seems to be relatively underexplored. Considering that LLM steering is predominantly done on its activations (Activation Addition, Directional Ablation, Manifold Steering, Angular Steering etc.), I think the paper does not deliver sufficiently in this important aspect. The following questions are to specify why exactly I think the current study is insufficient."}, "questions": {"value": "1. If there is no such concept subspaces like \"safety\" in activation space, how would authors explain the success of activation steering methods?\n2. Do results of this paper have any implications on linear separability hypothesis [2] which led many to believe there exist linear concept subspaces [3, 4]? \n3. What is the reason to choose the MATH dataset as a counterpart for \"harmful prompts\"? Would using any refusal dataset instead of a generic \"useful\" dataset impact the observation?\n\n___\n\nOverall, I believe the topic of the paper is relevant and important for the ICLR community, and the paper offers nontrivial contribution (see Strength). Therefore, I am willing to increase my score upon satisfactory responses to the questions raised.\n\n___\n\n### References\n\n1. Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang.Safe lora: the silver lining of reducing safety risks when fine-tuning large language models, 2025.\n2. Kiho Park, Yo Joong Choe, Victor Veitch. The Linear Representation Hypothesis in Language Models. NeurIPS 2023\n3. Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong. Mitigating Overthinking in Large Reasoning Models via Manifold Steering. arXiv:2505.22411\n4. Rachel S.Y. Teo, Laziz U. Abdullaev, Tan M. Nguyen. The Blessing and Curse of Dimensionality in Safety Alignment. COLM 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjzvOrncgG", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_Xrmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_Xrmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702938395, "cdate": 1761702938395, "tmdate": 1762916574816, "mdate": 1762916574816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work challenges the prior assumption that safety relevant features lie in a specific subspace of the weight space, hence, can be isolated and defended against jailbreaks. Using 2 metrics (Energy-kept ratio, Mode Subspace Overalp), authors find that safety subspaces are entangled with general utility subspaces hindering the applicability of the prior assumption and defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significance:** The paper addresses broadly believed hypothesis that a concept-related features are essentially contained in a specific lower dimensional subspace.\n2. **Novelty:** While mostly building upon [1], the authors take a critical standpoint to better investigate the source of the positive results in [1] and tradeoffs it poses (utility drops similarly with the removal of \"harmful subspace\"). There have been works to observe that most jailbreak defenses are also likely to hurt the general utility of the model, yet a dedicated investigation was lacking in the literature.\n3. **Writing:** The writing is mostly clear and easy to follow."}, "weaknesses": {"value": "Although the paper claims their findings for both weight space and activation space, the activation space seems to be relatively underexplored. Considering that LLM steering is predominantly done on its activations (Activation Addition, Directional Ablation, Manifold Steering, Angular Steering etc.), I think the paper does not deliver sufficiently in this important aspect. The following questions are to specify why exactly I think the current study is insufficient."}, "questions": {"value": "1. If there is no such concept subspaces like \"safety\" in activation space, how would authors explain the success of activation steering methods?\n2. Do results of this paper have any implications on linear separability hypothesis [2] which led many to believe there exist linear concept subspaces [3, 4]? \n3. What is the reason to choose the MATH dataset as a counterpart for \"harmful prompts\"? Would using any refusal dataset instead of a generic \"useful\" dataset impact the observation?\n\n___\n\nOverall, I believe the topic of the paper is relevant and important for the ICLR community, and the paper offers nontrivial contribution (see Strength). Therefore, I am willing to increase my score upon satisfactory responses to the questions raised.\n\n___\n\n### References\n\n1. Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang.Safe lora: the silver lining of reducing safety risks when fine-tuning large language models, 2025.\n2. Kiho Park, Yo Joong Choe, Victor Veitch. The Linear Representation Hypothesis in Language Models. NeurIPS 2023\n3. Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong. Mitigating Overthinking in Large Reasoning Models via Manifold Steering. arXiv:2505.22411\n4. Rachel S.Y. Teo, Laziz U. Abdullaev, Tan M. Nguyen. The Blessing and Curse of Dimensionality in Safety Alignment. COLM 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjzvOrncgG", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_Xrmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_Xrmm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702938395, "cdate": 1761702938395, "tmdate": 1763636037743, "mdate": 1763636037743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether safety behaviors in large language models correspond to distinct linear subspaces that can be isolated without degrading general capability. It constructs “safety subspaces” from the principal components of fine-tuning updates between different stages of fine-tuning, then measures their geometric overlap using metrics such as Mutual Subspace Overlap and Energy Kept Ratio. Across multiple model families, the authors find that safety and utility update directions share most of their variance and projection along these subspaces reduces both helpfulness and harmlessness simultaneously. The results suggest that safety alignment is not linearly separable but deeply entangled within the same representational space as capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes “safety subspaces” concretely by building them from principal components of weight differences. Although PCA on weight deltas is common in task-vector or model diffing work, applying it here provides an interesting way to identify safety regions beyond analyses that focus only on activations popular in recent AI safety works.\n\n2. It introduces two sound measures of subspace overlap that are intuitively meaningful, providing a clear alternative to relying solely on downstream evaluation metrics to quantify model diffing methods.\n\n3. The presentation is clear: each research question is addressed in a dedicated section and supported by plots and analysis and the claims in this paper is well supported. The reported metrics and ablations are convincing, and the additional validation in activation space increases confidence that the findings are not artifacts of a single parameterization."}, "weaknesses": {"value": "1. The conclusions may depend on the specific design choices for subspace dimension and PCA aggregation (it seems that SafeMERGE supports a form of layer-wise separability). The paper fixes the number of principal components and the layer aggregation strategy without sensitivity analysis, so the observed overlap might partially reflect those implementation choices rather than a conclusive property.\n\n2. The experiments use a single alignment method and do not test how geometric overlap changes under different alignment objectives, especially under common alignment methods like RLHF and DPO, which usually yield better alignment. Without this comparison, it is unclear whether the observed entanglement is inherent to model structure or specific to the alignment method used and prevailing in all safety representations.\n\n3. The evaluation measures helpfulness and harmlessness separately but does not analyze their joint behavior on adversarial inputs. Testing whether projection affects over-refusal, for example by using XSTest for benign prompts that should not be refused, would better assess whether safety information can be isolated."}, "questions": {"value": "1. Would you say your conclusions align with the findings of Wei et al. (2024) despite the methodological differences? From my understanding, they appear to have reached a similar conclusion.\n\n2. Do you expect the observed entanglement behavior to change under Mixture-of-Experts (MoE) model families, where safety and capability routing may be distributed across distinct experts?\n\n3. How is the k in the top-k principal components chosen, and what proportion of the total variance does this selection typically explain?\n\n\nWei, B., Huang, K., Huang, Y., Xie, T., Qi, X., Xia, M., Mittal, P., Wang, M., Henderson, P. (2024). Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications. arXiv:2402.05162. https://arxiv.org/abs/2402.05162"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Safety related"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cUO0cv3SjS", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_YiuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_YiuK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990097566, "cdate": 1761990097566, "tmdate": 1762916574604, "mdate": 1762916574604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically investigates whether safety subspaces exist in LLM weight or activation space that uniquely encode alignment through 4 studies: Projecting helpful and harmful updates into alignment/safety subspaces, orthogonally filtering contaminated updates, comparing subspace overlaps among useful/harmful/alignment directions, and analyzing activations for harmful vs. benign prompts. The authors find that high-impact directions amplify both utility and harmfulness, and activations for harmful and benign prompts substantially overlap. The results indicate safety is entangled with general learning rather than linearly separable, so projection or filtering along “safety” directions cannot selectively suppress harmful behavior without similar utility loss; this challenges subspace-based defenses and raise a need for alternative strategies for maintaining safety during continued training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper does an extensive empirical analysis extends from weight update directions to activations, and evaluates across multiple open source LLM families (Llama, Qwen) and fine tuning regimes (helpful, harmful, contaminated).\n- The work draws a concrete practical implication: subspace-based defenses cannot selectively suppress harmful behavior without proportional utility loss, guiding future safety strategies away from linear subspace filtering.\n- The results provide consistent evidence in both weight and activation spaces that harmful and benign signals substantially overlap, indicating these directions are not clearly separable."}, "weaknesses": {"value": "- The focus of this work focus on defense mechanisms based on linear orthogonal projections, leaving non-linear alternatives unexplored.\n- The paper reports activation-space results as averages over a mid-depth slice (about 65–90% of layers) rather than layer by layer, which may not fully capture layer-specific behavior.\n- The paper lacks a theoretical investigation of the results, relying on empirical evidence without a formal framework explaining why safety and utility directions overlap. This is reasonable for an empirical study, but a concise theoretical model would strengthen the contribution."}, "questions": {"value": "- Can you provide layer-by-layer activation-space results to better demonstrate the layer-specific behavior.?\n- Is there a theoretical explanation or intuition for why high impact directions amplify both helpfulness and harmfulness, and under what conditions might they be separable? For example, would adding a contrastive objective during training help disentangle directions associated with helpfulness and harmfulness?\n- Can we learn a system prompt that, when used for conditioning, reduces the overlap between safety and utility directions, potentially making subspace based defenses practical even though current finetuning results do not show clear separability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mIAjkpcFcq", "forum": "Fj6LakRHcT", "replyto": "Fj6LakRHcT", "signatures": ["ICLR.cc/2026/Conference/Submission3153/Reviewer_hV1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3153/Reviewer_hV1p"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997340351, "cdate": 1761997340351, "tmdate": 1762916574357, "mdate": 1762916574357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}