{"id": "mCoJN10RV5", "number": 325, "cdate": 1756735458297, "mdate": 1759898267570, "content": {"title": "Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning", "abstract": "Diffusion Policy has dominated action generation due to its strong capabilities for modeling multi-modal action distributions, but its multi-step denoising processes make it impractical for real-time visuomotor control.\nExisting caching-based acceleration methods typically rely on $\\textit{static}$ schedules that fail to adapt to the $\\textit{dynamics}$ of robot-environment interactions, thereby leading to suboptimal performance.\nIn this paper, we propose $\\underline{\\textbf{S}}$parse $\\underline{\\textbf{A}}$ction$\\underline{\\textbf{G}}$en $(\\textbf{SAG})$ for extremely sparse action generation. To accommodate the iterative interactions, SAG customizes a rollout-adaptive prune-then-reuse mechanism that first identifies prunable computations globally and then reuses cached activations to substitute them during action diffusion. To capture the rollout dynamics, SAG parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation and instantiates it with a highly parameter- and inference-efficient design for real-time prediction. Furthermore, SAG introduces a one-for-all reusing strategy that reuses activations across both timesteps and blocks in a zig-zag manner, minimizing the global redundancy. Extensive experiments on multiple robotic benchmarks demonstrate that SAG achieves up to 4$\\times$ generation speedup without sacrificing performance.", "tldr": "Sparse ActionGen is a rollout-adaptive caching approach that accelerates action generation without sacrificing performance.", "keywords": ["Efficient AI", "robotics", "Visormotor Policy", "Diffusion Policy", "Embodied AI", "Caching", "Pruning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27335ff2efed0625dcc62434a7f99a1eb8fe8560.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Sparse ActionGen (SAG), a novel method to accelerate diffusion policies for robotic control. The authors argue that existing acceleration techniques based on static caching schedules are suboptimal because they cannot adapt to the changing dynamics of robot-environment interactions during a task rollout. To address this, SAG proposes a rollout-adaptive, prune-then-reuse mechanism. The core contributions include (1) a real-time, observation-conditioned diffusion pruner that dynamically predicts which computations can be skipped in each rollout iteration; (2) a \"one-for-all\" reusing strategy that shares a cache across both denoising timesteps and model blocks to maximize activation reuse; and (3) a global sparsity loss to train the pruner, enabling non-uniform resource allocation under a predefined computational budget. The authors evaluate SAG on several visuomotor manipulation tasks, reporting up to a 4x speedup over a full-precision baseline while claiming to maintain or even improve task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, clearly structured, and easy to follow. The problem is framed effectively by decomposing the process into rollout, denoising, and block levels. The figures, particularly the framework diagram (Figure 2), are helpful in understanding the overall approach.\n- The experimental evaluation is extensive, covering multiple benchmarks (RoboMimic, Kitchen). The ablation studies are thorough and effectively validate the importance of each proposed component."}, "weaknesses": {"value": "- **Incomplete Baseline Comparison**: The paper's experimental validation suffers from a fundamental flaw. SAG is a training-based acceleration method, requiring an explicit training phase for the pruner. However, the authors almost exclusively compare it against caching-based methods (EfficientVLA, BAC) which are largely training-free. This is an inappropriate and misleading comparison. The most relevant and powerful baselines are other training-based acceleration techniques, such as policy distillation methods. The absence of a comparison to Consistency Policy (RSS 2024) [1] is a fatal omission. Without demonstrating superiority over such methods, the central motivation for proposing this complex pruning mechanism is fundamentally questionable. The authors must justify why their approach is necessary when simpler and potentially more effective distillation-based solutions exist.\n\n- **Inadequate Literature Review**: The literature review in the \"Related Work\" section is incomplete. The authors fail to cite several highly relevant and recent works on diffusion policy acceleration, such as Streaming Diffusion Policy (ICRA 2025) [2] and Falcon (ICML 2025) [3]\n\n- **Questionable Empirical Claims and Analysis**: The paper reports that SAG consistently outperforms the full-precision baseline (e.g., 94% vs. 90% in Table 1 (Square), 84% vs. 80% in Table 1 (Transport), and 54% vs. 50% in Table 1 (Tool). This is a counterintuitive result that is presented without any plausible explanation. Such a claim requires extraordinary evidence, yet the paper provides no statistical analysis (e.g., standard deviations over multiple seeds) to verify that this improvement is not simply an artifact of evaluation variance. This unsubstantiated claim undermines the credibility of the results.\n\n- **Lack of Real-World Validation**: As a robotics paper, the paper does not have any real-world experiments.\n\n\n[1] Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation: https://arxiv.org/abs/2405.07503\n\n[2] Fast Policy Synthesis with Variable Noise Diffusion Models: https://arxiv.org/abs/2406.04806\n\n[3] Falcon: Fast Visuomotor Policies via Partial Denoising: https://arxiv.org/abs/2503.00339"}, "questions": {"value": "1. Your method is a training-based approach. Why did you not compare it against other SOTA training-based acceleration methods, most notably Consistency Policy? Please provide a detailed comparison. What are the specific advantages (e.g., training cost, final performance, inference speed) of your prune-then-reuse paradigm over policy distillation?\n\n2. Given that this paper is positioned to address a challenge in robotics, can you provide the results in the real-world experiments?\n\n3. The results in Tables 1 and 2 show that SAG achieves a higher success rate than the full-precision model, which is paradoxical for a method based on pruning. Could you offer a concrete hypothesis for this phenomenon? More importantly, please provide standard deviations across multiple random seeds for both your method and the baselines to demonstrate that this performance gain is statistically significant.\n\n4. Can you provide some demos on the comparison of your method and diffusion policy?\n\nI will adjust my future rating based on the improvements made. If I have misunderstood any points, I am open to discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ULm2V7rhMR", "forum": "mCoJN10RV5", "replyto": "mCoJN10RV5", "signatures": ["ICLR.cc/2026/Conference/Submission325/Reviewer_ZwXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission325/Reviewer_ZwXH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761146138790, "cdate": 1761146138790, "tmdate": 1762915493852, "mdate": 1762915493852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Sparse ActionGen (SAG), a rollout-adaptive acceleration framework for diffusion-based visuomotor policies. The key idea is a prune-then-reuse pipeline that (i) predicts an observation-conditioned pruning mask in real time and (ii) performs one-for-all (cross-timestep & cross-block) activation reuse in a zig-zag pattern. Empirically, the authors report up to 4× speedup without sacrificing task performance on RoboMimic and Franka Kitchen tasks, outperforming static schedule baselines and an adapted learning-to-cache method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem framing and motivation.\nThe paper grounds the latency issue of diffusion policies in realistic control frequencies (e.g., 50 steps × 1 ms ≈ 50 ms → 20 Hz on RTX 4090; insufficient for Franka 50–1000 Hz), which is a compelling, concrete rationale for acceleration beyond image generation settings. \n\n\n2. Methodological novelty: observation-conditioned, real-time pruning.\nThe real-time diffusion pruner predicts a binary mask for all K timesteps and 3L blocks in a single forward pass, using sinusoidal encodings for timestep/block indices and concatenating them with an observation embedding; the authors emphasize <0.3% FLOPs overhead. This is a principled departure from fixed schedules or post-hoc profiling. \n\n\n3. Global prune–then–reuse design.\nThe paper formalizes a global sparsity loss (Eq. 10) to directly enforce a target global pruning rate ρ across timesteps and blocks, combined with a policy fidelity term; this encourages non-uniform, importance-aware allocation rather than uniform per-block pruning. \n\n\n4. One-for-all reusing strategy (cross-block + cross-timestep).\nInstead of independent per-block caches, SAG maintains shared buffers by block type and reuses activations across blocks/timesteps in a zig-zag manner, with explicit residual/cache update equations (Eqs. 12–13). This directly targets cross-block redundancy that prior work largely ignored. \n\n\n5. Comprehensive and favorable empirical results.\nAcross RoboMimic PH/MH and Franka Kitchen, SAG consistently matches or exceeds full-precision success while achieving ~3.6–4.0× speedups; in Kitchen, the paper reports 4.03× with no drop in success. Baselines include EfficientVLA, BAC, and L2C with task details and implementation notes."}, "weaknesses": {"value": "1. Lack of real-robot validation.\nAll evaluations appear to be simulation-based (RoboMimic tasks, Franka Kitchen). For claims of real-time control, a small-scale hardware validation (latency stability, sensor noise, control jitter) would substantially strengthen the case.\n\n\n2. Runtime analysis is mostly relative; absolute latencies are under-reported.\nWhile speedup factors are clear, the paper would benefit from absolute inference time per control step (ms) and achieved control frequency (Hz) for each task/checkpoint. The introduction’s RTX 4090 example motivates this well, but the same concreteness is not mirrored in the results tables. \n\n\n3. Comparisons omit distillation-based accelerators.\nThe baselines focus on caching/static-schedule families. A head-to-head with Consistency Policy / One-Step Diffusion Policy / SDM-style distillation (even on a subset) would provide a fuller picture of the accuracy-speed trade space relative to single-step or few-step policies. (The current baseline suite and settings are otherwise described fairly.) \n\n\n4. Algorithmic clarity for zig-zag reuse could improve.\nFigure 4(b)(c) and the textual description are helpful, but a short pseudocode or explicit cache-update schedule table per timestep/block would further de-ambiguate when/where each cached activation is read/written under pruning decisions. \n\n\n5. “Lossless” wording is a bit strong.\nThe tables show parity or gains on average, but some tasks are sensitive; it would be safer to qualify “losslessly” and include statistical tests / confidence intervals for success rates.\n\n\n6. Limited dataset diversity and absence of more complex benchmarks (e.g., MimicGen).\nThe experiments are conducted only on RoboMimic (PH/MH) and Franka Kitchen datasets, which—while standard—primarily consist of single-object, short-horizon or mid-horizon manipulation in relatively clean and scripted environments. Recent benchmarks such as MimicGen, Push-T, or real-world multi-object datasets provide more dynamic, contact-rich, and compositional scenarios with greater visual and temporal complexity. The paper does not test SAG in these settings, so it remains unclear whether the proposed pruning-and-reuse strategy generalizes to noisier demonstrations, variable robot embodiments, or human-generated suboptimal trajectories typical in MimicGen. This makes the evaluation somewhat limited to “clean” datasets and may overestimate robustness."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "knAklL6cyX", "forum": "mCoJN10RV5", "replyto": "mCoJN10RV5", "signatures": ["ICLR.cc/2026/Conference/Submission325/Reviewer_xsZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission325/Reviewer_xsZQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960657361, "cdate": 1761960657361, "tmdate": 1762915493687, "mdate": 1762915493687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Sparse ActionGen (SAG)**, which employs a **rollout-adaptive prune-then-reuse mechanism**. The framework integrates an **observation-conditioned diffusion pruner** for environment-aware adaptation and a **one-for-all reuse strategy** within the overall pipeline. Extensive experiments on the **RoboMimic** and **Franka Kitchen** benchmarks demonstrate that **SAG** achieves superior performance compared to existing **diffusion policy caching** and **acceleration** methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a **highly important and timely research problem**, especially as diffusion models continue to gain prominence in **imitation learning**, **reinforcement learning**, and **Vision-Language-Action (VLA)** modeling.\n\n2. The paper presents **extensive simulation experiments**, offering strong empirical evidence for the effectiveness and robustness of the proposed method.\n\n3. The paper is **well-written**, **clearly structured**, and **easy to follow**, effectively communicating the technical contributions and motivations."}, "weaknesses": {"value": "### Major Weakness:\n\n1. The authors are strongly encouraged to include comparisons with traditional **diffusion acceleration methods**, such as **DDIM** or **Consistency Policy**, to enhance the **completeness** and **thoroughness** of the paper’s experimental evaluation.\n\n2. It is noted that in RoboMimic tasks, SAG even achieves higher performance compared to Diffusion Policy with full denoising process. The authors are highly recommended to dive deeper into this phenomenon instead of just concluding it as \"surprisingly\".\n\n3. The authors should report the **number of random seeds** used in the experiments and provide the corresponding **standard deviations** of the success rates.\n\n4. The authors are encouraged to include a comparison of the **actual running time** between **SAG** and the baseline methods. This would provide a more intuitive understanding of the acceleration achieved, beyond simply reporting the speedup ratio.\n\n\n### Minor Weakness:\n\n1. The authors are encouraged to include **real-robot experiments** to further validate the effectiveness of the proposed **SAG** method. It is understood, however, that conducting such experiments may not be feasible within the rebuttal period if the necessary robotic setup is not yet available in the lab, so this addition is not required.\n\n2. The authors are recommended to discuss more categories of diffusion policy acceleration methods:\n\nHøeg, Sigmund H., Yilun Du, and Olav Egeland. \"Streaming diffusion policy: Fast policy synthesis with variable noise diffusion models.\" arXiv preprint arXiv:2406.04806 (2024).\n\nChen, Zhuoqun, et al. \"Responsive noise-relaying diffusion policy: Responsive and efficient visuomotor control.\" arXiv preprint arXiv:2502.12724 (2025)."}, "questions": {"value": "1. **(Related to Weakness 2)** Could the authors provide a deeper explanation of why **SAG** outperforms **Diffusion Policy** with the full denoising process in the **RoboMimic** tasks?\n\n2. Could the authors elaborate further on **Figure 1** and clarify how the corresponding experiments were conducted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ao3CK8mOHs", "forum": "mCoJN10RV5", "replyto": "mCoJN10RV5", "signatures": ["ICLR.cc/2026/Conference/Submission325/Reviewer_ajUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission325/Reviewer_ajUH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989910717, "cdate": 1761989910717, "tmdate": 1762915493567, "mdate": 1762915493567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to learn a Diffusion policy with sparse activations conditioned on the current observation. The motivation for using a Diffusion policy is that Diffusion networks are good at modelling multi-modal distributions, and the motivation for conditioning the pruning of computations during the denoising process is that it might provide a better speedup/performance trade-off than the previously proposed static pruning mechanisms. \n\nAt the heart of the method is an architecture that predicts an observation conditioned sparsity pattern for the K diffusion steps and the several computational blocks within each diffusion step. The sparsity pattern is binary and if a computational block is deactivated, the output is replaced with a cached value computed online from the outputs of past blocks. The whole policy is trained end-to-end with a behavioral cloning loss and a sparsity enforcing loss. Results show on several imitation tasks that the method achieves better performance and speed-up compared to other pruning methods for Diffusion networks. An ablation study also highlights the importance of each introduced design choice (observation conditioned sparsity pattern, the specific caching mechanism and the use of a global sparsity loss instead of enforcing sparsity within each block)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "All design choices seem reasonable and are properly ablated, and overall improvement over other diffusion pruning methods seems substantial."}, "weaknesses": {"value": "- I found the motivation in the abstract and introduction at odds with the actual algorithm. The authors motivated the study of Diffusion policies by their ability to model multi-modal distributions. Since many deep RL algorithms' exploration heuristics use stochastic policies, it seems indeed important to have policies that can model a wider range of distributions. However, the authors only consider behavioral cloning of an expert policy, and while multi-modal and stochastic policies might be helpful during reinforcement learning, it is debatable whether a Diffusion policy is really necessary for imitation. Nonetheless, even if we consider a setting where one wants to imitate a distribution of experts, perhaps where each expert is using a distinct strategy to solve the task and thus necessitating a multi-modal approach, I fail to see how would the loss in Eq. 9 capture this multi-modality. It seems that it might only end-up approximating the median or mean of the distribution as is. Thus I question whether the Diffusion network is really capturing multi-modal distributions as motivated in the abstract. \n\n- Given the previous point, it would have been good to have baselines using other policy classes than Diffusion policies. I am not familiar with the considered tasks and datasets, and given the current state of the experiment section, it is hard to get a sense on whether the proposed method is improving the state-of-the-art of behavioral cloning in terms of performance or inference speed. Especially since even during inference, the method needs to compute a pruning mask at each time-step and this model is already quite deep.\n\n- The experiment discussed in Sec. 2.2 does not seem to follow sound methodology. The goal of the experiment is to motivate the use of an observation conditioned pruning mask compared to a fixed pruning mask, but the authors use randomly generated pruning masks to prove their point. I think at the very least, one of the the masks should be optimized using existing methods instead of being generated randomly, otherwise it does not negate the possibility that there exist a fixed mask that would work well for all rollout steps."}, "questions": {"value": "- How multi-modal does the policy end-up being? And how important is it to learn multi-modal policies for the considered tasks?\n- How does the Diffusion policy compare to other architecture in terms of performance/inference speed? For which applications does it become an interesting choice?\n- Why did you choose random masks in Sec. 2.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "64HQNajoQI", "forum": "mCoJN10RV5", "replyto": "mCoJN10RV5", "signatures": ["ICLR.cc/2026/Conference/Submission325/Reviewer_Ud3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission325/Reviewer_Ud3K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183377038, "cdate": 1762183377038, "tmdate": 1762915493389, "mdate": 1762915493389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}