{"id": "AlYT0ZD51A", "number": 17618, "cdate": 1758278408508, "mdate": 1759897164510, "content": {"title": "Convergence Analysis of Nesterov's Accelerated Gradient Descent under Relaxed Assumptions", "abstract": "We study convergence rates of Nesterov's Accelerated Gradient Descent (NAG) method for convex optimization in both deterministic and stochastic settings.\nWe focus on a more general smoothness condition raised from several machine learning problems empirically and theoretically.\nWe show the accelerated convergence rate of order $\\mathcal{O}\\left(1/T^2\\right)$ in terms of the function value gap, given access to exact gradients of objective functions, matching the optimal rate for standard smooth convex optimization in \\citep{nesterov1983method}.\nUnder the relaxed affine-variance noise assumption for stochastic optimization, we establish the high-probability convergence rate of order $\\tilde{\\mathcal{O}}\\left(\\sqrt{\\log\\left(1/\\delta\\right)/T}\\right)$ and this rate could improve to $\\tilde{\\mathcal{O}}\\left(\\log\\left(1/\\delta\\right)/T^2\\right)$ when the noise parameters are sufficiently small.\nHere, $T$ denotes the total number of iterations and $\\delta$ is the probability margin.\nUp to logarithm factors, our probabilistic convergence rate reaches the same order of the expected rate obtained in \\citep{ghadimi2016accelerated} where the assumptions of  bounded variance noise and Lipschitz smoothness are required.", "tldr": "", "keywords": ["Nesterov's Accelerated Gradient", "Convergence Theory", "Convex Optimization", "Stochastic Optimization", "Affine Variance"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d7a1a26f779a8602f5d7658c51bbcb6bd6f768b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides an analysis of Nesterov's accelerated gradient descent method for stochastic first-order optimization with weakened assumptions on both the smoothness of the objective and the bounds on the noise in the gradient. In my opinion, the paper makes a nice theoretical contribution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem addressed is interesting, especially since Nesterov's method is widely used in machine learning. The technical assumptions are explained fairly well and the relationship with the existing literature is well-documented. The technical arguments appear correct to me as far as I can tell."}, "weaknesses": {"value": "There are a few typos and issues with the presentation which I have also detailed below. These issues can be easily addressed, I think.\n\nThe work is quite technical and could be perceived as an incremental improvement over existing analyses. In my opinion, the paper would benefit if the authors would better describe the novel technical ideas which distinguish their approach from existing analyses. For example, could this be explained in the introduction, in addition to stating the new results?\n\nNumerical experiments are missing."}, "questions": {"value": "I think there is a typo in equation (2) (should the RHS be multiplied by |x-y|?)\n\nIn points (c) and (d) in the introduction, what are A,B, and C. It appears to me that only B has been defined in equation (4). I see these parameters have been introduced later in Assumptions 3 and 4, but I think it would made more sense to state it earlier in the paper.\n\nI'm curious what results can be obtained in the strongly convex case. Do you have any ideas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KU8fi4DHFG", "forum": "AlYT0ZD51A", "replyto": "AlYT0ZD51A", "signatures": ["ICLR.cc/2026/Conference/Submission17618/Reviewer_rtaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17618/Reviewer_rtaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681588932, "cdate": 1761681588932, "tmdate": 1762927482011, "mdate": 1762927482011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the convergence speed of the Nesterov accelerated gradient (NAG) algorithm applied to convex functions, that are not assumed to be L-smooth as it is standard in many former analysis, but instead verify a generalized smoothness assumption. The authors also study the convergence of a stochastic version of this algorithm, under \"relaxed-affine variance\"-like assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The optimal $1/T^2$ rate of NAG for smooth convex functions is an important result in optimization. As recent works suggest generalized smoothness assumption fits some non convex optimization problems (e.g. neural network) better that the classical L smooth assumption, it is indeed interesting to see that NAG preserves its acceleration property over gradient descent. Also, the noise assumption on the gradient is more general that many others considered in other works (bounded variance, strong growth condition, etc.). In this regard, this work is a nice generalization of existing works in the deterministic and stochastic cases."}, "weaknesses": {"value": "- The authors claim that their first contribution (contribution (a)) is to provide \"a more general and realistic smoothness condition\". \nIf it is obvious that it is more general, the fact that is it more realistic than former concept of generalized smoothness [Zhang et al.\n2020] seems not justified in the paper. If this claim remains in the paper, I suggest to clarify why and in which context Definition 1 is more realistic.\n\n- Under Assumption 4 with $A = C = 0$ (strong growth condition), it is known in the smooth convex setting that we can obtain a $O(1/T^2)$ rate, without the additional $O(1/\\sqrt{T})$ term. In your result, specifically in Theorem 3, the bound does not reduces to $O(1/T^2)$ in this setting, making the result sub optimal in this specific case. Is it a limit of your analysis ? I think it is not a major concern, but I suggest to at least briefly discuss it, e.g. under Theorem 3. \n\n- In the end of Section 4, \"our smoothness assumption could be more general\". I think it is important to clarify this statement, specifically what is meant by \"could\". As written, it is thus unclear how it extends existing works.\n\n- On page 1: \"Furthermore, this complexity bound is known to be optimal for large enough dimension d,\nas shown by (Nemirovskij, Yudin, 1983), without further assumptions.\" I think the sentence could be misleading. Please precise it is optimal among gradient based algorithms.\n\n- I believe there is a major typo in (2), where a $|| x-y ||$ factor is missing. This typo affects the further discussion.\n\n- In section 1, \"As usual, one typically focuses on the function value gap for convex objectives and the squared gradient norm for non-convex ones.\" I know the authors want to stay evasive as it is an introduction, but the second part of the sentence is a bit too evasive to me. There is a whole litterature about minimization of non-convex function such that the focus remains the function value gap, e.g. for Polyak Lojasiewicz functions [Karimi et al.], (strongly) quasar convex functions [Hinder et al.], (strongly) quasiconvex functions [Grad et al.]. I suggest to clarify a bit this statement.\n\n\nMinor points and typos.\n\n- P.5 219-220 \", while for the stochastic case in Section 4.2.\" seems grammatically not correct.\n\n- On page 2: \"Assumption 2 is commonly used in the analysis for stochastic optimizations.\". I also suggest to mention that it is a relevant assumption to study many practical setting, which is even better than simply being commonly used in former analysis.\n\n- In section 6 about \"$\\mathcal{F}_2$\", please avoid to use notations in the main text without at least referring the equation which defines it.\n\nReferences: \n\n-Hamed Karimi, Julie Nutini, Mark Schmidt, \"Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition\", 2016\n\n-Oliver Hinder, Aaron Sidford, Nimit S. Sohoni, \"Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond\", 2019\n\n-Sorin-Mihai Grad,  Felipe Lara Raul T. Marcavillaca,  \"Strongly quasiconvex functions: what we know (so far)\", 2024"}, "questions": {"value": "- Last sentence of Section 2 \"(Wang et al., 2023) gave a counter-example showing the necessity of prior knowledge on problem parameters for step sizes under (L0, L1 )-smoothness.\". Do you mean that adaptive method to compute parameters are not suited to generalized smoothness assumptions ? The sentence seems a bit evasive to me.\n\n- On page 2: \"Obviously, Definition 1 covers a broader range of relaxed smoothness than the generalized smoothness\". I think it is important to at least refer here to your Appendix A, currently not referred in your main text. Could you please give some intuition about which kind of functions verify your condition and will not verify the former condition of Zhang et al. (2020)? Is the function introduced in Appendix A such an example ? \n\n- Could you please give a bit of intuition about how much Assumption 3 is stronger than Assumption 4 ? For example, could you provide an example of a setting where Assumption 4 is verified and not Assumption 3 ?\n\n- Could you justify why you call Assumption 4 \"Relaxed affine variance-noise\", while it is introduced as \"Expected smoothness\" in (Khaled & Richtarik, 2023)?\n\n- By curiosity, replacing Assumption 3 by Assumption 4 in Theorem 2, can you obtain a bound in expectation trivially with your current analysis ? \n\n\n- The work [Hermant et al.] shows that a stochastic version of NAG for convex and L-smooth functions, under the strong growth condition ($A=C=0$ in your Assumption 4), achieves a $o(1/T^2)$ rate almost surely. Do you think such almost sure rate could be deduce in your setting ? \n\n- As an opening question, [Guillet et al.] shows that the L-smooth assumption is, compared to other assumption, not \"stable\". Do you think generalized smoothness assumptions such as yours avoid this problem ?\n\nReferences: \n\n-Ahmed Khaled, Peter Richtárik, \"Better Theory for SGD in the Nonconvex World\", 2020\n\n-Julien Hermant, Marien Renaud, Jean-François Aujol, Charles Dossal, Aude Rondepierre, \"Gradient correlatio is a key ingredient to accelerate SGD with momentum\", 2025\n\n-Charles Guille-Escuret, Baptiste Goujaud, Manuela Girotti, Ioannis Mitliagkas, \"A Study of Condition Numbers for First-Order Optimization\", 2020"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MP8yZPOBJf", "forum": "AlYT0ZD51A", "replyto": "AlYT0ZD51A", "signatures": ["ICLR.cc/2026/Conference/Submission17618/Reviewer_bvGq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17618/Reviewer_bvGq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746305033, "cdate": 1761746305033, "tmdate": 1762927481605, "mdate": 1762927481605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is concerned with the convergence analysis of Nesterov Accelerated Gradient Descent  (NAG) both in deterministic and stochastic settings, under a convexity assumption. The main contribution of the paper is to relax the classical assumptions made on the function to minimize. The classical assumtion is that the function is L-smooth (i.e. that its gradient is L-Lipshitz). Howerver, such an assumption is not satisfied in many machine learning problems. The author thus replace it with a more general (and also technical one).\nNevertheless, the authors are able to show that they get the same type of speed of convergence as in the classical case."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Minimisation of large dimension functions is a bottlneck in machine learning at the moment. The main tools used to tackle it are first order methods, and Nesterov acceleration is therefore one of the most promising venue. The main drawback of NAG is its setting (assumtions on the function to minimize) that is too strong for machine learning. Relaxing this setting is therefore major issue at the moment within the optimization/machine learning community.\n\n* Both detremintic case and stochastic cases are considered. The deterministic setting is easier to understand, and it helps understanding the stochastic one (which is the one used in practive for machine learning problems).\n\n* Most of the obtained convergence results are optimal."}, "weaknesses": {"value": "* Relaxing the L-smoothness assumption is indeed a good point. However, the main limitation when using NAG is the convexity assumption. This convexity assumption is of course not satisfied by machine learning problems. As a consequence, this limits the interest of the work, since even by relaxing the L-smoothness assumption, we are still far from the setting that happens in practice for machine learning problems.\n\n* The proofs are technical and sometimes hard to follow. In the deterministic case, would it be possible to derive a continuous analysis (with some ODEs) ? see e.g. the paper by Su, Boyd and Candes\nhttps://arxiv.org/abs/1503.01243\n\n* The authors have made a very nice bibliography work. However, they could also refer to an ICLR 2025 paper :\nGradient Correlation is a key ingredient to accelerate SGD with momentum, by Hermant et al\nhttps://arxiv.org/abs/2410.07870"}, "questions": {"value": "* There are several updates rule for NAG (see e.g. the last reference of the previous section).\nAre the results of the paper also true for those other update rules, or are the proofs specific to the choice made in the paper ?\n\n* The authors quickly explain the difference of their proof with respect to the classical assumption of L-smoothness in section 6.3\nI followed the proof in the appendix, but I could not get whether the new proof can be split into 2 parts (or if they are mixed):\na first part to get some control of the gradient thanks to the new assumption, and then a second part which would basically be the classical proof once the estimate of the gradient has been obtained.\nIf so, this would give a framework to relax even further the L-smoothness assumption.\n\n* I have some difficulties understanding the class of functions that satisfy the new smoothness assumption (which is much more involved than the classical L-smoothness assumption). I found the example in the appendix a good idea to understand further what it means. However, are there some more intuitive general properties that are satisfied by such functions ?\nThis would help understanding how much the set of functions has been extended thanks to the relaxation of the smoothness property.\n\n* Any hint on how to get rid of the convexity assumption ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ywsckAypE", "forum": "AlYT0ZD51A", "replyto": "AlYT0ZD51A", "signatures": ["ICLR.cc/2026/Conference/Submission17618/Reviewer_t1mr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17618/Reviewer_t1mr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767793602, "cdate": 1761767793602, "tmdate": 1762927481213, "mdate": 1762927481213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present a new convergence proof for the Nesterov accelerated algorithm (NAG) for convex function in both deterministic and stochastic setting under relaxed assumption. In particular, a new relaxed assumption of the Lipschitz smoothness of the fonction is introduced and acceleration is proved under this new assumption. The work focuses on the convergence of NAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A new convergence result of NAG in both deterministic and stochastic setting is proved.\n- The proofs seem correct."}, "weaknesses": {"value": "Major Weaknesses:\n- The assumption of this work are far from real-world machine learning applications. First, the fonction needs to be convex which is not verified for neural network training. Then, the relaxed assumption introduced in this work is verified by a very particular neural network detailed in Appendix A. However, it is not clear why this \"relaxed assumption\" is interesting for real-world applications. This \"relaxed assumption\" of smoothness is only motivated by the fact that it is \"relaxed\".\n- No experiments or real-world applications of the convergence results are provided reducing strongly the impact of the work.\n- The paper is not pedagogical. In particular, I appreciate the effort to give insides on the proof of Theorem 2 in Section 2. However, Section 6 is very technical and unclear for me without looking at the detailed proof in Appendix. I suggest to remove it of the main paper.\n\nMinor Weaknesses:\n- In the introduction [e.g. in Contributions (c), (d) or in line 166], you mention $A$, $B$, $C$ the constants involves in Assumption 4 but without introduced it before. I suggest to move Assumption 4 around equation (4) in order to state notations before using them.\n- Some notations are confusing. In Algorithm 1, you use $x_t$, $x_t^{md}$ and $x_t^{ag}$ without introducing what denote \"md\" and \"ag\". I will suggest to use different letters for each of these intermediary sequence, e.g. $x,y,z$. Moreover, I suggest to state explicitly that $x_t^{ag}$ is the output of Algorithm 1.\n\nTypos/imprecisons:\n- Definition 1: because $\\|x-y\\| \\le \\min\\{\\frac{1}{L_1}, \\frac{1}{L_2}\\}$, you need that $L_1, L_2 > 0$ not only $L_1, L_2 \\ge 0$. Or could you precise that you mean by $1/0 = +\\infty$.\n- Line 351 : What is $\\mathcal{F}_2$ ? Could you add the definition in section 6 ?\n- Equation (19): It seems that $\\tilde{d} = d\\times m$, could you state it for clarity ? The weights $a_j$ seem to not be trainable (it is the setting of [1]), could you state it clearly ?\n- Line 593. In your notation, according to [1], it should be $w_i$ instead of $x_i$ in the definition of $R$.\n- Lemma A.2, $F^\\star$ has not been introduced before in Section A.\n- Line 632-635: It is not clear for me why the constraint $L_2\\log{L_2} \\ge h \\log{H}$ implies $L_2 \\ge H \\exp{\\frac{h}{L_2}}$. Could you detail this point ?\n- Could you provide a reference for the proof of Lemma B.1 ?\n- In the proof of Lemma B.3, by applying the Young inequality, you assume that $p$ and $q$ are strictly positive which is not the case in your definition of $(L_0, L_1, L_2)$-smoothness. Could you precise this case when $p = 0$ or $q = 0$ ?\n- Line 802: it is not evident for me that $\\beta \\sqrt{g(\\mathcal{F}_1)}$ is smaller that $\\min\\{1/L_1, 1/L_2\\}$, could you justify this statement ?\n- Line 846: I suggest to recalled the definition of $\\lambda_l$ for clarity\n- Line 248: The point 2. of Lemma 4.1 could be precise into $B_t < A_t$ since the step-size is strictly positive. It will simplify the computation in line 848.\n- Line 852: I suggest to recall the definition of $\\beta$ for clarity.\n- Lemma D.2: The definition of $\\xi_k$ is not provided.\n- Line 1102: I will detail that you use the definition of $\\beta$ and in particular the inequality with $\\mathcal{G}_{1,1}$.\n- Line 1161: It should have a $\\frac{1}{2}$ factor in front of the term in $\\|\\xi_l\\|^2$\n- Line 1166: I suggest to recall the setting of $\\lambda_l$ for clarity as it is changing through theorems.\n- Line 1178: The definition of $\\beta$ related to $\\mathcal{G}_{1,3}$ is used implicitly, could you make it explicit ?\n- Line 1093-1095: The introduction paragraph is a bit confusing. I suggest to reformulate with an explicit use of Lemma D.1\n- Equation (66), what is the definition of $t$ ? I think it should be $T$ instead.\n- Line 1214: It is not very precise. Equation (59) is used but not Lemma D.5. You should state clearly that equation (51) implies (59), which is used there.\n- Lemma D.7 in line 1292: You use Equation (66) (of Lemma D.6) to demonstrate this lemma. However, Equation (66) is derived under the hypothesis that $f(x_{k}^{md}) - f^\\star \\le \\mathcal{F}_2$ which is the inequality that you aim to demonstrate in Lemma D.7. I suspect a circular argument. Can you make explicit in each Lemma (D.5, D.6, D.7) which are the assumptions and re-write the paragraph in line 1292 to clarify the arguments ?\n\n[1]  Hossein Taheri and Christos Thrampoulidis. Fast convergence in learning two-layer neural networks with separable data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp.9944–9952, 2023."}, "questions": {"value": "- Could you comment how this paper is relaxed with the following works ? \nY. Carmon, J.C. Duchi, O. Hinder, and A. Sidford, “Lower bounds for finding stationary points,” Mathematical Programming, vol. 184, no. 1-2, pp. 71–120, 2020.\nJ. Hermant, M. Renaud, J.F. Aujol, C. Dossal, A. Rondepierre, \"Gradient correlation is a key factor to accelerate SGD with momentum\", ICLR 2025\nY. Hong, J. Lin \"On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions\", Neurips 2024\n- In section A, an example of function (L_0, 0, L_2)-smooth is given. What is the practical interest of the relaxed $L_1$, do you have concrete example of function which are (L_0, L_1, L_2)-smooth and not (0, L_1, L_2)-smooth or (L_0, 0, L_2)-smooth or (L_0, L_1, 0)-smooth ?\n- In which real-world application, the relaxed smoothness assumption appears ? What is the motivation for looking at this assumption ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9bk47EyOZA", "forum": "AlYT0ZD51A", "replyto": "AlYT0ZD51A", "signatures": ["ICLR.cc/2026/Conference/Submission17618/Reviewer_Rmjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17618/Reviewer_Rmjo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928984277, "cdate": 1761928984277, "tmdate": 1762927479232, "mdate": 1762927479232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}