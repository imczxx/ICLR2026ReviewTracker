{"id": "kWM0etSpBG", "number": 10296, "cdate": 1758166364734, "mdate": 1763708216490, "content": {"title": "Evaluating GFlowNet from partial episodes for stable and flexible policy-based training", "abstract": "Generative Flow Networks (GFlowNets) were developed to learn policies for efficiently sampling combinatorial candidates by interpreting their generative processes as trajectories in directed acyclic graphs. In the value-based training workflow, the objective is to enforce the balance over partial episodes between the flows of the learned policy and the estimated flows of the desired policy, implicitly encouraging policy divergence minimization. The policy-based strategy alternates between estimating that divergence and updating the policy, but reliable estimation of the divergence under directed acyclic graphs remains a major challenge. This work bridges the two perspectives by showing that flow balance also yields a principled policy evaluator that measures the policy divergence, and an evaluation balance objective over partial episodes is proposed for learning the evaluator.  As demonstrated on both synthetic and real-world tasks, the flow balance condition not only strengthens the reliability of policy-based training but also broadens its flexibility by seamlessly supporting parameterized backward policies and enabling the integration of offline data-collection techniques.", "tldr": "", "keywords": ["Probabilistic Inference", "Reinforcement Learning", "Combinatorial Optimization"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5258a2019dcc5b41bd0621d7613838bd3aee9464.pdf", "supplementary_material": "/attachment/4fef3a8fa9432a162e35523abb9d2f4d598a8286.zip"}, "replies": [{"content": {"summary": {"value": "The focus of this paper is policy gradient based learning of GFlowNets. The paper presents a sub-trajectory based loss for learning the evaluation function of a GFlowNet forward policy. The evaluation function is also theoretically connected to flow functions of GFlowNets. The learnable evaluation function is then utilized in the policy gradient algorithm to train the GFlowNet forward policy. The algorithm also allows for learning the backward policy. The approach is experimentally evaluated on hypergrids, biological sequence environments and bayesian structure learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an interesting and important direction of applying policy gradient methods to GFlowNets. I also found the presented connections between evaluation functions and flows to be interesting and insightful (although not entirely new, see Weaknesses)."}, "weaknesses": {"value": "The main weakness of this work in my opinion is its limited novelty. The definition of evaluation function (Equation 3) coincides with the negative entropy-regularized value in the construction that frames GFlowNet learning as an entropy-regularized RL problem from [1]. The connections between entropy-regularized values and flows were also previosly presented in [1] (see Theorem 1 and Proposition 1 of [1]). Theorem 3.1 in the present paper can be seen as a generalization of results connecting flows and values from Theorem 1 and Proposition 1 of [1], however, from what I understand, this can be shown by a simple modification of the proof of Proposition 1 from [1]. In addition, such contributions and their novelty must be correctly framed with respect to the previous literature, while the work [1] is neither discussed nor cited in this paper.\n\nThe proposed SubEB objective (Equation 8) exactly coincides with SubTB loss [2], which is already well-known and widely studied in the GFlowNet literature. The only difference is that this loss is used to learn only flows/values in the proposed algorithm for the current policy, while the policy uses a different expression for gradient update. The policy gradient training of GFlowNets itself is also not new and was previously studied in [3]. From what I understand, the difference of the proposed algorithm from the algorithm in [3] is a different way to evaluate states/trajectories (by learning the evaluation function via SubEB/SubTB).\n\nThe writing in this work seems unclear in a number of places. In line 46 it is stated that the direct optimization of distributional divergences between forward and backward trajectory distribution is not possible since the normalizing constant is unkown. However, for example, the normalizing constant does not affect the optimization of KL divergence, and directly optimizing it is possble (see [4]). Moreover, in line 345 the authors themselves acknowledge that the are works that directly optimize this KL divergence. I also found the writing regarding backward policy optimization in Section 3.2 and Section 3.3 to be a bit jumbled up and hard to get through (e.g. $W$ is used in the text prior to its definition), so the clarity could be improved here in my opinion. However, since this is mostly a retelling of the results from the prior work [3], I think this is a minor point. In addition, Algorithm 1 does not specify how the gradients wrt forward policy are calculated. Does it use the expression from Equation 4?\n\nAnother minor point, but since the paper partially focuses on the backward policy optimization, it would be good to cite and discuss prior works on this topic in the text [5, 6].\n\nNext, the presented empirical results on sequence design tasks, which are the only non-synthetic experiments in the main text, are unconvincing. The improvements are very marginal in comparison to SubTB in Figure 3 in the case when the local search is used. When it is not used, the improvements are notable only in the first two plots. \n\nIn addition, all the experiments in the main text are on environments of a very small scale: 256 × 256 grid (65 thousand terminal states), 64 x 64 x 64 grid (262 thousand terminal states), SIX6 (65 thousand terminal states), QM9 (58 thousand terminal states), PHO4 (1 million terminal states), sEH (34 million terminal states). All of these environments can be considered toy since the space of objects we work with is small enough that it is possible to iterate over all objects in reasonable time, thus the task of training a GFlowNet policy for sampling is artificial and redundant. There are experiments on a larger-scale structure learning task in Appendix, so I would suggest moving them to the main text and putting more emphasis on them.\n\nOverall, I found this to be an interesting paper, but the presented contributions in my opinion are not enough to recommend acceptance to a conference of this level. \n\nReferences:\\\n[1] Tiapkin et al. Generative Flow Networks as Entropy-Regularized RL. AISTATS 2024\\\n[2] Madan et al. Learning GFlowNets from partial episodes for improved convergence and stability. ICML 2023\\\n[3] Niu et al. GFlowNet Training by Policy Gradients. ICML 2024\\\n[4] Malkin et al. GFlowNets and variational inference. ICLR 2023\\\n[5] Jang et al. Pessimistic Backward Policy for GFlowNets. NeurIPS 2024\\\n[6] Gritsaev et al. Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization. ICLR 2025"}, "questions": {"value": "0) See Weaknesses.\n\n1) Why are there expected values in Equations 1 and 5? Subtrajectory balance conditions mean that the forward and backward subtrajectory flows should be equal for all possible subtrajectories, not only on average (see [1]).\n\nReferences:\\\n[1] Madan et al. Learning GFlowNets from partial episodes for improved convergence and stability. ICML 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vLqoOWaWAc", "forum": "kWM0etSpBG", "replyto": "kWM0etSpBG", "signatures": ["ICLR.cc/2026/Conference/Submission10296/Reviewer_C8bA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10296/Reviewer_C8bA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248340720, "cdate": 1761248340720, "tmdate": 1762921646498, "mdate": 1762921646498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a novel algorithm for GFlowNet training, based on a policy-iteration-style framework, where the SubTB/PCL-style loss is used only to optimize the value and backward policy. In contrast, a policy gradient objective is applied to optimize the forward (generative) policy with an additional GAE-style variance reduction. The authors provided experimental validation of their method across several tasks, including hypergrids, sequence design, and Bayesian network learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The first (to my knowledge) policy-iteration style approach for GFlowNet training, which shows a consistent performance on various benchmarks, including a high-dimensional one such as 10-vertex BNs and SEH;"}, "weaknesses": {"value": "- The theoretical results automatically seem to follow from the GFlowNet-RL equivalence, described in the works (Deleu et al. 2024, Tiapkin et al. 2024). In particular, thanks to the graded DAG structure, Theorems 3.1 and 3.2 follow from Proposition 1 of Tiapkin et al. (2024), applied to a sub-DAG that is rooted at the vertex $s_h$, as well as the interpretation of a value function as negative KL-divergence up to a log-normalizing constant. Also, the paper does not cite a related work by Tiapkin et al. (2024).\n- The value loss itself is equal to on-policy SubTB loss, which also does not bring any novelty there: in many practical cases, the SubTB loss is applied in an on-policy manner, which is exactly the same as SubEB loss up to the lack of backpropagation through the forward policy.\n- Lack of RL-inspired baseline comparison, such as Rectified Policy Evaluation (He et al. 2025) and Muchnausen DQN (see Vieillard et al. (2020) and its application to GFlowNets in Tiapkin et al. 2024). \n- No discussion or comparison with the gradient variance reduction techniques of da Silva et al. (2024); if I understand correctly, the main difference between Sub-EB loss and Sub-TB loss is the usage of GAE-style variance-reduced objective for the forward policy, whereas the on-policy optimization through Sub-TB will result in a weaker variance reduction; thus, the comparison with other gradient variance reduction techniques applied in GFlowNets seems to be important.\n- On the large-scale task of 10-vertex Bayesian Network generation, the presented metrics do not show how the method actually solves the sampling problem. I would recommend computing the reward correlation using a test set of randomly generated graphs, or using the FCS metric proposed by da Silva et al. (2025). \n\n- Minor remark: Due to the on-policy nature of the algorithm, it's impossible to use a simple exploration technique such as $\\varepsilon$-greedy exploration or replay buffers;\n\n### References\n\nDeleu, T., Nouri, P., Malkin, N., Precup, D., & Bengio, Y. (2024). Discrete probabilistic inference as control in multi-path environments. arXiv preprint UAI-2024;\n\nTiapkin, D., Morozov, N., Naumov, A., & Vetrov, D. P. (2024, April). Generative flow networks as entropy-regularized RL. In International Conference on Artificial Intelligence and Statistics (pp. 4213-4221). PMLR.\n\nHe, H., Bengio, E., Cai, Q., & Pan, L. (2024). Random policy evaluation uncovers policies of generative flow networks. ICML-2025\n\nVieillard, N., Pietquin, O., & Geist, M. (2020). Munchausen reinforcement learning. Advances in Neural Information Processing Systems, 33, 4235-4246.\n\nSilva, T., de Souza da Silva, E., & Mesquita, D. (2024). On divergence measures for training gflownets. Advances in Neural Information Processing Systems, 37, 75883-75913.\n\nSilva, T., Alves, R. B., da Silva, E. D. S., Souza, A. H., Garg, V., Kaski, S., & Mesquita, D. (2025). When do GFlowNets learn the right distribution?. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "- Is it possible to provide a simple toy experiment that shows that indeed using the SubTB-style loss provides a faster convergence of the value function to an actual regularized value of the current policy?\n- Is it possible to provide a direct ablation on the variance reduction quality for the gradient of the forward policy?\n- Do you use a shared backbone for your policy and flow estimators? \n- How is it possible to introduce additional exploration into this algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yp2xv0aQu7", "forum": "kWM0etSpBG", "replyto": "kWM0etSpBG", "signatures": ["ICLR.cc/2026/Conference/Submission10296/Reviewer_P5HE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10296/Reviewer_P5HE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387303550, "cdate": 1761387303550, "tmdate": 1762921646195, "mdate": 1762921646195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors build upon prior work on GFlowNet training with policy gradients where an evaluation function is defined using rewards based on the log-ratio of forward and backward transitions. This formulation establishes a direct connection between minimizing the value function and minimizing the KL divergence between the forward and backward policies.\n\nIn this work, the authors propose a novel objective called Subtrajectory Evaluation Balance (Sub-EB), which enables more balanced learning compared to the conventional RL λ-TD objective. Sub-EB incorporates information from subtrajectories that both start and end at a given step whereas the conventional $\\lambda$-TD objective focuses on events starting at a step and edge-wise mismatches only while also allowing for arbitrary weighting schemes, unlike λ-TD, which is constrained by an exponential λ-decay. It additionally establishes a theoretical connection between the flow function and their evaluation function. \n\nFurthermore, the authors extend their approach to off-policy learning, in contrast to Niu et al. (2024). They introduce an evaluation function for backward trajectories, whose minimization likewise reduces the KL divergence. Because the objective is defined over backward trajectories, this formulation allows the use of a behavior policy distinct from the forward policy to generate states from which the backward rollouts are performed.\n\nTheir results demonstrate strong performance compared to the Sub-TB objective as well as the $\\lambda$-TD one. Their objective allows for faster convergence with less variance when looking at total variation. Likewise, they outperform Sub-TB in mode discovery with their best performance coming from Sub-EB-p that parametrizes the backward policy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong theoretical analysis and results\n- I appreciated the theoretical connections established between the evaluation function and the flow function."}, "weaknesses": {"value": "N/A"}, "questions": {"value": "- Figure 3 is too small and hard to read. It would be better if the font is made bigger.\n- It would be good to compare against LED-GFN since they learn energy decompositions that can be seen as a proxy for evaluating state-transitions.\n- In theorem 3.2, it would be good to clarify in the main text what the minimum is taken over exactly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XTYlzfaZ82", "forum": "kWM0etSpBG", "replyto": "kWM0etSpBG", "signatures": ["ICLR.cc/2026/Conference/Submission10296/Reviewer_wVfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10296/Reviewer_wVfJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938612244, "cdate": 1761938612244, "tmdate": 1762921645814, "mdate": 1762921645814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Subtrajectory Evaluation Balance (Sub-EB), a new training principle for GFlowNets that enables learning a value function V(s) from partial trajectories, serving as a critic for forward policy improvement. The authors derive Sub-EB as a sufficient condition for flow consistency, connecting GFlowNets to reinforcement learning style. Empirical results across synthetic and real-world tasks demonstrate improved training stability, flexibility in backward policy design, and effectiveness in off-policy learning scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Sub-EB gives an alternative way for optimization of value-based and policy-based GFlowNet;\n\n2. Both on-policy and off-policy training regimes are considered, which broadens the method's applicability."}, "weaknesses": {"value": "1. The result analogous to theorems 1-2 in current submission are well-known from the point of view of entropy-regularized (soft) RL, see [Tiapkin et al, 2024]. In particular, Proposition 1 in [Tiapkin et al, 2024] is precisely the result of Theorem 1 for complete trajectories. In light of this result, novelty and technical originality of theorems 1 and 2 is limited.\n\n2. It’s unclear how much faster the algorithm is in wall-clock time, rather than in terms of the number of steps;\n\n3. The paper does not provide a theoretical guarantee that optimizing the Sub-EB objective leads to convergence of the target distribution, even under ideal conditions; \n\n4. The paper does not provide sufficient comparisons with other approaches that use learnable backward policies, e.g. [Gritsaev et al, 2024], [Jang et al, 2024]. Also the paper does not include Sub-TB-B or Sub-EB-B experiments on the Bayesian network structure learning task.\n\nReferences: \n\n[Tiapkin et al, 2024] Tiapkin, D., Morozov, N., Naumov, A., and Vetrov, D. P. (2024). Generative flow networks as entropy-regularized RL. In AISTATS-2024.\n\n[Gritsaev et al, 2024] Gritsaev, T., Morozov, N., Samsonov, S., and Tiapkin, D. (2025). Optimizing backward policies in GFlownets via trajectory likelihood maximization. In ICLR-2025.\n\n[Jang et al, 2024] Jang, H., Jang, Y., Kim, M., Park, J., and Ahn, S. (2024a). Pessimistic backward policy for GFlowNets. In Neurips-2024."}, "questions": {"value": "1. Is it possible to prove that Sub-EB condition imply that the learned forward policy $\\pi_F$ is optimal?\n\n2. Is it possible to perform ablation studies on the update frequency of the evaluator V?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1EhLtsYwlX", "forum": "kWM0etSpBG", "replyto": "kWM0etSpBG", "signatures": ["ICLR.cc/2026/Conference/Submission10296/Reviewer_hiRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10296/Reviewer_hiRW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989927240, "cdate": 1761989927240, "tmdate": 1762921645387, "mdate": 1762921645387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}