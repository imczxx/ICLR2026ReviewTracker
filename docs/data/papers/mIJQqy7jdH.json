{"id": "mIJQqy7jdH", "number": 18733, "cdate": 1758290506144, "mdate": 1759897084243, "content": {"title": "Attention Localization Through Separator Tokens: Unlocking Long Numerical Sequence Processing in LLMs", "abstract": "Despite possessing massive context windows, Large Language Models (LLMs) exhibit a sharp decline in performance when processing long numerical sequences, a critical failure for precision-sensitive applications. We identify the root cause as the models' inability to focus attention on a manageable sequence segment, leading to dispersed attention and inaccurate results. To address this, we introduce \\textbf{Sep}arate \\textbf{N}umerical \\textbf{S}equences (SepNS), a training-free inference framework that guides LLMs by strategically inserting separators into numerical inputs. This simple modification encourages a ``separate and focus'' strategy, which we verify through attention analysis showing that separators induce localized focus on distinct segments.\nExtensive experiments on nine high-performance LLMs show SepNS substantially boosts accuracy, achieving average gains of \\textbf{35.6\\%} across all evaluated datasets with less overhead. \nOur work demonstrates that simple, structured input formatting acts as a powerful attention-focusing mechanism, unlocking long numerical processing capabilities in LLMs without any retraining.", "tldr": "", "keywords": ["Large Language Model Applications", "Numerical Sequence Processing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d66cdde16544d5570e206199771380646915adef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper examines why LLMs struggle with long numerical sequences and attributes this to overly dispersed attention. It introduces Separate Numerical Sequences (SepNS), a simple training-free method that inserts separator tokens (e.g., \\n, \\r, or \\\\) at fixed intervals to partition the sequence and encourage localized attention. The authors argue that certain heads treat separators as boundaries, creating “attention sinks” that suppress cross-segment attention and improve precision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I like the fact that the paper is simple and actionable, as it is training-free formatting trick that practitioners can try immediately.\n- There is a broad set of empirical settings covered by the authors. Multiple tasks, lengths, models, and ablations (as well as intervals and separator types)\n- The attention-localization story is easy to grasp and practically useful."}, "weaknesses": {"value": "- The main aspect that makes this paper unconvincing to me is its limited engagement with prior work. In particular, it is surprising that the authors do not cite (Barbero et al., NeurIPS 2024), which links long numerical sequences to representational collapse and already analyzes the role of separator tokens. Although that work is more theoretical, it directly overlaps with the present study and should be discussed to clarify the conceptual and empirical differences.\n\n- Moreover, the paper’s central argument (that separator tokens act as attention sinks and thus help “partition” attention) would require evidence that this behavior arises consistently across models, layers, and token types. While a full analysis may be beyond this paper’s scope, the generality of this effect should at least be acknowledged and discussed.\n\nBarbero, Federico, et al. “Transformers need glasses! information over-squashing in language tasks.” Advances in Neural Information Processing Systems 37 (2024): 98111-98142."}, "questions": {"value": "- Could the authors further investigate whether the most effective separator tokens can be identified by measuring the formation or strength of attention sinks?\n- Do the authors view separator tokens as introducing additional “sharpness” in attention, potentially mitigating the over-mixing effects described by Barbero et al. (2025)? A brief discussion of this connection would be valuable.\n- Finally, does this induced sharpness persist in extremely long contexts, and could it be related to the softmax dispersion phenomena reported by Veličković et al. (2025)?\n\n\nBarbero, Federico, et al. “Why do LLMs attend to the first token?.” arXiv preprint arXiv:2504.02732 (2025).\n\nVeličković, Petar, et al. “Softmax is not Enough (for Sharp Size Generalisation).” arXiv preprint arXiv:2410.01104 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xB5diJzn2U", "forum": "mIJQqy7jdH", "replyto": "mIJQqy7jdH", "signatures": ["ICLR.cc/2026/Conference/Submission18733/Reviewer_6nQp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18733/Reviewer_6nQp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464257286, "cdate": 1761464257286, "tmdate": 1762928443884, "mdate": 1762928443884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Separate Numerical Sequences (SepNS), a training-free inference technique that inserts separator tokens (e.g., “\\n”) into long numerical inputs to localize attention to recent segments. The authors first diagnose a failure mode: even with external tools, LLMs struggle on long numeric sequences, tending to spread attention across the entire prefix and thus missing segment-level signals. Motivated by prior observations that separator tokens often encode summaries of preceding content, SepNS interleaves separators to create implicit boundaries/sinks that focus attention locally. Across synthetic counting/indexing and real stock/weather tasks, SepNS yields substantial gains over vanilla, CoT, and one-shot prompting on nine models, with ablations on separator interval, symbol choice, and model size. Overall, SepNS is a simple, training-free method that improves numerical sequence handling without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1). The proposed trick that inserting separator tokens (e.g., '\\n') into long numerical sequences is empirically strong and theoretically motivated to improve LLM numerical processing without retraining. It’s trivial to deploy, which is just a formatting change, and readily extensible to other sequence types.\n\n(2). The goal of reducing dispersed attention on long sequences is well supported, with before/after attention maps that convincingly illustrate how separators localize attention. \n\n(3). The experiments are extensive and convincing, which clearly demonstrate the effectiveness of SepNS on enhancing numerical processing capabilities in LLMs across 10 tasks and 9 different models with significant performance gain. Moreover, the ablation study on interval, separator choices, and model size also meaningful for deployment guidance. \n\n(4). The manuscript is easy to read and follow. The structure is sound."}, "weaknesses": {"value": "(1). The paper attributes gains to 'separator-as-sink' selective suppression, but inserting separators also shifts positional phases and thereby changes QKV for all subsequent tokens. With RoPE, adding a token alters every Q and K states, so improvements may stem from positioning effects rather than sink-mass gating. The softmax argument then no longer isolates a denominator-only effect; and relative weights can change because the numerators change.\n\n(2). The tested tasks omit harder aggregation problems like the top k, median and variance of the numerical sequences, or the long-carry addition or multiplication problems. Moreover, this work does not verify the effectiveness of SepNS on more longer sequences like 1k, 2k, or 4k numerical length. Therefore, the robustness of the proposed method remains unknown.\n\n(3). In terms of Figure 4(a), it is clear that the interval sensitivity is substantial, especially for the instruction models. Moreover, it is still unclear whether other models have the similar interval sensitivity, and the current default interval value is cherry-picky. Therefore, it would be highly recommended to propose an automatic interval heuristic instead of this manually grid search determination."}, "questions": {"value": "(1). How will SepNS perform with much longer numeric sequences? Will the performance gain maintain?\n\n(2). How will SepNS perform on other long-context tasks like retrieval tasks? Will adding separators still be helpful? \n\n(3). Beyond a fixed interval, it is highly recommended to test uneven or adaptive schedules like exponentially increasing gaps or entropy/perplexity-driven insertion). \n\n(4). Will SepNS be compatible with KV cache compression methods? Does SepNS reduce compression error by localizing attention, or does it conflict with eviction policies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12s88FTbe3", "forum": "mIJQqy7jdH", "replyto": "mIJQqy7jdH", "signatures": ["ICLR.cc/2026/Conference/Submission18733/Reviewer_1Jg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18733/Reviewer_1Jg1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799921167, "cdate": 1761799921167, "tmdate": 1762928443334, "mdate": 1762928443334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to solve the LLM for long numerical sequence problem."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow. The problem is to use the LLM for long-context problem, especially for numerical sequence.\n2. The experience is good."}, "weaknesses": {"value": "1. The reserch topic seem to be trivial. It is obvious the LLM is not perfect, i.e., LLM cannot sorting long-context array, since LLM is a language model. It is not very suprising that LLM cannot deal with long numerical sequence as those kind of data will not apprear in the training corpous.\n\n2. the paper claim the long-context numeiral sequence. However, in the experient, the extract large is only for 512 numbers. Compare to like 2M context widonw in genimi, I don't see why length=512 is called long numerical sequence.\n\n3. Insert seperator into transformer is not new as: \nGuoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo\nLi, Weiyang Liu, and Chao Huang. SepLLM: Accelerate large language models by compressing\none segment into one separator. Vienna, Austria, 2024a.\nor in Vision Transformer\nhttps://arxiv.org/abs/2309.16588\n\n4. The authors doesn't show if add this seperator will hurt the performance of LLM in other task. For example, the modified LLM may be good at deal with long numerical sequnce, but may fail in many other tasks, like question-answer. Those task are good at by LLM, and is the main function of LLM."}, "questions": {"value": "1. The reserch topic seem to be trivial. It is obvious the LLM is not perfect, i.e., LLM cannot sorting long-context array, since LLM is a language model. It is not very suprising that LLM cannot deal with long numerical sequence as those kind of data will not apprear in the training corpous.\n\n2. the paper claim the long-context numeiral sequence. However, in the experient, the extract large is only for 512 numbers. Compare to like 2M context widonw in genimi, I don't see why length=512 is called long numerical sequence.\n\n3. Insert seperator into transformer is not new as: \nGuoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo\nLi, Weiyang Liu, and Chao Huang. SepLLM: Accelerate large language models by compressing\none segment into one separator. Vienna, Austria, 2024a.\nor in Vision Transformer\nhttps://arxiv.org/abs/2309.16588\n\n4. The authors doesn't show if add this seperator will hurt the performance of LLM in other task. For example, the modified LLM may be good at deal with long numerical sequnce, but may fail in many other tasks, like question-answer. Those task are good at by LLM, and is the main function of LLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G51tojpsYF", "forum": "mIJQqy7jdH", "replyto": "mIJQqy7jdH", "signatures": ["ICLR.cc/2026/Conference/Submission18733/Reviewer_rrQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18733/Reviewer_rrQY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762322221754, "cdate": 1762322221754, "tmdate": 1762928442516, "mdate": 1762928442516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SepNS, a training-free formatting method that improves the handling of long numerical sequences in large language models by inserting periodic separator tokens to promote localized attention. The authors claim that this modification improves average accuracy from 51.6% to 69.9% across 9 models and 10 tasks, and slightly reduces response length and total tokens on average. A theoretical explanation is provided, arguing that separators act as “attention sinks” that suppress cross-segment interference, supported by qualitative attention visualizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Simple and effective idea**. The proposed method requires no retraining and can be easily applied during inference. It achieves consistent accuracy gains on numeric tasks, including large improvements on indexing and counting benchmarks.\n\n- **Broad empirical scope**. Evaluation spans 9 different LLMs and both synthetic and real-world datasets, suggesting the effect is robust across architectures.\n\n- **Clear mechanistic framing**. The hypothesis that separators serve as attention sinks is intuitively presented with helpful visualizations, offering an interpretable account of why attention localizes."}, "weaknesses": {"value": "- **Mechanism claims lack a falsification test**. The theorem assumes separators have high QK similarity to preceding tokens and thereby suppress cross-segment attention, but the paper does not measure head/layer-wise QK structure or show that removing separator heads (or masking their keys) collapses the gains. A targeted “lesion” experiment by zeroing attention into the separator positions would directly test whether separators are the cause rather than a correlate.\n\n- **Efficiency claim lacks empirical support**. The paper repeatedly states that SepNS “reduces inference burden,” but several major models (e.g., DeepSeek-V3, Claude-3.7, and GPT-4o) actually show increased total token usage. No latency or KV-cache measurements are provided, and input length is not reported, making the efficiency argument unsubstantiated.\n\n- **Causal attribution is under-identified**. The paper attributes gains to “attention localization,” but does not rule out simpler confounds from prompt scaffolding and output formatting. There is no ablation that (i) preserves the same visual/line structure without separators, (ii) randomizes separator placement, or (iii) swaps separators for inert tokens, to test whether locality is the active ingredient.\n\n- **Unclear failure boundary**. The paper shows strong gains on local numeric tasks like counting and indexing, but gives little analysis of when SepNS breaks down. In the number-list task, accuracy barely improves while response length and total tokens rise sharply, implying that separators can suppress useful long-range attention. Without diagnosing such cases or proposing adaptive interval control, the method’s generalization limit remains poorly understood."}, "questions": {"value": "How was the separator interval k determined for each task and model? Was it fixed a priori or tuned on evaluation data?\n\nDo you have empirical evidence (e.g., QK-similarity or locality indices) supporting the assumption that separators act as attention sinks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Lg0cEb4zW", "forum": "mIJQqy7jdH", "replyto": "mIJQqy7jdH", "signatures": ["ICLR.cc/2026/Conference/Submission18733/Reviewer_bKXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18733/Reviewer_bKXN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762481920404, "cdate": 1762481920404, "tmdate": 1762928441666, "mdate": 1762928441666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}