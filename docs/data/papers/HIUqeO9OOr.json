{"id": "HIUqeO9OOr", "number": 24547, "cdate": 1758357856316, "mdate": 1759896760980, "content": {"title": "Rethinking Code Similarity for Automated Algorithm Design with LLMs", "abstract": "The recent advancement of Large Language Models (LLMs) has revolutionized the algorithm design patterns. A new paradigm, LLM-based Automated Algorithm Design (LLM-AAD), has emerged to generate code implementations for high-quality algorithms. Unlike the traditional expert-driven algorithm development, in the LLM-AAD paradigm, ideas behind the algorithm are often implicitly embedded within the generated code. Therefore, measuring similarity for algorithms may help identify whether a generated algorithm is innovative or merely a syntactic refinement of an existing code implementation. However, directly applying existing code similarity metrics to algorithms raises a critical limitation: they do not necessarily reflect the similarity between algorithms. \n\nTo address this, we introduce a novel perspective that defines algorithm similarity through the lens of its problem-solving behavior. We represent the problem-solving trajectory of an algorithm as the sequence of intermediate solutions progressively generated by the algorithm. The behavioral similarity is calculated by the resemblance between two problem-solving trajectories. Our approach focuses on how an algorithm solves a problem, not just its code implementation or final output. We demonstrate the utility of our similarity measure in two use cases. (i) Improving LLM-AAD: Integrating our similarity measure into a search method demonstrates promising results across two AAD tasks, proving the effectiveness of maintaining behavioral diversity in the algorithm search. (ii) Algorithm analysis. Our similarity metric provides a new perspective for analyzing algorithms, revealing distinctions in their problem-solving behaviors.", "tldr": "", "keywords": ["Algorithm Similarity", "Automated Algorithm Design", "Large Language Model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/703923585fbde2278e99a51e47fc2517ee05c2f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper notes existing code similarity metrics (e.g., CodeBLEU) fail to reflect algorithm similarity in LLM-AAD. It proposes BehaveSim, which measures similarity via algorithms’ problem-solving trajectories. Using DTW to align trajectories and normalize distances, BehaveSim outperforms traditional metrics in validation. It enhances LLM-AAD performance and aids algorithm analysis, with limitations on non-iterative algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The perspective of the paper is quite novel, as it calculates similarity based on the problem-solving trajectories of algorithms. The paper argues that traditional code similarity metrics fail to reflect true algorithm similarity, so it proposes BehaveSim, which quantifies behavioral similarity by analyzing sequences of intermediate solutions (trajectories) generated during algorithm execution."}, "weaknesses": {"value": "The proposed behavioral similarity metric, is only designed for iterative algorithms (e.g., sorting, optimization algorithms) and cannot be directly applied to other types of algorithms like machine learning models."}, "questions": {"value": "1. Regarding the differences in trajectory lengths among different iterative algorithms, what methods are adopted in the paper to avoid biases in BehaveSim's similarity calculation? \n\n2. How is the hyperparameter p_s1 (probability of inter-island selection) tuned in the paper, and what empirical evidence supports that p_s1=0.5 is the optimal value for balancing exploration and exploitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WVamuCthBI", "forum": "HIUqeO9OOr", "replyto": "HIUqeO9OOr", "signatures": ["ICLR.cc/2026/Conference/Submission24547/Reviewer_TTPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24547/Reviewer_TTPL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744060455, "cdate": 1761744060455, "tmdate": 1762943119588, "mdate": 1762943119588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BehaveSim, a novel similarity metric designed to measure algorithm similarity from a behavioral perspective, rather than code-level or output-level similarity. The authors argue that existing metrics (token-, AST-, embedding-, or execution-based) fail to capture the problem-solving behavior of algorithms—especially in the context of LLM-based Automated Algorithm Design (LLM-AAD), where generated code can differ syntactically yet implement equivalent ideas.\nBehaveSim represents each algorithm as a problem-solving trajectory, i.e., a sequence of intermediate solutions generated during execution. The similarity between two algorithms is then defined as the resemblance between their trajectories, computed via Dynamic Time Warping (DTW). Experiments show that BehaveSim better differentiates between algorithms that have similar code but distinct behavior (e.g., BFS vs DFS, insertion sort vs bubble sort), and vice versa.\nThe paper further demonstrates two applications: (1) improving behavioral diversity in LLM-AAD search (enhancing FunSearch performance on ASP and TSP tasks), and (2) clustering algorithm behaviors for interpretability and discovery."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel Perspective:\nThe paper identifies a clear conceptual gap between code similarity and algorithmic behavior similarity, proposing an elegant behavioral abstraction based on execution trajectories. This reframing is insightful and well-motivated in the context of LLM-generated algorithms.\n\nConcrete Implementation (BehaveSim):\nThe definition of behavioral trajectories and the use of DTW distance provide a simple yet powerful operationalization of behavioral similarity. The methodology is well formalized, reproducible, and extensible.\n\nComprehensive Benchmark:\nThe authors curate a systematic dataset with four categories (Type-1 to Type-4) decoupling code-, behavior-, and result-level similarities, offering a rigorous evaluation against existing metrics such as CodeBLEU, CodeBERTScore, and execution-based scores.\n\nStrong Empirical Results:\nBehaveSim achieves intuitive and consistent performance across all dataset types (e.g., correctly scoring 1.0 for Type-3 pairs with equivalent behaviors). Integration with FunSearch also improves both convergence and final performance on ASP and TSP benchmarks, validating practical relevance.\n\nInterpretability and Analysis:\nThe algorithm clustering experiment (Fig. 5) compellingly illustrates how BehaveSim distinguishes semantically similar but syntactically different implementations, supporting new interpretability avenues in algorithm discovery."}, "weaknesses": {"value": "Scope Limitation:\nBehaveSim applies only to iterative algorithms producing discrete trajectories. Many LLM-generated algorithms, including stochastic, differentiable, or recursive paradigms, are excluded. This significantly restricts generality.\n\nMetric Design Choices:\nThe use of DTW on normalized edit or Euclidean distances is heuristic; there’s limited justification for why DTW best captures “behavioral similarity.” Ablation on alternative measures (ERP, cosine, etc.) is included but not theoretically grounded.\n\nDependence on Trajectory Definition:\nDefining what constitutes a “partial solution” may require manual instrumentation of each algorithm, limiting scalability and automation for arbitrary code. This makes BehaveSim less plug-and-play for general LLM evaluation pipelines.\n\nComparative Baseline Gaps:\nAlthough the work compares to standard code metrics, it lacks comparison to semantic or dynamic program analysis methods (e.g., symbolic execution traces, graph-based semantic embeddings). These could offer a fairer baseline.\n\nModerate Empirical Gains:\nWhile improvements in AAD tasks (Table 3) are consistent, they are modest (~10–15% relative gap reduction) and limited to small benchmark scales. Broader tests on complex algorithmic synthesis domains would strengthen impact.\n\nOverclaiming “Novelty” for AAD:\nIntegrating diversity measures into LLM-AAD (FunSearch + BehaveSim) is conceptually incremental to existing multi-island or MAP-Elite-based diversity frameworks. The true novelty lies more in behavioral similarity than in AAD improvement."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89TintRWuM", "forum": "HIUqeO9OOr", "replyto": "HIUqeO9OOr", "signatures": ["ICLR.cc/2026/Conference/Submission24547/Reviewer_KsNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24547/Reviewer_KsNQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878121413, "cdate": 1761878121413, "tmdate": 1762943119069, "mdate": 1762943119069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BehaveSim, a novel similarity metric for measuring algorithm similarity from a behavioral perspective rather than code structure.The paper demonstrates that existing code similarity metrics (token-based, AST-based, embedding-based, execution-based) fail to capture algorithmic behavioral differences. To solve this problem, the paper proposes measuring similarity via problem-solving trajectories - sequences of intermediate solutions generated during algorithm execution, compared using Dynamic Time Warping (DTW)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The curated dataset with 4 algorithm pair types (varying code/behavior/result similarity combinations) provides rigorous validation. The results clearly demonstrate that BehaveSim achieves 1.0 similarity on Type-3 pairs (same behavior, different code) while existing code metrics fail, and correctly identifies behavioral differences where code metrics show high similarity."}, "weaknesses": {"value": "1. The evaluation methodology does not use any AI models or AI-related methods. BehaveSim is essentially a general algorithm comparison technique based on execution traces and DTW, which appears equally applicable to comparing human-written code. The source of code (LLM-generated versus human-written) seems irrelevant to the core methodology, raising questions about whether this is fundamentally a software engineering contribution rather than an AI/ML contribution suitable for ICLR.\n2. The method only applies to iterative algorithms such as sorting, search, and optimization, excluding ML algorithms and non-iterative approaches. This significantly limits the practical applicability and generalizability of the approach.\n3. The similarity dataset contains only two AAD tasks, and all code exampels are hand-crafted, well-known algorithms. The experimental scope is very limited. Besides, the paper does not demonstrate whether BehaveSim can distinguish novel, unseen LLM-generated algorithms, which is critical for the claimed contribution to \"algorithm discovery.\" This represents a significant gap between the evaluation (known algorithms) and the application (discovering novel algorithms)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SbduD6fDcX", "forum": "HIUqeO9OOr", "replyto": "HIUqeO9OOr", "signatures": ["ICLR.cc/2026/Conference/Submission24547/Reviewer_rhhD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24547/Reviewer_rhhD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989310914, "cdate": 1761989310914, "tmdate": 1762943118851, "mdate": 1762943118851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BehaveSim, a new metric for measuring algorithm similarity from a behavioral perspective rather than focusing on code structure or final outputs. The key idea is to represent an algorithm by its problem-solving trajectory, which records intermediate solutions generated step by step. The similarity between two algorithms is then defined as the resemblance between their trajectories, measured through DTW. The authors demonstrate that BehaveSim can distinguish algorithms that are behaviorally different but structurally similar, and that integrating this measure into LLM-AAD improves search performance by promoting behavioral diversity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "[1] The paper addresses an important and overlooked gap by redefining algorithm similarity from the behavioral viewpoint. This is a clear and original contribution.\n\n[2] The distinction among code-level, behavior-level, and result-level similarity is well presented, and the taxonomy of four types of algorithm pairs is intuitive and pedagogically useful.\n\n[3] The idea of representing algorithm behavior through trajectories and computing DTW-based similarity is theoretically grounded and applicable to both continuous and discrete problems."}, "weaknesses": {"value": "[1] BehaveSim is currently designed for iterative algorithms only. It cannot yet handle recursive, dynamic programming, or machine-learning-based algorithms. This limits its generality.\n\n[2] Several heuristic parameters, such as trajectory truncation, normalization constants, and distance scaling, are not systematically analyzed. Their influence on stability and reproducibility is unclear.\n\n[3] The benchmark for similarity evaluation mainly includes synthetic or classical algorithm examples. Broader testing on more diverse algorithmic domains would strengthen general claims.\n\n[4] The integration details of BehaveSim into the multi-island FunSearch framework are brief in the main text, making reproduction difficult without reading the appendix.\n\n[5] Some recent semantic or execution-trace-based code similarity methods are discussed only briefly without quantitative comparison."}, "questions": {"value": "[1] Could the authors elaborate on how BehaveSim would handle non-iterative algorithms or those with stochastic internal states (e.g., randomized search or Monte Carlo methods)? \n\n[2] Would the DTW-based comparison still be meaningful in these contexts, and if not, how might the behavioral similarity concept be adapted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzUBbEmqrN", "forum": "HIUqeO9OOr", "replyto": "HIUqeO9OOr", "signatures": ["ICLR.cc/2026/Conference/Submission24547/Reviewer_SVva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24547/Reviewer_SVva"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169725875, "cdate": 1762169725875, "tmdate": 1762943118606, "mdate": 1762943118606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}