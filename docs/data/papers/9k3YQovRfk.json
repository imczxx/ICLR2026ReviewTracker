{"id": "9k3YQovRfk", "number": 23806, "cdate": 1758348692824, "mdate": 1759896795934, "content": {"title": "FERA: Uncertainty-aware Federated Reasoning for Large Language Models", "abstract": "Reasoning capabilities in large language models (LLMs) are critical for advancing beyond pattern recognition toward systematic problem-solving, logical inference, and reliable decision-making. Most reasoning LLM (rLLM) approaches rely on fine-tuning but are constrained by privacy barriers that prevent centralizing domain-specific reasoning data. Federated methods enable privacy-preserving collaboration but come with prohibitive computational and communication costs. To address these challenges, we propose the Uncertainty-aware Federated Reasoning (FERA) framework that eliminates costly training and reduces communication overhead while preserving strong reasoning performance under heterogeneous data. FERA introduces Uncertainty-Aware Aggregation, using iterative server–client collaboration where uncertainty estimates are progressively refined across communication rounds to guide response generation. We establish theoretical convergence guarantees and validate our framework through extensive experiments, demonstrating that FERA achieves state-of-the-art reasoning accuracy with substantially greater efficiency than existing approaches.", "tldr": "We introduce FERA, a training-free framework for uncertainty-aware federated reasoning with large language models. FERA reduces computational and communication overhead while maintaining strong reasoning performance under heterogeneous data.", "keywords": ["Federated Learning", "Reasoning", "Uncertainty-aware", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/969274bb2a3dc9d6c06698139a6489cdc0109a5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors claim that the rLLM development is hard in real deployment setting due to privacy issue, and the Federated methods come with high computation and communication cost. To address these issues, they propose a parameter-free federated reasoning framework (FERA). In FERA, the server distributes queries to clients, which generate reasoning steps and answers using their private data. These reasoning steps are added to each client’s dataset and used to refine subsequent predictions. To handle data heterogeneity, the authors introduce two aggregation mechanisms (UA-WA and UA-SCA), which weight client contributions based on confidence and reasoning consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe concept of a parameter-free federated reasoning framework is novel and remains relatively underexplored in the current literature.\n\n2.\tThe paper is well-written, with clearly stated motivations and challenges. The proposed contributions align well with these challenges, and the overall structure is coherent."}, "weaknesses": {"value": "1.\tAs mentioned earlier, the concept of FERA is novel and intuitively aligns with human collaborative reasoning. However, it does not yet appear robust enough for practical deployment, as the reasoning traces generated by LLMs are inherently noisy. Existing LLM documentation explicitly states that “models generate text token by token based on patterns in data, not by performing deliberate reasoning or internal planning.” Thus, the reasoning steps produced by local LLMs should be viewed as patterns learn from training rather than faithful representations of actual reasoning processes.\n\n2.\tEven if we assume that the reasoning steps reflect the model’s real thinking process, the proposed aggregation mechanism still faces a critical issue. The central goal of FERA claimed is to aggregate reasoning, not to re-reason from scratch. However, during aggregation, the server’s LLM summarizes all client reasoning patterns to extract a dominant one, which may easily injects its own reasoning bias into the process. Therefore, the final aggregated reasoning trace may no longer represent the authentic reasoning patterns of the clients.\n\n3.\tThe paper lacks a detailed system overview diagram. Figure 1 is insufficient to fully convey the workflow and interactions within the proposed framework.\n\n4.\tThe evaluation is not comprehensive. Additional comparisons with established federated learning and parameter-free methods are needed, and there is no assessment of the truthfulness or faithfulness of the generated reasoning steps."}, "questions": {"value": "1.\tHow do the authors ensure that the reasoning steps generated by LLMs are reliable, given that these steps may not reflect true reasoning but only text patterns?\n\n2.\tHow does the server prevent its summarization process from adding its own bias, so that the final reasoning truly represents the clients’ reasoning rather than the server’s interpretation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6F1Cpjr8rT", "forum": "9k3YQovRfk", "replyto": "9k3YQovRfk", "signatures": ["ICLR.cc/2026/Conference/Submission23806/Reviewer_R7vv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23806/Reviewer_R7vv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679988263, "cdate": 1761679988263, "tmdate": 1762942815457, "mdate": 1762942815457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FERA (Uncertainty-Aware Federated Reasoning), a framework for performing collaborative reasoning with large language models (LLMs) in federated settings where data cannot be shared. Instead of training or fine-tuning models, FERA enables clients to perform reasoning locally and share their responses — along with uncertainty estimates — with a central server. The server then aggregates these outputs using an uncertainty-aware weighting mechanism, and further refines the reasoning through a self-critique process. This approach aims to reduce communication costs and handle data heterogeneity more effectively than traditional federated learning or simple prompting-based collaboration.\n\nExperiments on several reasoning benchmarks show that FERA achieves better performance than standard baselines while requiring less communication. Theoretical analysis on a simplified model provides convergence insights for the uncertainty-weighted aggregation. Overall, the paper contributes a novel parameter-free federated reasoning paradigm and introduces uncertainty-aware and self-critique aggregation mechanisms. However, the approach relies on access to capable local LLMs and assumes synchronous participation, which may limit its practicality in real-world federated systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel parameter-free federated reasoning framework that enables collaboration without local model updates, balancing privacy preservation and computational efficiency.\n2. The introduction of uncertainty-aware aggregation allows the system to weigh client contributions by confidence, improving robustness under heterogeneous data distributions.\n3. The approach is supported by both theoretical convergence analysis and extensive experiments, providing convincing evidence of its effectiveness across multiple reasoning tasks."}, "weaknesses": {"value": "1. I am not an expert in federated learning, so I may not be able to fully assess the technical depth of this work or identify all potential weaknesses."}, "questions": {"value": "It would be helpful to clarify how the uncertainty estimates are computed on each client. Are they derived directly from the LLM’s output probabilities, or through an additional calibration step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "cUPgWIJSx3", "forum": "9k3YQovRfk", "replyto": "9k3YQovRfk", "signatures": ["ICLR.cc/2026/Conference/Submission23806/Reviewer_Fjet"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23806/Reviewer_Fjet"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008815082, "cdate": 1762008815082, "tmdate": 1762942814231, "mdate": 1762942814231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FERA, an uncertainty-aware federated reasoning framework for LLMs that addresses the fundamental trade-off between privacy preservation and computational efficiency in distributed reasoning tasks. The framework eliminates the need for costly parameter updates through iterative server-client collaboration, where client responses are aggregated using uncertainty-weighted mechanisms. The authors provide theoretical convergence guarantees for a simplified linear attention model and demonstrate empirical improvements across MMLU-Pro, AQUA-RAT, and GSM8K benchmarks, showing that FERA achieves competitive performance while maintaining significantly lower communication overhead than training-based federated learning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an innovative training-free federated reasoning framework that effectively balances privacy, efficiency, and performance. The theoretical analysis using linear self-attention models provides solid mathematical foundations for the uncertainty-aware aggregation strategy.\n2.  FERA's modular design with UA-WA for simple aggregation and UA-SCA for complex reasoning tasks, combined with detailed implementation protocols and prompts, makes the framework readily deployable while addressing real-world constraints of federated learning environments."}, "weaknesses": {"value": "1. Given that the framework targets scenarios with private, domain-specific data across clients, the exclusive use of general-purpose benchmarks (MMLU-Pro, AQUA-RAT, GSM8K) may not adequately demonstrate FERA's effectiveness in specialized domains where clients possess unique expertise that pre-trained models lack.\n2. While the paper considers different client models (Qwen3-4B, LLaMA-3.1-8B), it lacks thorough investigation of how varying model capabilities and their inherent uncertainty characteristics affect aggregation quality, particularly when mixing models with substantially different scales or architectures.\n3. The paper mentions using token-level entropy-based uncertainty (Equation E.1-E.2) but provides limited justification for this choice over alternatives like semantic uncertainty or model-specific confidence scores, and doesn't discuss calibration issues that could affect aggregation weights."}, "questions": {"value": "1. How does FERA perform when clients have highly specialized domain knowledge not present in pre-trained models?\n2. What is the sensitivity of the aggregation mechanism to uncertainty calibration across heterogeneous models?\n3. How exactly is the uncertainty score in Equation 3.3 computed in practice? The main paper refers to temperature-scaled softmax but lacks implementation details.\n4. How does FERA handle adversarial or low-quality clients? The uncertainty weighting assumes honest clients, but what mechanisms prevent malicious clients from manipulating uncertainty scores to gain disproportionate influence in aggregation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqMHZWzonJ", "forum": "9k3YQovRfk", "replyto": "9k3YQovRfk", "signatures": ["ICLR.cc/2026/Conference/Submission23806/Reviewer_LBiw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23806/Reviewer_LBiw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104665623, "cdate": 1762104665623, "tmdate": 1762942813839, "mdate": 1762942813839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FERA, a new federated reasoning framework for LLMs. The key idea is to avoid expensive training while keeping data private. Instead of updating parameters, FERA uses iterative server-client collaboration. Clients share responses weighted by uncertainty scores. The authors back this up with convergence proofs for a linear attention model. They test on MMLU-Pro, AQUA-RAT, and GSM8K. Results show FERA matches existing methods but with much lower communication costs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The training-free approach is clever. It solves a real problem, balancing privacy and efficiency. The math using linear self-attention models is solid. The uncertainty-based aggregation makes sense theoretically.\n2.  I appreciate the practical focus. They split the framework into UA-WA (for simple tasks) and UA-SCA (for complex reasoning). The implementation details are thorough enough that someone could actually deploy this."}, "weaknesses": {"value": "1. My biggest concern is the evaluation setup. They claim this is for private, domain-specific data, but then only test on general benchmarks. What about scenarios where clients have specialized knowledge that isn't in pre-trained models? That's the whole point of federated learning, right? This feels like a missed opportunity.\n2. The heterogeneous model experiments are too limited. They test Qwen3-4B and LLaMA-3.1-8B, but what happens with bigger capability gaps? Or different architectures entirely? The aggregation might break down when mixing very different models, but we don't know.\n3. The uncertainty estimation feels under-explored. They mention token-level entropy (Equations E.1-E.2) but don't justify why this is the right choice. What about calibration issues? Different models might be overconfident or underconfident in systematic ways. This could really mess up the aggregation weights."}, "questions": {"value": "1. Have you tested this on actual domain-specific tasks? I'm curious how it performs when clients genuinely have unique expertise.\n2. How robust is the aggregation to poorly calibrated models? Some LLMs are notoriously overconfident.\n3. Can you clarify the implementation of Equation 3.3? The paper mentions temperature-scaled softmax but I couldn't find the actual computation details. This seems important for reproduction.\n4. What about malicious clients? The uncertainty weighting seems to assume everyone's honest. But couldn't a client fake low uncertainty to dominate the aggregation? Any thoughts on defense mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqMHZWzonJ", "forum": "9k3YQovRfk", "replyto": "9k3YQovRfk", "signatures": ["ICLR.cc/2026/Conference/Submission23806/Reviewer_LBiw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23806/Reviewer_LBiw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104665623, "cdate": 1762104665623, "tmdate": 1763700950287, "mdate": 1763700950287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}