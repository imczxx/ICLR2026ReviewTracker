{"id": "oQ3d4O8BaX", "number": 14432, "cdate": 1758235235103, "mdate": 1759897370532, "content": {"title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "abstract": "While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts often fail to retain reasoning from previous tasks and apply it in future contexts.\nWe introduce **L**og-**A**ugmented **G**eneration (LAG), a novel framework that *directly reuses* prior computation and reasoning from past logs at test time, enabling models to learn from previous tasks and perform better on new, unseen challenges, without sacrificing the system's efficiency or scalability.\nOur approach represents task logs as key-value (KV) caches that encode the full reasoning context of prior tasks, while storing KV values for only a selected subset of tokens. When a new task arises, LAG retrieves KV values from relevant logs to augment generation.\nUnlike reflection-based memory mechanisms, which require additional extraction or distillation steps, LAG reuses prior reasoning verbatim.\nMoreover, it extends beyond existing KV caching techniques, which have primarily targeted efficiency, by explicitly improving accuracy through log reuse.\nExperiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.", "tldr": "", "keywords": ["log", "KV cache", "generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd76cfa29f66d76029ddc24b578c9e7544fdda9d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Log-Augmented Generation (LAG), a framework designed to enable LLMs to reuse prior computations and reasoning traces at test time. The core technical innovation is to represent past execution logs not as text, but as key-value (KV) caches. The authors leverage the insight that a token's KV representation encapsulates the semantics of the entire preceding context. Their method encodes the full reasoning trace of a completed task into KV values but only stores the KV values corresponding to the final model response, creating a compact yet contextually rich representation. For new tasks, relevant logs are retrieved and their KV caches are injected into the model's generation process after adjusting for positional information. The paper evaluates LAG on both knowledge-intensive and reasoning-intensive QA datasets, demonstrating significant improvements in accuracy and efficiency over baselines, including standard agentic frameworks and reflection-based memory systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Novel Use of KV Cache for Reasoning:** The paper's primary contribution is the repurposing of KV caching from a tool for computational efficiency to a mechanism for improving reasoning and accuracy by reusing past computations. This presents a compelling and conceptually distinct alternative to reflection-based memory systems.\n\n*Effective Encoding Mechanism:** The technical approach of encoding the full reasoning history while only storing the KV values for the last model response is a clever solution to the trade-off between context richness and storage cost .\n\n**Strong Empirical Performance:** The paper's claims are well-supported by a rigorous set of experiments with comprehensive baselines, including a no-memory agent, a text-based version of LAG, and prominent reflection-based systems. It demonstrates consistent and significant improvements across four challenging datasets spanning both knowledge- and reasoning-intensive tasks."}, "weaknesses": {"value": "**Static Log Store Evaluation:** The experiments are conducted using a static log store that is built offline. This is a simplified setting that sidesteps critical challenges of a true lifelong learning system, such as the computational cost of retrieval in a massive and ever-growing log store, and the long-term effects of noise accumulation.\n\n**Insufficient Analysis of Error Propagation:** The log store is intentionally populated without filtering for correctness to simulate a realistic scenario. While Table 3 shows a net positive impact, it also confirms that logs can mislead the model ($C \\rightarrow I$) 7. The paper lacks a deeper qualitative or quantitative analysis of these failure cases and does not propose mechanisms to mitigate the risk of negative transfer from reusing flawed reasoning traces.\n\n**Simple Retrieval Mechanism:** Retrieval is based on standard top-k semantic similarity 8. While effective in these experiments, this simple approach may become a bottleneck as the log store grows and the diversity of reasoning traces increases, potentially failing to retrieve the most relevant computational context."}, "questions": {"value": "1. The experiments use a static log store. What new challenges would arise in a fully dynamic setting where new logs are added after every task? Specifically, how would the system manage retrieval efficiency and the risk of \"polluting\" the log store with noisy or incorrect reasoning over a long period?\n\n2. Table 3 shows that logs can cause previously correct answers to become incorrect. Could you provide a qualitative example of this failure mode? What types of retrieved logs are most likely to mislead the model, and does this suggest a need for a more advanced mechanism to filter or weight retrieved logs based on some confidence metric?\n\n3. The paper argues against abstraction because it can be lossy. However, abstraction also enables generalization. Your method seems optimized for tasks with significant sub-problem overlap. How do you expect LAG to perform on new tasks that are only distantly or conceptually related to problems in the log store, where direct computational reuse is not possible? Would an abstraction-based method potentially perform better in such a scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6nqQdNGjNg", "forum": "oQ3d4O8BaX", "replyto": "oQ3d4O8BaX", "signatures": ["ICLR.cc/2026/Conference/Submission14432/Reviewer_JAu2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14432/Reviewer_JAu2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760549753648, "cdate": 1760549753648, "tmdate": 1762924838453, "mdate": 1762924838453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Log-Augmented Generation (LAG), a well-motivated and novel framework for enabling LLMs to reuse past computations by storing reasoning traces as KV caches. The key technical insight is to decouple the encoding context (the full reasoning trace) from the stored content (the KV values of the last response), which is clearly differentiated from prior work. This approach yields strong and consistent empirical improvements in both accuracy and efficiency over well-chosen baselines across multiple datasets. The experimental model in this paper is singular, lacking a formal analysis of the framework's reliability, generalization ability, or potential error propagation issues. Furthermore, the default method's storage costs are substantial, and the paper lacks an analysis of retrieval failures that lead to performance degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear and Motivated Problem Setting:** The paper identifies a practical and under-explored challenge—enabling LLMs to effectively reuse prior computation at test-time—mirroring a natural aspect of human reasoning. The distinction between reusing prior reasoning and simply increasing context length is well argued, particularly in the early discussion and Figure 1.\n2. **Novel Use of** **KV** **Cache Representations:** Unlike existing KV cache approaches that target efficiency, LAG innovatively leverages the richer context encoded in KV values, advocating for selective storage (particularly from the last model response) to balance context richness and storage constraints. The distinction in Figure 2 between existing and proposed methods clearly demonstrates this key conceptual difference.\n3. **Mathematical Clarity and Soundness:** The paper provides concrete mathematical details for handling positional encodings when reusing KV caches across different contexts, specifically for rotary embeddings. The approach to “stripping” and “reapplying” positional information is well formulated and critical to correctly reusing stored KV values.\n4. **Comprehensive Experimental Evaluation:** Results in Tables 1 and 2 and subsequent ablations provide compelling evidence that LAG outperforms standard agentic, reflection, and existing KV cache methods in both accuracy (EM/F1) and efficiency (iterations), across diverse benchmarks. Qualitative examples and analyses further illuminate the mechanism by which LAG improves performance, especially via knowledge and insight reuse.\n5. **Insightful Analysis:** The work dives into ablation over stored token count, the number of retrieved logs, and the tradeoff between accuracy and storage costs—which is rarely discussed rigorously in similar works."}, "weaknesses": {"value": "1. **Related Work Positioning—Missing Direct Recent Papers:** The paper omits discussion of several key recent works that closely align with LAG's aims of scaling test-time computation and effective reuse for retrieval-augmented generation. Relevant missing works include:\n\n   1. *Yue et al. (2025): Inference Scaling for Long-Context Retrieval Augmented Generation*. \n\n   2. *Geiping et al. (2025): Scaling up Test-Time Compute with Latent Reasoning\n\n       The absence of discussion or comparison with these is non-trivial: some address scaling and dynamic retrieval or propose closely related “augmented” or logic-based test-time reasoning, which could either complement or reveal subtle differences with LAG.\n\n2. **Selection/Relevance Filtering Is Rudimentary:** Selection of logs uses standard semantic similarity on embeddings, but the method lacks adaptive or feedback-based techniques to mitigate harmful interference from irrelevant or misleading prior logs (see swings noted in **Table 3**, where some previously correct or unsolved problems become incorrect upon log incorporation). No mechanisms for detecting or counteracting negative transfer are proposed beyond manual thresholding.\n\n3. **Lack of End-to-End Theoretical Analysis or Guarantees:** The framework is empirically strong, and equations for RoPE handling are rigorously presented, but the broader questions of reliability, generalization, or even error propagation with retrieved KV caches across very long log histories are not analyzed. There's no formal proof or error bound on when reused reasoning might be counterproductive.\n\n4. The entire experimental validation is conducted on a **single model** (Llama-3.1-8B-Instruct). It is unknown how these findings generalize to other model architectures or, more importantly, to much larger models (e.g., 70B+). Larger models might be less reliant on logs, or their KV representations might behave differently."}, "questions": {"value": "- **Cross-context** **KV** **Cache Reuse:** When reapplying RoPE to “stripped” KV values from logs, does the accumulated error remain negligible when these caches are reused repeatedly across highly diverse contexts or multiple retrieval rounds? Is there a scenario in which retrieved KV values harm rather than help due to drift or misalignment?\n- **Task Drift and Out-of-domain Generalization:** How robust is LAG to highly dissimilar or out-of-distribution new tasks given log stores with only weakly relevant or misleading past traces? Are there any fallback mechanisms or metrics to assess when not to use retrieved logs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5SChnlnkVY", "forum": "oQ3d4O8BaX", "replyto": "oQ3d4O8BaX", "signatures": ["ICLR.cc/2026/Conference/Submission14432/Reviewer_TQuH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14432/Reviewer_TQuH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993811514, "cdate": 1761993811514, "tmdate": 1762924838043, "mdate": 1762924838043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"LAG\", Log-Augmented generation, which modifies standard RAG procedures to use logs/generated reasoning sequences. Furthermore, it uses KV-cache intermediate values as the retrieved representations. The paper presents strong empirical evidences for the method proposed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Use of retrieval for reasoning is very nice and the idea of using intermediate KV cache representations. This is a very nice idea and I believe an important area of study as it helps to decouple knowledge representation and reasoning representations.\n* The empirical successes of the paper especially using the KV cache values rather than full text representation of logs motivates future exploration of a wide variety of topics in retrieval augmented methods."}, "weaknesses": {"value": "* **Scaling with Context Length**:  The KV cache vs text result is most interesting I believe, but also under explored. It is of course related to context length limitations, but it is hard to know exactly how the context size of the model changes this (e.g., Llama 8B's performance: https://arxiv.org/pdf/2504.06214v1). \n* **Depth of contribution**: It's not clear to me how to evaluate the novelty of the contribution here. I understand the novelty of retrieving logs, but it is not a sea-change from past work that retrieved data (e.g., https://arxiv.org/pdf/2203.08773 or https://arxiv.org/pdf/2311.07850). \n* **Overall Metric Scores**: It's perhaps hard to evaluate the quality of the method, when as I understand the metrics for GPQA are pretty far off of SOTA?"}, "questions": {"value": "1. What is the context length for the retrieval model? (Sorry If I missed this) What is the length of the logs retrieved?\n2. Given the logs are likely OOD for the retrieval model, why do you think you see such nice retrieval performance? Is it the quality of snowflake in general? Or a characteristic of the data?\n3. What exactly is the query of the retrieval step? Formatting, context, instruction, etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ebbmwer2nc", "forum": "oQ3d4O8BaX", "replyto": "oQ3d4O8BaX", "signatures": ["ICLR.cc/2026/Conference/Submission14432/Reviewer_QLNm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14432/Reviewer_QLNm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130135593, "cdate": 1762130135593, "tmdate": 1762924837696, "mdate": 1762924837696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}