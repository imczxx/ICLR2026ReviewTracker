{"id": "0fgsHvmBBI", "number": 22044, "cdate": 1758325264827, "mdate": 1759896889162, "content": {"title": "AutoSP: Unlocking Long-Context LLM Training Via Compiler-Based Sequence Parallelism", "abstract": "Large-language-models (LLMs) demonstrate enormous utility in long-context tasks which require processing prompts that consist of tens to hundreds of thousands of tokens. However, existing LLM training libraries do not provide easy to use abstractions to optimize for long-context training, instead focusing on optimizations for models with large parameter counts through ZeRO-3/FSDP, Tensor and Pipeline parallelism. This forces users to rewrite LLM training libraries to incorporate compositions of various complex long-context optimizations, such as sequence-parallelism, to training pipelines; a process that requires in-depth expertise, reducing developer productivity. To tackle these challenges, we introduce AutSP: the first automated solution to automatically optimize LLM training for longer-contexts. AutoSP compiles models and applies a targeted set of optimizations: automated sequence parallelism, and long-context aware activation-checkpointing, to drastically enhance LLM trainability at negligible cost to throughput. Our evaluation demonstrates AutoSP's capability on both NVIDIA and AMD hardware, increasing training contexts by upto 2.7$\\times$ and 2.5$\\times$ respectively at negligible cost to runtime performance over competitive hand-written baselines.", "tldr": "An automated approach for lifting Sequence Parallelism, and other targeted memory optimisations for long-context training, into the compiler.", "keywords": ["Long context training", "Sequence Parallelism"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2eeed1421a0c73151f8740f09f36635c4167a3d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The compilation of a PyTorch 2.0 model goes through multiple stages. Dynamo, the stage responsible for running the model for the first time and recording the computation graph, can be split into two passes: Torch-IR (a higher-level graph that still resembles model layers) and Aten-IR (a lower-level graph of primitive tensor ops, e.g. matrix multiplies, permutes, convolutions, data moves). The authors present a PyTorch compiler patch integrated into the DeepSeek library that implements two optimizations:\n\n1. Takes the implementation of sequence parallelism from DeepSpeed-Ulysses into the Torch-IR pass, so that the attention layers can be automatically distributed across GPUs for parallel processing of long context sequences. The compilerized version works with arbitrary PyTorch models with minimal code changes.\n\n2. Adds a layer of optimization to the Aten-IR pass, optimizing activation checkpointing (AC). They show that PyTorch’s stock AC is too conservative for long-sequence training because it forbids rematerializing classically compute-heavy ops (e.g., matmuls, convs). They show that as sequence length grows, MLP/linear matmuls constitute a vanishing fraction of total FLOPs, so these “heavy” ops can be rematerialized cheaply. By removing this restriction, they enable a longer context to fit into the memory.\n\nThey conduct an extensive empirical study, demonstrating that their implementation \n- is compatible with different types of hardware (NVIDEA, AMD), \n- allows long sequences to fit into memory when compared to ZeRO-3 and hand-written DS-Ulysses, \n- Scales well with SP group size"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The greatest value of this work is that it automates the implementation of SP for practitioners, requiring minimal code changes. This has the potential to accelerate research and development of language model architectures with a wide impact. The authors present an original contribution of an optimized activation checkpointing strategy and provide empirical results that demonstrate the effectiveness of the proposed changes. The paper is clearly written and provides sufficient documentation on the empirical experiments."}, "weaknesses": {"value": "- The paper lacks low-level technical details about how AutoSP manipulates Torch-IR in complex models beyond toy examples. The authors should make sure this is addressed during the release of the source code.\n- The paper overemphasizes the benefits of AutoSP without detailing potential downsides, such as the scenarios where the overhead of recomputation in AC becomes substantial."}, "questions": {"value": "- Comparing AutoSP to other context-length extension strategies (e.g., HuggingFace Accelerate with Context Parallelism) would make the results more trustworthy"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SP6uxjDtO8", "forum": "0fgsHvmBBI", "replyto": "0fgsHvmBBI", "signatures": ["ICLR.cc/2026/Conference/Submission22044/Reviewer_4FNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22044/Reviewer_4FNs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663362594, "cdate": 1761663362594, "tmdate": 1762942033345, "mdate": 1762942033345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Responses to Common Questions"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are encouraged that reviewers highlighted the originality (SrRm, 4FNs), significant impact (SrRm, 4FNs), and ease of use (K3Y6, SrRm, 4FNs) of our method. We first address common questions raised by multiple reviewers, followed by responses to specific reviewer comments. \n\n**Why AutoSP is novel**\n\nWe appreciate the reviewers’ questions. AutoSP is *the first system to lift sequence parallelism into the PyTorch-2's compilation stack via IR analysis and rewrites*. Implementing SP inside a compiler, rather than through eager-mode APIs, enables capabilities that existing SP libraries cannot provide: \n\n1. Zero code changes for arbitrary PyTorch models. AutoSP automatically discovers attention/MLP structures in Torch-IR and inserts correct SP collectives. Eager SP (e.g., DeepSpeed-Ulysses, RingAttention) requires manual code refactoring, e.g., manual instantiation of SP groups, and sharding of input tensors to the training pipeline. This has to be done for every new framework and model that wishes to use SP.\n2. Composable parallelism and optimizations (SP + FSDP + AC). Expressing SP a compiler pass allows it to compose cleanly with other compiler passes. For example, our SP-pass and AC-pass jointly optimize memory for long contexts.  On the other hand, implementing SP via an eager-mode implementation would require users to think through how to compose SP with other techniques.\n3. Hardware-agnostic parallelization. Because AutoSP works at Pytorch’s middle-end IR, it automatically supports all PyTorch backends without per-backend engineering. In contrast, solutions like DeepSpeed-Ulysses only support a few attention backends, e.g., FlashAttention-v1/v2, Triton-FlashAttention. \n\n**Why implementing SP in a compiler is non-trivial**\n\n1. Implementing a compiler is substantially more challenging than implementing SP in eager mode:\nChoosing the correct IR with correct semantics. PyTorch-2 has three distinct IRs (torch-IR, Aten-IR, Inductor-IR). Backend graphs are generated between the lowering of torch-IR to Aten-IR. Therefore, incorrect rewrites break autograd semantics. We designed AutoSP to rewrite at Torch-IR so that AOTAutograd generates a correct backward graph.  \n2. Joint forward/backward rewriting. Eager SP only inserts Python-level collectives in forward/backward functions. At the compiler level, we must make sure that IR rewrites in the forward graph automatically propagate to the generated backward graph without violating autograd invariants. \n3. Robust pattern detection across arbitrary models. Program analysis must correctly identify which tensors to partition across the head-dimension (those in attention layers) and which tensors to partition across the sequence dimension (those in MLP layers). This is straightforward in eager mode, but non-trivial in fine-grained IR where the structure has already been transformed, e.g., flattened and fused. \n4. Correct collective scheduling through other passes. Collectives must be inserted in IR positions that remain semantically correct after downstream passes (e.g., AOTAutograd graph generation, lowering,and our AC passes). Ensuring this invariance is non-trivial and requires significant efforts to maintain graph correctness.  \n\nThese challenges are inherent to compiler design and are absent from eager-mode implementations. We will clarify this more explicitly in the revision."}}, "id": "RYsXDT0sCr", "forum": "0fgsHvmBBI", "replyto": "0fgsHvmBBI", "signatures": ["ICLR.cc/2026/Conference/Submission22044/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22044/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22044/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763263112161, "cdate": 1763263112161, "tmdate": 1763263112161, "mdate": 1763263112161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AutoSP, a novel compiler-based system for PyTorch-2.0 that automates optimizations for training Large Language Models (LLMs) with very long contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The article's writing is commendable, particularly for its very clear and accessible explanations of technical details."}, "weaknesses": {"value": "1.  I am skeptical of the core claim that context parallelism is difficult to implement. An API-based approach, inspired by Flash Attention (e.g., `flash_attn_func(query, key, value)`), seems more suitable than integrating it to `torch.compile()`. While also a one-line modification, this API provides users with greater transparency and explicit control, rather than obscuring the underlying logic.\n\n2.  The comparison between the proposed compile-stage activation checkpointing and traditional layer-wise checkpointing is vague. The paper primarily demonstrates advantages over a standard `torch.compile()` baseline but fails to clearly articulate its benefits over conventional layer-wise gradient checkpointing. This is a significant omission, as the latter is a common, effective, and simple-to-use practice in LLM training.\n\n3.  Most critically, the ideas presented are neither groundbreaking nor particularly novel. They appear to be targeted optimizations for implementing a specific function more efficiently within an existing general purpose framework, rather than a substantive innovation. As such, this contribution seems more appropriate for a pull request to the PyTorch repository than a publication at ICLR."}, "questions": {"value": "1.  Why was DeepSpeed Ulysses selected as the baseline? Does it exclusively support eager-mode, $O(N^2)$ attention? If so, it is an inappropriate baseline. Standard $O(N^2)$ attention is outdated; modern implementations using Flash Attention or PyTorch SDPA can easily train 16K contexts on a single 80GB GPU, especially with DeepSpeed ZeRO-3 enabled. A valid comparison would require using faster context-parallel (CP) implementations, such as Megatron-LM's CP or Ring Flash Attention [1].\n\n2.  Using `torch.compile()` for activation checkpointing is uncommon in practice. Standard LLM implementations (e.g., in Hugging Face) wrap each layer with `torch.utils.checkpoint`. This standard, LLM-specific approach should be discussed. The official PyTorch API is not complex:\n\n    ```python\n    torch.utils.checkpoint.checkpoint(layer_function, *inputs, use_reentrant=False)\n    ```\n    Please clarify the efficiency difference between your activation checkpointing method and this standard `torch.utils.checkpoint` implementation.\n\n[1] https://github.com/zhuzilin/ring-flash-attention"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YSb0leqJCJ", "forum": "0fgsHvmBBI", "replyto": "0fgsHvmBBI", "signatures": ["ICLR.cc/2026/Conference/Submission22044/Reviewer_AhGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22044/Reviewer_AhGd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838630468, "cdate": 1761838630468, "tmdate": 1762942032434, "mdate": 1762942032434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces two compiler passes to achieve sequence parallel. It inserts all-to-all collectives and transform single GPU model into sequence parallelsized models. It also consider activation checkpointing that selectively rematerializes activations by finding best memory and perf trade offs. Similar to existing work like simpleFSDP and deepcompile, it capture the graph single GPU model and transform the IR with collectives and IR passes. It mains simple UX and single-GPU style authoring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "fair originality: this is indeed the 1st paper I saw to use compiler pass to achieve sequence parallel. DeepSpeed-Ulysses and RingAttention implement SP at the framework level that affects the single gpu authoring. This paper lifts SP into the compiler layer. Joint optimization with activation checkpointing is non-trivial and essential to make good memory perf trade offs. \n\nenough quality: it touched the reasoning behind choosing which layer of IRs, and how collectives are inserted and optimized for better perf.   Evaluation is done on both NVIDIA and AMD GPUs, including eager and compiler baselines\n\ndescent clarify: The main technical sections (Sections 3.1–3.2) may challenge readers unfamiliar with PyTorch internals. Fortunately figures and intros effectively lowed the bar to keep up with the content.\n\nhighly significant: it's critical to showcase how to use IR passes to achieve SP. Long-context LLM training is a critical challenge. It's espectially benifitial to minimal developer friction. This could potentially influence future designs of pytorch compiler solution for parallelsims, like simpleFSDP"}, "weaknesses": {"value": "originiality mainly comes from using IR to implement SP. SP itself was previously implemented in open source library like DeepSpeed-Ulysses. There are also similar work that moves FSDP into IR passes. This paper is more like an extension of the idea to more parallelsims. \n\ntechnical depth: it remains questionable how non-trivial it is to come up with IR passes to achieve SP, considering we have open source implementation in eager mode. The non-trivial evaluation should be done for people with enough understanding of pytorch 2 compiler stack. But I agree joint optimization with activation checkpoint is non-trivial"}, "questions": {"value": "Explain why it's non-trivial to come up with IR passes according to open source eager SP implementation like DeepSpeed-Ulysses\n\nFor maximum sequence length, analyze memory snapshot for each baseline and show more insights into memory usage: when did the peak happen, % of memory on model/opt state, activation, and intermidate tensors"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EqpiCaKKpr", "forum": "0fgsHvmBBI", "replyto": "0fgsHvmBBI", "signatures": ["ICLR.cc/2026/Conference/Submission22044/Reviewer_SrRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22044/Reviewer_SrRm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983104979, "cdate": 1761983104979, "tmdate": 1762942032213, "mdate": 1762942032213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoSP, a compiler-based system designed to optimize the training of large language models (LLMs) for long-context scenarios. AutoSP automates sequence parallelism and activation checkpointing specifically tailored for long-contexts, aiming to enhance trainability without sacrificing runtime performance. The system is evaluated on both NVIDIA and AMD hardware, showing significant improvements in trainable sequence lengths compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a compiler-based solution for optimizing LLM training in long-context scenarios, which maybe easy to use."}, "weaknesses": {"value": "1. Lack of novelty. Automatic parallelism has been thoroughly studied and there are lots of work about the automatic or dynamic sequence parallelism including selective activation checkpointing. This article integrates these elements into the compiler, which more like an engineering project.\n 2.The paper's baseline comparisons may not fully represent the current state-of-the-art techniques, particularly in terms of hand-optimized implementations. \n3.Lots of errors. Such as line 328 , “Grouped-Query-Attention (GQA) or Full-Attention” GQA and Full-attention are completely compatible."}, "questions": {"value": "see weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nfo906jFLH", "forum": "0fgsHvmBBI", "replyto": "0fgsHvmBBI", "signatures": ["ICLR.cc/2026/Conference/Submission22044/Reviewer_K3Y6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22044/Reviewer_K3Y6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986265239, "cdate": 1761986265239, "tmdate": 1762942031902, "mdate": 1762942031902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}