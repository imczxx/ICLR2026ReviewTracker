{"id": "SI2OZJuvPO", "number": 11081, "cdate": 1758188793631, "mdate": 1763413854710, "content": {"title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning", "abstract": "Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these \"natural\" subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to \"variables\" used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.", "tldr": "We introduce an unsupervised method to decompose representation space into interpretable subspaces", "keywords": ["Mechanistic Interpretability", "Unsupervised Learning", "Representation Space Geometry"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a69d24c4fd30ec5375c704302c321d532d160bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Neighbor Distance Minimization (NDM) as a method for performing unsupervised identification of interpretable subspaces within the latent spaces of neural models. NDM operates under the hypothesis that groups of mutually exclusive features form subspaces, and the features within these subspaces exist within superposition. Through experiments on GPT-2, the authors demonstrate that these subspaces behave similarly to the “variables” used by the model. This approach provides a new avenue for interpretability research within and between subspaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivation and Construction: The motivation of NDM and its subsequent demonstration on the toy models of superposition setting provides a clear intuition as to why we’d expect it to work on larger models and how we can interpret its results.\n2. Novel Perspective: The developed perspective offers an alternative avenue for future research in the field of interpretability, specifically by considering the interaction between or within subspaces."}, "weaknesses": {"value": "1. Lack of Details on the Computational Aspect of NDM: There is a lack of information regarding the computational burden imposed by using NDM. Indeed, 2B parameter models are not large-scale by current standards. This raises questions about the practicality of these methods in real-world settings.\n2. Single Domain Evaluation: NDM is only evaluated on text models. It would also be important to evaluate the effectiveness of exploring image models.\n3. Comparison to other Interpretability techniques: Although the intuitive differences between NDM and other interpretability methods, such as sparse autoencoders, are provided, no practical comparison is made.\n4. Limited Granularity: By construction, NDM can only identify as many subspaces as there are dimensions in the latent space. In fact, it is much less than that, as the subspaces often have relatively large dimensions. Therefore, it is unclear how granular these “variables” corresponding to subspaces are, and it is likely that additional interpretability techniques would have to be applied to the individual subspaces."}, "questions": {"value": "1. What are the computational requirements for applying NDM?\n2. How sensitive is NDM to the provided N model activations?\n3. Have you explored an iterative application of NDM? Namely, re-applying NDM to the identified subspaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ohid3Sl0KI", "forum": "SI2OZJuvPO", "replyto": "SI2OZJuvPO", "signatures": ["ICLR.cc/2026/Conference/Submission11081/Reviewer_bKFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11081/Reviewer_bKFF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399002755, "cdate": 1761399002755, "tmdate": 1762922264004, "mdate": 1762922264004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Reply Regarding SAEs (1/2)"}, "comment": {"value": "We thank all reviewers for their thoughtful reviews and constructive feedback!\n\nWe notice that some concerns are raised by multiple reviewers, especially about sparse autoencoders (SAEs). We summarize and address them in this global comment, so this is a shared reply for all reviewers. We have updated the paper and added these new discussions and experiments (Appendix F).\n\n> ### Why not compare with SAEs?\n\n**In quantitative evaluation**, we patch each subspace and compute its average effect on the output. SAEs do not provide such subspaces directly. Comparing NDM with SAEs thus requires a method for applying an analogous evaluation to SAEs. One way is to patch the basis elements that SAEs provide, the features. This means that we replace a feature's activation value with its value on the corrupted input. For example, in the clean input on the token after \"John\", a feature representing *\"previous token is John\"* is highly activated -- while, in the same position in the corrupted input, a feature representing *\"previous token is Mary\"* is highly activated. If we patch the \"prev-Mary\" feature, the new activation would say *\"previous token is both John and Mary\"*. If we patch the \"prev-John\" feature, the new activation has no information about previous token. In other words, we need to add a feature while removing another one (and it's hard to select which one since there are multiple active features), which requires understanding of feature dependency that SAEs do not provide (\"prev-Mary\" and \"prev-John\" are mutually exclusive). More importantly, patching the \"prev-John\" feature would not have any real effect on inputs where the names are not John. In other words, conceptually, one would expect such patching to only have effect on a few specific inputs, and only to confuse the model. In contrast, patching subspace can be thought of patching meaningful feature *groups*. Moreover, applying such patching to SAEs requires running evaluation on each of the features, which is too computationally expensive due to the large amount of features. \n\nBecause of these fundamental differences, we did not compare with SAEs in the original submission. Nevertheless, multiple reviewers are wondering how SAEs would perform under the same evaluation used in the paper. To test this, we derived subspaces from SAE features. We use the method introduced in [1] (Sec. 4), where the authors cluster feature vectors and then, for each cluster, find multi-dimensional subspaces spanned by activations reconstructed only with features in this cluster. Specifically, they inspect pairs of principal components of the cluster-specific reconstructions and manually determine the most interesting pairs. We adapt this by producing one subspace for each cluster, spanned by the principal components with the largest eigenvalues. To keep the subspaces orthogonal to each other, we first sort the clusters by their average cosine similarity or their size in descending order, then iteratively project each cluster on the orthogonal complement of the already-selected dimensions before finding the eigenvectors.\n\nIn sum, we follow the original method from [1] as much as we can, while adapting it to construct a full decomposition of the space. The details are in the updated version of the paper (Appendix F.2). We test this method on the GPT-2 test suite, with many different hyperparameter configurations. The full results can be found in the updated paper (Table 8). Here we show the results of the best performing configuration, denoted by \"Feature Clusters\"\n\n(to be continued)"}}, "id": "3MecVEz5Wi", "forum": "SI2OZJuvPO", "replyto": "SI2OZJuvPO", "signatures": ["ICLR.cc/2026/Conference/Submission11081/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11081/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11081/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763414492990, "cdate": 1763414492990, "tmdate": 1763414807549, "mdate": 1763414807549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper begins from the premise that mutual exclusiveness could be a fundamental condition for superposition. Building on this idea, it proposes that data may contain groups of mutually exclusive features; for example, features encoding categorical variables such as different subjects, where only one is active at a time (L. 110). Motivated by this observation, the paper introduces an unsupervised method, termed NDM, designed to identify subspaces of mutually exclusive features. The method does so by finding subspaces in which data points are projected to similar locations. The paper demonstrates the effectiveness of NDM both on a synthetic (toy) model and on representations from large language models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I really appreciate the core idea and intuition behind the argument that mutual exclusiveness may be a fundamental condition for superposition, as well as the proposed method built on this insight.  \n- The results on the toy model effectively illustrate and complement the central narrative.  \n- The paper does a good job explaining the intuition behind the approach and is, for the most part, well written and thorough in its exposition.  \n- Overall, I found this to be one of the more enjoyable papers to read, with a clear take-home message.\n- While the evaluation still leaves considerable room for improvement, the paper does at least include some empirical assessment of the proposed approach."}, "weaknesses": {"value": "- I did not find the qualitative examples in Figure 2 particularly convincing. The positions shown in panel (c) still exhibit a fairly large range, and for the other examples, it seems plausible that sparse autoencoders could identify similar concepts. Given the narrative developed in the paper, I would have expected the qualitative examples to focus more on illustrating the idea of distinct variables, rather than on specific concepts that might also be captured by SAEs.\n- In Table 1, it is unclear why no comparison to sparse autoencoders (SAEs) is included. A theoretical justification would be sufficient if there is a solid reason for omitting such a comparison. Additionally, it would be helpful to include an analysis of how the number of subspaces affects the results. For instance, if only a single subspace were used, the condition that the high-level \"variable\" lies within the same subspace (L. 261) would trivially hold.\n- The method relies on the mutual information (MI) threshold, but it is not clear how this threshold should be selected in practice. Although the paper includes ablation studies on this parameter, the specific effect of the threshold remains unclear, as the quantitative experiments are relatively small in scale."}, "questions": {"value": "- In Eq. (1), why is $h = W x x'$ instead of $h = W x$, as used in the experimental setup of Elhage et al. (2022)? It is possible that the equation is incorrectly formatted and that $x'$ is intended to appear on the right-hand side?\n- How to choose the MI threshold and how sensitive is the method to that threshold?\n- How would sparse autoencoders (SAEs) perform under the proposed evaluation setup, both in quantitative metrics and in qualitative analyses?\n- According to the manuscript, superposition occurs when mutual exclusiveness holds, and the proposed method aims to identify subspaces of mutually exclusive features. However, it is not clear why we should expect these subspaces to be inherently interpretable if superposition still occurs within them. Wouldn’t we, in many cases, need an additional method to disentangle or remove the remaining superposition? Furthermore, how can we identify situations in which such additional disentanglement would be necessary?\n\nDue to the open questions regarding the evaluation and the distinction from sparse autoencoders (SAEs), I would currently lean toward rejecting the paper. However, I believe the work has potential, and if my concerns are addressed convincingly and no major issues are raised by other reviewers, I would be happy to reconsider and improve my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oUz2YawnGr", "forum": "SI2OZJuvPO", "replyto": "SI2OZJuvPO", "signatures": ["ICLR.cc/2026/Conference/Submission11081/Reviewer_MPxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11081/Reviewer_MPxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559413269, "cdate": 1761559413269, "tmdate": 1762922263536, "mdate": 1762922263536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A core objective in interpretability research is to understand content and geometry of activations. The authors propose a novel, original idea to learn a set of linear subspaces, where each subspace holds mutually exclusive features encoded via superposition. Different subspaces are orthogonal thus independent. The authors propose a method to learn these subspaces unsupervised from activation data alone, by learning a rotation matrix (similar to DAS) such that data points within each subspace have minimal distance. They provide intuitive understanding, show their approach in toy models and prove its applicability in small and medium language models. Specifically, they show for some known tasks that all features of interest lie in the same subspace, and they show that subspaces are interpretable (ie monosemantic).\n\nThe paper is well-written, proposes an original idea, and elegantly combines mathematical intuition, toy models, and translation to LLMs. My main concern is that some assumptions/hypotheses were not empirically validated. For example, it wasn't shown that features within a subspace are mutually exclusive, or that the orthogonality requirement is faithful of real activation space geometry. They critique SAEs but don't use them as baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- great presentation, paper is well-written and I found it easy to follow. The paper provides both intuitive understanding and mathematical precision.\n- the paper elegantly combines mathematical intuition, validation in toy models, and translation to LLMs\n- the paper posits an interesting, novel idea for an important problem rather than an incremental improvement\n- I do find the author's work interesting from a slightly different perspective that wasn't as highlighted as it could: Feature independence. When steering with SAEs, editing individual features results in an OOD reconstruction because many SAE features co-occur which is a real problem."}, "weaknesses": {"value": "Major:\n1. \"Mutual exclusiveness\" doesn't seem like a more fundamental condition, or much different from sparsity at all. Elhage 2022 say that when features are sparse, they can be encoded via superposition. Sparse features are already \"almost mutually exclusive\" but the \"almost\" seems important as strong guarantees are hard to make for neural networks. \nThe authors write that superposition wouldn't work if 2 or more features are active but this is false. Yes, interference and error would grow, but only by a tiny amount in practice. Best proof are SAEs that work well with e.g. 20 features active. \nIn fact, feature co-occurrence is baked into superposition theory: A prediction of their theory and toy models is that features that are mutually exclusive should make heavy use of superposition, while features that often co-occur, should be better separated, ie encoded as orthogonal directions (at least more orthogonal than mutually exclusive ones). So no new theory is needed to predict geometry of feature groups so I don't understand how \"mutual exclusiveness and feature groups\" are fundamentally different from \"sparsity and superposition\". It's good to have methods to decompose this, but it follows from superposition theory, and not new theory/hypothesis has to be invented.\n2. The authors assume that features within a subspace are mutually exclusive but you never test this (in fact, you don't even extract features within a subspace).\n3. The authors for some tasks that most task-relevant features land in a single subspace but they don't show that this subspace only contains those features, aka is interpretable and monosemantic. It could very well be the case that completely unrelated information is stored within the same subspace as many unrelated features are mutually exclusive.\n4. The number of subspaces is strictly bound by the dimensionality of activation space as they must be exactly orthogonal. This might limit the method's potential and interpretability. As we see with SAEs, representation space can be well-approximated with tens of Millions of features and having only few hundred feature groups available at most might be quite limiting, and many different features might be packed into the same subspace (because different features are often mutually exclusive as well). This would limit interpretability. My concern is that in reality, the independence-between-feature-groups assumption doesn't hold 100% and we would be better off with relaxing this restriction a little to allow more feature groups to exist and be more interpretable. A possible approach here could be to use MOLTs (Lindsey, Anthropic 2025).\n5. The authors heavily criticize superposition and SAEs but they never directly compare against SAEs although direct comparisons should be possible. They state that other baseline comparisons aren't possible because their subspaces are learned unsupservised from activations, but SAEs are as well. In fact, I think that their method doesn't disagree with superposition and sparsity at all.\n\n\nOther things that would improve the paper:\n- Insight into LLM computation. This paper mainly proposes a new method and validates it but there's no new mechanistic insight about LLM computation. It would improve the paper a lot if the authors could prove by example that this method can discover things that other methods like SAE, DAS, etc can't\n- More experiments with LLMs. Qualitative examples in Figure 2 lack rigor or quantification. IOI/greater than results don't measure logit difference recovered when only patching an individual subspace, more evidence that subspaces are monosemantic and interpretable, etc."}, "questions": {"value": "1. Many things are mutually exclusive. For example, if the text is about software licenses, it's not about a fiction novel or a mental health consultation. Features from all these different contexts could be squished into a single subspace. Especially since the number of subspaces is extremely limited. Do you observe this in practice? Doesn't this imply that mutual exclusiveness != interpretability? Doesn't this hurt interpretability of those subspaces and the applicability of this method a lot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FS7bhEGRGU", "forum": "SI2OZJuvPO", "replyto": "SI2OZJuvPO", "signatures": ["ICLR.cc/2026/Conference/Submission11081/Reviewer_EMRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11081/Reviewer_EMRg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975534619, "cdate": 1761975534619, "tmdate": 1762922263110, "mdate": 1762922263110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes and evaluates an unsupervised method — neighbor distance minimization — for decomposing representation space into interpretable subspaces."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Well-written with clear explication:** The paper presents complex concepts with clear language, helpful intuitions, and useful examples.\n\n**Vast amount of work:** The paper, combined with the appendices, reflects a vast amount of experimental work."}, "weaknesses": {"value": "**Evaluation**: As the authors say, “The key question is NDM’s applicability to real-world neural models.” The evaluation relies on the intuition that “when processing inputs, key intermediate results should ideally lie in a single subspace.” On the one hand, this idea is appealing, partially because it provides a fairly clear evaluation criterion for methods for decomposing representation space into interpretable subspaces. On the other hand, this criterion could both under under-determine and over-determine useful results. That is, essentially useless decompositions could satisfy this criterion, and useful decompositions could violate it. Ultimately, a more convincing evaluation would use an end-to-end criterion (i.e., one that shows that an MI pipeline produces more useful results when it includes NDM rather than some other subspace discovery approach). That’s a very tall order, but one that is far less error-prone than this paper’s current evaluation criteria. This reflects a more general problem with current MI research: whole MI pipelines cannot be easily created because we don’t have all the components, and candidate components cannot easily be created because we don’t have the pipeline."}, "questions": {"value": "In Section 5.1, you say that “The results of NDM using the best hyperparameters are shown in Table 1…”  This raises the possibility of overfitting because only the “best” hyperparameters are shown. How were the best hyperparameters selected, and is this method reasonable in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "acWdWDPFkC", "forum": "SI2OZJuvPO", "replyto": "SI2OZJuvPO", "signatures": ["ICLR.cc/2026/Conference/Submission11081/Reviewer_rZmX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11081/Reviewer_rZmX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206694189, "cdate": 1762206694189, "tmdate": 1762922262483, "mdate": 1762922262483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}