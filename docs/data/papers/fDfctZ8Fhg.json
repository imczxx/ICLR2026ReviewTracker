{"id": "fDfctZ8Fhg", "number": 25471, "cdate": 1758368377812, "mdate": 1759896719823, "content": {"title": "Not All Who Wander Are Lost: Hallucinations as Neutral Dynamics in Residual Transformers", "abstract": "We separate onset from persistence and prove that persistence follows from the neutral dynamics of pre-LayerNorm residual transformers. Exact operator norms for LayerNorm, residual blocks, and the softmax decoder yield conservative upper bounds showing the absence of contractive or expansive bias at the decoded level. These bounds are sharpened by working with corridor constants that remain explicit and falsifiable. For open probes, drift decomposes into a predictable component bounded by the sharpened corridor and a centered martingale component controlled by concentration and central limit arguments. Neutrality is then lifted from paired rollouts to populations by casting trajectories or blocks as exchangeable agents in a mean-field game, yielding a population-invariant stable under depth and width scaling. Predictions are tested with controlled randomization audits up to GPT2-large: closed probes are centered and behave as bounded martingale differences, while open probe drift stays within the predicted corridor with magnitudes consistent with the sharper constants. Together, these theoretical and empirical results provide the first structural account of persistence, explaining why hallucinations persist across model scales without re-auditing hundreds of millions of parameters, and showing that interventions, which do not alter the residual backbone, cannot eliminate it once onset has occurred.", "tldr": "", "keywords": ["Transformer architectures", "Mean-field Games", "Hallucinations", "Stability and Dynamics"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7906252147b467ff9388cb00c4067d4d1301a417.pdf", "supplementary_material": "/attachment/fb59cd2610a6e6e6e850625d16c5a1ad81e5e289.zip"}, "replies": [{"content": {"summary": {"value": "The authors present a theoretical treatment of persistence for hallucinations: in other words, after initial divergence, is there pressure in any direction for two paired rollouts to converge vs. diverge? The authors introduce closed and open probes as paired trajectories to test this pressure, and theoretically show closed probes have no drift (neutrality), while open probes have a predictable drift bounded by a corridor term. Empirical tests confirm this behavior on GPT2 at various scales."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Well structured and new theoretical design for analyzing persistence and drift between coupled rollouts.\n\n- Valuable and provable insight into why trajectory deviation remains persistent."}, "weaknesses": {"value": "- The presentation could greatly benefit from more explicitly grounding the discussion (e.g. concepts such as \"neutrality\" and \"drift\") more often with concrete language examples, especially related to hallucinations.\n\n- The set-up for the empirical section is a bit unclear: it is unclear how rollouts are being generated/in response to what prompt, and how the + and - from the CRN come to play in this generation."}, "questions": {"value": "- It's not clear from the presentation what the + and - arms are of the CRN rollouts. In the language of hallucinations and persistence, how are the + and - rollouts constructed?\n\n- Do the results for neutrality and predictability of the open probe drift hold for more modern base models like Qwen/Llama? \n\n- Is it possible to predict whether the bounded bias term is positive or negative? Is there intuition for when to expect positive vs negative drift when it is predictable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T7Mr41HJCB", "forum": "fDfctZ8Fhg", "replyto": "fDfctZ8Fhg", "signatures": ["ICLR.cc/2026/Conference/Submission25471/Reviewer_uH9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25471/Reviewer_uH9y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969657731, "cdate": 1761969657731, "tmdate": 1762943446972, "mdate": 1762943446972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the stability of autoregressive Transformer generation by modeling rollouts as sequences of probability distributions and studying how small perturbations propagate over time. The authors show that pre-LayerNorm residual Transformers exhibit neutral dynamics: small deviations neither systematically contract nor expand (in expectation). This behavior is empirically verified on GPT-2 variants."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Modeling the model’s rollouts as a sequence of probability distributions is certainly interesting and allows studies based on stochastic control.\n\n- The theory seems valid. The modeling is sensible and the technical execution is careful. In particular, the use of martingale tools, operator norm bounds, and controlled randomization networks is mathematically clean and reflects a strong command of stochastic process techniques, with the additional difficulty of applying to LLM generation modeling."}, "weaknesses": {"value": "My main concerns are related to the claims made in the paper (in particular, in connection to hallucinations) and the general prose of the paper.\n\nThe paper does not measure hallucinations or semantics. It measures dynamical drift in autoregressive probability space, i.e. how two sequences of probability distributions (induced by the softmax) representing two different rollouts with the same initial context diverge over time. The results therefore demonstrate architectural neutrality of perturbations, not hallucination persistence in the semantic or factual sense. \n\nThis makes some claims not supported by the evidence:\n\n> “Together, these theoretical and empirical results provide the first structural account of persistence, explaining why hallucinations persist across model scales without re-auditing hundreds of millions of parameters, and showing that interventions, which do not alter the residual backbone, cannot eliminate it once onset has occurred.”\n\nThe connection to hallucinations is very loose and actually misleading. Given a rollout representing the “truth,” i.e. a rollout without hallucinations, the given metric cannot tell whether a second rollout, once a hallucination is present, won’t self-correct. This is because it could be that it outputs a semantically self-correcting sequence of tokens, yet not time-aligned with the “ground-truth” rollout. Thus, the divergence metric used here cannot exclude the possibility of semantic convergence, only token-synchronous convergence.\n\nI feel the paper uses a lot of terminology in a non-conventional way, and this makes it quite hard to fully understand its content. For instance, this starts very early in the abstract:\n\n> “Exact operator norms for LayerNorm, residual blocks, and the softmax decoder yield conservative upper bounds showing the absence of contractive or expansive bias at the decoded level.”\n\nWhat is a softmax decoder? What is a contractive or expansive bias here? I feel more context has to be given.\n\n> “These bounds are sharpened by working with corridor constants that remain explicit and falsifiable.”\n\nWhat is a corridor constant, and what does it mean to be explicit and falsifiable?\n\nSome claims are never supported, for instance:\n\n> “yielding a population-invariant stable under depth and width scaling.”\n\nI do not see any lemma or empirical results showing how the stability varies across depth and width.\n\nOverall, the paper provides a mathematically interesting stability analysis of autoregressive rollouts, but the connection to hallucinations is not demonstrated and the terminology makes the narrative difficult to follow."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3hu434G1rk", "forum": "fDfctZ8Fhg", "replyto": "fDfctZ8Fhg", "signatures": ["ICLR.cc/2026/Conference/Submission25471/Reviewer_ZZth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25471/Reviewer_ZZth"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002607693, "cdate": 1762002607693, "tmdate": 1762943446609, "mdate": 1762943446609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework to explain the persistence of hallucinations in pre-LN residual Transformers, disentangling it from the onset of such hallucinations and positioning it as a consequence of the architecture itself, rather than of the training process or objective. This is achieved by deriving an upper bound on the drift between paired rollouts, showing that once a deviation has occurred (onset), it continues to persist since the autoregressive dynamics, as a result of the architecture, are neutral, i.e., neither contractive nor expansive. The authors validate the results of their theoretical analysis on GPT-2 models of various sizes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Understanding LLM hallucinations is an important topic of research and identifying architectural biases that cause the persistence of hallucinations could be a strong contribution.\n* The paper contains both theoretical and empirical results, although due to my lack of expertise, I am unable to verify completely whether the claims are actually validated and the correctness of the proofs."}, "weaknesses": {"value": "* While I am not an expert in this field, I find the writing to be really opaque, right from the abstract. The opening words of the abstract are jargon and right until the last sentence I had no clear understanding of the problem being dealt with. This persists during the introduction as well, where I acknowledge that my comments might arise from my own ignorance and lack of expertise, but some sentences just sound like jargon to me when the whole problem could have been motivated in a much better way. Some other writing issues:\n  * Line 81 undefined reference\n  * References that don't exist – which is quite serious in my opinion – I was either not able to find the following papers or their links were undefined or both:\n    1. Hayou et al. \"On the impact of residual connections on the lipschitz constant of neural nets.\" In Advances in Neural Information Processing Systems, 2019 – does not exist\n    2. Manakul et al. \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.\" – [provided link](https://aclanthology.org/2023.emnlp-main.722/) is incorrect\n    3. N. Mündler and colleagues. \"Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.\" In International Conference on Learning Representations (ICLR), 2024 – [provided link](https://openreview.net/forum?id=hgtX9Z8H6z) does not exist\n    4. Kaiqing Yang, Guanghui Lan, and Tamer Basar. Learning deep mean field games for modeling large population behavior. In International Conference on Learning Representations (ICLR), 2018. URL https://openreview.net/forum?id=ryxY-pZAW – paper does not exist, but is a potentially hallucinated version of https://arxiv.org/abs/1711.03156.\n\n  Given the above points, I seriously doubt the truthfulness of the LLM usage statement: \"Large language models were used only for polishing language, fixing minor coding errors, and triaging related work. The proofs, analyses, and results are by the authors, and every cited reference was verified directly.\" Could the authors please clarify this?\n* It seems that the reproducibility statement is also riddled with incorrect details. The [Colab link](https://colab.research.google.com/embedded/projects/prj-prd-data-learning-ddb6/locations/europe-west4/repositories/ce0ee1f7-19db-4562-b14f-52907a2e3e70) provided at the head of the file does not work and only the `neutrality_audit.py` script was provided. There is no repository or requirements files as claimed, nor is SciPy used as stated. It is unfortunate that a paper on hallucinations should be riddled with what are potentially the effects of LLM hallucinations as well. Again, I hope I have not misunderstood anything, but could the authors clarify this?\n* The authors have only considered horizons of 32 in their experiments, which seems quite small.\n* All models are only GPT-2 variants, there exist more modern LLMs with open weights/architectures that satisfy the architectural assumptions here. Furthermore, if the authors would want to actually compute the bounds derived in the paper, a much simpler setup could be considered (a toy model).\n* The theoretical bounds derived seem to be loose and very conservative, based on my understanding.\n* Some of the CIs in Table 2 are really wide. Could the authors comment on this?"}, "questions": {"value": "None aside from the points raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "I acknowledge that I am not an expert in this domain, but it seems like the paper has several details as pointed out in my Weaknesses section that could be the result of improper/undeclared and unverified LLM usage. Erring on the side of caution, I think this paper needs ethics reviews. I am unsure if this counts as a research integrity issue or responsible research issue, but my concern is that there are several inaccurate details in the paper that could be the result of poor research standards."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lToIhx6z5J", "forum": "fDfctZ8Fhg", "replyto": "fDfctZ8Fhg", "signatures": ["ICLR.cc/2026/Conference/Submission25471/Reviewer_tE7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25471/Reviewer_tE7k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185572868, "cdate": 1762185572868, "tmdate": 1762943445960, "mdate": 1762943445960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's central claim appears to be that hallucinations in transformers are a 'persistent' and inherent property, suggesting interventions cannot eliminate them. The primary issue with this submission is its severe lack of clarity, which renders the paper inaccessible. The writing is extremely opaque. \n\nTo illustrate, the abstract alone introduces a deluge of specialized, undefined terms that make the paper's premise inaccessible. Just from the first few lines, a reader is forced to ask: What are \"onset\" and \"persistence\"? The paper says it \"separates\" them, but the \"of what\" (presumably hallucinations) is only implied. What does the \"absence of contractive or expansive bias at the decoded level\" mean in a practical, understandable sense? What are \"corridor constants\"? How can a \"constant\" be \"falsifiable\"? What are \"open probes\"? What is the \"drift\" being referred to? What is the \"Neutrality\" being proven?\n\nThese questions are merely a sample from the abstract; this systemic lack of clarity continues throughout the paper.\n\nDue to these fundamental presentation flaws, I cannot provide a competent or fair evaluation of the paper's technical soundness. \n\nIn its current state, the work is not ready for publication at ICLR. It would require a complete and substantial rewrite to become understandable and reviewable. Therefore, I must recommend **strong rejection**."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A."}, "weaknesses": {"value": "See above."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LrVhBCJ24K", "forum": "fDfctZ8Fhg", "replyto": "fDfctZ8Fhg", "signatures": ["ICLR.cc/2026/Conference/Submission25471/Reviewer_rgqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25471/Reviewer_rgqj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762487475150, "cdate": 1762487475150, "tmdate": 1762943445564, "mdate": 1762943445564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}