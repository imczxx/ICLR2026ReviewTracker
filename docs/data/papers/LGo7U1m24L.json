{"id": "LGo7U1m24L", "number": 20923, "cdate": 1758311706957, "mdate": 1762984924330, "content": {"title": "Continual Learning via Sparse Memory Finetuning", "abstract": "Modern language models are powerful, but typically static after deployment. A major obstacle to building models that continually learn over time is catastrophic forgetting, where updating on new data erases previously acquired capabilities. Motivated by the intuition that mitigating forgetting is challenging because trainable parameters are shared across all tasks, we investigate whether *sparse parameter updates* can enable learning without catastrophic forgetting. We introduce sparse memory finetuning, leveraging memory layer models (Berges et al., 2024), which are sparsely updated by design. By updating only the memory slots that are highly activated by a new piece of knowledge relative to usage on pretraining data, we reduce interference between new knowledge and the model's existing capabilities. We evaluate learning and forgetting compared to full finetuning and parameter-efficient finetuning with LoRA on two question answering tasks.\nWe find that sparse memory finetuning learns new knowledge while exhibiting substantially less forgetting: while NaturalQuestions F1 drops by 89\\% after full finetuning on new facts and 71\\% with LoRA, sparse memory finetuning yields only an 11\\% drop with the same level of new knowledge acquisition. Our results suggest sparsity in memory layers offers a promising path toward continual learning in large language models.", "tldr": "Finetuning a sparse set of parameters in memory layer models enables learning with significantly less forgetting.", "keywords": ["memory layer", "memory", "catastrophic forgetting", "continual learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19f5e7bb3489523ab2b68573d0f320a368e058fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the catastrophic forgetting problem faced by LLM in continuous learning. The authors propose a sparse memory finetuning method based on a special LLM architecture with a \"memory layer.\" The core idea is that when fine-tuning with new knowledge, instead of updating all activated parameters, the TF-IDF algorithm is used to identify and update only a small subset (top-t) of memory slots most specific to the current knowledge. This sparse update strategy aims to write new knowledge into dedicated parameters, thereby minimizing interference with the model's original capabilities.\n\nThe authors conducted experiments on fact learning and document question answering tasks, comparing their method with full fine-tuning and LoRA. Experimental results show that, while learning new knowledge, sparse memory fine-tuning significantly reduces performance degradation on retention tasks compared to baseline methods, demonstrating a superior learning-forgetting tradeoff."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Continuous learning and catastrophic forgetting are core challenges in deploying large-scale models, making this paper's topic relevant to real-world applications.\n\n2. Combining the sparse activation characteristics of the memory layer with task-specific parameter updates offers some inspiration."}, "weaknesses": {"value": "1. The novelty of this paper is relatively limited. The Memory Layer Model itself is not original; it is from existing work. The authors mainly adapt it to the specific task of catastrophic forgetting. Furthermore, the core contribution of the paper is concentrated in Chapter 4, but the TF-IDF algorithm used therein is also a mature technique. Overall, this work seems more like a combined application of existing methods, lacking significant original breakthroughs.\n\n2. The paper mentions experiments on a 1.3B-scale model, but does not specify which type of model architecture was used. More importantly, all experiments were conducted on a relatively small-scale model, making it difficult to fully verify the scalability and generalization ability of the proposed method on larger models. In addition, the memory layer model requires replacing the FFN in the original model with a memory layer structure, which may bring additional optimization challenges in large-scale modelsâ€”the authors do not discuss whether the performance of downstream tasks can still be maintained on larger models.\n\n3. The experiments were only conducted on two general QA datasets, resulting in a relatively limited evaluation scenario. To fully validate the method's generality, the authors should supplement it with more domain-specific fine-tuning experiments (such as in medical and legal fields) and adopt more diverse evaluation benchmarks to more comprehensively measure the method's effectiveness in mitigating catastrophic forgetting.\n\n4. The paper only reports the absolute performance of the proposed method but does not provide a systematic comparison with current mainstream catastrophic forgetting mitigation methods. Therefore, readers cannot determine the method's actual competitiveness and advantages among existing technologies."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dlo59FcFgC", "forum": "LGo7U1m24L", "replyto": "LGo7U1m24L", "signatures": ["ICLR.cc/2026/Conference/Submission20923/Reviewer_PrtR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20923/Reviewer_PrtR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879072285, "cdate": 1761879072285, "tmdate": 1762999998255, "mdate": 1762999998255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackles the problem of continual learning and propose Sparse Memory Finetuning (SMF). SMF fine-tunes a memory layer that replaces the transformer FFN with product-key lookup matrices. During fine-tuning, the method counts which memory indices are accessed by a batch, ranks indices via TF-IDF relative to some background corpus, and updates only the top $t$ value vectors while freezing everything else.\nDuring inference, the model just performs the sparse memory lookup top-$k$ retrieval per memory head. The authors consider two cases: 1) streaming fact learning and 2) streaming document learning. The paper shows that SMF has similar target-task learning to full FT and LoRA but dramatically less forgetting on held-out tasks (NQ / HellaSwag / GSM8K). The authors analyzed TF-IDF vs TF-only ranking and the choice of background corpus."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea is pretty easy to follow\n- The idea is clear, simple, and is easy for the community to build upon\n- Conditioning on a background corpus is a novel approach to control what kind of forgetting one is targeting to prevent; this might be an interesting new design choice to explore\n- I think the evaluation setup makes sense to test the claim (e.g., the fact vs doc learning settings) with caveats described in the following section"}, "weaknesses": {"value": "Although the main idea is appealing and the experiments show positive signals, the coverage of the experiments seems rather limited. \n\n**Major issues**\n- One main result is missing (i.e., the results for Document QA). This is the main reason for the low soundness rating.\n- Often times the benefit of forgetting mitigation simply comes from the reduced number of parameters that are updated. The flip side is the reduced learning capacity. While some of this is tested in the paper (Fig 4), the experiments don't seem comprehensive enough to show that the learning capacity of the memory layer is as good as LoRA or Full FT only base on the TriviaQA learning result. One experiment to test is multi-stage fine-tuning and see if sparse update actually has less forgetting throughout.\n\n**Analyses that i think would strengthen the argument of the paper**\n- Along the line of memory layer capacity, it would be great to see the a sweep top $k$ indices in the memory layer is fine-tuned. \n- The background corpus analyses in the paper are valuable, but I would love to see a different corpus tested beyond DCLM. Especially, if one wants to preserve the capability of a particular domain, does choosing a corresponding background corpus help this?\n- \"We note that ranking based on batches makes no assumption about task boundaries; consecutive batches can be from the same or totally different data distributions.\" It would be really nice to see this tested, which can be done with the aforementioned sequential training setting.\n\n**Minor**\n- L40: a citation is missing\n\n\nWhile I appreciate the idea, my general sense is that the experiments and analyses are a bit limited or might be over-indexing on the specific datasets. I would be happy to increase my scores if the above points are addressed."}, "questions": {"value": "- Why only insert one memory layer? What would happen if the memory layer is inserted throughout the model?\n- Why is GSM8K measured in NLL and not the standard accuracy? It doesn't seem to be justified in the paper.\n- Fig 6: should the caption be $t$ instead of $k$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gWTxU4zsUv", "forum": "LGo7U1m24L", "replyto": "LGo7U1m24L", "signatures": ["ICLR.cc/2026/Conference/Submission20923/Reviewer_LZhp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20923/Reviewer_LZhp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930116450, "cdate": 1761930116450, "tmdate": 1762999999322, "mdate": 1762999999322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses catastrophic forgetting in continual learning for large language models by proposing Sparse Memory Finetuning, a method that updates only a small number of memory slots that are highly activated by new data but rarely used during pretraining. Experiments show that this method learns new knowledge effectively while greatly reducing forgetting compared to full finetuning and low-rank adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This approach does not require any architectural changes beyond adding memory layers.\n- The sparse update mechanism limits parameter interference, preserving previously learned knowledge and mitigate the forgetting.\n- This method enables continual learning without requiring data replay, which improve the scalability and efficiency."}, "weaknesses": {"value": "- Evaluation is limited to two QA tasks (fact learning and document QA), it would be great to have the result in multi-task, reasoning, or multilingual continual learning.\n- Experiments only include full finetuning and LoRA, I think the authors need to compare with other continual learning approach as the baseline. For now, it unclear how the method compares to those."}, "questions": {"value": "- If the new knowledge is similar to previously learned knowledge but not identical, will the model choose to update entirely new memory slots, or will it incorporate and refine the existing knowledge stored in the previously used slots?\n- You evaluated your method only on a 1.3B parameter model. Could you clarify why this particular scale was chosen, and whether you expect the results to hold for larger models? Usually, we use smaller-scale model for interpretability, but I did not see explicit interpretive analysis in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "13MikxTSjT", "forum": "LGo7U1m24L", "replyto": "LGo7U1m24L", "signatures": ["ICLR.cc/2026/Conference/Submission20923/Reviewer_fD2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20923/Reviewer_fD2i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984559854, "cdate": 1761984559854, "tmdate": 1762938773771, "mdate": 1762938773771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}