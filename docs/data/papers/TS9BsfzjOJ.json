{"id": "TS9BsfzjOJ", "number": 8718, "cdate": 1758095848612, "mdate": 1763463970694, "content": {"title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction", "abstract": "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model’s path planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.", "tldr": "", "keywords": ["Language Models", "Path Planning", "Multi-Token Prediction", "Transformers"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61af1b650e76f0fcc2b1afe1800e8eb2692445ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to use multi-token predictions to improve language models' \"planning\" capabilities (e.g. path finding), which is an example of learning transitive relations.\n\nSpecifically, in addition to standard next-token prediction, they use a _transfer layer_ to predict a few future tokens; the same transfer layer is shared across different future positions.\n- Most of the results are on a graph path finding setup following ALPINE (Wang et al. 24), where the model is expected to learn the true reachability matrix (i.e. whether a pair of nodes are connected) given paths observed in the training set. \n\nFor theoretical results, they show with a simplified model that how predicting two-steps head (rather than only the next step) affects the model weights, in terms of the signs of gradients.\n\nFor empirical results, they apply multi-step prediction to improve the accuracy on the above synthetic path finding task and Blocksworld.\nAs implementation details, they use:\n- next-token injection, which feeds in the ground truth tokens as input during training.\n- a Transformer layer as the transfer layer, where the self-attention is across different dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper applies multi-token prediction to improve the transitive reasoning abilities in language models, which is well-justified.\n- The paper discusses both the pros and cons of multi-token prediction: while multi-token prediction helps capture higher-order reachability (Theorem 1,2), the paper also mentions that it may incorrectly bias the 1-step transition probability.\n  - The paper claims the benefit of improving transitive reachability outweighs the risk of biasing 1-step transitions.\n- The empirical results are promising:\n  - For path finding, 2-token or 3-token prediction improves the accuracy especially on paths that require composing information from more than 2 training data sequences. \n  - For Blockworld, the paper shows that multi-token prediction provides some improvement over the 1-token baseline at different path lengths.\n- The paper provides mechanistic study on how multi-token prediction affects the learned model."}, "weaknesses": {"value": "I'm concerned about insufficient contribution.\n\n- Theorem 1 and 2 (the results are about the signs of per-step gradients on a much simplified model) do not offer much beyond the intuitive explanation that the weight matrices are made to capture ground truth transitions.\n  - This also comes with the cost of introducing notations (that are unnecessary for getting the key messages of the section) that impedes reading, so I'd suggest to keep the informal theorem in the main paper and move the details to the appendix.\n\n- The experiment results are too limited to be convincing.\n  - Multi-token prediction aligns well with the structure of the path finding task, so improved performance is expected. For Blocksworld, the amount of improvement is minimal, and even worse than the baseline when using a linear transfer layer. The paper does not demonstrate how much multi-token prediction would benefit more general setup.\n  - For the path finding setup, the paper only considers Erdos graphs (with p=0.1) and not more structured graphs which are closer to applications (e.g. stochastic block models)."}, "questions": {"value": "- Sec 5.2, weight analysis: My understanding is that the transfer layer $W^T$ is not a Transformer layer in this case, since otherwise projecting $W^T$ doesn't make sense. Please clarify this in the writing.\n- Table 3: It's not clear how to compare the value of \"0.82\" and \"4.01\". What are the ranges of the values? What's the variance in these values when averaged over several runs? Are normalization layer used? If yes, are these values before or after normalization layers?\n- Table 1, 5: how many layers are used for the 1-token baseline? If it's 1-layer 1-head, then it's unclear whether the gain from multi-token prediction is due to the use of more Transformer layers (as transfer layers), or from multi-token training.\n\nMinor: on line 112: has $N$ been defined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yJttVGjS8j", "forum": "TS9BsfzjOJ", "replyto": "TS9BsfzjOJ", "signatures": ["ICLR.cc/2026/Conference/Submission8718/Reviewer_qfv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8718/Reviewer_qfv3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760822479332, "cdate": 1760822479332, "tmdate": 1762920519703, "mdate": 1762920519703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how multi-token prediction (MTP) can improve learning of transitive relations for path planning with Transformers. It analyzes a simplified one-layer Transformer with a shared output head and a linear “transfer layer” $W^T$ used to predict tokens multiple steps ahead. The analysis argues that: (i) WT is updated by the multi-step loss to approximate adjacency; (ii) gradients backpropagated through $W^T$  encourage the backbone to encode higher-order reachability; and (iii) some spurious adjacency may also be introduced. Two training/architectural tweaks are proposed: Next-Token Injection (NTI) that injects the ground-truth next-token embedding into the transfer input during training, and a Transformer-based transfer layer. Experiments on synthetic DAG path planning and Blocksworld show improvements over next-token training and re-implementations of “Meta-style” and “DeepSeek-style” MTP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The research question studied is critical and timely, with a clean framing (transitivity bottleneck in planning).\n\n2. The paper proposes a practical training tweak (NTI) that reliably improves results\n\n3. The evaluations are comprehensive, including multiple MTP depths, backbone variants, and scaling across graph sizes."}, "weaknesses": {"value": "1. Theorems are proven under a highly stylized setting: no positional embeddings, identity embeddings and output projection, fixed and manually set attention that always attends to the target (second position), linear FFN, single head/layer, and linear $W^T$ acting on logits.\n\n2. The result that “$W^T$ learns adjacency” depends critically on $(W^M_{(i,d)}+W^V_{(j,d)})>0$. This is not guaranteed; if the sum is negative (or zero), the update direction flips (or vanishes). The paper partially acknowledges this by conditioning statements on positivity but then informally equates “positive-correlated intermediate nodes” with feasible intermediates.\n\n3. The statement that $W^T_{(n−1)}$ “approximates the (n−1)-th power of the adjacency matrix” is not established. There is no proof of convergence or identification, and in the general case $W^T$ can absorb both adjacency and reachability information due to the non-uniqueness of the factorization with $W_t$ and $W_o$.\n\n4. Transformer-based transfer layer description is unclear. The paper says it “leverages self-attention to model dependencies across dimensions” of $h_n$. A standard Transformer attends over the sequence dimension; with a single vector $h_n$ as input, self-attention degenerates. If you reshape features into a length-d sequence, please specify the exact reshaping, positional encoding, number of heads, masking, and why this is preferable to a simple MLP. As written, this component is conceptually confusing and under-specified.\n\n5. Experimental details are missing. Decoding strategy, optimizer, learning rate schedule, batch size, epochs/steps, temperature, label smoothing, and regularization are not stated in the main text. Although a code link is provided, there is only a README file in it.\n\n6. Only DAGs are used. Real planning graphs often contain cycles. An evaluation on cyclic graphs (with appropriate sequence formatting to avoid trivial loops) would strengthen claims."}, "questions": {"value": "Suggestions:\n\n1. Some figures use very small fonts. Adjusting them could make it easier to read.\n2. Validate the gradient-sign predictions empirically by logging the sign of parameter updates vs the (p̂−pdata) terms during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yjUvNy7k1S", "forum": "TS9BsfzjOJ", "replyto": "TS9BsfzjOJ", "signatures": ["ICLR.cc/2026/Conference/Submission8718/Reviewer_ynaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8718/Reviewer_ynaJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512387926, "cdate": 1761512387926, "tmdate": 1762920519199, "mdate": 1762920519199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that making LLMs predict multiple future tokens instead of just the next one helps them plan better on graph-like tasks.\nThey prove that this setup makes the model’s extra prediction layer basically learn multi-hop adjacency so it actually understands longer paths."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Novel loss function to better train the model. Under MTP the transfer layer aligns with multi-hop adjacency, thus directly tying weights to structure. \n\n- Practical tweaks (Next-Token Injection and a Transformer-based transfer layer) are well-motivated and easy to slot into existing stacks. \n\n- Measurable gains on harder generalization (degree-2/3 paths) and Blocksworld, not just easy cases. \n\n- Learned transfer matrices progressively approximate true adjacency, thus giving a readable handle on structure."}, "weaknesses": {"value": "- Scope is narrow: focus on DAG path-planning; little evidence for labeled/heterogeneous graphs or richer semantics (node/edge features).\n\n- Scaling is lacking: experiments appear mid-scale; limited analysis for larger graphs, long paths, or real-world distributions.\n\n- Lack of baselines: The experiments only compare against standard next-token training. There’s no evaluation versus other planning-aware or structure-inducing methods. So it’s unclear whether MTP is uniquely effective or just another form of multi-step supervision."}, "questions": {"value": "- Permutation robustness: How stable are the learned transfer matrices under node relabeling or shuffled IDs? Any invariance tricks or results?\n\n- Can the approach handle weighted/noisy edges or node/edge features (e.g., typed relations)? What changes in the theory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NTLQUzmVrH", "forum": "TS9BsfzjOJ", "replyto": "TS9BsfzjOJ", "signatures": ["ICLR.cc/2026/Conference/Submission8718/Reviewer_7Rg3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8718/Reviewer_7Rg3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957214461, "cdate": 1761957214461, "tmdate": 1762920518797, "mdate": 1762920518797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how multi-token prediction enhances the planning capability of language models. On a synthetic path-finding task, the authors provide theoretical analysis and empirical validation showing that MTP training enables models to capture transitive reachability relations. The findings are also validated on the Blocksworld planning benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper theoretically and empirically shows that multi-token prediction enables the backbone model to learn transitivity reachability relations, which standard single-token prediction fails to capture. This finding contributes to our understanding of how MTP may improve the planning capability of language models, which is very interesting to me."}, "weaknesses": {"value": "1. The theoretical analysis focuses on one-step gradients without convergence guarantees. .\n2. Effectiveness of MTP:\n   1. In the simplified model, training with MTP allows the parameter $W^V$ to learn transitive reachability, whereas $W^M$ may capture spurious adjacency.   If I understand correctly, the degradation of $W^M$ could become more pronounced as the number of predicted tokens $n$ increases. It would be helpful for the authors to clarify the trade-off between these two effects as the number of predicted future tokens $n$ increases. \n   2. I am also curious whether a similar trade-off arises when applying MTP to more complex, real-world tasks.\n   3. If my understanding is correct, MTP with a constant number of predicted future tokens $n$ can only enhance the model’s planning ability within that constant number of steps. This raises the question of whether MTP fundamentally improves planning capability. Previous work [2] also proposes training the model to directly predict future tokens ahead of the original next token. Compared to MTP, this method can predict a token arbitrarily many steps after the current token. I wonder how the two methods compare.\n3. Novelty:\n   1. The proposed Next-Token Injection and Transformer-based transfer layers appear similar to the multi-token prediction components described in DeepSeek-V3 [1]. It would be great if the authors could make the distinctions explicit if there are substantial differences. Otherwise, these components may not constitute a main novelty.\n   2. Previous works already showed that multi-token prediction enhances planning capability. [3] \n4. The experiments are limited to synthetic tasks. It remains unclear whether MTP would significantly enhance planning capabilities in real-world or more complex reasoning benchmarks\n5. It would be helpful to discuss related studies examining MTP’s role in planning, including [2, 3].\n6. Minor comments: In Section 3.2, the notation $k$ is used both as a token index (l. 264) and as a node identifier (l. 284), which may cause confusion.\n\n[1] DeepSeek-AI. DeepSeek-V3 Technical Report.\n\n[2] Thankaraj et al. Looking beyond the next token.\n\n[3] Bachmann and Nagarajan. The Pitfalls of Next-Token Prediction."}, "questions": {"value": "1. Could the authors confirm whether my understanding is correct: By incorporating multi-token prediction, the matrix $W^V$ learns rechability relations beyond the observed $R^{\\text{obs}}$. For instance, a 2-token prediction, if edge $k,k'$ appears in the training data, and path $k' \\to t$ also appears, then $k\\to t$ will also be enhanced in $W^V$, even when the full path $k\\to t$ may not exist in the training data? However, for $W^M$, spurious adjacency may be introduced due to similar reasons. \n2. I wonder if the authors have observed degration in performance when increasing the number of predicted future tokens $n$ ? \n3. Could the authors clarify the main differences between the proposed NTI + Transformer transfer layer and the MTP architecture used in DeepSeek-V3 [1]?\n4. l156–157: The paper states that “the two architectures are mathematically equivalent.” I am unsure why this equivalence holds when the transfer layers are nonlinear. If additional assumptions (e.g., linear transfer layers) are required, I suggest making them explicit.\n5. l237: The main text argues that $W_{(j,k)}^V$ will increase when the model predicts a lower probability for $k'$ than the ground truth path $i\\to k\\to k'$. However, in Theorem 2, the condition is $\\hat P_{i,j}(k') < P^{\\text{data}}_{i,j}(k')$, where the node $k$ is not involved. My understanding is that the latter condition is the correct one. Can you confirm this or if I have misunderstood anything? \n\nPlease also see the weakness.\n[1] DeepSeek-AI DeepSeek-V3 Technical Report."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ylcyGSNLLe", "forum": "TS9BsfzjOJ", "replyto": "TS9BsfzjOJ", "signatures": ["ICLR.cc/2026/Conference/Submission8718/Reviewer_KRVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8718/Reviewer_KRVp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998680206, "cdate": 1761998680206, "tmdate": 1762920518417, "mdate": 1762920518417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}