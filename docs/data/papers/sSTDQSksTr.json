{"id": "sSTDQSksTr", "number": 20060, "cdate": 1758302000758, "mdate": 1763649728912, "content": {"title": "The Multi-Block DC Function Class: Theory, Algorithms, and Applications", "abstract": "We present the Multi-Block DC (BDC) class, a broad class of structured nonconvex functions that admit a DC (“difference-of-convex”) decomposition across parameter blocks. This block structure not only subsumes the usual DC programming, it turns out to be provably more powerful. Specifically, we demonstrate how standard models (e.g., polynomials and tensor factorization) must have DC decompositions of exponential size, while their BDC formulation is polynomial. This separation in complexity also underscores another key aspect: unlike DC formulations, obtaining BDC formulations for problems is vastly easier and constructive. We illustrate this aspect by presenting explicit BDC formulations for modern tasks such as deep ReLU networks, a result with no known equivalent in the DC class. Moreover, we complement the theory by developing algorithms with non-asymptotic convergence theory, including both batch and stochastic settings, and demonstrate the broad applicability of our method through several applications.", "tldr": "This work proposes and studies the broad class of multi-block DC functions with many applications.", "keywords": ["Multi-block DC", "DC programming", "non-convex optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e609573e69c2c47ca4f0deedda7e010efef08ff.pdf", "supplementary_material": "/attachment/a51e75a3bd1fcb33bc1f07876a797d7dacdea688.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Multi-Block DC (BDC) function class, a generalization of difference-of-convex (DC) programming to functions that admit DC decompositions per parameter block. The authors provide explicit, constructive BDC formulations for practical problems, including deep ReLU networks (with extensions to MSE regression and cross-entropy classification losses). The paper further develops the Block DC algorithms, including proximal and stochastic versions, with non-asymptotic convergence guarantees. Empirical performance is also illustrated through experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The BDC class extends DC programming to multi-block settings, addressing practical challenges in finding global DC decompositions. The BDC definition is simple yet powerful. \n\n2. The paper provides explicit tools and examples for formulating problems as BDC, making it accessible for applications in machine learning (e.g., tensor decomposition, neural network training).\n\n3. The proposed BDCA algorithms, including stochastic extensions, come with rigorous non-asymptotic convergence rates, which are solid and clearly stated."}, "weaknesses": {"value": "1. While the abstract mentions \"several experiments\", the paper focuses heavily on theory and algorithms. The current results compare mainly to vanilla SGD and on modest datasets. Full evaluation (e.g., benchmarks against baselines like ADAM on real datasets) is not enough, potentially weakening claims of practical superiority.\n\n2. For the deep ReLU BDC formulation, how does it handle common extensions like batch normalization, dropout, or other activations (e.g., GELU)? Is there a way to extend the constructive approach to these?\n\n3. The paper should also talk about computation and memory overhead of the proposed methods versus standard training."}, "questions": {"value": "See the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PfSpDfnbKz", "forum": "sSTDQSksTr", "replyto": "sSTDQSksTr", "signatures": ["ICLR.cc/2026/Conference/Submission20060/Reviewer_uPqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20060/Reviewer_uPqw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639688098, "cdate": 1761639688098, "tmdate": 1762932952081, "mdate": 1762932952081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study a class of (multi-)block DC programs that is **provably** broader than the classical class of DC programs. Specifically, given an objective function $f(\\theta)$ with $\\theta \\in \\mathbb{R}^d$, it is said to be a multi-block DC function if there exists a partition of the $d$ coordinates into $n$ blocks such that $f$ admits a DC decomposition with respect to each block of coordinates when the remaining blocks are fixed.\n\nThe main motivation stems from the observation that identifying a block DC structure is often much easier than finding a full DC structure, especially in problems with coupled variables. The authors further demonstrate that for monomials, an exponential number of atoms is required to represent them as DC functions, whereas only a polynomial number of atoms suffices under the block DC formulation.\n\nExploiting this block DC structure, the authors propose a block DC algorithm, which combines the principles of DCA with randomized block selection. Under the assumption that the first DC components (in each block) are $L$-smooth (or generalized $L$-smooth), the squared gradient norm of the objective function---or its expected value in the stochastic case---converges to zero at a rate of $\\mathcal{O}(1/K)$, matching the known rate for both stochastic and deterministic DCA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed framework is quite general and practically relevant. In many applications, it is indeed much easier to identify a block DC structure than a full DC structure, particularly when the variables are coupled. The presentation of the block DC formulation for neural networks with ReLU activation is also neat compared to existing approaches that rely on full-form DC decompositions for such composite functions. **P.S.** Please also do take a look at the paper *Cui, Y., He, Z., & Pang, J. S. (2020). Multicomposite nonconvex optimization for training deep neural networks. SIAM Journal on Optimization, 30(2), 1693-1723.* that gives explicit DC/MM structure for ReLU-activated neural networks of arbitrary depth.\n\n- The observation that DC decompositions of monomials exhibit exponential complexity, in contrast to the polynomial complexity of BDC, is insightful and provides solid motivation for the paper.\n\n- The paper is generally well-written."}, "weaknesses": {"value": "- The novelty of the paper appears quite limited. The core algorithmic design was previously introduced by [Pham et al., 2022] for DC functions (albeit not for BDC), and this prior work is not cited. Given this, the main contribution of the present paper likely lies in the non-asymptotic convergence analysis (for a larger class of BDC), which provides a rate comparable to that of standard (stochastic) DCA. If this is indeed the key advancement, the authors should explicitly emphasize this analytical contribution and highlight the technical contribution to obtain this results.\n\n- Another major concern is that the experiments do not show the advantages of the proposed method. The BDCA's performance is actually worse than simple SGD in all cases. Also, the classification accuracies of 90% for MNIST and 50% for CIFAR10 are very far from SOTA results. This makes me wonder if the method really works for complicated tasks.\n\n**REFERENCE**\n\nPham, V. T., Luu, H. P. H., & Le Thi, H. A. (2022). A block coordinate DCA approach for large-scale kernel SVM. In International Conference on Computational Collective Intelligence (pp. 334-347). Cham: Springer International Publishing."}, "questions": {"value": "- The definition of the gradient norm $\\mathcal{G}$ seems invalid to me. The notion of $\\partial f$ is ill-posed for DC functions, what kind of $\\partial$ is it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XQ4n3TSHxX", "forum": "sSTDQSksTr", "replyto": "sSTDQSksTr", "signatures": ["ICLR.cc/2026/Conference/Submission20060/Reviewer_qDo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20060/Reviewer_qDo4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728381940, "cdate": 1761728381940, "tmdate": 1762932951681, "mdate": 1762932951681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces multi-block DC (BDC) functions, a special class of nonconvex functions. Each function is split into blocks of variables, and within each block, it can be written as a convex function minus another convex function. This allows some functions, such as monomials or deep ReLU networks, to be represented more compactly than with standard DC decomposition.\n\nThe authors show how to decompose deep ReLU networks into BDC form, develop algorithms to minimize BDC functions (including a stochastic block method), and prove convergence guarantees. They also provide numerical experiments to illustrate the practicality of the decomposition for analyzing neural networks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is conceptually interesting and clearly demonstrates an advantage of the BDC formulation over classical DC decomposition, particularly in settings such as the training of deep ReLU networks. The authors provide convergence guarantees for their proposed algorithms, which adds theoretical rigor to the work. In addition, the paper validates its contributions with numerical experiments on relevant examples, showing that the proposed methods are practical."}, "weaknesses": {"value": "The encoding size of the BDC decomposition for deep ReLU networks is not clearly discussed, and it seems that there could be an exponential blow-up with the number of layers. It would be helpful if the authors could clarify this point. Furthermore, the theoretical convergence guarantees assume $L_i$-smoothness of the functions, which does not hold for ReLU networks. A discussion of this limitation and its implications for practical use would strengthen the paper."}, "questions": {"value": "- Can the authors provide an estimate of the encoding size or complexity of the BDC decomposition for deep ReLU networks, and discuss - In the paper https://arxiv.org/pdf/2411.03006, Proposition 4.2 presents a more efficient way to decompose a ReLU function into a difference of two convex functions (using maxout, but this should be adaptable to ReLU). Can this approach be used to obtain a smaller BDC decomposition of the training problem?\n- How do the assumptions in the convergence proofs (e.g., $L_i$-smoothness) affect the applicability of the algorithms to ReLU networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "65JZTAg0uM", "forum": "sSTDQSksTr", "replyto": "sSTDQSksTr", "signatures": ["ICLR.cc/2026/Conference/Submission20060/Reviewer_SV7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20060/Reviewer_SV7Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820256968, "cdate": 1761820256968, "tmdate": 1762932951323, "mdate": 1762932951323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Multi-Block DC (BDC) class—a broad new family of structured nonconvex functions.\n\nThe authors show that this block-wise structure is more general and powerful than the classical DC class.\n\nThey provide explicit, constructive BDC decompositions for modern machine learning models, including deep ReLU networks.\n\nThey also establish non-asymptotic convergence guarantees under batch, stochastic, and generalized smoothness settings.\n\nThe method is shown to be applicable to tasks such as sparse dictionary learning, multitask feature learning, and neural network training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper shows that deep ReLU networks can be reformulated as a non-smooth DC function, as presented in Equation (3.2).\n\n2. This paper considers a suite of algorithms—batch, stochastic, and proximal—with detailed non-asymptotic convergence analyses under various conditions."}, "weaknesses": {"value": "1. The authors reformulate ReLU neural networks as equivalent DC optimization problems and solve them using proximal DC algorithms. However, the motivation behind this reformulation and the choice of the proximal DC approach is not clearly justified. It is also unclear what advantages these methods offer over simpler and widely used algorithms such as SGD.\n\n2. The proximal DC algorithm and its theoretical analysis lack novelty. Many of the results do not appear particularly new. The algorithm discussed in Section 4 is a well-established method—essentially a form of coordinate descent—that has been extensively used for various nonconvex optimization problems (including those listed in Section 5). Hence, both the algorithm and the analysis feel incremental.\n\n3. While the paper presents numerical experiments, the empirical section is rather limited compared to the theoretical development. The experiments mainly serve as proof-of-concept demonstrations rather than comprehensive validations against state-of-the-art methods.\n\n4. The proposed algorithms introduce considerable complexity, as each block update involves solving a convex subproblem. The computational cost of these inner-loop optimizations—particularly for complex blocks such as neural network layers—is not thoroughly discussed or compared with simpler alternatives like SGD.\n\n5. Sections 3 and 5 feel somewhat disconnected; the applications are presented in a fragmented manner rather than being integrated into a cohesive narrative.\n\n6. It remains unclear why the ReLU network is assumed to satisfy the (L_i)-smoothness condition."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSOGk0hKjh", "forum": "sSTDQSksTr", "replyto": "sSTDQSksTr", "signatures": ["ICLR.cc/2026/Conference/Submission20060/Reviewer_xn9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20060/Reviewer_xn9a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015041066, "cdate": 1762015041066, "tmdate": 1762932950931, "mdate": 1762932950931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}