{"id": "LRpJ5sYgcy", "number": 24922, "cdate": 1758361955436, "mdate": 1759896742351, "content": {"title": "BayesShift: Evolving Domain Generalization via Hamiltonian Monte Carlo", "abstract": "Evolving Domain Generalization (EDG) addresses learning scenarios where the data distribution evolves over time, a setting crucial for real-world applications under varying environmental conditions. Recently, structure-aware variational models have shown promise by disentangling static and variant information, but their reliance on point estimates for model parameters neglects parameter uncertainty, limiting both adaptability and reliability. We propose BayesShift, a full Bayesian framework that parameterizes a latent structure-aware autoencoder to capture static features, distribution drift, and categorical shifts. Unlike standard variational inference, our method leverages Hamiltonian Monte Carlo (HMC) to approximate the posterior over latent variables, enabling principled quantification of uncertainty, which not only improves robustness to evolving distributions but also provides confidence estimates for predictions, a critical property in safety-sensitive domains. Experiments on benchmark datasets demonstrate that BayesShift achieves higher robustness to evolving distributions, outperforming state-of-the-art baselines in both predictive accuracy and adaptability. These results highlight the effectiveness of Bayesian inference for evolving domain generalization.", "tldr": "We propose a full Bayesian framework that parameterizes a latent structure-aware autoencoder to capture static features, distribution drift, and categorical shifts, leveraging Hamiltonian Monte Carlo to approximate the posterior over latent variables", "keywords": ["Evolving Domain Generalization", "Hamiltonian Monte Carlo", "Variational Autoencoder"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be5487dd2334950f8cdef6b65e19ea865dc87974.pdf", "supplementary_material": "/attachment/94f28a1615084008707a86064cca2689ee50e413.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces BayesShift, a novel framework for Evolving Domain Generalization (EDG) that addresses performance degradation under evolving data distributions. The core idea is to replace the standard variational inference in a latent variable model with Hamiltonian Monte Carlo (HMC) sampling to achieve a more accurate posterior approximation. This approach aims to better disentangle static features from distribution and concept shifts, while also providing principled uncertainty quantification. The authors conduct experiments on several benchmarks, claiming superior accuracy over state-of-the-art methods and demonstrating the value of HMC through a strong ablation study."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a key limitation in current EDG research—the inadequate modeling of uncertainty. The introduction of HMC for posterior inference over latent variables in this context is a novel and theoretically sound contribution, providing a more principled Bayesian approach to handle the dynamics of evolving domains.\n\n2. The paper demonstrates a key benefit of its Bayesian approach: principled uncertainty quantification. The experiments clearly show that the model's predictive uncertainty increases as domains shift away from the source distribution, highlighting its potential for building more trustworthy and reliable AI systems.\n\n3. The paper is well-written and easy to follow. The authors clearly articulate the problem and position their work effectively. By building upon a known architecture, they successfully focus the reader's attention on their main innovation: the HMC-based inference mechanism."}, "weaknesses": {"value": "1. The experimental validation (Table 1) for the model's claimed ability for distribution drift and categorical shift is insufficient. The paper lacks a granular analysis showing performance on two shift types, and the reported performance of a key baseline (MMD-LSAE) is suspiciously low compared to its original publication, which questions the fairness of the comparison and the proposed method's advantage.\n\n2. The claim that uncertainty estimation is valuable for safety-critical applications is not substantiated with a concrete demonstration. The paper shows that the model can produce uncertainty estimates, but fails to provide a case study or experiment illustrating how these estimates could be used in practice to trigger a safety mechanism or improve decision-making.\n\n3.  While the authors briefly mention the high computational cost of HMC in the conclusion, the evaluation requires a quantitative analysis of this critical trade-off. Please provide empirical data on key performance metrics such as inference latency, FLOPS, and number of parameters."}, "questions": {"value": "Pls refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qlbEkIh1Me", "forum": "LRpJ5sYgcy", "replyto": "LRpJ5sYgcy", "signatures": ["ICLR.cc/2026/Conference/Submission24922/Reviewer_ny84"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24922/Reviewer_ny84"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707075408, "cdate": 1761707075408, "tmdate": 1762943244685, "mdate": 1762943244685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BayesShift, a Bayesian framework for Evolving Domain Generalization(EDG). It extends latent structure aware autoencoders by incorporating Hamiltonian Monte Carlo (HMC) sampling to estimate the posterior over latent variables, aiming to capture both static and dynamic domain features under evolving distributions. Theoretically, the method replaces the traditional variational inference (VI) used in prior DG work with a fully Bayesian sampling scheme. Experiments are conducted on six datasets (Circle, Sine, RMNIST, Caltran, PowerSupply, Portraits), and the authors claim that BayesShift achieves higher accuracy and robustness compared to several domain generalization baselines, including MMD-LSAE (an earlier EDG model). However, the experimental evaluation mainly compares against standard DG methods, and the only EDG baseline (MMD-LSAE) is not representative of current state-of-the-art EDG models such as SDE-EDG (ICLR 2024) or SYNC (ICML 2025). Therefore, while the HMC-based Bayesian formulation is technically sound, the evidence presented does not convincingly demonstrate progress in the EDG field."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tIntegrates Bayesian inference (via HMC) into domain generalization, providing uncertainty quantification.\n•\tClear modular structure with disentangled latent variables for static and dynamic components."}, "weaknesses": {"value": "•\tIncomplete EDG benchmarking: only one EDG baseline (MMD-LSAE) is tested, which is not representative of current SOTA.\n•\tLack of temporal modeling: the framework does not explicitly capture the dynamics between consecutive domains.\n•\tLimited novelty: substituting VI with HMC adds sampling fidelity but not conceptual progress for EDG.\n•\tMissing analysis: no discussion of HMC efficiency, convergence, or hyperparameter sensitivity.\n•\tUnclear theoretical justification: derivations are abbreviated and omit key intermediate steps."}, "questions": {"value": "1. Why were newer EDG methods such as FORESEE, CTDG, or EvoS not included in comparison?\n2. How sensitive is BayesShift to HMC hyperparameters (step size ε , leapfrog steps L, number of samples N)?\n3. What is the computational cost relative to variational inference?\n4. How does uncertainty estimation specifically help adaptation under evolving domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "No5IzeLbwt", "forum": "LRpJ5sYgcy", "replyto": "LRpJ5sYgcy", "signatures": ["ICLR.cc/2026/Conference/Submission24922/Reviewer_fd71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24922/Reviewer_fd71"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095778210, "cdate": 1762095778210, "tmdate": 1762943244169, "mdate": 1762943244169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BayesShift, a Bayesian framework for Evolving Domain Generalization (EDG) that augments a latent, structure-aware autoencoder with Hamiltonian Monte Carlo (HMC) to sample posteriors over latent variables capturing static content, distribution drift, and categorical shift. Replacing variational ELBO with HMC-based posterior sampling enables uncertainty quantification and, empirically, improves robustness to evolving domains. Across six benchmarks (synthetic and real), BayesShift attains the best average target-domain accuracy and provides calibrated confidence estimates; ablations indicate substantial gains from HMC over variational inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Models both distribution drift and concept shift with an explicit generative formulation, and uses HMC to capture posterior uncertainty rather than point estimates.\n2. Provides predictive entropy and confidence intervals that increase with domain shift, aligning with safety-critical requirements.\n3. Derives gradients as posterior expectations, details HMC with leapfrog integration, and integrates seamlessly with a structure-aware autoencoder."}, "weaknesses": {"value": "1. HMC is expensive; the work lacks wall-clock/FLOP reporting, scalability analysis, and comparisons to efficient samplers (e.g., SGHMC/SGLD) or online/continual EDG settings.\n2. Limited comparisons to recent EDG/CDA or Bayesian baselines; MMD-LSAE outperforms on Sine; tuning protocols and statistical significance (seeds, CIs) are under-reported.\n3. HMC hyperparameters (step size, number of leapfrog steps), latent dimensionality, and encoder roles may strongly affect results; little analysis of sensitivity or disentanglement quality."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wvOeIRSLDZ", "forum": "LRpJ5sYgcy", "replyto": "LRpJ5sYgcy", "signatures": ["ICLR.cc/2026/Conference/Submission24922/Reviewer_ckBD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24922/Reviewer_ckBD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147058310, "cdate": 1762147058310, "tmdate": 1762943243912, "mdate": 1762943243912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}