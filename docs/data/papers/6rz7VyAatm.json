{"id": "6rz7VyAatm", "number": 16302, "cdate": 1758262940884, "mdate": 1759897248857, "content": {"title": "BadDet+: Robust Backdoor Attacks for Object Detection", "abstract": "Backdoor attacks threaten the integrity of deep learning models by allowing adversaries to implant hidden behaviors that activate only under specific conditions. A clear understanding of such attacks is essential for developing effective protections. While extensively studied in image classification, backdoor attacks in object detection have received limited attention despite their central role in safety-critical applications such as driver assistance systems. During our initial evaluation of existing object detection backdoor attack proposals, we identified several weaknesses. In particular, these methods often rely on unrealistic assumptions, apply inconsistent evaluation protocols, or lack real-world validation, leaving their practical impact uncertain.\nWe address these gaps by introducing BadDet+, a principled penalty-based attack framework that unifies region misclassification (RMA) and object disappearance (ODA) under a single mechanism. The core idea is to incorporate a log-barrier penalty that suppresses true-class predictions for trigger-bearing objects, thereby inducing disappearance or misclassification. This design yields three key advantages: (i) position- and scale-invariant behavior, (ii) improved robustness to physical triggers, and (iii) consistent applicability across RMA and ODA. \nOn a real-world benchmark, BadDet+ achieves stronger synthetic-to-physical transfer than prior work, outperforming existing RMA and ODA baselines while preserving clean-task performance. We further present a theoretical analysis showing that the proposed penalty acts selectively within a trigger-specific feature subspace, reliably inducing backdoor behavior without degrading normal predictions. Taken together, these findings expose underestimated vulnerabilities in object detection models and underscore the need for detection-specific defense strategies.", "tldr": "", "keywords": ["backdoor attack", "object detection", "adversarial machine learning", "machine learning security"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa16b66fc7913b5ab8e7ae3221ddde6492ba547d.pdf", "supplementary_material": "/attachment/92c1ed7101ad76d9e5d862ba93ffc45e56b11399.zip"}, "replies": [{"content": {"summary": {"value": "This work exposes weaknesses in existing backdoor attacks on object detection models and proposes **BadDet+**, a unified and more practical attack framework. BadDet+ uses a log-barrier penalty to force triggered objects to disappear or be misclassified, achieving **position- and scale-invariance**, **robustness to physical triggers**, and **consistent attack behavior**. Experiments show it transfers well from digital to real-world settings and outperforms prior methods without harming clean performance. Theoretical analysis explains how it operates in a trigger-specific feature space, highlighting overlooked vulnerabilities in object detection and the urgent need for better defenses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written. It is easy to follow the key idea of this paper and follow the proposed scheme.\n\n2. Physical benchmark evaluation. BadDet+ achieves stronger synthetic-to-physical transfer than prior work, outperforming existing RMA and ODA baselines while preserving standard performance."}, "weaknesses": {"value": "1. Lack of evaluation of robustness. In line 92, the authors claim that the proposed scheme has improved the robustness. However, the experimental section only evaluate the fine-tuning defense to support this claim, which is really insufficient. There are plenty of defenses including image transformation, image detection, model pruning defenses to evalute the robustness.\n\n2. Lack of visual demonstrations. I am curious about the proposed backdoor attack's visual demonstrations, which are more direct to show its effectivensss.\n\n3. Lack of intuition behind the proposed loss penalty. Why the sigmoid function can achieve the proposed effect? It needs to be further clarified. \n\n4. Lack of technical contribution of the proposed scheme. The only design of the proposed backdoor attack lies in the sigmoid function, which seems less challenging and lacks of novelty."}, "questions": {"value": "1. Where is the formulation of the loss $\\mathcal{L}_{det}$?\n\n2. The \"ICLR 2025\" should be \"ICLR 2026\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OlrmqFRLp3", "forum": "6rz7VyAatm", "replyto": "6rz7VyAatm", "signatures": ["ICLR.cc/2026/Conference/Submission16302/Reviewer_ty81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16302/Reviewer_ty81"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932240930, "cdate": 1760932240930, "tmdate": 1762926443123, "mdate": 1762926443123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that existing backdoor attacks in object detection are inadequate in terms of inconsistent evaluation and/or unrealistic assumptions. They propose a method that unifies RMA and ODA attacks with a mechanism that suppresses true class predictions. Experiments demonstrate generally improved performance compared to prior approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper argues that there are issues with current evaluation approaches, e.g. ASR overstating success and mAP results being skewed by duplicate detections. The proposed TDR is a sound measure to help alleviate some of the issues.\n\nBadDet+ itself is well motivated, and while the underlying idea is simple, unifying untargeted ODA and RMA under a single mechanism is attractive, and the comprehensive experimental results demonstrate its effectiveness.\n\nOverall, this is a good contribution, though perhaps quite incremental."}, "weaknesses": {"value": "As the authors themselves identify, the approach assumes that training is controlled by the adversary. This is a very strong assumption, though it is not unreasonable to assume such a worst case in some scenarios.\n\nThe idea is quite simple and incremental on prior work.\n\nMinor:\n- The poor performance compared to BadDet on YOLO for MTSD and PTSD is not adequately highlighted in the main text, where it is stated as \"on par with BadDet\". Some discussion is in the appendix, but this main text mention feels a bit understated.\n\n- The caption on figure 1 reads a bit confusingly"}, "questions": {"value": "A couple of minor weaknesses above could perhaps be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gHHHWpTGca", "forum": "6rz7VyAatm", "replyto": "6rz7VyAatm", "signatures": ["ICLR.cc/2026/Conference/Submission16302/Reviewer_mYnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16302/Reviewer_mYnH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933108668, "cdate": 1760933108668, "tmdate": 1762926442724, "mdate": 1762926442724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BadDet+, a penalty-based backdoor attack framework for object detection, unifying region misclassification and object disappearance with log-barrier penalties, achieving strong physical robustness and transferability while maintaining clean accuracy, revealing critical security vulnerabilities in detection models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a clear and comprehensive related work.\n2. The authors identify the incomplete success of traditional backdoor attacks and propose a new loss function to optimize these bad cases, making the backdoor more robust and effective."}, "weaknesses": {"value": "1. The paper actually adopts a stronger attacker assumption — the ability to manipulate the loss function (i.e., the training process) — which is clearly different from traditional data poisoning attacks. Although I acknowledge the validity of this threat model, I believe the authors should clearly explain these points before introducing their method. Otherwise, comparing it with other data poisoning–based backdoor attacks is, in my view, of limited significance. \n2. The paper does not clearly explain in the methodology section how it becomes “a single formulation that generalizes to RMA and untargeted ODA settings.” I believe providing concrete examples would help make this point much clearer."}, "questions": {"value": "1. I’m still not quite clear on how Section 4.1 FORMULATION demonstrates that ODA and RMA can be unified under a single framework — I couldn’t find a clear explanation of this point. Also, what if more types of attacks are considered? Can they also be incorporated into this unified framework? In other words, why are ODA and RMA specifically chosen?\n\n2. Why does the paper claim that BadDet+ bridges the synthetic-to-physical performance gap and achieves position- and scale-invariant backdoor behavior? These benefits don’t seem to come from the core method described in Section 4.1 but rather from common data augmentation techniques. If that’s the case, any backdoor attack could be similarly enhanced. In other words, these advantages are not intrinsic to the proposed method itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DfPXVmYsFD", "forum": "6rz7VyAatm", "replyto": "6rz7VyAatm", "signatures": ["ICLR.cc/2026/Conference/Submission16302/Reviewer_Nubo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16302/Reviewer_Nubo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298234374, "cdate": 1761298234374, "tmdate": 1762926442403, "mdate": 1762926442403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes novel backdoor poisoning attack BadDet+, which includes a loss into the training objective that penalizes the appearance of the true label in top classes (at prediction time).\nUsing this regularization, the methodology ensures that the scores of the true label drops in presence of the trigger.\nAlso, the paper proposes novel evaluation metrics, since other attacks could still exhibit the real label as one of the most likely class during prediction, and this is confirmed by the experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ the simplicity of a training time regularizer makes this technology easy to understand and also to stage\n+ the paper also propose an ablation study on the parameters of the methodology, highlighting the depth of the study\n+ novel metrics show that previous work might have overfitted the goal of mislabelling rather than being sure that the true label is not considered"}, "weaknesses": {"value": "- the fact that the paper is presenting a new metric, and with this the performances of the proposed methods are way way better than the literature might rise the doubt on the metric being overfit by BadDet+. Hence, it would be better to show some examples also on regular metrics, or ablation studies also on the 0.5 that has been deemed threshold on the IoU. \n- there is no clear technical description of the nature of the trigger, could its shape and color change the entire method? Like triggers that, by chance, are similar to the object on which they are applied, thus being less effective. The paper should better discuss how scenarios like these were avoided.\n- even with a different metric, it is different to compare results: were the methods trained in the same settings? Same poisoning ratios? Same trigger sizes? Results might change a lot, considering that the paper states that default parameters have been used for the techniques of state of the art. The appendix provides some insights, but they are not incredibly clear and should be moved to the main paper to some extent.\n- limitations are not discussed, the paper draws conclusions without considering possible issues of the proposed approach."}, "questions": {"value": "1) what happens in same backdooring conditions, i.e. same ratio of poisoning samples, same trigger sizes, etc?\n2) what happens whether the metrics AS@50 changes ratio? Like 10 to 90? How this is can be connected to the standard ASR when varying these quantities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Osd3nOubvW", "forum": "6rz7VyAatm", "replyto": "6rz7VyAatm", "signatures": ["ICLR.cc/2026/Conference/Submission16302/Reviewer_Pccc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16302/Reviewer_Pccc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400212196, "cdate": 1761400212196, "tmdate": 1762926441973, "mdate": 1762926441973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}