{"id": "nDrZow7fCF", "number": 8500, "cdate": 1758088040126, "mdate": 1763738311754, "content": {"title": "UniRestorer: Universal Image Restoration via Adaptively Estimating Image Degradation at Proper Granularity", "abstract": "Recently, considerable progress has been made in all-in-one image restoration. Generally, existing methods can be degradation-agnostic or degradation-aware. However, the former are limited in leveraging degradation estimation-based priors, and the latter suffer from the inevitable error in degradation estimation. Consequently, the performance of existing methods has a large gap compared to specific single-task models. In this work, we make a step forward in this topic, and present our UniRestorer with improved restoration performance. Specifically, we perform hierarchical clustering on degradation space, and train a multi-granularity mixture-of-experts (MoE) restoration model. Then, UniRestorer adopts both degradation and granularity estimation to adaptively select an appropriate expert for image restoration. In contrast to existing degradation-agnostic and -aware methods, UniRestorer can leverage degradation estimation to benefit degradation-specific restoration, and use granularity estimation to make the model robust to degradation estimation error. Experimental results show that our UniRestorer outperforms state-of-the-art all-in-one methods by a large margin, and is promising in closing the performance gap to specific single-task models. The code and pre-trained models will be publicly available.", "tldr": "", "keywords": ["low-level vision", "image restoration", "all-in-one image restoration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94f8319db44775d0789b33803d5dbce5127ed6b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "UniRestorer presents a strong universal image restoration framework that achieves impressive performance across single, mixed, OOD, and even real-world degradations. Its combination of multi-granularity degradation representations, hierarchical expert specialization, and uncertainty-aware routing enables both robustness and precision, outperforming prior all-in-one baselines and approaching single-task SOTA results.\nHowever, the core mechanism raises conceptual concerns: the training pipeline is overly complex, and the single-expert activation strategy appears suboptimal and parameter-inefficient. Clarification on these design choices—especially how the MoE routing behaves under mixed degradations—would be crucial to fully assess the method’s contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper’s fine-grained degradation representation learning is a clear strength. By retraining a DA-CLIP–based extractor with fine-grained textual labels (e.g., light/medium/heavy noise or haze) and contrastive supervision, the authors enable the model to capture not only degradation types but also intensity levels.  Supplementary t-SNE visualizations results show that these representations are separable at both coarse and fine granularity, providing a solid foundation for the subsequent hierarchical clustering and multi-granularity expert design.\n2. Demonstrates strong cross-distribution generalization, maintaining stable PSNR/SSIM across single, mixed, OOD, and real-world degradations, with zero-shot gains on unseen types.\n3. Uncertainty-aware hierarchical routing adaptively selects coarse or fine experts for robustness or precision, reducing routing ambiguity and representation conflict while matching or surpassing single-task performance.\n4. Comprehensive evidence beyond parameter scaling: broad baselines and ablations show gains arise from division of labor and routing, with the LoRA variant retaining near full-model performance."}, "weaknesses": {"value": "1. The inference scheme activates only a single expert at a time, which limits the expressive power of the MoE and introduces substantial parameter redundancy, as many experts remain unused for each input.\n2. When the MoE system encounters mixed degradations, how frequently does the router fall back to the 0-th level (coarse) expert?\nIf this fallback occurs in most cases, it is unclear why the model significantly outperforms a single Restormer trained directly on mixed degradations. Conversely, if the router instead activates a fine-level expert (e.g., a “rain” or “haze” expert) for a mixed input such as rain-haze, it would contradict the intended degradation clustering principle and could potentially degrade performance. Clarification on the router’s behavior and expert selection under mixed-degradation inputs is needed.\n3. The overall training pipeline is overly complex and resource-intensive. It requires first training a degradation extractor, then performing hierarchical clustering, followed by separate training for multiple experts and an additional router stage.\n4. The paper does not clarify how data sufficiency is ensured for the fine-granularity experts. Since the hierarchical clustering process recursively divides the training set into smaller subsets, some fine-level clusters may contain only a limited number of samples. It is unclear whether the authors applied any method to avoid underfitting or data imbalance across experts."}, "questions": {"value": "The second weakness is the main issue that confuses me the most. I would appreciate a clear explanation, and I may consider raising my rating if it is addressed convincingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xUUQtmJWIn", "forum": "nDrZow7fCF", "replyto": "nDrZow7fCF", "signatures": ["ICLR.cc/2026/Conference/Submission8500/Reviewer_Vp55"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8500/Reviewer_Vp55"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204439118, "cdate": 1761204439118, "tmdate": 1762920373038, "mdate": 1762920373038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UniRestorer, a universal all-in-one image restoration framework that bridges degradation-agnostic and degradation-aware paradigms. It performs hierarchical clustering on degradation representations to build a multi-granularity mixture-of-experts (MoE) model. Through degradation and granularity estimation, the router adaptively selects fine- or coarse-grained experts, achieving robustness against estimation errors. Experiments on single- and mixed-degradation benchmarks show that UniRestorer surpasses prior all-in-one methods and nearly matches single-task performance"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a multi-granularity degradation representation that unifies coarse- and fine-grained experts. The idea of granularity estimation to quantify degradation uncertainty and guide routing is novel and intuitive.\n2. Comprehensive experiments: covers 7 single-degradation and 11 mixed-degradation settings, plus real-world and unseen tasks.\n3. Paper is well-organized and technically detailed with intuitive figures (especially Fig. 3 illustrating routing).\n4. The hierarchical MoE design could inspire cross-domain generalization and adaptive restoration architectures."}, "weaknesses": {"value": "1. While granularity estimation is conceptually convincing, there’s no formal uncertainty-theoretic or probabilistic analysis of its behavior.\n2. Each granularity level adds parameters and routing complexity; actual FLOPs and latency comparisons are limited. The LoRA variant helps, but trade-offs between full and LoRA experts could be better quantified.\n3. The K-means–based clustering assumes a consistent degradation embedding space; sensitivity to clustering hyperparameters (number of clusters, feature normalization) is not reported.\n4. Evaluation primarily uses synthetic degradations; more real-world degradation diversity (e.g., motion blur, ISP artifacts) would strengthen the claim of universality.\n5. The effect of routing noise or errors in granularity estimation itself is underexplored; visualization of routing confidence would add interpretability."}, "questions": {"value": "1. How sensitive is performance to the number of levels (l = 3) or cluster count per level? Could adaptive clustering improve robustness?\n2. Can you visualize which experts are selected for different degradations and how granularity affects routing under ambiguous inputs?\n3. Could the same hierarchical expert tree generalize to unseen degradation combinations(e.g., low-light + blur + noise) without retraining?\n4. In hybrid usage, can user-provided degradation cues override granularity routing? Would it further close the gap with single-task models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qMw03xkIDw", "forum": "nDrZow7fCF", "replyto": "nDrZow7fCF", "signatures": ["ICLR.cc/2026/Conference/Submission8500/Reviewer_tpRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8500/Reviewer_tpRc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901734695, "cdate": 1761901734695, "tmdate": 1762920372683, "mdate": 1762920372683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UniRestorer proposes a multi-granularity MoE framework for all-in-one image restoration. It (1) hierarchically clusters degradation space, (2) trains multi-level MoE experts, and (3) uses joint degradation + granularity estimation to route inputs to the most suitable expert. Claims large gains over SOTA all-in-one models and narrows gap to single-task models. Code and models will be released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Granularity estimation elegantly handles degradation estimation noise — robust and practical.\nHierarchical clustering + MoE scales well across 15+ degradation types.\nLarge, consistent gains (e.g., +2.1 dB PSNR on mixed test sets).\nComprehensive ablations (granularity levels, routing loss, expert count).\nClean figures: t-SNE of degradation space, expert activation heatmaps."}, "weaknesses": {"value": "Clustering is offline and static — no online adaptation to new/unseen degradations.\nGranularity estimator adds overhead — no inference latency reported (vs. PromptIR, AirNet).\nNo theoretical justification for hierarchical clustering choice (e.g., why 3 levels?).\nEvaluation limited to synthetic degradations — no real-world camera pipeline (e.g., RAW → ISP).\nMoE training unstable? No mention of load balancing loss or expert collapse."}, "questions": {"value": "Report inference FPS on RTX 3090 for 512×512 input — how much slower than PromptIR?\nTest on real-world degradations (e.g., DND, SIDD, GoPro real blur).\nAblate dynamic clustering — can granularity be learned end-to-end without offline K-means?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ODQYicWArj", "forum": "nDrZow7fCF", "replyto": "nDrZow7fCF", "signatures": ["ICLR.cc/2026/Conference/Submission8500/Reviewer_TgnW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8500/Reviewer_TgnW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905887455, "cdate": 1761905887455, "tmdate": 1762920372304, "mdate": 1762920372304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing all-in-one restoration schemes are either degradation-agnostic or degradation-aware, leaving a clear performance gap to single-task experts. This paper proposes UniRestorer, which first hierarchically clusters the degradation space and trains a multi-granularity mixture-of-experts (MoE) network. At inference it jointly estimates degradation type and granularity to activate the most suitable expert. Extensive experiments show that UniRestorer significantly surpasses state-of-the-art all-in-one competitors and narrows the gap to dedicated single-task models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes UniRestorer, the first framework that simultaneously exploits degradation and granularity estimation to overcome the inherent limitations of both degradation-agnostic and degradation-aware restoration methods.\n2. Extensive quantitative and qualitative experiments convincingly demonstrate the superiority of UniRestorer over existing all-in-one baselines and its competitive performance against task-specific models.\n3. The idea of granularity-aware expert selection is clearly articulated and technically grounded; it offers a fresh insight that could inspire future work on robust universal image restoration."}, "weaknesses": {"value": "1. The paper lacks a quantitative analysis of the proposed hierarchical degradation-clustering step.\n2. No comparison or discussion is provided against alternative clustering strategies (e.g., the spectral clustering adopted in SEAL).\n3. It is unclear whether the Restormer baseline in Table 1 was re-trained under exactly the same degradation protocol and parameter budget; an ablation that removes both degradation and granularity estimation while keeping the backbone capacity fixed would better isolate the gain of the proposed method."}, "questions": {"value": "Please check the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kS6MPjoJI7", "forum": "nDrZow7fCF", "replyto": "nDrZow7fCF", "signatures": ["ICLR.cc/2026/Conference/Submission8500/Reviewer_xgD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8500/Reviewer_xgD4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009641078, "cdate": 1762009641078, "tmdate": 1762920371901, "mdate": 1762920371901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for all the precious comments and valuable advice. In view of these suggestions, we conducted several additional experiments and revised our article accordingly. The major changes are summarized as follows.\n\n- We added a **quantitative analysis** of the proposed hierarchical degradation **clustering** in Appendix C.\n\n- We provided additional **comparisons and discussions** of our model using alternative clustering strategies, including **spectral clustering and adaptive clustering** in Appendix E.\n\n- We conducted **additional experiments** of our model on real-world degradations, such as **real-world denosing, real-world deblurring, and ISP artifacts removal** in Appendix D. \n\n- We included a comprehensive computational cost analysis, including **inference latency, FPS, parameters, training time, and inference time** in Appendix D.\n\n- We **visualized the granularity routing** and evaluated the **effect of routing errors** in granularity estimation of our method in Appendix F.\n\n- We provided **a theoretical justification** for our choice of hierarchical clustering in Appendix C and conducted an **uncertainty-theoretic analysis** for granularity estimation in Appendix A.\n\n- We added an analysis of our model with **user-provided degradation cues** in Appendix E."}}, "id": "vKWhqaooDe", "forum": "nDrZow7fCF", "replyto": "nDrZow7fCF", "signatures": ["ICLR.cc/2026/Conference/Submission8500/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8500/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission8500/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763734567796, "cdate": 1763734567796, "tmdate": 1763734567796, "mdate": 1763734567796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}