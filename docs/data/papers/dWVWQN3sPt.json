{"id": "dWVWQN3sPt", "number": 25464, "cdate": 1758368321031, "mdate": 1759896720157, "content": {"title": "Beyond Anecdotal Evidence: A Systematic Framework for Evaluating Neuron Interpretability", "abstract": "A central challenge in mechanistic interpretability is how to evaluate whether individual neurons genuinely capture meaningful features. Existing work relies heavily on *activation selectivity*, but this single metric quickly saturates and fails to distinguish among units, leaving many interpretability claims anecdotal. We propose **InterpScore**, a reproducible four-axis framework that integrates **Selectivity**, **Causal impact**, **Robustness**, and **Human consistency** into a compact composite measure. Applied to 10 high-selectivity neurons from CLIP RN50x4's penultimate layer (Radford et al., 2021), InterpScore reveals meaningful variation across neurons, about **14% coefficient of variation** where Selectivity alone shows none, demonstrating that multi-axis evaluation surfaces distinctions overlooked by single metrics. The framework is numerically stable across seeds and its axes capture complementary, independent aspects of neuron behavior. These results move neuron-level claims beyond anecdotes toward a more objective, systematic, and reproducible basis for assessing and comparing interpretability frameworks. Looking ahead, InterpScore offers a reproducible protocol for principled neuron evaluation across diverse vision architectures.", "tldr": "We propose InterpScore, a multi-dimensional framework that moves beyond activation selectivity to systematically evaluate neuron interpretability in vision models.", "keywords": ["mechanistic interpretability", "neuron evaluation", "vision models", "CLIP", "activation selectivity", "interpretability metrics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1033091f0b8420a10fd64238b5f25b57140119fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of evaluating whether individual neuron captures specifc concept. To address this problem, the authors propose InterpScore, which summarizes Selectivity, Causality, Robustness and Human consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes new dimensions to evaluate whether neuron captures specific concept  beyond selectivity, including robustness, causality and human consistency."}, "weaknesses": {"value": "1. The proposed metrics' ability to evaluate neuron-concept match is not well justified: For the causal impact metric, the only part related to concept is using concept image as test inputs. My intuition is this metric is more measuring the **importance of the neuron** to output embedding, instead of the quality of the concept. Similarly, the robustness metric is measuring robustness of neuron. \n2. The empirical evidence is not fully convincing. The evidence 1 suggests InterpScore has better discrimination ability than that of selectivity alone. However, this is expected as more metrics are introduced and does not imply InterpScore is a better metric. Evidence 2 shows stability of InterpScore, which is a desired property but provides no information on whether it's good for evaluation neuron-concept pairs."}, "questions": {"value": "1. Typos: Citation missing in Ln 66. Reference missing on Ln 388. \n2. For causality and robustness evaluation, what happens if random images are used instead of images containing the specific concept? What about using images containing another concept? A full comparison is needed to justify those metrics capture information related to the concepts. \n3. For human consistency, a potential problem with the evaluation protocol is it may favor general/ambiguious concept. For example, if a concept is \"a photo\", it may always be captured in highly activated images but this does not indicate it's a good concept. \n4. A detailed case study will be helpful to justify the proposed InterpScore. For example, a case where selectivity only fails (high selectivity but not good concept) while the proposed InterpScore can identify it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mq7ai0qQDO", "forum": "dWVWQN3sPt", "replyto": "dWVWQN3sPt", "signatures": ["ICLR.cc/2026/Conference/Submission25464/Reviewer_dfbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25464/Reviewer_dfbX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699186874, "cdate": 1761699186874, "tmdate": 1762943441378, "mdate": 1762943441378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for aggregating four dimensions of unit-wise interpretability (selectivity, causal impact, robustness, human consistency) into a scalar interpretability score, which quantifies how interpretable an individual unit of a vision model is. The authors also implement a proof-of-concept for an openAI microscope-style website for network inspection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is fairly well written and clear, apart from some open questions (see below).\n- The paper demonstrates how the openAI microscope could be improved, in principle.\n- Moving beyond just unit sensitivity is a good idea, in principle."}, "weaknesses": {"value": "- As it stands, the paper does not really yield any new insights. The authors propose a way of computing interpretability scores, but don't motivate well why this would actually be useful. What do we do with these scores once we have them?\n- The proposed approach is not easily scalable, because (a) the concepts used to evaluate neurons seem to be hand-picked and require manual labor and (b) the human consistency requires experiments that will be prohibitively expensive at scale. \n- Averaging the four dimensions is a parameter-free, but still somewhat arbitrary choice. The scalar interpretability score could also be any other linear combination of the four dimensions. \n- At the core of any neuron-level \"interpretability score\" should be how well humans can interpret the respective neuron. Operationalizing this notion is admittedly difficult, but the approach proposed in this work is not particularly convincing: 1. There could be fairly interpretable polysemantic neurons which activate for n concepts. In the current approach, the H-score of such units would not exceed 1/n, even if the concepts themselves were very clear. 2. Characterizing a unit by its behavior in the high-activity regime alone seems overly simplistic, since 95% of what the unit contributes to the network is by definition not captured by this operationalization.\n\nThis paper is generally well written, and attempts to move the field of interpretability towards quantifiable evaluations, which is laudable. The way in which this quantification is achieved is a reasonable starting point, and I appreciate the demo of the microscope-style website. But it is currently unclear how this approach could be scaled, and the paper does not sufficiently motivate the assignment of interpretability scores itself. It may be worth asking why the original microscope was discontinued: Even if the approach was scaled, what is the user story? How do we expect to derive new insights from such a microscope? Nothing in the paper is incorrect to the point of being outright wrong, but the contribution is not sufficient, in my opinion."}, "questions": {"value": "- How are the concepts, for which images are then collected, chosen? This selection would have to be automated as well for the approach to scale.\n- Not a question as such, but I suggest to rephrase the following sentence from the abstract, as it is hard to parse: \"InterpScore reveals meaningful variation across neurons, about 14% dispersion where selectivity alone shows none, demonstrating that multi-axis evaluation surfaces distinctions overleaped by single metrics.\"\n- There are missing references in line 65 and 388.\n- In figure 3, the figure caption mentions points but no points are depicted.\n- Very minor point and somewhat subjective, but I find it weird to have a \"main research question\"-block that does not contain a question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NCsYxBgeNY", "forum": "dWVWQN3sPt", "replyto": "dWVWQN3sPt", "signatures": ["ICLR.cc/2026/Conference/Submission25464/Reviewer_5gEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25464/Reviewer_5gEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753992793, "cdate": 1761753992793, "tmdate": 1762943441136, "mdate": 1762943441136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 4-dimensional interpretation score (SCRH) to evaluate the interpretability of single neurons in neural networks.\n\nThe scores are illustrated on 10 neurons from a CLIP layer. \nThe paper demonstrates how going 4D provides more information than just reporting a neuron’s selectivity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper is clearly written, well organized, and conceptually well motivated. The narrative flows logically, and the motivation for studying interpretability is easy to follow.\n\nS2. Each of the four proposed scores is well defined, intuitive, and supported by clear reasoning, making the framework easy to understand and apply.\n\nS3. The running example of the sailing boat, and the well-designed figures, conveys the ideas very well.\n\nS4. The authors reimplemented a previously closed-source tool, thereby contributing to open and reproducible research in interpretability. This is a really big strength."}, "weaknesses": {"value": "W1. The experimental evaluation is limited. The analysis is conducted on only 10 neurons (are they cherry-picked for their high selectivity?), and drawn from a single model (clip). A broader evaluation, across multiple models, and on neurons that vary in selectivity, would strengthen the paper.\n\nW2. The writing could be improved. Several references are missing or incomplete (e.g., “see:(?)” and “Table 1, App. ??”). This undermines the paper’s polish.\n\nW3. The related work section seems insufficient. For instance, the paper does not describe psychophysics-inspired approaches to measuring interpretability (e.g., work by Zimmermann and others).\n\nThe paper does not describe approaches in language models, where the interpretability research is potentially more developed. What quantitative metrics exist there? The authors could discuss automated interpretability research for language models (eg, Bills et al., 2023; Cunningham et al., 2023; Gurnee et al., 2023; Bricken et al., 2023).\n\nW4. Relatedly, the comparison to baselines is limited. Beyond the selectivity metric, it would be useful to discuss or empirically evaluate other established measures of interpretability, clarifying how the proposed scores compare in practice.\n\nW5. Limitations section is missing."}, "questions": {"value": "Q1. What are the concrete insights gained from the proposed 4D scoring framework? While it clearly captures more information than simple selectivity, it remains unclear what new understanding or _actionable_ conclusions this provides about neural representations. In other words: what does this added dimensionality tell us about interpretability?\n\nQ2. How were the ten neurons used in the experiments selected? Were they chosen randomly or based on specific properties such as high selectivity or high variance? Clarifying the selection criteria is essential to assess the robustness and generality of the findings.\n\nQ3. It would be helpful to include comprehensive tables in the related works listing existing interpretability metrics—one for language models and one for vision models. Such a summary would contextualize the contribution within the broader landscape of interpretability research and make it easier to compare approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JbiDH9GhsZ", "forum": "dWVWQN3sPt", "replyto": "dWVWQN3sPt", "signatures": ["ICLR.cc/2026/Conference/Submission25464/Reviewer_Q2he"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25464/Reviewer_Q2he"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867012830, "cdate": 1761867012830, "tmdate": 1762943440897, "mdate": 1762943440897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}