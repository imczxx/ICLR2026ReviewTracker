{"id": "oijKOpfSmX", "number": 3854, "cdate": 1757555002036, "mdate": 1759898066161, "content": {"title": "KeyVID: Keyframe-Aware Video Diffusion for Audio-Synchronized Visual Animation", "abstract": "Generating video from various conditions, such as text, image, and audio, enables precise spatial and temporal control, leading to high-quality generation results. Most existing audio-to-visual animation models rely on uniformly sampled frames from video clips. Such a uniform sampling strategy often fails to capture key audio-visual moments in videos with dramatic motions, causing unsmooth motion transitions and audio-visual misalignment. To address these limitations, we introduce KeyVID, a keyframe-aware audio-to-visual animation framework that adaptively prioritizes the generation of keyframes in audio signals to improve the generation quality. Guided by the input audio signals, KeyVID first localizes and generates the corresponding visual keyframes that contain highly dynamic motions. The remaining frames are then synthesized using a motion interpolation module, effectively reconstructing the full video sequence. This design enables the generation of high frame-rate videos that faithfully align with audio dynamics, while avoiding the cost of directly training with all frames at a high frame rate. Through extensive experiments, we demonstrate that KeyVID significantly improves audio-video synchronization and video quality across multiple datasets, particularly for highly dynamic motions", "tldr": "", "keywords": ["Audio to Video Generation", "Keyframe Generation", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f60940cc49c25d0c1d80cd6ba7e12c871a3505ce.pdf", "supplementary_material": "/attachment/0b933e53d80ad38eb778051c1a6b9b040f984fe8.zip"}, "replies": [{"content": {"summary": {"value": "This paper argues that most existing ASVA (Audio-to-Visual Animation) models adopt the strategy of uniformly sampling video frames, which leads to two core problems in high-dynamic motion scenarios: (1) Failure to capture key audio-visual moments, resulting in unsmooth motion transitions. (2) Audio-visual temporal misalignment, especially for low-frame-rate models, which struggle to match the fine-grained temporal information of audio.\nTherefore, this paper proposes a keyframe-aware audio-to-visual animation framework that first localizes keyframe positions from the input audio and then generates the corresponding video keyframes using a diffusion model, which designs a keyframe generator network that selectively produces sparse keyframes from the input image and audio, effectively capturing crucial motion dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe thinking of uniform frames vs. keyframes generation and the keyframe-oriented pipeline in Figure.1 are interesting and beneficial to the research community.\n2.\tThe design of multi-condition cross attention fusion is delicate.\n3.\tThe quantitative comparison results and demos show the effectiveness of proposed method, which is convincing to me."}, "weaknesses": {"value": "1.\tThe ablation studies are not very convincing since the results of Table.2 are similar. Especially in terms of the “w.o. Frame Index” setting, the FVD improvement is 1.7% and the degradations of synchronization metrics are 2.1% ~ 2.4%. So it is not clear for me to understand the necessaries of Frame Index. \n2.\tThere is no computation efficiency analysis, which is essential for real-world applications. I am wondering that whether it is heavy to conduct the multiple-condition CA in the U-Net blocks. \n3.\tThe paper does not analyze the performance differences of the proposed method across different scenarios. The paper claims that its method is particularly advantageous in \"intensive motion\" scenarios (in Line.485), but this lacks quantitative analysis and verification."}, "questions": {"value": "1.\tDiscuss and explain the effectiveness of proposed technical in this paper, especially the “Frame Index”.\n2.\tCompare the time efficiency of the proposed method with those of baselines. For example, RealTime Factor(RTF) and GFlops should be taken into considerations.\n3.\tAdd more comparisons with baselines on intensive-motion scenarios and non-intensive-motion scenarios, and discuss the differences.\n4.\tWill the code and pretrained model be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BL6bdEdX12", "forum": "oijKOpfSmX", "replyto": "oijKOpfSmX", "signatures": ["ICLR.cc/2026/Conference/Submission3854/Reviewer_vQ68"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3854/Reviewer_vQ68"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588969341, "cdate": 1761588969341, "tmdate": 1762917067648, "mdate": 1762917067648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach for audio-driven image animation, where static images are animated into videos synchronized with input audio both semantically and temporally. The method decomposes the animation process into two stages: the first generates keyframes corresponding to key actions derived from the audio, and the second interpolates between these keyframes to produce continuous motion.  Both stages use a video inbetweening model to generate frames."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I appreciate the idea of generating keyframes or key actions first, which need not be uniformly distributed. This design effectively mitigates the potential mismatch between audio and generated video arising from differences in their sampling frequencies."}, "weaknesses": {"value": "1. I am skeptical about the definition of keyframes as frames with peak motion scores. The authors should discuss the applicability and limitations of this definition. For instance, in dance videos, key movements often occur on musical beats, where the motion velocity is near zero—these moments would not correspond to frames with the highest motion scores.\n2. I would like the authors to provide further justification for this keyframe definition.\n3. Based on the provided video result, the method appears to be applicable primarily to sound events.  Moreover, the paper presents too few video examples to convincingly demonstrate the effectiveness of the proposed approach."}, "questions": {"value": "See the above weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mUJIYK1MXB", "forum": "oijKOpfSmX", "replyto": "oijKOpfSmX", "signatures": ["ICLR.cc/2026/Conference/Submission3854/Reviewer_J6Af"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3854/Reviewer_J6Af"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780216061, "cdate": 1761780216061, "tmdate": 1762917067390, "mdate": 1762917067390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is for adding audio conditon to existing text-image to video (TI2V) model. \n1. The backbone is DynamiCrafter, the dataset is open-source audio-visual generation dataset AVSyncD. The generated videos are around 2s (48 frames). \n2. The target is to solve the audio-visual misalignment. While the idea is first select audio keyframes, then generate keyframes using selected audio, and finally do video interpolation. \n    -   The authors train a audio-to-optical flow network to predict optical flow and select audio keyframes based on local minimum/maximum.\n    -   Use this keyframes audio feature, image and text, and the target generate frame idx to generate video\n    -   Video interplotation is by finetuning the DynamiCrafter with Wan 2.2 style image mask condtioning. \n3.  The objective score beats SoTA and 7 videos results attached."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The evaluation contains both objective and subjective mertic/samples and it shows results better than previous methods.\n3. The authors included the detail of each module in appendix."}, "weaknesses": {"value": "1. The high level idea sounds rule-based and do not have enough evidence why it is better than generating all frames in once. \n    - limition of rule based design: using optical-flow and picking local minimum/maxmum may not suitbale for some smooth audio, e.g., river, plane takes off. the idea maybe not general enough to push to boundary of current ATI2V model. it may require a more general mapping model, for example based on the contrastive learning like text and image.\n    - how to set the threshold of key frames number? for the hammer case, if the speed of hitting is very fast, e.g. 10 times in 2 second, should we have a 20-frame keyframe at least.\n2.  The implemenation, using a video model to generate discontinus frames by a learned frame embeding but keeping the original rope sounds not strightforward. \n    - firstly only using select audio keyframes feature, will this be enough? considering a hammer case only the sounds of hitting is captured.\n    - adding the frame idx condtion to the network, is it possible to directly modify existing position embedding?"}, "questions": {"value": "Overall this is a paper that clear written, and have completive experiments. My concern is the idea itself sounds rule-based and not general. I'm wondering for 2s audio-video generation, this is a length we have enough GPU memory to train directly, maybe end2end modeling could get good results after filteriing out the misaliged audio-visual data from the dataset.  The details of my questions are in weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTPC3awQYS", "forum": "oijKOpfSmX", "replyto": "oijKOpfSmX", "signatures": ["ICLR.cc/2026/Conference/Submission3854/Reviewer_Lpze"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3854/Reviewer_Lpze"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864390492, "cdate": 1761864390492, "tmdate": 1762917067221, "mdate": 1762917067221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KeyVID, a keyframe-aware diffusion framework for generating videos that are temporally synchronized with input audio. The core idea is to exploit the correlation between peaks in the motion signal (optical flow intensity) and peaks in the audio signal to determine key moments of action. The system decomposes the task into three modules: a Keyframe Localizer that predicts motion peaks from audio, a Keyframe Generator that synthesizes visual frames conditioned on audio, text, and the first image, and a Motion Interpolator that fills intermediate frames for smooth transitions. While the underlying assumption “strong sounds correspond to large motions” is conceptually simple, the paper demonstrates that modular design and diffusion-based conditioning yield high-quality, audio-synchronized animations, outperforming prior methods (e.g., AVSyncD) in both quantitative metrics and human preference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strength lies in its clear conceptual simplicity combined with strong engineering design. Instead of introducing a novel generative paradigm, it isolates key factors affecting audio-visual synchronization and builds an effective three-stage system around them. The modular structure (localization–generation–interpolation) makes the overall process interpretable and flexible. The idea of learning motion saliency from audio peaks via optical-flow supervision is intuitive yet elegantly implemented, enabling temporal precision without requiring explicit motion labels. Moreover, the integration of first-frame conditioning and frame index embeddings ensures temporal consistency and visual coherence across non-uniformly sampled keyframes—an aspect that many prior diffusion-based approaches fail to achieve. Experimental results are convincing, showing SOTA performance on both synchronization and visual quality metrics. The paper is also well-written, with clear motivation and comprehensive ablations that help readers understand the contribution of each module. The proposed framework feels robust, scalable, and generalizable beyond its training distribution."}, "weaknesses": {"value": "Despite its strong empirical results, the conceptual novelty is somewhat limited. The paper’s main assumption—that audio peaks align with motion peaks—is simple and well-known in the audio-visual literature. The novelty mainly comes from a careful engineering decomposition rather than a new theoretical insight. The keyframe selection mechanism remains heuristic (based on fixed thresholds and local extrema), which, while effective, feels ad hoc and could limit robustness for more complex or subtle motion types. For instance, the model performs less consistently on “subtle-motion” videos (e.g., violin, trumpet) or single-event sequences (e.g., frog croaking), where perceptual synchronization is harder to judge and the heuristic peak detection may fail. Furthermore, the 2-second clip length used in both training and user studies constrains the evaluation of long-term consistency and overall narrative quality. The model’s dependence on the first frame also raises concerns about appearance drift or overfitting to static conditions when generating longer sequences."}, "questions": {"value": "In addition to the weakness, it would be great if authors can response to the following minor comments.\n- The paper would benefit from more discussion of failure cases, especially where KeyVID underperforms in the user study (e.g., low-motion or single-event clips).\n- Figure 5 and Appendix F could be expanded to show visual differences in subtle-motion scenarios, not just high-intensity ones.\n- The authors might consider exploring learnable or probabilistic keyframe selection instead of the fixed heuristic used in Section 3.1.\n- The limitation of using short 2-second videos for subjective evaluation should be explicitly acknowledged; looping or extended clips could help reduce perceptual bias.\n- It would be interesting to see comparisons against pose-based or structure-aware baselines such as TANGO, to assess generalization to human-centric motion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ITEL8ceibs", "forum": "oijKOpfSmX", "replyto": "oijKOpfSmX", "signatures": ["ICLR.cc/2026/Conference/Submission3854/Reviewer_aTg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3854/Reviewer_aTg2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880028575, "cdate": 1761880028575, "tmdate": 1762917067015, "mdate": 1762917067015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}