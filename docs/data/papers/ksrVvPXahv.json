{"id": "ksrVvPXahv", "number": 5502, "cdate": 1757916045943, "mdate": 1759897970871, "content": {"title": "Temperature-Driven Escape Explains Critical Learning Rates in Adaptive Optimization", "abstract": "The learning rate is a central control parameter in neural network training, even for adaptive optimizers such as Adam, which reduce sensitivity to gradient magnitude differences across parameters. It can either trap models in sharp minima or guide convergence toward flatter regions, yet its selection has largely remained empirical. Here, we introduce a temperature-driven escape framework that grounds learning-rate dynamics in statistical physics. By decomposing Adam’s mini-batch updates into deterministic drift and stochastic fluctuations, we define an effective temperature $T_\\text{eff}$ induced by gradient noise and apply Kramers’ escape theory to derive a closed-form expression for the critical learning rate $\\alpha_c$. Experiments on vision tasks (MNIST, CIFAR-10 with MLP, CNN, ResNet) and language tasks (SST-2 with BERT, GPT-2, TinyLlama) show that the theoretically predicted $\\alpha_c$ achieves better generalization, whereas far deviations from this critical value lead to degraded performance. Beyond initialization, re-estimating $\\alpha_c$ also serves as a diagnostic tool for probing the nature of minima reached by well-trained models. Our results elevate the learning rate from an empirical hyperparameter to a theoretically principled quantity, providing both a predictive rule for initialization and a new perspective on optimizer dynamics.", "tldr": "", "keywords": ["Critical learning rate", "Effective temperature", "Adaptive optimization", "Neural network training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f37849e48a173556a6770664dd0dd893f2847444.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper adopts a thermal escape formalism to analyze optimization of NNs using Adam, and how to select the optimal global learning rate. The main claim is that an optimal learning rate can be obtained by a few cheap measurements at initialization, which probe local curvature and what the authors define as effective temperature. The method is then tested on a bunch of image and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Theorizing about the optimal choice of learning rate is of course of high importance since this is a crucial hyper parameter that significantly affects training. Understanding how to optimally choose it would be both of fundamental theoretic importance and of practical use. I find that the application of Kramers-like escape formalism in order to attack this problem is interesting and it should result in interesting insights. The paper is relatively well written and provides a conceptual picture."}, "weaknesses": {"value": "I find that the theoretical part of the paper quite weak, and that many implicit assumptions, which are required for Kramers' theory to hold, are not mentioned and, what is worse, are violated in practical settings. Some of the argumentation is too loose, and overall I think the main points of the paper an unsupported. In detail:\n\n1.  First, Kramers theory holds for one dimensional dynamics, while neural optimization happens in many dimensions. A generalization of Kramers theory to high dimension was done by Jim Langer in the 60's for the overdamped limit ([Statistical theory of the decay of metastable states](https://doi.org/10.1016/0003-4916(69)90153-5) and many similar theories followed since. The main difference from 1D is that in high dimension there is also an *entropic* contribution that has to do with the number of ways one can escape a given well (that is - the energy in the exponent is *free energy*). Also, the Hessian of the saddle point between the states is important.  \nIn contrast, the authors treat the dynamics as if each weight is undergoing an uncoupled 1D escape, which is not how real networks operate. It might be an approximation (I think it's not a good one), but this should at least be stated.\n\n2. Kramer's theory applies for the escape rate from local minima. However, it is not at all clear that networks are trapped in minima, rather than wonder in shallow basins. Multiple works show that generically training dynamics is not hopping from minimum to minumim, but rather a slow drift down a rugged low-loss manifold, not at all dominated not by barrier crossings: See [Essentially No Barriers in Neural Network Energy Landscape](https://proceedings.mlr.press/v80/draxler18a.html), [Qualitatively characterizing neural network optimization problems](https://arxiv.org/abs/1412.6544) and similar works.  For example, [the hessian may generically have negative eigenvaues during training](https://openreview.net/forum?id=S1iiddyDG), which will make the escape formalism completely inapplicable.\n\n3. The authors assume that if batches are sampled iid, then the gradient noise has diagonal correlation and no memory (Eq. 5) in appendix A. Both these assertions are wrong, as far as I understand. Batch noise is correlated over time steps and has a non trivial covariance structure. This has been heavily studied. For example, see: [1) A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks](https://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf), [2) Shape Matters: Understanding the Implicit Bias of the Noise Covariance](https://proceedings.mlr.press/v134/haochen21a.html) and [3) The Anisotropic Noise in Stochastic Gradient Descent](https://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf) (which is cited in the manuscript).\n\n4. The manuscript describes two scaling relations, for \"sharp\" and \"flat\" minima. The trade off between these two sets the optimal learning rate (Eq. 16). It is not at all clear to me that there are only two types of minima,  and more importantly, the authors do not describe, even hand wavingly,  a quantitative criterion to distinguish the two. \"flat\" or \"sharp\" with respect to what?\n\n5. The central result of the paper, Eq. 17, depends heavily on the choice $f_{neq}\\sim\\sqrt{b}$. However,  $f_{neq}$ is not properly defined in the manuscript, its introduction is purely heuristic, and the choice is unjustified theoretically.\n\n6. The numerical results are not quite convincing. The relative improvement does not seem statistically significant except in, maybe, the MLP on MNIST problem, and no quantitative comparison of the accuracy/loss gain is provided. The minimal convincing evidence would be to compare the improvement in a metric to the standard deviation of that metric across different initializations/stochastic seeds etc.\n\n7. the discussion in Sec. 4.5 is quite loose and inexact, including manifestly wrong statements such as \"with very large batches Adam behaves like deterministic gradient descent\" and that for small batches gradient noise violates the CRT."}, "questions": {"value": "I wrote my questions as weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4njyx4dbV0", "forum": "ksrVvPXahv", "replyto": "ksrVvPXahv", "signatures": ["ICLR.cc/2026/Conference/Submission5502/Reviewer_CxD2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5502/Reviewer_CxD2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580311672, "cdate": 1761580311672, "tmdate": 1762918099114, "mdate": 1762918099114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission analyzes the escape phenomena in Adam optimization, using tools from statistical physics. Specifically, they derive an analytical expression for the critical learning rate above which the Adam escapes the local minima. They test their theory across training setups."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The analysis is novel to the best of my knowledge\n* The problem of Adam instability is relevant to the community"}, "weaknesses": {"value": "**Assumptions without justifications**: The theoretical analysis assumes a lot of simplifications without providing any justification. For instance:\n1. The Taylor expansion (Equation 2) assumes that the gradients are zero\n2. Equation 7 assumes that the stochastic nature only comes from the moment estimate, whereas the variance part is static. \n\n**Reference to sharp / flat minima without justification**: Throughout the paper, the authors refer to flat or sharp minima without showing any measure of loss curvature (sharpness, trace, etc) and many claims (such as line 335) are unsupported. \n\n**Missing Related Works**: The paper does not compare or cite many related works on the escape phenomena of neural networks. \n1. The large learning rate phase of deep learning: the catapult mechanism, 2020\n2. How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective, 2018\n2. Stepping on the Edge: Curvature Aware Learning Rate Tuners, 2024\n4. Adaptive Gradient Methods at the Edge of Stability\n5. Adaptive Preconditioners Trigger Loss Spikes in Adam, 2025\n6. Why Warmup the Learning Rate? Underlying Mechanisms and Improvements, 2025\n\n**Usage of inaccessible terminology**: The paper uses terms like 'mesoscopic regimes' and 'sharp/flat' minima without defining or justifying them. More generally, I think the writing of the paper can be improved.\n\n**Batch size dependence section is quite vague**: Section 4.5 provides a qualitative picture of the results rather than providing any concrete quantitative experiments to support the definition of their phases."}, "questions": {"value": "* How does the critical learning rate estimate compare to the pre-conditioned Hessian threshold $\\eta_c = (2+2\\beta_1) / ( (1 - \\beta_1) \\lambda_{\\max}(P^{-1}H) )$ studied in prior works[1, 2, 3]?\n* At initialization, the minima assumption (gradient being zero) is clearly violated. What is training escaping early in training?\n* I am unsure why the learning rate depends on the square root of the maximum eigenvalue. In traditional analysis of convex optimization, the critical learning rate is inversely proportional to the maximum eigenvalue of the Preconditioned Hessian [1].\n* How does the critical learning rate prediction changes on using standard schedules consisting of learning rate warmup and decay?\n* In most experiments, learning rates are sampled one order magnitude apart (Figure 2b, d), which is quite large. Can you sweep the learning rate finely and compare if critical learning rate is still a good estimate? For instance, in Figure 2 ResNet expriments, the it would be helpful to add the learning rate = 1e-04 curve for reference.\n* Line 335: The fluctuations referred by the authors is a property of Adam near a minima (which can be observed in convex landscapes with fix curvatures as well) and are unrelated to transitioning between large or small minima. To support their claim, the authors should provide how the curvature is evolving during training.\n* What does 'periodically rest to its initial predicted value' mean in line 381? Are the authors measuring critical learning rate every 1000 steps and setting the learning rate to it?\n* What are the takeaways from Section 4.4? Sharp minima are more stable under reset and flat minima are not? Thats counterintuitive. Also, please provide curvature measurements to justify flat or sharp minima.\n\n\n\nReferences:\n\n[1] Adaptive Gradient Methods at the Edge of Stability, 2022\n\n[2] Why Warmup the Learning Rate? Underlying Mechanisms and Improvements, 2024\n\n[3] Adaptive Preconditioners Trigger Loss Spikes in Adam, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bo6IcgHbeZ", "forum": "ksrVvPXahv", "replyto": "ksrVvPXahv", "signatures": ["ICLR.cc/2026/Conference/Submission5502/Reviewer_wV4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5502/Reviewer_wV4E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661432811, "cdate": 1761661432811, "tmdate": 1762918098832, "mdate": 1762918098832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a statistical physics inspired theoretical framework for selecting the optimal learning rate for the Adam optimizer. By viewing the mini-batch gradient noise as an effective temperature, the authors apply the Kramers’ escape theory to analyze Adam’s behavior in different regions of the loss landscape (flat or sharp). The main contribution of this paper is the proposed critical learning rate ($\\alpha_c$), which balances the probability of escaping in sharp low-generalizing minima and the probability of converging to flatter high-generalization basins. The authors show that this critical learning rate can be estimated from the gradient noise, batch size, and the dominant Hessian eigenvalue. Based on this estimation method, they provide extensive empirical results over a wide range of different network architectures, covering both vision tasks (MNIST, CIFAR-10 with (MLP, CNN, ResNet) and language tasks (SST-2 with BERT, GPT-2, TinyLlama). The experimental results show that using the estimated critical learning rate can consistently yield better generalizability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This paper provides a novel theoretical framework to connect statistical physics to the optimization dynamics of Adam in neural networks. Viewing noise as an effective temperature is a well-established method in statistical physics, which further supports the robustness of the proposed theory.\n- The core strength of this theoretical framework is that the proposed theory yields a predictive closed-from equation for the best initial learning rate, which can be validated empirically via different experiments.\n- The empirical experiments presented in this paper are strong, covering a wide range of different network architectures and different tasks. Networks using the predicted best learning rate can consistently outperform their counterparts trained with other learning rates."}, "weaknesses": {"value": "- The proposed theory is primarily dependent on the concept of flatness. The entire theory is built on the assumption that flat minima should present better generalization compared with those sharp ones, where the flatness is approximated by the dominant Hessian eigenvalue. However, as shown in many previous studies, the flatness scales with network weights and needs to be normalized (like by weight norm) to properly correlate with generalizability. It seems that this factor is not considered here.  \n- The choice of non-equilibrium enhancement factor seems to be arbitrary. In addition, the proposed theory assumes that the noise sampled from small batches is normally distributed, which is likely to be violated in practice especially when using small batches, as the authors also acknowledged.\n- The diagnostic application of $\\alpha_c$ in section 4.4 is unclear and a more detailed presentation is needed. For example, how should we interpret the results of resetting learning rate and why this can make re-estimating $\\alpha_c$ a 'diagnostic tool' for the metastate?\n\nMinor points:\n- The statement that SGD ‘treats flat and sharp minima the same’ could be misleading, since we know SGD has an implicit bias towards flat minima. I think what the authors mean is that the sqrt($v_t$) term in Adam, making it update differently when residing in sharp or flat minima. I would suggest the author rephrase there to avoid potential confusion.\n- It might be worthwhile to discuss the relationship with: https://arxiv.org/abs/2505.11411, which also interprets the neural networks generalizability through the lens of physical dynamics."}, "questions": {"value": "1)  While Adam is known for its insensitivity to the learning rate, this paper argues that there is a single critical learning rate which can yield the best generalizability. How should we reconcile these two ideas? Is the claim more likely to be Adam is robust for convergence but sensitive (to learning rate) for the best generalizability (as shown in Figures 2 and 3)? \n2) As a follow-up to the first point of weakness, how is the proposed theory affected by the weight scaling? For example, If one rescales the weights of two adjacent layers (assume using ReLU) by a factor of c and 1/c, the output should remain the same but the calculated flatness would change. Does this imply that the critical learning rate should also be also different?\n3) During the early stage of training, the model is very likely not yet to be in a loss minima. In this case, how should the theory of \"escaping a minimum\" be interpreted, and why is the $\\alpha_c$ calculated from this initial state predictive for the entire training trajectory?\n4) In Figure 2, it seems that the critical learning rate can sometimes induce larger fluctuations in training. Can the authors provide some insight about why it occurs?\n5) In Figure 3, maximum test accuracy is used as a metric. Is this a good metric given that fluctuations when using the critical learning rate could be large in some cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATLlJPI6As", "forum": "ksrVvPXahv", "replyto": "ksrVvPXahv", "signatures": ["ICLR.cc/2026/Conference/Submission5502/Reviewer_ZKz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5502/Reviewer_ZKz7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755548147, "cdate": 1761755548147, "tmdate": 1762918098156, "mdate": 1762918098156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper derives a critical learning rate, $\\alpha_c$​, for the Adam optimizer. The idea: view Adam as a stochastic process with drift and noise, define a temperature, and find the rate at which the optimizer escapes local minima most efficiently. This $\\alpha_c$ depends on the batch size, gradient noise, and curvature of the loss. The authors show that, across several models and tasks, training and generalization are best near $\\alpha_c$. The paper claims this makes Adam’s learning rate predictable rather than guessed. I did not follow all of the math. The theory sits outside my background. Still, the motivation and the clarity of the argument were easy to appreciate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) A clear and explicit derivation grounded in statistical physics.\n2) A practical formula that can remove trial-and-error from learning-rate tuning.\n3) Empirical results match the theory: performance peaks near the predicted $\\alpha_c$​.\n4) Transparent discussion of assumptions and limits.\n5) Addresses an important need. Adam is widely used, but hyperparameters remain guesswork."}, "weaknesses": {"value": "1) Experiments are small. Most are toy models or moderate-scale fine-tuning.\n2) The study compares only Adam and its variants, not other optimizers such as Muon.\n3) Estimating the Hessian’s largest eigenvalue may be infeasible for large models.\n4) The theory assumes Gaussian noise and moderate batch size.\n5) Sensitivity of the method to rough curvature estimates is not tested."}, "questions": {"value": "1) What is the computational cost of estimating $\\sigma$ and $\\lambda_{max}$ for a billion-parameter model?\n2) How sensitive is performance to a 25–50 % error in these estimates?\n3) How does the theory interact with standard learning-rate schedules such as warm-up and cosine decay?\n4) Have you tried this on other adaptive optimizers (e.g., Muon, Adafactor)?\n5) When does the model of Gaussian noise fail?\n6) Can you offer a cheaper approximation for curvature estimation and show how close it stays to the optimal αc\\alpha_cαc​?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JTtYoBbqCk", "forum": "ksrVvPXahv", "replyto": "ksrVvPXahv", "signatures": ["ICLR.cc/2026/Conference/Submission5502/Reviewer_4uBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5502/Reviewer_4uBz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766680276, "cdate": 1761766680276, "tmdate": 1762918096364, "mdate": 1762918096364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Promising statistical-physics view of Adam that yields a closed-form learning-rate rule via an escape-from-sharp-basins argument, but hinges on an ad hoc non-equilibrium scaling, lacks a direct validation of the Kramers picture, and is only shown on relatively easy setups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear and appealing narrative (Adam -> effective temperatures -> escape rates)\n\nExplicit, easy-to-compute LR formula tied to measurable quantities.\n\nEmpirical results broadly consistent with the predicted LR.\n\nBridges SDE/flatness literature with adaptive optimizers."}, "weaknesses": {"value": "The overall idea and concept are very nice, however I do not find that they are supported by solid evidence at the moment, and that it would be very important for this paper to show a gain in practice (i.e., a true advantage in usability over other learning rate schedules taken as competitors). In particular:\n\n1. The current choice of the (\\sqrt{b}) scaling looks selected for convenience. You should either derive it more rigorously or show that nearby choices ((b^{1/4}, b^{2/3}, 1)) lead to similar LR predictions.\n\n2. You need a controlled toy setup (e.g. 2-well landscape) showing that Adam’s escape times follow the proposed effective-temperature law in the Kramers picture. Otherwise the physics argument remains just a nice speculation.\n\n3. Add at least one modern setting (e.g. a realistic LLM fine-tune) and show the predicted LR matches or reduces tuning compared to a strong competing baseline. (otherwise, it is just a method among many other that work)\n\n4. You claim 20 grads + 10 power iters is cheap and reliable even on bigger models. That’s not obvious. You should show sensitivity. Report sensitivity to errors in (\\lambda_{\\max}) and gradient-noise estimates (e.g. +-2x). This would make the method look usable in practice."}, "questions": {"value": "Please address the 4 weaknesses convincingly and I will consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sNUdGmZNoH", "forum": "ksrVvPXahv", "replyto": "ksrVvPXahv", "signatures": ["ICLR.cc/2026/Conference/Submission5502/Reviewer_XMBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5502/Reviewer_XMBg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998529226, "cdate": 1761998529226, "tmdate": 1762918095844, "mdate": 1762918095844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}