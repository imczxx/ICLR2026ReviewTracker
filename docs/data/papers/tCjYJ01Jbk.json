{"id": "tCjYJ01Jbk", "number": 19332, "cdate": 1758295424478, "mdate": 1759897045026, "content": {"title": "Safe and Efficient In-Context Learning via Risk Control", "abstract": "Large language models (LLMs) demonstrate a remarkable ability to learn new tasks from a few in-context examples.  However, this flexibility introduces safety concerns: LLMs can be influenced by incorrect or malicious demonstrations -- for example, if an adversary tampers with or injects harmful examples without a human supervisor noticing. This motivates principled designs in which the system itself includes built-in mechanisms to guard against such attacks. We propose a novel approach to limit the degree to which harmful demonstrations can degrade model performance. First, we define a baseline ``safe'' behavior for the model -- the model's performance given no in-context demonstrations (zero-shot). Next, we apply distribution-free risk control (DFRC) to control the extent to which in-context samples can decay performance below zero-shot. We achieve this by leveraging dynamic early exit prediction, ignoring later attention heads that attend the most to the unsafe inputs. Finally, we propose modifications to DFRC that allow it to both control risk for harmful inputs \\textit{and} leverage performance and efficiency gains on helpful inputs.  We present both theoretical and empirical results showing that our approach can effectively control risk for harmful in-context demonstrations while simultaneously achieving substantial computational efficiency gains with helpful demonstrations.", "tldr": "", "keywords": ["LLM", "in-context learning", "AI safety", "risk control", "efficiency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5219989b1a8157f2ca868eb1424ac77ee1843da7.pdf", "supplementary_material": "/attachment/f4a64ec21396ea13450d08b9065bd38a9cac6df4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a risk-controlled early-exit framework for making in-context learning (ICL) safer and more efficient. The core idea is to use the model's zero-shot performance as a safe baseline and employ Distribution-Free Risk Control (DFRC) to prevent the model's performance from degrading below this baseline when exposed to potentially harmful in-context demonstrations. A key technical contribution is a risk transformation method to handle the non-standard, often negative-valued ICL loss within the Learn-then-Test framework. Experiments across multiple models and datasets aim to show that the approach controls risk as theoretically guaranteed, provides computational efficiency gains, and mitigates the impact of incorrect demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Problem Formulation: Framing ICL safety as a risk control problem relative to a zero-shot baseline is a clear and intuitive formulation.\n\n- Technical Adaptation: The proposed risk transformation method to handle the full range of the ICL loss within LTT is a solid technical contribution that could be useful for other applications.\n\n- Comprehensive Evaluation: The experimental evaluation is thorough, covering multiple models, datasets, and proportions of correct/incorrect demonstrations, which robustly demonstrates the average-case behavior of the framework."}, "weaknesses": {"value": "- Lack of Conditional Safety Guarantees: This is the most significant weakness. A safety mechanism that cannot provably protect against the specific harmful inputs it is designed to detect has limited practical utility. The paper explicitly acknowledges this shortcoming.\n\n- Weakened Utility: The primary safety mechanism often involves reverting to zero-shot performance. In many applications, this negates the purpose of using ICL, which is to adapt and improve performance with context. The trade-off is shown, but the paper does not convincingly argue that its method provides a superior trade-off compared to simpler alternatives (e.g., using a separate classifier to detect harmful demonstrations).\n\n- Assumptions on Adversarial Scenarios: The method is evaluated on randomly permuted labels as \"incorrect demonstrations,\" which may not reflect the sophistication of real-world adversarial attacks or persistent user errors."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M4Gg54rdXs", "forum": "tCjYJ01Jbk", "replyto": "tCjYJ01Jbk", "signatures": ["ICLR.cc/2026/Conference/Submission19332/Reviewer_FF3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19332/Reviewer_FF3D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901651330, "cdate": 1761901651330, "tmdate": 1762931275079, "mdate": 1762931275079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of LLM-based classification given in-context demonstrations, in a setting where the demonstrations might be helpful but could also be incorrect/adversarial.\n\nThe proposed method computes confidence scores for early-exit classifications after each layer and makes a classification as soon as a certain confidence threshold is reached. If by the final layer, the confidence threshold hasn't been reached, the method falls back to a baseline of classifying without using any of the in-context examples.\n\nThe paper shows that with an appropriate confidence threshold, this method can often benefit from correct demonstrations (i.e. outperform the zero-shot baseline) without being harmed much by incorrect demonstrations, on tasks like sentiment classification or hate speech detection."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- It's an interesting approach to the problem of overthinking on incorrect examples, which is still able to benefit from correct examples\n- The paper is very well written and describes experiments etc. clearly and in enough detail\n- There are a good amount of ablations, and more broadly the experiment design seems very thoughtful\n- The method seems to work well in many of the studied settings\n\nOverall, I think this is a nice idea that's executed well."}, "weaknesses": {"value": "- The paper discusses adversarial demonstrations as one important motivation in the abstract/introduction. But I think the methods and results don't imply much adversarial robustness and seem more centrally about examples that are simply incorrect (e.g. the experiments simply flip the labels of examples). For many adversarial threat models, an attacker would also be able to modify the text of the examples themselves (and/or could in principle try to modify labels more adversarially than making all of them incorrect: maybe a mix of correct and incorrect labels could lead to inflated confidence scores and still give incorrect early-exit answers?) I think the paper would be more accurately presented if it didn't focus its motivation on the adversarial case (or at least made it very clear that it does not provide assurances against actual adversaries if I understand correctly). (This is the only reason I rated Soundness as 3 instead of 4.)\n- It's unclear whether the method could be generalized well to the kinds of broader tasks that the introduction mentions (such as a coding agent). If it only works for classification using few-shot prompted LLMs, where the few-shot examples can't be trusted, that is a rather narrow application.\n- In cases where the model developer/deployer is providing the few-shot demonstrations, it might be better to verify the demonstrations and ensure they are correct rather than implement this method. So I think the main potential of the method comes from cases where few-shot examples are provided dynamically, e.g. if a model is used for many different user-defined classification tasks. But then fig. 3 makes me worried because it suggests that the right lambda will depend a lot on the specific classification task, which creates more effort for each new classification task (and again raises the question whether trying to find correct examples might be better). Similar to the previous bullet, this doesn't invalidate the method, but I think it meaningfully restricts its applicability.\n\nI'm open to updating my score if these issues are addressed (whether through changes to the presentation, additional experiments, or convincing examples showing that I'm wrong about the limited applicability.)"}, "questions": {"value": "1. In fig. 3, what exactly does \"accuracy relative to zero-shot\" mean? Would 0.2 mean that the accuracy is 20 percentage points higher than the zero-shot baseline? Then it seems even incorrect demonstrations outperform the zero-shot baseline, since the y-axis doesn't have negative numbers. Or is there some re-calibration to make the zero-shot baseline have 50% accuracy? But then on AG News, even correct demonstrations would be worse than the baseline.\n2. Re applicability of the method: do you have specific practical applications in mind where you think the method in its current form would actually be the best choice? Alternatively, what kinds of future extensions do you think could be feasible that would have broader applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FeWg405hDT", "forum": "tCjYJ01Jbk", "replyto": "tCjYJ01Jbk", "signatures": ["ICLR.cc/2026/Conference/Submission19332/Reviewer_XUPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19332/Reviewer_XUPs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949170133, "cdate": 1761949170133, "tmdate": 1762931274745, "mdate": 1762931274745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the potential for overthinking, as well as the safety and efficiency issues raised by LLMs in ICL. The authors propose a safe and efficient early-exit ICL framework by defining the ICL risk with respect to a zero-sample baseline and utilizing a distribution-independent hierarchical risk control method to dynamically decide when to early-exit in the reasoning process, thereby avoiding unnecessary computation. Experiments demonstrate the effectiveness of the method on eight classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sufficient theoretical support. The paper provides rigorous theoretical proofs in the appendix to formalize the effectiveness of risk transformation strategies in controlling ICL risk.\n2. Novel framework. The authors introduce a risk control framework to the LLM security problem, where overthinking is weighed against performance loss through dynamic threshold selection."}, "weaknesses": {"value": "1. Lack of clarity on the types of security problems. Although the paper claims to control “security risks,” it is not clear what types of security problems (e.g., demonstration poisoning, jailbreak attacks, fake content generation, etc.) are targeted. Different types of security risks correspond to different attacks and defense methods, and the authors should clarify the scope of application and attack assumptions. In addition, several of the above attacks have explicit methods, and authors should discuss these methods.\n2. Insufficient articulation of methodological innovations with existing frameworks. The authors extend based on existing risk control theories but do not clearly articulate what new mechanisms or assumptions are added to this foundation and how these extensions specifically address the security risks of LLMs.\n3. lack of comparison with existing defense approaches. The related work section does not explain how existing security mitigation techniques (e.g., adversarial training, risk weighting, jailbreak detection) compare to the risk control framework, and the experiments do not provide comparative validation, so the current results are insufficient to fully demonstrate the effectiveness of the approach.\n4. Efficiency advantages are not quantified. Although the title emphasizes EFFICIENCY, the text does not provide algorithm complexity analysis, reasoning latency or computational overhead comparison, resulting in EFFICIENCY lackingempirical support.\n5. Single task type. The experiments are all categorization tasks (e.g., SST2, AG News, TweetEval, etc.) and do not cover generative/open-ended tasks, and the generality of the method for LLM remains to be verified."}, "questions": {"value": "Please examine the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u0Z3Vcytw8", "forum": "tCjYJ01Jbk", "replyto": "tCjYJ01Jbk", "signatures": ["ICLR.cc/2026/Conference/Submission19332/Reviewer_hBNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19332/Reviewer_hBNu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986084379, "cdate": 1761986084379, "tmdate": 1762931274414, "mdate": 1762931274414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a risk-controlled early-exit framework for safe and efficient in-context learning in large language models. The authors leverage zero-shot predictions as a safety baseline and apply distribution-free risk control (DFRC) to ensure that in-context examples do not degrade performance below this baseline. To mitigate overthinking caused by harmful demonstrations, the method employs early-exit mechanisms based on confidence thresholds. A novel ICL-specific loss is introduced to quantify the extent of overthinking, and the Learn-then-Test (LTT) framework is adapted to select thresholds under non-monotonic and potentially negative-valued losses. Experimental results across eight classification tasks and four models demonstrate that the approach effectively controls predictive risk while achieving significant computational speedups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a clear and reasonable approach to simultaneously address safety and efficiency in in-context learning, with writing that is easy to follow.\n\n2. The method is theoretically grounded through the use of a risk control framework, and the experimental results demonstrate its effectiveness."}, "weaknesses": {"value": "1. The motivation for jointly addressing both safety and efficiency is unclear; it is not well justified why these two objectives must be addressed together rather than separately.\n\n2. The method and analysis primarily adapts existing techniques, such as early exiting and risk control, which makes the overall contribution appear somewhat incremental.\n\n3. Some of the mathematical formulations (e.g., Equations (1) and (2)) are unnecessarily verbose and could be streamlined for clarity and conciseness."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QY6scZ8hxK", "forum": "tCjYJ01Jbk", "replyto": "tCjYJ01Jbk", "signatures": ["ICLR.cc/2026/Conference/Submission19332/Reviewer_dbng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19332/Reviewer_dbng"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762358803142, "cdate": 1762358803142, "tmdate": 1762931273697, "mdate": 1762931273697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}