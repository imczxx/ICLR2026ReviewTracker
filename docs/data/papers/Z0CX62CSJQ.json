{"id": "Z0CX62CSJQ", "number": 11767, "cdate": 1758203630791, "mdate": 1763445074411, "content": {"title": "Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "abstract": "Efficiently editing knowledge stored in Large Language Models (LLMs) enables model updates without large-scale training. One promising solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of factual knowledge. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits.  In this paper, we model existing linear L\\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module. With simple modification over L\\&E methods, our framework not only significantly extends the capacity of knowledge editing but also eliminates the associated side effects. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFact datasets, including GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB excels in all metrics of editing success while maintaining original performance evaluated by six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50}$\\mathbf{\\times}$ more than in prior work).", "tldr": "From the perspective of neural KV databases, we extend the capacity of current knowledge editing for LLMs. The code is provided in Supplementary Material for reproduction.", "keywords": ["knowledge editing", "locate and editing", "life-long learning", "large language model", "Transformers"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bee65b7a3fefe2c134a542a281f6cc802ae72568.pdf", "supplementary_material": "/attachment/7a8484a43b2ce6109ae8f7b01b327fee5fdcfda4.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes existing Locate-and-Edit methods and finds that they return the residual vector corresponding to the edited fact, while returning a zero vector for unrelated questions. Based on this observation, the paper proposes **NeuralDB**, which explicitly stores keys and residuals as a KV database. During testing, the most relevant residuals are matched through a **non-linear gated function** and then injected into the hidden state stream."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Viewing the key and residual flows as a KV database is a novel perspective.\n\n* NeuralDB is simple yet highly effective, capable of scaling to a large number of edits while preserving the model’s general capabilities.\n* The paper validates the effectiveness of NeuralDB through extensive experiments across multiple models."}, "weaknesses": {"value": "* The paper lacks a comparison with **MEMOIR [1]**, which identifies relevant edits by comparing the sparse activation patterns of new queries with those stored during editing. Both MEMOIR and NeuralDB share a similar **store–identify–inject** paradigm.\n* Although NeuralDB demonstrates outstanding performance on the **CounterFact** and **ZsRE** datasets, its effectiveness on **multi-hop reasoning editing tasks** remains unknown, such as **RippleEdit [2]** and **MQuAKE [3]**. If NeuralDB also performs well on these datasets, it would further highlight its applicability.\n* NeuralDB requires an additional **KV database**, which increases deployment complexity.\n\n[1] MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs\n\n[2] Evaluating the Ripple Effects of Knowledge Editing in Language Models\n\n[3] MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"}, "questions": {"value": "The L&E method requires incorporating the position of the subject token when computing $k$ and $v$. I’m curious whether the $k$ used here is computed in the same way as in L&E. If it is, how is the position of the subject token obtained during testing? If it is not, what exactly is the computation method for $ k $ in this case? Moreover, why is it that even with a different computation method, key matching can still be achieved (i.e., if the last token’s hidden state is used as the key, it should differ from the key at the subject token’s position, and intuitively, the matching success rate should be very low)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GLyEeFToV2", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_v7LD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_v7LD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030760631, "cdate": 1761030760631, "tmdate": 1762922792184, "mdate": 1762922792184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes existing Locate-and-Edit methods and finds that they return the residual vector corresponding to the edited fact, while returning a zero vector for unrelated questions. Based on this observation, the paper proposes **NeuralDB**, which explicitly stores keys and residuals as a KV database. During testing, the most relevant residuals are matched through a **non-linear gated function** and then injected into the hidden state stream."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Viewing the key and residual flows as a KV database is a novel perspective.\n\n* NeuralDB is simple yet highly effective, capable of scaling to a large number of edits while preserving the model’s general capabilities.\n* The paper validates the effectiveness of NeuralDB through extensive experiments across multiple models."}, "weaknesses": {"value": "* The paper lacks a comparison with **MEMOIR [1]**, which identifies relevant edits by comparing the sparse activation patterns of new queries with those stored during editing. Both MEMOIR and NeuralDB share a similar **store–identify–inject** paradigm.\n* Although NeuralDB demonstrates outstanding performance on the **CounterFact** and **ZsRE** datasets, its effectiveness on **multi-hop reasoning editing tasks** remains unknown, such as **RippleEdit [2]** and **MQuAKE [3]**. If NeuralDB also performs well on these datasets, it would further highlight its applicability.\n* NeuralDB requires an additional **KV database**, which increases deployment complexity.\n\n[1] MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs\n\n[2] Evaluating the Ripple Effects of Knowledge Editing in Language Models\n\n[3] MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"}, "questions": {"value": "The L&E method requires incorporating the position of the subject token when computing $k$ and $v$. I’m curious whether the $k$ used here is computed in the same way as in L&E. If it is, how is the position of the subject token obtained during testing? If it is not, what exactly is the computation method for $ k $ in this case? Moreover, why is it that even with a different computation method, key matching can still be achieved (i.e., if the last token’s hidden state is used as the key, it should differ from the key at the subject token’s position, and intuitively, the matching success rate should be very low)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GLyEeFToV2", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_v7LD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_v7LD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761030760631, "cdate": 1761030760631, "tmdate": 1763620394750, "mdate": 1763620394750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents NeuralDB, a scalable knowledge editing framework that can efficiently and robustly integrate up to 100,000 factual edits. NeuralDB reconceptualizes the Locate and Edit (L&E) paradigm as a process of querying a neural key-value (KV) database and introduces a nonlinear gated retrieval module. The proposed approach aims to mitigate catastrophic forgetting of general capabilities and maintain LLM consistency even after large-scale edits. Experiments conducted on three LLM architectures including GPT-2 XL, GPT-J (6B), and Llama-3 Instruct (8B), as well as benchmarks such as ZsRE and CounterFact, demonstrate that as the number of edited facts increases from thousands to 100,000, NeuralDB maintains high editing success rates while preserving performance on general language understanding tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. NeuralDB successfully scales knowledge editing to 100,000 facts, which is an order of magnitude higher than previous methods such as AlphaEdit. As the number of edits increases, NeuralDB maintains both average editing effectiveness and general capability, forming a sharp contrast with the performance degradation observed in baseline methods.\n2. The paper provides a coherent reinterpretation of previous Locate and Edit (L&E) approaches as linear key-value (KV) databases, and supports this perspective with mathematical derivations.\n3. The NeuralDB approach is simple yet effective, addressing the inherent linearity limitations of existing techniques and enabling robust scalability."}, "weaknesses": {"value": "1. The details before Section 4 are primarily about other work and could be condensed.  \n2. In the methodology part of Section 4, the approach essentially constructs a plug-and-play knowledge base and uses a gating mechanism to determine when to perform edits. What is the key difference between this idea and \"Improving Sequential Model Editing with Fact Retrieval\"?  \n3. In the experimental section, the overall results appear impressive. However, if 10,000 data entries are updated in a plug-and-play manner, the updates primarily rely on the advantages of MEMIT or AlphaEdit. It is recommended to validate the approach on more complex datasets, such as MQuAke, which involves multi-hop editing tasks.  \n4. The paper mentions T-Patch, but there is no experimental comparison with it.  \n5. The most critical issue is whether this method continuously modifies the model parameters. For instance, after updating the Nth data entry, do the updates to the 1st to (N-1)th data entries remain effective?"}, "questions": {"value": "The comparison in Table 1 is limited to methods that continuously update parameters, which creates an unfair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VN3x5uc59v", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_i8Ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_i8Ez"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817965396, "cdate": 1761817965396, "tmdate": 1762922791795, "mdate": 1762922791795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents NeuralDB, a scalable knowledge editing framework that can efficiently and robustly integrate up to 100,000 factual edits. NeuralDB reconceptualizes the Locate and Edit (L&E) paradigm as a process of querying a neural key-value (KV) database and introduces a nonlinear gated retrieval module. The proposed approach aims to mitigate catastrophic forgetting of general capabilities and maintain LLM consistency even after large-scale edits. Experiments conducted on three LLM architectures including GPT-2 XL, GPT-J (6B), and Llama-3 Instruct (8B), as well as benchmarks such as ZsRE and CounterFact, demonstrate that as the number of edited facts increases from thousands to 100,000, NeuralDB maintains high editing success rates while preserving performance on general language understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. NeuralDB successfully scales knowledge editing to 100,000 facts, which is an order of magnitude higher than previous methods such as AlphaEdit. As the number of edits increases, NeuralDB maintains both average editing effectiveness and general capability, forming a sharp contrast with the performance degradation observed in baseline methods.\n2. The paper provides a coherent reinterpretation of previous Locate and Edit (L&E) approaches as linear key-value (KV) databases, and supports this perspective with mathematical derivations.\n3. The NeuralDB approach is simple yet effective, addressing the inherent linearity limitations of existing techniques and enabling robust scalability."}, "weaknesses": {"value": "1. The details before Section 4 are primarily about other work and could be condensed.  \n2. In the methodology part of Section 4, the approach essentially constructs a plug-and-play knowledge base and uses a gating mechanism to determine when to perform edits. What is the key difference between this idea and \"Improving Sequential Model Editing with Fact Retrieval\"?  \n3. In the experimental section, the overall results appear impressive. However, if 10,000 data entries are updated in a plug-and-play manner, the updates primarily rely on the advantages of MEMIT or AlphaEdit. It is recommended to validate the approach on more complex datasets, such as MQuAke, which involves multi-hop editing tasks.  \n4. The paper mentions T-Patch, but there is no experimental comparison with it.  \n5. The most critical issue is whether this method continuously modifies the model parameters. For instance, after updating the Nth data entry, do the updates to the 1st to (N-1)th data entries remain effective?"}, "questions": {"value": "The comparison in Table 1 is limited to methods that continuously update parameters, which creates an unfair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VN3x5uc59v", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_i8Ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_i8Ez"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817965396, "cdate": 1761817965396, "tmdate": 1763533701426, "mdate": 1763533701426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Neural KV Database (NeuralDB) framework to significantly scale up knowledge editing in Large Language Models (LLMs). The authors re-frame existing Locate-and-Edit (L&E) methods as querying a Key-Value (KV) database, which allows for simultaneous modifications of a massive number of factual knowledge edits. The primary motivation is to overcome the limitations of current L&E methods, which often lead to compromised general abilities and forgetting of previously edited facts when scaling beyond thousands of edits. The proposed NeuralDB claims to scale knowledge editing up to 100,000 facts while maintaining high efficacy and model integrity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper successfully tackles a critical bottleneck in knowledge editing: scalability. Claiming and demonstrating results up to 10,000 edits (with the promise of 100,000 in the full paper) represents a significant leap forward in the field, far surpassing the typical limits of previous L&E techniques\n\n2. The method directly confronts the major issue of maintaining LLM General Abilities while performing massive knowledge updates, which is vital for practical deployment."}, "weaknesses": {"value": "1. Although the title claims scalability to 100,000 facts, the quantitative experiments only cover up to 10,000 edits, leaving the paper’s core contribution without direct empirical support.\n\n2. Can edited knowledge support reasoning? I suggest adding a Portability metric.\n\n3. How does the framework ensure that the embeddings for 10,000 or 100,000 distinct keys remain sufficiently orthogonal during the retrieval and editing process to prevent mutual interference? If two factual contexts (Keys) are semantically similar, the retrieval mechanism risks activating and modifying non-target KV pairs, leading to subtle, \"soft\" side effects beyond the targeted edit. The current paper lacks a thorough analysis of this embedding space's decoupling capacity at scale."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FcYIlAk8Gp", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_4zaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_4zaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912189158, "cdate": 1761912189158, "tmdate": 1762922791386, "mdate": 1762922791386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a key limitation of existing Locate-and-Edit (L&E) knowledge editing (KE) methods: their failure to scale to thousands of facts without causing catastrophic forgetting and degrading the model's general abilities. NeuralDB2 scales knowledge editing to 100 k facts by replacing implicit linear L&E updates with an explicit gated neural KV memory, curbing catastrophic forgetting and preserving model competence. Finally the paper demonstrates impressive experimental results, showing NeuralDB scales to 10,000 and even 100,000 facts while maintaining high editing success and preserving general model performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Fresh Perspective with Solid Grounding**: The paper cleverly rethinks existing linear editing methods like MEMIT and AlphaEdit as lookups in a hidden key-value store—backed up by both theory and experiments that show the updates behave almost like one-hot vectors.\n\n2. **Smart, Simple Fix**: Once they spot that sparsity, they swap out the cramped linear system for an explicit neural KV database. A lightweight, cosine-based gate fetches the right “patch” or returns zero if nothing matches—clean and intuitive.\n\n3. **Scales Like a Champ**: NeuralDB keeps its accuracy even after 10 k edits and still protects the model’s general skills, something earlier SOTA methods couldn’t do; they even push the demo to 100 k facts without breaking a sweat."}, "weaknesses": {"value": "**1. On the Sensitivity and Selection of the Gating Hyperparameter $\\gamma$**\n\nMy primary concern is the selection of the gating threshold $\\gamma$, which is the single most critical hyperparameter for preserving general abilities. In the main experiments, the authors use a single value ($\\gamma=0.65$) across three different models (GPT2-XL, GPT-J, and Llama-3). This appears to be an unsubstantiated, convenient assumption. This concern is amplified by the ablation study in Appendix I.2 (Table 11), which was *only* conducted on Llama-3. This study reveals that the model's performance is *extremely sensitive* to this parameter. For instance:\nIncreasing $\\gamma$ from *0.65* to *0.75* causes the Generalization (G) score to drop sharply from 85.9 to 74.1. At $\\gamma=0.9$, the Generalization ability collapses almost entirely to 28.7.\n\nGiven this high sensitivity, how can the authors justify applying $\\gamma=0.65$ to the other models without a similar ablation? It is highly probable that the optimal (or even safe) $\\gamma$ is model-dependent. This sensitivity implies a very narrow \"safety boundary.\" How can we be confident that unrelated knowledge queries ($k_{old}$) won't accidentally cross this sensitive threshold as the database scales, especially if the chosen $\\gamma$ is not truly optimal for that specific model architecture?\n\n**2. On the True Cost of \"Scalability\" (Memory and Compute)**\n\nThe paper's claims of scalability and \"controllable\" overhead seem to obscure a significant trade-off in memory and compute.\n\n**Memory:** The authors state in Appendix G that editing 10,000 facts for Llama-3-8B adds 150M parameters.This implies that the paper's headline 100,000-fact model carries approximately **1.5B** in additional parameters. For an 8B model, this is an extra 19% in size. This is a very significant cost and can hardly be described as \"controllable.\" This trade-off should be made explicit in the main paper.\n**Computation:** More critically, the gated retrieval (Eq. 11)  requires calculating the cosine similarity of the current key $k^l$ against *all $m$ keys* in $K_1$, an $O(m)$ operation for every forward pass. The claim of only a 1.5% evaluation time increase for $m=10,000$ is surprising. Does this latency scale linearly with $m$? If so, the $m=100,000$ model would be substantially slower. \n\n**3. On the Unverified Claims of \"Modify\" and \"Delete\" Operations**\n\nThe authors repeatedly claim that NeuralDB is \"easy to manage for supporting operations such as appending, modifying, and deleting\". While appending is well-demonstrated, the \"modifying\" and \"deleting\" operations are completely unverified by experiments.\n\nThis is not a trivial claim. For example:\n\n**Delete:** What happens when a $(k_i, r_i)$ pair is deleted from the database? Does the model revert to its original, pre-trained knowledge for that query? Or does it become confused and generate an error?\n**Modify:** How is \"modification\" defined? Is it simply updating an existing residual $r_i$ for a key $k_i$? Or is it adding a new, conflicting fact about the same subject (which raises the question of key collision, a point not fully addressed in the paper)?\n\nTo validate these practical claims, could the authors provide a concrete case study? For instance: (1) Edit a fact. (2) Test that the edit is successful. (3) Delete the fact's $(k_i, r_i)$ pair from the NeuralDB. (4) Re-test the *same* prompt and report the model's output. Does it successfully revert to the original, pre-trained knowledge?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GcMVYd1TvZ", "forum": "Z0CX62CSJQ", "replyto": "Z0CX62CSJQ", "signatures": ["ICLR.cc/2026/Conference/Submission11767/Reviewer_HArh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11767/Reviewer_HArh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957420478, "cdate": 1761957420478, "tmdate": 1762922790994, "mdate": 1762922790994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}