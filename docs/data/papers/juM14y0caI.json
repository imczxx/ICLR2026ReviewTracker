{"id": "juM14y0caI", "number": 3895, "cdate": 1757562814566, "mdate": 1759898063993, "content": {"title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "abstract": "Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. \nIn this work, we analyze these fundamental challenges and provide a simple yet effective solution. \nTo maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress–process–recover pipeline, enabling stable and effective codebook training. \nBy combining VQBridge with learning annealing, our VQN achieves full (100\\%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ).\nThrough extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100\\% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.", "tldr": "We present FVQ, a discrete tokenizer framework that resolves key VQ challenges, sets new state-of-the-art results, and enables vanilla AR models to outperform VAR and DiT.", "keywords": ["Generative Model", "Image Quantization", "Autoregressive Modeling", "Image Generation", "Image Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e627b42b61f638ae8bc4bfd6fafe8dd036fc8076.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the training challenges of vector quantization (VQ) in image generation, identifying straight-through estimation bias, one-step-behind updates, and sparse gradients as the root causes of instability and codebook collapse. To address these issues, the authors propose VQBridge, a projector with a “compress–process–recover” pipeline, and combine it with learning rate annealing to form FVQ. Experiments show that FVQ consistently achieves 100% codebook utilization, significantly improves reconstruction and generation quality, and enables autoregressive image models to outperform both existing autoregressive and diffusion models, while maintaining strong scalability and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies and analyzes the three core challenges of VQ training (STE bias, one-step-behind updates, sparse gradients) with both theoretical and empirical support.\n2. Proposes a simple yet effective solution (VQBridge) that ensures 100% codebook utilization, addressing long-standing codebook collapse issues.\n3. Strong experimental results: FVQ outperforms prior tokenizers in both reconstruction and generation quality, and even enables autoregressive models to surpass diffusion models.\n4. Demonstrates robustness and scalability across codebook sizes (up to 262k), vector channels, and training epochs, with consistent improvements.\n5. Shows generalization to multi-code VQ methods (RQ-VAE, VAR), suggesting broader applicability."}, "weaknesses": {"value": "1. Novelty may be somewhat incremental, as the idea of using projectors (linear layers, joint optimization) has been explored; the main contribution is scaling with a stronger projector (ViT-based).\n2. Experimental comparisons focus mainly on ImageNet and a few baselines; evaluation on more diverse datasets or real-world tasks would strengthen claims.\n3. While FVQ achieves 100% codebook usage, the practical benefit of full utilization versus ~95–99% is not fully clarified.\n4. The method’s additional training cost and complexity (e.g., ViT blocks in VQBridge) are not extensively analyzed.\n5. Limited discussion of limitations or potential failure cases (e.g., extremely large-scale pretraining, cross-modal tokenizers)."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7CkBZPAosK", "forum": "juM14y0caI", "replyto": "juM14y0caI", "signatures": ["ICLR.cc/2026/Conference/Submission3895/Reviewer_pncL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3895/Reviewer_pncL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761277549638, "cdate": 1761277549638, "tmdate": 1762917087846, "mdate": 1762917087846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for VQ-VAE called VQBridge, which enables robust and scalable codebook optimization. The proposed FVQ achieves 100% codebook usage for various codebook size. Extensive experiments demonstrate that FVQ outperforms prior methods and significantly boosts the generative performance of autoregressive models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well written and easy to follow.\n\n- The proposed method achieves robust and scalable codebook optimization, enabling 100% codebook usage even with very large codebooks.\n\n- The authors conduct various experiments to show the effectiveness of the proposed results, both quantitatively and qualitatively."}, "weaknesses": {"value": "- While the empirical results are strong, the paper does not include an in-depth theoretical justification for why the proposed method achieves 100% codebook usage while VQGAN-LC cannot, especially considering that the only difference between VQGAN-LC and FVQ is the network architecture (MLP vs. ViT).\n\n- The authors need to provide experiments using larger codebooks (e.g., 65k or 262k) for image generation to demonstrate the necessity of using a large codebook."}, "questions": {"value": "- Why does learning rate annealing have such a significant impact on the MLP?\n\n- Does using a large codebook pose challenges for generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sqZAHqLEFh", "forum": "juM14y0caI", "replyto": "juM14y0caI", "signatures": ["ICLR.cc/2026/Conference/Submission3895/Reviewer_93aA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3895/Reviewer_93aA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730969777, "cdate": 1761730969777, "tmdate": 1762917087657, "mdate": 1762917087657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the long-standing problems of training instability and codebook collapse in vector-quantized (VQ) networks, especially when scaling up the codebook size. The authors attribute these issues to three fundamental challenges: straight-through estimator (STE) bias, one-step-behind updates, and sparse codebook gradients. To solve this, the paper proposes a training framework called FVQ (FullVQ), featuring a novel, ViT-based projector named VQBridge. VQBridge uses a \"compress-process-recover\" pipeline to jointly optimize the codebook vectors. This method, combined with learning annealing, aims to enable stable and scalable training. Experimental results demonstrate that FVQ achieves 100% codebook usage across various configurations (up to 262k) and attains state-of-the-art image reconstruction performance (rFID 0.88). More importantly, FVQ significantly boosts the performance of downstream autoregressive (AR) image generation models, enabling LlamaGen-XL to outperform strong diffusion models like DiT-XL/2."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Significance: Addresses a core bottleneck in VQ training—codebook collapse—which is critical for scalable discrete representation learning.\n 2. Effectiveness: The results are outstanding. Achieving 100% usage on a 262k codebook and an rFID of 0.88 is a major engineering and empirical achievement. \n3. Downstream Impact: One of the paper's strongest points is demonstrating how a good tokenizer empowers AR models. By only swapping the tokenizer, LlamaGen-L's FID improved from 3.81 to 2.39, and LlamaGen-XL's from 3.39 to 2.07, surpassing DiT-XL/2 (2.27). This is a very compelling result. \n4. Clarity: The paper is clearly written and well-organized. The problem, challenges, and solution are all well-articulated. 5. Thoroughness: The experimental evaluation is comprehensive, covering detailed architectural ablations, scaling analysis, and generalization tests."}, "weaknesses": {"value": "1. Limited Novelty: The core idea of using a projector for joint codebook optimization is not entirely new. The paper itself cites prior work that used linear layers for this. The main novelty lies in the design of a much more powerful, DiT-inspired projector (VQBridge). While highly effective, this feels more like an excellent architectural improvement than a fundamental conceptual leap.\n 2. Training Overhead: The VQBridge module introduces significant computational cost during training. Table 10 shows that for a 16k codebook, VQBridge has ~4x the FLOPS of a linear layer, and for 262k, it's ~3.2x. While the paper correctly states it adds no inference cost, this training overhead is a practical limitation that warrants more discussion. \n3. Analysis of Annealing: The paper claims learning annealing is crucial and shows a large performance gain in Table 3. However, the analysis in Figure 3 is confusing: it shows a linear layer's usage drops significantly with annealing, while VQBridge's remains at 100%. This suggests VQBridge is simply robust to annealing, rather than it being a synergistic component. Why does annealing hurt the linear projector's usage so badly? This is not fully explained."}, "questions": {"value": "1. (Re: Weakness 2) Could the authors elaborate on the practical training time (wall-clock) and VRAM overhead of VQBridge compared to baselines (e.g., VQGAN or VQGAN-LC w/ linear projector)? Table 10 gives FLOPS, but what is the practical impact for reproducibility and adoption? \n2. (Re: Weakness 3) The interaction with learning annealing is interesting. In Figure 3, annealing drastically hurts the linear projector's usage but has no effect on VQBridge's. Yet, Table 3 shows annealing is vital for VQBridge's performance (rFID). Could the authors clarify this? Does VQBridge's powerful joint optimization prevent the collapse that annealing would otherwise cause in a weaker projector?\n 3. The VQBridge design is inspired by DiT. Did the authors experiment with other powerful interaction mechanisms? It's surprising that a 5-layer MLP failed so badly (20% usage). Why do the authors believe the ViT block (Patchify -> Process -> Unpatchify) architecture is so much more effective than a deep MLP for this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "13rZMxtigB", "forum": "juM14y0caI", "replyto": "juM14y0caI", "signatures": ["ICLR.cc/2026/Conference/Submission3895/Reviewer_wFm6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3895/Reviewer_wFm6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829799620, "cdate": 1761829799620, "tmdate": 1762917087467, "mdate": 1762917087467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses three key errors in Vector-Quantized Networks (VQNs): the bias introduced by the straight-through estimator (STE), the misalignment caused by one-step-behind updates, and the inefficiency resulting from sparse codebook utilization. These issues become particularly severe when scaling up codebook size and vector dimensions. The authors propose FVQ, a training framework that combines a novel projector module called VQBridge with learning rate annealing. VQBridge is designed as a \"compress-process-recover\" module utilizing ViT blocks to enable global joint optimization of the entire codebook. The experimental results are compelling: the method achieves 100% codebook utilization across various scales, including a massive 262k codebook, and establishes new state-of-the-art reconstruction performance. Furthermore, when integrated into autoregressive image generation pipelines, it significantly enhances generation quality, surpassing strong baselines such as VAR and DiT, thereby unlocking greater potential. The proposed framework also maintains high scalability across other methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The analysis is thorough and precise: This paper presents a clear and well-structured examination of the fundamental optimization challenges in VQ training. The explanations are well-supported by comprehensive comparative experiments and mathematical proofs. It is particularly noteworthy how the paper elaborates on the positive role of learning rate annealing in mitigating the one-step-behind update error, which significantly enhances the theoretical credibility.\n\n2. High-Impact Results: The achievement of 100% codebook utilization at scale (up to 262k codes) is a major result . The reported reconstruction (rFID) and generation (FID) metrics are state-of-the-art, convincingly demonstrating the practical benefit of solving the codebook collapse problem.\n3. The design is elegant and practical: The VQBridge module presents a clever and well-designed solution. The \"compress-process-recover\" pipeline not only makes scaling to large codebooks computationally feasible but also significantly enhances the representational capacity of the projection function, effectively overcoming the codebook collapse issue inherent in linear projectors. A key advantage is that the module is employed exclusively during training, introducing zero overhead during inference, which makes it highly appealing for practical deployment.\n4. Extensive Experimentation: The authors perform a wide array of experiments, including ablation studies on the VQBridge components, scalability studies across codebook size, vector channels, and training epochs, and generalization tests on multi-code VQ methods (RQ-VAE, VAR). This leaves little doubt about the robustness and effectiveness of the proposed method."}, "weaknesses": {"value": "1. Issues with the Mathematical Formulation of the Framework: Mathematical definitions are provided for the framework in the methodology section; however, there appears to be an apparent contradiction between the statement \"For each group, the vectors are compressed into a single vector\" and its corresponding matrix representation. Additionally, there seem to be inconsistencies in the tensor shapes described in the subsequent reshaping operation. The authors need to verify the accuracy of this part of the mathematical representation.\n\n2. Computational Cost of Training: Although the inference cost remains unchanged, the introduction of the ViT-based VQBridge significantly increases the computational cost of training, particularly for large codebooks. The authors do not discuss these newly introduced training costs in the paper.\n3. Ablation on Annealing Schedule: The paper heavily relies on the combination of VQBridge + Learning Annealing. While an ablation in Table 9 shows that the schedule type (cosine/linear) has a negligible impact, it does not ablate the *necessity* of the specific annealing parameters (warmup proportion, decay schedule) used. The robustness of the method to different annealing strategies could be better explored.\n4. Comparison to Closest Contemporaries: While comparisons to VQGAN-LC are provided, a more direct and detailed comparison with other recent methods aiming for high usage with large codebooks (like IBQ) in terms of training efficiency, stability, and final performance on a level playing field would further strengthen the paper."}, "questions": {"value": "1. Q1: Mathematical Formulation: Could the authors please verify the mathematical formulation in the methodology section? Specifically, what is the precise meaning behind the dimensionality of the matrices described in the framework?\n\n2. Q2: Training Efficiency: Could the authors provide an estimate of the relative increase in total training time (e.g., for the 16k codebook configuration) when incorporating VQBridge compared to the VQGAN baseline? This would help practitioners better assess the trade-off between final performance and computational budget.\n3. Loss Function: The authors mention in the paper that beyond estimation error, \"codebook usage can therefore serve as a complementary metric.\" However, the paper does not explicitly state whether the framework involves a specific loss function for the codebook. Could the authors clarify how the codebook loss function is incorporated?\n\n4. Q3: Design Choices: The scaling law between codebook size `K` and patch size `p` (`p = sqrt(K)/4`) is found empirically. Was there any intuition or theoretical justification for this relationship? Why does compressing the codebook by this specific factor lead to optimal performance?\n5. Q4: Failure Mode Analysis: In the rare case that codebook usage does not reach 100%, what are the observed failure modes? Does the training become unstable, or does performance just plateau at a sub-optimal level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CBFt5sREzw", "forum": "juM14y0caI", "replyto": "juM14y0caI", "signatures": ["ICLR.cc/2026/Conference/Submission3895/Reviewer_kziK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3895/Reviewer_kziK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975640868, "cdate": 1761975640868, "tmdate": 1762917087143, "mdate": 1762917087143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies presents a well justified and supported study into vector quantization for image tokenization. Generating discrete tokens from images is a non-trivial, yet crucial task for application in a myriad of modern vision methods. To address the training and scaling of the codebooks as compared to other methods, the author tackle challenges of Straight Through Estimation bias, lagging updates, and sparse updates. They present a method in VQBridge that uses mixing between entires at training, better initial alignment, and a different update method to handle the mentioned challenges. The authors accompany this introduced method and theory with good ablation studies and experiments to motivate the application of the introduced method in different studies, making this a very useful study in the explored topic."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The presented challenges are each sufficiently well motivated with examples and theory \n2. Each presented challenge is suitably addressed in the building of the suggested VQBridge method \n3. The suggested method is studied in a large domain of suitable of applications, showing good results in the strong suite of experimentation\n4. Comprehensive ablation studies are included across different vector and codebook sizes, as well as the training efficiency is explained - which justify the claims of scalability \n5. Generalization studies are added to show applicability in multi code representation as well \n6. The effects of the use of the tokens produced are also studied in downstream tasks, which motivates the applicability of the method well"}, "weaknesses": {"value": "1. The argument for the proposed method could be made stronger with the inclusion of more details on computational cost. In particular, the added computational cost for training and how the training cost differs between the compared methods since only training epochs is used as an example \n2. The larger codebooks retain the 100% utilization and are still shown to have great performance. Still, given that the patchification process induces sharing between entires, some insight into the uniqueness of the tokens generated and/or the distribution of active tokens could further ground the presented results."}, "questions": {"value": "1. Expanding on the results in table 3, Is codebook size the only factor that influences the optimal patch size? Have the authors studied, as an example, how the patch size and vector channels interact ? \n2. The smaller codebooks produced by FVQ seem to be provide strong results in reconstruction. Does the same benefit of not needing a larger codebook extend to downstream tasks ? Have the authors been able to study downstream tasks from models using different-size codebooks generated by FVQ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ifT3vBkPN9", "forum": "juM14y0caI", "replyto": "juM14y0caI", "signatures": ["ICLR.cc/2026/Conference/Submission3895/Reviewer_M7kB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3895/Reviewer_M7kB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000759783, "cdate": 1762000759783, "tmdate": 1762917086902, "mdate": 1762917086902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}