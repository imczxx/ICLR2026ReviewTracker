{"id": "AzvnkMm0iv", "number": 21301, "cdate": 1758316035514, "mdate": 1759896929986, "content": {"title": "Unlearning with asymmetric sources: improved unlearning-utility trade-off with public data", "abstract": "Achieving certified data erasure in machine unlearning faces a fundamental trade-off: preserving model utility requires less noise, but formal privacy guarantees demand more. This tension typically degrades model performance. In this work, we study this challenge in Langevin Unlearning, a noisy variant of SGD that is uniquely amenable to theoretical analysis.\nWe introduce an asymmetric unlearning setting assuming that datasets contain both private data (subject to unlearning) and public data (permanently retained). Our framework demonstrates that incorporating public data enables better unlearning-utility trade-offs without additional noise or restrictive differential privacy assumptions. We prove that public data volume quadratically reduces the Rényi divergence between unlearning and retraining distributions, allowing control over unlearning guarantees through data composition rather than noise amplification. The framework also provides a fine-grained analysis of how distributional alignment between public and private data affects performance preservation. Empirical validation using variational Rényi divergence estimation confirms our theoretical predictions, showing that strategic public data injection achieves comparable unlearning efficacy while significantly improving model performance and computational efficiency.", "tldr": "Incorporating public data in Langevin unlearning improves the unlearning-utility trade-off", "keywords": ["Machine unlearning", "stochastic optimization", "domain adaptation", "Langevin dynamics"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/923f8da4a94558addcf46aac2ff0dcc6347d7e9a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates machine unlearning in a practical \"asymmetric\" setting, where the training dataset comprises both private data (subject to unlearning requests) and public data (always retained). The authors focus on Langevin Unlearning, a noisy gradient-based method, and demonstrate both theoretically and empirically that leveraging public data can significantly improve the fundamental trade-off between unlearning efficacy and model utility.\n\nThe core theoretical contribution is showing that the volume of public data reduces the Renyi divergence between the unlearned model distribution and the ideal retrained distribution (generated by retraining from scratch). This provides a new mechanism for achieving certified unlearning, relying on \"data composition\" rather than \"noise amplification,\" which typically degrades utility.\n\nEmpirical validation is provided through two distinct methods: a direct estimation of Renyi divergence on a vision task, which confirms the theoretical scaling, and a standard membership inference attack, which confirms the practical unlearning efficacy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a highly practical and important problem. As models are increasingly trained on mixed-sensitivity data, methods that can leverage this asymmetry are essential.\n\n2. The paper provides a clear story for its core claim: public data makes the initial model and the retrained model closer in distribution (Thm 3.1), and this favorable starting point makes the final unlearning process more efficient and effective (Thm 3.2).\n\n3. Theorem 3.3 provides a formal, interpretable bound that explicitly links post-unlearning utility to the distributional mismatch ($D_{\\infty}(P_{priv} || P_{pub})$), which is an important factor in practice.\n\n4. The claims are supported by two different experimental approaches (direct divergence estimation and MIA) on two different data modalities, which strengthens the paper's conclusions."}, "weaknesses": {"value": "1. A finding in Table 1 is that model utility was preserved almost perfectly, despite a significant misalignment between the public and private  domains. The authors correctly note this implies their utility bound (Thm 3.3) is \"overly conservative.\" The paper would be stronger if it discussed this interesting gap more deeply. Is this an artifact of the pre-trained embeddings (DinoV2) used, which might already align these domains? \n\n2.  As mentioned, the theoretical novelty lies in the application of existing analysis tools to this new, asymmetric setting, rather than in the development of new theoretical machinery. This is a mild weakness, as the application itself is insightful, but it means the paper is more of a strong application paper than a fundamental theory paper."}, "questions": {"value": "1. Regarding the \"overly conservative\" utility bound and the strong empirical utility results in Table 1: Do the authors have a hypothesis for why the utility was so high even with the Clipart/Quickdraw misalignment? Could this be related to the use of pre-trained DinoV2 embeddings? \n\n2. The theoretical analysis is based on full-batch Projected Noisy Gradient Descent. How do the authors anticipate these results, particularly the quadratic scaling in Theorem 3.1, would transfer to the more practical mini-batch setting?\n\n3. The experiments for Rényi estimation use $\\alpha=2$. Why not numerically optimize over it and pick the lowest one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BAJAzY1Nsd", "forum": "AzvnkMm0iv", "replyto": "AzvnkMm0iv", "signatures": ["ICLR.cc/2026/Conference/Submission21301/Reviewer_gRLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21301/Reviewer_gRLK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487773398, "cdate": 1761487773398, "tmdate": 1762941680693, "mdate": 1762941680693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates machine unlearning in a setting with both public and private data sources. The authors use Langevin Unlearning and theoretically argue that adding public data can improve the unlearning-utility trade-off. They provide a bound showing that public data volume quadratically reduces the Rényi divergence between the unlearning and retraining distributions, suggesting unlearning can be achieved with less noise, thereby preserving utility."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea of using asymmetric data sources (public and private) to improve the unlearning-utility trade-off is timely and relevant.\n\n2. The paper attempts to provide a theoretical grounding for this idea by analyzing Langevin Unlearning and deriving bounds based on Rényi divergence, which is a rigorous approach."}, "weaknesses": {"value": "1. **Disconnect from Premise:** A core theoretical issue is the symmetric role that n_pub (public data size) and n_priv (private data size) seem to play in the main theorem (Theorem 3.1). The analysis suggests the same benefit could be achieved by simply adding more private data. This appears to contradict the paper's central premise, which is that leveraging *asymmetric* sources provides a unique benefit that is different from just having a larger private dataset.\n\n2. **Potentially Vacuous Utility Bound:** The main utility trade-off bound appears potentially vacuous, particularly due to the term that increases exponentially with the max divergence (!) between the public and private distributions. This is a major concern. The only scenario where this bound seems non-vacuous is if this divergence is extremely small or zero. However, if the distributions are nearly identical, the theoretical contribution feels less significant, as the public data is effectively just more private data (tying back to the first point).\n\n3. **Missing Comparisons & Restrictive Assumptions:** The paper's contribution is difficult to situate without a comparison to key recent work, such as Koloskova et al. (2025) \"Certified Unlearning for Neural Networks\", which reportedly offers certified unlearning with fewer assumptions. The assumptions made in this paper (e.g., log-sobolev) are quite restrictive and may severely limit the practical applicability of the theoretical results.\n\n4. **Gap in Experimental Validation:** There is a disconnect between the theoretical claims for unlearning and the main experimental setup. The experiment uses a linear model on static embeddings, not a modern, non-convex deep neural network. It's highly unclear if any of the observed trade-offs (e.g., in Table 1) would hold in a more complex and realistic setting. The reported requirement of training 30,000 models to get this result also raises questions about the method's practicality.\n\n5. **Weak Link Between Theory and Practice:** Following the previous point, the link between the theoretical values and the experimental parameters is weak. For instance, the noise level chosen is low, but the corresponding privacy guarantee (epsilon) is never calculated or reported. This makes it impossible to judge if the \"unlearning\" is meaningful (epsilon could be extremely large). Furthermore, for the utility experiments in Section 4.1, Membership Inference Attack (MIA) results are surprisingly absent from the main body for this first experiment, which is a standard way to empirically validate unlearning.\n\n6. **Limited Empirical Results:** The empirical evidence presented is not fully convincing. In Figure 2a, for example, the confidence intervals are large and appear to overlap significantly. This makes it difficult to draw firm conclusions from the experiment and weakens the empirical support for the paper's claims."}, "questions": {"value": "1. Could you elaborate on the symmetric roles of n_pub and n_priv in your theoretical results? If they are symmetric, how does this support the core claim of an asymmetric benefit, especially given the utility bound's exponential dependence on the max divergence?\n\n2. What is the effective epsilon privacy guarantee for the noise level used in your experiments? And why were Membership Inference Attack (MIA) results for the Section 4.1 experiments not included in the main paper?\n\n3. Can you justify the choice of a linear model on static embeddings for the main experiment? How do you expect these results to translate to non-convex deep learning models, which are the standard for most modern unlearning discussions?\n\n4. How does your Langevin Unlearning approach compare to Koloskova et al. (2025) \"Certified Unlearning for Neural Networks\"? A discussion on this and the necessity of the strong log-sobolev assumption would be very helpful for positioning your contribution and the corresponding assumptions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HUNw0fv5cG", "forum": "AzvnkMm0iv", "replyto": "AzvnkMm0iv", "signatures": ["ICLR.cc/2026/Conference/Submission21301/Reviewer_cRhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21301/Reviewer_cRhB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755139272, "cdate": 1761755139272, "tmdate": 1762941680424, "mdate": 1762941680424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This proof-based paper investigates the theoretical foundations of machine unlearning by analyzing the role of public and private data in achieving effective unlearning. The authors derive formal guarantees showing that as the amount of public data increases, the Rényi divergence between the retrained and unlearned model distributions decreases quadratically. This result highlights the critical utility of public data in approximating the retraining outcome without full retraining. Furthermore, the paper provides a detailed theoretical analysis of how distributional mismatch between public and private datasets influences the efficiency and reliability of the unlearning process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow, and the main message is conveyed clearly.\n\n2. The theoretical analysis is rigorous and well-supported by formal proofs.\n\n3. The paper addresses an important and timely problem in the field of machine unlearning."}, "weaknesses": {"value": "1. The main claim—that increasing the amount of public data improves unlearning performance—appears somewhat self-evident. It is conceptually similar to stating that reducing the proportion of forget data helps maintain retain accuracy. This insight has already been discussed in prior theoretical works (e.g., [A], [B]), which diminishes the novelty of the contribution.\n\n[A] Certified Data Removal from Machine Learning Models. (Guo et al.)\n[B] Towards Source-Free Machine Unlearning (Ahmed et al.)\n\n2. In Theorem 3.3, the authors show that the expected loss on retain data under the unlearned model is upper bounded by the domain distribution gap between the public and private datasets. However, this result seems somewhat tautological — if the two distributions are identical, it trivially reduces to the case of having less forget data within a unified distribution; if they differ, the bound naturally worsens, which is self-evident. Hence, the practical utility or new insight offered by this theorem is unclear.\n\n3. The paper lacks sufficient experimental validation. Key quantitative results—such as forget accuracy, retain accuracy, and test accuracy across different datasets or classes—are missing. The evaluation is limited to the DomainNet dataset, and no diverse metrics are reported to substantiate the theoretical claims."}, "questions": {"value": "1. In Table 1, the reported numbers appear identical across rows, and the gap values are all 0.0. Could the authors clarify the purpose of this table? If the intention is to show that increasing public data reduces the gap, the results do not reflect that trend. Moreover, as the amount of public data increases, it seems the size of the forget data simultaneously decreases—how is this a fair or controlled comparison?\n\n2. The explanation of the weight distribution mechanism using N models is unclear. A concise pseudocode or algorithmic description would greatly help in understanding this procedure and verifying its theoretical soundness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5vTMUkRAjI", "forum": "AzvnkMm0iv", "replyto": "AzvnkMm0iv", "signatures": ["ICLR.cc/2026/Conference/Submission21301/Reviewer_DwwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21301/Reviewer_DwwT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938425240, "cdate": 1761938425240, "tmdate": 1762941680021, "mdate": 1762941680021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes Langevin unlearning under a more realistic asymmetric setting in which a subset of the training data is public and never needs to be forgotten. Theoretical analysis demonstrates that incorporating public data enables better unlearning-utility trade-off without additional noise or restrictive differential privacy assumptions, allowing control over unlearning guarantees through data composition rather than noise amplification. The authors provide a fine-grained analysis of how distributional alignment between public and private data affects this trade-off. Empirical results corroborate the theoretical analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a realistic asymmetric-unlearning setup that separates public data (never forgotten) from private data (subject to removal), breaking the traditional noise-vs-utility deadlock.\n2. The authors provide the first rigorous proof that increasing public-data volume quadratically shrinks the Rényi divergence between the unlearned and retrained distributions.\n3. The paper is overall well-organized."}, "weaknesses": {"value": "1. The idea lacks novelty, as the incorporation of public data represents a fairly natural extension and is already a well-established technique in the field of differential privacy. The main part of the theoretical derivation is built upon the analysis of Langevin Unlearning, with only minor modifications to the parameters $m$ and $n$.\n2. Section 2.3 provides an insufficiently detailed introduction to Langevin Unlearning. The reviewers suggest supplementing it with a description of LU's training procedure or including pseudocode for the LU algorithm. Additionally, the last paragraph of Section 2.3 contains repetitive content.\n3. In Chapter 3, the symbols $\\theta_T$, $\\mathcal P$, $\\pi_0$, and $P_{train}$ are not defined.\n4. The reviewer suggests that the citations in Theorem 3.1 and Theorem 3.2 should refer specifically to the relevant theorems in the cited works.\n5. The experimental section lacks figures or tables illustrating the unlearning-utility trade-off, as well as results showing how accuracy changes as $ K $ increases.\n6. There is a confusing or inconsistent use of $K$ in Table 1.\n7. The analysis of Figure 3 should be presented in the main text rather than in the figure caption.\n8. The writing in the section “Retraining performance bound” lacks logical coherence with the preceding content."}, "questions": {"value": "1. In Definition 3.1, should $I(Q,P)=E_P[\\cdot]$ rather than $E_P[\\cdot]$?\n2. In Theorem 3.1, is the numerator on the right-hand side of the inequality missing an \\(\\eta\\) term?\n3. The paper states: “We evaluate post-unlearning performance on the private data distribution only, reflecting realistic deployment scenarios where the primary concern is maintaining model quality on the sensitive data that remains after unlearning.” The reviewer is confused as to why the focus is solely on maintaining model quality on the sensitive data that remains after unlearning, and not on maintaining model quality on the public data as well, since public data is a part of the training dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UxIqpw0ALO", "forum": "AzvnkMm0iv", "replyto": "AzvnkMm0iv", "signatures": ["ICLR.cc/2026/Conference/Submission21301/Reviewer_TMUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21301/Reviewer_TMUa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984054145, "cdate": 1761984054145, "tmdate": 1762941679577, "mdate": 1762941679577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}