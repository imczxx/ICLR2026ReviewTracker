{"id": "EOTCSANb3l", "number": 4285, "cdate": 1757655249232, "mdate": 1763119557512, "content": {"title": "SpatialTree: Branching Out Spatial Intelligence in MLLMs via a Capability Tree", "abstract": "Spatial Intelligence (SI) is rapidly becoming a cornerstone capability for MLLMs, enabling them to seamlessly perceive, reason about, and interact with complex 3D environments — a critical step towards truly embodied AI systems. However, previous works typically focus on a few specific 3D tasks, offering only a fragmented view of MLLMs’ spatial abilities. Inspired by cognitive science studys, we propose SpatialTree, a hierarchical taxonomy that organizes SI into a capability tree—from low level perception (L1), mental mapping (L2), mental simulation (L3), to high level agentic competence (L4). Building on this taxonomy, we introduce the first capability-centric benchmark that thoroughly evaluates the spatial abilities of MLLMs. Moreover, extensive experiments are conducted to investigate the compositional nature of spatial abilities, examining the dependencies among the abilities and identifying the atomic abilities that exert the greatest influence on others. Furthermore, we introduce SpatialEngine, an extensible framework that integrates 3D vision perception models with MLLMs into a progressive annotator, enabling comprehensive data annotation across the entire tree.", "tldr": "", "keywords": ["Spatial Intelligence; MLLMs; Capability Taxonomy"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dd3deade1cde906759f20b28c19858e7f43276cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SpatialTree, a suite of spatial benchmarks organized into a hierarchical taxonomy, referred to as a capability tree. SpatialTree arranges 12 benchmarks into a four-level hierarchy: low-level perception (L1), mental mapping (L2), mental simulation (L3), and high-level agentic competence (L4). To address capabilities not covered by existing benchmarks, the authors developed SpatialPlus. The experiments evaluated mainstream open-source and Proprietary models of various sizes. Results show that lower-level capabilities are relatively independent, while higher-level capabilities exhibit strong interdependencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SpatialTree introduces a hierarchical taxonomy that organizes 12 benchmarks into a four-level structure: L4 (robotics tasks), L3 (route planning, etc.), L2 (perspective-taking, cognitive mapping, etc.), and L1 (perception tasks such as metric measurment). Figure A clearly illustrates how existing datasets are reorganized into these four capability levels.\n- To address capabilities not covered by existing benchmarks, the authors developed SpatialEngine to synthesize the SpatialPlus dataset.\n- Comprehensive experiments were conducted to evaluate mainstream open-source and Proprietary models of various sizes. Particularly, Figure.4 analyzes the relationships between capabilities. Results indicate that lower-level capabilities (L1/L2) are relatively independent, while higher-level capabilities (L3/L4) exhibit strong interdependencies and benefit from low-level information."}, "weaknesses": {"value": "I believe it is important to provide a more thorough discussion of recent works.\n\n**Benchmark Scope**\n\n- Several contemporaneous studies ([1,2,3]) share a similar scope with this work, focusing on collecting or re-annotating existing benchmarks. A detailed comparison of the similarities and differences between these studies and this work is necessary. Additionally, given these works, it may be worth reconsidering the statement in L108 claiming this as “the **first** comprehensive benchmark for spatial intelligence in MLLMs.”\n- CoreCognition [4] also conducted task correlation analyses, and SpatialTree’s Figure 4 is notably similar to their analysis. The relationship between low-level and high-level capabilities could further benefit from a comparison with CoreCognition's findings. Moreover, since CoreCognition is also grounded in cognitive science, it would be valuable to discuss the similarities and differences between its taxonomy and that of SpatialTree.\n\n**Experimental Results**\n\n- The observations in Table 2 differ from those in [2,3], where GPT-5 demonstrated strong spatial capabilities. Notably, [2] highlights GPT-5's performance under different thinking modes, while in Table 2, GPT-5 is not classified as a thinking model. The authors may need to clarify the experimental settings for GPT-5.\n- Sec. 5.4 (L466-477) shows that augmenting input images with visual correspondence annotations led to a 12% performance improvement for Gemini-2.5-pro in L4 tasks. However, this contrasts with observations in MMSI [5], where its Figure 5 shows that visual prompting yields only slight performance gains. This discrepancy needs further discussion.\n\n[1] SITE: towards Spatial Intelligence Thorough Evaluation\n\n[2] Holistic Evaluation of Multimodal LLMs on Spatial Intelligence\n\n[3] How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective\n\n[4] CoreCognition: Core Knowledge Deficits in Multi-Modal Language Models\n\n[5] MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"}, "questions": {"value": "I believe this paper needs to include more comparisons with recent benchmarks in terms of scope and experimental results. Details can be found in the weakness section. I am willing to raise my score if my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eW2oaPdPwu", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Reviewer_36Yc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4285/Reviewer_36Yc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995420002, "cdate": 1761995420002, "tmdate": 1762917274641, "mdate": 1762917274641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5q5yN7wAGZ", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119556756, "cdate": 1763119556756, "tmdate": 1763119556756, "mdate": 1763119556756, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SpatialTree, a hierarchical framework to evaluate spatial intelligence in multimodal large language models (MLLMs).\nIt organizes spatial abilities into four levels—perception, mental mapping, mental simulation, and agentic competence—and builds the SpatialTree-Bench benchmark with balanced coverage across these layers.\nUsing SpatialEngine, the authors generate annotations and systematically analyze inter-level dependencies, showing that foundational perceptual skills (e.g., geometry, distance) support higher-level reasoning and navigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Capability-Centric Framework:\nThe hierarchical “SpatialTree” design offers a structured and interpretable perspective on spatial intelligence in MLLMs, bridging cognitive science theory with computational evaluation.\n\nComprehensive Benchmarking:\nSpatialTree-Bench integrates diverse datasets and introduces SpatialPlus to fill ability gaps, covering perception to action-level intelligence. The breadth of coverage surpasses prior spatial reasoning benchmarks (e.g., MMSI-Bench, VSI-Bench).\n\nSystematic Analysis:\nThe work quantifies inter-capability dependencies (Fig. 4) and empirically demonstrates that low-level perception abilities correlate with high-level reasoning and planning. This analysis is both original and insightful.\n\nPractical Tooling Contribution:\nSpatialEngine provides a modular annotation and evaluation pipeline that can be reused for future benchmarks or training pipelines for embodied AI.\n\nStrong Empirical Evaluation:\nThe authors benchmarked over a dozen advanced MLLMs under consistent settings (Table 2), offering valuable insights into model hierarchy and reasoning performance."}, "weaknesses": {"value": "Limited Novelty in Model Design:\nThe paper’s primary contribution lies in benchmarking and taxonomy design, rather than proposing new MLLM architectures or learning algorithms. The “SpatialEngine” and evaluation setup are valuable but largely engineering extensions.\n\nBenchmark Construction Complexity:\nThe pipeline integrates many datasets and annotations, but the details of harmonization and standardization (e.g., difficulty balancing, metric normalization) are not deeply elaborated. This may affect reproducibility.\n\nEvaluation Bias:\nWhile broad, the benchmark still relies heavily on simulated and indoor datasets (e.g., Matterport3D, games, robotics footage), limiting ecological diversity. Outdoor or large-scale geospatial reasoning is not addressed.\n\nLimited Validation of Hierarchical Causality:\nThe paper infers inter-level dependencies through correlations, but does not demonstrate causal relationships (e.g., ablating perception abilities to observe downstream degradation).\n\nInterpretability of “Capability Scores”:\nThe paper defines multiple overlapping metrics (accuracy, error distance, GPT-judged reasoning), but it is unclear how scores across layers are calibrated or weighted for fair comparison."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WsSqernmqP", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Reviewer_KKTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4285/Reviewer_KKTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012774281, "cdate": 1762012774281, "tmdate": 1762917274105, "mdate": 1762917274105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SpatialTree, a novel capability-centric framework for systematically analyzing, evaluating, and enhancing spatial intelligence in multimodal large language models (MLLMs). To operationalize this framework, the authors develop SpatialEngine, a scalable data generation and annotation engine that supports multi-level task construction across spatial reasoning subtasks. Leveraging these tools, the paper introduces a benchmark encompassing more than 15 spatial subtasks and conducts a hierarchical analysis of inter-level dependencies across prominent MLLMs such as GPT-4V, Gemini, and Qwen-VL. Overall, the paper contributes both a conceptual hierarchy and a comprehensive benchmark, providing a structured lens for diagnosing and potentially improving the spatial reasoning capabilities of multimodal foundation models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[1] The proposed SpatialEngine represents a strong engineering and methodological contribution, offering a scalable and reproducible pipeline for generating multi-modal, multi-level spatial reasoning datasets.\n[2] The SpatialTree framework presents a meaningful conceptual advance by organizing spatial reasoning into a hierarchical capability structure. This enables more interpretable diagnostics and may guide future research in compositional generalization, embodied reasoning, and spatial grounding for robotics and vision-language models."}, "weaknesses": {"value": "[1] While the analysis is comprehensive, the paper primarily delivers diagnostic insights rather than methodological advances. To enhance practical impact, the authors are encouraged to complement their analytical framework with concrete improvement strategies, such as targeted training interventions, adaptive curriculum design, or model adaptation techniques informed by the proposed hierarchy.\n[2] Many spatial subtasks are synthetically generated via SpatialEngine, which raises concerns about ecological and practical validity. It remains unclear whether the observed performance patterns in these synthetic benchmarks generalize to real-world embodied reasoning tasks, such as 3D navigation or physical manipulation.\n[3] The claimed hierarchical dependencies between low-level (L1–L2) and high-level (L3–L4) spatial capabilities are inferred primarily through correlation-based visualization. However, without causal or intervention-based experiments (e.g., selectively perturbing lower-level representations or excluding specific capability subsets), it is difficult to establish causal grounding of the hierarchy. Including such analyses would substantiate the “capability tree” argument and strengthen the paper’s empirical validity."}, "questions": {"value": "[1] Have the authors tested whether improvements in SpatialTree-derived metrics (e.g., L1–L3) correlate with actual performance gains in embodied tasks, such as manipulation or navigation?\n[2] Can atomic prompting be formalized into a modular prompting strategy compatible with multi-turn dialogue or interactive reasoning?\n[3] While SpatialEngine generates synthetic annotations, how are the correctness and diversity of the tasks validated? Are there human evaluations or inter-rater checks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Im117p001B", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Reviewer_qkT6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4285/Reviewer_qkT6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328946427, "cdate": 1762328946427, "tmdate": 1762917273803, "mdate": 1762917273803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses spatial intelligence (SI) in LMs, a critical capability to bridge the gap between LMs and robotics. They introduce a hierarchy of capabilities which they dub “spatialtree” which can inform evaluation of SI in LMs, and build that into a benchmark using SpatialEngine, a framework for annotating various subtasks from the tree on 3D data.\n\nThey start from a hypothetical “spatial AI agent” to describe the ideal of SI. They use cinematography concepts like panning tilting and trucking to introduce primitives around moving the camera, which are formalized as transformation matrices. Similar analogies are used to motivate other elements in the SpatialTree hierarchy, such as mental mapping, perception, etc.\n\nUsing SpatialEngine they reorganize existing benchmark samples to capture SI. They build these using some undescribed pipelines, into a SpatialBench benchmark, which they use to evaluate a broad set of models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Ambitious effort to taxonomize spatial intelligence.\n\nNovel benchmark that (allegedly) aligns with the proposed schema\n\nExtensive evaluation of free weight and proprietary models. The benchmark isn’t saturated yet, so it should be useful going forward."}, "weaknesses": {"value": "The paper spends a lot of time introducing fancy concepts, for sparse description of methods. A lot of the equations are superfluous, taking up space that would be better spent on concrete examples. In general, too much “what” and not enough “how.”\n\nI have trouble understanding the value of the SpatialTree taxonomy, which is really central to the paper. Lots of time is spent on the “what” and very little on the “how”. An emblematic passage is sections 3.2 and 3.3. They describe concepts that may be important for SI like causal reasoning, sequential planning, and “mental mapping,” but they provide no details on how those actually are evaluated. \n\nHow am I supposed to assess how well your benchmark captures “mental simulation” when you haven’t even said which datasets those specific samples have come from? I would really need to see a list of tasks instead of hand wavy “e.g.” phrases here. \n\nWhat are the data statistics? How many samples in what domains? \n\n(Line 307-311): “We construct several reusable pipelines…” how do you do this? What are these pipelines? What are the expected outputs? \n\nAll of these are very basic main text stuff.\n\nFrankly, I do not come away with an understanding of what you concretely did when I read the paper."}, "questions": {"value": "Curious to see your reactions to weaknesses. Please correct any misunderstandings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CeOb112Yj", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Reviewer_fppU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4285/Reviewer_fppU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932078515, "cdate": 1762932078515, "tmdate": 1762932078515, "mdate": 1762932078515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses spatial intelligence (SI) in LMs, a critical capability to bridge the gap between LMs and robotics. They introduce a hierarchy of capabilities which they dub “spatialtree” which can inform evaluation of SI in LMs, and build that into a benchmark using SpatialEngine, a framework for annotating various subtasks from the tree on 3D data.\n\nThey start from a hypothetical “spatial AI agent” to describe the ideal of SI. They use cinematography concepts like panning tilting and trucking to introduce primitives around moving the camera, which are formalized as transformation matrices. Similar analogies are used to motivate other elements in the SpatialTree hierarchy, such as mental mapping, perception, etc.\n\nUsing SpatialEngine they reorganize existing benchmark samples to capture SI. They build these using some undescribed pipelines, into a SpatialBench benchmark, which they use to evaluate a broad set of models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Ambitious effort to taxonomize spatial intelligence.\n\nNovel benchmark that (allegedly) aligns with the proposed schema\n\nExtensive evaluation of free weight and proprietary models. The benchmark isn’t saturated yet, so it should be useful going forward."}, "weaknesses": {"value": "The paper spends a lot of time introducing fancy concepts, for sparse description of methods. A lot of the equations are superfluous, taking up space that would be better spent on concrete examples. In general, too much “what” and not enough “how.”\n\nI have trouble understanding the value of the SpatialTree taxonomy, which is really central to the paper. Lots of time is spent on the “what” and very little on the “how”. An emblematic passage is sections 3.2 and 3.3. They describe concepts that may be important for SI like causal reasoning, sequential planning, and “mental mapping,” but they provide no details on how those actually are evaluated. \n\nHow am I supposed to assess how well your benchmark captures “mental simulation” when you haven’t even said which datasets those specific samples have come from? I would really need to see a list of tasks instead of hand wavy “e.g.” phrases here. \n\nWhat are the data statistics? How many samples in what domains? \n\n(Line 307-311): “We construct several reusable pipelines…” how do you do this? What are these pipelines? What are the expected outputs? \n\nAll of these are very basic main text stuff.\n\nFrankly, I do not come away with an understanding of what you concretely did when I read the paper."}, "questions": {"value": "Curious to see your reactions to weaknesses. Please correct any misunderstandings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CeOb112Yj", "forum": "EOTCSANb3l", "replyto": "EOTCSANb3l", "signatures": ["ICLR.cc/2026/Conference/Submission4285/Reviewer_fppU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4285/Reviewer_fppU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4285/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932078515, "cdate": 1762932078515, "tmdate": 1763112094576, "mdate": 1763112094576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}