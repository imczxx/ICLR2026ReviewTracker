{"id": "KVQJpmCYDn", "number": 22636, "cdate": 1758333803898, "mdate": 1759896855634, "content": {"title": "POLICY DEGENERACY IN DEEP REINFORCEMENT LEARNING FOR RECOMMENDATIONS: AN EMPIRICAL STUDY", "abstract": "Deep Reinforcement Learning (RL) offers a promising framework for learning\nadaptive policies in recommender systems, particularly for the cold-start prob-\nlem where balancing precision and discovery is crucial. In this work, we provide\na transparent and reproducible benchmark to investigate this challenge, training\na highly-optimized Deep Q-Network (DQN) agent within a high-fidelity offline\nsimulation to learn a dynamic recommendation policy. Contrary to expectations,\nour final evaluation reveals that the trained agent’s performance is at a statistical\ntie with a simple, static heuristic baseline across a suite of key metrics, including\ncumulative reward and NDCG@10. However, we show that this statistical parity\nin outcomes masks a fundamental divergence in behavior: the heuristic employs\na conservative, exploitation-heavy strategy, while the RL agent learns a radically\ndifferent and more exploratory policy. We argue that this result is not a failure of\nthe agent, but rather a crucial insight into the limitations of offline evaluation. The\nfinding provides powerful, empirical evidence that the simulation environment it-\nself can create a ”performance ceiling,” lacking the fidelity to distinguish between\na good policy and a potentially great one. Our work thus serves as a crucial bench-\nmark and cautionary tale, signaling an urgent need for the community to develop\nricher offline evaluation environments or prioritize hybrid online-offline methods\nto bridge the gap between simulation and real-world impact.", "tldr": "A tuned DQN ties a simple heuristic in a cleaned offline recommender simulator, revealing a performance ceiling imposed by the evaluation substrate.", "keywords": ["reinforcement learning; recommender systems; offline evaluation; simulation; DQN; reproducibility"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c4e3362ac09af34eabf52b17ec9551c3691b8c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript aims to address the precision-exploration trade-off in recommendation cold-start problems by replacing the static heuristic policy of the MARS system with a DQN agent. After correcting data leakage issues in the initial shaped reward design, it conducts offline simulation experiments. Results show the DQN agent exhibits statistically indistinguishable performance from the MARS heuristic but adopts a more exploratory strategy. Notably, the study relies heavily on a MARS-related literature (Thakkar, 2026) labeled as a WSDM'26 conference paper as its core baseline."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The experimental design has basic rigor, such as identifying and rectifying data leakage in the shaped reward function. It also provides partial reproducible details, including DQN hyperparameters (e.g., learning rate 0.0001, gamma 0.98) from grid search."}, "weaknesses": {"value": "1. The MARS literature (Thakkar, 2026) cited as the core baseline is labeled WSDM'26, but WSDM'26 remains in review—no formal acceptance/publication proof exists, invalidating its use as \"accepted-in-press.\"\n2. Exploration relies on Sentence-BERT-based semantic similarity, limiting it to user-preference-related content. Practical exploration requires covering non-semantically similar new interests, reducing real-world applicability.\n3. The study fails to cite KuaiSim[1]. KuaiSim is a foundational benchmark for offline recommendation simulations, and omitting it breaks connections to existing academic context, leaving the study’s simulation design ungrounded in prior advances.\n\n[1] KuaiSim: A comprehensive simulator for recommender systems. NeurIPS'23"}, "questions": {"value": "1. Can official proof for the MARS literature (Thakkar, 2026) from WSDM'26 be provided?\n2. Why is KuaiSim[1]—a foundational work in offline recommendation simulation—not cited, and how does this omission affect the study’s alignment with prior research?\n\n[1] KuaiSim: A comprehensive simulator for recommender systems. NeurIPS'23"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5hqtIFmWsl", "forum": "KVQJpmCYDn", "replyto": "KVQJpmCYDn", "signatures": ["ICLR.cc/2026/Conference/Submission22636/Reviewer_YovQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22636/Reviewer_YovQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760454670517, "cdate": 1760454670517, "tmdate": 1762946711031, "mdate": 1762946711031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates deep reinforcement learning (RL) for recommender systems, focusing on whether a DQN agent can outperform a heuristic baseline in an offline cold-start simulation. The authors report that the RL agent achieves statistically indistinguishable performance from a simple heuristic but exhibits qualitatively different behavior (more exploratory vs. conservative)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Good writing quality and clear structure.\nTransparent reporting of invalid experiments and hyperparameter tuning.\nHonest recognition of reproducibility and evaluation challenges in RL-based recommenders.\nInclusion of behavioral policy analysis beyond scalar metrics is thoughtful."}, "weaknesses": {"value": "Demonstrating that RL ties heuristics in a constrained offline simulation is unsurprising and provides limited new understanding. The claimed “performance ceiling” is an expected artifact of restricted data and evaluation bias.\n\nThe binary action and reward space are overly reductive and cannot represent realistic recommender dynamics. This simplicity undermines the claimed insight into reinforcement learning behavior.\n\nThe experiments rely on one dataset and lack ablations on architecture, reward shaping, or OPE techniques. The absence of comparisons with modern offline RL methods limits credibility.\n\nThe proposed “benchmark” lacks diversity and realism, preventing it from being a useful tool for broader community evaluation. The simulation’s simplicity contradicts its framing as a “high-fidelity” environment."}, "questions": {"value": "please refer to weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tgpSCWy5xr", "forum": "KVQJpmCYDn", "replyto": "KVQJpmCYDn", "signatures": ["ICLR.cc/2026/Conference/Submission22636/Reviewer_tkTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22636/Reviewer_tkTF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760577607706, "cdate": 1760577607706, "tmdate": 1762942313174, "mdate": 1762942313174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\n\nIn this work, the authors provide a transparent and reproducible benchmark to investigate this challenge, training a highly-optimized Deep Q-Network (DQN) agent within a high-fidelity offline simulation to learn a dynamic recommendation policy. Contrary to expectations,\ntheir final evaluation reveals that the trained agent’s performance is at a statistical tie with a simple, static heuristic baseline across a suite of key metrics, including cumulative reward and NDCG@10. However, they show that this statistical parity in outcomes masks a fundamental divergence in behavior: the heuristic employs a conservative, exploitation-heavy strategy, while the RL agent learns a radically\ndifferent and more exploratory policy. They argue that this result is not a failure of the agent, but rather a crucial insight into the limitations of offline evaluation. The finding provides powerful, empirical evidence that the simulation environment itself can create a ”performance ceiling,” lacking the fidelity to distinguish between a good policy and a potentially great one. Their work thus serves as a crucial benchmark and cautionary tale, signaling an urgent need for the community to develop richer offline evaluation environments or prioritize hybrid online-offline methods to bridge the gap between simulation and real-world impact.\n\nContribution:\n* It provides a transparent and reproducible benchmark for studying RL in recommender systems, including a validated offline simulation environment.\n\n* It delivers a powerful, empirical demonstration that statistical parity in competing heuristic and RL models in traditional RS metrics can mask their fundamental, qualitative differences in learned policies.\n\n* It presents a rigorous case study of a \"performance ceiling\" in offline RL evaluation, arguing convincingly that the simulation's fidelity of the benchmark could potentially hinder the act of distinguishing between a good policy and a potentially great one, thus creating the need of a richer offline evaluation environments construction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological Rigor and Transparency: The paper's greatest strength is its honesty. The authors discovered their initial experiments with a shaped reward were scientifically invalid due to data leakage (\"reward contamination\"). Instead of hiding this, they document it (Appendix A.1) and detail their pivot to a new, valid simulation with a sparse reward. This transparency builds significant trust and strengthens their final conclusions.\n\n2. Clear and Interpretable Problem Formulation: The decision to simplify the action space to a binary choice ('Precision' vs. 'Discovery')  is highly effective. While an abstraction, it perfectly isolates the high-level strategic trade-off at the heart of the cold-start problem. This simplification is what enables the \"smoking gun\" behavioral analysis (Figure 3)  that reveals the core insight of the paper.\n\n3. Insightful Analysis: The paper does not stop at the \"disappointing\" result (DQN = Heuristic). It digs deeper to find the reason, using the behavioral analysis to uncover the divergent policies. The resulting \"performance ceiling\" thesis is a valuable and non-obvious insight for the OPE and RL-for-RecSys communities.\n\n4. Strong Baselines: The comparison is not just against a strawman. The heuristic baseline is the incumbent, production-style rule from prior work, and the authors also include standard baselines like Popularity and Item-kNN. The DQN agent was also rigorously tuned via a grid search to ensure it was a strong competitor."}, "weaknesses": {"value": "1. Simplified Action Space: The paper's main strength is also its primary weakness, which the authors acknowledge. In a real-world system, an agent would not choose either precision or discovery; The binary-choice setup cannot explore this more realistic and nuanced action space, more importantly, we don't know if such discovered insights still make the rules in the more diverse action space.\n\n2. Simulation fidelity from static offline dataset: The simulation is built from a static, historical dataset (MovieLens). A \"hit\" is defined only as matching a movie the user actually did positively rate in the test set. This design inherently and completely excludes the possibility of rewarding new discoveries, making the \"performance ceiling\" an almost unavoidable artifact of the experimental design itself."}, "questions": {"value": "Several simple but important questions:\n1. The author uses tuned DQN as main operating model for the analysis. However, since DQN is operating on the offline simulation and suffers from low return metric values because of its exploratory behaviors which are actually great, why not try some offline learning methods instead of directly jumping into improving the benchmark?\n\n2. Since the authors claim richer offline simulation or mix of online-offline simulaiton helps on avoiding such incorrect masking between reported metric performance and actural behaviors among baselines, why not compare some simulations (for example, those traditional old offline simulation) to their newly proposed richer simulation or online mixed ones to more soundly demonstrate their conclusions?\n\n3. Authors should compare with more exploratory, advanced, tuned RL baselines such as SAC, PPO, DPO to demonstrarte their claims. Since the time limit, I understand it's not reasonable to compare everything deeply in one paper, just for future suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aGImbNOx0r", "forum": "KVQJpmCYDn", "replyto": "KVQJpmCYDn", "signatures": ["ICLR.cc/2026/Conference/Submission22636/Reviewer_iBgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22636/Reviewer_iBgK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790309167, "cdate": 1761790309167, "tmdate": 1762942312930, "mdate": 1762942312930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is about reinforcement learning environment for recommender systems. \nIt is a purely experimental study that compares the performance of DQN with simpler baselines. \nThe authors present this work as a cautionary tale by providing interpretation on the experimental results."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The question of how to test recsys method and how to extrapolate simulations results to live outcomes is a very important topic."}, "weaknesses": {"value": "The paper is not in a shape that would fit a conference like ICLR: \n* 6 elements in the bibliography, for a paper at the intersection of two enormous fields (RL, recsys)\n* no modeling, no theoretical contribution\n* experiments are done on a small scope \n-> need to test on more environments, more baselines, more variant\n* the claim itself is too vague and not sufficiently backed\n* the tone is not adapted for the audience, the beginning of the paper sounds more like a lab journal\n* the content itself lack clarity"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTbQQi611G", "forum": "KVQJpmCYDn", "replyto": "KVQJpmCYDn", "signatures": ["ICLR.cc/2026/Conference/Submission22636/Reviewer_iwkB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22636/Reviewer_iwkB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986190367, "cdate": 1761986190367, "tmdate": 1762942312543, "mdate": 1762942312543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}