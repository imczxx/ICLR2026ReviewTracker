{"id": "OOiKGlYtQZ", "number": 11462, "cdate": 1758199667787, "mdate": 1759897573950, "content": {"title": "Cross-Architecture Knowledge Distillation via Information Alignment", "abstract": "Transformer architectures have demonstrated remarkable success in capturing long-range dependencies and global contextual information, whereas Convolutional Neural Networks (CNNs) remain dominant in many industrial applications due to their efficiency and strong local feature modeling. Bridging the complementary strengths of these architectures, Cross-Architecture Knowledge Distillation (CAKD) has emerged as a promising approach to transfer global knowledge from Transformers to CNNs. However, existing methods either rely on generic distillation strategies that fail to address inductive bias discrepancies, or reduce informative features to logits, which limits generalization across tasks. To overcome these issues, we propose a novel feature-based framework that aligns representations from both structural and semantic perspectives. Structurally, we refine a global information supplement module to extract residual cues through global-local comparison, facilitating more compatible feature transfer. Semantically, we apply the $\\ell_{1}$-regularization to encourage sparse and meaningful global compensation patterns, mimicking Transformer's attention outputs. Extensive experiments on image classification and instance segmentation benchmarks demonstrate that our method effectively mitigates the feature misalignment between Transformers and CNNs, yielding consistent improvements over state-of-the-art works, with up to 2.7\\% gains on CIFAR-100 and 0.9\\% on ImageNet-1K, respectively.", "tldr": "A novel feature-based knowledge distillation method for transferring knowledge from Transformers to CNNs.", "keywords": ["cross-architecture", "knowledge distillation", "feature alignment"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd62a97f1f712f69b2f3ce2f474a0b9ec56e76c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a feature-based knowledge distillation method to transfer knowledge from Transformer-based models to CNN-based models. It consists of two components: (1) a global information supplement module is introduced to enhance global information (in CNN knowledge) for heterogeneous distillation; and (2) an additional sparsity term is added to the constructed attention map to mimic the attention map of Transformer-based models. Experiments on image and instance segmentation tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well-written\n- Experiments on multiple tasks are conducted.\n- The method part has a lot of information."}, "weaknesses": {"value": "- The current proposed scheme only leverages the Transformer-based teachers to supervise CNN-based students. Can the proposed method be used to transfer knowledge from CNN-based models to Transformer-based ones, similar to the referred methods OFA and TAS?\n\n- Compared with distilling knowledge in the logit space (e.g., OFA), does the proposed method require more memory and computational overhead in training phase? Additionally, would integrating the proposed method with logit-based KD (e.g., OFA) result in better outcomes?\n\n- In Eq. (3), there is no $\\odot$. Moreover, $ F_{sl}^T $ should be ${ F_{sl}^i}^{T} $. In Eq. (5), $ F_{sl}^i \\in {{\\mathbb{R}}^{HW \\times C}} $ and $ F_{t}^i \\in {{\\mathbb{R}}^{L \\times C}} $, leading to a dimension mismatch.\n\n- In Table 4, to demonstrate the effectiveness of the added $F_{sg}$ (Eq. (3)), the results of 'KD' and 'KD+GIS' should be compared.\n\n- More recent studies should be referred, such as [1,2,3,4].\n\n    [1] Liu Y, Cao J, Li B, et al. Cross-architecture knowledge distillation[C]. ACCV 2022. \\\n    [2] Zhang W, Liu Y, Ran W, et al. Cross-Architecture Distillation Made Simple with Redundancy Suppression[C]. ICCV 2025. \\\n    [3] Xu L, Liu K, Liu J, et al. Local Dense Logit Relations for Enhanced Knowledge Distillation[C]. ICCV 2025. \\\n    [4] Lin J H, Yao Y, Hsu C F, et al. Perspective-Aware Teaching: Adapting Knowledge for Heterogeneous Distillation[C]. ICCV 2025."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LC9w7XofgU", "forum": "OOiKGlYtQZ", "replyto": "OOiKGlYtQZ", "signatures": ["ICLR.cc/2026/Conference/Submission11462/Reviewer_7XXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11462/Reviewer_7XXQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878432173, "cdate": 1761878432173, "tmdate": 1762922571377, "mdate": 1762922571377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the cross-architecture knowledge distillation (CAKD) setting, where a Transformer teacher and a CNN student exhibit a “global vs. local” inductive-bias mismatch, and proposes a feature-level distillation framework, The core idea is to insert a Global Information Supplement (GIS) module on the student side: it aggregates a “global compensation” from the student’s local features via an attention-style mechanism, fuses this with the original features, and then aligns the result to the teacher’s intermediate features using MSE. In parallel, an ℓ1 sparsity regularization is applied to the aggregation weights to make them resemble the sparse pattern commonly observed in Transformer attention. The authors further find that no explicit positional encoding is required to achieve better alignment. Experiments on CIFAR-100 and ImageNet-1K and COCO show consistent gains over generic KD and several CAKD baselines, and ablations plus visualizations (t-SNE, attention maps) validate each component. The added modules and losses are training-only, so inference incurs no extra overhead, yielding good practical utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed method is simple and elegant, but significantly effective.\n2.The observation that does not require explicit PE is enlightening."}, "weaknesses": {"value": "1.Some expressions in the article require careful consideration. For example, 133-134\"Purely feature-based approaches remain largely unexplored\". The following articles all conducted explorations from the perspective of features：\"Cross-Architecture Distillation Made Simple with Redundancy Suppression\",\"Cross-Architecture Knowledge Distillation\" and so on. Also,138-139\"In this paper, we attribute the bottleneck in cross-architecture knowledge transfer to the overlooking of learning about global knowledge in Transformer-like teacher models.\" is quite ambiguous, you may point out it's student’s insufficient learning of the teacher’s global knowledge.\n2.During training, the method introduces the GIS module and a sparsity regularizer and requires access to the teacher’s intermediate features. The training resource threshold and memory overhead remain unquantified."}, "questions": {"value": "1.The comparison methods mentioned in the article only cover up to 2024. Have you compared the methods for the first half of 2025?\n2.Is using only the penultimate layer optimal? Have you explored the benefits and costs of multi-layer or hierarchical distillation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EpqfRAhTtf", "forum": "OOiKGlYtQZ", "replyto": "OOiKGlYtQZ", "signatures": ["ICLR.cc/2026/Conference/Submission11462/Reviewer_hshi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11462/Reviewer_hshi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918153985, "cdate": 1761918153985, "tmdate": 1762922570923, "mdate": 1762922570923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a knowledge distillation approach to distill a Transformer model into a CNN model. The authors first present the related works of vision transofrmer and knowledge disttilation. Then, they propose their method, including a global information supplement method and a semantic information alignment. In addition, they analyze the loss function of the global information supplement method. Finally, they conducted an experimentation to show the advantages of the proposed approach. The idea of the global information method and semantic information alignment is interesting. In addition, the experimental results demonstrate the advantages of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach, including the global information supplement method and the semantic information alignment is interesting and appears to be effective in the knowledge distillation.\n2. The experimentation results show clear advantages of the proposed approach.\n3. The analysis of the global information provides additional breakdown of the global information supplement method."}, "weaknesses": {"value": "1. The experimental results exploit two datasets, i.e., CIFAR-100 and ImageNet. More datasets can be exploited to show the generalization of the proposed approach.\n2. The structure of the paper can be improved, e.g., the analysis section can be moved before semantic information alignment.\n3. Theoretical analysis of the proposed approach can be provided to show the convergence.\n4. The \"\"\" in \"”PE”\" can be adjusted."}, "questions": {"value": "1. How to adjust the hyperparameters, e.g., β.\n2. Are W and W1 trained during the knowledge distillation process?\n3. Is there any theoretical analysis for the convergence of the proposed approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VkMNKxLINw", "forum": "OOiKGlYtQZ", "replyto": "OOiKGlYtQZ", "signatures": ["ICLR.cc/2026/Conference/Submission11462/Reviewer_ZMxK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11462/Reviewer_ZMxK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977198918, "cdate": 1761977198918, "tmdate": 1762922570491, "mdate": 1762922570491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### **Paper Summary and Key Contributions**  \nThis paper addresses the challenge of **Cross-Architecture Knowledge Distillation (CAKD)**, aiming to transfer global contextual knowledge from Transformers (strong at long-range dependency modeling but computationally heavy) to CNNs (efficient with excellent local feature modeling, dominant in industrial applications). Existing CAKD methods either fail to resolve inductive bias discrepancies between the two architectures or reduce informative features to logits (limiting generalization across tasks) .  To overcome these issues, the authors propose a **feature-based CAKD framework**. It's has three key contributions: 1. Proposes the **GIS module** to capture global supplementary information beyond local CNN features, helping CNN students better absorb knowledge from Transformer teachers .  2. Applies **ℓ₁-regularization** to supplementary components to mimic Transformer attention characteristics, realizing semantic information alignment .  3. Achieves state-of-the-art performance on both image classification and instance segmentation tasks, outperforming existing advanced CAKD methods consistently ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **List of Strengths**  \n1. **Targeted Solution to Existing CAKD Limitations**  \n   The paper directly addresses the two core drawbacks of current Cross-Architecture Knowledge Distillation (CAKD) methods: failure to resolve inductive bias discrepancies between Transformers (global context-focused) and CNNs (local feature-focused), and over-reliance on logits that weakens downstream task generalization. By proposing a feature-based framework instead of generic or logits-centric strategies, it effectively mitigates these issues and bridges the knowledge transfer gap between the two architectures .  \n\n2. **Dual-Perspective Feature Alignment Design**  \n   The framework achieves thorough alignment of Transformer-CNN representations through two complementary components:  \n   - **Structural alignment**: The Global Information Supplement (GIS) module (inspired by non-local modules) captures residual cues via global-local comparison, enabling compatible feature transfer for CNNs to absorb Transformer global knowledge .  \n   - **Semantic alignment**: ℓ₁-regularization enforces sparse global compensation patterns, mimicking the intrinsic attention characteristics of Transformers and ensuring semantic consistency . This dual design ensures both structural compatibility and semantic similarity, rather than one-dimensional optimization.  \n\n3. **Strong Practical Applicability**  \n   The method aligns with industrial needs: it enhances CNN performance (the dominant architecture in industrial applications) using Transformer knowledge, without increasing CNN computational complexity. Additionally, it avoids redundant operations (e.g., proving explicit position encoding is unnecessary for CNNs, as their features already contain spatial information), further optimizing efficiency for real-world deployment ."}, "weaknesses": {"value": "### **Weaknesses and Restrictions of the Proposed Method**  \n1. **Limited Coverage of Teacher-Student Architecture Pairs**  \n   The method’s validation is restricted to a narrow range of teacher and student models, lacking exploration of more diverse architecture combinations. For teacher models, only small/medium-sized Transformers (Swin-T, ViT-S, DeiT-T) are used, while larger Transformer variants (e.g., ViT-B/16, Swin-B) or domain-specific Transformers (e.g., medical image-focused Transformer) are not tested. For student models, only lightweight CNNs (ResNet-18, MobileNet-V2, ResNet-50) are evaluated, excluding more complex CNN architectures (e.g., ResNeXt, EfficientNet, or industrial-specific defect detection CNNs). This limits the generalizability of the method to scenarios with larger teacher models or non-lightweight CNN students .  \n\n2. **Unaddressed Computational Overhead of the GIS Module**  \n   The proposed Global Information Supplement (GIS) module is inspired by non-local modules, which are inherently associated with high computational complexity . However, the paper does not provide quantitative analysis of the GIS module’s computational cost compared to baseline CAKD methods or vanilla CNNs. For industrial applications sensitive to real-time performance, unquantified computational overhead may become a critical barrier to deployment .  \n\n3. **Lack of Analysis on Hyperparameter Sensitivity**  \n   The method relies on key hyper-parameters (e.g., the weight β of the distillation loss, the strength of ℓ₁-regularization, and parameters of the GIS module’s linear transformation W₁) to achieve optimal performance. However, the paper does not conduct a systematic sensitivity analysis. This increases the practical application cost, as users may need extensive tuning to adapt the method to their specific scenarios .  \n\n4. **Restricted Exploration of Distillation Feature Layers**  \n   The paper specifies that teacher and student features are extracted from the **penultimate layer** for alignment, but does not explore the impact of distilling features from other layers (e.g., shallow layers for low-level texture information, middle layers for semantic cues). It remains unclear whether the method’s effectiveness is limited to the penultimate layer, or if multi-layer distillation (combining features from different layers) could further improve performance. This one-dimensional layer selection restricts the method’s flexibility in adapting to different architecture characteristics ."}, "questions": {"value": "### **Questions and Suggestions for  Authors**  \n\n1. **How Does the Method Perform Under Small-Sample or Imbalanced Data Scenarios?**  \nAll experiments use well-annotated, large-scale datasets with no testing on small-sample or class-imbalanced data. Since industrial scenarios often lack sufficient labeled data, it is unclear whether the method’s feature distillation can still bridge Transformer-CNN gaps when training data is limited.  \n\n2. **Can the Method Be Combined With Data-Free Knowledge Distillation?**  \nThe paper focuses on data-dependent CAKD but does not discuss compatibility with data-free distillation. Given that privacy-preserving AI is critical in industries like healthcare, it is unknown whether the GIS module and ℓ₁-regularization can be adapted to data-free settings.  \n\n3. **Add Ablation Studies on Feature Layer Selection**  \nThe paper uses only the penultimate layer for feature alignment . Conduct ablation studies on distilling features from shallow (low-level texture) or middle (semantic) layers to determine if multi-layer distillation (combining penultimate + middle layers) can further improve performance. This would clarify the optimal layer choice for different architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DryEVWzKx0", "forum": "OOiKGlYtQZ", "replyto": "OOiKGlYtQZ", "signatures": ["ICLR.cc/2026/Conference/Submission11462/Reviewer_i5NX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11462/Reviewer_i5NX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11462/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984290915, "cdate": 1761984290915, "tmdate": 1762922570002, "mdate": 1762922570002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}