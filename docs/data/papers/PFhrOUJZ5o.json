{"id": "PFhrOUJZ5o", "number": 5794, "cdate": 1757935547920, "mdate": 1759897952793, "content": {"title": "LAION-Comp: Unlocking Controllable and Compositional Generation with Structural Annotations", "abstract": "Despite their success in generating high-quality images, text-to-image (T2I) models struggle to generate compositional scenes with multiple objects and their intricate relationships. We attribute this issue to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To resolve this, we construct LAION-Comp, a large-scale dataset of 540K+ aesthetic images structurally annotated with detailed scene graphs explicitly encoding multiple objects, corresponding attributes, and intricate relations. The annotation pipeline employs a large vision-language model followed by partial human verification. Using LAION-Comp, we train 4 baseline models on diffusion and flow matching backbones augmented with a designed scene graph encoder. For proper evaluation, we introduce CompSGen Bench, a benchmark with 20,838 testing samples designed to systematically evaluate complex compositions. Experiments show that the 4 models trained on LAION-Comp outperform their original prompt-only counterparts and advanced scene-graph-based methods on both our new and existing compositional benchmarks. Furthermore, the learned structural conditioning naturally enables fine-grained, object-level image editing, demonstrating its potential as an effective editing interface. Our work validates the advantages of explicit structural annotation and contributes the community with a foundational resource to advance controllable and compositional image synthesis.", "tldr": "", "keywords": ["dataset", "compositional image generation", "diffusion model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/089d1a537b3175e0b7f11c66b639496d699240e8.pdf", "supplementary_material": "/attachment/8eb23f98789ad293291fe5ef349c2c671f0d7c18.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors construct a large-scale scene-graph (SG) dataset (LAION-Comp) with annotations of high-aesthetics LAION images via a MLLM pipeline with partial human verification. And the authors use a GNN-based SG encoder to embed scene-graph annotation so that inject SG embeddings into diffusion and flow-matching backbones for controllable image generation. Compared with original text counterparts, the proposed richer scene-graph annotation leads to more precise annotation-following image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Large-Scale Scene-graph Dataset. In this paper, the authors construct a dataset pipeline with MLLM (GPT-4) to produce a large-scale txt-scene-graph dataset for compositional generation, which can contribute to the incremental investigation of scene-graph-based methods in the community.   \n\n2. Good Performance. The authors demonstrate that the mainstream image generation backbones fine-tuned on the proposed dataset can achieve superior performance in complex scene synthesis."}, "weaknesses": {"value": "1. Concerns about Overclaim. The claim of “our work represents a pioneering effort in annotating complexity on existing image datasets…” is not proper and misleading. It is obvious that  COCO Stuff and Visual Genome are earlier works on scene-graph annotation. \n\n2. Limited Annotation Diversity Improvement. Except for larger scale and richer per-node/edge details, the dataset’s annotation scheme lacks variety: it sticks to the standard scene-graph paradigm seen before. Consequently, there is limited conceptual insight here—the advances are incremental in data scope rather than introducing a more expressive or innovative annotation form.\n\n3. Underdeveloped Benchmark Justification. For the proposed CompSGen Bench, the paper clearly defines sample selection and reports FID/CLIP alongside SG-IoU/Entity-IoU/Relation-IoU, but it does not demonstrate why this benchmark evaluates better than existing ones—there is no systematic comparison on human correlation, robustness to parser/threshold choices, or justification of the chosen complexity threshold. This makes the benchmark feel like a reasonable task specification rather than a fully argued contribution with demonstrated measurement advantages. \n\n4. Lack further exploration of the scene-graph relation among objects. Although the dataset is dominated by non-spatial relations, the paper reports only aggregate metrics. Without relation-type–stratified analyses or stress tests, it remains unclear whether spatial or non-spatial relations are better controlled, and which SG components actually drive controllability. \n\n5. Lack of multiple Object Editing Results. With richer scene-graph annotations and data, robustness to multiple object editing about relation and attribute should be verified to further evaluate the enhanced large-scale dataset."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JvWj9Hz10E", "forum": "PFhrOUJZ5o", "replyto": "PFhrOUJZ5o", "signatures": ["ICLR.cc/2026/Conference/Submission5794/Reviewer_iUAg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5794/Reviewer_iUAg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208372983, "cdate": 1761208372983, "tmdate": 1762918265840, "mdate": 1762918265840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LAION-Comp, a large-scale dataset of 540K LAION-Aesthetics images annotated with detailed scene graphs (objects, attributes, relations) produced by GPT-4o and partially human-verified. The authors design a graph neural network (GNN) scene-graph encoder that conditions diffusion and flow-matching text-to-image backbones (SDXL, SD3.5, FLUX) to produce SG-conditioned variants (e.g., SDXL-SG, FLUX-SG). They also propose CompSGen Bench, a 20,838-sample test split focusing on complex scenes (>4 relations), and define evaluation metrics (SG-IoU, Entity-IoU, Relation-IoU) as well as annotation quality metrics (SG/Entity/Relation-IoU+). Experiments show consistent gains over text-only and prior SG2IM baselines on compositional accuracy with minimal cost increase, and better performance when trained on LAION-Comp vs COCO/VG. The paper further presents a training-free, SG-consistent RF inversion pipeline for object-level image editing and a world-knowledge-aware agent to parse user edits into graph operations. Human verification reports high annotation accuracy (objects 98.8%, attributes 97.5%, relations 95.7%). Ablations show monotonic improvements with more LAION-Comp data. The authors acknowledge limitations (occasional VLM hallucinations, narrower object-type vocabulary vs free-form LAION text, no bounding boxes) and discuss broader impact."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Scale and execution: 540K SG-image pairs with clear annotation guidelines and partial human verification; lightweight SG encoder integrates cleanly into strong backbones.\n\n2. Broad evaluation: dedicated complex-scene benchmark, multiple metrics, consistent gains across SDXL/SD3.5/FLUX; useful demonstration of SG-based editing.\n\n3. Practicality: Minimal overhead, reproducible architecture, data-centric positioning is timely."}, "weaknesses": {"value": "1. Limited novelty: The data-centric + SG-conditioned generation paradigm (SG encoder + GNN + conditioning into existing backbones) closely follows prior art; this reads as a scaled re-implementation and integration rather than a conceptual advance.\n\n2. Insufficient causal attribution: Missing head-to-head comparisons against the strongest recent SG2IM/multi-instance control methods and alternative annotation sources under matched protocols; unclear how much improvement stems from more/cleaner data versus method.\n\n3. Annotation bias analysis: Human verification reports overall accuracy but lacks distributional/error-mode breakdown (e.g., rare relations, small/occluded objects, fine-grained attributes, non-spatial/functional relations). Hallucination/failure analysis is anecdotal.\n\n4. Text-to-SG confound: For text benchmarks, performance depends on an additional conversion pipeline; its information loss is not adequately quantified per relation/attribute/quantity type, making attribution ambiguous.\n\n5. Expressiveness constraints: 1,429 object types, limited coverage of style/abstract cues; SG-only conditioning narrows applicability relative to text prompts for long-tail and stylistic control.\n\n6. Benchmark provenance and fairness: CompSGen derived from the authors’ test split (>4 relations); more analysis on robustness (different resolutions/samplers, human preference studies) would strengthen claims."}, "questions": {"value": "1. Please report accuracy by relation type (spatial vs. non-spatial), object scale/occlusion, and category frequency buckets. Are there systematic biases (e.g., consistent errors on “wear/hold/ride” or fine-grained attributes)?\n\n2. Causal attribution: Under identical images and training budgets, compare SG sources (e.g., VG/COCO SGG predictions, human-verified subset, different VLM annotators) and recent strongest SG2IM/multi-instance baselines. How much gain is due to data vs. method?\n\n3. Have you tried SG + original text/style conditioning to recover long-tail and stylistic control? Any evidence it improves non-spatial semantics or user preference?\n\n4. On T2I-CompBench, quantify the gap between human-extracted SGs and automatic Text-to-SG. Which information is most frequently lost (attributes, non-spatial relations, counts)? Provide ablations to bound the end-to-end impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "73EIyMKPQq", "forum": "PFhrOUJZ5o", "replyto": "PFhrOUJZ5o", "signatures": ["ICLR.cc/2026/Conference/Submission5794/Reviewer_VzmP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5794/Reviewer_VzmP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810760578, "cdate": 1761810760578, "tmdate": 1762918265511, "mdate": 1762918265511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs **LAION-Comp**, a large-scale dataset of **540K+** aesthetic images with **explicit scene-graph (SG)** annotations (objects, attributes, relations) generated by an MLLM with partial human verification. Based on this dataset, the authors train **four** T2I baselines **augmented with a scene-graph encoder**, and introduce **CompSGen Bench** to evaluate compositional complexity. Models trained with structural conditioning outperform prompt-only counterparts and prior SG-based methods on both the new and existing compositional benchmarks, and the learned structural interface naturally supports **fine-grained, object-level editing**."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. A large, well-structured SG corpus (~540K) with clear schema and audited quality\n2. Shows consistent gains on object/relation accuracy with solid ablations and human checks, while adding minimal compute/latency."}, "weaknesses": {"value": "1. Lacks OOD validation: no clear zero-shot tests on unseen (subject–relation–object) combinations or non-aesthetic domains to show generalization.\n2. Lacks a retrained T2I baseline built from the same structured inputs, so the gains from clearer prompts vs. graph conditioning aren’t disentangled.\n3. Lacks locality-focused editing evaluation"}, "questions": {"value": "1. Add zero-shot and cross-domain tests to demonstrate generalization.\n2. Fine-tune a T2I backbone with structure-derived prompts and compare against SG-conditioned to disentangle gains.\n3. Evaluate editing locality and multi-step stability to show changes stay confined.\n4. Reformat result tables to separate T2I, SG2IM, and Proposed, and clearly highlight your method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bJ891j2Ytx", "forum": "PFhrOUJZ5o", "replyto": "PFhrOUJZ5o", "signatures": ["ICLR.cc/2026/Conference/Submission5794/Reviewer_gcTU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5794/Reviewer_gcTU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973449054, "cdate": 1761973449054, "tmdate": 1762918265094, "mdate": 1762918265094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a limitation in text-to-image generation models: their inability to generate complex compositional scenes with multiple objects and intricate inter-object relationships. The authors identify this as fundamentally a data problem rather than an architectural issue. They propose that large-scale, high-quality structural annotations are crucial for advancing controllable and compositional image synthesis, providing a dataset and effective baseline models for the community."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Dataset: Large-scale (540K) scene graph dataset for aesthetic images, helping bridge the gap between small-scale structured datasets (COCO, VG) and large unstructured text datasets (LAION). The paper uses an annotation pipeline using GPT-4o with constraints (unique IDs, abstract attributes, concrete relations) for consistency and quality. There are 77.48% non-spatial relations vs. Visual Genome's 41.98%, capturing more complex interactions beyond spatial positioning. Comparative analysis with existing datasets shows superior annotation quality (Table 1: higher accuracies with longer, richer annotations).\n\n2. Benchmark: CompSGen Bench is the first benchmark specifically for complex scene generation (20,838 samples with >4 relations). The paper proposes new metrics (SG-IoU+, Entity-IoU+, Relation-IoU+) for evaluating annotation accuracy.\n\n3. Evals: The paper does multiple rounds of verification: human validation on 1,000 samples (error rates: 1.16% objects, 2.5% attributes, 4.27% relations - all below 5% threshold). User study with 20 participants across 100 samples (63% preference for SG-generated images). The authors also run several ablation studies (10%, 20%, 50%, 100% data proportions), evaluating across multiple benchmarks (CompSGen Bench, T2I-CompBench, COCO-Stuff, Visual Genome).\n\n4. Four model variants were tested (SDXL-SG, SD1.5-SG, SD3.5-SG, FLUX-SG) spanning diffusion and flow matching backbones. These models were compared with fairly strong baselines (SDXL, SGDiff, SG-Adapter). Multiple evaluation metrics were used FID (quality), CLIP (similarity), SG-IoU/Entity-IoU/Relation-IoU (compositional accuracy) and  both image generation and editing tasks evaluated."}, "weaknesses": {"value": "1. Dataset and annotation: Only 300 samples (0.06%) manually verified out of 540,005, which is likely insufficient for establishing dataset quality guarantees. Entire annotation relies on GPT-4o, making the pipeline non-reproducible with exact results and potentially prone to any GPT-4o specific biases.\n\n2. Vocabulary: Object types reduced from 5,811 → 1,429 (75% reduction) by excluding proper nouns. This can sometimes be suboptimal since proper nouns \"Eiffel Tower\", \"Golden Gate Bridge\" can carry important semantic information.\n\n3. Architecture: The choice of GNN design (5-layer, 512/1024 dim) appears arbitrary with no ablation on architecture choices.\n\n4. Baselines: Only comparisons with SGDiff (2022) and SG-Adapter (2024) are reported.  The paper could mention recent methods: R3CD (Liu & Liu 2024), SGG-IG (Wang et al. 2025). Many cited compositional methods (MIGC, MIGC++, Attend-and-Excite, RealCompo, BoxDiff, GLIGEN, Ranni) are also mentioned but never compared quantitatively.\n\n5. Models trained on LAION-Comp only evaluated on LAION-Comp test set (same distribution). There's limited evaluation on COCO/VG for comparison."}, "questions": {"value": "1. The vocabulary reduction (5,811 to 1,429 object types, 75% decrease) seems substantial: what is the theoretical / empirical rationale for completely excluding proper nouns (as opposed to mapping them to categories)? How does this impact generation of culturally/geographically specific scenes?\n\n2.  The GNN architecture (5-layer, 512/1024 dim) appears chosen without ablations. It would help to compare with GNNs with different layers. Also regarding the α scaling factor (Eq. 1, 7, 13),  it would help to investigate sensitivity to α initialization.\n\n3. The SG-IoU/Entity-IoU/Relation-IoU metrics extract scene graphs from generated images using GPT-4o which is same model used for annotation, which can create potential confirmation bias, has this been validated with independent SG extraction methods? Could the authors also report results using a completely different VLM for evaluation?\n\n4. The user study (20 participants, 100 samples) is relatively small, can the paper also report inter-annotator agreement? It would also help to outline participant demographics and any observed effects of the same.\n\n5. Loss analysis: Figure 14 shows failure cases but lacks more thorough analysis, it would help to see a quantitative breakdown of failure types (object misalignment, incorrect shape, wrong attributes, missing objects). What is the accuracy stratified by number of objects, number of relations, and relation complexity?  Also which specific relation types are most challenging (holding, wearing, riding vs. spatial relations)?\n\n6. GPT-4o dependency: Have the authors tested the annotation pipeline with open-source alternatives? What is the annotation agreement rate between GPT-4o and alternative VLMs? If GPT-4o becomes unavailable or changes, how would that affect dataset reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fqBfJjnFRi", "forum": "PFhrOUJZ5o", "replyto": "PFhrOUJZ5o", "signatures": ["ICLR.cc/2026/Conference/Submission5794/Reviewer_DihD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5794/Reviewer_DihD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997521744, "cdate": 1761997521744, "tmdate": 1762918264762, "mdate": 1762918264762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}