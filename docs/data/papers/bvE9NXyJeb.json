{"id": "bvE9NXyJeb", "number": 1740, "cdate": 1756913644961, "mdate": 1759898190703, "content": {"title": "BADI: Black-box and Anytime-valid Dataset Identification for Large Language Models", "abstract": "Large language models (LLMs) are trained on massive, uncurated internet datasets that often include copyrighted material, making training data identification essential for intellectual property protection. *Dataset inference (DI)* addresses this challenge by extracting diverse training membership features for a suspect set, aggregating them, and applying statistical tests to assess if that suspect set contributed to the model’s training. However, current DI methods face two major limitations that hinder their practical deployment. First, they require gray-box access to token probabilities, while state-of-the-art LLM APIs usually return only generated tokens. We address this issue by approximating per-token probabilities from label-only outputs, making *black-box DI* feasible. Second, existing DIs rely on p-value for statistical tests that necessitate a fixed suspect set and a predetermined significance level. This either leads to high computational costs for large suspect sets, especially in the black-box setup, or yields inconclusive results for smaller sets, since adding new suspect data points post-hoc might be necessary to provide strong enough evidence, but it invalidates statistical guarantees based on p-values. To overcome this limitation, we introduce a black-box DI framework based on *e-values* and sequential testing. The e-values offer anytime-valid guarantees and support optional continuation, enabling safe accumulation of evidence, reducing inconclusive outcomes and compute costs. Through these two fundamental advances, our **B**lack-box and **A**nytime-valid **D**ataset **I**dentification (BADI) method enables practical data auditing for LLMs, supporting their trustworthy deployment.", "tldr": "Black-box and Anytime-valid Dataset Inference framework for LLMs that enables practical and statistically robust identification of their training datasets.", "keywords": ["LLM", "Dataset Identification", "Copyright"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b979a10b00c4c025ea99f56dcbe06da79fa1b8d7.pdf", "supplementary_material": "/attachment/4f70e11f6e41359d6b8976c462d67e9942793985.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces BADI, a black box method for dataset identification. The proposed black-box DI framework used e-values for hypothesis testing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-organized.\n- As the authors stated, the testing framework is based on e-values. This could introduce more flexibility in the task."}, "weaknesses": {"value": "- Only the Pythia model family is studied in this work. \n- The rationale of using Eq.(1) for token probabilities needs further justification.\n- Few baselines are studied in the experiment section."}, "questions": {"value": "- In the experiment stage, only the Pythia model family is studied. This is a model released at 04.2023, it is trained on **300B tokens** [1]. Typically, today, LLMs like Meta-Llama-3.1 [2], \"We pre-train Llama 3 on a corpus of about **15T multilingual tokens**, compared to 1.8T tokens for Llama 2.\" The Pythia model only consumes **1/50 tokens** of Meta-Llama-3.1. And the training tokens are always increasing, for example, \"Qwen3 has been pre-trained on **36 trillion tokens**\"[3].\n\n- With the increasing of training tokens, many questions occur. For example, there is a large chance that similar examples to the test one exist, and this will lead to false-positive results. It is a well-known problem of the traditional DI/MIA method [4]. When nearly all web data is used for training, the detection will become hard, and choosing/creating a held-out set is a problem.\n\n- A summary of the above two points, the reviewer believes that we should conduct more analysis with recent models rather than only using the Pythia model. This could give us more information about the proposed method. Can it really deal with modern LLMs training on web-scale data?\n\n- For Eq.(1), it assumes a token-to-token mapping relationship between the generated sentence and ground truth. Will a semantically similar generated sentence get a low score with the ground truth? It may have different word orders, but basically the same meaning. How do we deal with and consider this case?\n\n- In Figure 3, it seems that the reviewer only provides PETAL and Baseline for comparison. How about the performance of other DI/MIA methods? Could the authors provide other metrics like AUC?\n\n[1] https://huggingface.co/EleutherAI/pythia-12b\n\n[2] https://arxiv.org/pdf/2407.21783\n\n[3] https://arxiv.org/pdf/2505.09388\n\n[4] Do Membership Inference Attacks Work on Large Language Models? https://arxiv.org/pdf/2402.07841"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYo63t9DLa", "forum": "bvE9NXyJeb", "replyto": "bvE9NXyJeb", "signatures": ["ICLR.cc/2026/Conference/Submission1740/Reviewer_ck6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1740/Reviewer_ck6T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712175769, "cdate": 1761712175769, "tmdate": 1762915875670, "mdate": 1762915875670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BADI, a new framework for black-box and anytime-valid dataset inference in LLMs. The goal is to determine whether a suspect dataset was used during an LLM’s pretraining, addressing concerns around copyright and data transparency. The method is empirically evaluated on multiple Pythia models (410M, 6.9B, 12B) and diverse Pile subsets. Experiments show that BADI identifies training datasets with higher efficiency (fewer samples required) and lower false-positive rates than baselines (PETAL and a RoBERTa-similarity baseline), offering a practical framework for real-world LLM auditing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper situates itself well within the context of data auditing, copyright compliance, and LLM transparency.\n2. The authors argued that they proposed the first black-box DI framework that avoids reliance on per-token logits.\n3. The step-by-step framework (Figure 1) is clear and pedagogically strong."}, "weaknesses": {"value": "- The paper claims generality to “API-based black-box models,” but does not empirically test BADI on true commercial APIs (GPT, Claude, Gemini). All experiments are performed on open Pythia models (Biderman et al., 2023). A key claim \"BADI enables accurate dataset identification in black-box settings\" is not fully validated since Pythia allows full access.\n- While e-values provide anytime-validity, the choice of betting strategy (λt bounds, ONS parameters) may significantly affect power. No experiments systematically compare BADI’s sequential testing to classical p-value–based tests under equivalent conditions.\n- Some formulae (e.g., kernel MMD payoff mapping) and hyperparameters for the online regressor are only briefly described, lacking mathematical detail or convergence justification. Morover, the description of “STRIP-K% PROB” lacks sensitivity analysis across K values."}, "questions": {"value": "- Have the authors tested BADI on real black-box APIs (e.g., OpenAI, Anthropic, Google)? If not, how confident are they that the token similarity estimation remains meaningful when model outputs are diverse or truncated?\n- How was the sigmoid mapping in Eq. (1) empirically chosen? Is there a theoretical justification for using 2·sim as the scaling factor, and how sensitive is performance to this parameter?\n- The paper adopts an Online Newton Step (ONS) for adaptive staking. How critical is this choice? Would simpler strategies (fixed λ or proportional betting) degrade performance substantially?\n- The paper reports ≈1 % false positive at 5 % significance. Was this averaged over datasets, or verified under a global Type-I control across all tests?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lE7Vi5AvKU", "forum": "bvE9NXyJeb", "replyto": "bvE9NXyJeb", "signatures": ["ICLR.cc/2026/Conference/Submission1740/Reviewer_Xq4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1740/Reviewer_Xq4R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924949488, "cdate": 1761924949488, "tmdate": 1762915875220, "mdate": 1762915875220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for identifying if certain training data has been used in the training of an LLM. They do this also with black box models behind an API by approximating per token probabilities. \n\nThe estimation of token probabilities for black box models is done with a surrogate model and a computation of semantic similarity between the surrogate model's response and the true token predicted by the black box model. These are mapped to token probabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel approach for both estimating token probabilities in black box settings as well as predicting inference attacks"}, "weaknesses": {"value": "Existing LLMs, when they are properly aligned, they are not really trained to generate continuations but to answer with reasoning given a query. In such setting, any inference membership extraction method will be limited. The evaluation is done with the Pythia models which are trained on subsets of the Pile dataset. These models are not aligned by default so the applicability of the method will be limited about this. Please let me know if I misinterpreted this.\n\nThis is a small concern, but I would suggest to define e-values before describing why they are suited to this particular issue or at least refer to the section where this is mentioned."}, "questions": {"value": "Have you tested the estimation of token probabilities with models that you have access to the actual token probabilities?\n\nWould this approach work with models that are fully aligned with RL optimization or similar methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p2rjMX9hJa", "forum": "bvE9NXyJeb", "replyto": "bvE9NXyJeb", "signatures": ["ICLR.cc/2026/Conference/Submission1740/Reviewer_ZAf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1740/Reviewer_ZAf3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937264979, "cdate": 1761937264979, "tmdate": 1762915874456, "mdate": 1762915874456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, this is a good practical work that addresses important issues in dataset inference. While the evaluation could benefit from additional robustness tests, I believe readers of ICLR would find this work useful and interesting. Hence, I believe it should be accepted. \n\n\nThe authors outline current limitations of dataset inference (DI) methods: they require gray-box access to token probabilities, and they rely on p-value statistical tests which incur high computational costs and allow “gaming” significance levels. The authors propose to estimate token probabilities and use “e-values” with sequential testing to overcome these limitations, leading to BADI. This is more practical, as specifying testing dataset size becomes a flexible, sequential process, and one does not require gray-box access to LLMs.\n\nThe general method is to aggregate dataset inference features (largely results of membership inference techniques) for a given dataset, construct an e-value from the features. The features often leverage token-level probabilities – these are estimated using a sigmoid-based calibration method which avoids use of a surrogate model. The semantic similarity of a produced token with a reference token is used as a proxy for the probability of the original token.  Membership inference features are computed from the estimated token probabilities. The membership features are then used to continuously train a scoring model. The scores are passed to a betting model which should accumulate evidence in the case of true positives (true training samples). One may stop the process if sufficient evidence has been gathered to reject the null hypothesis (that the samples were not part of training).\n\nExperiments with Pythia+Pile validate the method presented by the authors. They compare BADI with two baseline black-box methods leveraging RoBERTa scores and PETAL. The experiments would benefit from additional robustness tests (see weaknesses).\n\n\nMinor comments:\nLine 131 consider rephrasing “However, most existing MI methods for LLMs, even the stronger gray-box attacks do not perform better than random guessing.” Perhaps “However, for most existing MI methods for LLMs, even the stronger gray-box attacks do not perform better than random guessing.”\nLine 152: grammar issue “Consider a randomly ordered points” \nLine 160 “BADI rely” -> “BADI relies”"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is strongly motivated in practical scenarios and presents a useful solution to a challenging problem: dataset inference in black-box scenarios with flexible resource expenditure\n\nThe methods of the paper are sound, and the claims are largely substantiated by the experiments with Pythia+Pile."}, "weaknesses": {"value": "The description of estimating token probabilities is not very clear. How is this semantic similarity measured? The “ground truth sequence” appears to be the one you would like to estimate the probability for – it would be helpful to refer to it as that if so.\n\nThere is not a study of robustness to choice of held-out distribution data. What if your trusted hold-out data is polluted with training samples (some percentage) or its distribution does not match that of the training samples? How does this affect evidence growth rate?\n\nI would like to see more comparisons with additional dataset inference (DI) methods, however I believe there could be difficulties in creating apples-to-apples comparisons between p-value methods and e-value methods."}, "questions": {"value": "How sensitive is the method to choice of non-member data? \n\nDo the non-member data need to come from the same distribution as the member data? If so, how would one acquire this data in practical settings? Is it robust to pollution from the training set?\n\nWhat if you do have access to the logits? How does this impact evidence gathering rate compared with probability estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRt4B7bSMi", "forum": "bvE9NXyJeb", "replyto": "bvE9NXyJeb", "signatures": ["ICLR.cc/2026/Conference/Submission1740/Reviewer_roVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1740/Reviewer_roVQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944758437, "cdate": 1761944758437, "tmdate": 1762915874119, "mdate": 1762915874119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}