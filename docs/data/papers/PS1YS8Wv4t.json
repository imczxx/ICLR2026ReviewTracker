{"id": "PS1YS8Wv4t", "number": 8985, "cdate": 1758105908549, "mdate": 1759897749958, "content": {"title": "Learning from the Electronic Structure of Molecules across the Periodic Table", "abstract": "Machine-Learned Interatomic Potentials (MLIPs) require vast amounts of atomic structure data to learn forces and energies, and their performance continues to improve with training set size. Meanwhile, the even greater quantities of accompanying data in the Hamiltonian matrix $\\mathbf{H}$ behind these datasets has so far gone unused for this purpose. Here, we provide a recipe for integrating the orbital interaction data within $\\mathbf{H}$ towards training pipelines for atomic-level properties. We first introduce HELM ('Hamiltonian-trained Electronic-structure Learning for Molecules'), a state-of-the-art Hamiltonian prediction model which bridges the gap between Hamiltonian prediction and universal MLIPs by scaling to $\\mathbf{H}$ of structures with 100+ atoms, high elemental diversity, and large basis sets including diffuse functions. To accompany HELM, we release a curated Hamiltonian matrix dataset, 'OMol\\_CSH\\_58k', with unprecedented elemental diversity (58 elements), molecular size (up to 150 atoms), and basis set (def2-TZVPD). Finally, we introduce 'Hamiltonian pretraining' as a method to extract meaningful descriptors of atomic environments even from a limited number atomic structures, and repurpose this shared embedding space to improve performance on energy-prediction in low-data regimes. Our results highlight the use of electronic interactions as a rich and transferable data source for representing chemical space.", "tldr": "Pretraining on electronic structure helps to learn generalizable features for ML interatomic potentials from limited data.", "keywords": ["Interatomic potentials", "electronic structure", "materials science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5f093fc4c9f700d834fce7bf00c31e81ad30ff8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HELM, a model that predicts Hamiltonian matrices for molecules, enabling the integration of orbital interaction into interatomic potential training. Accompanying HELM, the authors release OMolCS-H58k—a large-scale Hamiltonian dataset featuring 58 elements and molecules up to 150 atoms. They also propose “Hamiltonian pretraining,” a method that leverages Hamiltonian-derived embeddings to enhance energy prediction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-structured and clearly written, providing a concise and accessible overview of the problem and the proposed solution.\n* The authors conduct a thorough analysis, enhancing understanding through techniques such as UMAP visualization.\n* Moreover, they introduce a new dataset of Hamiltonians, which is a valuable contribution to the open-source community in this field."}, "weaknesses": {"value": "I do not find any obvious weaknesses in this paper. I believe that many researchers working in the areas of Hamiltonian prediction and machine learning force fields may have had similar ideas, so the concept itself is reasonable, and the experimental results appear solid. However, I do have some questions about the paper—please see Questions. If the authors address my concerns, I will consider increasing my score."}, "questions": {"value": "* In the Introduction, the paper states: “For every system of N atoms, there is 1 energy label, O($N$) force labels, and O($N^2$) labels in the Hamiltonian matrix.” However, the number of labels in the Hamiltonian matrix should be the total number of orbitals, not the number of atoms.\n* In \"Learning orbital interactions\" of Background and Previous Work, both the initial models and following work are cited as [1]. Is this expected?\n* In Fig. 2(d), the “Element–element interaction” is shown. How are these numerical values computed?\n* In Fig. 3(c), are the results shown the same as those reported in Table 4? In Fig. 3(c), the pretrained-frozen model appears to achieve better validation results, whereas Table 4 shows that the finetuned model performs better. These seem inconsistent. I guess that this be due to the loss functions being different (MSE in the figure vs. MAE in the table)? In the section Loss and scalar referencing, the paper describes the loss function used for Hamiltonian prediction, but since the model is also trained to predict energy, I could not find a clear explanation of the loss used for energy prediction. Could the authors clarify this?\n* The work applies a cutoff to the interactions. Appendix B.2 mentions: “Note that this also sets a limit on the precision to which the total energy can be recomputed from this data.” Could a quantitative error analysis be provided here? How much error does this cutoff introduce in downstream tasks?\n* In Appendix D.1, Fig. 9(c) shows a trend that appears inconsistent with Fig. 6 in [2] and Fig. 1 in [3], both of which display a divergent trend. Could the authors explain the source of this discrepancy?\n\n[1] Gong X, Li H, Zou N, et al. General framework for E (3)-equivariant neural network representation of density functional theory Hamiltonian[J]. Nature Communications, 2023, 14(1): 2848.\n \n[2] Wang Z, Liu C, Zou N, et al. Infusing self-consistency into density functional theory hamiltonian prediction via deep equilibrium models[J]. Advances in Neural Information Processing Systems, 2024, 37: 89652-89681.\n\n[3] Li Y, Xia Z, Huang L, et al. Enhancing the scalability and applicability of kohn-sham hamiltonians for molecular systems[J]. arXiv preprint arXiv:2502.19227, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "55j5rTwlcr", "forum": "PS1YS8Wv4t", "replyto": "PS1YS8Wv4t", "signatures": ["ICLR.cc/2026/Conference/Submission8985/Reviewer_QD9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8985/Reviewer_QD9Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198912593, "cdate": 1761198912593, "tmdate": 1762920716119, "mdate": 1762920716119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HELM (Hamiltonian Electronic Learning Model), an SO(3)-equivariant graph neural network that jointly learns to predict Kohn-Sham Hamiltonians and molecular energies. The key innovation is leveraging the rich electronic structure information encoded in Hamiltonian matrices—routinely computed but typically discarded during DFT calculations—as a pretraining task to improve machine learning interatomic potentials (MLIPs). The authors construct OMol-CSH-58k, a new dataset of ~56.7k closed-shell molecules covering 58 elements with up to 150 atoms, and demonstrate that Hamiltonian pretraining yields substantial improvements in energy prediction accuracy, particularly in low-data regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*  The paper makes a good case for exploiting Hamiltonian matrices as supervisory signals. The connection between electronic structure and atomistic properties is conceptually novel, and the two-head design bridges DFT and MLIP domains.\n*  OMol-CSH-58k addresses a  gap in existing Hamiltonian datasets by covering 58 elements (vs. 8-10 in prior work), molecules up to 150 atoms, and using the larger def2-TZVPD basis. The dataset composition analysis (Fig. 2, Table 1) is thorough.\n*  HELM achieves ~60 µEh error on ∇²DFT compared to 520-20700 µEh for prior methods (Table 2). The margin is meaningful  The analysis connecting matrix errors to eigenvalue and energy errors (App. D.1-D.2) strengthens the claims.\n*  The pretraining protocol shows energy MAE improvements, especially on ∇²DFT with limited training data (up to ~2× reduction, Table 3). The frozen vs. finetuned head comparisons are informative, and UMAP visualizations (Fig. 4) support the claim of learning better representations.\n*  Distance-decay analysis justifying the 12Å cutoff (App. B.2, Fig. 6), element-referenced scalar normalization (App. B.3), and batching strategies for handling size heterogeneity (Table 7) demonstrate careful implementation."}, "weaknesses": {"value": "* There is a concerning mismatch between the exchange–correlation (XC) functionals used for generating the training data and those used for reconstructing energies from predicted Hamiltonians. Appendix A.2 reports ωB97M-V for energy reconstruction, whereas the ∇²DFT data rely on ωB97X-D/def2-SVP (Table 2). Such inconsistency may induce systematic biases and plausibly account for outliers observed in Fig. 10. The paper should clearly specify: (i) the XC functional used for energy reconstruction in each dataset, (ii) whether this matches the functional that produced the reference data, and (iii) the quantitative impact of any mismatch.\n\n* The model integrates several design elements—SO(2) convolutions, gated nonlinearities per irrep, parity-aware constraints, and element-referenced normalization—but their individual contributions are not analyzed. It remains unclear whether the observed gains stem primarily from architectural innovations or auxiliary components such as normalization or loss design. A systematic ablation table is necessary to disentangle these effects and to provide guidance for future model development.\n* Table 4 and Fig. 3c show cases where fine-tuning on OMol leads to degraded performance relative to the frozen pretrained head, suggesting overfitting. While the authors attribute this to high elemental diversity, the explanation remains speculative. A more rigorous analysis should explore regularization strategies (e.g., layer freezing, dropout, weight-decay sweeps, early stopping) and report per-element error breakdowns to identify whether rare elements disproportionately drive the issue.\n* Energy errors are inconsistently reported—sometimes per-molecule (Table 3) and sometimes per-atom (Table 9)—without clear indication. The notation alternates between H and F (Fock) across sections. Units and symbols should be standardized throughout (e.g., meV / atom) and summarized in a concise notation table for clarity."}, "questions": {"value": "1. Can a model pretrained on def2-SVP (∇²DFT) transfer to def2-TZVPD (OMol) with light head retraining, or vice versa?\n2. Can you also add a force head to your model to see pretraining on H helps with force prediction?\n3. What modifications would HELM require for open-shell/charged systems?\n4. Please report detailed computational performance metrics, including:  (i) Wall-clock time per epoch for ∇²DFT-2k on V100 and A100 GPUs  (ii) Peak GPU memory usage as a function of $l_{\\\\text{max}}$ (iii) Inference time per molecule across different molecular size ranges"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1vCc1Ay7cG", "forum": "PS1YS8Wv4t", "replyto": "PS1YS8Wv4t", "signatures": ["ICLR.cc/2026/Conference/Submission8985/Reviewer_RqiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8985/Reviewer_RqiD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848846744, "cdate": 1761848846744, "tmdate": 1762920715779, "mdate": 1762920715779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HELM, a scalable equivariant graph neural network that learns from Hamiltonian matrices and proposes “Hamiltonian pretraining” as a route to enhance molecular property prediction. The idea is well-motivated, technically sound, and experimentally verified on large and diverse datasets. The proposed OMol CSH 58k dataset is likely to have significant value for the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong and well-motivated idea — leveraging electronic-structure information that is usually discarded in MLIP training is both physically meaningful and practically impactful.\n2. Technical soundness — the HELM architecture is carefully designed with rotational equivariance and SO(2) convolutions for scalability to large basis sets and diverse elements.\n3. Dataset contribution — the new OMol CSH 58k dataset fills a gap between small-molecule Hamiltonian data and modern large-scale MLIP datasets."}, "weaknesses": {"value": "1. Conceptual novelty is moderate: the idea of predicting or using Hamiltonian matrices has been explored in literature; this paper mainly provides a unified and scalable realization.\n2. Performance improvements are modest: while pretraining improves energy prediction in low-data regimes, the gain is typically 1.5–2×, and the benefit diminishes for larger datasets.\n3. No demonstration on forces or MD trajectories: the method is not yet shown to improve dynamical stability or energy–force consistency, which are critical for MLIP applications.\n4. Dataset construction: The details about the settings of how the data points were computed are not clear to me. The authors claimed that the dataset expands over many elements in the periodic table. However, Gaussian basis sets calculations are unknown to fail for many heavy elements. So, how did you handle these issues, to be specific, how to choose basis or effective core potentials for those molecules?"}, "questions": {"value": "The idea of pretraining on Hamiltonian matrices is certainly reasonable and physically motivated. However, this approach requires maintaining detailed edge-level information in the molecular graph, which could substantially increase memory usage. In particular, traditional MLIPs often rely on relatively low-order spherical harmonics for edge features, whereas predicting Hamiltonian elements likely demands much higher angular degrees to capture orbital interactions. This would significantly raise both computational and memory costs.\nCould the authors comment on this trade-off? Specifically, how does the computational and memory footprint of HELM compare to that of conventional MLIPs trained only on energies and forces? Moreover, under the same computational budget, does HELM still outperform existing energy-prediction models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QcTLRbhezm", "forum": "PS1YS8Wv4t", "replyto": "PS1YS8Wv4t", "signatures": ["ICLR.cc/2026/Conference/Submission8985/Reviewer_7bnb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8985/Reviewer_7bnb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887681189, "cdate": 1761887681189, "tmdate": 1762920715015, "mdate": 1762920715015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HELM, a novel approach for predicting molecular potential energies with the help of large amounts of electronic Hamiltonian matrix data, and a curated Hamiltonian matrix dataset OMol_CSH_58k. HELM adopts a novel backbone model architecture, first pre-trains model to predict Hamiltonian matrix then trains model to predict energy. Experiments show HELM achieves better accuracy than baselines in Hamiltonian prediction and good energy prediction accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Propose a novel approach for molecule energy prediction. The idea of first pre-training on Hamiltonian prediction task then training on energy prediction task is quite useful and generalizable to multiple different models.\n- Experiment results demonstrate good performance of the proposed HELM approach.\n- The writing of this paper is clear and well-organized."}, "weaknesses": {"value": "- Need clarification on results comparison in Table 2: are HELM and all baselines in Table 2 pre-trained on the OMol_CSH_58k Hamiltonians to ensure fair comparison? If so, authors are encouraged to highlight what is the advantages of the architecture design in HELM over baselines that leads to better Hamiltonian prediction accuracy.\n- In Table 2, there is no baseline comparison in energy prediction so there is no idea how good the performance of HELM is. Authors are suggested to add baseline performance. If baseline methods are using the predicted Hamiltonian matrices to compute energy, compare with the energy prediction accuracy of HELM under the same setting."}, "questions": {"value": "No additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1m2pqLc0uf", "forum": "PS1YS8Wv4t", "replyto": "PS1YS8Wv4t", "signatures": ["ICLR.cc/2026/Conference/Submission8985/Reviewer_atd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8985/Reviewer_atd4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970829294, "cdate": 1761970829294, "tmdate": 1762920714423, "mdate": 1762920714423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}