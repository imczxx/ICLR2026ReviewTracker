{"id": "e4t1775UJ1", "number": 10258, "cdate": 1758165256315, "mdate": 1759897662513, "content": {"title": "Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection", "abstract": "Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.", "tldr": "", "keywords": ["3D Object Detection", "Mamba", "Foreground Representation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ef04ddf7cd7a1a3ed224f794ba6e604e12dc39a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Fore-Mamba3D, a novel 3D object detection framework that focuses on foreground-enhanced encoding using Mamba-based architectures. The key insight is that previous Mamba-based methods encode entire non-empty voxel sequences containing abundant background information, while foreground voxels occupy only a small portion of the scene. The method introduces two main components: (1) Regional-to-Global Sliding Window (RGSW) strategy to address response attenuation in foreground-only sequences, and (2) Semantic-Assisted and State Spatial Fusion Mamba (SASFMamba) to enrich contextual representation with semantic and geometric awareness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The motivation for foreground-focused encoding is compelling and well-supported by empirical analysis.\n2) The RGSW strategy effectively addresses the response attenuation problem in foreground-only sequences through a regional-to-global information propagation mechanism. \n3) The method achieves state-of-the-art performance across multiple benchmarks (nuScenes: 72.3 NDS, KITTI, Waymo), demonstrating consistent improvements over existing Mamba-based approaches while reducing computational overhead."}, "weaknesses": {"value": "1) While the combination is novel, individual components are relatively incremental. The foreground sampling is essentially top-k selection based on predicted scores, and the Hilbert curve rotation is a straightforward solution to regional truncation issues.\n2)  The paper lacks comprehensive ablation studies on key hyperparameters. How sensitive is performance to the number of rotations r,  and the foreground sampling ratio α? The impact of different space-filling curves beyond Hilbert is not explored.\n3) The paper doesn't discuss failure cases or limitations of the approach. How does performance vary with different foreground/background ratios across datasets?"}, "questions": {"value": "1) How does the method perform when foreground prediction accuracy is low?\n2) How sensitive is the method to the choice of space-filling curves (Z-order vs. Hilbert)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JEfSyBI6Qc", "forum": "e4t1775UJ1", "replyto": "e4t1775UJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10258/Reviewer_3Dbr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10258/Reviewer_3Dbr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478975726, "cdate": 1761478975726, "tmdate": 1762921615239, "mdate": 1762921615239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Fore-Mamba3D is a Mamba-based 3D perception backbone for LiDAR-based object detection. It focuses on foreground voxels predicted by a learned scoring function. It propose a Regional-to-Global Sliding Window module which enhances long-range voxel interaction to prevent information loss from sparse foregrounds. It also introduce a Semantic-Assisted State Fusion module for fusing semantic and geometric features. It achieves efficient linear modeling and how state-of-the-art performance on nuScenes, KITTI, and Waymo."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting and effective idea by selectively adapting foreground voxels for Mamba-based modeling. This approach substantially reduces computational overhead while maintaining high representational quality, showing a smart trade-off between efficiency and performance.\n\n2. The method is validated across multiple benchmark datasets (nuScenes, KITTI, Waymo) with consistent performance gains. The visual results Fig3 are clear and insightful, effectively illustrating how the proposed model can improve the contextual understanding of fused voxel features."}, "weaknesses": {"value": "1. Although the proposed method introduces innovative foreground-focused encoding, the overall performance gain compared to prior Mamba-based or Transformer-based 3D detectors is relatively modest. The improvements, while consistent, may not fully justify the added architectural complexity or the additional design components.\n\n2. Unclear Spatial Consistency in the Regional-to-Global Sliding Window (RGSW): In the proposed RGSW strategy, the authors directly fuse the later half of the sequence with the previous half, which raises concerns about spatial coherence. Since the voxel sequence is constructed by selectively sampled voxels along a Hilbert curve, their ordering does not guarantee true spatial adjacency. This makes the fusion operation confusing, the merged regions may not correspond to physically neighboring spatial areas. As a result, the process may not fully respect local spatial correlation, and in extreme cases, it could behave similarly to random voxel concatenation, weakening the interpretability of the regional-to-global transition.\n\n3. The authors completely discard background voxel feature extraction, directly concatenating the processed foreground features. While this design choice further reduces computational cost, it introduces a potential issue, semantic strength inconsistency between foreground and background regions. In fact, background voxels play an important role in defining object boundaries and separating adjacent instances. Ignoring background information entirely may lead to inaccurate boundary localization, especially in cluttered or occluded scenes."}, "questions": {"value": "Have the authors evaluated how alternative sequence ordering strategies, such as random shuffling or different spatial traversal patterns, affect the performance of the Regional-to-Global Sliding Window (RGSW)? If so, how sensitive is the model to the chosen voxel sequence order, and does it significantly influence spatial consistency or detection accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YEZLrJdAbu", "forum": "e4t1775UJ1", "replyto": "e4t1775UJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10258/Reviewer_jtLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10258/Reviewer_jtLi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626788276, "cdate": 1761626788276, "tmdate": 1762921614401, "mdate": 1762921614401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "3D object detection is a critical task in computer vision with broad applications in autonomous driving. In this paper, the authors proposed a novel LiDAR based 3D object detector, named as FORE-MAMBA3D. It demonstrates the better performance in the KITTI, NuScenes, and Waymo datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well-structured and supported by sufficient experiments, showing great potential for acceptance. \n1. Experiments are conducted in the mainstreams datasets, such as KITTI, NuScenes, and Waymo datasets. These results show the effectinvess of the proposed method.\n2. Overview of the proposed framework is very clear. It shows the novelty of the proposed method.\n3. Motivation of the proposed method is also clear."}, "weaknesses": {"value": "The following points are suggested for further improvement.\n​​Q1. In the Introduction, the mention of \"incomplete and imprecise sampling\" seems disconnected from the preceding context, which might confuse readers about the purpose of this sampling operation. Please clarify its objective. For instance, if foreground-only encoding is used, does it require sampling foreground voxels? Please explain this clearly.\n​​Q2. In the Related Work section, the authors highlight the limitations of existing methods. It would be better to further emphasize how the proposed method in this paper specifically addresses these limitations.\n​​Q3. In Section 3.2, the authors introduce a local token to summarize information within each patch. It is recommended that the authors explain (1) their motivation (e.g., to address response attenuation) (2) explain  the need for a local token to re-summarize information, (3) finally delve into the specific computational details. Currently, the local token is introduced somewhat abruptly.\n​​Q4. Section 3.3.1 does not clearly explain the computational process and the motivation behind the proof, even with supplementary materials. The authors need to clarify:\na. How exactly are voxels rearranged using the semantic categories S? Is cosine similarity used for this rearrangement?\nb. From the diagram, it appears that x undergoes only one SSM operation and one 1D convolution to obtain h'. However, Equation 6 seems confusing. For example, is Nk(i) \"the original index of the semantically neighboring feature\" (as in Section 3.2) or the \"index of semantically neighboring feature\" (as in the supplementary material)? Furthermore, in Equation 6, is index j the index before or after rearrangement? Also, does merely changing the order allow the transformation from Equation 5 to Equation 6, given that the SSM operation is performed on the original sequence?\nc. It would be beneficial to separate the description of the network's computational process from the theoretical proof in Section 3.2. At the very least, indicate to readers that the subsequent formulas are primarily for theoretical justification and are not necessary during network inference.\n​​Q5. Please specify under what configuration (one 4090D? 1batch size?) the network's inference speed was evaluated in the experiments. Furthermore, I am curious about how the FLOPs for the Mamba module within the 3D backbone were computed."}, "questions": {"value": "The following points are suggested for further improvement.\n​​Q1. In the Introduction, the mention of \"incomplete and imprecise sampling\" seems disconnected from the preceding context, which might confuse readers about the purpose of this sampling operation. Please clarify its objective. For instance, if foreground-only encoding is used, does it require sampling foreground voxels? Please explain this clearly.\n​​Q2. In the Related Work section, the authors highlight the limitations of existing methods. It would be better to further emphasize how the proposed method in this paper specifically addresses these limitations.\n​​Q3. In Section 3.2, the authors introduce a local token to summarize information within each patch. It is recommended that the authors explain (1) their motivation (e.g., to address response attenuation) (2) explain  the need for a local token to re-summarize information, (3) finally delve into the specific computational details. Currently, the local token is introduced somewhat abruptly.\n​​Q4. Section 3.3.1 does not clearly explain the computational process and the motivation behind the proof, even with supplementary materials. The authors need to clarify:\na. How exactly are voxels rearranged using the semantic categories S? Is cosine similarity used for this rearrangement?\nb. From the diagram, it appears that x undergoes only one SSM operation and one 1D convolution to obtain h'. However, Equation 6 seems confusing. For example, is Nk(i) \"the original index of the semantically neighboring feature\" (as in Section 3.2) or the \"index of semantically neighboring feature\" (as in the supplementary material)? Furthermore, in Equation 6, is index j the index before or after rearrangement? Also, does merely changing the order allow the transformation from Equation 5 to Equation 6, given that the SSM operation is performed on the original sequence?\nc. It would be beneficial to separate the description of the network's computational process from the theoretical proof in Section 3.2. At the very least, indicate to readers that the subsequent formulas are primarily for theoretical justification and are not necessary during network inference.\n​​Q5. Please specify under what configuration (one 4090D? 1batch size?) the network's inference speed was evaluated in the experiments. Furthermore, I am curious about how the FLOPs for the Mamba module within the 3D backbone were computed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ErkQJQH8P4", "forum": "e4t1775UJ1", "replyto": "e4t1775UJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10258/Reviewer_7P9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10258/Reviewer_7P9E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719391438, "cdate": 1761719391438, "tmdate": 1762921613864, "mdate": 1762921613864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Mamba-based backbone for 3D object detection focused on more effective foreground voxel encoding. The method selects top-scoring foreground voxels, serializes them using a rotation-augmented Hilbert curve, and processes them with RGSW for propagating local and global context, and SASFMamba to enhance context representation via state space modeling. Experiments are performed on nuScenes, KITTI, and Waymo benchmarks, along with ablation studies to justify each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies a major inefficiency in prior Mamba-based 3D object detectors: unnecessary computation over background voxels, and the resulting context loss when naïvely restricting to the foreground.\n2. Foreground selection is performed using a trainable scoring mechanism with effective sampling and serialization, alleviating regional truncation problems. It is well-motivated both by experiments and design illustrations.\n3. The RGSW and the SASFMamba fusion modules are described in substantial detail, with concrete mathematical rationale, and the implementation roadmap is well clarified.\n4. The method is compared against a rich spectrum of modern baselines on three benchmarks. Relative improvements are numerically strong."}, "weaknesses": {"value": "1. Limited discussion of causal sequence modeling approaches: Although the method addresses linear sequence encoding with foreground focus, it omits explicit comparison to other causal sequence modeling alternatives or other advanced state-space models.\n2. Modeling limitations: The method largely treats background voxels as non-informative, but in autonomous driving, background structure or clutter can contain subtle contextual cues (for object contextual priors, occlusion estimation, etc.). The effect of completely de-emphasizing background is not discussed in terms of potential blindspots for categories with ambiguous boundaries."}, "questions": {"value": "1. How robust is the method to imperfect, noisy, or ambiguous foreground scoring? Do detection performance or ablations degrade gracefully if the scoring network fails to highlight the correct voxels?\n2. How do the authors envision handling the omission of background information in domains where background context could be a valuable cue (e.g., occlusions, instance support)?\n3. Please elaborate on any observed failure modes (missed detection, false positives/negatives), especially with reference to dataset bias, rare classes, or environmental conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VcvZ9cIFvD", "forum": "e4t1775UJ1", "replyto": "e4t1775UJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10258/Reviewer_cXEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10258/Reviewer_cXEa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920930554, "cdate": 1761920930554, "tmdate": 1762921613311, "mdate": 1762921613311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}