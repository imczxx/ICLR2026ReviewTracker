{"id": "d49AMt3IBf", "number": 20544, "cdate": 1758307248160, "mdate": 1759896972038, "content": {"title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts", "abstract": "LLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints}. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs.", "tldr": "", "keywords": ["Clinical/Healthcare Application; Medical LLM Editing"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/115cf97195ca7e221010b170a32fa7a5c1bba2ef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of knowledge editing in the medical domain and introduces a retrieval-based framework designed for batch editing, along with a new benchmark for evaluating batch edit performance. To mitigate the issue of overlapping keys, the proposed method embeds subject–relation pairs and employs an attention-based encoder to generate soft prompts. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance across multiple evaluation metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of batch edits is reasonable and the proposed benchmark can potentially inspire future studies.\n2. The experiment is comprehensive and demonstrates the proposed model’s effectiveness."}, "weaknesses": {"value": "1. Regarding the methodology, it appears that this paper largely follows the design of RECIPE, which also formulates knowledge editing as training retrieved soft prompts. Although the authors introduce some specific modifications, such as using a pretrained RoBERTa encoder and a shared MLP projection, these seem to be more engineering adjustments rather than fundamentally new technical contributions.\n2. The idea of the attention-based prompt encoder is also unclear to me. Since the attention is performed over a single embedding, the resulting soft prompt essentially comes from distributing attention scores over the projected value embedding. I am uncertain about the underlying criterion here and wonder how the performance would compare if the model simply projected z_v into T embeddings directly, without the attention operation.\n3. Moreover, it remains unclear how these proposed techniques specifically address the challenges of batch editing.\n4. As for the ablation study, `w/o shared MLP` means removing the MLP (only use H?) or using separate MLPs? What does `w/o Attn Prompt Enc.` represent? How do you obtain the soft prompts in this variant?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2auj7llyUf", "forum": "d49AMt3IBf", "replyto": "d49AMt3IBf", "signatures": ["ICLR.cc/2026/Conference/Submission20544/Reviewer_7Vo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20544/Reviewer_7Vo1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886427183, "cdate": 1761886427183, "tmdate": 1762933961665, "mdate": 1762933961665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem & Motivation**\n\nThe paper addresses the unreliability of LLMs in high-stakes medical applications. Due to the rapid evolution of medical science and potential training data errors, LLMs often generate outdated or inaccurate information (hallucinations). While \"model editing\" (updating model knowledge without full retraining) is a potential solution, standard methods face critical limitations in the medical domain.\nThe authors identify three categories of model editing, but focus on the critical flaws of the two parameter-modifying approaches:\n1. *Locate-then-edit*: These methods directly modify a small subset of the model's weights. The paper argues this is ill-suited for medicine because it \"compromises locality,\" meaning an edit to one fact can unintentionally alter unrelated knowledge.\n2. *Meta-learning-based*: These methods manipulate gradients to perform global parameter updates.\n3. *Retrieval-based editing*: This approach avoids altering the original LLM parameters by storing new knowledge in an external memory (a key-value store) and retrieving it at inference time; e.g., RECIPE uses continuous prompt learning to prefix knowledge to the input query. This is the paper's preferred approach as it inherently preserves locality.\n\nThe paper's method is motivated by two specific failures of existing retrieval-based editors in the medical domain:\n- Medical knowledge contains many textually similar but factually distinct items (e.g., \"drug-drug interaction with Clofedanol\" vs. \"drug-drug interaction with Camostat\"). This causes representation overlap in the retrieval space, leading the model to retrieve the wrong fact and reducing editing accuracy.\n- Lack of batch-editing evaluation: existing benchmarks (like MedCF) focus on single-fact edits. However, real-world medical updates (e.g., new guidelines, retracted studies) often require updating multiple related facts at once (batch-editing).\n\n**Method**\n\n- **MedVersa.** To address the lack of a suitable benchmark, the authors first create MedVersa. It is constructed from the MedMCQA dataset and is designed to be superior to the previous MedCF benchmark.\n   - Batch-editing support. It fixes a flaw in MedCF where \"efficacy\" (what you edit) and \"locality\" (what should not change) prompts overlapped, which made true batch-editing evaluation not possible.\n   - Broader coverage. It spans 20 medical subjects, instead of 12 of MedCF.\n\n- **MedREK (Medical Retrieval-based Editing with Key-aware prompts)**, the main contribution, a new retrieval-based editing framework. Its mechanism is trained in two phases. First, a knowledge base is built where each new fact (s, r, o) is encoded into a key-value pair: a Shared Query-Key MLP (MLP_qk) encodes the (s, r) pair to create the Key, and an Attention-based Prompt Encoder encodes the full (s, r, o) triplet to create the Value. A user's query is encoded by the same MLP_qk. If the query vector matches a Key (and is not an unrelated query, as determined by a threshold), the corresponding Value embedding is retrieved and prepended to the query, thus acting as a continuous prompt. The total loss L_total = L_edit + L_contra trains the modules for these two distinct goals:\n- L_edit: An editing loss that trains the Prompt Encoder to generate \"Values\" (continuous prompts) that successfully guide the LLM to the new answer.\n- L_contra: A contrastive loss (InfoNCE) that trains the Shared MLP_qk to pull matching query/key pairs together and push mismatched pairs apart, mitigating the representation overlap problem.\n\n**Experimental setup**\n\n- The framework is evaluated on two medical-domain LLMs: Meditron-7B (LLaMA2-based) and HuatuoGPT-01-8B (LLaMA3-based).\n- Baselines. MedREK is compared against:\n   - Parameter-based: MEND, MEMIT, and MedLaSA (a medical-specific editor).\n   - Retrieval-based: RECIPE (the previous state-of-the-art retrieval editor).\n- Benchmarks: MedCF++ and the new MedVersa.\n\n**Key findings**\n\n- Superior performance. MedREK achieves the highest average scores across all metrics (Efficacy, Generality, Locality) on both benchmarks and both LLMs.\n- Batch-editing success. MedREK's performance remains high even in batch-editing (10, 50, 100 edits), while the baseline retrieval-editor (RECIPE) sees its performance degrade significantly as the batch size increases.\n- Locality. Parameter-based methods (MedLaSA, MEMIT) show a significant drop in Locality, validating the paper's motivation for using a retrieval-based approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses knowledge editing within the medical domain. By focusing on the \"locality\" requirement—ensuring edits do not corrupt unrelated knowledge—it tackles a critical safety and reliability problem specific to high-stakes applications.\n- Dual contribution. The authors provide both a new resource and a new method.\n- MedREK ablations. The paper includes an ablation study on MedREK's two primary components (the shared query-key MLP and the attention-based prompt encoder). This analysis, as referenced in the \"Key Findings\" and experimental setup, aims to validate the authors' design choices by isolating the contribution of each module to the framework's overall performance."}, "weaknesses": {"value": "- *Limited comparison.* The results position MedREK primarily against RECIPE. While RECIPE is a strong baseline, the claim of SOTA performance would be substantially stronger if MedREK were benchmarked against a wider array of recent retrieval-based editors. The conclusion that MedREK solves the retrieval problem for medical text is contingent on RECIPE being a sufficient proxy for all retrieval-based methods.\n- *Absence of statistical significance testing.* The presented results appear robust. However, for a paper making strong claims of superiority, the lack of statistical significance testing (e.g., p-values from t-tests or similar) is a methodological omission. It is unclear if the reported average performance gains are statistically significant across the entire test set or if they are driven by a high-leverage subset of the benchmark.\n- *Prompt encoder motivation.* While the prompt encoder uses multi-head attention to generate prompt tokens, it is not fully transparent from the text or ablation exactly why attention-based prompts yield such significant locality gains. More interpretability analysis (e.g., what information is captured by learned prompts) would enhance understanding and trust in the mechanism.\n- *Lack of cross-domain generalization results.* The evaluation is tightly focused on medical LLMs and medical datasets. Experiments on out-of-domain tasks could demonstrate the generality or limits of the approach, especially since the core architectural ideas (retriever + prompt generation) are not specific to the medical domain.\n- *Limited model generalizability.* The framework is only validated on two medical LLaMA-based LLMs (Meditron-7B and HuatuoGPT-01-8B). This is a very small model pool, and it remains unproven how MedREK would perform with other model architectures or non-medical-domain backbones.\n- *Questionable encoder choice.* The shared query-key MLP is built on a general-purpose RoBERTa. Given the paper's strong emphasis on the medical domain, the failure to test or justify this choice against domain-specific encoders is a clear methodological gap. A domain-specific encoder might inherently resolve some of the \"representation overlap\" issues.\n- *Unspecified efficiency.*  The paper does not provide a clear analysis of the computational overhead of MedREK, as well as exact VRAM requirements.\n- *Presentation quality.* The paper suffers from presentation issues that hinder readability. The submission retains a template title (\"FORMATTING INSTRUCTIONS...\"), and the methodological description (particularly the interaction between the two encoders) is not always easy to parse. Different typos have been identified, e.g., missing punctuation in Section 5.1.\n- *No stated data/code sharing plan.* The paper lacks a clear commitment or plan for sharing the MedVersa benchmark or the MedREK implementation code, even for peer review. This omission hinders reproducibility and prevents independent verification of the experimental claims."}, "questions": {"value": "- Could the authors provide targeted case studies of failed edits or unintended locality violations, particularly in batch-editing?\n- How sensitive is the performance to the selection of RoBERTa as the encoder for keys/queries? Would other encoders or domain-specific pretraining yield different trade-offs?\n- What steps were taken to ensure that the automatic rewriting and rephrasing steps (using Gemini) did not introduce subtle semantic distortions? Is there any human (expert) validation of sampled MedVersa questions and counterfactuals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wYHNuWCMRH", "forum": "d49AMt3IBf", "replyto": "d49AMt3IBf", "signatures": ["ICLR.cc/2026/Conference/Submission20544/Reviewer_waxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20544/Reviewer_waxX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010919891, "cdate": 1762010919891, "tmdate": 1762933961374, "mdate": 1762933961374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of medical Large Language Models (LLMs) in generating outdated or inaccurate information due to rapid medical knowledge evolution and training data errors. It proposes two core contributions: (1) MedVersa, a medical knowledge editing benchmark that supports both single and batch-editing scenarios, with broader coverage of 20 medical subjects (vs. 12 in prior MedCF++) to avoid over-reliance on pharmacology; (2) MedREK, a retrieval-based editing framework integrating a shared query-key MLP (for precise query-knowledge alignment) and an attention-based prompt encoder (for knowledge-specific prompts). Experiments on MedCF++ and MedVersa with Meditron-7B and HuatuoGPT-o1-8B show MedREK outperforms baselines (e.g., MEND, MEMIT, RECIPE) across Efficacy, Generality, and Locality metrics, providing the first validated batch-editing solution for medical LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. It pioneers the construction of a batch-editing benchmark (MedVersa) for medical LLMs, addressing the gap in prior benchmarks (e.g., MedCF, MedCF++) that only support single-editing. MedVersa’s 20 medical subjects (including Pediatrics and Social & Preventive Medicine) fill the void of narrow domain coverage in existing benchmarks.\n2. It innovatively adapts retrieval-based editing to the medical domain, designing a shared query-key MLP to resolve representation overlap (a critical issue in medical knowledge retrieval) and an attention-based prompt encoder to generate fine-grained prompts, overcoming the limitations of parameter-based editing (e.g., MedLaSA) that harms Locality.\n3. Experimental design is rigorous: it validates MedREK on two medical LLMs (Meditron-7B, HuatuoGPT-o1-8B) and two benchmarks (MedCF++, MedVersa), covering single (1-edit) and batch (10/50/100-edit) scenarios. Ablation studies confirm the individual contributions of the shared query-key MLP and attention prompt encoder.\n4. Benchmark construction is systematic: it first refines MedCF into MedCF++ to eliminate prompt overlap between Efficacy and Locality, then expands to MedVersa using MedMCQA, ensuring each data entry includes Efficacy/Generality/Locality QA pairs for comprehensive evaluation."}, "weaknesses": {"value": "1. The paper uses Gemini 2.0 Flash to rewrite MedMCQA’s original phrases into interrogative questions (for Efficacy/Generality) but provides no evidence of semantic consistency or medical term accuracy. There is no mention of manual verification (e.g., by medical professionals) or automatic metrics (e.g., BERTScore) to rule out ambiguity or term misuse, which may undermine MedVersa’s reliability.\n2. RoBERTa is a general pre-trained model, but the paper does not compare it with medical-specific models (e.g., BioBERT, PubMedBERT) in terms of embedding quality for medical text. Without this comparison, it cannot be confirmed that RoBERTa is optimal for capturing medical knowledge representations.\n3. Batch-editing is only tested on 10/50/100 edits. The paper does not explore larger batch sizes (e.g., 500 edits) to assess performance degradation, nor does it analyze the scalability of MedREK when the knowledge base expands to millions of medical facts (e.g., whether retrieval speed degrades and if index optimizations like FAISS are needed).\n4. The ethics section only mentions compliance with the ICLR Code of Ethics but lacks details: (1) whether MedVersa’s source data (MedMCQA) contains patient-related information and has undergone de-identification; (2) whether ethical committee (IRB) approval was obtained for dataset construction; (3) how user medical queries are protected during MedREK’s deployment."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "1. Unclear data privacy protection: The paper does not specify whether MedVersa’s source dataset (MedMCQA) contains patient-related information (e.g., medical records). If such information exists, there is no statement on whether de-identification (e.g., removing personal identifiers) was performed, which violates medical data privacy protection norms.\n2. Lack of ethical approval documentation: It does not mention whether the construction of MedVersa (including data collection and processing) and the use of pre-trained medical LLMs (Meditron-7B, HuatuoGPT-o1-8B) obtained approval from an Institutional Review Board (IRB), which is a core requirement for medical-related research involving human-derived data.\n3. Unaddressed deployment privacy risks: MedREK is intended for clinical scenarios, but the paper provides no plan for protecting user medical queries (e.g., symptoms, medical history) during deployment (e.g., end-side retrieval, data encryption), posing potential risks of sensitive information leakage."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VtVD9KmFZE", "forum": "d49AMt3IBf", "replyto": "d49AMt3IBf", "signatures": ["ICLR.cc/2026/Conference/Submission20544/Reviewer_n5UC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20544/Reviewer_n5UC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090739803, "cdate": 1762090739803, "tmdate": 1762933960963, "mdate": 1762933960963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}