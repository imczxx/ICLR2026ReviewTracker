{"id": "biJqDcw6i9", "number": 13535, "cdate": 1758218984405, "mdate": 1759897430370, "content": {"title": "PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered—for instance, in physical computing, where software must interact with and control physical hardware —their effectiveness has not been fully explored. To address this gap, we introduce PCEVAL (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, PCEVAL provides the first reproducible and automatically validated empirical assessment of LLMs’ ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. PCEVAL advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.", "tldr": "We introduce PCEVAL, a new benchmark to evaluate LLMs' capabilities in physical computing, assessing both logical and physical circuit generation alongside code generation.", "keywords": ["Physical Computing", "Large Language Models", "AI for Education"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1388c2411d095cfc53ace9d7a1a73fad6b494efc.pdf", "supplementary_material": "/attachment/df6b7ee892d77f657a5367789f4e1e4d5c6b0681.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces PCEVAL, a benchmark for evaluating large language models (LLMs) in physical computing tasks—where software must interact with and control hardware. Unlike prior benchmarks focusing only on logical or coding abilities, PCEVAL enables fully automated and reproducible evaluation of LLMs in both logical and physical aspects of hardware-related projects, eliminating the need for human assessment."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a benchmark for evaluating LLM in physical computing tasks."}, "weaknesses": {"value": "1) This work focuses on physical computing but primarily adds components related to breadboard and logic design. The breadboard design tasks are relatively simple and not strongly connected to embedded system design. Moreover, even in cases where breadboard design is necessary, such tasks could likely be automated using traditional algorithms, and it remains unclear why an LLM-based approach is required here.\n\n2) Regarding the logic design aspect, LLM-based methods in this area have already been extensively explored, with numerous existing benchmarks available. The contribution of this work therefore appears to be largely a combination of prior benchmarks. The key distinctions between this benchmark and existing logic design benchmarks should be clearly articulated."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDFs9E7gId", "forum": "biJqDcw6i9", "replyto": "biJqDcw6i9", "signatures": ["ICLR.cc/2026/Conference/Submission13535/Reviewer_QYXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13535/Reviewer_QYXd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483863384, "cdate": 1761483863384, "tmdate": 1762924136547, "mdate": 1762924136547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark for evaluation the physical computing capabilities of large language models. Specifically different from prior work, the benchmark also includes new tasks such as logical circuit generation and physical circuit generation (physical circuit layout breadboard implementations). and automatic evaluation protocal. The authors also evaulated on 13 leading models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Novel and well-motivated benchmark addressing previously unexamined domain. Clear task decomposition and fully automated evaluation framework ensuring reproducibility"}, "weaknesses": {"value": "(1) Evaluations lack variance measures, such as pass@k metrics etc. LLMs results could be noisy under temperature sampling.\n\n(2) The benchmarks scope is largely Arduino-centric. This raises question about generality and difficulty of the task on real-world example use cases (other than for educational purposes).\n\n(3) It seems the LLM constently make mistakes on problems easily to correct (i.e. physical contraint violation). Would incorporating such feedback in a agentic framework improve the results?"}, "questions": {"value": "(1) Some aspects of the automated validation pipeline are insufficiently detailed for replication. Will the benchmark be open-sourced?\n\n(2) Can the authors present more metrics on evaluation such as pass@k etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tePm3WJQcL", "forum": "biJqDcw6i9", "replyto": "biJqDcw6i9", "signatures": ["ICLR.cc/2026/Conference/Submission13535/Reviewer_EG1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13535/Reviewer_EG1x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596335805, "cdate": 1761596335805, "tmdate": 1762924136166, "mdate": 1762924136166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PCEval, the first reproducible and verifiable assessment suite to test LLMs in the context of designing physical computing for educational purpose. Empirical evidence suggests that large language models are still insufficient in relevant tasks. It means that we have to use LLMs with caution in physical computing, and more attention should be devoted to this area."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The investigated question is novel and interesting: how good LLMs are in physical computing for educational purposes. It is well motivated. With the development of LLMs in many fundamental tasks such as reasoning, it is important to understand how useful they are in real-life tasks.\n\n2. The study design is carefully constructed. It starts by interviewing multiple CS educators about what problems are critical in their context, making sure that the investigated problems are relevant to real-world deployment.\n\n3. The proposed framework is scalable and verifiable. This is important in the future impact of this work in relevant and broader areas.\n\n4. The presentation is very clear and easy to follow and understand."}, "weaknesses": {"value": "Mitigation methods can be more thoroughly discussed. For instance, why CoT works on some models but not others? Is there any method to improve model performance instead of simply prompting?"}, "questions": {"value": "Please refer to weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TLllwkIfkP", "forum": "biJqDcw6i9", "replyto": "biJqDcw6i9", "signatures": ["ICLR.cc/2026/Conference/Submission13535/Reviewer_b5X5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13535/Reviewer_b5X5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694435551, "cdate": 1761694435551, "tmdate": 1762924135758, "mdate": 1762924135758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PCEval, the first benchmark for evaluating the capabilities of large language models in physical computing-that is, reasoning and generating both logical and physical circuits, as well as the code that operates them.\nThe benchmark breaks down physical-computing reasoning into four tasks:\n1. Logical circuit generation (D,C--L), 2. Physical circuit generation (D,C--P), 3. Code generation from logical circuits (D,L--C), 4. Code generation from physical circuits (D,P--C). \nPCEval includes 50 Arduino-based projects spanning four complexity levels that can be fully executed in simulation using the Wokwi environment.  Each project includes a test procedure for automated validation, which eliminates subjective expert judgment common in previous works such as MICRO25 or EmbedTask. \nResults from 13 leading models show that while LLMs perform reasonably on code generation (60-70% success), they struggle dramatically with physical layout generation (<10% success for most models)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Evaluation Dimension\nThe paper precisely identifies the missing capability in current LLM evaluation: the ability to reason about and execute tasks that require physical computing.\n\n- Reproducible Evaluation Pipeline\nThe benchmark's use of fully automated simulation (Wokwi) ensures objective, quantitative validation of the generated circuits and code. This contrasts with previous work (e.g., EmbedTask, MICRO-25), which relied on subjective human grading or partial execution tests. The inclusion of physical-level error metrics (pin conflicts, bypass errors, and isolated components) provides an unusual level of granularity and makes the results understandable to engineering reseachers.\n\n- Comprehensive Model Coverage and Systematic Analysis\nThe evaluation covers 13 prominent LLMs, including both closed-source (GPT-4o, o3-mini, Gemini-2.5-Pro, Claude 3.7) and open-source (Mistral-Large, Qwen-VL-Max, Llama-3-70B) systems.  Consistent prompting and multi-trial runs produce a reliable cross-model comparison. The analysis goes beyond raw accuracy to reveal failure modes by reasoning stage (for example, layout versus logic consistency), which is extremely useful for model diagnosis.\n\n- Educational and Practical Grounding\nInterviews with eight computer science educators guide the benchmark's development, ensuring that the tasks chosen reflect realistic student exercises (sensor integration, actuation control, sequential logic).  This foundation strengthens the benchmark's authenticity and demonstrates its potential educational impact( for example, as a diagnostic or formative assessment tool in engineering courses).\n\n-Exploration of Self-Corrective Techniques\nThe inclusion of self-improvement and chain-of-thought prompting (resulting in a +10-18% improvement) shows that the benchmark can directly stimulate methodological research rather than serving solely as a static evaluation dataset."}, "weaknesses": {"value": "- Dataset's Scale and Diversity Limitations\nWith only 50 projects, PCEval's coverage is limited when compared to large-scale code or reasoning benchmarks. The tasks are primarily for introductory Arduino applications (LEDs, sensors, and servos), with less emphasis on other topics such as real-time signal processing.\n\n-Limited Comparison\nLack of comparisons with classical or hybrid design-automation systems (e.g., symbolic circuit solvers, search-based algorithms). This makes it hard for the audience to contextualize LLM weaknesses in relation to domain-specific benchmarks. Such comparisons could help determine whether failures are due to reasoning limitations or a lack of embedded knowledge.\n\n-Evaluation Fairness  \nDetails about prompt standardization (temperature, token limit, and input modality) are not fully specified. Such parameters have a significant impact on model rankings, particularly when comparing multimodal and text-only systems."}, "questions": {"value": "Dataset Balance & Expansion - Given 50 projects, how do you ensure adequate representation of diverse physical computing paradigms (sensing vs actuation control)? Are there plans to scale beyond Arduino while preserving automated validation?\n\nEvaluation Fairness and Reproducibility - Were all LLMs prompted with the same temperature, context length, and output format constraints? How sensitive are results to prompt reformulation (particularly in D,P→C tasks)?\n\nSelf-improvement and COT Prompts - How was the iterative refinement protocol developed (failure-log feedback schema)? Did any models show overfitting or oscillatory corrections during the multi-turn refinement?\n\nIn the focus-group study, were educators asked to rank task readability or error traceability? Could the authors release annotated examples labeled by usability to facilitate future \"AI-pedagogy alignment\" studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QPBkK8nXCF", "forum": "biJqDcw6i9", "replyto": "biJqDcw6i9", "signatures": ["ICLR.cc/2026/Conference/Submission13535/Reviewer_XjH7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13535/Reviewer_XjH7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010343962, "cdate": 1762010343962, "tmdate": 1762924135320, "mdate": 1762924135320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}