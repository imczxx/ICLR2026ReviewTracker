{"id": "vy05zizo2h", "number": 5951, "cdate": 1757948329752, "mdate": 1759897942585, "content": {"title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs).\nHowever, existing RLVR methods often suffer from exploration inefficiency due to mismatches between problem difficulty and model capability: overly difficult problems hinder reasoning path discovery, while overly simple problems offer little learning signal.\nTo address this, we first formalize the effect of problem difficulty by quantifying the relationship between loss descent magnitude and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a supervision-aided RLVR framework that dynamically adjusts problem difficulty to lie within the high-performance region.\nSEELE augments each training sample by appending a hint (part of a full solution) for difficulty reduction. \nUnlike previous hint-based approaches, SEELE deliberately computes the hint length for each individual problem to achieve an optimal difficulty.\nThe optimal hint length is determined via multi-round rollout sampling, where an item response theory model fits accuracy–hint pairs from previous rounds to predict the next-round hint.\nThis instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. \nExperiments show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +10.0 and +8.4 points, respectively, and exceeds the best prior supervision-aided approach by +3.8 points on average across six math reasoning benchmarks.", "tldr": "A novel RL framework for training reasoning model with dynamic supervision incoporation.", "keywords": ["Large language model", "reasoning model", "chain-of-thought", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/250811be0266504ea8ef3b9fb384f4746b739356.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies how to improve training efficiency in RLVR by dynamically adjusting per-instance problem difficulty via hint scaffolding. The authors present a theoretical analysis arguing learning efficiency is upper-bounded by a quadratic function of rollout accuracy and is maximized near 50% accuracy. Building on this, they propose SEELE, a multi-round rollout scheme that fits a predictor online using an IRT/3PL model over collected (hinting rate, accuracy) pairs and appends an instance-specific hint whose length is predicted by the predictor. Experiments on multiple base models show consistent gains over GRPO, SFT, and recent hint/supervision-aided baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear with theoretical analysis\n2. The method is sound and novel.\n3. Strong empirical gains across tasks and model families"}, "weaknesses": {"value": "1. Efficiency concerns: SEELE converts the original single-round rollout generation in GRPO into a multi-round process, which inevitably introduces additional computational overhead compared with single-round parallel generation. However, the paper does not report any efficiency metrics such as wall-clock time or overall compute cost.\n\n2. Applicability concerns\n- The multi-round rollout scheme requires a sufficient number of rollouts to fit the predictor reliably, which may limit its applicability under low-resource settings.\n- The method is tightly coupled with group-based RL algorithms such as GRPO, and cannot be directly applied to algorithms like PPO that do not rely on group rollouts.\n- Both the theoretical analysis and the predictor design are tailored to binary-reward settings, restricting generalization to more complex reward structures.\n\n3. Lack of hyperparameter sensitivity analysis: \n   The paper does not study how performance varies with respect to key hyperparameters such as $k_0, v_0$, leaving the robustness of the method unclear.\n\n4. Lack of limitation discussion\n\n5. Missing discussion of related works:\n   The idea of selecting or emphasizing prompts with intermediate success rates has appeared in prior works on Prompt Curriculum and Prompt Selection [1,2,3]. However, these connections are not discussed, which would help situate SEELE within the broader landscape of adaptive prompt and curriculum methods.\n\n[1] Self-Evolving Curriculum for LLM Reasoning\\\n[2] Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning\\\n[3] Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zuytl0orUq", "forum": "vy05zizo2h", "replyto": "vy05zizo2h", "signatures": ["ICLR.cc/2026/Conference/Submission5951/Reviewer_xs8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5951/Reviewer_xs8N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479885010, "cdate": 1761479885010, "tmdate": 1762918369922, "mdate": 1762918369922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SEELE, a supervision-aided RLVR framework that dynamically adjusts problem difficulty. SEELE appends instance-specific partial solutions as hints and adaptively adjusts their length to maintain rollout accuracy near 50%, a theoretically justified optimum for learning efficiency. The framework employs multi-round estimation and an prediction model to infer the relationship between hint ratio and accuracy based on Item Response Theory. Experimental results show that SEELE achieves stronger generalization across multiple models and benchmarks, outperforming SFT, GRPO, and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and presents a coherent motivation for the proposed approach.\n2. The choice of maintaining a 50% rollout accuracy is theoretically supported, and the integration of Item Response Theory provides a rigorous and interpretable framework for modeling task difficulty and hint rate.\n3. SEELE achieves consistent performance gains across multiple model families and reasoning benchmarks. The paper also conducts thorough ablation studies on target difficulty levels and multi-round configurations, providing a convincing evaluation of the framework’s design efficacy."}, "weaknesses": {"value": "1. Some notations are confusing\n- The paper does not clearly specify whether $f_{\\phi}$ is a global predictor shared across all instances or a local model optimized separately for each instance.\n- $w$ is not defined in Algorithm 1.\n2. I think the current evaluation setup does not clearly demonstrate the benefits of scaffolding compared with the original GRPO method, as the training datasets are biased toward harder examples. It would be more informative to analyze SEELE’s effectiveness across different difficulty regimes—for example, by training on (1) hard questions with hints, (2) tractable questions that the model can already solve (with or without hints), and (3) a mixed setting: training on datasets with both hard questions and tractable questions. Such a comparison would better clarify the advantages of adaptive scaffolding methods, i.e., learning from harder examples."}, "questions": {"value": "1. Could you clarify the \"j\" in Figure 2?\n2. For the partial solutions used as hints, were only correct reasoning traces included, or were all generated traces (including incorrect ones) used? Additionally, is this the same dataset used in SFT, and how many reasoning traces per question were retained?\n3. What is the computational overhead introduced by the multi-round estimation compared with original GRPO? It would be helpful to include a quantitative comparison showing total wall-clock cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lDIIz0NG3m", "forum": "vy05zizo2h", "replyto": "vy05zizo2h", "signatures": ["ICLR.cc/2026/Conference/Submission5951/Reviewer_3tXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5951/Reviewer_3tXM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679669790, "cdate": 1761679669790, "tmdate": 1762918369433, "mdate": 1762918369433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SEELE is a supervision-aided RLVR framework that fixes a key problem in current RL-with-verifiable-rewards methods: the task difficulty often doesn’t match the model’s current ability, which makes exploration inefficient. SEELE keeps training inside that region by adding hints to each problem and dynamically choosing the hint length per instance so that the task is neither too hard nor too easy. To pick that hint length, it runs multi-round rollouts and fits an item-response-theory (IRT) model on accuracy–hint pairs to predict the best hint for the next round. The authors evaluate the method over several reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces SEELE, a method that leverages instance-level hints to make RLVR exploration more effective.\n2. The method is evaluated on several math-reasoning benchmarks and shows consistent gains even with relatively small models."}, "weaknesses": {"value": "1. The paper should clarify how the hints are produced and whether they risk leaking too much target information, effectively turning the setup into SFT; more analysis of how different hint lengths affect training dynamics would help.\n2. In Figure 4, reward increases while response length drops sharply and accuracy improves only modestly, which suggests the training dynamics may not be fully understood or may be unstable.\n3. The experiments are limited to small models (e.g., Qwen2.5-3B); it would be stronger to show results on larger models (e.g., Qwen2.5-32B or QwQ-32B) to confirm the method scales and is not just fixing small-model artifacts."}, "questions": {"value": "Please refer the limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YGqAKeHOB1", "forum": "vy05zizo2h", "replyto": "vy05zizo2h", "signatures": ["ICLR.cc/2026/Conference/Submission5951/Reviewer_yejB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5951/Reviewer_yejB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979660374, "cdate": 1761979660374, "tmdate": 1762918369172, "mdate": 1762918369172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Seele, a multi-round hint framework, for RL-Finetuning of LLMs, which is adaptable to problem difficulty. The authors begin by providing a theoretical foundation for identifying how rollout accuracy correlates with learning efficiency. They then propose an adaptive multi-round framwork"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretical contribution for relationship between learning efficiency and learning accuracy\n- Empirical evaluation on a wide range of math + general domain reasoning benchmarks."}, "weaknesses": {"value": "- Measuring the difficulty from the average success rate + hinting has been studied in prior work, making the contribution strictly the integration of these approaches, which seems limited from a novelty perspective.\n- Using all mxn rollouts for advantage calculation seems incorrect, in particular for the baseline computation, where the hint is different per round. A value baseline (V(s)) should strictly be a function of the state, which isn't static in this instance, given that the loss is computed only on generated tokens. \n- The design decisions (e.g 3PL and multi-round sampling) aren't independently ablated, making it unclear what the contribution of each component is to the approach"}, "questions": {"value": "- How does the flop equivalent performance compare for Seele vs other approaches due to additional adaptable component contributing to additional computation?\n- How does Seele work for LongCOT models (responses are quite short at 800 tokens) with lower scores on math reasoning benchmarks?\n- See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oCdtFvWWbc", "forum": "vy05zizo2h", "replyto": "vy05zizo2h", "signatures": ["ICLR.cc/2026/Conference/Submission5951/Reviewer_Knso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5951/Reviewer_Knso"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097130047, "cdate": 1762097130047, "tmdate": 1762918368915, "mdate": 1762918368915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}