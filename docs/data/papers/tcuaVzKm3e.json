{"id": "tcuaVzKm3e", "number": 9441, "cdate": 1758122687585, "mdate": 1759897724526, "content": {"title": "Task Vector Bases: A Unified and Scalable Framework for Compressed Task Arithmetic", "abstract": "Task arithmetic, representing downstream tasks through linear operations on task vectors, has emerged as a simple yet powerful paradigm for transferring knowledge across diverse settings. However, maintaining a large collection of task vectors introduces scalability challenges in both storage and computation. We propose Task Vector Bases, a framework compressing $T$ task vectors into $M < T$ basis vectors while preserving the functionality of task arithmetic. By representing each task vector as a structured linear combination of basis atoms, our approach supports standard operations such as addition, negation, as well as more advanced arithmetic ones. The framework is orthogonal to other efficiency-oriented improvements in task arithmetic and can be used in combination with them. We provide theoretical analysis showing that basis compression retains addition generalization guarantees and enables principled unlearning, with error bounds depending on reconstruction quality. Empirically, our proposed basis construction methods consistently outperform heuristic basis construction baselines and, in some cases, even surpass the performance of full task vector collections across diverse downstream applications while reducing storage and computational requirements.", "tldr": "", "keywords": ["task arithmetic", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c77c8f5644e600fa2508090a634578c0f6e76eab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenge of linearly increasing memory and merging costs in task arithmetic as the number of task vectors grows. To overcome this, the authors introduce a compression framework that reduces T task vectors into M task vector bases, while preserving the functional integrity of task arithmetic. The method leverages an autoencoder equipped with softmax activation to ensure that each base vector can be expressed as a convex combination of the original task vectors. Theoretically, the authors prove that arithmetic operations using the compressed vectors retain performance guarantees---up to a constant factor---across addition consistency, out-of-distribution (OOD) generalization, and forgetting resilience. Empirically, the proposed approach achieves task arithmetic performance that matches or exceeds that of random selection and PCA-based compression on a range of vision and NLP tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical relevance of the problem formulation: Storing a full set of task vectors for each task in large-scale models is both memory-intensive and computationally expensive during merging. The desire to perform task arithmetic across multiple tasks with a minimal set of vectors reflects a realistic and pressing need in practical deployment settings. The authors' focus on this challenge is timely and well-motivated.\n\n- Limitations of PCA-based compression and proposed remedy: The paper highlights that conventional PCA compression yields task vector bases that are not expressible as non-negative linear combinations of the original task vectors, thereby breaking compatibility with standard task addition. To address this, the authors propose a solution that preserves compositional compatibility."}, "weaknesses": {"value": "- Why autoencoders?: While the authors rightly note that PCA-based compression does not yield basis vectors that can be expressed as non-negative linear combinations of task vectors, the paper does not clarify why alternative dimensionality reduction techniques with inherent non-negativity constraints were not explored. For example, Nonnegative Matrix Factorization (NMF) is a well-established method that decomposes a non-negative matrix into a product of non-negative basis and coefficient matrices. Applied to the task vector matrix $T$, one could decompose $T$ into $T^+$  (where negative entries of $T$ are set to zero) and $T^-$ (where non-negative entries are set to zero) such that $T = T^+ + T^-$, and then apply NMF separately to $T^+$ and $-T^-$. This would enable reconstruction via non-negative combinations, preserving compatibility with conventional task addition. Without a clear justification for favoring autoencoders over such interpretable and constraint-aligned methods, the choice of architecture appears insufficiently motivated.\n- Reconstruction in task negation: If my understanding is correct, the authors suggest that task negation is not performed directly in the compressed basis space, but instead involves reconstructing the full set of T task vectors prior to executing the forgetting operation. If such reconstruction is deemed acceptable in the context of negation, it raises a key question: why isn’t the same reconstruction strategy employed for task addition? In fact, if reconstruction is permitted, one could simply store PCA-compressed task vectors and reconstruct them at merge time to apply conventional task arithmetic techniques. The paper does not address this inconsistency, leaving a significant conceptual gap in the proposed framework.\n- Experimental configuration of M: In the experiments, the number of compressed bases M is set as a fixed proportion (e.g., 50% or 25%) of the total number of task vectors T. However, in realistic applications where computational and storage resources are limited, it is far more common for M to be fixed independently of T. This makes the experimental design less reflective of practical deployment conditions. Providing results under a fixed M setting regardless of T would offer a more robust and practically relevant evaluation. Demonstrating effectiveness under such constraints would significantly strengthen the credibility of the proposed method.\n- Competition with Parameter-Efficient Fine-Tuning (PEFT): While task arithmetic traditionally involves full-parameter task vectors, recent advances increasingly leverage PEFT methods such as LoRA[1], which offer significant improvements in memory efficiency and merging cost. These approaches may not only be more scalable than full-parameter models but could also outperform the proposed base-compression strategy in both efficiency and task performance. Despite this, the paper lacks a comprehensive comparison between the proposed method and PEFT-based alternatives in terms of memory footprint, merging complexity, and empirical performance.\n\n[1] Zhang, Jinghan, Junteng Liu, and Junxian He. \"Composing parameter-efficient modules with arithmetic operation.\" Advances in Neural Information Processing Systems 36 (2023): 12589-12610."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6DGXUaaC5w", "forum": "tcuaVzKm3e", "replyto": "tcuaVzKm3e", "signatures": ["ICLR.cc/2026/Conference/Submission9441/Reviewer_U1TV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9441/Reviewer_U1TV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760667908686, "cdate": 1760667908686, "tmdate": 1762921040375, "mdate": 1762921040375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "For Future Readers"}, "comment": {"value": "We want to thank all reviewers for questions and comments. After our discussion, we plan to withdraw our paper, but we still want to clarify a few concerns here for future readers:\n\n1. Motivation (yqA6): There are many practical scenarios (supported by U1TV) where we do need to constrain ourselves to a fixed number of task bases, given that:\n    * Even for 8-20 tasks, checkpoint storage and GPU memory cost (line 41-45) for LLMs (Sec F.2) are both nontrivial.\n    * For an online setting (line 92, algorithm 1, Sec 4.1.3), $T$ can grow to infinity and it’s simply impossible to have access to all task vectors in the stream.\n2. Why theory (PS3P): Note that under the framework of Sec 3.2.3, we can **compare the performance of our bases methods and original task arithmetic methods theoretically**. As yqA6 pointed out, our empirical observations later on in Sec.4 (where subsections exactly relate to each of our theorems) align with our theorems. Therefore, Sec 3.2.3 is one of the novel contributions that suits well with the context of this paper.\n3. “Why autoencoders” (U1TV), not Nonnegative Matrix Factorization: We’ve explained the detailed reasons in Sec. A.4. In what follows we provide a high-level summary: For the method that U1TV described, line 924-929 elaborated the details, and roughly speaking, the advantage of our method is **softmax**ed-AE where softmax activation brings \n    * First, the bases-merged model (See Fig.1b) still lives in the original task addition cone, thus not losing too much information after compression. \n    * Second, allows us to guarantee that (line 178) “each basis vector can be interpreted as a **convex combination of input task vectors**,” so in Theorem 3.5 & 3.6 we only see almost no penalty with softmax-AE bases for addition and OOD generalization from compression. Nonnegativity itself is not sufficient, and AE is a basic framework where we can insert additional constraints. **For next steps, we welcome the community to think of data-aware compression methods built on top of our work.**\n4. Why reconstruction in task negation but not in addition (U1TV): (line 107-108) All task vectors must be accessible during task negations (as you need to forget each target task), yet for addition since we only need the multitask capability, maintaining $T$ task is not necessarily needed and could be significantly improved with bases type of methods. Task vector compression is extremely important for online settings as well.\n5. Experiments:\n- Architecture sensitivity (yqA6): For sensitivity of temperature ($\\approx$ sparsity), see Sec E.1.\n- Compute/Storage savings against Accuracy (yqA6): See Figure 2c for results on standard model merging benchmarks.\n- Baselines:\n    * Linearized finetuning (PS3P, yqA6): Note that even in our softmax-AE bases, we do not restrict ourselves to a set of orthogonal bases. More importantly, orthogonality-enforced bases, such as PCA, as we illustrated both conceptually and empirically, are not sufficient to guarantee good task arithmetic performance. We expect the relationship between RandSelect, PCA, and AE will be the same on top of the linearly finetuned task vectors.\n    * PEFT (U1TV): This is similar to sparsity and quantization line of work (line 465-470) where compression is acted along the dimension of $d$ and our work focuses on compressing the dimension of $T$. That said, first, for the online setting (line 92, algorithm 1, Sec 4.1.3) when $T \\to \\infty$, we have to constrain ourselves to a fixed number of bases, thus making comparison with $d$-dim methods unfair. Second, for the offline setting, we can test bases on top of all these $d$-dim methods for further compression.\n    * Other mentioned baselines (yqA6): In Sec. 4, we’ve included TA, TIES, L&S, aTLAS, TSVM, in total 5 methods as the test bed for all bases methods comparison. Note that the last three are more recent methods and L&S achieved the best overall results in a recent LLM model merging benchmark [1]. On standard 8 task vision benchmarks, L&S, aTLAS, TSVM all achieve better performance than the papers [2,3] mentioned by yqA6. [4] is not directly comparable because [4] is evaluated on a 6-task vision benchmark.\n\n[1] He, Yifei, et al. \"MergeBench: A Benchmark for Merging Domain-Specialized LLMs.\" The 39th NeurIPS Datasets and Benchmarks Track (2025).\n\n[2] Ortiz-Jimenez, Guillermo, Alessandro Favero, and Pascal Frossard. \"Task arithmetic in the tangent space: Improved editing of pre-trained models.\" Advances in Neural Information Processing Systems 36 (2023): 66727-66754.\n\n[3] Du, Guodong, et al. \"Parameter competition balancing for model merging.\" Advances in Neural Information Processing Systems 37 (2024): 84746-84776.\n\n[4] Bowen, Tian, et al. \"Beyond task vectors: Selective task arithmetic based on importance metrics.\" arXiv preprint arXiv:2411.16139 (2024)."}}, "id": "cmUkdW1YCw", "forum": "tcuaVzKm3e", "replyto": "tcuaVzKm3e", "signatures": ["ICLR.cc/2026/Conference/Submission9441/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9441/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9441/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763574499708, "cdate": 1763574499708, "tmdate": 1763574499708, "mdate": 1763574499708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper advances Task Arithmetic (Ilharco et al., 2023) by proposing an orthogonal framework aimed at reducing storage and computational costs of task vectors. For achieving their goal, the authors propose to learn a basis of $M \\leq T$ from a set of the original $T$ task vectors, using a linear autoencoder with softmax activation. The authors provide both empirical and theoretical justifications to support their design choices, offering bounds on error incurred from compression. The proposed framwork is tested on offline and online Task Arithmetic test beds on both vision and language domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The positioning of the paper in literature is clear, offering a framework that tackles a novel problem.\n- The method is compelling, as it is solidly grounded in theory, allowing for using classical results from spectral analysis.\n- Considering the constraints of the problem, the experimental validation is extensive (also, it is very useful that the authors compared with simple PCA) and the results are promising.\n- The paper is well-written, the figures/plots help the narrative and understanding of the results. Finally, the mathematical notation is clear."}, "weaknesses": {"value": "Overall, the paper is solid. However, in light of recent advances in Task Arithmetic, the following points require some attention:\n\n**W1.**\n\n[1,2,3,4] prove and support the idea that linearization around the pre-trained parameters is the key enabler of proper Task Arithmetic. The intuition is that, when task vectors implement functions that are linear w.r.t. the weights of the model, the composed model will act as a linear combination of orthogonal functions (thus, allowing for minimal interference across task vectors eg. see Fig. 2 of [4] or Fig. 3 of [3]).\n\nBeing these vectors orthogonal by design (when tasks are statistically different), they should already form a basis by themselves, without any need for the proposed methodology: compressing linearized task vectors, intuitively, simply reduces to discarding some task vectors that, possibly due to tasks being very similar (in a statistical sense), will approximately point in the same direction in parameter space.\n\nIn essence, it could be interesting to observe what happens when compressing task vectors obtained from linearized fine-tuning [1].\n\n-------------------\n\n**W2.**\n\nI'm finding Section 3.2.3 weakly linked to the rest of the paper: up until the end of Section 3.2.1 the narrative focuses on basis arithmetic, reconstruction errors of your framework etc. Then, abruptly the paper presents some results regarding multi-task/OOD loss bounds. I'd suggest the authors to strengthen the connection with the previous section and why these bounds matter.\n\nTo be honest, I'm not even sure the theorems/bounds of this section are needed for the narrative of your paper, as they partially overlap with already established literature which proves slightly stronger results (eg. see [5], which also characterizes the second-order term of the Taylor expansion, or [6] which characterizes task addition, negation and OOD generalization with non-linear transformers under the light of task alignment). I'd suggest to either remove this part, or clearly highlight the contribution and novelty of your theoretical results.\n\n-------------------\n\n**Minor Weaknesses.**\n- I would reframe your contributions: the first three read as a single contribution (i.e. your framework).\n- Before/After Theorem 3.4 (LL204-206), I'd suggest to add a pointer to the proof.\n\n-------------------\n\n**_References:_**\n\n[1] Ortiz-Jimenez, Guillermo, Alessandro Favero, and Pascal Frossard. \"Task arithmetic in the tangent space: Improved editing of pre-trained models.\" NeurIPS 2023.\n\n[2] Jin, Ruochen, et al. \"Fine-tuning attention modules only: Enhancing weight disentanglement in task arithmetic.\" ICLR 2025.\n\n[3] Iurada, Leonardo, Marco Ciccone, and Tatiana Tommasi. \"Efficient model editing with task-localized sparse fine-tuning.\" ICLR 2025.\n\n[4] Tang, Anke, et al. \"Parameter efficient multi-task model fusion with partial linearization.\" ICLR 2024.\n\n[5] Porrello, Angelo, et al. \"A second-order perspective on model compositionality and incremental learning.\" ICLR 2025.\n\n[6] Li, Hongkang, et al. \"When is task vector provably effective for model editing? a generalization analysis of nonlinear transformers.\" ICLR 2025."}, "questions": {"value": "Thanking in advance for their response, I'd kindly invite the authors to address the points raised in the Weaknesses section of this review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zntfGvc1Xq", "forum": "tcuaVzKm3e", "replyto": "tcuaVzKm3e", "signatures": ["ICLR.cc/2026/Conference/Submission9441/Reviewer_PS3P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9441/Reviewer_PS3P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760963219348, "cdate": 1760963219348, "tmdate": 1762921039449, "mdate": 1762921039449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a task-vector–basis framework that compresses a large set of task vectors (T) into a small set of basis vectors (M) while attempting to preserve “task arithmetic” (addition/subtraction) semantics. The core mechanism is a linear autoencoder whose coefficient layer uses a softmax to produce simplex-constrained combinations, encouraging faithful reconstruction of the original task-vector geometry. The authors argue that PCA distorts the functional directions underlying task arithmetic and motivate their design with a geometric analysis. Empirically, the method shows advantages over simple baselines at higher compression ratios and evaluates both additive and subtractive compositions, in partial agreement with the theory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear diagnosis of PCA’s limitations. The geometric argument for why PCA fails to preserve task-arithmetic semantics is insightful and well-motivated, and it naturally motivates the proposed approach.\n2. Solid results at higher compression. When compressing a larger collection of task vectors, the method outperforms simple baselines; these observations align with the authors’ theoretical analysis.\n3. Covers both addition and subtraction. Evaluating both additive and subtractive task arithmetic offers a more complete view of functionality preservation."}, "weaknesses": {"value": "1. Questionable practical motivation for very large T. A major stated goal is reducing storage overhead for many task vectors. Yet in practice, the quality of simple task-vector merging typically degrades rapidly as T grows; much prior work evaluates in the ~8–20 range. If real deployments rarely maintain very large libraries of task vectors due to performance collapse, the storage-savings motivation is less compelling without concrete use cases.\n2. Limited baselines. The main experiments compare against only three Task-Vector variants. Recent, stronger baselines—e.g., Parameter Competition Balancing for Model Merging [1], Task Arithmetic in the Tangent Space [2], and Beyond Task Vectors: Selective Task Arithmetic [3]—report substantially better results than the original Task Vector. Omitting them weakens the empirical claim.\n3. Architecture sensitivity is unclear. The method relies on a softmax-activated linear autoencoder to form bases. There is no ablation on the autoencoder design (e.g., softmax vs. simplex constraints via projection, temperature, coefficient sparsity, nonlinearity), so robustness to architectural choices is unknown.\n4. Compute/storage savings not quantified. The paper lacks concrete measurements of wall-clock speed, memory footprint, and I/O savings attributable to compression. Without numbers (for, say, T=100 and T=1000), it is difficult to evaluate the cost-effectiveness of the approach.\n5. Non-trivial performance loss on many datasets. In several cases, the compressed basis representation trails the original Task Vector approach. The paper should analyze whether the accuracy drop is acceptable relative to storage/computation gains, ideally with a Pareto plot and quantified trade-offs.\n\nReferences:\n[1] Parameter Competition Balancing for Model Merging.\n[2] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models.\n[3] Beyond Task Vectors: Selective Task Arithmetic Based on Importance Metrics."}, "questions": {"value": "1. What concrete deployment scenarios require large T where storage dominates yet task arithmetic remains useful?\n2. How does the method compare to stronger baselines under their setups?\n3. How sensitive are results to the autoencoder design (softmax temperature, sparsity, projection)?\n4. What are the exact storage/compute savings and the accuracy–compression Pareto curves?\n5. Where does it fail most, and why (e.g., under-represented directions, bias toward majority tasks)?\n\nf the authors satisfactorily address these issues, I would raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z9MZqBgE9M", "forum": "tcuaVzKm3e", "replyto": "tcuaVzKm3e", "signatures": ["ICLR.cc/2026/Conference/Submission9441/Reviewer_yqA6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9441/Reviewer_yqA6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975810216, "cdate": 1761975810216, "tmdate": 1762921039138, "mdate": 1762921039138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}