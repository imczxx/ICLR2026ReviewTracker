{"id": "rUtESaVvG8", "number": 8879, "cdate": 1758100931696, "mdate": 1763216131813, "content": {"title": "Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers", "abstract": "Model inversion is a widely adopted technique in data-free learning that reconstructs synthetic inputs from a pretrained model through iterative optimization, without access to original training data. Unfortunately, its application to state-of-the-art Vision Transformers (ViTs) poses a major computational challenge, due to their expensive self-attention mechanisms. To address this, \\textit{Sparse Model Inversion} (SMI) was proposed to improve efficiency by pruning and discarding seemingly unimportant patches, which were even claimed to be obstacles to knowledge transfer. However, our empirical findings suggest the opposite: even randomly selected patches can eventually acquire transferable knowledge through continued inversion. This reveals that discarding any prematurely inverted patches is inefficient, as it suppresses the extraction of class-agnostic features essential for knowledge transfer, along with class-specific features. In this paper, we propose \\textit{Patch Rebirth Inversion} (PRI), a novel approach that incrementally detaches the most important patches during the inversion process to construct sparse synthetic images, while allowing the remaining patches to continue evolving for future selection. This progressive strategy not only improves efficiency, but also encourages initially less informative patches to gradually accumulate more class-relevant knowledge, a phenomenon we refer to as the \\textit{Re-Birth} effect, thereby effectively balancing class-agnostic and class-specific knowledge. Experimental results show that PRI achieves up to 10$\\times$ faster inversion than standard \\textit{Dense Model Inversion} (DMI) and 2$\\times$ faster than SMI, while consistently outperforming SMI in accuracy and matching the performance of DMI.", "tldr": "We propose Patch Rebirth Inversion (PRI), which accelerates ViT inversion by allowing patches to continually evolve and accumulate transferable knowledge, rather than being prematurely discarded.", "keywords": ["Model inversion", "Data-free learning", "Vision Transformer"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7286da09ccc3821d013663164d8dd302de14f28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed PRI (Patch Rebirth Inversion): During the inversion process of the ViT model, the most important patches are extracted in stages to form a sparse image and saved, and the remaining patches are continued to be inverted for gradual rebirth, so that one inversion trajectory can generate multiple sparse samples, taking into account both efficiency and transferability; in the two major tasks of quantization and distillation, it is faster and more accurate than SMI and closer to DMI"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. A novel and simple approach: using attention scores to estimate patch importance, but rather than discarding low-scoring patches, PRI performs potential rebirth (further optimization) to ultimately produce multiple, non-overlapping, sparse images. The implementation includes clear pseudocode.\n\n2. Excellent efficiency and effectiveness:\n\n   - Efficiency: On DeiT, PRI improves throughput by up to 2 $\\times$ compared to SMI and up to 10 $\\times$ compared to DMI; it also significantly reduces FLOPS and memory usage.\n   \n   - Quantization and Distillation: Quantization Attempts (QAT) outperform DMI at moderate sparsity and SMI at high sparsity; distillation outperforms SMI overall, approaching DMI.\n\n3. Strong evidence of transferability: Students in single-class inversion experiments generalize to all classes under PRI; the confidence distribution is closer to real data, showing a trend from class-independent to class-dependent.\n\n4. Thorough theoretical analysis: A cost comparison between SA and FFN is presented, demonstrating that under the common ViT setting of $$N/D<3$$, PRI outperforms both SA and FFN, as well as DMI and SMI."}, "weaknesses": {"value": "1. When comparing complexity, the SMI was set to an ideal state (assuming pruning was completed once before inversion), which is slightly different from the actual situation.\n\n2. The article uses attention scores as the sole criterion for determining important patches. Are there other more robust criterion?\n\n3. Why is it that after removing important patches, the remaining patches are more likely to synthesize class-related features? Is there any further research on this?\n\n4. Typo: Line 363 should indicate GPU usage, not usuage."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D5tXpAfjAf", "forum": "rUtESaVvG8", "replyto": "rUtESaVvG8", "signatures": ["ICLR.cc/2026/Conference/Submission8879/Reviewer_UkW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8879/Reviewer_UkW6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761332773573, "cdate": 1761332773573, "tmdate": 1762920638336, "mdate": 1762920638336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the efficiency of reconstructing synthetic inputs from a pretrained model (i.e., model inversion). Specifically, the authors prune and discard unimportant patches for acceleration and they propose Patch Rebirth Inversion (PRI) that incrementally detaches the most important patches. Experimental results show that PRI achieves much faster inversion than standard Dense Model Inversion (DMI)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. The authors reveal the phenomenon that even randomly selected patches can eventually acquire transferable knowledge through continued inversion.\n3. The proposed PRI method achieves faster inversion than both dense inversion and previous sparse inversion methods."}, "weaknesses": {"value": "Please refer to Questions."}, "questions": {"value": "Training efficiency and accuracy are difficult to balance, and I have several questions regarding this:\n\n1. Regardless of the sparsity level or sparsity method used during training, if training time is sufficiently long, will the final student model accuracy always converge to the same level? In other words, if we disregard training time, do different sparsification methods fundamentally affect the ultimate accuracy?\n\n2. For both SMI and PRI, if we fix the total training time, could we achieve better performance by training at a higher sparsity level (e.g., 75%) for three times longer compared to a lower sparsity level (e.g., 25%)? Under such a constraint, what is the optimal trade-off between sparsity level and training duration? Specifically, can increasing the sparsity level—thereby allowing more training iterations within the same time budget—lead to higher final accuracy?\n\n3. In Table 3(b), the advantage of PRI over SMI appears more pronounced than in Table 3(a). What causes this difference? For SMI in Table 3(b), would further increasing the training time improve its accuracy? If not, why not? If yes, approximately how much more training time would be needed for SMI to reach the same accuracy as PRI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hCpJfwXrEI", "forum": "rUtESaVvG8", "replyto": "rUtESaVvG8", "signatures": ["ICLR.cc/2026/Conference/Submission8879/Reviewer_pfYh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8879/Reviewer_pfYh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551372387, "cdate": 1761551372387, "tmdate": 1762920637898, "mdate": 1762920637898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of model inversion for vision transformer (ViT) architectures in data-free settings. Their main contributions are:\n\n* The authors observe that while model inversion (i.e., reconstructing synthetic inputs from a pretrained model without original training data) is well studied for CNNs, applying it to modern ViTs is challenging due to expensive self-attention layers and patch-based architecture. They note that prior works (e.g., sparse model inversion) prune away patches during inversion assuming they are unimportant, but this may hinder transferability. \n\n* The authors show that patches which seem unimportant early in inversion can later acquire transferable knowledge if allowed to continue evolving (rather than being discarded). In other words, prematurely discarding patches reduces the ability to capture class-agnostic and class-specific features. \n\n* They propose a new inversion algorithm that incrementally detaches the most “important” patches during inversion (i.e., progressively freeze or remove them) but allows the remaining patches to continue optimizing (“rebirth”) and accumulate knowledge for future selection. This balances between focusing on high-utility patches and giving less-useful ones a chance to evolve.\n\n* They demonstrate empirically that PRI achieves significant speed-ups (e.g., up to ~10× faster than standard Dense Model Inversion, and ~2× faster than prior Sparse Model Inversion) while maintaining or improving inversion quality (in terms of downstream transferability of the synthetic data) on ViTs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* As vision transformers become ubiquitous, methods for data-free learning, model inversion, and knowledge transfer for ViTs are increasingly important. This paper addresses a timely gap.\n\n\n* The “Re-Birth” effect of patches is an interesting observation, challenging the assumption that low-importance patches early on can simply be discarded. This insight adds value beyond just proposing another method.\n\n\n* The design of PRI — progressively detaching important patches, allowing the rest to evolve — is well motivated and intuitively appealing for patch-based architectures.\n\n\n* The reported speed-ups (10× vs. dense inversion, 2× vs. sparse inversion) plus maintenance or improvement of transfer performance is a compelling result that suggests real practical value."}, "weaknesses": {"value": "* The method relies on identifying which patches are “important” and detaching them during the process. It would help to see more detailed justification of how importance is measured, how thresholds are set, and how robust this is across architectures/datasets.\n\n\n* The experiments appear focused on vision transformers; it is less clear how well the method would extend to other modalities (e.g., NLP transformers) or hybrid architectures. The paper could elaborate on this.\n\n\n* While the Re-Birth effect is interesting, it would be beneficial to provide more analysis/visualisation of how patches evolve, what kinds of features they capture, and how that relates to transferability. Without that, the claim is less grounded."}, "questions": {"value": "* How exactly do you measure patch “importance” in your method? Is it based on attention scores, gradient magnitude, feature magnitude, or another metric? How sensitive is the performance to the choice of this metric?\n\n\n* What are the hyperparameters governing when and how many patches to detach in each iteration? Did you perform ablation studies on these choices? How do different schedules (e.g., detach early vs later) affect both speed and quality?\n\n\n* What downstream tasks did you evaluate the synthetic images on (e.g., fine‐tuning, zero‐shot, domain adaptation)? How much of the transfer gain comes from class‐agnostic features vs. class‐specific features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Ok8L7eRjc", "forum": "rUtESaVvG8", "replyto": "rUtESaVvG8", "signatures": ["ICLR.cc/2026/Conference/Submission8879/Reviewer_UtL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8879/Reviewer_UtL8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669925509, "cdate": 1761669925509, "tmdate": 1762920637468, "mdate": 1762920637468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper challenges the core assumption of Sparse Model Inversion (SMI) by showing that even randomly selected patches can acquire transferable knowledge through continued inversion. This motivates Patch Rebirth Inversion (PRI), a progressive strategy that:\n- Dynamically detaches high-importance patches for sparse synthetic image construction .\n- Keeps remaining patches evolving to accumulate class-relevant knowledge (the Re-Birth effect)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Challenging SMI dogma: Demonstrates that discarding patches early is harmful, and even randomly selected patches can acquire transferable knowledge with continued optimization.  \n\n- This Re-Birth effect is novel, well-motivated, and empirically substantiated — a genuine contribution to understanding inversion dynamics in ViTs."}, "weaknesses": {"value": "(1) I'm not sure if it's fair to make such a cross-sectional comparison between PRI and Sparse Model Inversion (SMI).\nPRI allows patches to continue evolving after initial selection, effectively granting it more optimization steps per patch than SMI, which discards low-importance patches permanently. This violates the core constraint of SMI: fixed sparsity throughout inversion.  \nbaseline would require:  \n- SMI with re-selection or reinitialization of discarded patches (currently absent)  \n- Or PRI with hard sparsity constraints (fixed patch set)\nWithout such controls, PRI’s efficiency advantage is confounded by increased computational flexibility.  \n\n(2) PRI reports results at 50% (v=2), 75% (v=4), 86% (v=7) — but v is undefined in the table.\n\n(3) The negligible gains on ImageNet-1K (Table 3 a: +0.07 to +0.87 at matched sparsity) — while dramatic on CIFAR-100 distillation (Table 3 b: +6–19 pts) — reveal a critical scaling limitation of PRI."}, "questions": {"value": "- How many total gradient steps are allocated to each patch across PRI vs. SMI?  For PRI: Do evolving patches receive additional updates beyond the initial fixed budget of SMI? If so, report per-patch update counts and total compute (FLOPs) for both methods.\n- At convergence, what is the final active patch ratio in PRI? Is it strictly comparable to SMI’s fixed sparsity (e.g., 50%)? \n- Compare PRI against an SMI variant with periodic re-ranking: every k steps, re-evaluate patch importance and reinstate top N (same budget as PRI’s dynamic set).  Does this closed-loop SMI close the gap in accuracy and speed?\n- Show learning curves per patch type (detached early, late, or never) in PRI. Do randomly initialized “reborn” patches truly catch up in class relevance, or do gains come from over-optimizing a smaller elite set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cR5o385Izm", "forum": "rUtESaVvG8", "replyto": "rUtESaVvG8", "signatures": ["ICLR.cc/2026/Conference/Submission8879/Reviewer_XhHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8879/Reviewer_XhHS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222127119, "cdate": 1762222127119, "tmdate": 1762920636964, "mdate": 1762920636964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all four reviewers for reading our work so carefully. We addressed every question with detailed explanations and new experiments, and we added the necessary analyses to the Appendix to make our reasoning clear. We also appreciate the overall positive reception reflected in the reviews, which was very encouraging for us. We hope that our clarifications and additional results help all reviewers feel more confident about the contribution of this work."}}, "id": "ZJUYcEBR3Y", "forum": "rUtESaVvG8", "replyto": "rUtESaVvG8", "signatures": ["ICLR.cc/2026/Conference/Submission8879/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8879/Authors"], "number": 20, "invitations": ["ICLR.cc/2026/Conference/Submission8879/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763229185357, "cdate": 1763229185357, "tmdate": 1763229185357, "mdate": 1763229185357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}