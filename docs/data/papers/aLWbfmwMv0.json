{"id": "aLWbfmwMv0", "number": 20285, "cdate": 1758304454128, "mdate": 1759896986306, "content": {"title": "Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces", "abstract": "Recent text-only models demonstrate remarkable mathematical reasoning capabilities. Extending these to visual domains requires vision-language models to translate images into text descriptions. However, current models, trained to produce captions for human readers, often omit the precise details that reasoning systems require.\nThis creates an interface mismatch: reasoners often fail not due to reasoning limitations but because they lack access to critical visual information.\nWe propose Adaptive-Clarification Reinforcement Learning (AC-RL), which teaches vision models what information reasoners need through interaction. Our key insight is that clarification requests during training reveal information gaps; by penalizing success that requires clarification, we create pressure for comprehensive initial captions that enable the reasoner to solve the problem in a single pass.\nAC-RL improves average accuracy by $4.4$ points over pretrained baselines across seven visual mathematical reasoning benchmarks, and analysis shows it would cut clarification requests by up to $39$\\% if those were allowed.\nBy treating clarification as a form of implicit supervision, AC-RL demonstrates that vision-language interfaces can be effectively learned through interaction alone, without requiring explicit annotations.", "tldr": "We use clarification-aware reinforcement learning to post-train vision-language models, teaching them to communicate effectively with downstream reasoning systems.", "keywords": ["Vision Language Models", "Reinforcement Learning", "Post-Training", "Modular AI"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95851623707bdc3da428147c8226e04efe8eaef5.pdf", "supplementary_material": "/attachment/e1649517c8260f9dfb11929cf454b9595ffcce1a.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the \"interface mismatch\" in decoupled vision-reasoning systems, where VLM-generated captions often lack the specific details required by downstream text-only reasoners. The authors propose Adaptive-Clarification Reinforcement Learning (AC-RL), which uses a \"training scaffold\" where a reasoner can request clarification. The core idea is a tiered reward structure. The authors claim this reward penalty creates optimization pressure for the VLM captioner to \"front-load\" critical information into its initial description."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies the potential of introducing dense reward and clarification in the reasoning process.\n\n2. The paper include analysis to support how the proposed mechanism works."}, "weaknesses": {"value": "1. Unjustified Premise and Limited Scope: The paper's core premise—that multimodal reasoning is best solved by decoupling perception (a captioner) from reasoning (a text-only LLM)—is presented without justification. This is a counter-intuitive paradigm and not widely adopted as for recent state-of-the-art VLMs, as it assumes complex visual problems can be 'flattened' into a single text pass. This lack of justification is compounded by the paper's narrow benchmark focus. The success on math-heavy tasks suggests this method primarily optimizes visual data extraction (of text, numbers, facts) rather than holistic perceptual reasoning. It is thus narrowing the generalizability of the proposed approach and is highly questionable if this approach would work on perception-intensive tasks (e.g., visual search, complex spatial relations) where iterative grounding with the image is required.\n\n2. The current manuscript suffers from presentation and clarity issues, requiring the reader to make guesses when reading.\n\n- Undefined Jargon: The paper uses terms like \"GRPO-style optimization\" (Section 4.1) without definition or citation. Other core concepts, like \"BNPO\" (Section 3.4), are cited but without even an intuitive explanation, forcing the reader to consult external papers to understand the methodology.\n\n- Inferential Leaps: The analysis makes claims that are not fully supported by the data. For instance, in Section 4.3, the data in Table 5 shows that clarification requests from the AC-RL model are more critical; the paper leaps to the conclusion that this proves the model \"learns to distinguish between recoverable and irrecoverable information gaps,\" which is a strong claim for which other explanations exist (e.g., it simply fails to front-load the most difficult facts, which are by definition critical).\n\n- Overstated Conclusions: The conclusion generalizes the findings from math benchmarks to \"any modular architecture,\" which is a significant overstatement of what has been demonstrated.\n\n3. Missing Ablation on Key Hyperparameter ($\\alpha$): The partial reward value $\\alpha=0.7$ is central to the entire method, but it is presented without any justification or sensitivity analysis. How was this value chosen? How does performance vary with a stronger penalty (e.g., $\\alpha=0.3$) or a weaker one ($\\alpha=0.9$)? The lack of an ablation study on this critical hyperparameter makes the results difficult to interpret.\n\n4. Risk of Overfitting to the Reasoner: The captioner is trained to adapt to the information needs of a single, frozen reasoner (DeepSeek-R1-Qwen-3B). It is an open question whether the learned captioning policy is generically better or simply \"overfit\" to the specific quirks and biases of this one reasoner. The claims would be much stronger if the authors showed that the AC-RL-trained captioner also improves performance when paired with a different, unseen reasoner at inference time."}, "questions": {"value": "1. Can you please justify the choice of a decoupled paradigm over a standard end-to-end VLM? Given the counter-intuitive nature of this approach, what evidence suggests it is a necessary or superior path for multimodal reasoning?\n\n2. Could you please define terms like \"GRPO-style optimization\" and briefly explain the mechanics of \"BNPO\" as they relate to your method?\n\n3. Could you please provide a sensitivity analysis or ablation study for the clarification reward hyperparameter $\\alpha$? How does performance change as this value is varied?\n\n4. Can you elaborate on the \"negative clarification gap\" in Table 4? Why exactly does allowing clarification hurt the AC-RL model on MathVerse? Does this suggest the frozen clarification model ($\\pi_{ref}$) is providing low-quality or conflicting information, and if so, how does that affect the training signal?\n\n5. Related to the WeMath regression (Table 1) and the subject-level analysis (Figure 2), do you see evidence of a trade-off? Does the optimization pressure to \"front-load\" quantitative and geometric details (where AC-RL excels) come at the cost of performance on other types of reasoning (e.g., logic, descriptive)?\n\n6. How well does the AC-RL-trained captioner generalize? If you were to swap the DeepSeek-based reasoner for a different frozen reasoner (e.g., one based on Llama 3 or Claude 3) at inference time, does the improved captioning policy still provide a performance benefit?\n\n7. Given that the decoupled method excels on tasks requiring visual data extraction (geometry, algebra), how do you expect this approach would perform on benchmarks that are more perception-intensive (e.g., visual search, fine-grained attribute recognition, or abstract spatial reasoning) which may not be solvable by a single, 'front-loaded' text description?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WniynZE4T", "forum": "aLWbfmwMv0", "replyto": "aLWbfmwMv0", "signatures": ["ICLR.cc/2026/Conference/Submission20285/Reviewer_JaUc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20285/Reviewer_JaUc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992753417, "cdate": 1761992753417, "tmdate": 1762933757557, "mdate": 1762933757557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive-Clarification Reinforcement Learning (AC-RL), a framework that teaches vision-language models to generate reasoner-aligned captions by treating clarification requests as implicit supervision. The model uses a tiered reward structure that penalizes reliance on clarifications during training, encouraging more information-rich initial captions for single-pass inference. Empirical results on seven math reasoning benchmarks show that AC-RL achieves consistent gains over several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach of using clarification as supervision through multi-turn interaction is novel and well-motivated.\n2. AC-RL consistently outperforms baseline methods (binary-reward RL and decoupled architectures) under single-pass inference on extensive mathematical reasoning benchmarks, confirming effectiveness without adding inference cost."}, "weaknesses": {"value": "[Major Weakness]\n1. Tab. 4 shows AC-RL performs worse under clarification-enabled evaluation on MathVerse_MINI, compared to single-pass (34.26% vs 36.80%), which is counter-intuitive since more interactions should help, but the existing provided justification (i.e., additional clarification introduces noise to well-aligned caption) is not convincing enough, given the slight improvement on MathVision and the fact that the reasoner knows to stop requesting once it is satisfied.\n2. The qualitative comparison of the generated captions from the captioner before and after RL is missing, which helps to illustrate how AC-RL improves beyond binary reward RL.\n3. Training logs that track the frequency of clarification requests from the reasoner (e.g., plotted over every 100 steps) should be provided to show how the policy adapts over time.\n\n[Minor Weakness]\n1. Additional analyses are potentially insightful, and thus recommended to provide to benefit the community:\n     - A. Reporting clarification-enabled performance on additional benchmarks (e.g., DynaMath and LogicVista), along with a cost/benefit analysis of extra interaction rounds, would provide a more complete evaluation.\n     - B. Plotting accuracy as a function of the number of clarification rounds (and observing whether the curve saturates) would help clarify how much clarification is actually beneficial.\n2. The definition of $Acc_{deny}$ in Tab. 5 and the calculation process of obtaining it is unclear, which should be clearly stated.\n3. Tab. 1 lists \"Qwen-3B\" which is a text-reasoning model, not a VLM, making it unclear how visual reasoning results are obtained for this configuration."}, "questions": {"value": "1. Will the codebase be made public to the community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WVYjpAQs6y", "forum": "aLWbfmwMv0", "replyto": "aLWbfmwMv0", "signatures": ["ICLR.cc/2026/Conference/Submission20285/Reviewer_cc99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20285/Reviewer_cc99"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995487559, "cdate": 1761995487559, "tmdate": 1762933756674, "mdate": 1762933756674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AC-RL, a method that trains a caption model to generate visual captions that contains information needed for the accompanying reasoning model to answer the question correctly. The core idea of the paper is to assign partial rewards to captions that gets the correct answer by asking additional clarification questions. This will make the reward denser and encourage the captioner to generate more detailed caption with the necessary information needed for the reasoning model. The paper conducts experiments on multi-modal reasoning benchmarks using InternVL2B model and Qwen3B model as the captioning model and the text only deepseek model as the reasoning model and shows that such training enables the reasoning model to better answer the question."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a new method to more effectively train a captioning model to output the information needed for a reasoning model by assigning partial reward to a caption when the model can use the caption to answer the question correctly with clarification. \n- The paper demonstrates the effectiveness of their method on multiple math reasoning benchmarks using 2 different models. The performance does show gain over baseline methods."}, "weaknesses": {"value": "- the hyper parameter alpha here is arbitrarily chosen. \n- The reward design is problematic, the method is assigning partial rewards to the cases where the reasoning model decides to ask for clarification and then gets the correct answer, and assigns zero reward when the reasoning model gets the incorrect answer no matter what the caption model generates. However, this reward design is not suitable to train the caption model, because when the answer is wrong, it could be that (1) the caption is not good enough (2) the caption is already good but the reasoner simply can't solve the problem since the problem is too hard. Assigning zero could be wrongly penalizing a good caption.\n- The reward assignment using clarification is not well motivated. The reasoner model can always chose to ask for clarification when the caption is not detailed enough (eg, an empty string), and in such case, whether the caption gets penalized or rewarded solely depends on the capacity of the reasoner model and the clarification model, not from the behavior of the caption model itself. This does not really make sense.\n- The caption model still receives a sparse reward and not any feedback from the clarification process. I think it would make more sense to have the caption model refine their caption based on the clarification process (eg, what questions are asked) since this is an important signal from the reasoner model."}, "questions": {"value": "- How is the hyper-parameter alpha chosen here in this paper?\n- Could the author give an explanation/consideration of the design choices? See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lPojiCpXux", "forum": "aLWbfmwMv0", "replyto": "aLWbfmwMv0", "signatures": ["ICLR.cc/2026/Conference/Submission20285/Reviewer_5pEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20285/Reviewer_5pEc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154420166, "cdate": 1762154420166, "tmdate": 1762933756130, "mdate": 1762933756130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive-Clarification Reinforcement Learning (AC-RL), a reinforcement learning framework designed to improve the alignment between vision-language models (captioners) and downstream reasoning systems. The key idea is to use clarification requests during training as implicit supervision, encouraging the captioner to generate more comprehensive initial descriptions that reduce the need for multi-turn interaction. The method is evaluated on seven visual mathematical reasoning benchmarks, where it improves average accuracy by up to 4.4 points and reduces clarification dependency by up to 39%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The tiered reward structure based on clarification need is a clever and intuitive way to address sparse reward challenges in RL. It provides richer training signals and encourages the model to anticipate and preemptively address the reasoner’s information needs.\n- The paper includes extensive experiments across seven diverse mathematical reasoning benchmarks, with comparisons to strong proprietary and open-weight models. Ablation studies and behavioral analyses provide strong empirical support for the method’s effectiveness.\n- The framework is designed for single-pass inference, making it highly applicable to real-world systems where multi-turn interaction is infeasible. The use of frozen reasoners and reference models during training also ensures modularity and compatibility with existing architectures."}, "weaknesses": {"value": "- The method assumes a frozen, fixed reasoner during training. This limits the framework’s adaptability in scenarios where the reasoner is updated or fine-tuned, which is common in practice. Co-adaptation between the captioner and reasoner is not explored, potentially leaving performance gains on the table.\n- The reliance on structured clarification may not transfer well to less formal or more open-ended tasks.\n- The choice of the partial reward value (α = 0.7) is not thoroughly justified or ablated. The performance may be sensitive to this hyperparameter, and its optimal value could vary across tasks or reasoners. A sensitivity analysis would strengthen the robustness claim.\n- The clarification responses during training are generated by a frozen reference model, which may limit the quality and diversity of the supervision signal."}, "questions": {"value": "Have you experimented with non-mathematical vision-language tasks (e.g., VQA)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bDJDD9Vdom", "forum": "aLWbfmwMv0", "replyto": "aLWbfmwMv0", "signatures": ["ICLR.cc/2026/Conference/Submission20285/Reviewer_13NB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20285/Reviewer_13NB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192491630, "cdate": 1762192491630, "tmdate": 1762933755846, "mdate": 1762933755846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}