{"id": "4RD7LzmP6l", "number": 19404, "cdate": 1758295943496, "mdate": 1759897041019, "content": {"title": "AnimalGS: 4D Animal Reconstruction from Monocular video with 3D Gaussian Splatting", "abstract": "Reconstructing 4D animals from monocular videos is challenging due to large inter-species variation, complex articulations, and the lack of reliable templates. \nWe introduce AnimalGS, a test-time optimization framework built on a 3D Gaussian Splatting representation for high-fidelity 4D reconstructions from single videos. \nGrounded in the insight that robust reconstruction emerges from pose-guided optimization rather than strict shape priors, AnimalGS treats priors as coarse initializations and integrates joint-aware and symmetry-aware designs to progressively disentangle motion and appearance. This leads to empirically strong generalization across diverse species and robustness to mismatching with shape priors. \nExtensive experiments demonstrate the superior performance of our approach in geometry, motion, and temporal consistency across a wide variety of animal species.", "tldr": "A 4D reconstruction method from monocular video for animals", "keywords": ["Animal; 3D Gaussian Splatting; 4D Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f88ab50c4676215e6bbb9989fb31887970a94f38.pdf", "supplementary_material": "/attachment/153d896b918d20806fbff44c47d4f23092ec9494.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents AnimalGS, a test-time optimization method for dynamic 3d animal reconstruction from a video, using 3DGS as the geometry representation.\n\nIt starts with a coarse shape from the 1st image by Fauna (Li et at 2024) and performs two-stage optimization: a pose refinement step using learnable joint anchors and symmetry augmentation, followed by pose-guided non-rigid deformation represented as Gaussian offsets.\n\nIt produces good 4D reconstructions across a variety of animal species, outperforming state-of-the-art methods such as GART, D-3DGS, and GVF-Diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is technically solid and works better than the referenced existing works.\n  - The use of symmetry prior by augmentation is interesting, although related ideas has been used in prior methods.\n- Thorough evaluation that includes quantitative results and user studies.\n  - The authors assembled a dataset of 87 monocular videos and reported both input and novel view metrics.\n- The dynamic 3DGS representation allows fast rendering and interactive visualization."}, "weaknesses": {"value": "- The paper has limited conceptual difference compared to existing literature. Test-time optimization, LBS-based deformation, and symmetry prior, and skinned 3DGS, are largely extensions of existing works. \n- The improvement from introduced components (e.g., Tab 3) are not that significant.\n- The visual results are not great. The reconstruction suffer from blurriness and appearance artifacts.\n- Missing baseline and related work: it seems DreamMesh4d has more appealing result and should be compared against.\n\n[A] DreamMesh4D. https://arxiv.org/abs/2410.06756"}, "questions": {"value": "- It would help if the authors could highlight the major difference compared to existing works in writing, such as LASR and DreamMesh4d.\n- Compared to symmetry prior, image-to-3d models provides a stronger 3d prior and could help disentangle shape and motion.\n- For evaluation, I found the current NVS metrics not convincing as those are distribution-level. Multiview dataset such as \"Artemis: Articulated Neural Pets with Appearance and Motion Synthesis\" can be an alternative to measure novel view view synthesis scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nTQx9XNjNJ", "forum": "4RD7LzmP6l", "replyto": "4RD7LzmP6l", "signatures": ["ICLR.cc/2026/Conference/Submission19404/Reviewer_GMuu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19404/Reviewer_GMuu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456003121, "cdate": 1761456003121, "tmdate": 1762931321622, "mdate": 1762931321622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a test-time optimization framework for generating time-variant 3D Gaussian Splatting (3DGS) representations of animals from a single monocular video. The method first leverages Fauna, an off-the-shelf model pretrained on large-scale animal data, to acquire a coarse 3D mesh reconstruction. This mesh then initializes a 3DGS field, which is subsequently refined through a joint-aware and symmetry-aware optimization process. The proposed method demonstrates state-of-the-art (SOTA) performance on established benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's methodology is logically structured, effectively leveraging animal-specific geometric priors to achieve high-quality reconstruction.\n2. The proposed method proves effective, with experiments demonstrating strong performance in both seen-view and novel-view settings.\n3. The manuscript is clearly written and easy to understand, supported by high-quality figures."}, "weaknesses": {"value": "1. The method's reliance on a strong animal segmentation prior to remove background influence needs clarification. This dependency seems at odds with Figure 1, which depicts a complex scene with multiple similar animals (e.g., camels). The paper does not appear to detail how the method would distinguish or handle such challenging multi-entity scenarios, making the figure's inclusion potentially misleading.\n2. The methodology requires further clarification on several key points:\n•\tThe baseline Fauna model provides per-frame deformed meshes, which this method reportedly discards. Please clarify the rationale for omitting this information and explain its relationship (or lack thereof) to the modules proposed in this paper.\n•\tLine 198: The initialization of attributes for the 3D Gaussian representation is unclear. Please specify the method and any statistical distributions used (e.g., for random initialization).\n•\tSymmetry-Aware Encoding: Please clarify if this is implemented simply as 2D data augmentation. Furthermore, the axis of reflection (e.g., horizontal, vertical) should be explicitly stated.\n•\tLine 215: The notations $J$ and $K$ are used without prior definition.\n•\tLine 256: The notation $t$ appears to be overloaded. In Line 187, it seems to represent time, whereas here it likely refers to the optimization iteration. Please clarify and consider using distinct notation to avoid ambiguity.\n3.  The experimental analysis is missing a comparison of computational and time costs against previous methods. Given that this is a test-time optimization approach, quantifying the efficiency-performance trade-off is essential for a complete evaluation of the method's practical utility."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "huqezE7njI", "forum": "4RD7LzmP6l", "replyto": "4RD7LzmP6l", "signatures": ["ICLR.cc/2026/Conference/Submission19404/Reviewer_1qYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19404/Reviewer_1qYS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958660947, "cdate": 1761958660947, "tmdate": 1762931321248, "mdate": 1762931321248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnimalGS, an optimization-based framework for 4D animal reconstruction from monocular video using 3DGS as the explicit representation. Unlike previous methods based on category-specific templates or generative priors, the animal shape prior predicted by Fauna is used only as initialization, rather than as constraints.\n\nThe innovation of this paper mainly focuses on model design: 1. Joint-Aware achors for robut pose refinement; 2. Symmetry-aware temporal encoding for bilateral cues; Pose-guided deformation based on cross-attention between joint and gaussian features.\n\nExperimental comparisons with Fauna, GART, D-3DGS, and GVF-Diffusion are provided to demonstrate the effectiveness of the proposed pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a template-free pipeline for 4D animal reconstruction with requiring dataset-level supervision.\n2. The joint-anchor and symmetry encoding are interpretable designs, which stablizes optimization process.\n3. The pose-conditioned deformation provides a clever constrain on 3DGS pipeline.\n4. The paper is well-written and easy to understand."}, "weaknesses": {"value": "1. Optimization-based methods require per-sample training, which is time-consuming.\n2. Quantitative metrics are proxy-based without 3D ground truth, which cannot assess the geometric accuracy.\n3. The Deformable-3DGS is not fairly compared in an identical setting: Initialize the Gaussians with Fauna and use RGB and Silhouette together for training. This critical result is missing.\n4. The method still depends on Fauna for initialization; if Fauna fails catastrophically, it’s unclear how robust AnimalGS remains."}, "questions": {"value": "Please address my concerns in the weakness part, and I will increase the score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0HvVQdG3pk", "forum": "4RD7LzmP6l", "replyto": "4RD7LzmP6l", "signatures": ["ICLR.cc/2026/Conference/Submission19404/Reviewer_PeBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19404/Reviewer_PeBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986822968, "cdate": 1761986822968, "tmdate": 1762931320934, "mdate": 1762931320934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AnimalGS, a test-time optimization (TTO) framework that reconstructs 4D animal geometry and motion from a single monocular video using a canonical 3D Gaussian Splatting (3DGS) representation. The pipeline (i) initializes from a coarse FAUNA prior (single-image category-agnostic animal model), (ii) performs pose refinement via learnable joint anchors with a symmetry-aware temporal encoding, and (iii) applies pose-guided non-rigid deformation to capture residual motion/appearance. The method optimizes photometric/silhouette losses plus a normal-smoothness regularizer. Experiments on DAVIS/Online/APTv2 report improved input-view PSNR/LPIPS and novel-view KID/FVD vs. Fauna, GART, D-3DGS, and a diffusion baseline “GVF-Diffusion (Trellis-based).” The authors claim strong perceptual quality and present a small user study."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a high-impact problem: single-video 4D animal reconstruction.\n\nThe two-stage TTO with symmetry cueing is practical and fits 3DGS well; ablations indicate each piece helps.\n\nShows consistent improvements over baseline TTO/3DGS choices reported in the paper.\n\nImplementation appears competent; real-time 3DGS rendering is attractive for downstream use."}, "weaknesses": {"value": "Visual quality not consistently convincing: Even in curated figures, limbs and fine appendages exhibit shape drift/temporal wobble, and small non-rigid details are often smoothed out; some sequences look over-regularized (likely from the normal-smoothness phase). The paper does not show challenging real-world clips with fast motion/occlusion and admits remaining failure modes (head/tail subtleties). \n\n\nMissing strong baselines for direct 3D/4D generation: No side-by-side against Hunyuan3D (image→3D) or Trellis-based pipelines (structured 3D latents). The paper only evaluates GVF-Diffusion as a proxy; that is insufficient—you should compare reconstruction fidelity and geometric consistency vs. (i) Hunyuan3D (image→3D + pose retarget), (ii) Trellis+render-supervised 4D generation, and (iii) a hybrid pipeline (Hunyuan3D init → your TTO).\n\nUnder-analyzed comparisons to the articulated-reconstruction literature the community expects:\n\nDreaMo (single casual video → articulated model and motion)\n\nLIMR (Learning Implicit Representation for Reconstructing Articulated Objects)\n\nS3O (dual-phase dynamic shape + skeleton from single video)\n\nPAD3R (pose-aware dynamic reconstruction from casual videos)\n\nMagicArticulate (prepare static models for articulation)\nEven if these target broader articulated objects (not only animals), their architectural choices, priors, and metrics are highly relevant. A clear, protocol-matched comparison (or at least cross-dataset transfer) is needed to justify the claimed advantages.\n\nOver-reliance on priors without robustness study: Replace or degrade FAUNA/camera priors; inject mask/trajectory noise; quantify sensitivity.\n\nAssumptions not stress-tested: What if first frames are not static? Provide an automatic canonical-frame detection ablation.\n\nEvaluation scope: Mostly short clips, restrained motions, and curated species. Add unconstrained YouTube sequences, long clips, severe occlusions, and small articulators."}, "questions": {"value": "Hunyuan3D / Trellis: Can you provide direct, protocol-matched comparisons? For Hunyuan3D, try (a) image→3D per key frame + tracking; (b) image→3D initialization followed by your TTO; (c) evaluate fidelity vs. plausibility trade-offs. For Trellis, evaluate a structured latent 4D pipeline with your same datasets/metrics.\n\nArticulated-reconstruction baselines: Please add DreaMo, LIMR, S3O, PAD3R, MagicArticulate to discussion.\n\nRobustness to priors: How does performance change if FAUNA predictions are noisy/misaligned (pose jitter, wrong joints), masks are imperfect, or cameras are biased? Provide degradation curves.\n\nCanonical-frame assumption: Show results when the first frames contain motion; can you auto-select canonical segments?\n\nRuntime/memory: Report training steps, seconds/iteration, Gaussians count vs. resolution, and Ablate λsmooth schedule; compare to GART / D-3DGS under identical hardware.\n\nFailure analysis: Provide videos where tails/ears/paws fail; diagnose whether pose refinement or deformation is the bottleneck.\n\nPhysical plausibility: Do you enforce joint limits or bone-length constraints? If not, quantify bone-length variance over time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bKI4v2TAGa", "forum": "4RD7LzmP6l", "replyto": "4RD7LzmP6l", "signatures": ["ICLR.cc/2026/Conference/Submission19404/Reviewer_YxkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19404/Reviewer_YxkL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039956441, "cdate": 1762039956441, "tmdate": 1762931320494, "mdate": 1762931320494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}