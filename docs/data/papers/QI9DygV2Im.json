{"id": "QI9DygV2Im", "number": 11047, "cdate": 1758187992882, "mdate": 1759897612391, "content": {"title": "Old but Gold: Adaptive Coreset Selection for Robust Dataset Compression", "abstract": "The computational and storage costs of large-scale datasets present a significant bottleneck in modern artificial intelligence (AI). While dataset distillation and coreset selection aim to mitigate this by compressing the original datasets into small ones, both have critical limitations. Dataset distillation produces synthetic images that exhibit architectural overfitting and poor transferability to downstream tasks. Conversely, existing coreset selection methods rely on fixed scoring functions, leading to redundant sample selection and performance saturation as the data budget increases. To address these challenges, we propose Adaptive Coreset Selection (ACS), a novel framework that learns an optimal selection strategy for a given budget. ACS employs a multi-stage approach, first building a foundational set of representative samples and then iteratively training models on the selected images to identify hard samples. This adaptive process ensures the final coreset balances representativeness and diversity. We demonstrate the efficacy of ACS on CIFAR-10 and ImageNet, where it outperforms state-of-the-art dataset distillation and coreset selection methods. Notably, on CIFAR-10 with 200 images-per-class, ACS surpasses all baselines by 2\\%p in validation accuracy and shows superior generalization to downstream tasks, establishing it as a more robust and scalable solution for dataset compression.", "tldr": "", "keywords": ["dataset distillation", "machine learning", "coreset selection"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64918fdf5e125a4f2e2770de1c28a6876f2a08c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Adaptive Coreset Selection (ACS). ACS iteratively builds a coreset of examples. First selects representative examples and then iteratively adds hard examples as the model gets trained. ACS is compared against coreset selection and dataset distillation methods. Methods are evaluated on the CIFAR10 and ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well written and understandable.\n- The method proposed challenges the idea of static coreset selection with good arguments.\n- Authors do a good job in showing weaknesses of related work."}, "weaknesses": {"value": "Insufficient evidence for cost claims and practicality: Paper asserts high “financial and computational costs” but provides no concrete measurements (Questions 1).\n\nComparison to related work: Paper seems to claim novelty on building the coreset dynamically, however that has been proposed in the past (Questions 2, 3). Additionally, comparison to more recent methods would benefit the paper [2,3,4,6].\n\nDecision reasoning: choices such as starting with easy examples are stated without evidence or reasoning, no experiments as why not to start with hard examples.\n\nPlease find further weaknesses in Questions."}, "questions": {"value": "1. “The financial and computational costs associated with storing and training on such data present major obstacles (Strubell et al., 2019), limiting the adoption of advanced models.” (line 036). However, there are no measurements supporting this claim. For example, could you report time-to-accuracy and wall-clock time benchmarks? How are costs reduced if one must first pretrain an entire model on the full dataset? What is the overhead of using ACS? How much can training time be reduced end-to-end?\n\n 2. “Current approaches employ the same greedy selection strategy regardless of budget size” (Line 053): [3] have adapted these methods to perform dynamic sampling during training and to sample from both static and dynamic distributions.\n\n\n3. Could you clarify how your method differs from Forgetting as adapted in [3]\n\n\n4. “Fig. 2b supports our claim. Images selected with EL2N cluster tightly in the feature space, leaving other regions uncovered.” (Line 246): If the goal is to cover all regions, why not maximize distribution coverage using an explicit metric (e.g. Maximum Mean Discrepency (MMD))? More broadly, how do you determine that maximizing distribution coverage is desirable, shouldn’t this depend on the data distribution and the subset size?\n\n\n5. “Unlike existing methods that selects all samples at once using a fixed score, ACS considers the coreset holistically and dynamically adjusts it selection strategy based on what has already been selected.” (Line 263): Dynamic sampling has already been proposed; see [5-6] and [3]. Please clarify the novelty relative to these approaches.\n\n\n6. “In the first segment, we select the easiest samples,” (Line 298). Why begin with easy examples, and how is this choice validated? [1] provide theoretical conditions under which one should start with easy versus hard examples.\n\n\n7. What is the motivation for stratified sampling? How do you determine that having equal numbers per class is preferable to allocating more samples to class A than to class B? For example, when in your case having 10 IPC, why is it not optimal to take 15 images of class A and only 5 images of class B?\n\n\n8. In general, reporting IPC together with the selection/pruning ratio is more informative. For CIFAR-10/100 this is easy to compute, but otherwise stating that a model is trained on “2%” of the data is more informative than stating it is trained on 10 IPC.\n\n\n9. “This indicates smaller models are better suited for our algorithm, as larger models require more data to learn meaningful representations” (Line 455): I do not believe this follows from Table 5. To support the claim, it would help to show performance when each model is trained on the entire dataset; otherwise, the result could reflect architectural differences rather than an interaction with ACS. Additionally, given scaling laws, shouldn’t one prefer a method that scales with the number of model parameters?\n\n\n[1] Sorscher, Ben, et al. \"Beyond neural scaling laws: beating power law scaling via data pruning.\" Advances in Neural Information Processing Systems 35 (2022): 19523-19536.\n\n[2] Kolossov, Germain, Andrea Montanari, and Pulkit Tandon. \"Towards a statistical theory of data selection under weak supervision.\" (ICLR’24)\n\n[3] Okanovic, Patrik, et al. \"Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning.\" The Twelfth International Conference on Learning Representations.\n\n[4] Abbas, Amro Kamal Mohamed, et al. \"Effective pruning of web-scale datasets based on complexity of concept clusters.\" The Twelfth International Conference on Learning Representations.\n\n[5] Mirzasoleiman, Baharan, Jeff Bilmes, and Jure Leskovec. \"Coresets for data-efficient training of machine learning models.\" International Conference on Machine Learning. PMLR, 2020.\n\n[6] Qin, Ziheng, et al. \"InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning.\" The Twelfth International Conference on Learning Representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xST0fz9scp", "forum": "QI9DygV2Im", "replyto": "QI9DygV2Im", "signatures": ["ICLR.cc/2026/Conference/Submission11047/Reviewer_gVDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11047/Reviewer_gVDY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663503671, "cdate": 1761663503671, "tmdate": 1762922227538, "mdate": 1762922227538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the limitations of fixed-score coreset selection and dataset distillation for dataset compression, proposing an Adaptive Coreset Selection (ACS) framework. ACS employs a multi-stage strategy: firstly it selects representative easy samples, then iteratively adds hard examples identified through models which trained on the current subset, progressively constructing a diverse and representative coreset. Comprehensive experiments on CIFAR-10 and ImageNet demonstrate that ACS outperforms state-of-the-art dataset distillation and coreset selection methods, especially at large data budgets, and achieves better generalization on downstream and OOD tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear identification of limitations:** Both dataset distillation and standard coreset selection methods have clear limits. Dataset distillation often leads to overfitting because it uses synthetic data. Standard coreset methods rely on fixed scoring rules, which can pick too many similar samples. This causes redundancy and makes the selection less effective.\n2. **Methodological innovation**: ACS uses a multi-stage, adaptive scoring method which updates how important each sample is as the coreset grows. Most earlier approaches assume samples are independent, but ACS breaks that assumption. The paper explains this idea clearly. It also shows it well in the pipeline diagram.\n3. **Comprehensive experimental validation**: The paper provides robust empirical evidence  on both CIFAR-10 and ImageNet for various images-per-class setups, covering accuracy, transferability, OOD robustness, and ablation on architectural backbones and segmentation strategies.\n4. **Qualitative analysis**: Visualization of selected images from early and late stages demonstrates that ACS achieves both representative base coverage and challenging sample inclusion—evidence for improved diversity.\n5. **Reproducibility**: Hyperparameters are fully provided in Appendix B, and the algorithm is presented in clear pseudocode, supporting claims of transparency in implementation."}, "weaknesses": {"value": "1. **Lack of formal theoretical analysis of generalization**: Apart from the intuitive rationale and empirical results, there’s no formal proof or bound concerning why ACS-selected subsets would achieve better generalization or diversity than other approaches. For instance, there’s no analysis of marginal gains or redundancy reduction, nor is there a clear link to submodular optimization or curriculum learning theory.\n2. **Scalability and compute cost**: While the paper claims ACS is scalable, the multi-stage process (involving multiple model trainings) likely incurs noticeably larger compute than single-pass, fixed-score methods. There is no analysis (empirical or theoretical) on computational cost versus performance, nor is wall-clock time or total FLOPs reported in any table.\n3. **Algorithmic ablations lacking**: Apart from backbone and segment number (see **Table 5** and **Table 6**), there is no thorough ablation of, for example, whether selecting only from misclassified samples at later stages is essential, or how sensitive ACS is to the exclusion of hard or easy samples in various regimes. This makes it hard to judge whether the method’s advantage is robust to minor tweaks.\n4. **Notational clarity**: While the pseudocode in Appendix A and equations throughout are generally sound, some ambiguity remains. For example, the notational switch between $\\mathcal{C}_t$ (cumulative coreset) and $\\mathcal{S}_t$ (current segment) could be more visually separated throughout the main text and algorithm. Additionally, the loss function notation is at times overloaded."}, "questions": {"value": "1. **Choice and generality of scoring function**: Have you experimented with alternative context-aware scoring metrics beyond classification loss, such as prediction margin, uncertainty (entropy), or uncertainty-gradient products? Would such measures yield better/robuster coresets? Please comment on the generalizability of the principle.\n3. **Diversity metrics**: Beyond qualitative t-SNE and segment visualizations, have you quantified the diversity of ACS-selected samples (e.g., feature-space coverage, cluster spread, redundancy metrics)? Could you add such assessments to strengthen the evidence for ACS’s claims?\n5. **Hyperparameter scheduling**: Is there any principled scheme to set the number of segments ($T$) based on data or model properties? Have you considered automatic tuning or meta-learning approaches?\n6. **Sensitivity to model/backbone choice**: Table 5 suggests varying robustness, but how stable is ACS performance across a wider range of architectures, especially for current large transformer models? Is there a risk of over-specialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VKqhDanOHl", "forum": "QI9DygV2Im", "replyto": "QI9DygV2Im", "signatures": ["ICLR.cc/2026/Conference/Submission11047/Reviewer_o3t9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11047/Reviewer_o3t9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896478722, "cdate": 1761896478722, "tmdate": 1762922227164, "mdate": 1762922227164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Coreset Selection (ACS), a novel framework for dataset compression that overcomes limitations of both dataset distillation and existing coreset methods. ACS adaptively selects samples in stages, balancing representativeness and diversity by iteratively training models and selecting misclassified examples. It outperforms state-of-the-art methods on CIFAR-10 and ImageNet. Moreover, ACS also shows good generalization and robustness across architectures and downstream task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work reveals the fundamental limitations of dataset distillation and traditional coreset selection methods, and proposes ACS—a more robust and scalable data compression framework—demonstrating that carefully selected real images outperform synthetic ones in terms of cross-architecture generalization and out-of-distribution robustness.  \n- We introduce the first adaptive coreset selection framework based on context-aware scoring, which breaks away from the assumption of fixed sample scores. It models sample importance as a dynamic property that evolves with the already-selected samples and naturally preserves diversity through a multi-stage iterative strategy.  \n- The method consistently surpasses existing coreset selection and dataset distillation approaches on CIFAR-10 and ImageNet, and continues to improve even under high budgets, significantly outperforming random selection baselines.  \n- Baseline methods are reproduced following standard protocols and using publicly available codebases (e.g., DeepCore), ensuring strong reproducibility."}, "weaknesses": {"value": "1. The proposed method heavily relies on the previously selected batch of data, which introduces a significant efficiency bottleneck. While this has minimal impact on small-scale datasets (e.g., CIFAR-10), the time cost for sample selection grows dramatically as dataset size increases. For instance, on ImageNet, the method achieves performance nearly on par with random selection but at a substantially higher computational cost. The authors should take this issue seriously: if applied to even larger datasets (e.g., LAION-400M), the method’s advantages may diminish entirely. Therefore, a comparison of actual runtime against baseline methods is essential.\n2. We know that random sampling remains highly effective on large-scale datasets and is compatible with any model architecture. In contrast, the method proposed in this paper relies heavily on the backbone architecture. Although the authors conducted related experiments, it remains unclear whether their approach exhibits strong generalization in terms of transferability. For example, why not use a ViT architecture for sample selection? How would sampling perform with an MLP-based model? Are models with the same architecture inherently more advantageous? The authors need to provide more theoretical insights and experimental analyses to address these questions.\n3. Why not compare against some dynamic dataset pruning methods [1]?\n\n[1] InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i8PEYivEsj", "forum": "QI9DygV2Im", "replyto": "QI9DygV2Im", "signatures": ["ICLR.cc/2026/Conference/Submission11047/Reviewer_pW7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11047/Reviewer_pW7m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978386665, "cdate": 1761978386665, "tmdate": 1762922226710, "mdate": 1762922226710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduced a multi-stage coreset selection framework ,named “Adaptive Coreset Selection (ACS)”, that adjusts its selection criteria based on model performance at different stages, in an adaptive manner.\n- The method selects coreset in an easy to hard manner. It selects the easy and most representative samples first and then iteratively train models on selected samples to select the harder samples.\n- It aims to address performance saturation at higher images-per-class setting selection for coreset methods.\n- The paper compares performance against various coreset selection and dataset distillation methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem motivation is clearly articulated, regarding addressing performance saturation for higher selection budgets.\n2. The method is intuitive and straightforward, borrowing ideas from curriculum learning. \n3. The method’s practical implementation does not require any complex optimisation.\n4. The paper has provided ablation studies for backbone architectures (Table 5) and number of segments (Table 6)."}, "weaknesses": {"value": "**Limited Novelty**\n\n- The methodology combines aspects from already well-established techniques such as curriculum learning and iterative hard example mining (standard practice in bootstrapping and active learning).\n\n**Limited comparison baselines**\n\n- The coreset selection methods used as baseline are used from DeepCore library , which was published in 2022. Recent coreset selection works which have shown better performance than these methods have not been considered. Some of the works are listed below:\n\nA) Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning (ICLR 2023)\n\nB) Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy (NeurIPS, 2023)\n\nC) Coverage-Centric Coreset Selection for High Pruning Rates (ICLR, 2023)\n\nE) Data Pruning via Moving-one-Sample-out (NeurIPS 2023)\n\nF) Noise-free Loss Gradients: A Surprisingly Effective Baseline for Coreset Selection (TMLR, 2025)\n\n**Mismatch in results reported**\n\n- Referring to Table 3, the accuracy values reported for SRe2L and RDED are significantly lesser than what these papers have reported in their results. For example, for ImageNet-1K, RDED reports (Table 2 of RDED paper) an accuracy of 42.0% and 56.5% for IPC=10 and IPC=50 respectively. While, Table 3 of this paper has reported 12.5% and 29.8% respectively, which are significantly less and misleading in nature for comparison purposes. A similar discrepancy is observed for SRe2L as well.\n\n**Lack of timing analysis**\n\n- The multi-stage nature of coreset selection raises a question regarding computational overhead in terms of GPU usage and time required for coreset selection. A comparative analysis of GPU requirement and timing for coreset selection with other baseline methods would be very helpful in understanding impact of ACS. \n\n**Number of segments**\n \n- From Table 6, number of training segments required to achieve higher performance for CIFAR-10 is 10 for IPC=100. The paper does not provide similar results for ImageNet-1K dataset. \n\n**Limited datasets for comparison**\n\n- Other standard benchmark datasets such as CIFAR-100 and Tiny ImageNet are not considered. A performance comparison on these two datasets would be insightful regarding efficiency of ACS on datasets of various number of classes."}, "questions": {"value": "- Please refer to the weaknesses section of the review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V93vlFtGrh", "forum": "QI9DygV2Im", "replyto": "QI9DygV2Im", "signatures": ["ICLR.cc/2026/Conference/Submission11047/Reviewer_DxkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11047/Reviewer_DxkA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011363017, "cdate": 1762011363017, "tmdate": 1762922226285, "mdate": 1762922226285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}