{"id": "jpz7e3jhRq", "number": 6305, "cdate": 1757966307017, "mdate": 1759897923424, "content": {"title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "abstract": "Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM approach or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 38 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing  when initializing from readily available pretrained CLM models, reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at \\url{https://huggingface.co/XXX} to foster further research.", "tldr": "", "keywords": ["Encoder Pretraining", "Masked Language Modeling", "Causal Language Modeling", "Text Representations", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3612ffd317367025d031014346db2ce56f624db0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper conducts a comprehensive empirical comparison between Masked Language Modeling (MLM) and Causal Language Modeling (CLM) for encoder pretraining. Using 38 models (210M–1B parameters) trained under matched computational budgets, the authors analyze performance across multiple NLP tasks and setups, including two-stage (CLM→MLM) and continued pretraining. The results show that MLM remains generally superior for text representation tasks, while CLM exhibits higher data efficiency and fine-tuning stability. Combining both (CLM followed by MLM) achieves the best overall trade-off between efficiency and final performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and logically organized. The narrative is easy to follow, and figures/tables effectively support the main findings.\n2. The experimental analysis is extensive and carefully controlled. The authors evaluate across diverse model sizes, masking ratios, and task types, ensuring that performance differences stem from the pretraining objective itself rather than confounding factors.\n3. The research question is highly relevant: as decoder-based models increasingly dominate embedding benchmarks, understanding whether CLM pretraining offers intrinsic advantages is crucial. The paper addresses this gap convincingly and provides practical insights for future encoder design."}, "weaknesses": {"value": "1. The paper establishes that MLM excels in tasks requiring bidirectional reasoning (e.g., QA, classification), while CLM performs competitively in token-level or retrieval settings. However, the authors stop short of elaborating on why certain pretraining paradigms align better with specific task types. A deeper discussion of this task–objective relationship would make the work more insightful and actionable for practitioners.\n2. Although training budgets are said to be matched, the paper does not provide explicit comparisons in FLOPs, runtime, or memory efficiency. Such quantitative reporting would strengthen the claims about CLM’s data and compute efficiency."}, "questions": {"value": "please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qneNo9j5o0", "forum": "jpz7e3jhRq", "replyto": "jpz7e3jhRq", "signatures": ["ICLR.cc/2026/Conference/Submission6305/Reviewer_hZNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6305/Reviewer_hZNQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924205791, "cdate": 1761924205791, "tmdate": 1762918607103, "mdate": 1762918607103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper compares MLM (masked language modeling) and CLM (causal language model) objectives in a large variety of scenarios. As the authors note, encoder models are typically trained solely with the MLM objective to encourage good representations, while decoder models use CLM towards the direct objective of next token predicting. The authors design an experimentation setup to do fair assessments of both of these methods, carefully ablating aspects like model size, masking rate, and curriculum incorporating each aspect. They test on a variety of common evaluations to find cases where different models have an advantage. This leads to some surprising conclusions around training encoder models in a data and compute efficient manner - specifically, that a \"biphasic training strategy\" can lead to good performance at the scales tested here. I also found the analysis of fine-tuning stability particularly valuable, and somewhat under-considered in other works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper has many interesting experiments and appears to explore a large space of possible model configurations. \n\n- Reasonable experimental setup using common best practices in small model encoder/decoder training: high quality data choice, learning rate schedule, controlled architecture\n\n- Carefully sweeps an interesting set of parameters: model size, masking ratio, curricula combining CLM + MLM at different ratios, careful task specific tuning to analyze model stability. It is uncommon to see all of these varied at reasonable experimental scale, so the authors should be commended. \n\n- The decision to test mixed objectives like applying MLM after CLM in the same training run is very interesting. The authors note that the this method is applicable to the case where an encoder model can be trained from an existing pretrained decoder only model, as it seems many industrial resources are focused on decoder training. \n\n- The experiments and finding seem very practically motivated. Finetuning stability is an important practical aspect of encoder models as commonly used, and a focus on limited data efficiency is also practical."}, "weaknesses": {"value": "My main weaknesses with this paper involve data choices and budget. This paper has many interesting experiments, some of which lead to further research questions. \n\n1. As the authors acknowledge in section 8, their dataset is limited to 100B tokens. While this is far beyond the decoder compute optimal ratio, it does not seem well established whether this is a reasonable budget for encoder model training. For example, the progression from the original BERT to RoBERTA (and XLM-R) involved a large increase in data scale. Further recent research has shown that considering inference compute costs pushes scaling to favor longer data durations with smaller models (\"overtraining\" - see the llama3 paper or https://arxiv.org/abs/2401.00448). There is closely related work (only briefly cited by the paper authors, possibly concurrent) that trains similar model sizes up to 2T tokens: https://arxiv.org/abs/2507.11412 (Weller et al. 2025). \n\n2. Related, the choice of 100B tokens is only focused on the fineweb edu data. This data is high quality for some domains, but does not represent the diversity of data present in a practical encoder data mixture. See the previously mentioned work or the ModernBERT paper (https://arxiv.org/abs/2412.13663), which additionally includes code and scientific papers. The original Fineweb edu work shows the greatest improvements in benchmarks like MMLU, which specifically draw from education-relevant sources. \n\n3. The total size of 100B tokens also limits the experimental ratios when combining MLM + CLM objectives. While motivated by using an existing pretrained LLM, the ratios of 25% budget increments (corresponding to 25B tokens), seem unrealistic. This would mean that the 75-25% scenario uses CLM on 75B tokens and MLM on 25B, which seems far from the more realistic scenario of a very large budget allocated to CLM with a smaller scale MLM. \n\nIn a similar vein, while the masking ratio experiments are very interesting, it is hard to judge whether the results are due to the relatively short training budgets. Perhaps 100B tokens is not enough to learn good representations and the very high or low rates. As the authors note in line 473:\n\n> but learning dynamics suggest that further performance improvements are possible with extended training\n\nMinor:\n\nFurther analysis or explanations of differences in IR and token level tasks (253-254). Other analysis of the interaction between LR decay and curriculum would be interesting \n\nThe tables in the appendix are appreciated but would benefit from formatting such as bolding extreme values."}, "questions": {"value": "- Any further analysis or intuition on the u-shaped masking ratio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HY1ERj2gx4", "forum": "jpz7e3jhRq", "replyto": "jpz7e3jhRq", "signatures": ["ICLR.cc/2026/Conference/Submission6305/Reviewer_e3tm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6305/Reviewer_e3tm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954309492, "cdate": 1761954309492, "tmdate": 1762918606811, "mdate": 1762918606811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "There has been much discussion in the community at the relative merits of Bert-style training for learning representations of text, in the age of next token prediction with large language models. This paper presents a careful empirical study as to  compare masked language modeling (MLM) and causal language modeling (CLM). They compare several models with less than or equal to 1B params, on several tasks. They find that MLM still generally beats out CLM, but training with CLM and then MLM generally outperforms any other alternative. They study continued pretraining where they find also that adapting a CLM trained model via MLM generally performs best."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Several studies have studied both MLM and CLM for learning representations of data, but to my knowledge none have such an in depth empirical study.\n\n2. Quality: This paper uses strong state-of-the-art models as the baseline and evaluate on a wide array of tasks. As such, I consider the empirical results to be high quality\n\n3. Clarity: The motivation and work done in this paper are quite clear.\n\n4. Significance: The paper asks a clear and important question and answers is with good empirical evidence\nThis paper provides concrete recommendation to practitioners."}, "weaknesses": {"value": "1. The scale is somewhat small, at 1B but is still within a reasonable range for such an experiment. It might be valuable to see how this looks for a 7B model.\n\n2. It would be good to see performance on all the tasks in the MTEB (Massive Text Embedding Benchmark)."}, "questions": {"value": "1. Are there any experiments where we first train with MLM and then with CLM? Related, could one feasibly alternate between the two, e.g. per epoch or per batch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MmxFHtjqxI", "forum": "jpz7e3jhRq", "replyto": "jpz7e3jhRq", "signatures": ["ICLR.cc/2026/Conference/Submission6305/Reviewer_JaBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6305/Reviewer_JaBv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966970641, "cdate": 1761966970641, "tmdate": 1762918606341, "mdate": 1762918606341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper runs controlled ablations (same data, tokenizer, and encoder family; sizes 210M/610M/1B) to test whether encoder pretraining should remain MLM-only or benefit from a two-stage CLM→MLM recipe. The authors show the following findings (in my understanding) (1) MLM wins on average for representation tasks; (2) CLM is more data-efficient early in training and yields more stable fine-tuning; (3) a two-phase schedule that starts with CLM and then switches to MLM outperforms MLM-only at fixed compute; and (4) in continued pretraining, adapting a CLM with MLM beats continuing MLM on an MLM model—suggesting a practical path that leverages widely available decoder checkpoints.       \nSetup details: EuroBERT encoders, FineWeb-Edu English data, LLaMA-3 tokenizer; most two-stage/CPT studies use the 610M model and 40% masking for MLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Thorough controls and scale of experiments. Same backbone family, shared data order, and systematic sweeps (sizes, masking ratios, schedules) reduce common confounders in MLM vs CLM comparisons. Compute and evaluation budgets are reported in detail. \n\n\n- Actionable recipe: Under fixed compute, CLM→MLM (e.g., 25%→75%) consistently beats MLM-only; continued-pretraining results make a practical case for initializing from a decoder checkpoint, then adapting with MLM.   \n\n\n- Early data-efficiency + robustness: CLM learns useful representations faster early on and is less sensitive to fine-tuning learning rates; CLM→MLM is also less brittle to masking-ratio choices.     \n\n\n- Clarity on task sensitivity: The paper separates effects across SC/TC/QA/IR, noting where bidirectionality matters most (e.g., QA) and where CLM narrows the gap (e.g., IR), which is useful guidance for practitioners."}, "weaknesses": {"value": "- General validity of the experiments might be narrow in scope: Results are tied to one encoder family and one tokenizer in English; generalization to other backbones (e.g., DeBERTa/ModernBERT), tokenizers, or languages is open.   \n\n\n- CPT evidence is concentrated at 610M and one masking ratio: While the authors sweep sizes and masks in base runs, most decisive experiments (schedules, CPT) are at 610M with 40% mask, which limits how confidently we can extrapolate.   \n\n\n- Magnitude of gains is modest: The CLM --> MLM uplift over strong MLM baselines appears small in many cases (Figure 2 scales across figures are a bit misleading in this regard, where each task have different scales)\n\n\n- Scaling anomalies might need more explanation: Some configurations show 1B not strictly dominating 610M (e.g., QA at certain masking ratios), which makes me wonder if there was other training issues that could have contributed to this. It would be good if the authors provided some explanation on this."}, "questions": {"value": "(covered above)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pLtqHR90F4", "forum": "jpz7e3jhRq", "replyto": "jpz7e3jhRq", "signatures": ["ICLR.cc/2026/Conference/Submission6305/Reviewer_9eov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6305/Reviewer_9eov"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972316265, "cdate": 1761972316265, "tmdate": 1762918605816, "mdate": 1762918605816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}