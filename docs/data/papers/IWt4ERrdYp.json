{"id": "IWt4ERrdYp", "number": 13757, "cdate": 1758222098390, "mdate": 1759897415261, "content": {"title": "InfGen: Scenario Generation as Next Token Group Prediction", "abstract": "Realistic and interactive traffic simulation is essential for training and evaluating autonomous driving systems. However, most existing data-driven simulation methods rely on static initialization or log-replay data, limiting their ability to model dynamic, long-horizon scenarios with evolving agent populations. We propose InfGen, a scenario generation framework that outputs agent states and trajectories in an autoregressive manner. InfGen represents the entire scene as a sequence of tokens—including traffic light signals, agent states, and motion vectors—and uses a transformer model to simulate traffic over time. This design enables InfGen to continuously insert new agents into traffic, supporting infinite scene generation. Experiments demonstrate that InfGen produces realistic, diverse, and adaptive traffic behaviors. Furthermore, reinforcement learning policies trained in InfGen-generated scenarios achieve superior robustness and generalization, validating its utility as a high-fidelity simulation environment for autonomous driving. Code and models will be made publicly available.", "tldr": "InfGen models scenario generation as next token group prediction, enabling dynamic and realistic traffic simulation with a unified autoregressive model. It supports agent insertion, motion rollout, and improves RL policy robustness.", "keywords": ["Autonomous driving", "Closed-loop simulation", "Scenario generation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09e11cae6635ad6558ec94025352b1166a2019fc.pdf", "supplementary_material": "/attachment/24495cd2c26bac8754c295c0b13b3fd7506ec53c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a framework to employ a single autoregressive model that produces both agent states and their motion trajectories as part of one continuous token sequence over a long horizon. Experiments show the proposed method achieves good performance on Scene Generation and Motion Prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The unified token scheme for unified motion and scenario generation is interesting (though not super novel).\n\nThe proposed method is an interesting way to present the two aspects of traffic simulation, scene generation and motion prediction, in a unified autoregressive architecture. This is an interesting way to scale the full simulation process. However, as will be mentioned in the Weakness section, this idea has been discussed in a prior work.\n\n2. Performance of Scenario Generation and Motion Prediction is good.\n\nThe performance of the proposed method on individual tasks of Scenario Generation and Motion Predictions is strong against prior work.\n\n3. Showing improved RL agent performance with InfGen scenarios.\n\nThe highlight of this paper's result is its ability to serve as a scenario generator for an effective data augmentation tool for closed-loop simulation."}, "weaknesses": {"value": "1. The main technical contribution is not that novel and misses some important references and discussions.\n\nThe main technical contribution of this paper is a unified framework to \"employ a single autoregressive model that produces both agents' initial states and their motion trajectories\". However, this idea is not new and has already been explored in a prior work already published [1]. Therefore, I think it is necessary to discuss and compare against [1] as it targets the same task and has a very similar idea.\n\nOn another level, this paper claimed that prior work like [2] \"separates static and dynamic phases\", which is not accurate. The method in [2] also achieves both scenario generation and motion prediction with a unified model, during which the model has the full unified context of agent placement and movement information. Although it's a diffusion model instead of an autoregressive model, I also think this paper should discuss its difference with [2], a prior work that unifies the two tasks. Note that [2]  does not do the \"fixed-order\" generation like UniGen.\n\n2. Does not conduct extensive experiments on many important aspects of the proposed model.\n\nAlthough the paper shows that the proposed method achieves strong performance for two tasks, there lack of experiments regarding some of the important capabilities mentioned in the main paper:\n\n- Scene editing.\n- Agent insertion/removal during simulation.\n- Traffic simulation (only in the appendix).\n- Extended long-term simulation (no formal experiment, just two demo runs from the supp video).\n\n\n3. Long-term simulation performance issue from the demo video. \n\nI appreciate the authors presenting the demo video of the scenarios generated by the proposed method. I found the long-term simulation capacity of the proposed method is weak:\n\n- (01:46) The model simulates the ego agent moving forward, but the two agents on the right edge of the scenario cannot exit the scene. These agents drives normally to move out of sight of the ego vehicle, which should be removed by the system. However, the system fails to do that and forces the two vehicles to \"come back\" to the region of interest, leading to the unsafe collision and off-road behaviors of these two vehicles. I think this might due to the fact that the proposed method does not learn when to remove exiting agents; therefore, to maintain the status of these agents, it forces them to \"stay inside\" the region of interest. This would become problematic when the simulation duration is longer than the training data.\n\n- Throughout the whole video, I cannot see any agent being added to the scenario. This would be problematic for scenarios like (02:15) where there are less and less agents in the scenario, making the traffic distribution unrealistic.\n\nI wish the author could discuss the reason behind these issues in a systematic way.\n\n\n4. Minor - Method naming conflict with [1].\n\nThe method in [1] is also named InfGen, which has already been published in ICCV. Given that both works focus on traffic or motion generation in simulation environments, this overlap may confuse future citations and discussions. Since the arXiv version of [1] was publicly available months before this submission, the identical naming may lead to confusion when referencing the two works. I suggest the authors consider renaming their method or adding a distinguishing qualifier (e.g., by emphasizing a unique aspect that they can also do initial scenario generation) to avoid ambiguity and ensure clearer attribution.\n\n[1] Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation. ICCV 2025.\n\n[2] SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout. NeruIPS 2024."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3879p3Z2HT", "forum": "IWt4ERrdYp", "replyto": "IWt4ERrdYp", "signatures": ["ICLR.cc/2026/Conference/Submission13757/Reviewer_h7bB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13757/Reviewer_h7bB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627337962, "cdate": 1761627337962, "tmdate": 1762924291945, "mdate": 1762924291945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents InfGen, a scenario generation framework that can generate realistic, diverse, and adaptive traffic scenarios autoregressively. The main advantage is the ability to simulate the addition and deletion of agents in a closed-loop manner during traffic simulation. But the positioned contribution and the experiments are a bit confusing and limited in the current presentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Closed-loop simulation\n2. Unified modeling of the whole scenario"}, "weaknesses": {"value": "1. Confusing positioned contribution and experimental setting\n2. Lack of comprehensive comparison"}, "questions": {"value": "1. From the description of the authors and my own understanding, the interaction between the ego agent and the background ones is bidirectional, which means that during decoding phase, the interaction effect should be reflected in a manner like bidirectional attention. But the current design is more like a sequential manner. \n\n2. In Table 1, the authors reported the results after relaxing the evaluation protocol by computing MMD over all agents instead of only vehicles within 50m of the ego vehicle. If the authors think this manner is better, why not compare all comparable baselines under this setting?\n\n3. Based on my knowledge, there have already been a few works focusing on closed-loop simulation, and hence the interaction among agents is not new to capture. The main contribution of this paper, to my understanding, is the ability to simulate the changes of participating agents, like adding agents. But the current experiments neither prove that InfGen can perform better than existing baselines in agent simulation, nor show that InfGen can simulate complicated scenarios with frequent agent changes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j5DjQ98ATt", "forum": "IWt4ERrdYp", "replyto": "IWt4ERrdYp", "signatures": ["ICLR.cc/2026/Conference/Submission13757/Reviewer_7cTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13757/Reviewer_7cTF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633669918, "cdate": 1761633669918, "tmdate": 1762924291385, "mdate": 1762924291385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Scenario Generation as Next-Token Group Prediction introduces a unified transformer framework for traffic-scene simulation, where InfGen represents heterogeneous traffic elements—maps, traffic lights, and agent states/motions—as discrete tokens within a single autoregressive sequence. This formulation supports dynamic agent injection, long-horizon closed-loop rollouts, and “infinite” scenario generation, enabling multiple downstream tasks such as motion prediction, scene densification, and RL-based planner training without architectural changes. However, while experiments on WOMD demonstrate reasonable realism and diversity, InfGen’s overall motion-prediction accuracy (e.g., mADE) lags behind top-performing baselines, making its empirical performance somewhat less compelling despite the novel formulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "● Unified formulation: Modeling the entire scene as a next-token sequence provides a unified autoregressive framework capturing spatiotemporal dependencies among maps, lights, and agents. This “traffic as language” design enhances long-horizon consistency, supports multiple tasks, and enables seamless dynamic scene evolution.\n● Dynamic agent injection: The model can add or remove agents at different timesteps, breaking from the fixed-agent assumption and better reflecting open-world traffic where participants continuously enter and exit. This improves interactivity and scene diversity, producing more realistic closed-loop simulations.\n● Strong downstream evaluation: The experiments are extensive, and it is commendable that InfGen-generated data are directly used for downstream tasks such as reinforcement learning, demonstrating the framework’s practical value and potential for real-world applications."}, "weaknesses": {"value": "● Limited performance on core WOMD metrics: Despite its novel formulation, InfGen does not achieve competitive results on the core WOMD leaderboard—particularly on mADE, which is the primary metric of the Waymo Challenge. Its overall scores lag behind recent strong baselines such as UniMM and CAT-K, raising concerns about whether the proposed architectural contributions and dynamic scenario generation truly translate into better motion accuracy or downstream utility. The claimed advantage in supporting downstream tasks may also be less convincing if stronger models like UniMM already achieve superior realism and forecasting fidelity.\n● Temporal downsampling and computational inefficiency: InfGen trains on 2 Hz downsampled data from the original 10 Hz WOMD, whereas strong baselines such as UniMM are trained at full 10 Hz using similar or smaller GPU resources (e.g., 8 × 4090). The 0.5 s frame interval is relatively coarse for autonomous driving, where 0.1 s resolution is crucial to capture fine-grained dynamics and trajectory continuity. This reduction likely limits the model’s ability to learn rich temporal features, contributing to its weaker mADE performance. Moreover, despite the lower temporal resolution, InfGen still requires 8 × 48 GB GPUs, suggesting that its unified tokenized architecture introduces significant computational overhead relative to its output fidelity.\n● Insufficient ablation experiments: While the paper introduces several innovations—grouped causal attention, relative attention bias, autoregressive state decoding, and dynamic agent injection—it lacks ablations to isolate their effects. Table 1 only tests the removal of autoregressive decoding, leaving unclear how other components contribute to performance. Evaluating variants such as fixed-agent vs. dynamic injection or with/without relative attention would better clarify which design choices truly drive InfGen’s gains."}, "questions": {"value": "Q1. InfGen’s overall WOMD performance, especially on the core mADE metric, is noticeably behind UniMM and CAT-K. Could the authors clarify whether this gap stems from model capacity, training configuration, or data resolution differences, and how they envision improving accuracy under the same benchmark settings?\n\nQ2. Since InfGen trains on 2 Hz downsampled data while UniMM uses 10 Hz, can the authors provide evidence or analysis showing how temporal resolution affects model fidelity? Would training InfGen on 10 Hz data close the gap, or is the architecture itself sensitive to long token sequences?\n\nQ3. The paper reports that training at 2 Hz already requires 8 × 48 GB GPUs. Which component contributes most to this computational overhead—token sequence length, dynamic agent injection, or multi-head decoding—and is inference similarly resource-intensive?\n\nQ4. Only one ablation (removing autoregressive decoding) is presented. Could the authors include or discuss experiments isolating other design choices, such as grouped vs. flat attention, relative vs. absolute encoding, or fixed-agent vs. dynamic injection, to verify each contribution’s effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rRRyYXEiTx", "forum": "IWt4ERrdYp", "replyto": "IWt4ERrdYp", "signatures": ["ICLR.cc/2026/Conference/Submission13757/Reviewer_JzqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13757/Reviewer_JzqF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937391853, "cdate": 1761937391853, "tmdate": 1762924290496, "mdate": 1762924290496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfGen, a novel generative framework for traffic scenario simulation essential for training and evaluating autonomous driving (AD) systems. InfGen models the entire dynamic driving scene, including traffic light signals, agent states, and motion vectors, as a single, structured sequence of discrete tokens. It employs a unified autoregressive Transformer model to generate these tokens step-by-step, enabling continuous and long-horizon scene rollout.\n\nA key contribution is the unified state and trajectory tokenization, making InfGen the first to use a single autoregressive model to produce both agent initial states and motions as a continuous token sequence, addressing the inflexibility of prior two-stage models. The framework features a novel autoregressive generation scheme for agent states by predicting the agent's type, its anchoring map segment ID, and its relative kinematic state, which improves semantic and physical consistency over flat decoding methods.\n\nThe model's design supports dynamic agent injection and infinite scene generation, overcoming limitations of static initialization and fixed agent populations in existing data-driven simulators. The versatility is demonstrated across multiple tasks including motion prediction, full-scenario generation, scenario densification, and closed-loop simulation. Experiments, particularly the deployment in a Reinforcement Learning (RL) pipeline, show that policies trained in InfGen-generated scenarios achieve superior robustness and generalization compared to log-replay baselines, validating its utility as a high-fidelity simulation environment."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "*Originality: The core idea of framing multi-agent, dynamic scenario generation as a unified next-token prediction task using a single autoregressive model (InfGen) is highly original in this domain. Specifically, the autoregressive generation of agent states (Type, Map ID, Relative State tokens: ⟨SOA,TYPE,MS,RS⟩_t) anchored to map segments is a clever mechanism for achieving physically and semantically consistent agent initialization, which is a major advancement over prior non-causal \"flat\" decoding methods.\n*Quality: The technical execution is robust. The introduction of state-forcing unifies the generation of new agents (via sampling) and the continuation of existing ones (via deterministic update), enabling seamless closed-loop simulation across variable agent sets. The detailed motion tokenization using a discretized control input space, optimized via Average Corner Error (ACE), provides a robust foundation for motion prediction quality.\n*Clarity: The paper is generally well-written and logically structured. Key concepts like the token sequence composition ([<MAP>; (<TL>,<AS>,<MO>)_1 ;…]) , the three main token groups (TL, AS, MO) , and the process of agent state generation are clearly explained and effectively illustrated in Figures 2 and 3/7. The ablations and RL experiments clearly validate the core components.\n*Significance: The results are highly significant for the autonomous driving community. InfGen's ability to produce traffic scenarios that significantly improve the robustness and generalization of downstream RL planners is a compelling validation of its utility as a powerful generative simulation platform. The approach directly addresses the critical limitations of log-replay and two-stage scenario generation, moving closer to truly reactive and diverse closed-loop simulation."}, "weaknesses": {"value": "1. Limited Motion Prediction Benchmarking: While the core focus is scenario generation, comparing InfGen's motion prediction performance only against its own ablated version (InfGen-Motion vs. InfGen-Full) is insufficient. The motion prediction task (Sec 3.2) is standard, and performance should be compared against state-of-the-art motion prediction baselines on the Waymo Open Motion Dataset (WOMD) to properly contextualize the model's trajectory-modeling capability.\n\n2. Lack of Diversity Metrics for Full Scenario Generation: The paper claims InfGen produces \"diverse\" traffic behaviors but only provides qualitative visualizations (Fig. 4, Fig. 9) and diversity for motion prediction (ADD and FDD). No quantitative metrics (e.g., scene-level diversity, number of distinct agent configurations, or scenario-level rarity metrics) are provided for the full scenario generation task (generating initial states + motions from scratch) or the scenario densification task. This omission makes it hard to objectively compare the diversity of generated scenes against baselines like TrafficGen and UniGen.\n\n3. Scalability Concerns and Trade-offs: The paper notes the high memory demand due to long token sequences for dense scenes and the use of KNN pruning for attention. However, the reported training limitations (max 28-36 agents during training vs. 128 max agents in WOMD)  raise questions about the model's true generalization capacity to high-density scenes. An explicit discussion or experiment on the performance drop or change in behavior when transitioning from low-density training data to high-density inference scenarios would be valuable.\n\n4. Clarity on Relative State Decoding: While the autoregressive decoding of the 8 relative state attributes r_i  is a strong point, the decision to use a tiny Transformer with AdaLN for the Relative State Head  is an architectural detail that could be further justified. Since each of the 8 dimensions is discretized and the sequence is fixed, why is a specialized Transformer (and not a sequence of simpler MLPs conditioned on prior outputs, which might be faster) necessary? This sub-module warrants a more in-depth rationale."}, "questions": {"value": "1. Comparative Motion Prediction Performance: Could the authors provide a comparison of InfGen-Motion or InfGen-Full's ADE/FDE metrics against a few established state-of-the-art motion prediction models (e.g., MTR++, Scene Transformer, or others from the literature review) on the WOMD validation set? This is crucial to quantify the quality of the trajectory modeling component, independent of the initial state generation.\n\n2. Quantitative Diversity Metrics for Scenario Generation: Please provide quantitative metrics (e.g., a simple measure of agent density or a diversity metric like ADE_k/FDE_k across multiple diverse samples of full scenario generation or scenario densification) to objectively support the claims of high diversity and realism against baselines. How diverse are the generated scenarios beyond motion?\n\n3. Role of Agent ID Embedding in ⟨AS⟩ Tokens: For a newly generated agent, the <SOA> token uses a unique Agent ID embedding, EmbAID(i). Since this agent is newly generated, this ID must be incrementally assigned. How is the sequence of future Agent IDs managed by the Transformer during autoregressive generation, and how is the embedding EmbAID(i) for a new, unseen ID i generated or retrieved, given that the ID is learned from the training data's fixed agent indices?\n\n4. Handling of Maximum Agent Count Discrepancy: The paper mentions limiting agents to N=28/36 during training but allowing up to 128 during inference. Can the authors elaborate on how the model generalizes to a significantly larger number of tokens (nearly 4×36×4≈576 tokens at N=36 vs. 4×128×4≈2048 tokens at N=128) for scenes with high agent density, especially considering the Transformer's quadratic complexity without KNN?\n\n5. Causal Flow between <AS> and <MO> Tokens: The Token Group Attention mechanism (Fig. 6) shows <MO> tokens attending to Current <AS> tokens, reflecting the causal flow of motion depending on the current state. However, during the sequential generation at inference time, the full set of <AS> tokens must be generated before the <MO> tokens are generated in a batch. This causality needs clarification: Is the dependency between the final <AS> tokens of all agents at time t and the <MO> tokens at time t fully respected by the attention mask?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a7GZ8NsJEi", "forum": "IWt4ERrdYp", "replyto": "IWt4ERrdYp", "signatures": ["ICLR.cc/2026/Conference/Submission13757/Reviewer_PzMo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13757/Reviewer_PzMo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762393180289, "cdate": 1762393180289, "tmdate": 1762924290016, "mdate": 1762924290016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}