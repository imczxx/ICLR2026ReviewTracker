{"id": "OxWnOV5q8w", "number": 12786, "cdate": 1758210308972, "mdate": 1759897486866, "content": {"title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation", "abstract": "Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.", "tldr": "We investigate the current risk indicators used in risk correlation experiments for NLG UE, identify their shortcomings and propose improvements, as well as an enhanced summarization technique.", "keywords": ["uncertainty", "natural language generation", "evaluation", "large language models", "elo", "judge"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c107127f1c6574a697fd7a8b31c7a8010d6faee1.pdf", "supplementary_material": "/attachment/e325498c0295fca8701d7825e6a09bbe3bb0cf4d.zip"}, "replies": [{"content": {"summary": {"value": "The paper shows that suboptimal NLG eval affects the evaluation of uncertainty quantifiers designed for parameterising decision making pipelines (such as selective prediction and OOD detection) using LLMs.\n\nThe paper shows that different automated proxies for NLG eval lead to different patterns of correlation between UQ and risk in decision making, with high disagreement across the available options. The paper shows that stochasticity inherent to an NLG evaluator (due to predictive uncertainty and/or effects of different prompts) too play a role.\n\nThe paper proposes to aggregate statistics from not one but a collection of NLG evaluators and not one but a collection of decision making settings, as well as to marginalise over choices and sources of stochasticity in evaluation. The findings suggest more robust conclusions are possible with the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear paper, except for cluttered notation here and there, but still mostly clear to me\n2. Evaluation of UQ is important, the paper offers a critical take on the limitations of the typical UQ evaluation protocols, with a reasonable proposal for improvement \n3. Proposed approach is simple, automated, appears to be effective"}, "weaknesses": {"value": "1. I think the paper needs an “oracle” experiment, where the errors in automated NLG evaluation are approximately eliminated by means of human eval. Of course, I don’t expect it to be as large scale as the rest, but still. Finding that the proposed approach allows us to get closer to the quality of conclusions derivable from this oracle setting is, to me, essential. With that in place, I can then take the larger scale, but more indirect, evidence presented with more optimism /\nconfidence that the seemly more coherent conclusions are indeed more meaningful.\n  \nThat is, to me, the main weakness (unless I misunderstand something, but then I’m happy to be corrected) and the reason for my cautious  scores for soundness and contribution."}, "questions": {"value": "Could you please address the weakness point above? Did I miss something in my interpretation of your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fZvBfEINb6", "forum": "OxWnOV5q8w", "replyto": "OxWnOV5q8w", "signatures": ["ICLR.cc/2026/Conference/Submission12786/Reviewer_4ZXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12786/Reviewer_4ZXg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861810320, "cdate": 1761861810320, "tmdate": 1762923594648, "mdate": 1762923594648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically analyzes the pitfalls in evaluating uncertainty estimation methods for natural language generation (NLG), especially in the context of hallucination/confabulation detection in LLMs. The authors demonstrate that commonly used approximate correctness functions (e.g., BLEU, ROUGE, LLM-as-a-judge) can lead to substantial disagreement and bias in the ranking of uncertainty estimation methods. They propose using multiple alternative risk indicators, marginalizing over LLM-as-a-judge variants, and structured/perturbation tasks to improve robustness. The paper also introduces an Elo rating system for summarizing method performance across settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "⦁\tAccurate problem identification, experiments reveal key pitfalls in the evaluation field.\n⦁\tEvaluation suggestions have practical guidance value.\n⦁\tThe paper is rigorous and well-argued."}, "weaknesses": {"value": "⦁\tMainly focuses on evaluation methods themselves, with limited guidance for designing new uncertainty estimation methods.\n⦁\tLacks ablation and robustness analysis for some suggestions (e.g., Elo rating, integrated evaluation).\n⦁\tLacks deeper theoretical discussion on how to fundamentally eliminate evaluation noise and improve consistency."}, "questions": {"value": "⦁\tHow stable is the Elo rating system across different datasets and evaluation metrics?\n⦁\tWhat is the computational cost and scalability of multi-metric fusion evaluation in large-scale practical evaluation?\n⦁\tAre there ablation experiments for different evaluation suggestions?\n⦁\tAre the paper's suggestions equally applicable to new NLG tasks (e.g., multi-turn dialogue, generative reasoning)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6a7AGSUp3C", "forum": "OxWnOV5q8w", "replyto": "OxWnOV5q8w", "signatures": ["ICLR.cc/2026/Conference/Submission12786/Reviewer_oJfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12786/Reviewer_oJfd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924464399, "cdate": 1761924464399, "tmdate": 1762923594128, "mdate": 1762923594128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that QA style selective prediction is brittle because approximate correctness functions may disagree and can be hacked. The proposal consists of alternative risk indicators (structured tasks with exact correctness, OOD, perturbation), SP-MoJI (marginalize across multiple judges/prompts/models), and Elo aggregation across settings. \n\nThe main motivation is that published research in the area mainly evaluates on QA with approximate correctness functions (Table 1), often with ROUGE/BLEU/BERTScore/LLM-as-judge, and little human evaluation. UE is formalized as ranking correlation between uncertainty and a risk indicator (AUROC of rank correlation). Three empirical properties that are posited as desirable are proposed, motivating three experiment families: SP (selective prediction), OOD detection, and perturbation detection. Correctness c_theta is defined and two label-perturbation effects on AUROC are shown. It is also shown how a sample-dependent bias yields a decomposition that mixes distorted/undistorted subsets, implying rank instabilities when we fail to marginalize. Figure 1 shows large disagreement among ROUGE/BLEU vs judge metrics, driven partly by extremely short reference answers; a ROUGE-2/BLEU implementation artifact is also reported. These disagreements translate into inconsistent UE method rankings. It is shown that correctness-hacking (Table 2) can substantially alter top-3 membership.\n\nFollowing the analysis, the following remedies are proposed. a) Exact correctness via structured tasks (code unit tests, constrained generation), which avoids parameterized metrics. b) SP-MoJI: averages the outer correlation across multiple judges/prompts/models to marginalize judge aleatoric/epistemic uncertainty (Eq. 6). Further, bootstraps show a single judge gives SD≈0.04 in AUROC; ~4 judges halves SD; diminishing returns past ~10. c) OOD/perturbation: treats OOD identifiers or corruption strength as risk indicators, implements with Known-Unknowns, SQuADv2, and word-shuffle perturbations. In view of all this they use Elo to aggregate pairwise wins across (dataset x model x task) experiments; 400 Elo roughly corresponds to 1:10 odds. This is good for incomplete overlap across method evaluations and for subsets (QA vs code vs constrained text; instruction-tuned vs pre-trained; OOD vs perturbation)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper doesn't propose brand new math or concepts, but it does present a creative integration of fixes: outer-expectation marginalization, structured tasks as risk indicators, and Elo aggregation. All together, this can meaningfully update evaluation practice. The proposed fixes are easily actionable.\n\nThe analysis before the remedies are proposed (e.g. for AUROC under noise/bias, careful empirical demos for disagreement matrices, correctness hacking) is quite useful. \n\nThe paper is generally clearly written. Explanations are generally clear."}, "weaknesses": {"value": "On page 5, the adversarial metric selection space does not seem to be pre-registered. Without a pre-defined grid, the \"correctness-hacking\" claim could itself involve cherry-picking. \n\nOne page 6, the SP-MoJI diversity factors seem under-specified. Which diversity (model family vs prompts vs decoding) contributes most to variance reduction? \n\nOn page 6: Multiple judges may still share family biases; cross-family calibration isn’t deeply analyzed. Wondering what the authors think of the potential impact. \n\nOn page 7: The paper is light on the Elo details. There is no K-factor/tuning, cycle handling, or uncertainty intervals; alternative ranking models not compared. \n\nI also feel like the paper under-acknowledges the breadth of prior practice. slightly overstating QA dominance. \n\nFinally, I also generally found the engagement with the broader literature slightly frustrating. The paper seems to cite papers by 2-3 groups (other than classical references), and develops around that. I am not going to suggest papers to cite, but placing the paper better would improve it."}, "questions": {"value": "See the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TAbbnXCUgv", "forum": "OxWnOV5q8w", "replyto": "OxWnOV5q8w", "signatures": ["ICLR.cc/2026/Conference/Submission12786/Reviewer_3yHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12786/Reviewer_3yHN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038156553, "cdate": 1762038156553, "tmdate": 1762923593647, "mdate": 1762923593647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper highlights the problem that unreliable generation quality estimation metrics can further mislead the uncertainty estimation evaluation. The authors demonstrate that you can hack the evaluation metric by choosing the proper LLM generation quality metric. This problem is not unique for uncertainty, but it is a challenge for the LLM evaluation in general.  \n\nThe authors suggest two methods:  \n1.\tThey suggest using multiple LLMs as a judge to reduce variance of generation quality assessment.  \n2.\tThey propose using ELO rating for aggregating the scores across multiple different tasks and datasets.  \nTwo problems with the paper:  \n1.\tBias due to inadequate choice of quality metrics were investigated previously in (Santilli et al., 2025) and some solutions to this problem were suggested in (Santilli et al., 2025) and (Vashurin et al., 2025).  \n2.\tELO rating looks redundant compared to simple averaging of metrics. It would be great if you could clarify the situations when it is better than simple averaging.  I think the paper could benefit from better analysis of ELO rating (cases when it is really needed).\n\nLiterature:   \nAndrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, and Sinead Williamson. 2025. Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 743–759, Vienna, Austria. Association for Computational Linguistics. https://aclanthology.org/2025.acl-short.60/"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors suggest two methods:  \n1.\tThey suggest using multiple LLMs as a judge to reduce variance of generation quality assessment.  \n2.\tThey propose using ELO rating for aggregating the scores across multiple different tasks and datasets."}, "weaknesses": {"value": "Two problems with the paper:  \n1.\tBias due to inadequate choice of quality metrics were investigated previously in (Santilli et al., 2025) and some suggestions to this problem were suggested in (Santilli et al., 2025) and (Vashurin et al., 2025).  \n2.\tELO rating looks redundant compared to simple averaging of metrics. It would be great if you could clarify the situations when it is better than simple averaging.  I think the paper could benefit from better analysis of ELO rating (cases when it is really needed)."}, "questions": {"value": "Can you provide particular examples that motivate ELO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A0BDyYfwI8", "forum": "OxWnOV5q8w", "replyto": "OxWnOV5q8w", "signatures": ["ICLR.cc/2026/Conference/Submission12786/Reviewer_Fa32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12786/Reviewer_Fa32"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12786/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273851152, "cdate": 1762273851152, "tmdate": 1762923593241, "mdate": 1762923593241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}