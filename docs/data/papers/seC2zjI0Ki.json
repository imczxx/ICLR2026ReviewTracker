{"id": "seC2zjI0Ki", "number": 9784, "cdate": 1758140261882, "mdate": 1759897697023, "content": {"title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline", "abstract": "If we cannot inspect the training data of a large language model (LLM), how can we ever know what it has seen? We believe the most compelling evidence arises when the model itself freely reproduces the target content. As such, we propose RECAP, an agentic pipeline designed to elicit and verify memorized training data from LLM outputs. At the heart of RECAP is a feedback-driven loop, where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and identifies discrepancies. These are then translated into minimal correction hints, which are fed back into the target model to guide subsequent generations. In addition, to address alignment-induced refusals, RECAP includes a jailbreaking module that detects and overcomes such barriers. We evaluate RECAP on EchoTrace, a new benchmark spanning over 30 full books, and the results show that RECAP leads to substantial gains over single-iteration approaches. For instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text extraction improved from 0.38 to 0.47 - a nearly 24% increase.", "tldr": "RECAP is a new method for extracting memorized data from LLMs. It uses iterative feedback and jailbreak prompts. Evaluated on the EchoTrace benchmark of books and papers, RECAP greatly outperforms previous methods in extracting verbatim passages.", "keywords": ["Copyrighted Content Detection", "Membership Inference Attacks", "Knowledge Tracing", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f2a058dcd0a91a87bcba96d4f16859c2b4ad1c6.pdf", "supplementary_material": "/attachment/823c0f0c2361dc20b96c4d1bc6d787676eb6ee02.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RECAP, an agentic framework designed to extract memorized training data from Large Language Models (LLMs), addressing concerns about the illegal distribution of copyrighted material. The core contribution lies in its Jailbreaker component, which circumvents model alignment, and a Feedback Agent that creates an iterative extraction loop. The method is evaluated on a new benchmark, EchoTrace, comprising copyrighted books and scientific papers, and is accompanied by a detailed ablation study. While the paper tackles a significant and timely problem, the contributions feel incremental. The motivation for a new benchmark is unclear, and the experimental results are not fully compelling due to the choice of closed-source models and a lack of comparison on established benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The primary originality lies in the iterative, agent-based design, specifically the Feedback Agent that refines extraction attempts based on previous failures. The Jailbreaker component, while sharing similarities with existing divergence attacks, is leading to the performance improvement.\n\nQuality: The paper includes a thorough ablation study that analyzes the effectiveness and efficiency of RECAP's components, which is a valuable contribution for practitioners and future research.\n\nClarity: The paper is generally well-written and the RECAP framework is described clearly.\n\nSignificance: The problem of copyright infringement via LLM memorization is a critical issue for the AI community. Providing tools to audit and measure this risk is of high significance."}, "weaknesses": {"value": "Novelty of Core Component: The application of the Jailbreaker, a core novelty, appears similar to divergent attacks proposed in prior work (e.g., Nasr et al., 2023), which also force models to deviate from their alignment. The paper would be strengthened by a clearer distinction and discussion of how this component differs from or builds upon existing attack paradigms.\n\nBenchmark Justification and Scope: The creation of the EchoTrace benchmark is not sufficiently motivated. The field has well-established benchmarks for training data extraction (e.g., the Model Extraction Benchmark, the-stack-smol used by Wang et al., 2024). The choice to create a new one requires justification, especially since its composition (only 35 books and 20 papers, with short 40-token target sequences) may limit the generalizability of the findings. An ablation on the length of the extractable data is notably absent.\n\nExperimental Setup on Closed-Source Models: The evaluation relies heavily on closed-source models (e.g., GPT-4.1, Gemini-2.5-Pro, Claude-3.7) for which the exact training data composition is unknown. This introduces a fundamental validity issue, as it is impossible to confirm whether the EchoTrace sources were actually in the model's training set. The choice of these models over open-weight alternatives is not adequately justified and weakens the evidence for RECAP's efficacy.\n\nComparative Evaluation: The results are less compelling due to the lack of comparison on established benchmarks used by contemporary work. For instance, comparing against Dynamic Soft Prompting (Wang et al., 2024) on the same test sets (e.g., the-stack-smol) would provide a more direct and convincing performance assessment."}, "questions": {"value": "1. What was the specific motivation for creating the new EchoTrace benchmark instead of using or extending well-established datasets like the Pile, the Stack, or its derivatives? Were there specific limitations in these existing benchmarks that EchoTrace aims to address?\n\n2. Could the authors extend their experimental results to include the test sets used by Wang et al., 2024 for Dynamic Soft Prompting? This would allow for a more direct and fair comparison with a closely related state-of-the-art method.\n\n3. The paper mainly focuses on closed-source models. What was the reasoning behind this choice? Could the authors also evaluate RECAP on open-source models with known training data (e.g., GPT-Neo, Pythia) to conclusively verify that the extracted text was indeed part of the training corpus? This would significantly strengthen the validity of the claims.\n\n4. There is a wrong citation of RedPajama (194)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L7xp8JrMt2", "forum": "seC2zjI0Ki", "replyto": "seC2zjI0Ki", "signatures": ["ICLR.cc/2026/Conference/Submission9784/Reviewer_pcch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9784/Reviewer_pcch"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757142127, "cdate": 1761757142127, "tmdate": 1762921269136, "mdate": 1762921269136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of verifying memorized training data in Large Language Models (LLMs) when training data inspection is unavailable. It proposes RECAP, an agentic pipeline featuring a feedback-driven iterative loop and a jailbreaking module, designed to elicit and verify verbatim memorized content from LLMs.Additionally, the paper introduces EchoTrace, a novel benchmark encompassing 35 full-length books and 20 arXiv research papers, totaling over 70,000 40-token passages for evaluation. Experimental results demonstrate RECAP's superiority: it achieves an average ROUGE-L score of 0.46 for copyrighted content across four model families, improves GPT-4.1’s copyrighted text extraction ROUGE-L from 0.38 to 0.47, and ensures no significant contamination from non-training data. The work also includes cost optimization and ethical considerations to avoid copyright misuse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The feedback loop (via Feedback Agent) iteratively refines extractions without injecting excessive external information, reducing false positives.\n2.The jailbreaking module effectively circumvents alignment-induced refusals, addressing a key limitation of prior methods like Prefix-Probing and Dynamic Soft Prompting (DSP).The hybrid memorization score filtering balances extraction quality and cost efficiency, addressing the practicality of iterative pipelines.\n3.The experiments are comprehensive and well-designed:Cover multiple models and domains, ensuring generalizability. Include ablation studies and cost analysis, providing actionable insights for real-world use."}, "weaknesses": {"value": "1.The benchmark overrepresents popular works for both public domain and copyrighted categories. This may overestimate RECAP's performance on less mainstream, rarely scraped texts—critical for assessing real-world applicability.\n2.Non-training data is limited to 5 books released in 2025, with no diversity in genre or timeframes. This makes it hard to validate RECAP's robustness against false positives across varied non-training scenarios.\n3.The jailbreaking module relies on a single static hand-crafted prompt, which the authors acknowledge may fail as LLMs’alignment updates advance. No comparison with dynamic jailbreaking methods is provided, leaving unclear whether static prompts are optimal or just a convenient choice.\n4.The module’s effectiveness is only measured by refusal rates, not by whether jailbroken outputs introduce noise or semantic distortions, which could undermine extraction reliability."}, "questions": {"value": "1.EchoTrace’s focus on popular works may overestimate RECAP's performance. Do you plan to expand the benchmark to include non-mainstream texts and validate RECAP's effectiveness on these?\n2. The jailbreaking module uses a static prompt. Have you tested dynamic jailbreaking methods and compared their success rate, extraction quality, and robustness to future LLM alignment updates?\n3.RECAP is not evaluated on open-source LLMs. Do you expect similar performance on these models, or would the pipeline require modifications ?\n4.The paper omits comparisons with 2024–2025 state-of-the-art methods . Do you plan to replenish these comparisons, and if so, what preliminary insights can you share about RECAP's relative performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XtdiPeb8eX", "forum": "seC2zjI0Ki", "replyto": "seC2zjI0Ki", "signatures": ["ICLR.cc/2026/Conference/Submission9784/Reviewer_os15"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9784/Reviewer_os15"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801921052, "cdate": 1761801921052, "tmdate": 1762921268506, "mdate": 1762921268506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper tackles the task of extracting memorized data from LLM pretraining corpora to provide verifiable evidence of what models have seen during training.\n2. It addresses two key challenges: (i) modern alignment safeguards that cause models to refuse reproducing even public-domain content, and (ii) the limited recall of single-iteration prompting methods that fail to elicit complete memorized passages.\n3. The proposed agentic RECAP pipeline, achieves the highest ROUGE-L scores across all tested models and datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The EchoTrace dataset is a valuable resource for future work. It covers diverse text types (public-domain, copyrighted, and unseen books). The segmentation and event summaries make it easy to test new elicitation or membership-inference methods.\n2. The proposed RECAP method directly addresses both identified challenges. The results are clear and statistically grounded, showing strong and consistent improvements across four major model families."}, "weaknesses": {"value": "In Prefix-Prompting baselines, longer or more detailed prefixes can sometimes lead to stronger verbatim reproduction. It would strengthen the paper if the authors analyzed whether prompt length differences contribute to RECAP’s performance gains."}, "questions": {"value": "Could RECAP framework regulate prompt length within its agentic loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CS6UxrtYOD", "forum": "seC2zjI0Ki", "replyto": "seC2zjI0Ki", "signatures": ["ICLR.cc/2026/Conference/Submission9784/Reviewer_Ytx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9784/Reviewer_Ytx5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992736093, "cdate": 1761992736093, "tmdate": 1762921268078, "mdate": 1762921268078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a method called RECAP to test or verify whether a large language model (LLM) has memorized specific text data during training—particularly potentially copyrighted material, such as full books. To enable this verification, we construct a new dataset comprising public domain works, copyrighted books, and non-training new books, categorized by their likelihood of being memorized, and further integrate research papers into the evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset used is comprehensive enough to effectively demonstrate that the proposed method can successfully guide the LLM to reveal memorized content.\n    \n- The experiments are thorough and examine the method from multiple angles, including interesting phenomena like how popular or “welcome” certain memorized content tends to be."}, "weaknesses": {"value": "- I’m really unsure about the writing style of this work. The method described in the main text feels more like a high-level idea,  very rough and vague. Almost all the actual details are buried in the appendix. I’m not sure if this kind of writing is acceptable, but honestly, it made it super hard for me to understand the method clearly,  so hard that I couldn’t even judge whether the approach is reliable or not. I think the appendix should only support the main text, not carry most of the technical details.\n    \n    - Specifically, the five steps in the RECAP method require very careful reading to fully grasp. Figure 2 is just a functional overview, even with the main text, it’s still confusing because of some unclear keyword usage. Maybe adding a concrete example directly into the Figure 2 flowchart would help a lot. Also, the four diagrams in Figure 2 — the left side seems to show related data, but then suddenly on the right, BERT and ELMo appear. Are those meant to represent language models (I think maybe it’s Parrot BERT?)? The logic connecting them isn’t clearly explained in the figure.\n        \n- I’m also not convinced that using ROUGE-L is the right way to measure an LLM’s ability to reproduce text. ROUGE-L looks at the longest common subsequence between two texts. it allows skipping words. But when we’re talking about copyright, we usually care about whether the model can reproduce the text *exactly*, or at least reproduce a long enough chunk to count as infringement [1]. If that’s the case, maybe we should try using Word Error Rate, like in speech recognition tasks?\n    \n    - Also, how do we interpret the ROUGE-L scores? Intuitively, a higher score means stronger evidence that the LLM remembers the book content. But since there’s a FEEDBACK AGENT involved, how can we tell whether the result comes from the model’s actual memory — or just from being guided by the agent?\n        \n- And one last thing, why doesn’t Table 1 show the DSP + Jailbreak results like Table 2 does?\n    \n\n[1] Copyright violations and large language models"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xs8l4aO0XS", "forum": "seC2zjI0Ki", "replyto": "seC2zjI0Ki", "signatures": ["ICLR.cc/2026/Conference/Submission9784/Reviewer_p8p2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9784/Reviewer_p8p2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998822553, "cdate": 1761998822553, "tmdate": 1762921267724, "mdate": 1762921267724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}