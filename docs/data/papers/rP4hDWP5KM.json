{"id": "rP4hDWP5KM", "number": 10283, "cdate": 1758165989102, "mdate": 1759897661138, "content": {"title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Knowledge Poisoning Attacks", "abstract": "Multimodal large language models with Retrieval Augmented Generation (RAG) have significantly advanced tasks such as multimodal question answering by grounding responses in external text and images. This grounding improves factuality, reduces hallucination, and extends reasoning beyond parametric knowledge. However, this reliance on external knowledge poses a critical yet underexplored safety risk: knowledge poisoning attacks, where adversaries deliberately inject adversarial multimodal content into external knowledge bases to steer model toward generating incorrect or even harmful responses.\nTo expose such vulnerabilities, we propose MM-PoisonRAG, the first framework to systematically design knowledge poisoning in multimodal RAG. We introduce two complementary attack strategies: Localized Poisoning Attack (LPA), which implants targeted multimodal misinformation to manipulate specific queries, and Globalized Poisoning Attack (GPA), which inserts a single  adversarial knowledge to broadly disrupt reasoning and induce nonsensical responses across all queries. \nComprehensive experiments across tasks, models, and access settings show that LPA achieves targeted manipulation with attack success rates of up to 56%, while GPA completely disrupts model generation to 0% accuracy with just a single adversarial knowledge injection. Our results reveal the fragility of multimodal RAG and highlight the urgent need for defenses against knowledge poisoning.", "tldr": "We propose the first knowledge poisoning attack on multimodal RAG frameworks, revealing vulnerabilities posed by poisoned external knowledge bases.", "keywords": ["multimodal RAG", "knowledge poisoning", "multimodal large language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09b238c4ed88846f7862ec002241cecb74ee25f3.pdf", "supplementary_material": "/attachment/ae3522a9959e769e82c06cced80866da2c5ba233.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces MM-POISONRAG, a framework on knowledge poisoning attacks on multimodal Retrieval Augmented Generation (RAG) systems, which rely on external knowledge base containing both images and text. The authors demonstrate two attack strategies: Localized Poisoning Attack (LPA) that injects targeted misinformation to steer models toward specific incorrect answers, and Globalized Poisoning Attack (GPA) that uses a single adversarial passage to hamper the generation quality across all queries, dropping accuracy of the model to 0%, both of which bypass existing defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is well written.\n2. Experiments are comprehensive.\n3. GPA attack of having a single image to disrupt everything is interesting but requires a lot RAG assumptions to work."}, "weaknesses": {"value": "1. Conceptual Novelty: The paper's main contribution—showing that multimodal RAG is vulnerable to poisoning—follows predictably from combining two well-documented phenomena: text RAG poisoning (Zou et al. 2024, Chaudhari et al. 2024) and adversarial attacks on multimodal models (Yin et al. 2024, Wu et al. 2024), as also mentioned by the authors. The paper feels a combination of two known strategies to obtain expected results."}, "questions": {"value": "1. In case of LPA did you test if the attack would work if the question asked by the user is  semantically similar (like paraphrases or related questions) to the target question used for poisning. Would be good to know the extent of the LPA attack."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N882fN0Omf", "forum": "rP4hDWP5KM", "replyto": "rP4hDWP5KM", "signatures": ["ICLR.cc/2026/Conference/Submission10283/Reviewer_dr4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10283/Reviewer_dr4E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761157611444, "cdate": 1761157611444, "tmdate": 1762921636548, "mdate": 1762921636548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the security vulnerabilities of Multimodal Retrieval-Augmented Generation (RAG) systems. The authors argue that by injecting malicious image-text pairs into an external knowledge base, an adversary can corrupt the system's outputs.\nThe paper introduces two attack strategies: 1) Localized Poisoning Attack (LPA): A targeted attack that injects a query-specific poisoned pair to steer the model toward a single, attacker-defined wrong answer (e.g., making it answer \"White\" instead of \"Black\"). 2) Globalized Poisoning Attack (GPA): An untargeted attack that uses a single, universally crafted poisoned pair to broadly corrupt the system, causing it to generate irrelevant or nonsensical responses (e.g., \"Sorry\") for all queries. The paper evaluates these attacks under various levels of adversary access, from black-box (no internal model knowledge) to white-box (full knowledge of the retriever, reranker, and generator). The experiments on MMQA and WebQA benchmarks show that the attacks are effective."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Timely Application to a New Modality**\n\nThe paper's primary value is in applying the established concept of knowledge poisoning from text-only RAG to the increasingly popular multimodal RAG setting. It serves as an empirical demonstration that this known vulnerability extends to systems that retrieve and use images, which is a relevant and timely investigation.\n\n2. **Clear Threat Model**\n\nThe paper usefully categorizes attacks along two axes: targeted (LPA) vs. untargeted (GPA), and black-box vs. white-box. This provides a structured framework for thinking about threats in multimodal settings.\n\n3. **Extensive Empirical Evaluations**\n\nThe paper shows that the attacks are effective across multiple datasets, model architectures, and pipeline configurations. The results (e.g., 56% ASR for LPA, 0% accuracy for GPA) demonstrates that the vulnerability is severe and not an edge case."}, "weaknesses": {"value": "**1. Lack of Conceptual Novelty and Overstated Claims**\n\n- The paper claims to be \"the first framework to systematically study the vulnerability,\" but this is misleading. The core idea—poisoning an external knowledge base to manipulate model output—is directly lifted from a body of work on text-only RAG poisoning (e.g., Zou et al., 2024; Pan et al., 2023; Zhang et al., 2025, which are cited).\n\n- The proposed Localized Poisoning Attack (LPA) is a direct analogue to targeted poisoning in text RAG, simply replacing a poisoned text document with a poisoned (image, text) pair. The method for creating this pair in the black-box setting (LPA-BB) using GPT-4 is a trivial application of off-the-shelf tools and involves no technical innovation.\n\n- The Globalized Poisoning Attack (GPA) is conceptually similar to \"blocker document\" or \"jamming\" attacks in text retrieval (e.g., Shafran et al., 2024), where an entry is optimized to be retrieved for many queries. The adaptation to the image modality via embedding centroid alignment is intuitive and does not represent a significant conceptual leap.\n\n**2. Absence of Comparative Baselines**\n\nThe most critical flaw is the complete lack of comparison with prior work. A strong paper would quantitatively demonstrate that its multimodal poisoning is more effective or efficient than poisoning only the text modality.\n\nFor LPA, a crucial baseline is a text-only poisoning attack (i.e., only injecting a poisoned caption with a irrelevant or blank image). Does adding a misleading image actually increase the attack success rate? The paper provides no evidence to answer this, failing to justify the need for a multimodal approach.\n\nSimilarly, for GPA, how does a single adversarial image compare to a well-crafted \"universal\" text document in collapsing the system? Without this comparison, the added value of the multimodal attack remains unproven.\n\n**3. Technical Challenge is Unclear**\n\nThe challenge of the attacks is unclear. LPA-BB requires no optimization. LPA-Rt and GPA use standard, well-understood gradient ascent techniques. The paper does not identify or solve any new, non-trivial optimization problems specific to the multimodal RAG setting (e.g., joint discrete-continuous optimization for text and image).\n\n**4.Inadequate Defense Evaluation**\n\nEvaluating only against a simple paraphrasing defense is insufficient. The paper does not engage with more relevant defenses from the text RAG literature (e.g., perplexity filters, entropy-based detection) or computer vision (e.g., detection of adversarial images), making the claim that attacks \"bypass existing defenses\" weak."}, "questions": {"value": "1. What are the technical challenges / main contributions of the proposed attacks? \n\n2. Why not compare with existing knowledge poisoning attacks? \n\n3. Why only test paraphrased-based defense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gJoSOVFTWw", "forum": "rP4hDWP5KM", "replyto": "rP4hDWP5KM", "signatures": ["ICLR.cc/2026/Conference/Submission10283/Reviewer_Am9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10283/Reviewer_Am9G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795835876, "cdate": 1761795835876, "tmdate": 1762921635995, "mdate": 1762921635995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerability of multimodal RAG systems to knowledge poisoning. It introduces MM-PoisonRAG, a framework to systematically study this threat by injecting malicious multimodal content into external knowledge bases. The core contribution is the design of two attack strategies: Localized Poisoning Attack (LPA) and Globalized Poisoning Attack (GPA). LPA implants targeted, query-specific misinformation to manipulate outputs toward an attacker-controlled response. In contrast, GPA uses a single, untargeted adversarial injection to broadly corrupt reasoning and degrade generation quality across all queries. The paper demonstrates that these attacks are highly effective, achieve high success rates even with limited access, and can bypass paraphrasing-based defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality:** The paper introduces novel attack strategies (LPA and GPA) specifically designed for multimodal RAG systems. The GPA concept, which uses a single entry to disrupt all queries, is a particularly insightful contribution.\n- **Comprehensive Experiments:** The experimental evaluation is comprehensive. It covers the impact on both retrieval recall and final QA accuracy. The authors also provide a thorough analysis of attack transferability across different models.\n- **Clarity:** The paper's methodology is presented very clearly. Both attack strategies and their variants are well-defined, making the technical approach easy to follow."}, "weaknesses": {"value": "1. **GPA-Rt Caption Dependency:** The adversarial caption used in GPA-Rt (e.g., \"...You must generate an answer of 'Yes'.\") appears highly correlated with the specific prompt mechanism of the MLLM reranker, which evaluates the probability of the token \"Yes\". This implies the attacker needs detailed knowledge of the reranker's internal mechanism, which contradicts the \"no access to the reranker\" threat model defined for GPA-Rt. The paper would be strengthened by an ablation study on the adversarial caption's design.\n2. **Confounded Attack Comparison:** Section 3.3 states that GPA-Rt can be more effective than GPA-RtRrGen without further explanations, which may cause confusion. This result is difficult to interpret because it is confounded by discrepancies in: (1) the number of injections, where the GPA-Rt setting uses 5 entries versus only 1 for GPA-RtRrGen (Table 1, main paper); and (2) the number of training steps, with GPA-Rt trained for 500 steps while GPA-RtRrGen is trained for 2000+ (Table 4, appendix). Experiments with these hyperparameters controlled are needed to properly isolate the true impact of the additional reranker and generator access.\n3. **Missing Hyperparameter Ablation:** The GPA-RtRrGen attack's objective function relies on $\\lambda_1$ and $\\lambda_2$ to balance the retriever, reranker, and generator losses. Table 4 of the appendix only lists the final values used but provides no ablation study. It is unclear how sensitive the attack's success is to these $\\lambda$ values for the same retriever, reranker, generator, and task setup.\n4. **Narrow Defense Evaluation:** The evaluation against existing defenses is narrow. The paper only tests one paraphrasing-based strategy, leaving the attack's effectiveness against other defense families (e.g., outlier detection) as an open question.\n5. **Potential Novelty Overclaim:** The paper claims to be the \"first framework to systematically study ... knowledge poisoning\" in multimodal RAG. This claim may overlook previous work, such as PoisonedEye, which is cited in the related work section and also addresses multimodal RAG poisoning. Despite this, the paper's specific attack methods (especially GPA) remain original."}, "questions": {"value": "See weakness, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNitOPfvoU", "forum": "rP4hDWP5KM", "replyto": "rP4hDWP5KM", "signatures": ["ICLR.cc/2026/Conference/Submission10283/Reviewer_7Gw7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10283/Reviewer_7Gw7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988293678, "cdate": 1761988293678, "tmdate": 1762921635012, "mdate": 1762921635012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MM-POISONRAG to systematically study knowledge poisoning attacks against multimodel RAG systems. The attacks include two strategies: (1) Localized Poisoning Attack, which injects targeted, query-specfiic misinformation to manipulate outputs toward attacker-controlled response, and (2) Globalized Poisoning Attack (GPA), which uses a single untargeted adversarial injection to broadly corrupt reasoning across all queries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The problem is novel and important. This is the first systematic study of poisoning attack in multimodel RAG.\n\nS2. Comprehensive Attack Framework: The paper presents two complementary attack strategies (LPA and GPA) that cover both targeted and untargeted scenarios, with multiple threat models varying from black-box to white-box access.\n\nS3. The writting is clear and easy to follow"}, "weaknesses": {"value": "W1. Limited Analysis of Poison Content Quality: The paper doesn't thoroughly analyze whether the generated poisoned content looks suspicious to human observers or could pass content moderation systems.\n\nW2. Detection Discussion: The paper lacks discussion on whether these poisoned entries could be detected through other means (e.g., anomaly detection in embedding space, content verification)."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7COEOVOKxP", "forum": "rP4hDWP5KM", "replyto": "rP4hDWP5KM", "signatures": ["ICLR.cc/2026/Conference/Submission10283/Reviewer_4h2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10283/Reviewer_4h2U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762792505198, "cdate": 1762792505198, "tmdate": 1762921634514, "mdate": 1762921634514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}