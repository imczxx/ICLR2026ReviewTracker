{"id": "DBFbNT65xO", "number": 21347, "cdate": 1758316492026, "mdate": 1759896927150, "content": {"title": "VectorGym: A Multi-Task Benchmark for SVG Code Generation and Manipulation", "abstract": "We introduce VectorGym, a new comprehensive multi-task benchmark for evaluating Vision-Language Models (VLMs) on Scalable Vector Graphics (SVG) code generation and manipulation. VectorGym addresses the critical need for systematic evaluation across diverse SVG-related capabilities in the emerging field of visual code generation. Our benchmark comprises four complementary tasks: Sketch2SVG conversion, SVG editing with natural language instructions, Text2SVG generation, and SVG captioning. It introduces Sketch2SVG and the first dataset of complex, human-authored SVG edits, with gold-standard human annotations across all tasks. We propose a novel automatic VLM-as-judge evaluation metric specifically tailored for SVG generation tasks, validated through human correlation studies across multiple state-of-the-art models. We provide a comprehensive evaluation of leading closed-source and open-source VLMs, which reveals significant performance variations across tasks, highlighting both current capabilities and critical limitations. VectorGym establishes a new standard for evaluating and advancing SVG generation capabilities, offering the research community a robust framework for measuring progress in this emerging field.", "tldr": "We introduce VectorGym, a benchmark suite for SVG generation and editing: Sketch2SVG, SVG Editing, and Text2SVG. It includes diverse, human-annotated test sets and model evaluations, offering new challenges and insights for vector graphics tasks.", "keywords": ["SVG", "Multimodal", "vector graphics", "code generation", "VLM", "LLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91ccc9a2121125dbc7987f1d248dedf510dcde86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **VectorGym**, a comprehensive benchmark dataset for evaluating SVG-related tasks.  \nVectorGym includes multiple tasks such as **sketch-to-SVG conversion, SVG editing, text-to-SVG generation, and SVG captioning**.  \nThe authors evaluate the performance of various **Vision-Language Models (VLMs)** on these datasets.  \nFurthermore, they assess the results using multiple evaluation metrics, including **VLM-as-a-judge** approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- 1. The paper proposes a human-annotated dataset for several **SVG generation tasks**.\n- 2. The paper provides a detailed evaluation of the **VLM-as-a-judge** framework.\n- 3. Multiple models are evaluated, and the characteristics of each task are analyzed."}, "weaknesses": {"value": "- 1. Although the created dataset is divided into train / validation / test splits for evaluation, only zero-shot evaluation is performed, so the effect of using the data for fine-tuning remains unclear.\n- 2. The paper mentions that an LLM was utilized to assist in drafting the Related Work section; however, the citation link for SVGEditBench directs to an entirely unrelated publication, indicating a potential instance of hallucination in the generated text. Moreover, it appears that the description in the paper actually refers to SVGEditBench2, yet in Table 1 it is simply cited as SVGEditBench, which reduces the accuracy and credibility.\n- 3. The procedure for constructing the SVG Edit dataset is unclear.\n    - 3.1. When humans performed the edits, did they use a drawing tool to modify the images, or did they manually rewrite the SVG code directly?\n   - 3.2. Section A.1.4 presents examples of “Required complex edits”, but how were these criteria defined? What other perspectives were considered?\n    - 3.3. In SVGEditBench2, edit instructions are generated based on differences between similar images, rather than simple rule-based edits, resulting in more complex instructions. Compared to such existing approaches, does the proposed dataset contain more complex and diverse edit instructions?"}, "questions": {"value": "- 1. Regarding the correlation with human evaluation In Table 2.\n    - 1.1 why are the results separated between generated samples and ground truth?\n    - 1.2. For the SVG edit task, why does the correlation become high for generated samples but drop significantly for ground truth? Does this mean that VLMs assign high scores even when the outputs are correctly generated? If that is the case, wouldn’t it imply that the metric fails to properly distinguish correct generations, thus questioning its reliability as a trustworthy evaluation measure?\n    - 1.3 In the Sketch2SVG task, why does the correlation appear lower compared to the other tasks?\n- 2. Benchmarks such as SVGenius also provide comprehensive evaluations across multiple tasks, including editing, understanding, and generation. In comparison to such prior benchmarks, what are the main differences or novel contributions of this paper? Are there distinctions beyond the inclusion of sketch-based data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2DBjRB9GmR", "forum": "DBFbNT65xO", "replyto": "DBFbNT65xO", "signatures": ["ICLR.cc/2026/Conference/Submission21347/Reviewer_8J5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21347/Reviewer_8J5j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921875258, "cdate": 1761921875258, "tmdate": 1762941712240, "mdate": 1762941712240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new human annotated multi-task benchmark for SVG code generation and manipulation, covering Sketch2SVG, instruction-guided SVG editing, Text2SVG, and SVG captioning. The authors curate 7,000 real-world SVGs, collect human sketches, and detailed captions, and evaluate a wide range of proprietary and open-source VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper seems to be the first to combine Sketch, Edit, Text and Captioning tasks, marking for its novelty.\n2. This paper provides extensive evaluations of different tasks using multiple open source models covering major evaluation metrics."}, "weaknesses": {"value": "1. The evaluation selects the best of 5 sampled outputs using the same VLM-judge metric. That creates an evaluation bias and may overstate real single-sample performance.\n2. The VLM-judge validation uses a validation set of only 50 samples per task to compute Pearson correlations with human annotators, which seems to be too small for robust judge selection, given the diversity of SVGs and edit types."}, "questions": {"value": "1. How sensitive is judge selection to the 50-sample validation set?\n2. How do rankings change if you (a) use a single deterministic sample, (b) use median/mean over n samples, or (c) use oracle selection based on human scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RTKK0EHeyf", "forum": "DBFbNT65xO", "replyto": "DBFbNT65xO", "signatures": ["ICLR.cc/2026/Conference/Submission21347/Reviewer_uxfE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21347/Reviewer_uxfE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929617490, "cdate": 1761929617490, "tmdate": 1762941711866, "mdate": 1762941711866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VectorGym, a multi-task benchmark designed to evaluate Vision-Language Models (VLMs) on Scalable Vector Graphics (SVG) generation and manipulation. It spans four tasks: Sketch2SVG, SVG Editing, Text2SVG, and SVG Captioning, supported by a 7k-sample human-annotated dataset. The authors propose a VLM-as-judge metric validated via human correlation and benchmark both proprietary and open-source models, finding GPT-5 and Claude-4 to perform best."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a real evaluation gap in SVG generation and editing with a well-motivated multi-task design.\n\n- Uses human-authored, complex edits and sketches rather than synthetic data.\n\n- Extensive model coverage with consistent zero-shot evaluation across tasks."}, "weaknesses": {"value": "- VLM-as-judge is not novel; prior works in vision-language evaluation (e.g., LLaVA-Bench, EvalAlign) already employ this approach. Framing it as a key contribution is overstated.\n- The term “complex human annotations” is vague and not operationalized--no quantification of complexity, annotator agreement, or examples showing what differentiates them from existing datasets.\n- Table 1 is misleading--it should explicitly mark whether prior datasets included any human annotation. The current comparison may overstate novelty.\n- All four tasks (Text2SVG, Sketch2SVG, Editing, Captioning) have been studied individually in previous benchmarks; the main novelty is unification, not new task design.\n- Sketch2SVG evaluation is questionable: sketches lack full color or geometric precision, but evaluation is done against SVGs which are visually richer. This biases visual-similarity metrics that penalize missing colors or fine details absent in the input.\n- Circular evaluation flaw: best-of-n generation and selection uses the same VLM-as-judge for scoring, biasing results.\n- Reproducibility concerns: heavy reliance on proprietary APIs (GPT-4o, GPT-5) without open-source judge substitute.\n- No statistical testing or confidence intervals on leaderboard results.\n- VLLM as a Judge prompt may not be specific enough since score thresholds are defined in ranges: while interpreting results, its hard to make a distinction between a score 7 or 8 since both are supposed to represent \"Mostyly accurate and complete, minor issues in detail or quality, clear and visually appealing\". Self consistency of these \n- Potential dataset contamination: LLMs (Qwen2-VL) used for caption validation may overlap with evaluated models. Human validation of the captions should have been supported."}, "questions": {"value": "- How are “complex human annotations” defined and measured?\n- In Table 2: Text2SVG and SVG Edit: Claude 4 Sonnet and Gpt4o have the exact same values upto 3 decimal places, as reported. Are you sure these are valid and not a mistake?\n- Following up from the previous question, can the authors provide variance or statistical significance for differences <0.2 in judge scores?\n-What exact filtering thresholds (token count, color entropy) were applied during curation?\n- Can you include examples of human validation vs LLMaaJ in the appendix? Also attach metrics with all qualitative examples to give an idea of how the metrics are significant.\n\nMinor concerns:\nStary characters like upside down ? in line: 385"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VRMcCLHqgK", "forum": "DBFbNT65xO", "replyto": "DBFbNT65xO", "signatures": ["ICLR.cc/2026/Conference/Submission21347/Reviewer_wMeh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21347/Reviewer_wMeh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952825574, "cdate": 1761952825574, "tmdate": 1762941711609, "mdate": 1762941711609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs various benchmarks to evaluate the ability of VLMs to understand SVG data and reports their performance across different models. Specifically, the benchmarks are organized into four tasks: Sketch (creating SVGs from drawings), Edit (editing SVGs), Text (generating SVGs from text), and Cap (captioning SVGs). Evaluations are reported for both open-source and proprietary models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The purpose of this paper is clear. It is meaningful to investigate how well LLMs/VLMs can understand the SVG data format. The authors specifically constructed a large-scale dataset and conducted this investigation.\n\nIt's also worth noting that various experiments were conducted and evaluations performed for both open-source and proprietary models."}, "weaknesses": {"value": "This paper has the following weaknesses.\n\n## Comparison with Previous Benchmarks\n\nThe paper lacks discussion on how it differs from the various benchmarks shown in Table 1. As indicated in Table 1, several benchmarks already exist for evaluating SVG understanding. However, the paper does not discuss what distinguishes its benchmarks from the existing ones, nor does it compare its conclusions and qualitative discussions with those of prior benchmark studies. The absence of such comparisons is problematic.\n\nIn the rightmost column of Table 1, previous studies are marked as lacking \"Complex Human Annotations,\" but the meaning of \"Complex\" is not explained. It is unclear in what sense the annotations in this paper are considered \"complex\" compared to those in previous works.\n\n## Citation of Nonexistent Papers\n\nAmong the papers listed in Table 1, there are significant bibliographic errors in two of them:\n\n- VGBench is cited in this paper as the Findings of EMNLP 2024 by Xia+, which is incorrect. The correct citation is the main conference paper at EMNLP 2024 by Zou+, with a different title. https://aclanthology.org/2024.emnlp-main.213/\n- SVGEditBench is attributed to Shu+ in this paper, which is also incorrect. The correct authors are Nishina+, with a different title and arXiv link. https://arxiv.org/abs/2502.19453\n\nIn particular, for SVGEditBench, \"Changyue Shu\" is listed as the author. However, no such person exists, according to a Google search. Therefore, this error is more than a simple BibTeX mistake.\n\nMisidentifying the most relevant comparative works is a serious flaw. It suggests that the authors may not have properly reviewed prior research, despite claiming that earlier works lacked \"Complex Human Annotations.\"\n\n## Insufficient Description of the Proposed Method\n\nThe section describing the proposed method (Sec. 3) is only one page long, and everything from page 4 onward is devoted to experiments. There are no details about how the task definitions were established or how the dataset was constructed. While the length of a section does not determine a paper's value, the lack of methodological detail makes the paper feel more like a technical report than a scientific research paper.\n\n## Legal Issues Regarding SVG Data\n\nThe authors do not address the legal issues surrounding the SVG data. In Section 3.2, it is stated that SVG data were obtained from the SVG-Stack dataset, and in Section 6 (\"Ethics Statement\"), it is claimed that licensing was handled appropriately. However, there is no explanation of how this was done. Are the SVGs used distributed under licenses such as MIT or CC that allow redistribution? Moreover, do the authors intend to release the dataset? These points are insufficiently discussed."}, "questions": {"value": "Any comments on legal issues?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "See \"Legal Issues Regarding SVG Data\" in the weaknesses section."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uKqdfk1Iro", "forum": "DBFbNT65xO", "replyto": "DBFbNT65xO", "signatures": ["ICLR.cc/2026/Conference/Submission21347/Reviewer_3jQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21347/Reviewer_3jQc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112845879, "cdate": 1762112845879, "tmdate": 1762941711306, "mdate": 1762941711306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}