{"id": "GpHG7VwbIY", "number": 8781, "cdate": 1758098030320, "mdate": 1763541724093, "content": {"title": "PilotRAG: Teaching LLMs Multi-Turn Hybrid RAG via Reinforcement Learning", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge, typically from unstructured texts or structured graphs. While recent progress has extended text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), existing graph-based and hybrid RAG methods generally rely on fixed or handcrafted multi-turn retrieval procedures rather than an RL-trained policy, and thus do not support adaptive, decision-based multi-turn reasoning. This limitation restricts their ability to incrementally integrate supplementary evidence as reasoning unfolds, thereby reducing their effectiveness on complex multi-hop questions. To address this limitation, we introduce PilotRAG, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG by dynamically interleaving reasoning, hybrid retrieval, and answer formulation. PilotRAG jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either unstructured texts or structured graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework with a reward function that accounts for both task outcome and retrieval efficiency. By rewarding answer accuracy and efficient retrieval while penalizing redundant retrieval operations, the model learns to retrieve selectively and reason effectively. Experiments on both simple and multi-hop question answering benchmarks demonstrate that PilotRAG significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL for enabling adaptive and iterative retrieval in complex reasoning scenarios.", "tldr": "We propose PilotRAG, an RL-based RAG framework that enables LLMs to perform multi-turn hybrid RAG with both high accuracy and efficiency.", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f237f82df77f723acc200541053df0dc80769a2c.pdf", "supplementary_material": "/attachment/4264a22b2e91ae89982835d29e2c092a0cb42406.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a RL method that enables LLMs to interleave reasoning and retrieval actions across both text and graph sources. The model uses a two-stage GRPO-based training procedure to balance answer accuracy and retrieval efficiency. Experiments on multiple QA benchmarks claim significant gains over previous RAG and graph-RAG systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts to unify text and graph retrieval with reinforcement learning, producing an end-to-end framework that is well-articulated.\n\n2. The model shows quantitative improvements over existing multi-turn RAG baselines such as Search-R1 and HippoRAG v2 across multiple benchmarks."}, "weaknesses": {"value": "1. The main technical idea is a straightforward combination of existing retrieval types (text + graph) within a standard GRPO framework. There is no genuinely new retrieval or RL formulation; the method feels incremental and engineering-oriented.\n\n2. The so-called “theoretical analysis” in the appendix is a bit trivial as it merely re-states GRPO variance reduction logic with algebraic reformatting, adding no formal insight or new guarantees.\n\n3. Although efficiency is emphasized as a reward signal, the paper provides no real complexity or runtime comparison with baselines beyond qualitative plots. There is no measurement of wall-clock savings, GPU utilization, or scalability under large graphs.\n\n4. The ablation only removes the efficiency reward; no experiment isolates hybrid retrieval, GRPO itself, or the contribution of multi-turn structure.\n\n5. Some other baselines, such as R1-Searcher and SimpleDeepSearcher, are not compared to."}, "questions": {"value": "1. What new insight does “hybrid RRF fusion” bring over existing ensemble retrieval methods like reciprocal rank fusion?\n\n2. Can the authors demonstrate any generalization beyond the closed HotpotQA-like domains, or is this yet another narrow QA-specific RL setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LSY7wJYnSi", "forum": "GpHG7VwbIY", "replyto": "GpHG7VwbIY", "signatures": ["ICLR.cc/2026/Conference/Submission8781/Reviewer_4PR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8781/Reviewer_4PR3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760940761761, "cdate": 1760940761761, "tmdate": 1762920556650, "mdate": 1762920556650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed hybridRAG that use RL to teach the LLM with the capability of multiturn RAG with multiple knowledge sources, such as graph and text.\n\nDisclaimer\nI am surprised to find that my review was labeled as \"fully llm-generated\" in https://iclr.pangram.com/. Here I would like to clarify that all the points I wrote here are my own judgments after reading the paper, and I used GPT to make the review more fluent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts to unify text and graph retrieval with reinforcement learning. In reality, these two source of knowledge can help each other.\n\n2. The model shows quantitative improvements over existing multi-turn RAG baselines such as Search-R1 and HippoRAG v2 across multiple benchmarks."}, "weaknesses": {"value": "1. The main technical idea is a straightforward combination of existing retrieval types (text + graph) within a standard GRPO framework. There is no genuinely new retrieval or RL formulation; the method feels incremental and engineering-oriented.\n\n2. The so-called “theoretical analysis” in the appendix is a bit trivial as it re-states GRPO variance reduction logic with algebraic reformatting, adding limited theoretical contributions compared to existing studies. It would be better to provide more rigorous math proofs. \n\n3. Although efficiency is emphasized as a reward signal, the paper did not provide analysis on real complexity or runtime comparison with baselines beyond qualitative plots. \n\n4. The ablation only removes the efficiency reward; but there are some other perspectives in the method design and the authors are encouraged to study several modules such as hybrid retrieval, the usage of GRPO, or the multi-turn structure.\n\n5. Some other baselines, such as R1-Searcher and SimpleDeepSearcher, are not compared to."}, "questions": {"value": "1. What new insight does “hybrid RRF fusion” bring over existing ensemble retrieval methods such as reciprocal rank fusion [1]?\n\n2. Can the authors demonstrate any generalization beyond the closed HotpotQA-like domains such as browsercomp or simpleqa, or is this yet another narrow QA-specific RL setup?\n\n[1] Cormack, Gordon V., Charles LA Clarke, and Stefan Buettcher. \"Reciprocal rank fusion outperforms condorcet and individual rank learning methods.\" Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. 2009."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LSY7wJYnSi", "forum": "GpHG7VwbIY", "replyto": "GpHG7VwbIY", "signatures": ["ICLR.cc/2026/Conference/Submission8781/Reviewer_4PR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8781/Reviewer_4PR3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760940761761, "cdate": 1760940761761, "tmdate": 1763342635789, "mdate": 1763342635789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly extends graph-text hybrid RAG into multi-turn scenarios. It utilizes RL training by constructing performance rewards and efficiency rewards. It conducts experiments on several datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The methods are clearly stated. Experiments validate the methods on multiple datasets."}, "weaknesses": {"value": "1. I think the innovation of the paper's methods is limited. In Section 3.1, the overall model architecture is very similar to previous iterative RAGs, such as Search-R1, except that the retrieval methods are expanded to hybrid ones. However, these retrieval methods are also implemented in previous work, as shown in Section 3.1.2. Regarding RL optimization, Stage 1 simply measures the reward for task completion. Although Stage 2 introduces total search time as a loss, this does not align with the core motivation in the abstract, and does not provide targeted optimization for Graph-RAG. Therefore, I consider this paper's approach to be incremental.\n2. I think Algorithm 1 needs to be optimized because it's unclear. While I can generally understand what it describes after reading the subsequent sections, some of the symbols used in this algorithm are not explained, making it a bit confusing.\n3. Experiments were conducted only on the Qwen 2.5-3B model. Validation on 7B and higher models is strongly recommended.\n4. In the provided anonymous Github repository, under \"Step 1: Start Services,\" it is mentioned that Llama-3.1-8B-Instruct was used in the experiments, but this is not mentioned in the paper. Please provide more explanations."}, "questions": {"value": "See weaknesses above. Please respond to these cons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ko2PwJYlCm", "forum": "GpHG7VwbIY", "replyto": "GpHG7VwbIY", "signatures": ["ICLR.cc/2026/Conference/Submission8781/Reviewer_vAcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8781/Reviewer_vAcV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705015817, "cdate": 1761705015817, "tmdate": 1762920556202, "mdate": 1762920556202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve current RL-based multi-turn RAG method with hybrid search. Specifically, each search action can be selected from passage retrieval, graph-based retrieval, or a hybrid of both. Then, the model is trained with GRPO with a 2-stage method. In the first stage, the model is trained only to achieve higher accuracy, and in the second stage, the model is also encouraged to be more efficient measured by time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Combining text-based and graph-based retrieval in the RL-based multi-turn RAG in principle should help the model achieve higher performance with more adaptive retrieval results\n- Experiment results show the method achieves strong performance, especially the stage-2 training shows it can improve efficiency while not sacrificing performance\n- Paper writing is clear"}, "weaknesses": {"value": "- The proposed method seems to achieve promising performance on multi-hop QA datasets but the performance on simple QA is clearly hurt with hybrid search (e.g., comparing search-r1 with PILOTRAG w/o efficiency training). Therefore, there should be more analysis on why hybrid strategy fails on simple QA and how to mitigate it.\n- Some efficiency comparison could be beneficial to show the effects of stage 2 training other than just improving performance."}, "questions": {"value": "- When testing the model, what is the ratio of the 3 search actions on different datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp3XLPStjk", "forum": "GpHG7VwbIY", "replyto": "GpHG7VwbIY", "signatures": ["ICLR.cc/2026/Conference/Submission8781/Reviewer_Gxyx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8781/Reviewer_Gxyx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028169005, "cdate": 1762028169005, "tmdate": 1762920555901, "mdate": 1762920555901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the limitation of one-shot retrieval in both traditional and graph RAG systems by fine-tuning a multi-step hybrid RAG policy LLM - PilotRAG. It features a two-stage RLVR pipeline by first performing outcome-oriented training then accuracy-efficiency training . By training a policy LLM, the model learns to interleave the three different retrieval actions and reasoning. After optimizing on hotpotQA, PilotRAG outperforms recent graph RAG and multi-RAG on same generator model (Qwen2.5) across multiple different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes an effective and practical reward system to help policy model balance between the reasoning efficiency and tool-use in a hybrid RAG setting (both textual and relational information are presented)."}, "weaknesses": {"value": "1. Some of the technical details are not described in the paper, such as generalization of the graph construction algorithm (re-uses hipporag graph).\n\n2. The paper is not aware of some important baselines that are published on the same topic: hyrbid text and graph rag. For example, HybGRAG[1] introduces hyrbid graph retrieval: choice between vector search and one-hop graph search with a pre-trained LLM (Sonnet3.5). These related work are not discussed, which also makes the claim in the abstract \"graph-based RAG remains limited to one-shot retrieval\" unappropriate. \n\n3. In case study, all of the Pilot-RAG examples selects the graph retrieval, neither text nor hybrid retrieval are selected. It seems the performance improvements are from the graph retrieval instead of the RL. \n\n[1] HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases, ACL 25'"}, "questions": {"value": "1. Does Pilot-RAG generalize to different graphs such as thoese in CRAG [1] and hyperlink graphs? \n\n2. The training dynamics showing non-convergent #actions and #response length. Does this mean model is not converged or trained properly since no emerging reasoning is observed as other R1* work.  \n\n3. Please refer to my other questions in weaknesses. \n\n\n[1] CRAG - Comprehensive RAG Benchmark"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DTOvYBRjVq", "forum": "GpHG7VwbIY", "replyto": "GpHG7VwbIY", "signatures": ["ICLR.cc/2026/Conference/Submission8781/Reviewer_6eey"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8781/Reviewer_6eey"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244188487, "cdate": 1762244188487, "tmdate": 1762920555148, "mdate": 1762920555148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}