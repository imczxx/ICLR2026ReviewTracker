{"id": "79wzDmy6nc", "number": 7083, "cdate": 1758007026999, "mdate": 1763726861253, "content": {"title": "A$^2$RM: Adversarial-Augmented Reward Model", "abstract": "Reward models (RMs) are central to aligning large language models via Reinforcement Learning. However, trained on static and finite preference datasets, they tend to learn spurious correlations rather than semantic preferences, making them vulnerable to out-of-distribution inputs and contributing to reward hacking. To overcome this, we propose Adversarial-Augmented Reward Model (A$^2$RM), a framework that systematically exposes and patches these vulnerabilities. A$^2$RM employs an adversarial generator, optimized with reinforcement learning, to transform standard preference data into inverted pairs.Within these pairs, an adversarial response is crafted to be semantically identical to the human preferred answer but scored by the RM as lower than the rejected response, directly creating a conflict between semantic content and the reward signal. By dynamically augmenting the training set with these identified high-information adversarial responses, A$^2$RM iteratively refines the reward model, compelling it to learn more robust preference representations. Comprehensive experiments validate that A$^2$RM achieves a 51.1\\% average higher accuracy on adversarial responses, while maintaining comparable performance on original ones.", "tldr": "", "keywords": ["Reward Model", "Adversarial Augment"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c3a4458ee81aec8aa4b4a675cc64ee547e7c8f0.pdf", "supplementary_material": "/attachment/6b8bec1a16879e8cc756361bc0144aeae70ef3ff.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes $A^2RM$ (Adversarial-Augmented Reward Model), a framework that improves the robustness of reward models (RMs) used in RLHF. Traditional RMs trained on static preference datasets often learn superficial correlations and are vulnerable to out-of-distribution (OOD) or adversarial inputs, leading to reward hacking. $A^2RM$ introduces a reinforcement learning–based adversarial generator that creates semantically equivalent but reward-breaking responses (chosen′), verified by a judge LLM for semantic consistency. These adversarial samples are then used to augment the dataset, iteratively retraining the RM to better distinguish semantically similar but reward-deceptive cases. Experiments on LLaMA-3.1 and Qwen2.5 families demonstrate large robustness gains on adversarial benchmarks while maintaining standard performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a concrete and consequential weakness in alignment pipelines: reward hacking, and presents a clear mechanism to directly expose and repair reward model blind spots.\n    \n2. The framework design is well-motivated and operationally practical, bridging adversarial data augmentation with preference learning.\n    \n3. Experiments are broad and results are strong, showing cross-model robustness and detailed analyses on data mixture ratios and iteration strategies that can inform future RM training protocols."}, "weaknesses": {"value": "1. This paper aims to address the reward hacking problem but evaluates only on preference-style adversarial tests (JudgeBench, RewardBench), without connecting the results to jailbreak or safety benchmarks, where reward hacking most visibly manifests.\n    \n2. Dependence on a large external judge LLM for semantic filtering adds cost and bias risk; the paper lacks sensitivity analysis on how weaker or domain-specific judges affect results.\n\n3. The paper assumes that semantic equivalence implies consistent reward ranking, but this can be false in practice. Two responses that are nearly semantically identical may differ substantially in reward quality, for example in code generation where a single-line bug makes one snippet incorrect while two responses are semantically identical; the paper lacks discussion addressing such counterexamples."}, "questions": {"value": "1. How stable is the min–max optimization over multiple iterations? Does the generator collapse to trivial perturbations or continue to expose new failure modes as the RM hardens?\n    \n2. The approach heavily relies on semantic consistency judged by another LLM, how do errors or biases in that model affect adversarial quality and downstream robustness? Or did the author analyze different judge models?\n    \n3. Jailbreaking may be the most direct real-world manifestation of reward hacking. Evaluating trained models on jailbreak benchmarks (e.g., AdvBench, JailbreakBench) and some jailbreaking methods (e.g., GCG, AutoDAN or PAIR) would critically verify whether the proposed robustness extends beyond semantic perturbations to true safety-critical exploits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g16KPpphc2", "forum": "79wzDmy6nc", "replyto": "79wzDmy6nc", "signatures": ["ICLR.cc/2026/Conference/Submission7083/Reviewer_wzXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7083/Reviewer_wzXQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172656922, "cdate": 1761172656922, "tmdate": 1762919265931, "mdate": 1762919265931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their careful and constructive feedback.\n\nWe are pleased to see that all reviewers acknowledged the novelty of our proposed adversarial augmented framework. As an adversarial training method, A$^2$RM substantially improves the robustness of reward models.\n\nAt the same time, we appreciate that all reviewers highlighted two common concerns: \n\n- The reliability of the judge model.\n- The discontinuity of the binary reward signal.\n\nIn response, we have added experiments and analyses. The judge model achieves approximately 85% agreement with human annotations, supporting its reliability. Additionally, we observe that continuous reward signals may introduce noise, resulting in weaker performance compared with the binary signal.\n\nOverall, we believe our work provides a valuable and thought provoking contribution to the community and will inspire future research in adversarial training for reward models."}}, "id": "fq9k17l8bI", "forum": "79wzDmy6nc", "replyto": "79wzDmy6nc", "signatures": ["ICLR.cc/2026/Conference/Submission7083/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7083/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7083/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763639719231, "cdate": 1763639719231, "tmdate": 1763639719231, "mdate": 1763639719231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the critical vulnerability of Reward Models (RMs) to \"reward hacking,\" which stems from their training on static and finite preference datasets. It proposes a novel framework, $A^2RM$ (Adversarial-Augmented Reward Model), to dynamically and iteratively harden these models. The core of this method is an \"AdvGenerator,\" a policy optimized with reinforcement learning, tasked with creating \"inverted pairs\". These are adversarial responses ($c'$) that are semantically identical to the human-preferred \"chosen\" response ($c$) but are scored by the target RM as being worse than the \"rejected\" response ($r$). To ensure semantic integrity, this generation process is constrained by a \"Judge LLM\" that filters for semantic consistency. By augmenting the training data with these challenging, high-information samples, $A^2RM$ compels the RM to learn more robust preference representations. The experimental results are strong, demonstrating that this method significantly improves accuracy on adversarial datasets (e.g., a 51.1\\% average gain) while successfully maintaining performance on standard, in-distribution benchmarks. In general. I think this paper could be accepted if the authors fix some the issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its new and highly effective methodology. The $A^2RM$ framework is an elegant solution to the static data problem, intelligently formulating the hardening process as a min-max optimization. The design of the adversarial reward function (Eq. 5) is particularly noteworthy, as it precisely optimizes for the desired \"inverted pairs\" by combining the RM's score difference with a semantic consistency check from a Judge LLM. This method is supported by comprehensive and convincing empirical evidence. First, the results in Table 1 show a dramatic improvement in robustness on both seen ($A^2Adv$) and unseen (StyleAdv) adversarial benchmarks, without the common trade-off of sacrificing performance on standard evaluations. Second, the ablation studies presented in Section 4.3 are exceptionally thorough and provide valuable, practical insights for the research community. These include key findings on the optimal 1:10 data mixing ratio, the importance of a cumulative data strategy (retaining samples from all previous iterations), and the superior performance of retraining both the RM and the AdvGenerator from their original (scratch/SFT) states in each iteration."}, "weaknesses": {"value": "Despite its strengths, the paper has several weaknesses that should be addressed. \n1) The most critical weakness is the framework's core dependency on an external, and presumably more powerful, \"Judge LLM\" (Qwen3-32B) to provide the semantic consistency signal $Cons(c,c^{\\prime})$. This introduces a significant, undiscussed single point of failure: if the Judge LLM is itself biased, flawed, or vulnerable, it could poison the augmentation dataset and compromise the entire training process. This reliance effectively \"transfers\" the robustness problem from the RM to the Judge LLM. It's better to provide a experiment results on LLM judgement vs human labelling.\n\n2) Another major weakness is the lack of analysis regarding the inconsistent attack performance shown in Table 2. The AdvGenerator demonstrates a near-perfect attack success rate on JudgeBench (e.g., 96.09\\%) but a substantially lower rate on RewardBench (e.g., 53.12\\%); the paper reports these figures but fails to investigate or explain this significant discrepancy. \n\n3) Finally, the paper's claims of being an \"efficient\" framework are unsubstantiated. It criticizes prior work as \"computationally expensive\" but later admits its own RL-based generator requires 40 hours of training per iteration on a powerful 8-GPU node, yet provides no direct cost comparison to justify its efficiency claims vs other approaches. It will be better to provide more experiment results on resource and time cost."}, "questions": {"value": "1) The Judge LLM provides a binary (0 or 1) consistency score. Would using a \"soft\" reward (e.g., a continuous semantic similarity score) provide a more stable and informative gradient for the AdvGenerator's RL training? If a binary reward is better, please explain why.\n\n2) The finding in Sec 4.3.3 that retraining the AdvGenerator from its initial SFT state ($\\pi_{\\psi_0}$) is superior to continual fine-tuning ($\\pi_{\\psi_1}$) is interesting. Does this imply that the AdvGenerator \"overfits\" to the specific vulnerabilities of a single RM iteration, thus requiring a reset to effectively discover the \\textit{new} vulnerabilities in the hardened $R_{\\theta_1}$?\n\n3) The data in Appendix C (Fig. 5) suggests that while adversarial accuracy saturates, standard benchmark performance on RewardBench begins to degrade by the third iteration. Does this point to a fundamental trade-off, where excessive adversarial training eventually harms in-distribution generalization?\n\n4) How do the authors respond to the critique that $A^2RM$ does not solve the robustness problem but merely \"transfers\" it to the external Judge LLM? What would happen to the framework's effectiveness if an attacker were to target the Judge LLM itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "On8Tq5YD0D", "forum": "79wzDmy6nc", "replyto": "79wzDmy6nc", "signatures": ["ICLR.cc/2026/Conference/Submission7083/Reviewer_FLFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7083/Reviewer_FLFU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721999810, "cdate": 1761721999810, "tmdate": 1762919265298, "mdate": 1762919265298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reward models trained on static preference datasets tend to learn spurious correlations, which can lead to reward hacking. The paper proposes a framework to systematical patch these vulnerabilities using an adversarial generator. An adversarial pair refers to one in which is semantically similar to the human preference pair but scored by RM to have the opposite preference. This process refines the RM iteratively to learn from adversarial pairs while maintaining performance on conventional preference pairs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe adversarial generation pipeline is reasonable, primarily seeking to reduce the reward of the adversarial samples while keeping the semantic meaning of the adversarial the sample. \n2.\tThe choice of benchmarks is good and diverse, although RewardBench is a little bit dated."}, "weaknesses": {"value": "1.\tThe approach used to assess if two responses are semantically similar is a little under-explored. For instance, RM-Bench [1] finds adversarial pairs by asking an LLM to change a few keywords in a good correct response to make it incorrect. If the “corruption” is as minor as changing a few words, it’s unlikely to result in a semantic change by simply passing it to a LLM-Judge. Therefore if the AdvGenerator only changes a few words, I think Cons(c, c’) will stay at 1 – meaning that as long as there’s no large change, this term doesn’t influence training. Did the team consider continuous measures of semantic similarity such as embedding similarity? This will also likely be much faster for inference. \n2.\tThe paper discusses Adv-RM (Bukharin et al., 2025) but did not compare to it in Table 2 – is there a reason for it? The other baseline methods seem slightly dated (e.g. 2024 or earlier). \n3.\tI’m not convinced by the evaluation results. The models perform better on A2Adv and StyleAdv which are expected given that the test samples are generated in a similar way as the training samples (and therefore might be overfitted). However, they do not show a boost in the standard benchmark scores. Why is that so? If the goal of building Adverserial RMs is to improve RMs, we should see the standard benchmarks improve as well. This is particularly for JudgeBench since JudgeBench responses are all generated by the same model with small variations, which should show some gains if the gains generalize.\n\n[1] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li. RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style. ICLR 2025."}, "questions": {"value": "1.\tDid the team consider using RM-Bench? It’s likely to be a good benchmark to test adversarial sensitivity given how it’s constructed.\n2.\tWas there some analysis on what kind of changes the AdvGenerator was doing? This will bring clarity into how the AdvGenerator works.\n3.\tWith the PPO algorithm, there’s an assumption that there’s a trained value function – is there? I don’t see that this is done in the paper. Otherwise, this seems like the classical REINFORCE algorithm using the r (or c) reward as the baseline in calculating advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ObRaBEEYeX", "forum": "79wzDmy6nc", "replyto": "79wzDmy6nc", "signatures": ["ICLR.cc/2026/Conference/Submission7083/Reviewer_PCAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7083/Reviewer_PCAe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869279053, "cdate": 1761869279053, "tmdate": 1762919264673, "mdate": 1762919264673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposesAdversarial-Augmented Reward Model(A2RM), a framework designed to improve reward model robustness through adversarial data augmentation. Specifically, a policy model generates semantically similar but adversarial responses that receive unexpectedly low scores from the current reward model. These challenging samples—validated by a Judge model for semantic consistency—are then incorporated into the reward model’s training data to iteratively refine its judgment and reduce reward hacking."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper addresses the critical issue of reward model vulnerability to semantically deceptive responses, an increasingly important problem in RLHF and preference optimization.\n\n2.The idea of generating semantically consistent but reward-contradictory samples and iteratively retraining the RM is novel and conceptually appealing.\n\n3.The full pipeline (AdvGenerator → Judge LLM → selective augmentation → retraining) is clearly explained, with algorithmic pseudocode and reproducible experimental settings.\n\n4.Empirical results show consistent gains in adversarial accuracy with minimal degradation in standard benchmarks."}, "weaknesses": {"value": "1.The Judge model plays a pivotal role in determining semantic consistency between the original response and adversarial response. Its correctness directly affects data quality, yet the paper provides no quantitative analysis of the Judge’s reliability.\n\n2.Only samples that are both semantically similar and receive lower RM scores than the rejected answer rrr are retained. This dual filtering likely leads to low data utilization, but the paper lacks statistics on selection rates or sample efficiency across iterations.\n\n3.The binary use of the Judge output (0/1) to gate adversarial rewards may cause unstable gradients and discard informative near-consistent samples.\n\n4.Since adversarial examples are produced by the same AdvGenerator used during training, the RM may overfit to that generator’s attack style rather than learning truly generalizable robustness."}, "questions": {"value": "1.How reliable is the Judge model in determining semantic consistency? Please provide a quantitative evaluationand analyze how variations in Judge quality affect the final robustness and stability of the reward model.\n\n2.After applying both semantic-consistency filtering and adversarial-score selection, what proportion of generated samples are actually retained for training? A detailed analysis of sample retention and effective data usage would clarify the framework’s efficiency.\n\n3.Could the current binary signal be replaced by a continuous score to yield denser reward feedback and smoother gradient updates? A comparison between binary and soft consistency would be informative.\n\n4.How well does the trained generator generalize beyond its own attack style? Have you evaluated cross-generator robustness, such as testing the RM against adversarial examples produced by a different generator or decoding configuration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ILrtCOtUO2", "forum": "79wzDmy6nc", "replyto": "79wzDmy6nc", "signatures": ["ICLR.cc/2026/Conference/Submission7083/Reviewer_PsK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7083/Reviewer_PsK3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992479179, "cdate": 1761992479179, "tmdate": 1762919264163, "mdate": 1762919264163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}