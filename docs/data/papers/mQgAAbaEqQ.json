{"id": "mQgAAbaEqQ", "number": 3469, "cdate": 1757438508101, "mdate": 1763665700074, "content": {"title": "HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling", "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and question-answering capabilities. However, they often produce inaccurate or unreliable content known as hallucinations. This unreliability significantly limits their deployment in high-stakes applications. Thus, there is a growing need for a general-purpose method to detect hallucinations in LLMs. In this work, we introduce HalluField, a novel field-theoretic approach for hallucination detection based on a parametrized variational principle and thermodynamics. Inspired by thermodynamics, HalluField models an LLM‚Äôs response to a given query and temperature setting as a collection of discrete likelihood token paths, each associated with a corresponding energy and entropy. By analyzing how energy and entropy distributions vary across token paths under changes in temperature and likelihood, HalluField quantifies the semantic stability of a response. Hallucinations are then detected by identifying unstable or erratic behavior in this energy landscape. HalluField is computationally efficient and highly practical: it operates directly on the model‚Äôs output logits without requiring fine-tuning or auxiliary neural networks. Notably, the method is grounded in a principled physical interpretation, drawing analogies to the first law of thermodynamics. Remarkably, by modeling LLM behavior through this physical lens, HalluField achieves state-of-the-art hallucination detection performance across models and datasets.", "tldr": "We introduce a field-theoretic framework and HalluField, an algorithm that detects hallucinations by tracking the energy‚Äìentropy dynamics of an LLM‚Äôs response under temperature perturbations.", "keywords": ["Large Language Models", "hallucination detection", "variational principle", "post-hoc/model-agnostic method"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d96af3fd3bd937d871288223457eb0bc0daca4de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method to detect hallucinations in LLM responses by using semantic instability to determine the likelihood of hallucinations. The authors do this by measuring the change in internal energy of the tokens, inspired by laws of thermodynamics. They add to the extensive literature on uncertainty-based methods for hallucination detection, and test their method on four QA benchmarks across several LLMs, in comparison to other hallucination detection baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-The time complexity of the authors‚Äô approach is much more efficient than the SOTA baselines. \n-Four different LLM-families are used to evaluate HalluField on four datasets, which is inconsistent with other experimental set-ups in the literature."}, "weaknesses": {"value": "- Several more recent hallucination-detection baselines are missing: SINdex (Abdaljalil et al., 2025) and RACE (Wang et al., 2025).\n- HalluField does not outperform other baselines in all contexts, as shown in the results tables (2-4)\n- Only open-source LLMs can be used for this method. Is there  a way to adapt this to black-box LLMs as well?"}, "questions": {"value": "- Why is it that the performance is the most inconsistent for the TriviaQA dataset? \n- See Weaknesses for further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rusE4Pdqgo", "forum": "mQgAAbaEqQ", "replyto": "mQgAAbaEqQ", "signatures": ["ICLR.cc/2026/Conference/Submission3469/Reviewer_nx7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3469/Reviewer_nx7L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754509012, "cdate": 1761754509012, "tmdate": 1762916740209, "mdate": 1762916740209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "After considering the reviews, we have decided to withdraw the submission to allow for further improvement."}}, "id": "Ayx5YiRFVs", "forum": "mQgAAbaEqQ", "replyto": "mQgAAbaEqQ", "signatures": ["ICLR.cc/2026/Conference/Submission3469/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3469/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763665699189, "cdate": 1763665699189, "tmdate": 1763665699189, "mdate": 1763665699189, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Goal:\n- This paper addresses the problem of hallucination detection, e.g., estimating how likely a model‚Äôs generated answer is correct or factually consistent. The goal is to produce a numerical score that reflects the reliability of each model response, with higher scores for factual answers and lower scores for hallucinated ones.\n\nMethod:\n- The proposed method builds on the line of research represented by Semantic Entropy (SE), which estimates uncertainty by sampling multiple responses and comparing their semantic variability. However, instead of relying on a separate semantic clustering step as in SE, this work directly analyzes the model‚Äôs internal token-level probabilities under different temperature settings. By quantifying how these probabilities fluctuate with temperature, the method captures the model‚Äôs intrinsic response stability.\n\nContribution\n- The main contribution is the new hallucination detection algorithm, HalluField, which measures uncertainty without any semantic clustering or auxiliary models. It leverages the variation of token probabilities across temperature perturbations to derive an uncertainty score that distinguishes factual from hallucinated outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithm itself is new and novel, without requiring an additional semantic clustering model.\n2. The evaluation metrics and datasets follow the standard literature."}, "weaknesses": {"value": "1. The paper‚Äôs exposition could be clearer. Introducing the first law of thermodynamics as an analogy does not necessarily make the story more compelling, especially when there is no theoretical grounding; instead, it adds unnecessary conceptual and notational complexity. The notation is rather dense and sometimes unconventional, making a relatively simple idea appear overly complicated. Readers need to spend substantial effort cross-referencing earlier equations to fully understand the computation process. For instance, Algorithm 1, which references seven equations from previous sections, is difficult to follow and fails to serve its intended role as a step-by-step illustration of the method.\n2. The experimental evaluation relies on a relatively narrow selection of benchmarks. It would be good to see how the method performs across more diverse or realistic application domains. Besides, what kind of prompt do you use for the experiments? Is it naive short-form QA, or does it also include COT reasoning? \n4. Only 1 single-sample-based method P(true) is considered.\n3. Regarding the evaluation pipeline, to what extent do you think the observed AUC improvements genuinely reflect better hallucination detection comparison between different models, since the accuracy and output generations for each compared method are also changing?"}, "questions": {"value": "1. Beyond the algorithmic formulation itself, what is the main conceptual or methodological takeaway you hope the audience will gain from this work? In other words, what broader insight or principle about the hallucination detection task and experiments do you think this paper will bring to this community?\n2. Have you considered conducting an ablation study to analyze how each component of your method contributes to the overall performance? It would be helpful to understand which part of the design is most critical for improving accuracy or stability.\n3. In the abstract, you describe the proposed approach as a general-purpose hallucination detection algorithm. Could you elaborate on how this method could be applied to more realistic or diverse application scenarios? Additionally, more insights on what kind of performance we should expect in such settings, and why this is the case, are appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nd9qOPw1Rz", "forum": "mQgAAbaEqQ", "replyto": "mQgAAbaEqQ", "signatures": ["ICLR.cc/2026/Conference/Submission3469/Reviewer_MqLb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3469/Reviewer_MqLb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807834450, "cdate": 1761807834450, "tmdate": 1762916739557, "mdate": 1762916739557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel hallucination detection framework inspired by thermodynamics. The authors model the generation process of large language models (LLMs) as an analogue to a thermodynamic system, introducing quantities such as free energy and temperature‚Äìentropy to describe token-level behaviors. The central algorithm, HalluField, estimates the \"internal energy variation\" of a sequence by aggregating two key terms: (1) the variation of token-level negative log-likelihood (\"free energy\") and (2) the variation of next-token entropy across multiple temperature perturbations. Empirically, HalluField is significantly faster because it does not require auxiliary LLM queries. Its variant, HalluFieldSE, achieves modest but consistent AUC gains over prior entropy-based detectors (e.g., Semantic Entropy and Kernel Language Entropy) across multiple benchmarks. \n\nWhile the proposed approach successfully identifies temperature-dependent stability patterns that correlate with hallucination likelihood, the paper‚Äôs extensive thermodynamic analogy is largely explanatory in nature. The empirical contributions could be presented and motivated without invoking physical conservation laws. Nonetheless, the paper presents interesting empirical findings about the sensitivity of token probability distributions under temperature perturbation and offers a practical and efficient detection scheme."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. *Empirical novelty*: The discovery that cross-temperature instability of token probabilities and entropies correlates with hallucination is an interesting and empirically supported observation.\n1. *Practical efficiency*: HalluField requires only base-model logits and is orders of magnitude faster than detectors relying on auxiliary LLMs.\n2. *Clarity of algorithmic pipeline*: Algorithm 1 and equations (13‚Äì17) describe a clear procedure.\n3. *Cross-dataset evaluation*: The authors test across multiple LLMs and QA datasets, providing a reasonably broad empirical view.\n4. *Complementarity*: Combined with Semantic Entropy (HalluFieldSE), the method often yields incremental performance improvements."}, "weaknesses": {"value": "1. *Unnecessary and weakly supported analogy*: The thermodynamic framework adds complexity without theoretical necessity. It lacks formal justification or derivation from model behavior. Token-level negative log-likelihood and entropy can be motivated directly from information theory and prior NLP literature.\n2. *Lack of statistical rigor*: Reported AUC gains (0.01‚Äì0.09) lack confidence intervals or significance tests, making it unclear whether the improvements are statistically meaningful.\n3. *Missing ablations*: No experiments isolate the contribution of each component, such as removing the temperature‚Äìentropy term, testing a single temperature, or varying the weighting scheme.\n4. *Overreliance on appendix*: Crucial figures (e.g., cross-temperature separation plots) and accuracy metrics are in the supplementary material. It undermines the completeness of the main paper."}, "questions": {"value": "1. Can you provide statistical confidence intervals or significance tests for the AUC improvements to demonstrate robustness?\n2. How sensitive are the results to the temperature schedule and weighting functions in Eq. (14)? Have you tested alternative schedules?\n3. Could you include ablations removing one component at a time (Œîùêπ only, Œî(ùëáùêª) only, or single temperature) to confirm the contribution of each term?\n4. For practical deployment, is there a recommended or fixed threshold (cutoff) for hallucination detection across models, or is threshold calibration required for each dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cv1e2VHqQa", "forum": "mQgAAbaEqQ", "replyto": "mQgAAbaEqQ", "signatures": ["ICLR.cc/2026/Conference/Submission3469/Reviewer_pMYv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3469/Reviewer_pMYv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882283834, "cdate": 1761882283834, "tmdate": 1762916739212, "mdate": 1762916739212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}