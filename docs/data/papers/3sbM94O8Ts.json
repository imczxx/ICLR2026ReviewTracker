{"id": "3sbM94O8Ts", "number": 11573, "cdate": 1758201936339, "mdate": 1759897566987, "content": {"title": "PartInfer: Enabling LLM Inference On Edge Devices", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a range of Natural Language Processing (NLP) tasks, but their high computational and memory demands pose significant challenges for deployment on resource-constrained edge devices. Existing approaches to model compression and optimization often rely on coarse-grained pruning or quantization, which can compromise accuracy or require re-training and fine-tuning. In this work, we introduce PartInfer, a neuron-level optimization framework that enables efficient LLM inference on edge devices by exploiting the task-specific activation patterns of neurons. By profiling and identifying both task-specific and general-purpose neurons using an offline LLM profiler, PartInfer implements two key optimizations: Partial Loading, which reduces memory footprint by loading only a subset of neurons that were identified to be most important during the offline stage, and Partial Computation, which dynamically computes only the most relevant neurons at runtime. Evaluation across multiple NLP tasks shows that PartInfer achieves significant reductions in memory footprint and computation while preserving task performance, making it a practical step towards enabling LLM deployment on edge devices.", "tldr": "By independently reducing memory usage and compute demands, PartInfer enables efficient LLM deployment on resource-constrained edge devices", "keywords": ["LLMs", "LLM inference", "deployment of LLMs", "LLMs on edge devices", "sparsity", "edge devices", "neuron-level optimization", "memory footprint reduction"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8be353859b988eededbba3643e0817a875c83cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PartInfer, a method for accelerating LLM inference through adaptive sparse activation. PartInfer is composed of two components, *partial loading* and *partial computation*. *Partial loading* loads only a subset of neurons to reduce memory burdens. Then *partial computation* selects important neurons within the loaded subset for computation, yielding a more efficient computation. PartInfer predicts the critical neurons with both offline statistics and online top-k selection during prefilling, which accelerates the LLM generation while maintaining the performance. Experimentsal results across various tasks validate the effectiveness of the strategy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Intriguing observations regarding sentence semantics: The observations regarding the common neurons and task-specific neurons are insightful.\n2. Improved neuron selection: The 2-step approach, *i.e.* partial loading and partial computation, reduce the memory burden and inference cost simultaneously.\n3. Boosted efficiency: PartInfer can achieve non-trivial inference speedups on real devices while maintaining accuracy on various tasks."}, "weaknesses": {"value": "1. **Generalization concern**: The major concern is the generalization ability of the proposed method. For long prefilling contexts or multi-turn dialogue, it is difficult to ensure the dynamic neurons are relevant. The computation cost for sorting neurons is also nonnegligible for long prefilling contexts. From my point of view, this method might not be suitable for general purpose, but applicable for specific tasks like stream LLM, which requires short context only.\n2. **Insufficient experiment**: \n   - Limited benchmarks against state-of-the-art static and dynamic pruning methods, such as PowerInfer. \n   - Tasks like WinoGrad, ARC, MMLU, and language modeling perplexity are important benchmarks for the research in inference speedup.\n   - Furthermore, given the claim that this paper focus on Llama family, it is expected to conduct experiments on more model sizes and discuss the generalization ability on other model types, such as Gemma, Qwen, etc.\n   - Lack of comparison with other methods .on the decoding speed.\n3. **Constraint on MLP**: This method is constrained on MLP block. However, as the development of dynamic activation such as MoE, the process of predicting critical neurons might be native to the latest architectures. A detailed discussion on the necessity of PartInfer is critical for assessing this paper."}, "questions": {"value": "1. **Inappropriate metric**: The paper presents the normalized metric in its experiments. How is the metric computed? Why not setting the dense model performance as 1.0? The experimental results are much confusing and reduce confidence in its advantages.\n2. **Hyperparameters**: How is the ratio set in Table 1? Why the overlap is counted on 40% critical neurons in Figure 1&2? There are too many magic numbers in this paper which reduce the readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBuv5L8Eg8", "forum": "3sbM94O8Ts", "replyto": "3sbM94O8Ts", "signatures": ["ICLR.cc/2026/Conference/Submission11573/Reviewer_8jFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11573/Reviewer_8jFi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760620700413, "cdate": 1760620700413, "tmdate": 1762922659060, "mdate": 1762922659060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PartInfer, a framework designed to optimize LLM inference on edge devices by leveraging offline analysis of neuron activation patterns. The method claims to distinguish between model-specific (cross-task general) and task-specific (task-dependent) neurons. During online inference, it employs Partial Loading to reduce memory usage and Partial Computation to lower compute cost. The authors report a 13× speedup when deploying Llama 3.2–3B on an NVIDIA Jetson device.\nHowever, the paper has several critical weaknesses. First, it lacks essential ablation studies: while it demonstrates that computing fewer neurons increases inference speed, it completely omits any accuracy–efficiency trade-off analysis. As a result, the chosen configuration (e.g., 40% computation) appears arbitrary and unsupported. Second, the experimental comparisons are limited and unfair—the claimed 13× improvement is measured only against disk offloading, which is an extremely weak baseline. The absence of comparisons with standard SOTA methods such as 4-bit quantization (GPTQ/AWQ) or pruning makes it difficult to assess the true effectiveness of PartInfer. Finally, all empirical findings are based solely on two models from the Llama 3.2 family, raising concerns about generality across architectures and tasks.\nOverall, while the idea of leveraging neuron-level analysis for adaptive inference is interesting and relevant to the ICLR community, the paper’s experimental validation and comparative rigor fall short of publication standards."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "a) Significance of the Problem: This paper tackles the highly challenging yet practically significant problem of deploying large language models (LLMs) on edge devices with limited memory and computational capacity. This topic is perfectly aligned with the core interests of the ICLR community.\nb) Decoupled Optimization: The proposed PartInfer framework decouples memory optimization (via partial loading) from computation optimization (via partial computation). Theoretically, this design allows flexible trade-offs according to device constraints and application requirements.\nc) Empirical Deployment Validation: The authors report successfully deploying a Llama3.2-3B model—previously unable to run due to insufficient memory—on an NVIDIA Jetson Orin Nano (8GB) device, achieving a 13× speedup compared to disk offloading."}, "weaknesses": {"value": "Implementation Ambiguity: In the Transformer architecture, the Feed-Forward Network (FFN) layer contains multiple weight matrices (e.g., gate_proj, up_proj, down_proj in Llama). However, the paper does not clarify what “loading only a subset of neurons” means in engineering terms. It remains unclear whether this requires modifying the computation graph, dynamically slicing the weight tensors, or implementing custom CUDA kernels to support unstructured sparse computation.\nMissing Key Ablation Studies: The core claim of the paper is that it can reduce memory and computation while maintaining task performance. However, no key ablation studies are provided to substantiate this claim. Figures 9 and 10 show that loading or computing fewer neurons leads to faster inference, but there is no analysis of how accuracy changes with respect to the percentage of computed neurons or the percentage of loaded neurons.\nInsufficient and Unfair Baseline Comparisons — Missing SOTA Methods: The paper cites quantization and pruning as primary competing approaches in the introduction, yet Sections 6.4–6.6 do not compare PartInfer against any standard quantization (e.g., 4-bit GPTQ or AWQ) or pruning baselines. Furthermore, the paper uses disk offloading as the only speed comparison baseline, which is an extremely weak (even strawman) choice. While PartInfer (70% loading, 40% computation) achieves 9.85 tokens/s, it is not compared against other sparse inference approaches (e.g., CoreInfer or PowerInfer) or even a runnable dense baseline (e.g., a 1B model). As a result, the reported 13× speedup lacks meaningful context.\nExtremely Limited Model and Architecture Coverage: As noted by the authors themselves, all conclusions in this paper are drawn solely from experiments on two models in the Llama 3.2 family (1B and 3B). It remains entirely unknown whether the proposed notion of model-specific and task-specific neurons generalizes to other architectures such as Mistral, Gemma, or Mixture-of-Experts (MoE) models. This narrow experimental scope substantially weakens the generality of the conclusions. Although the authors mention this as future work, such a limitation is quite significant for an ICLR submission."}, "questions": {"value": "For specific details, see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cOjhIyLMeu", "forum": "3sbM94O8Ts", "replyto": "3sbM94O8Ts", "signatures": ["ICLR.cc/2026/Conference/Submission11573/Reviewer_D6c8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11573/Reviewer_D6c8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708470936, "cdate": 1761708470936, "tmdate": 1762922658557, "mdate": 1762922658557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a critical challenge in LLM deployment: enabling efficient inference on resource-constrained edge devices (e.g., NVIDIA Jetson Orin Nano) while preserving task accuracy. The core insight is that LLMs exhibit structured neuron activation patterns—distinguishing between model-specific neurons (universally active across tasks) and task-specific neurons (selectively active for individual tasks). Building on this, PartInfer introduces an offline LLM profiler to identify these neurons, paired with two online optimizations: (1) partial loading (only loading critical neurons to reduce memory footprint) and (2) partial computation (dynamically computing task/input-relevant neurons to cut overhead). Evaluations on Llama 3.2-1B/3B models show promising results: 13× speedup over disk offloading, 1.26GB memory savings for Llama 3.2-3B, and competitive accuracy across QA, translation, and summarization tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Edge deployment of LLMs is critical for privacy-sensitive (e.g., healthcare on-device diagnostics) and low-latency (e.g., industrial IoT) applications. Unlike prior work that compromises accuracy or requires expensive retraining, PartInfer’s neuron-level optimization avoids these tradeoffs—filling a key gap in existing edge LLM tooling.\n\n2. The authors systematically quantify cross-task overlap and intra-task consistency using diverse datasets."}, "weaknesses": {"value": "1. The authors only evaluate Llama 3.2-1B/3B and mainly on  translation tasks. The paper omits recent popular downstream tasks for LLM evaluation such as commensence benchmark or code generation or few-shot learning, etc.\n\n2. The offline profiler is central to PartInfer, but the paper provides no details on its computational cost, data requirements, or scalability. \n\n3. The paper compares PartInfer to CoreInfer and disk offloading but omits critical baselines that practitioners currently use for edge LLMs such as quantization or popular edge frameworks like llama.cpp or TensorRT-LLM include optimizations (e.g., KV caching, kernel fusion)."}, "questions": {"value": "1. How long does it take to profile a task (e.g., QA with SQuAD) for Llama 3.2-3B? On what hardware (edge device vs. cloud) is profiling intended to run?\n\n2. If the profiling dataset includes only formal text, would informal text (e.g., social media) change the active neuron set enough to break partial computation?\n\n3.  How were the parameter values (δ=30%, γ=70%, φ=40%) determined, and how does accuracy/speed change if γ is reduced to 50%\n\n\n4. Can you add comparisons between PartInfer and 4-bit quantized Llama 3.2-3B (e.g., AWQ) or inference via llama.cpp on Jetson Orin Nano?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3DowtpGHP5", "forum": "3sbM94O8Ts", "replyto": "3sbM94O8Ts", "signatures": ["ICLR.cc/2026/Conference/Submission11573/Reviewer_i3Bv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11573/Reviewer_i3Bv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935267918, "cdate": 1761935267918, "tmdate": 1762922658090, "mdate": 1762922658090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PartInfer, a system that enables efficient and accurate inference of large language models (LLMs) on low-resource edge devices without retraining, by selectively computing and loading only important neurons. PartInfer accelerates LLM inference by partially loading only critical neurons into memory and computing a subset dynamically based on input activations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a important problem in the field: deploying LLMs on memory- and compute-constrained edge devices, which is crucial for applications requiring offline inference, privacy, and low latency."}, "weaknesses": {"value": "- Terminology inconsistency: There appears to be an inconsistency in the definition of base and secondary neurons. In the Introduction, base neurons are described as “general-purpose” and secondary neurons as “task-specific.” However, in Section 4 (line 235), the roles are reversed—base neurons are defined as task-specific. The authors should clarify and maintain consistent terminology throughout the paper.\n\n- Incomplete memory footprint analysis: In Section 6.5, the reported memory footprint reduction is based solely on the size of FFN parameters excluded via partial loading. However, this estimate does not account for actual memory usage during inference, which also includes KV cache, activations, and framework overhead. Moreover, the paper does not report total memory consumption before and after optimization, making it difficult to assess the practical impact of the reported 1.26 GB reduction. I strongly recommend including empirical measurements of end-to-end memory usage and reporting the percentage reduction.\n\n- Lack of absolute performance metrics: The paper reports only normalized performance values (e.g., accuracy relative to the full model), without providing absolute scores. This limits the ability to evaluate the effectiveness of the compressed models across tasks or datasets. Including raw metric values would significantly improve transparency, allow for cross-task comparison, and help readers assess real-world usability.\n\n- Limited trade-off analysis in Figure 9: Figure 9 presents decoding throughput across different neuron loading percentages, but it omits corresponding accuracy or task performance metrics. Without these, it's hard to evaluate the trade-off between speed and accuracy. I  suggest including performance curves and comparisons with baselines (e.g., CoreInfer, full model) to provide a more complete picture."}, "questions": {"value": "- Clarification on Table 3 results: In Table 3, the combination of CoreInfer and Partial Loading yields inconsistent results—performing worse than CoreInfer alone in some cases, but better on XSum with LLaMA-3B. What accounts for this behavior? Do these results suggest that Partial Loading does not consistently guarantee performance gains?\n\n- Concerns about task generalization: The use of task-specific base and secondary neurons, derived via offline profiling, raises concerns about generalization to unseen or mismatched tasks during online inference. The paper does not evaluate performance when test-time inputs differ significantly from the profiling workload. This limits our understanding of PartInfer’s robustness in real-world multi-task or zero-shot settings. Have the authors considered evaluating on out-of-domain tasks?\n\nI am open to discussing this further during the rebuttal and will be happy to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FNhPikLavK", "forum": "3sbM94O8Ts", "replyto": "3sbM94O8Ts", "signatures": ["ICLR.cc/2026/Conference/Submission11573/Reviewer_7KEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11573/Reviewer_7KEv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951557645, "cdate": 1761951557645, "tmdate": 1762922657604, "mdate": 1762922657604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}