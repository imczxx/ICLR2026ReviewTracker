{"id": "TQsaZQstpl", "number": 15481, "cdate": 1758251775621, "mdate": 1759897304193, "content": {"title": "Cross-Domain Pre-training of Transformers on Text-Attributed Graphs via Random Walks", "abstract": "Pre-training large-scale models with diverse data using the Transformer architecture has driven significant advances in natural language understanding. Motivated by this success, we explore pre-training strategies for graph representation learning that leverage the flexibility of Transformers. A key challenge is enabling a sequence-based Transformer to effectively encode graphs of varying sizes and from diverse domains. To address this challenge, we represent nodes as collections of random walks, allowing the Transformer to learn node embeddings from sequential contexts. We provide theoretical analysis on the expressive capacity of this representation for distinguishing graph structures. We also introduce a novel context prediction loss tailored to random walks. Empirically, we show that the proposed pre-training strategy can be adapted to various downstream graph tasks, highlighting its promise for processing and reasoning with graph-structured data.", "tldr": "We propose strategies for self-supervised pre-training of Transformers via random walks, enabling effective adaptation across diverse domains and downstream tasks at the node, link, and graph levels of text-attributed graphs.", "keywords": ["Graph Learning", "Transformers", "Random walks", "Pre-Training"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1e603647ffbbba894203265d2fb4e6ac29eb8e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified Transformer-based framework for cross-domain graph pre-training, focusing on text-attributed graphs. The key idea is to represent graph structures as sequential inputs via random walks, enabling the use of Transformer architectures similar to those in NLP.The paper outlines four desiderata for pre-training across diverse graphs and demonstrates strong transferability in out-of-domain evaluation."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality：Introduces a new paradigm for cross-domain graph pre-training, using random walks and LLM-based embeddings to unify diverse graph structures；3.1, the dataset-level virtual tokens to distinguish graphs is creative.\n\nQuality : Comprehensive experimental validation across multiple domains demonstrates strong and consistent performance;The results show good transferability even when pre-training on only a few representative datasets, confirming the framework’s generalization ability.\n\nClarify：.\nSignificance：Demonstrates that large-scale pre-training on a few datasets can generalize across domains, a valuable insight for developing graph foundation models\n\nSignificance：Demonstrates that large-scale pre-training on a few datasets can generalize across domains, a valuable insight for developing graph foundation models"}, "weaknesses": {"value": "While the paper discusses edge incorporation, more analysis on label leakage and task-specific fine-tuning would strengthen the claims.\nThe ablation on walk length ℓ and number of walks k could include a more detailed complexity discussion."}, "questions": {"value": "How does the model perform when the node text information is noisy or incomplete?\n\nHow would the framework adapt to temporal or dynamic graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "X7PTpATNbB", "forum": "TQsaZQstpl", "replyto": "TQsaZQstpl", "signatures": ["ICLR.cc/2026/Conference/Submission15481/Reviewer_nDuC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15481/Reviewer_nDuC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835003212, "cdate": 1761835003212, "tmdate": 1762925772080, "mdate": 1762925772080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a Transformer-based model for text-attributed graph representation learning, utilizing random walks to represent graph nodes as sequences. Through a custom context prediction loss for self-supervised pre-training, the approach can handle the graph types from small molecules to large citation networks, and can be easily adapted to various downstream tasks via fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a single yet effective approach by transforming nodes into random walk sequences that can be processed by a Transformer, making it adaptable to diverse graph types.\n2. The model leverages self-supervised pre-training to adapt to diverse datasets for cross-domain learning, offering significant transferability across tasks.\n3. The paper provides a theoretical analysis of random walks’ expressiveness."}, "weaknesses": {"value": "1. Lacking comparisons with recent studies, such as LLM-based methods like LLaGA  (Chen et al., 2024b), GraphGPT  (Tang et al., 2024), and unified GNN models like UniGraph  (He & Hooi, 2024), which are mentioned in Appendix A.\n2. The figures are kind of hard to interpret without accompanying figure captions or explanations."}, "questions": {"value": "1. In the few-shot learning results shown in Table 6, the proposed method achieves strong performance across nearly all settings, but it underperforms on the ARXIV 5-way task under both 1-shot and 5-shot conditions. Could the authors provide insights into the possible reasons for this?\n2. Molecular graphs and knowledge graphs differ significantly in scales. Could the authors discuss some theoretical or empirical guidelines for selecting the walk length $l$ and the number of walks $k$ for these different graph types?\n3. Why can representing molecular graph structures through random walks achieve performance comparable to graph transformer models such as GPS, which incorporate structural encodings? Could the authors provide more analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mwrSt8Esfv", "forum": "TQsaZQstpl", "replyto": "TQsaZQstpl", "signatures": ["ICLR.cc/2026/Conference/Submission15481/Reviewer_gSVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15481/Reviewer_gSVb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870165594, "cdate": 1761870165594, "tmdate": 1762925771491, "mdate": 1762925771491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RWPT, a cross-domain graph pre-training approach that encodes each node using multiple random walks and processes them with a Transformer, trained via a contrastive context prediction loss. The model transfers well across datasets and tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Effective cross-domain transfer without retraining the backbone.\n2. Random-walk representation captures long-range structure.\n3. Strong results across node, link, and graph classification."}, "weaknesses": {"value": "1. The topic has been widely explored before, lack novelty.\n2. Computationally heavy due to long sequences and Transformers.\n3. Relies on text-attributed nodes; less applicable to non-text graphs."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cHSUpB5gqV", "forum": "TQsaZQstpl", "replyto": "TQsaZQstpl", "signatures": ["ICLR.cc/2026/Conference/Submission15481/Reviewer_MqTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15481/Reviewer_MqTi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966431549, "cdate": 1761966431549, "tmdate": 1762925770028, "mdate": 1762925770028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores pre-training graph representations using Transformer architectures. It represents nodes through collections of random walks, enabling Transformers to model diverse graph structures. The study provides theoretical justification for this representation and introduces a random-walk-based context prediction loss, demonstrating strong transferability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The idea of sequentializing graphs with random walks is reasonable."}, "weaknesses": {"value": "1. The proposed method requires positional encoding based on shortest-path distances, which explicitly injects the graph’s inductive bias into the model. As a result, the reconstruction of local neighborhoods is unsurprising. In practice, computing shortest-path distances for large graphs can be time-consuming, limiting the method’s scalability.\n\n2. The proposed method claims to capture long-range dependencies; however, in the main experiments, the walk length was set to 4, which is insufficient to support such a claim. With 8×4 short walks, the sampled contexts likely resemble local k-hop neighborhoods, suggesting that the properties of random walks are not fully exploited.\n\n3. The method involves the use of advanced LLMs (e.g., LLaMA), which produce high-quality node features initially. For a fair comparison, the authors should clearly specify the feature types used in the baselines, as the observed improvements may stem from the strong textual embeddings rather than the proposed framework itself.\n\n4. Many experimental details are missing, such as dataset splits. Table 1 shows that pretrained graph models perform consistently better than individually trained baselines, which contradicts prior findings that graph pretraining often fails to yield significant gains.\n\n5. The transfer learning results in Table 2 are surprisingly strong, e.g., pretraining on a single out-of-distribution dataset leads to substantial improvements over training from scratch. This is counter-intuitive and arguably too good to be true, yet the authors provide no explanation for it.\n\n6. Table 2 and Fig. 4 show that RWPT achieves significantly larger gains on link- and graph-level tasks than on node-level tasks. The authors should clarify the factors contributing to this discrepancy.\n\n7. Following 5, a Transformer pretrained on limited (and potentially OOD) data is unlikely to learn complex random walk patterns. Given the strong results, the model may be relying on simple heuristics, such as mean aggregation. To verify that the model has learned meaningful interaction patterns, the authors are encouraged to analyze intermediate representations (e.g., attention patterns). Additionally, a simple baseline using non-parametric mean aggregation over LLM features, followed by an MLP task head (optionally with shortest-path encodings), should be included to provide a fair comparison and establish a starting point for evaluating more complex methods such as RWPT."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yboNpnICgJ", "forum": "TQsaZQstpl", "replyto": "TQsaZQstpl", "signatures": ["ICLR.cc/2026/Conference/Submission15481/Reviewer_199r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15481/Reviewer_199r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062515319, "cdate": 1762062515319, "tmdate": 1762925769119, "mdate": 1762925769119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}