{"id": "fZ9TNugCOt", "number": 21654, "cdate": 1758320172206, "mdate": 1759896910413, "content": {"title": "A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment", "abstract": "Large language models are increasingly integrated into biomedical research workflows, from literature triage and hypothesis generation to experimental design. A Biosecurity Agent is operationalized as a defense-in-depth framework spanning the model lifecycle with four coordinated modes: dataset sanitization (Mode 1), preference alignment via DPO+LoRA (Mode 2), runtime guardrails (Mode 3), and automated red teaming (Mode 4). On CORD-19, tiered filtering yields a monotonic removal curve of 0.46% (L1), 20.9% (L2), and 70.4% (L3), illustrating the safety-utility trade-off. Real alignment on Llama-3-8B reduces end-to-end attack success from 59.7% to 3.0% (meeting the ≤5% target); larger models assessed under simulated alignment maintain single-digit residual rates. At inference, the guard calibrated on a balanced 60-prompt set attains F1 = 0.694 at L2 (precision 0.895, recall 0.567, false-positive rate 0.067). Under continuous automated red teaming, the aligned 8B model records no successful jailbreaks under the tested protocol; for larger models, replay under the L2 guard preserves single-digit JSR with low FPR. Taken together, the agent provides an auditable, lifecycle-aligned approach that scales from 8B to ~70B parameters, substantially reducing attack success while preserving benign utility for biology-facing LLM assistance.", "tldr": "Defense-in-depth Biosecurity Agent for text-only LLMs: dataset sanitization, DPO+LoRA alignment, multi-signal guardrails with an FPR budget, and automated red teaming; standardized protocol across open-weight models (8B–72B).", "keywords": ["biosecurity", "large language models", "safety alignment", "guardrails", "red teaming", "DPO", "LoRA", "mixture-of-experts", "dataset sanitization", "dual-use risk"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f23a441fd86132212e3df59296662090c8ddbf4.pdf", "supplementary_material": "/attachment/9c53af749e8370693a3a5dc54b0259858dfa4058.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a Biosecurity Agent, as a comprehensive defense-in-depth framework for securing large language models in biomedical research. It integrates four coordinated modes: dataset sanitization, preference alignment, runtime guardrails, and automated red teaming, across the model lifecycle. Experimental evaluation on CORD-19 and Llama-3 models shows that it reduces attack success from 59.7% to 3.0% while maintaining low false-positive rates and preserving model utility. The framework claims to enable scalable, auditable biosecurity alignment from 8B to 70B parameters, effectively balancing safety and research performance."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Introduces a lifecycle biosecurity framework with four integrated modes: sanitized data, alignment tuning, runtime guardrails, and automated red teaming, for LLMs in biomedical contexts.\n2. Demonstrates some experiments on defense, reducing attack success while maintaining low false-positive and high utility performance."}, "weaknesses": {"value": "1. First of all, from the reading, the motivation of the work is very unclear and not well constructed. The same goes for the four modes presented: why these 04 and how they cover the entire lifecycle (connecting to both biosecurity and LLM modeling approaches), and the applications, how and where exploitation of these modes can be harmful in terms of biosecurity. Basically, the significance of the problem is not clear at all.\n2. The introduction section sounds very messy. A lot of things are done, which is appreciated, but a concise mention of contributions is expected. Lines 066-073 and 108-113 can be written in methods too. The introduction section doesn't clearly state what to expect later and where to put focus on.\n3. Equation 1 needs definitions of the terms used. Currently, there is no clear picture of what these terms represent.\n4. Modes and related evaluation information are not introduced properly (mathematically and from literature). Why did you choose to evaluate in the way you described? What is the reason behind the choice? And how do you know that this approach of evaluation is proper to evaluate the mode? There is no justification or reference or anything.\n5. The experimental and evaluation process is very unclear. If this is an AI agent, no information is given on tools and capabilities, and no ablation was done. What tools and capabilities are provided to the agent? Are there any ablation studies done on that? Then, no details on how the agent worked. For example, for mode 1, did the agent write and execute code to filter data, or what? The same for all other modes.\n6. The evaluation is also insufficient. Only one example (one model or dataset) per mode is not enough to judge the capabilities. Also, the tasks are not well-motivated or well-documented either.\n7. Attack success rate, red teaming and such terms were used; but again, there is no information on how, what, where and why it is done.\n8. The presentation is very, very poor. Always jumping between modes and terms, without proper context. Also, as said above, modes are not introduced properly; thus, everything is very unclear.\n9. The usage of the term \"scalability\" is questionable here. Is it said to be scalable because this agent framework can work with larger models? If yes, why and how? If not, explain. The term seems to be entirely irrelevant here."}, "questions": {"value": "1. Clarify motivation and significance.\n   * What specific biosecurity risks or misuse scenarios motivated this work?\n   * Why were these four modes (dataset sanitization, preference alignment, runtime guardrails, and automated red teaming) chosen, and how do they together cover the entire LLM lifecycle?\n   * Can you explicitly connect each mode to a concrete biosecurity threat and explain how its exploitation could cause harm?\n   * Consider adding a clear statement of the research gap and practical impact to strengthen the motivation.\n2. Improve the organization of the paper and clarity of the introduction. Clearly articulate the problem, objectives, and main contributions in a structured way. Provide a short roadmap at the end of the introduction outlining what each section contains.\n3. Define every variable and symbol in Equation 1; ensure that readers can interpret the equation without external assumptions.\n4. Clarify how each mode is formalized. Are there mathematical representations or probabilistic models behind them? Provide citations or reasoning for the chosen evaluation methods of each mode. Why are they appropriate for assessing each mode? Provide some justification.\n5. Add more details on experimental design, agent descriptions, and evaluations\n   * Describe the Biosecurity Agent’s architecture, tools, and operational capabilities in detail. Include ablation studies to evaluate the agent's performance in different setups to understand the limitations and capabilities in more detail.\n   * For each mode, specify how the agent functions: does it write code, filter text autonomously, execute scripts, or depend on predefined heuristics? \n   * Explain the experimental setup: what models, datasets, and baselines were used, and why were they chosen?\n   * Clarify the task designs for attacks, red teaming, and jailbreaking. What threat model or adversarial prompts were used? Were both automated and human red teaming approaches considered?\n   * Justify why these protocols are representative of real-world biosecurity threats.\n5. Add more examples and depth to the evaluation. Also, include statistical evidence (e.g., variance, significance tests) to support observed differences.\n6. Is there any biosecurity work that introduced the modes you tried to evaluate?\n7. How do you see this agent being used in real-world research and application scenarios? \n8. Include an end-to-end example of a task (how the agent is working and using its tools).\n9. Define scalability and why it is claimed to be a contribution here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p48JyeVT0K", "forum": "fZ9TNugCOt", "replyto": "fZ9TNugCOt", "signatures": ["ICLR.cc/2026/Conference/Submission21654/Reviewer_847G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21654/Reviewer_847G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760557773083, "cdate": 1760557773083, "tmdate": 1762941872021, "mdate": 1762941872021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lifecycle Biosecurity Agent that integrates dataset sanitization, preference alignment (DPO+LoRA), runtime guards (multi-signal stacking including BLAST/long-sequence/semantic/fuzzy/keyword checks) and automated red-teaming with rule/policy re-injection. The pipeline is evaluated on biological text scenarios and shows substantial end-to-end reductions in attack success rate (ASR) under the reported settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution is a comprehensive, end-to-end framework that unifies data governance, training-time alignment, runtime guards, and red-team evaluation under a single, auditable metric set (pre-JSR / ASR with confidence intervals). This unified design and measurement make the security posture easier to reason about and reproduce.\n\n2 .The framework is operationally flexible: guard levels and decision thresholds can be tuned to trade off false positive rate vs. recall, and the automated red-teaming loop enables dynamic updates (new rules or preference updates are fed back into guards and alignment), which is practical for deployed systems and continuous hardening."}, "weaknesses": {"value": "The paper presents a broad and well-engineered system and demonstrates that “the framework can work,” but it lacks several crucial validations around data, generalization, and comparisons to alternative approaches:\n\n1. The reported evaluation uses a small challenge set (e.g., 60 prompts, 30 harmful / 30 safe). The manuscript does not sufficiently describe the benchmark construction, prompt types, or coverage, so it is unclear which attack scenarios and model behaviors are actually measured.\n\n2. Experiments are restricted to biological text scenarios. It is not demonstrated whether the same pipeline (signals, thresholds, BLAST usage) generalizes to other high-risk domains (chemistry, materials, radiological, multi-modal or sequence-generation models).\n\n3. Mode 2 (DPO+LoRA) training provenance is under-specified: what positive/negative responses were used, how were y^+/y^- sampled, and how was leakage between guard rules and training data prevented? This raises concern about possible overfitting to the in-house red team / guard rules.\n\n4. The paper does not sufficiently justify why DPO is the best fit; other approaches (constitutional approaches, rejection-distillation, RLHF variants, robust decoding) could be competitive—comparative experiments are missing.\n\n5. The automated red team and its default components (e.g., BLAST toggles) are tuned to the in-house setting. There is little evaluation on external public benchmarks (HarmBench, Jailbreak sets) or against different attack generators to show the loop does not overfit to its own attackers."}, "questions": {"value": "Please provide more details for DPO training: sources and sizes of the (x, y^+, y^-) triples, sampling/filtering rules, inter-annotator agreement (if any), and whether training examples overlap or are derived from your red-team outputs.\n\nCan you run additional benchmarks, e.g., HarmBench or public jailbreak collections, and/or test against attack generators different from your automated red team, to show transfer robustness?\n\nCould you include baselines using alternative alignment strategies (Constitutional AI / rejection-distillation / robust decoding / RLHF variants) and report end-to-end ASR / pre-JSR so readers can judge whether DPO+LoRA is preferable?\n\nplease measure how much runtime cost (extra processing time per query and achievable queries per second) the runtime guards add, and plot how ASR decreases vs. how the false positive rate (blocking benign queries) increases as you change guard thresholds.\n\nPlease provide an analysis of BLAST / long-sequence screening operational costs and privacy implications: maintaining/accessing pathogen sequence databases at scale may have compute, latency and governance costs—can you measure these and discuss deployment constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hyyd7OzPt0", "forum": "fZ9TNugCOt", "replyto": "fZ9TNugCOt", "signatures": ["ICLR.cc/2026/Conference/Submission21654/Reviewer_iJxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21654/Reviewer_iJxk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488936737, "cdate": 1761488936737, "tmdate": 1762941871741, "mdate": 1762941871741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Biosecurity Agent framework integrating four safety modes—dataset sanitization, preference alignment, runtime guardrails, and automated red teaming—to reduce biological misuse risks in large language model. The authors evaluate multiple model families (8B–70B) under simulated or real alignment and report consistent reductions in jailbreak success rates (<5%) while maintaining usability. The work aims to demonstrate lifecycle-level LLM biosecurity alignment."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a topic of substantial importance, reducing biosecurity risks in large language models. It proposes a clearly structured workflow that integrates data sanitization, alignment fine-tuning, and adversarial red-teaming into a unified operational framework. The experiments cover multiple model scales (from 8B to 70B) and provide quantitative evidence of reduced misuse rates, indicating that the proposed approach can be practically applied in large-scale systems."}, "weaknesses": {"value": "1. Missing Baselines and SOTA Comparisons\nThis paper does not evaluate its method against existing alignment or defense approaches. The only benchmark metric is the jailbreak success rate of the unaligned model. Although several existing methods were mentioned, no comparisons are made. Simpler baselines such as keyword filters are also omitted. Therefore, it is currently unclear whether the proposed four-stage agent has any substantial advantage over existing or simpler methods. The paper demonstrates performance gains, but without comparisons, making it difficult to determine whether similar or even better improvements could be achieved using simpler defense strategies.\n2. Limited Evaluation Scope\nThe evaluation in this paper is very small in scale, using only about 110 adversarial prompts (7 categories), and the coverage is very limited. The tuning of the guard mechanism is also based on only 60 labeled samples. Such a design is prone to distribution bias, making it difficult to prove the model's generalization ability. The prompts are almost all short English sentences on the topic of synthetic biology, with limited content diversity. The authors also noted that the system performed poorly in terms of multilingual and long text prompts, which further indicates that the evaluation scope was too narrow. Furthermore, they did not experiment on a larger set of jailbreak test cases or on harmless tasks in general. Overall, this small-scale assessment not only lacks statistical power but also carries a significant risk of overfitting.\n3. Incomplete Statistical Reporting\nOnly some metrics include confidence intervals. Many results come from a single run, without any measures of variability or variance. Robustness is difficult to assess due to the lack of standard deviation, error bars, or seed-based results. Using only 60 prompts to assess guard performance, without any estimation of uncertainty, undermines the reliability of the statistics. Threshold selection can also be very sensitive to small validation sets. Incorporating broader indicators (such as AUROC) and conducting multiple sub-experiments will greatly improve the credibility of the conclusions.\n4. Insufficient Evidence for Cross-model Generalization\nThe paper claims scalability from 8B to 70B models. However, instead of aligning larger models, it simply replays the prompts generated by the 8B model on the “simulated aligned” versions of larger models. This approach cannot verify true cross-model generalization, nor does it take into account the vulnerability of specific models. The evaluation is one-way and limited to prompts generated by the 8B model.\n5. Unclear Attribution of Improvements\nThe paper attributes the performance improvement to the proposed four-stage agent, but does not specify which part plays the main role. Without ablation studies, it would be impossible to know the extent of the contribution at each stage. These changes in performance could come from several different factors. For example, a decrease in attack success rate (ASR) may simply be due to the addition of more red team data, rather than the application of DPO or LoRA methods. This improvement may also stem from changes in the model's output distribution after alignment, making harmful content easier to detect. Because these factors were not distinguished, the paper's causal claims are not convincing.\n6. Missing Deployment Considerations\nThe paper lacks analysis of runtime cost and usability. Its defenses are implemented during the inference phase, including fuzzy matching and semantic similarity checks, which can slow down the model. The reported false positive rate is non-negligible, but the paper does not discuss its impact on user experience. Furthermore, it fails to measure performance degradation on normal tasks, nor does it include any user study on the trade-off between safety and usability. Finally, it overlooks a real-world problem: maintaining a red-teaming mechanism after deployment would incur high engineering maintenance costs."}, "questions": {"value": "Could you provide results comparing your method with standard alignment baselines? Additionally, have you evaluated simpler baselines such as keyword filters or content moderation classifiers on the same adversarial prompts?\n\nGiven that the adversarial test set consists of only ~110 prompts across seven categories, how do you assess the generalizability of your findings to broader misuse scenarios? Have you considered evaluating on more diverse or larger-scale public jailbreak suites?\nSince the 70B-scale results rely on simulated alignment rather than actual fine-tuning, how confident are you that these results reflect true generalization across model scales? Have you tested whether adversarial prompts generated on larger models could bypass the defense?\n\nCould you provide ablation studies isolating the contribution of each component to the observed performance improvements? Specifically, what is the marginal gain of Mode 4 relative to Mode 2 or Mode 3 alone?\n\nWhat is the estimated latency and compute overhead introduced by your inference-time guard mechanisms under realistic deployment conditions? How would you mitigate potential usability issues arising from the reported false positive rates?\n\nHow do you ensure that the alignment and guard processes do not degrade helpfulness or coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "67OsI37IGR", "forum": "fZ9TNugCOt", "replyto": "fZ9TNugCOt", "signatures": ["ICLR.cc/2026/Conference/Submission21654/Reviewer_KxAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21654/Reviewer_KxAN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879851630, "cdate": 1761879851630, "tmdate": 1762941871429, "mdate": 1762941871429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a four‑mode defense‑in‑depth biosecurity agent that spans (M1) dataset sanitization, (M2) training‑time preference alignment via DPO+LoRA, (M3) a multi‑signal runtime guard, and (M4) automated red‑teaming, with results reported across one truly aligned model (Llama‑3‑8B‑Instruct) and three larger models under simulated alignment (Mixtral‑8×7B, Qwen‑2.5‑72B, Llama‑3.1‑70B). A central claim is reducing end‑to‑end attack success rate from 59.7% to 3.0% on an expanded adversarial set for the 8B model, while keeping benign utility reasonable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The agent unifies upstream data curation, alignment, inference‑time controls, and adversarial evaluation into one auditable pipeline. This end‑to‑end view is valuable for safety‑critical deployment. \n2. DPO+LoRA on the 8B model cuts end-to-end ASR from 59.7% to 3.0%, while the L2 guard attains F1≈0.694 at FPR≈0.067 on a balanced set. Iterative red-teaming shifts protection upstream without hurting safe-completion.\n3. The Ethics and Reproducibility sections explain disabled external sequence queries, templated unsafe fragments, CI reporting."}, "weaknesses": {"value": "1. The “Biosecurity Agent” operates more like a workflow than an autonomous agent. Its four modes run in a fixed, rule-based order without adaptive reasoning or self-directed control. The system orchestrates modules deterministically rather than deciding or planning actions autonomously. To merit the “agent” term, it would need context-aware decision loops or dynamic mode selection; otherwise, renaming it a “Biosecurity Workflow Framework” would more accurately describe its functionality.\n2. The guard is calibrated and evaluated on a small balanced 60-prompt set, raising overfitting and variance concerns - can the authors separate dev/test, report bootstrap CIs/k-folds, and add full HarmBench/JailbreakBench results under the same L1/L2/L3 operating points?\n3. The authors define pre‑JSR and ASR, but later alternate between ASR, JSR, and end‑to‑end success in ways that are easy to confuse."}, "questions": {"value": "1. The dataset sanitization stage reports large removal ratios at higher safety levels. How much benign scientific knowledge is lost as a result, and could this reduction impact the model’s ability to perform legitimate biomedical or chemical reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KSjxgY8ZYs", "forum": "fZ9TNugCOt", "replyto": "fZ9TNugCOt", "signatures": ["ICLR.cc/2026/Conference/Submission21654/Reviewer_SyN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21654/Reviewer_SyN5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930885132, "cdate": 1761930885132, "tmdate": 1762941871076, "mdate": 1762941871076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}