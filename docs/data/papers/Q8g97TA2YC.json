{"id": "Q8g97TA2YC", "number": 24799, "cdate": 1758360459715, "mdate": 1759896747685, "content": {"title": "Beyond Exploration–Exploitation: An Identification-Aware Bayesian Optimization Method under Noisy Evaluations", "abstract": "In this study, we investigate black-box optimization problems with heteroscedastic noise, a setting commonly encountered in hyperparameter tuning for machine learning models. Bayesian optimization (BO) is a popular framework for such problems, with prior work primarily focusing on designing acquisition functions or surrogate models to balance exploration and exploitation. However, a critical yet underexplored issue is the identification problem: BO algorithms often locate promising solutions but fail to reliably identify and return them to users. We take the first step toward addressing this challenge. We formally define the identification error within a standard BO framework and derive a myopic acquisition function that directly minimizes this error. A surprising theoretical result shows that the acquisition function for minimizing identification error is equivalent to the difference between two widely used criteria: the knowledge gradient (KG) and expected improvement (EI). Building on this insight, we propose a novel acquisition function, Identification-Error Aware Acquisition (IDEA), and establish its asymptotic no-regret property. The effectiveness of IDEA is demonstrated on benchmark test functions.", "tldr": "Identification-Aware BO", "keywords": ["black-box optimization", "Bayesian optimization", "acquisition function", "identification problem"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95895eb12ce5d51e394dc1b1b2d9eb461feed2bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies a problem of identification-aware Bayesian Optimisation. It is essentially the best-arm identification problem from the multi-armed bandits community, but framed in the context of Bayesian Optimisation (Gaussian Process bandits). That is, they focus on studying the problem of making sure the algorithm at the end of its runtime, is able to identify the point as close as possible to optimal, rather than simply focusing on the best regret. Authors show that minimising the one-step Bayes risk of misidentifying the optimal point is equivalent to minimising the difference between Expected Improvement and Knowledge gradient. Based on this observation, authors propose an algorithm and evaluate it on number of synthetic benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The result that minimising one-step Bayes risk is equivalent to minimising difference of EI and KG, is interesting and insightful."}, "weaknesses": {"value": "**Significance unclear**\n\nThe authors essentially focus on a different problem setting than the one studied in classical BO and even later on page 9 they admit their algorithm underperforms to existing baseline in terms of the standard metric of best regret because, quoting the authors:\n> This is because focusing solely on minimizing identification error encourages repeated evaluations\nof existing candidates or the selection of inferior points, rather than exploration of potentially superior ones.\n\nSo clearly, the objective in the adopted problem setting does not need to coincide with the best regret objective of classical BO. As such, I would like authors to demonstrate on practical problems, why should we care about best point identification rather than best regret. In the applications of BO, I am aware of, best regret is the metric of interest, e.g. in hyperparameter tuning, we simply wish to find a configuration of hyperparameters that performs best, and we do not care if the algorithm queries suboptimal points after finding the optimum, as long as the best solution found has a good objective value. \n\nWe could speculate that maybe in some practical problem settings, your objective would could more applicable, but this need to be demonstrated by the authors. As such, for now I am not convinced that the proposed problem setting is relevant. \n\n**Error in convergence proof**\n\nUnfortunately, I believe Theorem 1 is wrong. The error is in transitioning from the penultimate to the last line in the proof in A.1.4. The bound on $\\sum_{n=1}^N \\sigma_n(x^\\star)$ reads $N^{1/2} \\sigma_0(x^\\star) \\sqrt{N}$ — in other words, $N \\sigma_0(x^\\star)$ (just written in a very confusing way for some reason). But then when combining the bounds in the final line, you lose the $N^{1/2}$ factor, that is the second term should read $(3\\beta_N + c_0)\\sigma_0(x^\\star)N$ (instead of $\\sqrt{N}$), **making the bound linear and thus trivial**. An algorithm with linear regret does no better than just a naive strategy of constantly selecting the same point, as such, linear regret means the algorithm **does not converge**. This proof must be corrected, or the claims about convergence must be removed.\n\n**Experiments unconvincing**\n\nIt is not clear if the shaded areas are standard errors, standard deviations or CIs based on those. In almost all experiments, the shaded areas overlap meaning that the differences between the algorithms are not statistically significant. The authors initialise the algorithm by sampling 20 points and querying each 200 times to estimate noise, that is a total of 4000 queries to the blackbox, that typically already exceeds the budgets we have in BO, raising concerns as to how realistic this problem setting really is. The plots in the experiments section are too small to be realistically readable. \n\n**Issues with notation**\n\nThe notation is completely inconsistent. First authors define the black-box function as $f(x, \\xi)$ in Equation (1), then they switch to calling the function value $y(x)$ throughout most of theoretical derivations. Then in equation (12) they defined noisy function as $f(x) = sin(x) + \\epsilon(x)$, where $\\epsilon(x) \\sim \\mathcal{N}(0, x^2)$, so revert to calling it $f$, but for some reason omit the explicit dependance on randomness $\\xi$, even though function clearly depends on stochastic noise. Authors keep switching between capital $N$ and $n$ when referring to number of samples. Sometimes authors call the kernel function and prior mean as $k$ and $\\mu$, and sometimes $k_0$ and $\\mu_0$. \n\n\n**Minor**\n\nLine 294, there are two equations for $\\mu_n^\\star$, I believe the second one should be $\\mu_{n+1}^\\star$."}, "questions": {"value": "- What do the shaded areas in the plots represent? Are they standard errors/deviations or CIs based on them or some other quantity?\n- The reliance of the proposed algorithm on an additional hyperparameter $\\alpha_n$ or $\\lambda$ and $\\beta$ seems like a major limitation. How were they selected in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VLVwaOW7Bw", "forum": "Q8g97TA2YC", "replyto": "Q8g97TA2YC", "signatures": ["ICLR.cc/2026/Conference/Submission24799/Reviewer_jwEd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24799/Reviewer_jwEd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761043367220, "cdate": 1761043367220, "tmdate": 1762943201193, "mdate": 1762943201193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of Bayesian Optimization, where the objective is to optimize a black-box function using a fixed budget of queries. Within this framework, the paper focuses on the \"identification problem\" and aims to solve it by minimizing the identification error. The identification error is defined to be sub-optimality gap between the final output of the algorithm and best point in the query set. The authors show that myopic/one-step minimization of the identification error is equivalent to minimizing a difference between expected improvement (EI) and knowledge gradient (KG), providing a tractable solution to this problem. The authors also propose an algorithm that incorporates \"Identification-Error Aware Acquisition\" by taking a weighted mean of EI - KG and KG. Theoretical guarantees along with some numerical studies to support the theoretical guarantees are provided by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and easy to follow. The problem is clearly stated and the proposed solution is clearly explained."}, "weaknesses": {"value": "A glaring weakness of this paper, in my opinion, is its motivation. \n\n- Why is identification-awareness that important? Why is it important to return the \"best\" point among the set of queried points?\n- I agree with the authors that BO algorithms likely don't return the \"best\" query point. The authors seem to claim this is some \"prevalant weakness\" among BO algorithms. I don't see how or why this is a weakness as long the returned point behaves similar enough to the \"best\" point?\n- From a theoretical viewpoint, there are numerous algorithms whose performance in terms of simple regret is well-known. Since simple regret is an upper on identification error, these bounds on existing algorithms can immediately be extended to that on identification error. Based on that, we can easily conclude that the bounds on identification error provided by this algorithm are sub-optimal (since optimal algorithms are known). Moreover, the analysis techniques are well-known in the literature. So, clearly there is no obvious theoretical benefit of this approach over existing ones.\n- From a practical point of view, the numerical studies don't suggest any statistically significant advantage of the proposed approach over NEI or KG. So with no theoretical/practical benefit, I am struggling to see the motivation/need for an identification-aware approach."}, "questions": {"value": "In addition to the above questions, I have some additional questions:\n\n- How do arrive at Eqn 20 from Eqn 19? It is stated that there is an approximation. What is the approximation and how reasonable is it?\n- In proof of Theorem 1, in how do you arrive at line 667 from from 662-663? Shouldn't the RHS be $N\\sigma_0(x^{\\star})$ instead of $\\sqrt{N} \\sigma_0(x^{\\star})$? If so, the regret is a constant, unlike what is claimed in the Theorem.\n- Can the authors justify why is reasonable to assume that $\\tau$ is known or can be estimated? It is required to construct the posterior mean $\\mu$. \n- Even if $\\tau$ is known, the equation in lines 657-658 needs to appropriately modified to account for heteroscadastic noise, especially when $\\tau$ is very small. The result as stated only holds for homogeneous noise.\n- What is the choice of $\\alpha_n$ or equivalently $\\beta$, $\\lambda$ for the experiments?\n- From the description in lines 391-393, it seems the authors have used 4000 queries to estimate the mean and variance and they don't count towards the query budget. Am I understanding this correct? If so, this does not seem fair."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P5kx4WNY6L", "forum": "Q8g97TA2YC", "replyto": "Q8g97TA2YC", "signatures": ["ICLR.cc/2026/Conference/Submission24799/Reviewer_SL9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24799/Reviewer_SL9F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521185560, "cdate": 1761521185560, "tmdate": 1762943200872, "mdate": 1762943200872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim to contribute:\n- A formal definition of identification error in BO\n- An acquisition function that balances minimizing identification error with improving the best solution (which includes the explore/exploit tradeoff). Additionally, a way to balance the two, which requires a schedule, with an exponential function with two hyperparameters for the user to choose. Also provides a regret bound.\n- Claim improved results, though this isn’t clear."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Fig. 1 provides a nice illustration of how the algorithm samples promising candidates but does not ultimately return them. The identification error, though not motivated or defined super clearly, seems like an important quantity to address. My understanding is that it represents the effect of noise / distribution shift on optimization solution.\n- It’s interesting that minimizing the expected identification error at the next step leads to the difference of EI and KG acquisition functions. It’s also implementationally and theoretically convenient."}, "weaknesses": {"value": "- Doesn’t clearly articulate in the abstract/introduction why or how one would end up in a situation where the algorithm has identified promising regions of the search space but cannot identify/return them. Overall, the writing could use more work to clearly articulate the authors’ arguments and method.\n- Distinguishes BO from BAI / R&S by saying that in BO, the candidate set expands as the algorithm explores the search space. This doesn’t make sense; in BO, the search space is usually pre-defined, though different areas may be evaluated at different rounds. Definitions in related work are generally confusing and perhaps inaccurate — posterior best is the point “with the lowest posterior mean”. The related work doesn’t do a compelling job of convincing the reader that their method is needed. The connection to resource allocation strategies is referred to but never explicitly explained or made clear. \n- Only looks at two low-dimension synthetic functions as test cases, so hard to tell if this generalizes or is useful in more real-world cases. Further, discretizes each function to have only 100 candidate points, making it further from a realistic case.\n- Fig.2 identification error results are not very compelling — It doesn’t really look like NEI / KG have significantly higher identification errors compared to IDEA / identification loss only acquisitions. This calls into question whether better results, if obtained, would be caused by lower identification error.\n- Further, Fig.3,4 are not very impressive—IDEA doesn’t really seem to do much better than baselines, and their confidence intervals overlap. I’m not convinced that their method provides a consistent or substantive improvement. The claims made in the text are strong and don’t seem to be reflected in the results.\n\nGeneral feedback for improving paper:\nPlease format your citations appropriately, particularly making them parenthetical.\nFigure 1 is missing a caption for panel b.\nPlease print your paper out and then rescale your plots based on that — many of them are way too small to distinguish methods or read the legends. \nAlso, maybe put your plots on a log-scale so it’s easier for the reader to see what’s going on.\n\nRecommend reject — the results don’t seem to constitute a substantive improvement over existing baselines and writing could be improved a lot."}, "questions": {"value": "What is the distinction between the best-arm identification literature and standard multi-arm bandits? Why not just cite these works?\nCan you provide p-values to quantify whether the simple regrets (and quantities in other plots) are significantly different between IDEA and KG, and IDEA and NEI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VaIgz5j0kt", "forum": "Q8g97TA2YC", "replyto": "Q8g97TA2YC", "signatures": ["ICLR.cc/2026/Conference/Submission24799/Reviewer_MmcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24799/Reviewer_MmcK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869544963, "cdate": 1761869544963, "tmdate": 1762943200659, "mdate": 1762943200659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IDEA, an acquisition function for noisy Bayesian Optimization. Due to noisy observations, choosing the solution with the best observed value found can be suboptimal. In particular, IDEA tackles the identification error, which is the difference between the best solution based on the posterior mean and the best solution based on true objective function value (Eq. 16). The authors point out that this identification error can be minimized by minimizing the difference between EI and KG. Hence, the IDEA acquisition function is proposed, which simultaneously (1) maximizes the EI acquisition function and (2) minimize the EI – KG term. A theoretical convergence analysis is provided. Empirically, IDEA is evaluated with EI and KG on two synthetic benchmark problems with different noise settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The mathematical derivation of the final rule (EI – KG) is sound.\n- There is a convergence analysis to support the result."}, "weaknesses": {"value": "1.\tAccording to the problem formulation, the identification error arises only at the final stage - after completing the BO process - when selecting the final solution from the observed dataset. Therefore, it seems incorrect to use the rule as an acquisition function during the BO routine. During BO, identification error is not yet relevant; hence, the rule should only be applied when determining the final solution. This significantly reduces the contribution of the work.\n2.\tIn the acquisition function in Eq. 2, it is not clear why we only care about explicitly maximizing EI, and not maximizing KG, or both. The final acquisition function could have 3 goals simultaneously: (1) max EI, (2) max KG and (3) min (EI – KG).\n3.\tThe empirical performance does not show any significant improvement. This further strengthens the Weakness 1 that EI-KG rule does not contribute to the BO process, hence the IDEA method seems to be equivalent to EI.\n4.\tThere are only two synthetic benchmark problems. There should be more problems, especially real-world ones.\n5.\tMany baselines mentioned in the Related Work section are not included in the experiments, including Quan et al., 2013, Liu et al., 2014 and Dai et al., 2023.\n6.\tThere is no ablation study on the effect of weight settings in Eq. 22.\n7.\tMinor: In line 294, there are two notations of \\mu*_n."}, "questions": {"value": "1.\tDid the authors consider formulating Eq. 22 as a multi-objective acquisition function, e.g., consisting of the EI objective and the EI – KG objective? Formulating as a multi-objective optimization problem would remove the weight setting requirement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x1NQZ5u0um", "forum": "Q8g97TA2YC", "replyto": "Q8g97TA2YC", "signatures": ["ICLR.cc/2026/Conference/Submission24799/Reviewer_RM1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24799/Reviewer_RM1n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960830091, "cdate": 1761960830091, "tmdate": 1762943200444, "mdate": 1762943200444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}