{"id": "wnCJLnRBtb", "number": 24869, "cdate": 1758361374499, "mdate": 1759896744493, "content": {"title": "Context Similarity Structure Shapes the Emergence of Reliable In-Context and In-Weights Mixtures", "abstract": "We aim to train models that co-develop in-context learning (ICL) and in-weights learning (IWL), and flexibly switch between them based on context relevance. Such models should exploit closely related in-context examples while relying on IWL when examples are irrelevant. Although LLMs exhibit both modes, standard task-specific fine-tuning often erodes ICL, motivating IC-Train, a form of fine-tuning with in-context examples. When trained under IC-Train, prior work has shown that emergence of ICL depends on factors such as task diversity and training duration. We show that an overlooked factor is the similarity structure between target inputs and context examples. Of the two existing modes of context-target pairing, random context leads to IWL dominance, while only similar examples in context causes ICL to degenerate to copying labels without regard to relevance. To address this, we propose Contrastive-Context which enforces two types of contrasts: (1) mix of similar and random examples within a context to evolve a correct form of ICL, and (2) varying grades of similarity across contexts to evolve IWL-ICL mixtures. With experiments on real sequence to sequence learning tasks on four models, we show that Contrastive-Context strengthens ICL while preserving IWL, outperforming random and nearest-neighbor sampling in both in-domain and out-of-domain evaluation. Theoretical analysis and diagnostic probes confirm that contrasted contexts yield stable ICL–IWL mixtures, avoiding collapse into pure ICL, IWL, or copying. Our results establish similarity structure as a key driver of reliable ICL under fine-tuning an LLM for a task.", "tldr": "", "keywords": ["In-Context Learning", "Transformers", "Sequence to Sequence Learning", "Continuous Adaptation", "In-Weights Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/383b395d78e83f2d70cd6d4d6b207d4ece4363fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how structural similarities between target inputs and contextual examples influence the emergence of in-context learning (ICL) capabilities in large language models (LLMs). The authors observe that when contextual examples are overly similar, the ICL process tends to degenerate into simple label copying, disregarding the true relevance of examples. To mitigate this issue, they propose a method called Contrastive-Context, which enforces two types of contrastive structures:\n\n(1) mixing similar and random examples within a context to encourage the development of a more robust form of ICL, and\n\n(2) varying the degree of similarity across contexts to facilitate a balanced evolution between in-weights learning (IWL) and ICL.\n\nExperimental results demonstrate that Contrastive-Context enhances ICL performance while maintaining IWL effectiveness, outperforming both random sampling and nearest-neighbor sampling strategies in both in-domain and out-of-domain evaluations. Furthermore, the paper provides theoretical analysis based on a two-layer Transformer model to support its empirical findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The study addresses an important and timely question—how to improve the in-context learning ability of LLMs—which is of substantial relevance to the field.\n2. The authors not only introduce a novel technique, Contrastive-Context, supported by convincing experimental evidence, but also complement it with theoretical analysis and diagnostic probes that help elucidate the underlying mechanisms of their approach."}, "weaknesses": {"value": "1. The primary limitation of this paper lies in its insufficient positioning within the broader research landscape. As a result, both its contributions and innovations remain unclear. Specifically:\n\n   1. The claimed innovation of the paper rests on the assumption that *inter-example similarity is a critical but underexplored factor*. However, this assumption is neither adequately explained nor empirically validated. While numerous prior studies have examined how various factors influence the emergence of in-context learning (ICL), the authors do not convincingly justify why inter-example similarity should be considered more important than other factors. Moreover, the paper lacks comparative experiments that could substantiate the claimed significance of this factor.\n\n   2. Several existing works[1,2] have proposed methods to enhance the emergence of ICL by focusing on example selection. The authors should at least include comparisons with these related approaches to clarify the novelty and relative effectiveness of their method.\n\n        [1] Breaking through the learning plateaus of in-context learning in transformer.\n\n        [2] Task diversity shortens the icl plateau.\n\n\n   3. The paper makes several key claims that are not supported by sufficient evidence. For example:  \"A natural alternative is to fine-tune in ICL mode (IC-Train), but this is challenging due to competition with in-weights learning (IWL)\"[line 43-44]. and \"Such adaptation should boost accuracy on inputs resembling the new examples, while retaining generalization to inputs without close neighbors.\" [line 37-38]\n\n    \n\n2. The experiments are conducted only on the language translations tasks. It is questionable whether the findings also apply to other area, like math or coding"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3n73FxKrJ8", "forum": "wnCJLnRBtb", "replyto": "wnCJLnRBtb", "signatures": ["ICLR.cc/2026/Conference/Submission24869/Reviewer_gPby"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24869/Reviewer_gPby"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461188257, "cdate": 1761461188257, "tmdate": 1762943228793, "mdate": 1762943228793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how to fine-tune LLMs that balance in-context learning and in-weight learning, and allow switching between them based on the relevance of context examples to the target input. It provides analysis on how target-context similarity during training influences IWL and ICL balance and proposes a simple contrastive-context training strategy. They conduct an theoretical analysis with clearly defined assumptions on a regression task using a two-layer two-parameter model, to show how different contrasts in context-target influence the ICL behavior. They show that similarity structure between target and context is a key component for ICL, demonstrated on machine translation tasks over 4 open-source LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed in this work is quite relevant for the community. The proposed strategy is simple and effective. Convincing empirical evidence is provided to prove its effectiveness. The proposed data strategy is tested across four LLMs at different scales over four machine translation tasks. The empirical evaluation scheme over varying grades of similarity is quite insightful. This scheme can be used for future ICL research since it captures both ICL and IWL performance as two ends on the spectrum. \n\n2. It is a novel conceptual findings that random and similar-context pushes the model toward different extremes of ICL and IWL. Additionally, the method is theoretically grounded in a simplified two-layer model analysis, providing mechanistic understanding on how the similarity structure influences the performance. \n\n3. The paper is well-written and text part is easy to follow."}, "weaknesses": {"value": "1. Contrastive-context relies on synthetic paraphrases with epsilon=0.1, which is a form of data augmentation. It is unclear whether the observed improvement arises from this synthetic paraphrasing or just from the mixture of random and similar/augmented sequences. Two controlled ablations would clarify this. First, by either setting p=0 or epsilon=0 to isolate the effect. Second by varying paraphrasing strength from zero to high paraphrasing. \n\n2. The presentation of figures could be improved. Several plots lack axis labels which makes them hard to understand, especially the bottom right plots in Figure 2. Curves in Figure 3 appear overly smoothed which reduces the transparency of the experiment. Please provide raw values as well in future.\n\n3. The evaluation lacks robustness analysis. The method is only tested on machine translation tasks and the OOD setup represents only a mild domain shift within the same language-pair task. In Figure 3, IWL performance is higher for OOD set versus ID, implying overlap with pretrained data and undermining the OOD definition. Error bars of the different performance curves are also not provided."}, "questions": {"value": "1. Why did the authors select only sequence to sequence task for evaluation? Seq2seq is a relatively easy real-world task compared to tasks such as QA or other reasoning tasks. It would strengthen the findings, if the benefits are also demonstrated on other difficult tasks.\n\n2. Why do plots in Figure 3 show noticeable fluctuations over the training steps, especially for Copy-scores?\n\n3. Recently, few works [a][b]  have shown that repetitions or augmentations of the target sample in the context leads to strong ICL and IWL performance when trained with a mixture of random and bursty sequences, similar to this paper. How does proposed contrastive-context method conceptually differ from these studies.\n- [a] The emergence of sparse attention: impact of data distribution and benefits of repetition (Zucchet et al. 2025)\n- [b]  What Matters for In-Context Learning: A Balancing Act of Look-up and In-Weight Learning (Bratulic et al. 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9aSeYlAeBc", "forum": "wnCJLnRBtb", "replyto": "wnCJLnRBtb", "signatures": ["ICLR.cc/2026/Conference/Submission24869/Reviewer_ratK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24869/Reviewer_ratK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866595135, "cdate": 1761866595135, "tmdate": 1762943228589, "mdate": 1762943228589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a core problem prevalent in Large Language Model (LLM) fine-tuning: how to develop and utilize In-Context Learning (ICL) while preserving the generalization capabilities obtained through In-Weights Learning (IWL). The paper proposes a new training strategy called \"Contrastive-Context\" which involves: (1) mixing similar and random examples within a single context; and (2) varying the grades of similarity across different contexts. Operationally, the strategy selects, with a certain probability, either a \"most-similar\" real example, a \"highly-similar\" synthetic example (i.e., a paraphrase), or a \"random\" example, and fills the remaining $k-1$ slots with random examples. Experiments conducted on 4 Machine Translation (MT) tasks across 4 different LLMs (1B to 8B) show that Contrastive-Context performs exceptionally well in both In-Domain (ID) and Out-of-Domain (OOD) evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a very practical and important problem: the \"forgetting\" or \"degradation\" of ICL capabilities during fine-tuning. It clearly identifies \"context similarity structure\" as a key factor regulating the ICL-IWL balance.\n2. The claims are strongly supported through a multi-faceted approach:\n  - Empirical: It is evaluated on 32 configurations (4 models x 4 tasks x 2 evaluation sets), with detailed visual analysis across the entire similarity spectrum.\n  - Theoretical: A simplified two-layer Transformer model is used to mathematically analyze the parameter dynamics ($\\theta_1$ and $\\theta_2$) under different strategies, clearly explaining why the Random and Similar strategies fail while the Contrastive strategy succeeds.\n  - Diagnostic: The three designed probes (IWL-score, ICL-score, Copy-score) serve as a novel diagnostic tool, successfully linking the behavior of black-box LLMs to the mechanisms of the theoretical model and proving that the same trade-offs and failure modes exist in large-scale models."}, "weaknesses": {"value": "1. All empirical evaluations are concentrated on sequence-to-sequence Machine Translation (MT) tasks, lacking results on other task types (e.g., classification, reasoning, code generation).\n2. Creating \"highly-similar\" synthetic examples relies on an external model (e.g., gemini-2.0-flash-lite) to generate high-quality paraphrases . This adds complexity and dependency to the training pipeline, and the quality of this external model becomes a confounding variable that could impact the method's effectiveness."}, "questions": {"value": "1. How dependent is the effectiveness of the Contrastive-Context strategy on the capabilities of the external paraphrasing model (e.g., gemini-2.0-flash-lite)? If a weaker model is used, which generates lower-quality paraphrases (in terms of semantic fidelity or diversity), will the strategy's performance degrade significantly, or even fail completely?\n2. How can we determine if the role of the external model (Gemini) is merely to provide the \"highly-similar\" samples needed for \"contrast,\" or if it is simply providing \"better data\" (i.e., a form of high-quality data augmentation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4daLwPnzFr", "forum": "wnCJLnRBtb", "replyto": "wnCJLnRBtb", "signatures": ["ICLR.cc/2026/Conference/Submission24869/Reviewer_ZmL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24869/Reviewer_ZmL2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875640035, "cdate": 1761875640035, "tmdate": 1762943228358, "mdate": 1762943228358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how the similarity structure between target inputs and in-context examples affects the emergence and stability of in-context learning (ICL) and in-weights learning (IWL) in large language models (LLMs). While pre-trained LLMs can perform both ICL and IWL, standard fine-tuning often erodes ICL capabilities. The authors identify inter-example similarity as an overlooked but crucial factor in IC-Train. They introduce Contrastive-Context, a fine-tuning strategy that mixes similar and random examples within contexts. This contrast enforces models to use context information only when it is relevant and rely on in-weights knowledge otherwise. Experiments on machine translation demonstrate that Contrastive-Context consistently outperforms random and nearest-neighbor sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed Contrastive-Context approach effectively improves the model’s ICL performance across varying levels of example similarity.\n2.\tThe method is conceptually straightforward and easy to implement.\n3.\tThe authors evaluate multiple base models on machine translation tasks, demonstrating the approach’s robustness and generalization across different model architectures."}, "weaknesses": {"value": "1.\tThe proposed method can be viewed as a hybrid of Random-Context and Similar-Context strategies, which limits its novelty.\n2.\tExperiments are restricted to translation tasks, making it difficult to convincingly establish the generality of the approach across diverse task types.\n3.\tThe presentation of the paper could be improved — for example, the structure of the introduction section could be more organized, and there are several minor typographical errors (e.g., “IC-Trainwith” in Section 3.1)."}, "questions": {"value": "1.\tHow does Contrastive-Context perform on a broader range of tasks beyond machine translation?\n2.\tHow does it compare with a baseline that directly mixes Random-Context data and Similar-Context data during training, rather than creating contrast within examples in one context? \n3.\tIt would be helpful to include ablation experiments that remove the Highly-Similar component (using only Most-Similar examples) to verify the necessity of employing multiple similarity levels during training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOcfxcmDJ2", "forum": "wnCJLnRBtb", "replyto": "wnCJLnRBtb", "signatures": ["ICLR.cc/2026/Conference/Submission24869/Reviewer_fzhR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24869/Reviewer_fzhR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896101770, "cdate": 1761896101770, "tmdate": 1762943228152, "mdate": 1762943228152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}