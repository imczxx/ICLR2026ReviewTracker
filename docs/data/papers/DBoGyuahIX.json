{"id": "DBoGyuahIX", "number": 9798, "cdate": 1758140953563, "mdate": 1763749682267, "content": {"title": "Query Circuits: Explaining How Language Models Answer User Prompts", "abstract": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model’s behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, on average, a circuit covering only 1.3\\% of model connections can recover about 60\\% of performance on an MMLU question. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.", "tldr": "We introduce query circuit discovery, a task that traces the information flow inside the LLM from input to output, and demonstrate experimentally that it provides a promising path to faithful, scalable explanations of LLM decisions.", "keywords": ["large language model", "mechanistic interpretability", "local interpretability", "circuit discovery"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ceeb304f67ba5ac1269554522a6ba34c5c45321d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the task of finding input-specific circuits (subgraphs of model components) that explain a model's behavior on a given prompt. The paper first identifies issues with applying existing methods for finding and evaluating dataset-specific circuits, and then proposes a new evaluation metric, and a new circuit finding method, which is based on generating paraphrases of the given query and either aggregating the circuits from different paraphrases. Experiments on four datasets indicate that the proposed methods outperform baselines across sparsity levels."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses an interesting problem, explaining how LLMs make decisions for a given input.\n\n- The paper is written clearly and is relatively systematic. It provides convincing evidence that existing circuit finding methods perform poorly in this setting, and introduces a new metric along with a new method that empirically improves performance.\n\n- The best-of-N method seems sensible to me--it increases the set of prompts used for circuit finding, which could reduce noise. This could also mitigate against some issues with query-level circuit finding I discussed in the Weaknesses section, where a circuit could \"hard-code\" the model's output for a given query. This method can be thought of as creating a small \"capability\" dataset composed of paraphrases.\n\n- The experiments are thorough, covering four datasets and several different LMs. The results show that the proposed best-of-N methods outperform the single-query baseline, and the results generally improve with increasing N and with more edges.\n\n- The paper introduces two variants of the best-of-N method, which are faster to run.\n\n- The paper is generally clearly written and I found the presentation to be effective."}, "weaknesses": {"value": "_Soundness_\n\n- I think there is a conceptual issue with query-level circuits: if the objective is just to recover the output of model $M$ on a specific input $q$, there is nothing to prevent the method from finding a \"constant\" circuit that just always outputs $M(q)$, without reflecting the model's underlying computations.\n\n- Similarly, it is unclear to me that NDF (eq. 5) actually captures faithfulness. For the reason mentioned above, it is possible for some query circuit $C_q(q)$ to exactly match the model's output $M(q)$, without being faithful to the model. Specifically, $C_q$ could effectively \"hard-code\" the model's output, but be invariant to the input. I believe the definition of faithfulness needs to be grounded in some kind of counter-factual notion of faithfulness. See for example [1] for relevant discussion.\n\n- Section 3.3.1 argues that NFS is unstable for MMLU by showing that the score has high variance across number of edges. But this variance could be due to the circuit finding method rather than the metric. This experiment seems to be with query-level circuit finding, rather than task level circuit finding--it seems likely that the variance arises because the method finds different circuits at different N, not because of problems with the metric.\n\n- For this reason, I think it would be very informative to report the NFS score on the paraphrases generated in section 5. This would help understand if the issues identified in Sec. 3 are due to the metric, EAP-IG, or just applying these tools to a single query.\n\n_Contribution_\n\n- One of the stated contribution of this paper is to propose the task of finding input-specific circuits. There is relevant recent work that also studies input-specific circuits (albeit using transcoder features): [2]. I think this paper would benefit from a more extended discussion of that work. In particular, [2] discusses counter-factual experiments for evaluating whether the prompt-level circuits are faithful, which might be applicable here.\n\n- The paper does not give any illustrations of whether the resulting circuits can actually be used to interpret the model.\n\n_Minor comments_\n\n- The name \"Query circuits\" might be confusing given that \"query\" also describes a component in the attention mechanism--for example, see https://transformer-circuits.pub/2025/attention-qk/index.html about key-query circuits. A possible alternative could be something like \"prompt-level circuits\".\n\n- Typo in the abstract: \"For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on *an* MMLU questions.\" Also somewhat unclear to me--does this mean each one circuit for each MMLU question, with no circuit havings sparsity more than 1.3%?\n\n- There is a relevant prior work [3], which introduces an optimization-based method for finding edge circuits. This method mitigates issue with IE noted in section 3.3.2 (ignoring combinatorial effects among edges).\n\n_References_\n\n[1] Geiger et al., 2025. Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability.\n\n[2] Ameisen et al., 2025. Circuit Tracing: Revealing Computational Graphs in Language Models.\n\n[3] Bhaskar et al., 2024. Finding transformer circuits with edge pruning.\n\n**Summary:** I think the paper proposes a sensible method for an interesting problem, and the paper is generally thorough and clearly written. However, I have a number of doubts about the soundness of the results, given the possibility that query-level circuits can \"hard-code\" a response; and I feel the paper is missing a number of important baselines (like reporting NFS on paraphrased queries). I am open to increasing my score if the authors can add some discussion of how to address the soundness concerns; add more discussion of the connection to other work on prompt-level circuit finding; and add some of the results I mentioned above."}, "questions": {"value": "- In Fig 2a., did you try measuring NFS for capability circuits for MMLU, rather than query circuits? This could reveal if the variance is because NFS is unreliable on MMLU (as claimed in section 3.3.1), or because query circuits have high variance.\n\n- A natural baseline to compare with Best-of-N sampling is to simply generate $p$ paraphrases of query $q$ and then run EAP-IG with dataset $D = \\{q_1, \\ldots, q_p\\}$. This method might be simpler to implement and more general than the methods presented here, which involve interpolating between multiple potentially large score matrices. Is this the \"averaging\" baseline in Figure 6? It is not clear to me from the description in section 6.1.\n\n\n- For best-of-N sampling, do you also permute the order of the answer choices? Do you also paraphrase the possible answers? I think these changes could both help to reduce the likelihood that the circuit hard-codes an answer (although this will still be a possibility).\n\n\n- Are the results in figure 6 averaged over all of the queries in the dataset? I think it would be helpful to report some of these experimental details in section 6.1 and Figure 6: what is N for best-of-N, and how many examples are in each dataset.\n\n\n- Have you conducted any analysis into the relationship between query-level circuits for a given task (e.g. MMLU marketing)? For example, for a given task, how sparse is the union of query circuits? How much do the different circuits overlap?\n\n- Similarly, have you investigated the relationship between query-level circuits (e.g. for IOI) and task-level circuits? Are task-level circuits simply the union of query-level circuits? If the circuits are disjoint, this would be reason to suspect that the query-level circuits might be spurious.\n\n- Could this method be applied to circuits composed of SAE features, rather than coarse-grained model components, as in [1]?\n\n\n\n[1] Marks et al., 2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j56YWNOEWZ", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Reviewer_1YXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Reviewer_1YXG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753225065, "cdate": 1761753225065, "tmdate": 1762921284092, "mdate": 1762921284092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to explain a model's output on specific inputs through network connections, i.e., circuits. Unlike functional circuit discovery which focuses on general algorithmic capabilities, this method aims to provide local, query-level explanations. To better evaluate the faithfulness of discovered circuits, the paper introduces an improved evaluation metric (NDF). Two sampling methods are proposed that use paraphrases to help recover model behavior. On MMLU questions, circuits using only 1.3% of the model's connections can recover approximately 60% of the model's performance. However, the gap between \"finding sparse subnetworks that preserve performance\" and \"explaining how the model actually processes this input\" remains substantial."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured with clear problem formulation. The distinction between capability circuits and query circuits is well-articulated, and the motivation for local explanations is compelling. The proposed metric is a simple and well-motivated fix.\n- Using paraphrases to provide semantically-equivalent but slightly perturbed samples is intuitive and straightforward. The \"lottery ticket\" framing helps conceptualize why this approach works. The results are also strong and consistent across diverse benchmarks.\n- The experimental design is thorough. Tests span multiple benchmarks with varying complexity levels, include multiple baselines and ablations."}, "weaknesses": {"value": "- Selecting the \"best\" circuit through empirical search demonstrates that some set of connections can reproduce outputs effectively. However, it's unclear whether this constitutes finding the query circuit that explains how the model actually processed that specific input, or merely identifies one among many possible circuits that happen to work.\n- Functional circuit work (e.g., for addition or IOI) typically provides detailed mechanistic interpretations showing how circuits implement specific computations. This paper doesn't establish such demonstrations for query circuits, no analysis of what roles different nodes/edges play or why discovered circuits work.\n- The general query circuit discovery in this paper bears a strong assumption that there exists a meaningful general query circuit for each input that explains the model's reasoning. However, Is there actually a unique, identifiable circuit the model uses for each query? Or are there multiple valid circuits that could produce the same output? Can we distinguish a true \"query circuit\" from an arbitrary subset of connections that happens to preserve performance?\n- While the sampling method successfully identifies subnetworks that govern generation abilities, it doesn't provide explanations for how or why they work. The method could succeed even if the underlying theoretical motivation is incorrect, it's fundamentally a search procedure over candidates. Alternatively, it is difficult to determine whether the paper has found the actual query circuit used by the model or a subnetwork storing important knowledge."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZhRNSalYgn", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Reviewer_3hrb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Reviewer_3hrb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880557308, "cdate": 1761880557308, "tmdate": 1762921283535, "mdate": 1762921283535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new concept, query circuits, which recover LLM behavior by mimicking the information flow through the network. The authors measure the efficacy of their query circuits by their Normalized Deviation Faithfulness, which looks at the model's recovery on corrupted queries. Finally, the authors propose BoN for circuit discovery, and find that it does much better than previous algorithms for circuit discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a natural direction for circuit discovery (looking at local information flow as opposed to global algorithms) and also develops a solid method (BoN) that scales with more compute. \n- The paper derives a more natural method for BoN and studies many of its derivatives. \n- The results are promising and suggest that query circuits are recovering meaningful aspects of information flow in the model."}, "weaknesses": {"value": "- The discovered circuits are able to recover performance in the network, but they are likely not as interpretable and informative as more global capability circuits. While I don't think this is an inherent limitation, as there are many novel applications that can be explored by having good query circuits, there does appear to be a fundamental limitation in \"understanding\" LLM behavior if one only uses query circuits."}, "questions": {"value": "- Could there be more discussion of how these query circuits might be used in model interpretability? Identifying harmful query circuits (e.g., for jailbreaks) and pruning them + adjusting model behavior would be interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gA4IHdUhoS", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Reviewer_UiWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Reviewer_UiWe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982424342, "cdate": 1761982424342, "tmdate": 1762921282896, "mdate": 1762921282896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Response to Shared Questions and Concerns (iii)"}, "comment": {"value": "**3. Better to give an example of using the resulting circuits to interpret the model.**  \nWe agree that usage examples would greatly help illustrate the value of this direction, and we plan to include them in the camera-ready version. We didn’t incorporate this since translating model components to human-readable concepts is a big topic that could go for a complete paper. Instead, we chose to focus this submission on our primary contribution: establishing the setting of in-place query circuit discovery, identifying issues in prior approaches and evaluations, and demonstrating the feasibility and promise of this direction.  \n\nWe will generate illustrations via one of the two approaches: (1) follow prior capability circuit work (e.g., IOI) to manually investigate attention scores/logit attribution on query circuits of a few MMLU questions to identify the roles of internal components; (2) replicate and adapt SAE-based feature circuits to the prompt-level setting, with our insights (perturbating the prompts and using NDF) incorporated.  \n\nHowever, note that prior in-place circuit discovery studies have mostly targeted circuit diagrams (without manually investigating the roles of each component) as their intended granularity [1-7]. We have the same granularity as them, with the circuit diagrams shown in Table A5 in the revised manuscript.\n\n[1] Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms (COLM'24)  \n[2] Towards Automated Circuit Discovery for Mechanistic Interpretability (NeurIPS'23)  \n[3] Finding Transformer Circuits with Edge Pruning (NeurIPS'24)  \n[4] Sheaf Discovery with Joint Computation Graph Pruning and Flexible Granularity (EMNLP'25)  \n[5] Position-aware Automatic Circuit Discovery (ACL'25)   \n[6] Information Flow Routes: Automatically Interpreting Language Models at Scale (EMNLP'24)  \n[7] Circuit Stability Characterizes Language Model Generalization (ACL'25)"}}, "id": "IGLaXgMDlc", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754299345, "cdate": 1763754299345, "tmdate": 1763758556065, "mdate": 1763758556065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses mechanistic interpretability’s circuit discovery for LLMs at the single-query (atomic) level, instead of the frequent capability/dataset level. To analyze these queries/prompts it uses edge-attribution patching methods (EAP,  EAP-IG) on different benchmarks (IOI, arithmetic, MMLU, and ARC). As circuits on the atomic level quickly lead to unstable faithfulness results on symmetric faithfulness (NFS), the paper proposes an updated faithfulness metric (NDF) to evaluate the discovered circuits and also uses multiple samples per query, where it produces different paraphrases of the original query for stabilization.\nIt uses GPT-2 Small and Llama-3.2-1B-Instruct and shows the discovered circuits recover ~60% of the model’s behavior with only ~1–2% of edges, where the edge-attribution score matrices from several paraphrases share similar patterns, while at the same time small score differences between paraphrases can change them, highlighting the necessity of using several paraphrases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Analyzing prompts/queries on circuit level is an interesting addition to the interpretability toolbox, highlighting e.g., a model’s in-context learning capabilities. \n- Similarly, showing that circuits can also be analyzed on an atomic level opens up new avenue and using sampled paraphrasing is a simple yet effective way to overcome brittleness.\n- The paper proposes an updated faithfulness metric tocomparte different prompts/paraphrases per query.\n- The approach is used on real-world datasets (beyond toy datasets)."}, "weaknesses": {"value": "- The discovered circuits are still to be treated with caution as small shifts in one query paraphrase can highly influence the scores of multiple paraphrases.\n- The paper only uses small models, so the generalizability is somewhat limited here.\n- The method relies in huge parts on previous work from circuit discovery and as such presents only moderate methodological novelty."}, "questions": {"value": "Can you quantify variance in circuits across paraphrases for each prompt and task? (That would make the brittleness directly visible.)\nDo the high-scoring edges across paraphrases cluster in specific layers/heads (i.e. is there a consistent “always-on” subgraph)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4m0oq21qL3", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Reviewer_7mAG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Reviewer_7mAG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992369678, "cdate": 1761992369678, "tmdate": 1762921282278, "mdate": 1762921282278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Response to Shared Questions and Concerns (ii)"}, "comment": {"value": "**2. How do you know the discovered circuit is not a trivial one (e.g., a circuit always producing the “C” token)?**  \n\nThere are three main reasons why studies on in-place circuit discovery (those not on SAEs/CLTs, such as ACDC [2]), including us, believe that the automatically discovered circuits are non-trivial to the model’s underlying computation:\n\n(a) Early works [1, 2] have found that, on datasets where ground-truth human-identified circuits are available (IOI, Greater-than, etc), circuits identified by IE-based circuit discovery have well-matched the ground truth.  \n\n(b) To strengthen the argument of nontriviality, some prior works [3] and also our paper conduct counterfactual experiments by evaluating the performance of the model with the discovered circuit ablated. It has been consistently found that the ablation breaks the model performance on the original task/query (e.g., Figure 3 of feature circuits [3] and Figure A14 of our paper). This finding suggests that the model indeed relies on the discovered circuits to perform the task/query.  \n\n(c) In-place circuit discovery (mostly IEs) identifies edges with high influence on the model output. Hence, even though there may be a trivial “constant-value circuit” inside the model, if the model doesn’t rely on it to generate the answer, the circuit should have low IE scores and won’t be picked. Note that we don’t deny the potential existence of trivial circuits; IE-based methods naturally pick circuits that are important for the model's computation.  \n\nAdditionally, based on the experimental results in Official Response to Shared Questions and Concerns (i), if the identified query circuit were merely a trivial graph that happens to produce a high logit for the answer token, it would be unlikely to share a subgraph with the IOI capability circuit and with the query circuits derived from different paraphrases.\n\n**Discussion on the “uniqueness of circuit” with Reviewer 3hrb:**  \nTo our knowledge, there’s no literature in circuit discovery arguing the uniqueness of circuits. In fact, insights from other domains have revealed the existence of multiple internal pathways when the model is generating the output [4]. For circuit discovery, this finding can be interpreted as follows: when you gradually enlarge the circuit (incorporate more edges), it may include more implicit pathways. Conceptually, the size of circuits simply depends on whether you want to focus on “the most critical computational pathway” or also “secondary pathways”.  \n\n[1] Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms (COLM'24)  \n[2] Towards Automated Circuit Discovery for Mechanistic Interpretability (NeurIPS'23)  \n[3] Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models (ICLR'25)  \n[4] Chain-of-Thought Is Not Explainability (alphaXiv)"}}, "id": "L8g21r5XyX", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754693517, "cdate": 1763754693517, "tmdate": 1763760291830, "mdate": 1763760291830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Response to Shared Questions and Concerns (i)"}, "comment": {"value": "**1. More analysis on the relationship between capability and query circuits, and among query circuits.**  \nWe use IOI dataset to conduct a thorough analysis. Experimental results are in the folder “more_circuit_analysis/” at https://anonymous.4open.science/r/iclr2026-query-circuit-rebuttal. They particularly answer whether BoN sampling (1) merely produces unrelated disjoint circuits where some coincidentally output the correct token, or (2) discovers variants of a common mechanism that preserve a shared set of critical edges (i.e., shared sub-circuit), regardless of how the query is phrased.  \n\nSpecifically, “more_circuit_analysis/” contains four experiments to support (2):\n\n(a) The Jaccard similarity consistently falls in [0.3, 0.4] when the IOI circuit is smaller than 2000 edges, as shown in *jaccard_similarity_qc_ltop.pdf* and *jaccard_similarity_qq_ltop.pdf* (*qc* means comparison between query and capability circuit; *qq* means among query circuits). We also report experiments on large IOI circuits, with Jaccard similarity becoming larger, as shown in *jaccard_similarity_qc_htop.pdf* and *jaccard_similarity_qq_htop.pdf*. These show non-trivial edge overlap among these circuits.\n\n(b) We calculate how many percent of edges in a query circuit are also in the capability circuit. Roughly, an IOI query circuit with 1000 edges has half of its edges also residing in the capability circuit, as shown in *intersect_ratio_qc_ltop.pdf* and *intersect_ratio_qc_htop.pdf*. This result echoes with (a).\n\n(c) UpSet plots in folder “edgeNum_{500 or 1000}\\_paraNum\\_{5 or 10}/” display detailed numbers of edge overlap between the capability circuit, query circuit discovered by a query, and query circuits discovered by the query’s paraphrases. The folder contains 10 randomly selected queries (sample index=0, 177,...).  \n\n(d) In the folder “circuit_plot/”, we plot the query circuits of a randomly selected query and the IOI capability circuit. Nodes and edges shared among all of them are marked in red, constituting a critical “common sub-circuit”; green nodes and edges are those unique to the residing circuit.  \n\nThe above results form a firm argument that BoN samples variants of a common, underlying mechanism that preserve a shared set of critical edges. We’ve incorporated them as Section 6.4 and Appendix B.9 in the revised manuscript."}}, "id": "vaLOcPceMO", "forum": "DBoGyuahIX", "replyto": "DBoGyuahIX", "signatures": ["ICLR.cc/2026/Conference/Submission9798/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9798/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9798/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755888472, "cdate": 1763755888472, "tmdate": 1763756437677, "mdate": 1763756437677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}