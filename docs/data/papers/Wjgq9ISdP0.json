{"id": "Wjgq9ISdP0", "number": 20527, "cdate": 1758307096956, "mdate": 1763747046153, "content": {"title": "Unlocking Out-of-Distribution Generalization in Transformers via Latent Space Reasoning", "abstract": "Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning---and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD algorithmic generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.", "tldr": "This paper explores a set of architectural mechanisms to support algorithmic generalization in Transformers through a recurrent latent space reasoning approach.", "keywords": ["algorithmic generalization", "out-of-distribution", "OOD generalization", "compositional", "systematic generalization", "transformer", "transformer architecture"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33bae732478f3b7f66334a78c88161073699cd95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies Transformer length generalization on a modular arithmetic on computational graph task. The authors show that while some degree of length generalization is possible using standard CoT training, four architectural changes: (i) recurrence; (ii) algorithmic supervision; (iii) anchored latent representations (iv) error-correction enable length generalization far beyond the training length. They also provide a mechanistic interpretability analysis showing how the required (length-generalizing) algorithm can be implemented with two attention layers and an MLP, performing copy, induction head, and modular addition roles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and well-written, the problem is interesting and important, and the four architectural mechanisms are well motivated and yield an impressive improvement in length generalization on the chosen task."}, "weaknesses": {"value": "My main concern is that only a single task, namely modular arithmetic on computational graph, is studied in this paper. It is unclear whether the four architectural mechanisms can yield similar improvements on other tasks. I appreciate that this task is somewhat representative of a fairly broad class of compositional problems, but I would be more convinced if improvement in length generalization after applying the four changes could be shown on other tasks as well. The architectural changes seem somewhat tailored to the specifics of the task. In order to broadly useful, the changes would also need to be helpful for a larger class of problems.\n\nThe mechanistic interpretability study is similarly limited to the single specific task, and it is not fully clear to what extent it can give insight into other types of problems/tasks. Also the mechanisms identified are well known (which is fine, just not a major source of novelty in the paper)."}, "questions": {"value": "Please see Weaknesses. In particular, can the authors comment on whether the four architectural changes are likely to be helpful for other tasks beyond modular arithmetic on computational graph? Or, what kinds of alternative architectural changes might be required for different types of algorithms tasks?\n\nAlso, could the authors please comment on modular arithmetic on computational graph as a representative of compositional problems more generally? Is there a sense in which a large and important class of problems is essentially equivalent to this task? In particular, is there a broader class of problem for which we can be confident that the four architectural changes will yield improvements in length-generalization?\n\nMinor:\nFig 5: var1 appears twice, was is supposed to be var2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XMGuF23tsU", "forum": "Wjgq9ISdP0", "replyto": "Wjgq9ISdP0", "signatures": ["ICLR.cc/2026/Conference/Submission20527/Reviewer_UtVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20527/Reviewer_UtVk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761029012082, "cdate": 1761029012082, "tmdate": 1762933947755, "mdate": 1762933947755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response [1/3]"}, "comment": {"value": "We sincerely thank all reviewers for their feedback. We summarize the goals, methods, and contributions of the paper, then the main strengths and concerns raised across reviews, and finally provide two global clarifications on (i) the generality and applicability of our architectural mechanisms and (ii) new methods and results on relaxing depth decomposition supervision (Mechanism 2).\n\nOur responses are numbered for easy reference (e.g., `Global 1`, `R2-3`).\n\n---\n\n## Summary of this work and its contributions\n\n**Key Goals:**\n- Identify why Transformers fail at OOD / length generalization on algorithmic tasks.\n- Introduce architectural mechanisms that induce scalable depth-invariant algorithms and enable robust OOD generalization\n- Connect OOD gains to specific internal computations via mechanistic analysis.\n\n**Methods:**\n- Use modular arithmetic on computation graphs as a controlled testbed with tunable depth and size.\n- Propose an architecture for latent recurrent reasoning via four key mechanisms:\n   1. Input-adaptive recurrence in latent space\n   2. Algorithmic latent space supervision\n   3. Anchored discrete latent representations\n   4. Error-correction via training-time perturbations of latent states\n- Carefully ablate each mechanism to evaluate its contribution\n- Compare against strong baselines, each with extensive hyperparameter tuning, including:\n  - Feedforward end-to-end Transformers.\n  - Recurrent Transformers without our mechanisms.\n  - Autoregressive Chain-of-Thought (CoT) training.\n- Analyze learned recurrent solution mechanistically.\n\n**Results:**\n- Feedforward, recurrent end-to-end, and CoT models fail to generalize to deeper graphs despite broad hyperparameter sweeps.\n- Combining the four mechanisms yields near-perfect OOD generalization on graphs several times deeper and larger than those seen in training.\n- Ablations isolate the role of each mechanism and show that all four are needed for robust compositional generalization.\n- Mechanistic analysis reveals a depth-invariant algorithm (copy, induction, modular addition).\n\n---\n\n## Summary of Reviews & Responses\n\n### Strengths\n- **Clarity and motivation**\n  - R-Ewgh: Paper is clear, well-written, and tackles an important problem: OOD / length generalization in algorithmic reasoning.\n- **Principled, controllable setup**\n  - R-y1eG, R-UtVk: Appreciate the use of a synthetic, controllable computation-graph domain instead of only natural-language tasks.\n- **Rigorous experimental methodology**\n  - R-y1eG: Praises extensive hyperparameter search and detailed ablations for each architecture.\n- **Substantial OOD gains from the proposed mechanisms**\n  - R-Ewgh, R-UtVk: Emphasize that our mechanisms yield impressive improvements in length generalization far beyond the training regime.\n- **Mechanistic interpretability complement**\n  - R-Ewgh: View the mechanistic analysis as a valuable complement to performance experiments.\n\n---\n\n### Main Concerns and where we address them\n\n- **C1. Generality and applicability of proposed mechanisms to algorithmic tasks**\n  - Raised by: R-Ewgh, R-y1eG, R-UtVk.\n  - We explain why we focus on a single, controlled task and how computation-graph modular arithmetic instantiates a general DAG-based abstraction that supports applying each mechanism to broader algorithmic tasks.\n  - See: `Global 1`, `R3-1`, `R2-3`\n\n- **C2. Relation to prior work**\n  - Raised by: R-y1eG.\n  - We expanded Related Work and clarified connections to prior benchmarks and method:\n    - Dziri et al. and Thomm et al. *evaluate* pretrained Transformers on compositional tasks and highlight failures of depth generalization, which motivates our architectural approach.\n    - Csordás et al. identify tuning \"tricks\" (positional encodings, early stopping) that we already explore and which are insufficient on this task.\n    - Abnar et al. and Wang et al. study recurrent and latent reasoning architectures on different tasks; we clarify how our mechanisms differ, in particular via discrete latent states, fixed-point halting, and explicit OOD-depth focus.\n  - See: `R2-1`, `R2-2`.\n\n- **C3. Dependence on oracle-like structure and supervision**\n  - Raised by: R-Ewgh\n  - Our supervision is weaker than CoT: it uses only coarse depth ordering, not full token-level traces. It can be produced using the same pipelines as CoT (human traces, programmatic oracles, or model-generated traces).\n  - We also describe how the model can infer depth structure without oracle decomposition and provide new experiments supporting this.\n  - See: `Global 2`, `R1-1`.\n\n- **C4. Nature of \"adaptive\" recurrence**\n  - Raised by: R-Ewgh, R-y1eG.\n  - The model iterates until reaching a fixed point in discrete latent space, giving a simple, exact, learned halting criterion.\n  - See `R1-2` and `R2-4`"}}, "id": "k6pS7nBh9W", "forum": "Wjgq9ISdP0", "replyto": "Wjgq9ISdP0", "signatures": ["ICLR.cc/2026/Conference/Submission20527/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20527/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20527/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755845242, "cdate": 1763755845242, "tmdate": 1763755845242, "mdate": 1763755845242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work analyses the OOD length generalization of feed-forward and latent recurrent Transformer architectures on a modular arithmetic task, where a model is trained on computational graphs of size up to $L=32$ and tested on up to $L=128$. After observing the failure of feed-forward Transformers (both in direct and CoT configurations), the authors investigate/propose ingredients that enable length generalization in this task: latent recurrence, step-level recurrence supervision, discretized latent representations, and a latent noise injection to introduce error-correction mechanisms. Combining all the ingredients indeed leads to length generalization. Finally, the inner workings of the trained recurrent latent Transformer are mechanistically analyzed on the attention level."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is very well written and motivated. Algorithmic generalization is an open and important challenge in AI; approaching the problem in a principled way with a controllable dataset that does not impose any secondary effects (as in natural language) is commendable.\n2.\tThe paper indeed goes to incredible lengths to make the performance of individual architectures hyperparameter invariant. Indeed, hyperparameters of every architecture are individually optimized, and the extensive Appendix describes all ablations. The rigor in hyperparameter search is highly appreciated."}, "weaknesses": {"value": "My main concern is the missing connection to related work. The paper is written like starting from a blank sheet of paper. While this makes the reading very coherent and easy to follow, it makes the distinction to already known insights from the literature difficult. Specifically, I would appreciate the following connections to related works:\n\n1.\tMissing connection to arithmetic tasks testing length generalization. There is an active area of research on evaluating the expressiveness of neural networks on algorithmic tasks [Dziri et al. 2023, https://arxiv.org/pdf/2305.18654 ] [Thomm et al. 2024, https://arxiv.org/abs/2402.05785 ], specifically targeting length generalization too. Having a comparison to some of the tasks, would be appreciated. It is crucial to situate the proposed task within existing benchmarks and compare the task's difficulty and characteristics against well-known algorithmic generalization tasks.\n2.\tMissing connection to results reported in the literature. As already mentioned in the strengths, the paper creates reasonable baselines for their task. Yet, it could still be that some techniques had been missed [Csordás et al. 2022, https://arxiv.org/pdf/2108.12284 ]. One way to have a more credible comparison is to use (or better reproduce) results from the literature. Given that there already exist tasks on algorithmic length generalization (see Weakness 1), one could use these tasks and baseline performance as an anchor point. To strengthen the claim that the current task is uniquely challenging or that the proposed architecture is necessary, the authors should reproduce and report the performance of the standard feed-forward Transformer on a canonical, related task to establish a clear anchor and ground the observed failures.\n3.\tMissing connection to findings on recurrent neural networks. I find it quite hard to categorize the insights gained in this work. The main issue lies in that many of the techniques have already been proposed and validated in previous works (though in different tasks): recurrence [35] [Abnar et al. 2023, https://arxiv.org/pdf/2310.08866 ], or discretization [Wang et al. 2025, https://arxiv.org/abs/2506.21734 ]. Hence, the main contribution of the paper can be read as finding the necessary ingredients to enable OOD length generalization on a single task. I am convinced that the findings can translate to other algorithmic tasks; it just would have to be demonstrated. The major issue is the task-specific nature of the findings. I strongly encourage the authors to demonstrate the generalizability of the necessary ingredients (recurrence + supervision + discretization) by applying the full successful model to at least one other standard OOD length generalization task from the literature."}, "questions": {"value": "Besides answering the main weaknesses (W1-W3), I have the following minor questions:\n\n1. Lines 229-230 state that the recurrent iterations (T) are adapted to input complexity. Is this done manually, or is there an automated mechanism?\n2. Appendix C.1 describes a task-specific latent state embedding structure. Could this structure also be applied to the feed-forward or CoT approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q0c8IBp9wZ", "forum": "Wjgq9ISdP0", "replyto": "Wjgq9ISdP0", "signatures": ["ICLR.cc/2026/Conference/Submission20527/Reviewer_y1eG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20527/Reviewer_y1eG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549562479, "cdate": 1761549562479, "tmdate": 1762933947211, "mdate": 1762933947211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies transformer's out-of-distribution generalization on problems longer and more complex than ones trained on. The authors designed a flexible graph-based, arithmetic task domain, and explored how several architectural modifications including recurrence, latent representation supervision and discretization, as well as error correction contribute to an improvement on OOD performance in this task domain better than CoT training. The behavioral gains are complemented by interpretability analysis to reveal how the solution is implemented in the model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- This work provides a detailed study of transformers generalization capabilities on a target task domain. The transformer variant with additional mechanisms achieve pretty strong generalization to harder tasks than seen during training.\n- The addition of the interpretability analysis is a plus to complement the performance evaluations.\n- The presentation is quite clear."}, "weaknesses": {"value": "My main concern is the generality of the results. While the work showed strong OOD performance on this particular task, it's unclear whether the same architectural mechanisms easily transfer to other tasks or improve the overall OOD generalization capabilities in transformers. Specifically:\n- Many proposed mechanisms such as per-layer supervision and discretization seem to strongly rely on oracle knowledge of the particular task. This would restrict application to tasks without clearly defined \"steps\" that progress towards a solution.\n- The recurrence is described as adaptive, but it's unclear if there is a learned exit strategy by the model, or if the # of iterations remains a hyperparameter.\n\nI think the paper would be greatly enhanced if the authors could experiment or at least discuss how these methods may extend to additional task domains."}, "questions": {"value": "- Could you clarify if the recurrence adaptation is learned or if the length of iteration is arbitrarily specified? If not learned, have you considered exploring a version where the exit strategy is learned?\n- Do you think the proposed model variant would hinge the learning of general language modeling capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YneNKfSqXr", "forum": "Wjgq9ISdP0", "replyto": "Wjgq9ISdP0", "signatures": ["ICLR.cc/2026/Conference/Submission20527/Reviewer_Ewgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20527/Reviewer_Ewgh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850469137, "cdate": 1761850469137, "tmdate": 1762933946400, "mdate": 1762933946400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}