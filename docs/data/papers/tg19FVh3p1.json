{"id": "tg19FVh3p1", "number": 23330, "cdate": 1758342237450, "mdate": 1759896820711, "content": {"title": "Weak-to-Strong Diffusion", "abstract": "The goal of generative diffusion models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations of current generative models lead to an inevitable gap between generated data and real data. To address this, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated gap between existing weak and strong models (i.e., weak-to-strong gap) to bridge the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong gap, W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving significantly improved performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90\\% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong gap further solidify its practical utility and deployability.", "tldr": "", "keywords": ["Diffusion Models", "Diffusion Sampling", "Text-to-Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8dc998f8310ad78d5e8cc4cbb8648a2821f767e3.pdf", "supplementary_material": "/attachment/149a2eb67e97ef2a2f43b6d68e34e91e0332fda8.zip"}, "replies": [{"content": {"summary": {"value": "- This paper introduces Weak-to-Strong Diffusion (W2SD), a novel, training-free framework designed to enhance the performance of diffusion models during inference. The core problem it addresses is the \"modeling gap\": the discrepancy between the data distribution learned by a diffusion model and the true, ideal data distribution.  \n- Essentially,  instead of trying to directly estimate this unattainable \"strong-to-ideal\" gap, W2SD leverages a more accessible proxy: the \"weak-to-strong\" gap. This is the difference in behavior between a high-performing \"strong\" model (e.g., a fine-tuned model like Juggernaut-XL) and a lower-performing \"weak\" model (e.g., a base model like SDXL).  \n- The framework operates through a reflection mechanism. During sampling, it alternates between:\n  - A denoising step using the strong model (to move towards a high-quality output).  \n  - An inversion step using the weak model (to add back a controlled amount of noise, effectively \"reflecting\" the latent variable).  \n- A key contribution is the broad definition of what constitutes a \"weak\" and \"strong\" model. The paper demonstrates three types of gaps:\n  - **Weight Gap:** Different model weights (e.g., SD1.5 vs. DreamShaper, base model vs. a LoRA-adapted model, strong vs. weak experts in a Mixture-of-Experts model).\n  - **Condition Gap:** The same model under different conditions (e.g., high vs. low classifier-free guidance scale, detailed vs. simple text prompts).  \n  - **Sampling Pipeline Gap:** Different inference pipelines (e.g., ControlNet/IP-Adapter vs. standard DDIM sampling).  \n\n  As shown in the paper, when combined, they usually yield better results.  \n\n- Extensive experiments across image and video generation tasks show that W2SD consistently improves performance on metrics like human preference (HPS v2, PickScore), aesthetics (AES), and fidelity (FID), often outperforming related methods like Auto-Guidance and Z-Sampling. Crucially, the performance gains are shown to outweigh the additional computational cost, making W2SD efficient under equal time constraints."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow.  \n- **High Flexibility and Generality:** W2SD is not a single technique but a meta-framework. Its ability to incorporate various types of model pairs (weights, conditions, pipelines) makes it widely applicable across many use cases without retraining.  \n- **SOTA Empirical Results:** The paper provides comprehensive quantitative and qualitative evidence across multiple benchmarks (Pick-a-Pic, DrawBench, ImageNet), architectures (UNet, DiT), and tasks (image, video generation), demonstrating consistent and significant improvements.  \n- **Training-Free:** As an inference-time method, W2SD can be readily applied to existing models and pipelines without the need for costly additional training.  \n- **Diverse evaluation:** The method is tested on many datasets, different diffusion models, and various types of diffusion use-cases.\n- **Ablation Studies:** The method is properly analyzed on both synthetic and real data cases. Each part of the method is thoroughly interpreted, highlighting its effectiveness. Additionally, a toy test is shown in the paper (Figure 6), demonstrating the benefit of the method, where failure cases of both the strong and the weak models were exploited to produce pleasing-looking images of cars."}, "weaknesses": {"value": "- The contribution of the result from Theorem 1 is minimal: It states a very naive result, without adding any contribution to the paper or supporting the method's effectiveness in any way.  \n- The method assumes that the score function is very smooth w.r.t. t, specifically the approximation in (4). This might be a limiting factor of the method in accelerated diffusion setups, where the jump between two consecutive steps is more significant.  \n- The method is not supported by theoretical justification.  \n- Performance Degradation: If the weak-to-strong gap is not a good proxy for the strong-to-ideal gap (or is even negatively aligned), the method can lead to worse results than the strong model alone.  \n- Limited Exploration of Limitations: Some practical limitations are mentioned only briefly in the appendix. A deeper discussion of failure modes or scenarios where W2SD is not applicable would be beneficial."}, "questions": {"value": "- How does this method perform when using a smaller number of diffusion steps? Does the assumption of the smoothness of the score degrade the generation performance?  \n- Is it possible to theoretically justify this improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SwYHdwTcvJ", "forum": "tg19FVh3p1", "replyto": "tg19FVh3p1", "signatures": ["ICLR.cc/2026/Conference/Submission23330/Reviewer_nwAH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23330/Reviewer_nwAH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928064595, "cdate": 1761928064595, "tmdate": 1762942608126, "mdate": 1762942608126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new approach to minimize the gap between the learned probability distribution and the actual probability distribution(intractable) of data. This uses the divergence of distribution of a weak model and a strong model as the proxy for divergence of real and learned distributions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple and general idea with practical reach. Using the difference between a weak and a strong model as proxy for the actual difference between real and learned distribution seems to be fair given the assumptions in the paper.\n2. The W2SD algorithm is backed by various experiments, showcasing that it can be practically used with pretrained models."}, "weaknesses": {"value": "1. The central claim is that the weak-to-strong gap $\\Delta_1$ is a useful proxy for the intractable strong-to-ideal gap $\\Delta_2$. The paper provides cosine-similarity evidence on CIFAR, but this may not generalize: for some kinds of differences (e.g., when $M_w$ is biased towards different modes than the ideal), $\\Delta_1$ might be orthogonal or adversarial to $\\Delta_2$. The presented sensitivity analyses (gap sign and magnitude) indicate failure modes, but there is not a principled rule for selecting pairs to guarantee positive gains.\n2. Even though the method shows gains in various quantitative scores, the increase in computation is not described in paper and is an important factor when comparing with techniques such as Z-sampling, AutoGuidance, CFG, etc."}, "questions": {"value": "1. What is the increase in computation for this method as compared to the SOTA methods such as Z-sampling, AutoGuidance etc?\n2. What is the relation between the values of $\\mathcal{T}$ and $\\lambda$? How are these decided based on the models? A discussion on this would be helpful as to how these parameters handles the proxy.\n3. Since LoRA like techniques are used for personalization i.e., to bias a model towards a specific distribution, what is the intuition behind considering the model strong? Strong in what terms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D9lFSyJwsM", "forum": "tg19FVh3p1", "replyto": "tg19FVh3p1", "signatures": ["ICLR.cc/2026/Conference/Submission23330/Reviewer_Xkjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23330/Reviewer_Xkjj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928582037, "cdate": 1761928582037, "tmdate": 1762942607715, "mdate": 1762942607715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Weak-to-strong diffusion models (W2SD), an inference‑time framework that uses a reflection operator combining a strong diffusion model (M_s) and a weak model (M_w) to push samples along the estimated weak‑to‑strong gradient gap.\n\nThe proposed framework is flexible in choosing weak/strong pairs, including weight gaps, condition gaps, MoE routing gaps, and pipeline gaps. Experiments cover text‑to‑image, personalized LoRAs, and even video generation, showing consistent improvements and cumulative gains across gaps"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The unified W2SD is general and applicable to different diffusion models. \n- Strong results covering different regions of interest."}, "weaknesses": {"value": "Overall, the presentation of this paper is a problem. There are many figures and tables in both the main paper and the supplementary. However, the authors failed to organize them well in the text, making the paper slightly hard to read. A few concrete comments include: \n- Figure 2 is hard to understand. No further explanations are associated with it. \n- Algorithm 1 did not show anything specific about the W2SD algorithm, but a widely used procedure for inference-based optimizations. \n- Eq 5 and the sentence below it are inconsistent, i.e., no \\delta_1(t) at all. \n- Also, Eq 5 cannot be called a \"Theorem\". Even with Section A in the Appendix, altogether they are a definition but not a strict proof. \n- l135-137: It is unfair to treat Z-Sampling (Bai et al., 2024) and other similar methods as instances of W2SD.\n\nOther weaknesses: \n- Comparison to stronger baselines. For most of the experiments, the authors compare the W2SD version with the corresponding baseline model. It is also better to include other baselines. For example, with W2SD, can we tweak SDXL to Flux? \n- Mathematical support for the correctness of W2SD is missing."}, "questions": {"value": "- How does W2SD compare to Auto‑Guidance and Z‑Sampling **under matched compute** across diverse prompts (quantitatively)?\n- Does repeated reflection risk **mode collapse** in long runs (i.e., is there guaranteed convergence?)? \n- Theoretically, how to justify or prove the claim of bridging the strong-to-ideal gap with the weak-to-strong gap? \n- How to precisely define a weak, strong, and idea model in the framework, for example, for specific scenarios, a common weak model can be \"strong\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TSMJSUaZyf", "forum": "tg19FVh3p1", "replyto": "tg19FVh3p1", "signatures": ["ICLR.cc/2026/Conference/Submission23330/Reviewer_99Dw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23330/Reviewer_99Dw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950216431, "cdate": 1761950216431, "tmdate": 1762942607461, "mdate": 1762942607461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Weak-to-Strong Diffusion (W2SD), a novel, training-free framework designed to enhance the performance of pre-trained diffusion models at inference time. The core idea is to leverage the \"weak-to-strong gap\"—the difference in score estimates between a capable (strong) model and a less capable (weak) one—as a proxy for the unattainable \"strong-to-ideal\" gap. The authors propose a reflective sampling mechanism, which alternates between denoising with the strong model and inverting with the weak model, to steer the sampling trajectory towards the true data distribution. W2SD is presented as a general meta-framework, and its effectiveness is demonstrated across a wide variety of \"weak-strong\" pairings, including gaps in model weights, conditional inputs (e.g., CFG scale, prompt quality), and sampling pipelines, showing consistent improvements on both qualitative and quantitative benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, Effective, and General Idea: The core concept of using the difference between a weak and a strong model to approximate the direction of improvement is both intuitive and powerful. The framework's ability to operate on various types of \"gaps\" (weight, condition, etc.) demonstrates its impressive versatility.\n\n2. Strong and Comprehensive Empirical Results: The paper provides extensive evidence of W2SD's effectiveness across multiple models (SD1.5, SDXL, DiT), tasks (image, video), and metrics (human preference, prompt adherence). The cumulative effect shown in Table 7 is particularly compelling.\n\n3. Excellent Clarity and Presentation: The authors have done an outstanding job of explaining a complex idea in a clear and accessible manner. The writing and visualizations significantly aid in understanding the method's mechanics and motivation."}, "weaknesses": {"value": "1. Idealized Assumption of the \"Weak-to-Strong\" Gap: The framework's core assumption is that the weak-to-strong gap vector is a reliable proxy for the strong-to-ideal gap. While this holds in the controlled 1D/2D experiments where models differ mainly in data bias, it may be too idealized for real-world scenarios. In practice, a strong and weak model may have qualitatively different failure modes or \"worldviews\" due to differences in architecture or fine-tuning data. In such cases, their difference vector could be noisy or even misaligned with the true direction of improvement. The paper could benefit from a discussion of this limitation.\n\n2. Necessity of the Reflection Mechanism: The proposed reflective operation (denoise-then-invert) is elegant, but it is not immediately obvious if this complexity is necessary. A simpler alternative, such as directly guiding the strong model's prediction with the difference vector, might achieve similar results with less computational overhead. The lack of comparison against such a baseline makes it harder to isolate the contribution of the specific reflection mechanism."}, "questions": {"value": "1. Regarding the mechanism itself: Could a simpler \"difference guidance\" approach work as well? For example, at each step, one could compute the predicted noise from both models, $\\varepsilon_s$ and $\\varepsilon_w$, and form a new prediction $\\varepsilon_{\\text{final}} = \\varepsilon_s + \\alpha \\cdot (\\varepsilon_s - \\varepsilon_w)$, where $\\alpha$ is a guidance scale. This seems to capture the same core idea but avoids the model inversion step. How does the proposed reflection operator compare to such a baseline?\n\n2. Regarding the visualizations in Figures 3 & 4: Could you provide more specific details on how these plots were generated? For instance, how were the probability densities for the \"Ideal,\" \"Strong,\" and \"Weak\" models in Figure 3 obtained? For Figure 4, what were the specific data distribution ratios used to train the models to induce the bias? This would improve the reproducibility and clarity of this excellent intuitive demonstration.\n\n3. Regarding the definition of \"weak-strong\" pairs: The paper shows many successful pairings. From your experience, are there examples of model pairs that are difficult to define as strictly \"weak\" or \"strong,\" or pairs that failed to produce improvements? For example, what if one model is better at global composition but worse at fine details, while the other is the opposite? A discussion on such challenging or failure cases would provide a more complete picture of the framework's boundaries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HAHSdzVzL4", "forum": "tg19FVh3p1", "replyto": "tg19FVh3p1", "signatures": ["ICLR.cc/2026/Conference/Submission23330/Reviewer_DEkr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23330/Reviewer_DEkr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005978040, "cdate": 1762005978040, "tmdate": 1762942607228, "mdate": 1762942607228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Weak-to-Strong Diffusion (W2SD), an inference-time method that alternates between denoising with a strong model and inverting (i.e. adding noise back) with a weak model. Using the weak-to-strong gradient gap ($\\Delta_1 = \\nabla \\log p_s  - \\nabla \\log p_w$) to approximate the strong-to-ideal gap ($\\Delta_2 = \\nabla \\log p_{\\text{data}}  - \\nabla \\log p_s$) the authors demonstrate a training-free to improve sampling using a variety of different weak/strong model pairs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**(S1)**: The approach is simple and training-free, and can flexibly be used with various different models.\n\n**(S2)**: Experimental results explore a variety of different diffusion models and architectures.\n\n**(S3)**: A discussion on compute-aware sampling is provided, with considerations for sampling quality within a fixed wall-clock budget. This is relevant for broader applicability."}, "weaknesses": {"value": "**(W1)**: The proof of Theorem 1 is weak. There is no solid justification given to \"neglect the approximation error\" (L727). The proof ignores Jacobian terms or any formulation for the approximation error. No conditions are provided for when $\\Delta_1 \\approx \\Delta_2$. Omitting these conditions is a significant weakness. This makes the justification more heuristic than theoretical.\n\n**(W2)**: The authors note that when the gap magnitude is negative (i.e. strong model weaker than the weak model), quality degrades (L440). There are no provided principled diagnostics or automatic safeguards to prevent harmful pairings or schedules. This leaves automatic model-pair selection an open practical issue of the method.\n\n**(W3)**: Incomplete baselines. There are strong, training-free inference baselines are only discussed but not thoroughly compared in tables: e.g., Auto-Guidance, Z-Sampling variants, TFG, other schedule- or guidance-optimization methods. Many tables compare only “base vs W2SD\". These omissions cloud discussions of relative merit.\n\n**(W4)**: Computational cost. The results in Figure 13 rely on a setting where $T_{w2s}$ is halved and $\\lambda$ is small. There are multiple experiments in the appendix, however, that state $\\lambda = T-1$, which implies the reflection operation (that uses two model passes) is performed at nearly every step according to Algorithm 1, effectively doubling the inference cost. Ablations on the reflection window and schedule are limited.\n\n**(W5)**: Hyperparameter tuning. Based on Figure 11, hyperparameter choices can significantly influence quality. If the gap is negative due to poor choices for LoRA scales, guidance scales, or expert selection, then performance may not be good. This might require extensive tuning, which takes away from the \"training free\" approach of the paper.\n\n**(W5)**: Omission of comparisons with “Reflected Diffusion Models.” The empirical and conceptual separation from related work like reflected diffusion models could be made more explicit.\n\nOverall, the weaknesses outweigh the strengths. The paper would benefit from stronger baselines and a clearer theoretical justification."}, "questions": {"value": "**(Q1)**: The term \"reflective operator\" is not well-justified. The operation $x_t = M_{inv}^{w}({M}^{s}(x_{t},t))$ is a \"denoise-then-invert\" step that applies a correction. The paper doesn't explain what is being \"reflected\" or from what surface/manifold. \n\n**(Q2)**: When does $\\Delta_1 \\approx \\Delta_2$ hold? Under what theoretical settings or formalized assumptions? \n\n**(Q3)**: Are there simple, training-free heuristics to pick the weak and strong models, or the reflection window?\n\n**(Q4)**: Could the authors report actual wall-clock time on a fixed GPU and per-image variance across seeds for several $T$ and $\\lambda$?\n\n**(Q)**: There are many minor typos (e.g., \"applues,\" \"socre,\" \"proivde\"), and sometimes ambiguity around whether $s_{\\theta}$ is a score or an $\\epsilon$-predictor."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tcLgDfVEvR", "forum": "tg19FVh3p1", "replyto": "tg19FVh3p1", "signatures": ["ICLR.cc/2026/Conference/Submission23330/Reviewer_RUsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23330/Reviewer_RUsL"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762413221536, "cdate": 1762413221536, "tmdate": 1762942606988, "mdate": 1762942606988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}