{"id": "NTvlBYvnoM", "number": 6014, "cdate": 1757950662829, "mdate": 1763623659343, "content": {"title": "KEPIL: Knowledge-Enhanced Prompt-Image Learning for Prompt-Robust Disease Detection", "abstract": "Vision–language models (VLMs) show promise for clinical decision support in ra-\ndiology because they enable joint reasoning over radiological images and clinical\ntext, thereby leveraging complementary clinical information. However, radiologi-\ncal findings are long-tailed in practice, leaving some conditions underrepresented\nand making zero-shot inference essential. Yet current CLIP-style medical VLMs\nare sensitive to prompt variations and often lack trustworthy external knowl-\nedge at inference time, which hinders reliable clinical deployment. We present\nKEPIL, a prompt-robust framework that integrates curated medical knowledge\nto stabilize zero-shot generalization. KEPIL comprises: (i) dynamic prompt en-\nrichment using ontologies with LLM assistance, (ii) a semantic-aware contrastive\nloss aligning embeddings of equivalent prompt variants via a dual-embedding ob-\njective, and (iii) entity-centric report standardization to yield ontology-aligned\nrepresentations. Across seven benchmarks, KEPIL achieves state-of-the-art zero-\nshot/finetuning performance in classification and segmentation; under prompt-\nvariation tests, it improves AUC by 6.37% on CheXpert and by 4.11% on average.\nAblations and qualitative analyses validate the contributions of enriched prompts\nand semantic alignment, while attention maps highlight clinically relevant regions.\nThese results show that structured knowledge and robust prompt design are key to\nclinically reliable radiology-facing VLMs. Code will be released at ***.", "tldr": "", "keywords": ["Robustness", "Medical Image Analysis", "Human-AI alignment", "Knowledge Injection", "Vision Language Model."], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f066e2c6ada79cea56e2bf55729789e0b0574f3.pdf", "supplementary_material": "/attachment/6dcb4b31e3c32e9c089362670fbdd1db1348b7b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes KEPIL, a knowledge-enhanced vision-language framework aiming to improve prompt robustness and zero-shot generalization in medical imaging tasks. It integrates curated ontologies (UMLS, Radiopaedia) with LLM-generated descriptions, introduces a semantic-aware contrastive loss (Lsc) to stabilize embeddings across prompt variants, and standardizes radiology reports via entity-centric preprocessing. Experiments on seven public chest X-ray datasets show improved zero-shot and finetuning performance compared to CLIP-style and medical-specific baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a practically relevant issue: prompt sensitivity and lack of knowledge grounding in medical VLMs.\n\n- Combines knowledge curation and contrastive learning in an interpretable manner.\n\n- Provides comprehensive experiments with both quantitative and qualitative analyses.\n\n- Demonstrates cross-modality transfer and prompt perturbation robustness, which are meaningful for clinical deployment."}, "weaknesses": {"value": "- The work is largely incremental and engineering-oriented. Its main components—ontology-guided prompt design, adapter alignment, and cross-attention fusion—closely follow prior work such as KAD, MAVL, and MedKLIP. The proposed “semantic-aware contrastive loss” is a simple consistency objective, and the “Knowledge Query Module” mainly reuses standard cross-attention. Overall, the paper does not introduce substantial novelty or new conceptual understanding of prompt robustness or knowledge integration.\n\n- The paper also relies heavily on ChatGPT-4o for generating and refining prompts. While this provides flexibility, it introduces potential issues with factual accuracy and consistency, as large language models can produce hallucinated or unstable medical text. The paper does not include verification or expert validation of these outputs. Since the generated text directly affects training, it is unclear whether the reported gains stem from true model robustness or from uncontrolled variations in GPT-generated data.\n\n- The experimental analysis lacks statistical rigor. The paper reports no standard deviations, repeated trials, or significance testing. The reported 1–3% performance gains may fall within normal variance. In addition, the prompt robustness experiments mainly test minor typos rather than diverse or semantically rephrased prompts, providing limited evidence of genuine robustness.\n\n- The claims of clinical generalization appear overstated. All experiments are conducted on public datasets rather than real-world or prospective clinical data. The CXR-to-CT transfer is a simplified setting that does not demonstrate true cross-modality adaptation. Without human or expert validation, the claim of “trustworthy clinical deployment” is not sufficiently supported."}, "questions": {"value": "Please refer to the Weaknesses section.\n\n**I am willing to raise my score according to the rebuttal.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VCVgbBBjby", "forum": "NTvlBYvnoM", "replyto": "NTvlBYvnoM", "signatures": ["ICLR.cc/2026/Conference/Submission6014/Reviewer_b7Jv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6014/Reviewer_b7Jv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596021129, "cdate": 1761596021129, "tmdate": 1762918415570, "mdate": 1762918415570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study introduces **KEPIL** (Knowledge-Enhanced Prompt Image Learning), a framework designed to improve VLMs for medical disease detection in radiology. The paper includes 2 main contributions:  **Knowledge-Grounded Prompt Enrichment**, and  **Semantic-Aware Contrastive Loss**. The proposed loss function make the model robust to prompt variations. It uses a dual-embedding objective to align the embedding of equivalent prompt variants, teaching the model that different phrasings of the same concept are related. **KEPIL** achieved SOTA performance in zero-shot setting classification. It demonstrated robustness to prompt variations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The proposed method re-defines prompt sensitivity problem in medical VLMs as a knowledge  alignment problem, aligning ontology-grounded text with visual features to stabilize predictions.\n\n2) Demonstrating robustness to real prompt noise, improving performance on rare/unseen diseases, indicates meaningful impact  in clinical setting.\n\n2) The study demonstrates consistent in zero-shot settings and smaller drops under prompt perturbations then competing baselines."}, "weaknesses": {"value": "1) I have one concern about the improvement from the vision encoder being pretrained on chest X-ray data. The current gains might partially reflect this pretraining rather than the proposed knowledge components.\n\n2) I am very confused because the experiment setting section mentions SIIM-ACR for the task of segmentation, but I could not find relevant report for this dataset in segmentation setting.\n\n3) The study generates prompt variants including rephrasing, typos, omissions, and incorrect punctuation. However, it is not enough for realistic clinical settings such as abbreviations, multilingual terms, or clinician-specific jargon.\n\n4) The study's introduction emphasis on long-tail distribution, but the results are not depicted the results for rare diseases. The results mainly focus on CXR diseases."}, "questions": {"value": "1) Could you provide experimental results to clarify the concern (1) in the **weakness** section.\n\n2) Could you provide experimental results on a rare disease dataset to support the paper claims?\n\n3) Could you provide experimental results to clarify the concern (3) in the **weakness** section.\n\n4) Could you provide additional subsection to explain the results for the segmentation task?\n\n5) How does the Knowledge Query Module (KQM) enhance localization compared to baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "leGjyyTHGA", "forum": "NTvlBYvnoM", "replyto": "NTvlBYvnoM", "signatures": ["ICLR.cc/2026/Conference/Submission6014/Reviewer_1Rj5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6014/Reviewer_1Rj5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601683294, "cdate": 1761601683294, "tmdate": 1762918415133, "mdate": 1762918415133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KEPIL (Knowledge-Enhanced Prompt–Image Learning) to mitigate two challenges of medical-imaging VLMs in zero-shot settings: prompt sensitivity and lack of external domain knowledge. The method has three key components: (i) ontology-constrained prompt expansion and standardization using external medical knowledge bases (e.g., UMLS, Radiopaedia) with LLM assistance; (ii) a semantics-aware contrastive loss that enforces representation consistency across text-side views via a lightweight adapter with dropout; and (iii) entity-centric report normalization using RadGraph to reduce free-text noise. The architecture uses a frozen clinical text encoder (e.g., BioClinicalMPBERT) with a trainable adapter, a ViT-B/16 visual encoder, and a Knowledge Query Module (KQM) for token-level cross-attention alignment between image patches and text tokens. Experiments cover seven chest X-ray benchmarks (classification/segmentation/localization), including seen/unseen/rare categories and cross-modality transfer (CXR→CT). KEPIL outperforms or matches strong baselines and exhibits reduced performance degradation under diverse prompt perturbations; ablations attribute gains primarily to knowledge enrichment and the proposed loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirics across seen/unseen/rare and cross-modality settings; consistent zero-/few-shot gains, including with limited labels for segmentation.\n- Robustness focus is carefully evaluated with multi-source LLM-generated and perturbed prompts; UMAP suggests tighter intra-class clusters.\n- Interpretability: entity-centric text and Radiopaedia cues produce attention maps that align with clinical findings."}, "weaknesses": {"value": "- Train–test gap in semantic alignment: The loss aligns two stochastic views of the same text, not explicit cross-variant positives (paraphrases, synonym mappings). Training with explicit variant pairs would better match the robustness claim.\n- Theory is light: “Semantic-aware” is broad; a perspective via invariance subspaces, information bottleneck, or generalization bounds maybe would strengthen the conceptual grounding."}, "questions": {"value": "- What is the size of the entity set E and its coverage per disease category? How are conflicts between UMLS and Radiopaedia resolved; what fraction is human-audited?\n- Did you train with explicit paraphrase/synonym/noisy variant pairs as positives? If not, can you add such a loss and report gains vs. the current two-view objective? I think by incorporating cross-variant positive pairs during the training phase could further support the claim of being \"variant-robust\". \n- At inference, do you require LLM calls to generate prompts, or rely on a pre-built normalized library? \n- In robustness plots, are max token length and template structure matched across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VYF0L2SfHM", "forum": "NTvlBYvnoM", "replyto": "NTvlBYvnoM", "signatures": ["ICLR.cc/2026/Conference/Submission6014/Reviewer_JrKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6014/Reviewer_JrKC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884808227, "cdate": 1761884808227, "tmdate": 1762918414661, "mdate": 1762918414661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KEPIL, a knowledge-enhanced prompt-image framework designed to address the issues of prompt sensitivity and limited generalization in medical vision-language models. It incorporates a large amount of medical knowledge based on ontologies and leverages dynamic prompt enhancement guided by large language models (LLMs) to improve understanding. In addition, a semantics-aware contrastive loss is proposed to enhance prompt robustness, and entity-centered report standardization is employed to optimize information representation. Experiments on seven benchmark datasets demonstrate that KEPIL achieves state-of-the-art performance in zero-shot classification and segmentation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work is the first to integrate medical ontological knowledge, dynamic prompt enhancement, and semantics-aware contrastive learning to improve prompt robustness in medical vision-language models. With comprehensive experiments across multiple datasets and tasks, it demonstrates clear methodology and achieves significant performance gains, highlighting its practical value for medical AI."}, "weaknesses": {"value": "1.The LLM-based prompt enrichment lacks transparency and rigorous validation against raw knowledge sources, risking unquantified hallucinations.\n\n2.The superior segmentation scores lack qualitative validation (e.g., mask visualizations), leaving the clinical precision of improvements unproven.\n\n3.​​The complex inference-time prompts increase computational overhead, but efficiency (latency) is not benchmarked, hindering practicality assessment."}, "questions": {"value": "1.Provide radiologist-evaluated proof that LLM-enriched prompts are more clinically valuable than raw knowledge-base text.\n\n2.​​Was robustness tested beyond typos (e.g., clinical synonyms like \"opacity\" vs. \"consolidation\")?\n\n​3.​Why was dropout chosen over explicit text augmentation for creating positive pairs in the contrastive loss?\n\n​​4. Is performance on rare diseases due to unique feature learning or merely semantic proximity to common diseases in the knowledge graph?\n\nIf my main concerns are properly addressed, I would be willing to raise my evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EQSvyKf2UW", "forum": "NTvlBYvnoM", "replyto": "NTvlBYvnoM", "signatures": ["ICLR.cc/2026/Conference/Submission6014/Reviewer_Vgdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6014/Reviewer_Vgdq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897954361, "cdate": 1761897954361, "tmdate": 1762918414359, "mdate": 1762918414359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}