{"id": "PFR4E8583W", "number": 23320, "cdate": 1758342071915, "mdate": 1759896821221, "content": {"title": "Where LLM Agents Fail And How They can Learn From Failures", "abstract": "Large Language Models (LLMs) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the Agent Error Taxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct the Agent Error Benchmark, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24\\% higher all-correct accuracy and 17\\% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26\\% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents.", "tldr": "", "keywords": ["LLM", "Agents", "Failure Attribution", "LLM Agents"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37110b083c2751581df2a4ff94cd16087d5bd1df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies error propagation in LLM agents and contributes: (1) AgentErrorTaxonomy, a modular categorization of failure types (memory, reflection, planning, action, plus system); (2) AgentErrorBench, a 200-trajectory, human-annotated failure benchmark drawn from ALFWorld (100), WebShop (50), and GAIA (50), with root-cause labels and step-level tags; and (3) AgentDebug, a framework that identifies the earliest “critical” error and provides targeted feedback for re-rollouts. Reported results show sizeable gains in critical-error localization and up to 26% relative improvements in downstream task success over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses a barrier for deploying agents—cascading failures and root-cause localization, which is increasingly important as tasks get longer and toolchains more complex.\n\n2. It contributes a human-annotated benchmark of 200 failure trajectories from ALFWorld, WebShop, and GAIA, emphasizing root-cause labels and step-level annotations, along with inter-annotator agreement.\n\n3. This work offers informative error analyses (e.g., where errors arise within trajectories and which modules dominate), which can guide future research and system design.\n\n4. This paper shows substantial gains over baselines (e.g., Self-Refine, Best-of-N, ToT) across multiple environments and backbones, including up to 26% relative improvements in downstream success."}, "weaknesses": {"value": "1. This paper relies heavily on a single strong, proprietary adjudicator (GPT-4.1) for critical-error detection, and performance degrades with alternative models, which limits the method’s generality and portability.\n\n2. This paper addresses failure attribution and error taxonomy for LLM agents, which has been actively explored in recent works across multi-agent and tool-agent settings (e.g., MAST). Accordingly, the conceptual novelty of proposing a taxonomy and attributing root causes is limited. The contribution is better characterized as a well-engineered synthesis tailored to single-agent trajectories with a standardized evaluation harness and re-rollout debugging.\n\n3. The AgentErrorBench is limited in scale and diversity. Expanding the benchmark to include more diverse scenarios and agent configurations would significantly enhance its utility and generalizability.\n\n4. This paper frames debugging around identifying a single “minimal set of root-cause failures,” yet the reported inter-annotator agreement is only κ = 0.55. Under widely used interpretations, this only corresponds to moderate agreement, suggesting notable subjectivity in what counts as the true root cause and questioning the robustness of the ground truth. \n\n5. Contradictory Description of the Core Method. This paper describes Stage 2 (“Critical Error Detection”) inconsistently: Section 3.2 states that the method performs counterfactual testing with rollouts by substituting corrected actions and testing whether the rollout would succeed, whereas Algorithm 1 explicitly states “via LLM (no rollout/counterfactuals)”. This contradiction leaves it unclear whether the detector is an expensive environment-level search or a single, zero-rollout LLM adjudication, undermining clarity, reproducibility, and fair compute accounting. Please clarify and align the text with the actual implementation, and report equal-budget comparisons and cost metrics under the chosen design."}, "questions": {"value": "1. Section 3.2 (Stage 2) explicitly states that the method involves counterfactual testing with rollouts: \"we perform counterfactual testing step by step: at each point, we substitute a corrected action and test whether the rollout would succeed\". However, Algorithm 1 and its caption state the opposite: \"/* Stage 2: Critical Error Detection via LLM (no rollout/counterfactuals) */\" . Could the authors please clarify the exact procedure for \"Critical Error Detection\"? Does it involve actual environment rollouts as described in the prose, or is it a zero-rollout, prompt-based inference as stated in the algorithm?\n\n2. If AgentDebug is a zero-rollout method, how can its performance be fairly compared to the \"Brute Force\" baseline? Isn't this comparing the zero-shot reasoning of an LLM against a structured search, rather than a direct comparison of two error detection methods under a similar computational model?\n\n3. Could the authors more explicitly differentiate their taxonomy and debugging framework from prior work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fLNop6YSKM", "forum": "PFR4E8583W", "replyto": "PFR4E8583W", "signatures": ["ICLR.cc/2026/Conference/Submission23320/Reviewer_CMTu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23320/Reviewer_CMTu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760639111453, "cdate": 1760639111453, "tmdate": 1762942603135, "mdate": 1762942603135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work makes two concrete contributions for analyzing and debugging single-agent systems. The first is AgentErrorBench which is constructed from failure trajectories from ALFWorld, GAIA, and WebShop, and is based on the proposed taxonomy of agent failures. The second is AgentDebug, a system that identifies root-cause failures and provides corrective feedback. Experiments on AgentErrorBench demonstrates superior performance of AgentDebug over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In general this work is quite clear in presenting its contributions (one benchmark and one method). Experiments (although restricted to the proposed benchmark) do show notable improvements, with factors controlled to ensure comparison fairness (e.g., token usage)."}, "weaknesses": {"value": "Currently AgentDebug is only evaluated on the proposed AgentErrorBench. It would be much better if other existing benchmarks are also used to assess AgentDebug, for example the Who&When benchmark from \"Zhang et al. 2025\" (referenced in Section 1)."}, "questions": {"value": "1. In line 244 (Algorithm 1), the comment says \"(no rollout/counterfactuals)\" at Stage 2. This seems to contradict with the description in line 273-278 (\"we substitute a corrected action and test whether the rollout would succeed\"). I'd like to see some clarification on this.\n2. Can AgentDebug be applied to existing benchmarks like Who&When?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "62D2TEuhOZ", "forum": "PFR4E8583W", "replyto": "PFR4E8583W", "signatures": ["ICLR.cc/2026/Conference/Submission23320/Reviewer_SuCm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23320/Reviewer_SuCm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526433901, "cdate": 1761526433901, "tmdate": 1762942602899, "mdate": 1762942602899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors devise an error taxonomy for LLM agents and using this taxonomy construct a human-annotated benchmark. They then use the error taxonomy to construct a debugging prompt framework to improve LLM agent performance, finding that it performs favorably compared to other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark appears to consist of very high-quality data; graduate-level researchers annotate each step of the sequences, and the training process for annotators and consistency are documented well.\n2. Analysis of the specific types and modules that errors happen in is interesting and has many potential uses."}, "weaknesses": {"value": "Unrelated to the content, the authors reduce the font size halfway through the paper, which makes it harder to read and means this paper is over the page limit; this needs to be fixed. \n1. The description of the AgentDebug method is somewhat inexact; I feel that making it more explicit when there is a structured prompting framework/calls to a language model in the three-step description would better convey the workflow of this framework. \n2. Further description of the baseline implementations is necessary, and the results are difficult to interpret without this information. For instance, one of the metrics in section 4.1 requires the method to accurately give the taxonomy’s error type; is the information from the taxonomy provided for the baseline methods? The exceptionally low performance is understandable if they are expected to give errors from a taxonomy they have no prior knowledge of. \n3. The analysis of error compounding is not particularly informative. Figure 8 appears to show that there are more errors as the LLM goes along, but does not indicate how the errors are compounding (i.e. these errors may be entirely independent, in which case this does not demonstrate that an early-stage error has any more or less effect than a late-stage error)."}, "questions": {"value": "1. What exactly does Figure 8 show? What is a task? Which model is this?\n2. Have you performed any analysis of efficiency? With three separate steps and a multiple rollouts, I am curious if this method is achieving gains partially due to simply having a higher compute budget than baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N3gqxPO3B2", "forum": "PFR4E8583W", "replyto": "PFR4E8583W", "signatures": ["ICLR.cc/2026/Conference/Submission23320/Reviewer_Njyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23320/Reviewer_Njyp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940285694, "cdate": 1761940285694, "tmdate": 1762942602579, "mdate": 1762942602579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks a simple question: where do LLM agents fail? To answer the question, it proposes a “debugging framework” that seeks to identify the “minimal set” of root-cause failures, and provides corrective feedback. This AgentDebug framework is then used as part of inference (Algorithm 1) to improve robustness, leading to improved accuracy on several benchmarks, including ALFWorld, WebShop, GAIA."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* I found the analysis to be overall quite informative and interesting, with some figures (e.g. Figure 3) offering new insights on agent failures. The taxonomy is also helpful.\n* The presentation is overall quite good, with the main approach (AgentDebug) presented in sufficient detail and via both figures and a detailed inference procedure.\n* The results in Table 1 seem quite compelling; however, see “Weaknesses” for important caveats."}, "weaknesses": {"value": "* Key evaluation details are missing. In particular, it’s unclear how the proposed AgentDebug framework was validated. This makes it seem likely the approach was validated on test data. This needs to be clarified in the response.\n* While there is an explicit discussion of limitations (A.1), this misses some obvious issues, such as the additional runtime cost of applying the proposed approach, which may be prohibitive.\n* The technical contributions are quite minor; the extent to which the proposed framework (which amounts to a prompting scheme) is effective is largely a function of underlying LLMs. This is a systems paper."}, "questions": {"value": "How was model selection performed? How sensitive are the prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fp1O6KlJWm", "forum": "PFR4E8583W", "replyto": "PFR4E8583W", "signatures": ["ICLR.cc/2026/Conference/Submission23320/Reviewer_f5Wd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23320/Reviewer_f5Wd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974043453, "cdate": 1761974043453, "tmdate": 1762942602181, "mdate": 1762942602181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}