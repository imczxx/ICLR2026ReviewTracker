{"id": "wIYWEYLztY", "number": 392, "cdate": 1756737757192, "mdate": 1759898263481, "content": {"title": "Prior-based 4D Human-Scene Reconstruction from Monocular Videos", "abstract": "Accurately capturing dynamic humans as they interact with their 3D environment from a single camera is a pivotal goal for applications spanning from assistive robotics to AR. However, current monocular approaches fall short, as they are typically restricted to reconstructing either the person or the static background in isolation. Methods that capture both often rely on cumbersome multi-view setups, limiting their real-world applicability. To this end, we propose a novel framework that reconstructs the complete 4D human-scene representation from monocular video. We formulate the task as an ill-posed inverse problem and introduce a robust regularization strategy that leverages two complementary priors: a static 3D Gaussian Splatting representation of the scene, and an animatable, SMPL-based 3D Gaussian avatar of the human. Our method jointly optimizes the camera pose, human motion, and the parameters of both priors to faithfully reconstruct time-varying geometry, appearance, and physically plausible human-scene interactions. We validate our approach on a self-collected dataset featuring synchronized human acting videos, human and scene scan videos. Our results demonstrate state-of-the-art performance, achieving average 23 dB PSNR on challenging novel views and surpassing existing monocular baselines.", "tldr": "", "keywords": ["Gaussian Splatting", "4D Human-Scene Reconstruction", "Avatar Reconstruction", "Novel-view Synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/976b773689b752cfb01e8837ed40dcb2811592a7.pdf", "supplementary_material": "/attachment/12ff9fcf2e9af7c0ab707e8220a2370a307506b8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a 4D human-scene reconstruction method that integrates a 3DGS scene prior and a SMPL-based Gaussian avatar prior, and jointly optimizes these priors, achieving realistic rendering results"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Integrates human and scene reconstruction.\n- Provides clear qualitative comparisons and demo videos."}, "weaknesses": {"value": "- Lack of novelty and insufficient contribution. The method mainly combines existing 3DGS-based scene reconstruction and avatar models with only minor optimization refinements. The avatar deformation design appears identical to ExAvatar, offering no clear methodological innovation.\n\n- Unfair comparison. The proposed method uses three videos per subject for reconstruction, whereas ExAvatar operates on a single monocular video.\n\n- Invalid evaluation protocol. As stated in L398, \"All of the frames are used for both training and testing,\" which is not a reasonable practice. The absence of a proper train/test split undermines the validity of the reported metrics.\n\n- Misleading use of the term \"prior\". The \"scene prior\" and \"avatar prior\" are not true learned priors but pre-reconstructed assets obtained from other models or separate videos. This design relies on external pre-fitted results rather than an intrinsic modeling prior.\n\n- Poor organization and unclear notation. Several variables and equations are undefined or poorly explained. For example, in L250, the meanings of $E$ and $P_c$ are not clarified. The regularization loss is vaguely described and lacks a supporting mathematical formulation. Training details are missing, and typos (e.g., inconsistent use of \"Exavatar\" vs. \"ExAvatar\") further affect the paper’s clarity"}, "questions": {"value": "1. What's the difference between the proposed avatar deformation and ExAvatar's? Is this deformation sufficient to handle complex human–scene interactions, especially when obstacles or physical contacts occur?  \n2. Please provide explicit explanations for all notations used.\n3. The paper lacks important implementation details such as learning rates, loss weights, training iterations, and optimization settings. How efficient is your method compared with existing approaches in terms of training time and computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TgBjgiECHZ", "forum": "wIYWEYLztY", "replyto": "wIYWEYLztY", "signatures": ["ICLR.cc/2026/Conference/Submission392/Reviewer_mJFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission392/Reviewer_mJFr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827129401, "cdate": 1761827129401, "tmdate": 1762915510005, "mdate": 1762915510005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for 4D human-scene reconstruction from a single monocular video, aiming to overcome the limitations of current methods that either reconstruct the human or the scene in isolation or require complex multi-view camera setups. The proposed approach jointly reconstructs the dynamic human and the static 3D environment by integrating a static 3D Gaussian Splatting scene prior with an animatable, SMPL-based Gaussian human prior. By simultaneously optimizing for camera pose, human motion, and the parameters of both priors, the system effectively separates dynamic and static elements, achieving realistic human-scene interactions and smooth temporal coherence. The method addresses key challenges such as scale ambiguity between the human and scene, non-rigid clothing deformation, and appearance discrepancies, demonstrating state-of-the-art performance on a self-collected dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method enables joint reconstruction of humans and their surrounding 3D environments from a single monocular video, removing the need for multi-view setups and making real-world deployment more practical.\n- It integrates a static 3D Gaussian Splatting scene prior with an animatable SMPL-based Gaussian human prior, effectively separating static and dynamic components while preserving geometric detail.\n- By jointly optimizing camera poses, human motion, and Gaussian parameters, the system achieves smooth temporal coherence and realistic human–scene interactions."}, "weaknesses": {"value": "- Lack of evaluation on any publicly available benchmark. The method is only validated on a self-collected dataset, making it difficult to assess generalization or compare fairly with existing approaches.\n- The proposed method relies on two separate scanned videos—one for the human and one for the static scene—before performing the joint reconstruction. This assumption severely limits the method’s practicality and reproducibility. In real-world monocular scenarios, obtaining clean and aligned human and scene scans is often infeasible. Moreover, all sequences are self-captured, and the paper provides no information about how these scans are temporally or geometrically synchronized. As a result, the method’s applicability to in-the-wild videos remains unclear, and the evaluation cannot demonstrate generalization beyond the authors’ controlled recording setup.\n-  Using ExAvatar both as a component and as a baseline conflates ablation with external comparison, making the evaluation methodologically unsound."}, "questions": {"value": "- The details of how to benchmark the ExAvatar.\n- How the scene scan is constructed? Does the scene scan contains the human?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gAyXzvSFCc", "forum": "wIYWEYLztY", "replyto": "wIYWEYLztY", "signatures": ["ICLR.cc/2026/Conference/Submission392/Reviewer_7RWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission392/Reviewer_7RWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866519057, "cdate": 1761866519057, "tmdate": 1762915509796, "mdate": 1762915509796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to reconstruct complete 4D scenes involving human-scene interactions from monocular video. Three videos are taken as input: a monocular scanning video of the scene, a monocular scanning video of the human, and an acting video for joint human-scene reconstruction. The scene geometry is recovered via 3DGS optimization with Depth Anything V2 priors. The human avatar is recovered using off-the-shelf Gaussian avatar reconstruction ExAvatar. Then, the human and scene are jointly optimized to minimize a ground-plane contact loss, photometric loss, and normal loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors address a challenging yet important task of jointly reconstructing scenes and 4D humans from monocular video, by combining feed-forward priors (e.g. DAv2, Sapiens normals, HMR) with Gaussian optimization."}, "weaknesses": {"value": "- The paper provides limited quantitative and qualitative results. The main results shown are on a single self-collected dataset, which has limited complexity in scene and human motions (e.g. all three supplementary videos have a mostly static camera with the same action of turning and walking). Compared to existing published work for dense human-scene reconstruction (e.g. https://eth-ait.github.io/ODHSR/), the applicability of the method on truly in-the-wild videos seems limited.\n- Sec. 4.1 claims that the method requires three videos as input (a human scanning video, a scene scanning video, and a joint human-scene acting video). The need for human scanning video and scene scanning video is not assumed by existing work which operates directly on monocular videos (e.g. ODHSR, Shape of Motion), limiting the applicability of the method on unconstrained smartphone videos (as proposed in the introduction).\n- Quantitative and qualitative evaluation on at least one standard dataset (e.g. the SHSD proposed by HSR paper) would strengthen the results of the paper. There are many standard human-scene interaction datasets (EMDB, PROX, NeuMan) and multi-view studio captures permitting novel-view evaluation (Panoptic Studio, DNA-Rendering). Due to the need for human scanning and scene scanning videos as input, the paper claims that these datasets are not informative enough (personally I feel this is a limitation).\n- The method assumes constant foot contact with a ground plane, which is not the case even in simple human-scene interaction activites such as walking."}, "questions": {"value": "- To my understanding, the paper uses three stages of optimization (static scene reconstruction, human reconstruction, and joint optimization). What are the losses used during each optimization stage? Do the photometric loss, normal loss, and contact loss only apply to the joint optimization stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OAnYnD00WZ", "forum": "wIYWEYLztY", "replyto": "wIYWEYLztY", "signatures": ["ICLR.cc/2026/Conference/Submission392/Reviewer_Z7uV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission392/Reviewer_Z7uV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935206131, "cdate": 1761935206131, "tmdate": 1762915509550, "mdate": 1762915509550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to reconstruct both a moving human and the surrounding scene from a single camera video. The authors use two priors. One for the static scene with 3D Gaussian Splatting. One for the human body based on SMPL with 3D Gaussians. They train the system by optimizing camera pose, human motion, and both priors together. They use normal, contact, and photometric losses to make the result more stable. They also collect a small dataset of human and scene videos for testing. The results look good and beat several strong baselines like ExAvatar and HSR. It gives around 23 dB PSNR on novel view rendering. The method works well visually and shows clear improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear motivation and solid method.\n* The use of normal and contact loss helps with geometry.\n* Good results compared to strong baselines.\n* Figures show visible improvement.\n* Ablation studies are complete and easy to understand."}, "weaknesses": {"value": "* The first weakness is the small dataset. The authors only use a few self-captured videos and scans. This makes the evaluation narrow and limits how much we can trust the generalization. It would be better to test on public datasets or unseen environments.\n* I believe the work is missing discussion of runtime and efficiency. There is no information about how long optimization takes or how big the model is. It is important because Gaussian-based methods can be heavy, and readers want to know if this can run in real time or not.\n* Lack of large-scale comparisons is also considerable in my opinion. The paper only compares to a few baselines. It would be more convincing to include more recent dynamic Gaussian or NeRF-based models to show stronger evidence.\n* Another weakness is that the method might not handle diverse motions or multiple people. It is unclear if it can generalize beyond single-person scenes. The framework seems limited to simple interactions.\n* Finally, some parts of the method section are hard to follow, especially the optimization details. Variables appear without clear definition, and equations are sometimes dense. It would help to explain the intuition behind the math more."}, "questions": {"value": "1. How long does it take to optimize one sequence? Can it be used for real-time or interactive applications, or is it mostly offline?\n\n2. How general is the method? Have you tried videos with different people, outfits, or background settings? Does the model still perform well?\n\n3. You use your own dataset. Would the method still work on standard benchmarks such as Human3.6M or ZJU-Mocap?\n\n4. How sensitive is your approach to errors in SMPL fitting? If the initial pose or scale is wrong, does the optimization recover it?\n\n5. What would happen if the priors are noisy or low-quality? Does the system rely heavily on accurate scan priors, or can it still work with rough inputs?\n\n6. Could the framework be extended to handle more than one person or simple object interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TeoWHDeCpj", "forum": "wIYWEYLztY", "replyto": "wIYWEYLztY", "signatures": ["ICLR.cc/2026/Conference/Submission392/Reviewer_o48N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission392/Reviewer_o48N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999657786, "cdate": 1761999657786, "tmdate": 1762915509332, "mdate": 1762915509332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}