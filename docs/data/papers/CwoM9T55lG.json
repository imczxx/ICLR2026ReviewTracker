{"id": "CwoM9T55lG", "number": 18523, "cdate": 1758288714219, "mdate": 1759897098348, "content": {"title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "abstract": "With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient input-prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system’s intelligence cannot be separated from its judgment.", "tldr": "", "keywords": ["alignment", "safety", "cryptography"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/445c012c50a27c6672a84b33e75425ceb66b046b.pdf", "supplementary_material": "/attachment/25e6d80292b31dd42de9fedbd17fa367bb2b9fff.pdf"}, "replies": [{"content": {"summary": {"value": "The paper considers two natural methods of filtering LLM content to prevent the generation of unsafe information—input and output filtering. The core result is that, given the existence of time-lock puzzles, AI safety cannot be achieved through the use of external filters: in other words, white-box model access is required to ensure LLM alignment. The authors show this by first demonstrating that there are LLMs for which adversarial prompts that are cryptographically indistinguishable from benign prompts can be efficiently generated; for such LLMs, no efficient input filter exists. Moreover, they show conversely that there also exist LLMs that can generate outputs that are indistinguishable from benign outputs but which nevertheless have harmful consequences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The connections to cryptographic hardness is interesting.\n- Formalizing the difficulty of input/output filtering is a potentially useful step towards understanding the difficulties facing LLM alignment.\n- It is interesting that their result for output filtering holds for filters that are stronger than the base LLM"}, "weaknesses": {"value": "- Consider shortening the title\n- I’m not sure what scientific value figure 1 provides (perhaps worth keeping it for talks but not for a scientific paper)\n- The structure of the paper is very unusual for an ML conference paper and makes it hard to read. The introduction is very unusually long. It is followed by a shorter technical section and a short reflection in the end. For example, the related work and the background is scattered throughout. It is very hard for me to accept the paper in its current form based on this alone.\n- The setup in section 1.1 is quite dense and hard to read through.\n- Regarding the experiments in section 1.3 are: The theoretical results concern, respectively, the existence of LLMs for which no filter can filter harmful from benign inputs, and the existence of an LLM that can generate harmful outputs that are indistinguishable to any efficient output filter from those of a reference LLM. Why does it matter that these particular filters and particular LLMs can be bypassed? It just means some filters don't work.\n- The theoretical results consider \"worst-case\" LLMs, unclear how this lines up with empirical concerns, especially since their experiment doesn't seem that relevant to the theory."}, "questions": {"value": "- I do not understand the philosophical perspective. What do \"intelligence\" and \"judgement\" mean here, and why do these results prove these notions cannot be separated?\n- Why would one restrict themselves to only input/output filters when a company can filter intermediate embeddings as well?\n- Do the theoretical results extend to intermediate embedding filters as well?\n- Could the authors comment on how is their paper related to “Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches”?\n- How do the experiments in section 1.3 relate to the theoretical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yj3EMSqCqJ", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Reviewer_938F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Reviewer_938F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573250666, "cdate": 1761573250666, "tmdate": 1762928215844, "mdate": 1762928215844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fundamental limits of achieving AI alignment through external filtering mechanisms. The authors focus on two natural points of intervention: filtering the input prompt before it reaches a Large Language Model (LLM) and filtering the output after generation. The core contribution is demonstrating, under standard cryptographic hardness assumptions (specifically the existence of Time-Lock Puzzles and One-Way Functions), that both input-prompt and output filtering can be computationally intractable.\nhese results also extend to more expressive \"mitigation filters\" that can modify prompts or outputs. The authors conclude that an aligned AI system’s intelligence (the LLM internals) cannot be practically separated from its judgment (the external filters), necessitating \"internal\" alignment solutions. Empirical results with real-world filters (Llama Guard, Shield Gemma) are presented to support the theoretical claims."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a highly relevant and fundamental problem in AI safety and alignment, offering strong theoretical grounding for existing empirical challenges like \"jailbreaking.\"\n\nNovelty and Significance: The use of computational complexity theory and cryptographic hardness assumptions (Time-Lock Puzzles, OWFs) to model and prove the limits of alignment is a highly original and significant contribution. This elevates the discussion on alignment barriers from empirical observation to theoretical impossibility under standard assumptions.\n\nStrong Theoretical Claims: The core theorems (Theorem 1 on input filtering and Theorem 2 on output filtering) provide separation results that are robust and compelling. The explicit requirement that the filter be computationally weaker than the LLM in Theorem 1 is a realistic constraint for many practical black-box or proprietary LLM deployments.\n\nComprehensive Scope: The paper doesn't stop at simple detection filters but also considers mitigation filters (Section 1.4) and explores scenarios involving shared secrets or public keys (Section 1.5), offering a well-rounded analysis of potential external defenses.\n\nConnecting Theory to Practice: The inclusion of empirical evidence (Table 1) using state-of-the-art safety filters (Llama Guard, Shield Gemma) adds concrete, real-world relevance to the abstract theoretical findings. This successfully bridges the gap between the constructed, cryptographically-enabled LLMs and the observable failure modes of current models."}, "weaknesses": {"value": "While the theoretical framework is strong, several aspects of the presentation, technical rigor, and scope could be improved for an ICLR audience.\n* Lack of experimental support is the most critical factor. Otherwise, I think a position paper would be more suitable\n* Technical Sketch Insufficient (Section 2): The high-level technical overview of the construction is abstract. Given that the entire proof hinges on the construction of the indistinguishable pair (G,G‘) and the function g, the key challenges mentioned—recovering n from (h,h(n)), the issue of multiple inverses, and using hardcore bits—should be explained with more detail or a more concrete example to demonstrate the Recoverable-Randomness Sampling mechanism. It is difficult to fully assess the proof's validity."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "6Su1OZ3CUd", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Reviewer_xvj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Reviewer_xvj9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580284706, "cdate": 1761580284706, "tmdate": 1762928215369, "mdate": 1762928215369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the computational limits of achieving AI alignment through filtering techniques. The authors show that, under standard cryptographic assumptions, both input filtering (blocking harmful prompts) and output filtering (blocking harmful responses) face fundamental computational barriers. They construct examples of language models where no efficient filter can distinguish adversarial prompts or reliably detect harmful outputs. Even when filters are allowed to modify prompts or outputs instead of rejecting them outright, these more flexible strategies remain computationally constrained. Overall, the paper demonstrates that filter-based alignment methods are theoretically inadequate, emphasizing the need for a deeper understanding of the computational hardness behind AI alignment and for designing more robust regulatory and technical safeguards."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The analytical perspective of the paper is interesting."}, "weaknesses": {"value": "1. The results rely on cryptographic hardness assumptions that, while widely accepted, remain unproven. If these assumptions were invalidated, the impossibility results would no longer hold.\n2. The theoretical LLMs used in proofs involve contrived adversarial mechanism that may not fully reflect real-world systems. The gap between worst-case theoretical constructs and practical AI behavior limits its applicability."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "VLXgkBCuGW", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Reviewer_JDRh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Reviewer_JDRh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783217034, "cdate": 1761783217034, "tmdate": 1762963293872, "mdate": 1762963293872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the problem of using external filters (either on prompts or on outputs) to prevent harmful behaviour from LLMs. They show, under standard cryptographic hardness assumptions, that there exist models for which no efficient prompt‐filter can reliably distinguish safe from adversarial prompts (i.e., adversarial prompts that trigger harmful behaviour are computationally indistinguishable from benign prompts). They also show that, in a natural setting, output filtering is computationally intractable: even if you observe the output, deciding whether it’s harmful or not cannot in general be efficiently done for certain models. They further explore relaxed mitigation strategies (weaker filter models) and demonstrate additional computational barriers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "A definitely novel investigation on the limitation of the (light-weighted) filter methods, based on the cryptographic argument.\n\nIt assesses the worst-case limitation of the filter based methods."}, "weaknesses": {"value": "Practical relevance of Theorems 1 and 2 is limited, due to the assumption of the efficiency of the filter. It is not clear how much the filter needs to be powerful not to be considered as an “efficient filter.”\n\nThe claim of Theorem 2 is too informal to draw a meaningful message from it. Does the statement outputs of M′ are judged as harmful by H′” mean the outputs of M’ given arbitrary inputs are judged as harmful? Does the statement “​​no efficient output filter can distinguish the outputs generated by M′ from outputs of M.” mean one can not distinguish between the output of these two models for any input prompt?\n\nThe authors claim “The filter should be more efficient than the LLM; otherwise, we can ignore the given LLM and focus solely on training a new one from scratch, using it in place of the filter.” However, this may not always be acceptable. Because the safety guarantee is very important, it is natural to have more powerful filter models that are specialized for safety filtering at the cost of usefulness (such as guardian models)."}, "questions": {"value": "Please answer the above comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "4EnsDlEtSX", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Reviewer_Dtav"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Reviewer_Dtav"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961718393, "cdate": 1761961718393, "tmdate": 1762928214521, "mdate": 1762928214521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Practical relevance"}, "comment": {"value": "Several reviewers asked whether our result is relevant to practice.\n\nRecall that we *proved* (under cryptographic assumptions) the impossibility of alignment by filters whose runtime is significantly faster than the LLM, by demonstrating a cryptographic method to encode/hide harmful prompts that can be decoded and executed by LLMs but is impossible to decode (and even distinguish from benign prompt) by fast filters.\n\nTo show the significance of our attack, we ran a series of experiments. In the first experiment of the paper (Table 1), we use natural language jailbreak prompts $y$, which try to elicit a harmful response. To tailor our experiments more to our theoretical results, we performed additional experiments using a timed-release mechanism inspired by our construction: we create an attack $z$ which first tasks the model to decode $E(y)$ using a cipher $C$, where $E(y)$ is replaced by an encryption of $y$ using cipher $C$, where \n$C \\in \\{\\mathrm{Caesar}, \\mathrm{Substitution}, \\mathrm{Morse}, \\ldots\\}$. After decoding, the model is asked to execute the result. The process of encrypting $y$ and having this timed-release mechanism is an analog of locking a message with a time-lock puzzle from our construction.\n\nIn total, we ran three types of experiments: \n- Existing experiment in paper: Feed y to an input filter and if it passes, let an unaligned LLM answer the harmful prompt. Then feed the harmful answer to the output filter. We demonstrate that existing filters (Llama Guard 2 8B, ShieldGemma 2B) fail to detect that prompt y is dangerous, and that they are also unable to detect harmful outputs generated by unaligned LLMs (Llama 3.2 3B, Qwen 7B, Mistral 24B). \n- Additional rebuttal experiment: Feed the timed-release attack $z$ to an input filter (Llama Guard 2 8B, ShieldGemma 2B). Table A below shows that this timed-release mechanism achieves higher filter bypassing rates than directly feeding y to the input filters. \n- Additional rebuttal experiment: Feed the timed-release attack $z$ to a supposedly aligned LLM that handles alignment internally. We show that in some cases, Google Gemini 2.5 Flash refuses to answer on $y$ but provides a harmful response for its encoding $z$, demonstrating that the timed-release attack can improve on existing jailbreaks. \n\nThe additional experiments b) and c) will be added to the updated version of the paper. \n\nThe experiments we run in b) and c) used a mechanism similar to that of a follow-up work [1] (the link leads to a paper that references our work in an anonymized way).\n[1] designs an attack that successfully jailbreaks additional production models, e.g., Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). This attack is inspired, as explicitly said in [1], by our time-lock idea to hide harmful prompts $y$ under some amount of computation. It shows that the alignment mechanisms embedded inside production models are not able to detect harmful commands hidden with a time-lock-like mechanism, but the models can recover the commands and produce harmful outputs. Importantly, the models given harmful prompts $y$ in the clear (without the time-lock component) refuse to answer.  \n\n\nTable A:\n# Input filter bypass rates by guard and encoding type\n\n| Guard | No encryption | ASCII | Caesar | Morse | Substitution | Unicode |\n|-------|---------------|-------|--------|-------|--------------|---------|\n| Llama Guard 2 8B | 34.37% | 63.16% | 87.00% | 79.74% | 79.84% | 78.40% |\n| ShieldGemma 2B | 79.49% | 78.40% | 91.89% | 87.15% | 91.98% | 87.97% |\n\n[1] https://github.com/iclr18523/ControlledRelease"}}, "id": "LQRkEQAUkH", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729635041, "cdate": 1763729635041, "tmdate": 1763729635041, "mdate": 1763729635041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Practical relevance"}, "comment": {"value": "Several reviewers asked whether our result is relevant to practice.\n\nRecall that we *proved* (under cryptographic assumptions) the impossibility of alignment by filters whose runtime is significantly faster than the LLM, by demonstrating a cryptographic method to encode/hide harmful prompts that can be decoded and executed by LLMs but is impossible to decode (and even distinguish from a benign prompt) by fast filters.\n\nTo show the significance of our attack, we ran a series of experiments. In the first experiment of the paper (Table 1), we use natural language jailbreak prompts y, which try to elicit a harmful response. To tailor our experiments more to our theoretical results, we performed additional experiments using a timed-release mechanism inspired by our construction: we create an attack z which first tasks the model to decode E(y) using a cipher C, where E(y) is replaced by an encryption of y using cipher C, where \nC \\in {Caesar, Substitution, Morse, ...}. After decoding, the model is asked to execute the result. The process of encrypting y and having this timed-release mechanism is an analog of locking a message with a time-lock puzzle from our construction.\n\nIn total, we ran three types of experiments: \n- Existing experiment in paper: Feed y to an input filter and if it passes, let an unaligned LLM answer the harmful prompt. Then feed the harmful answer to the output filter. We demonstrate that existing filters (Llama Guard 2 8B, ShieldGemma 2B) fail to detect that prompt y is dangerous, and that they are also unable to detect harmful outputs generated by unaligned LLMs (Llama 3.2 3B, Qwen 7B, Mistral 24B). \n- Additional rebuttal experiment: Feed the timed-release attack z to an input filter (Llama Guard 2 8B, ShieldGemma 2B). Table A below shows that this timed-release mechanism achieves higher filter bypassing rates than directly feeding y to the input filters. \n- Additional rebuttal experiment: Feed the timed-release attack z to a supposedly aligned LLM that handles alignment internally. We show that in some cases, Google Gemini 2.5 Flash refuses to answer on y but provides a harmful response for its encoding z, demonstrating that the timed-release attack can improve on existing jailbreaks. \n\nThe additional experiments b) and c) will be added to the updated version of the paper. \n\nThe experiments we run in b) and c) used a mechanism similar to that of a follow-up work [1] (the link leads to a paper that references our work in an anonymized way).\n[1] designs an attack that successfully jailbreaks additional production models, e.g., Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). This attack is inspired, as explicitly said in [1], by our time-lock idea to hide harmful prompts y under some amount of computation. It shows that the alignment mechanisms embedded inside production models are not able to detect harmful commands hidden with a time-lock-like mechanism, but the models can recover the commands and produce harmful outputs. Importantly, the models given harmful prompts y in the clear (without the time-lock component) refuse to answer.  \n\n\nTable A:\n# Input filter bypass rates by guard and encoding type\n\n| Guard | No encryption | ASCII | Caesar | Morse | Substitution | Unicode |\n|-------|---------------|-------|--------|-------|--------------|---------|\n| Llama Guard 2 8B | 34.37% | 63.16% | 87.00% | 79.74% | 79.84% | 78.40% |\n| ShieldGemma 2B | 79.49% | 78.40% | 91.89% | 87.15% | 91.98% | 87.97% |\n\n[1] https://github.com/iclr18523/ControlledRelease"}}, "id": "LQRkEQAUkH", "forum": "CwoM9T55lG", "replyto": "CwoM9T55lG", "signatures": ["ICLR.cc/2026/Conference/Submission18523/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18523/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission18523/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729635041, "cdate": 1763729635041, "tmdate": 1763753572406, "mdate": 1763753572406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}