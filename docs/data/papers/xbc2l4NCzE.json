{"id": "xbc2l4NCzE", "number": 20919, "cdate": 1758311673949, "mdate": 1759896952184, "content": {"title": "Understanding the Design Space and Cross-Modality Transfer for Vision-Language Models", "abstract": "The training of multimodal models involves many design choices, such as the underlying modality-specific tokenizers, fusion mechanisms, and strategies for freezing model layers during different training stages. However, the individual impact of these decisions on downstream multimodal performance remains poorly understood due to the diversity of current practices. In this paper, we systematically investigate how choices in image tokenization, architectural design, and layer-freezing strategies affect the training and cross-modal generalization of vision-language models (VLMs). We train and evaluate over 50 VLM variants across a controlled suite of tokenizers, model architectures, and training recipes. Our experiments reveal several key trends: (1) image tokenizers designed with text alignment in mind, together with training recipes that further enhance image-text alignment, yield the best performance; (2) unfreezing the language model boosts in-domain results but can degrade out-of-domain generalization; and (3) fusion architectures based on the mixture-of-transformers architecture are effective, especially when language parameters are frozen. To further probe cross-modality transfer, we introduce three new synthetic datasets, which we use to evaluate our pretrained models.", "tldr": "", "keywords": ["multimodal", "mixture-of-transformers", "vision-language models", "fusion", "reinforcement learning", "fine-tuning", "transfer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b85e9f7ab8562df3ae7d02a5b6063cd34b2b55cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores what design choices have a large impact on VLMs' VQA and reasoning performance. \nThis paper examines image tokenizers, fusion architectures, and layer-freezing strategies for the design choices. \nThe authors train 50+ model variants on a Qwen3-0.6B backbone and evaluate on standard benchmarks plus three new synthetic datasets designed to probe cross-modality transfer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an interesting question of what steps create good VLMs.\n\nThe controlled experimental design effectively isolates individual design choices (tokenizer, architecture, freezing strategy) that are typically confounded in VLM research.\n\nProvides actionable insights for practitioners (e.g., Takeaways 1-8)."}, "weaknesses": {"value": "Limited model size. The authors only use a single sub-1B model. This begs the question of whether the results are still valid when scaling up the models. While I understand the authors may be limited in their available compute, the question they seek to answer requires more computational power. Hence, the authors should instead use their compute to explore questions within their realm. \n\nOnly single runs. No tests of significance.\n\nUnfair Architectural Comparisons. MoT has 400M+ extra parameters vs Joint-Decoder despite \"comparable FLOPs.\" Table 2 does not normalize for parameter count—MoT's advantage when all frozen may simply be due to more capacity. You should add parameter-matched baselines or explicitly report params/FLOPs for each config.\n\nWhile the authors explore what setup gives the best performance, they do not tackle the why?\n\nThere is also no error analysis.\n\nThere is no discussion of risks regarding data contamination. There are datasets from 2014, so models like SigLIP 2 have likely seen samples from it.\n\n“Our results show that the Mixture-of-Transformers (MoT) architecture is particularly effective.” Why do you state this? On what grounds? See under Questions for more details.\n\nThe reasoning performance metrics should include what random guessing would yield.\n\nThe table captions need to be above the tables as per the formatting instructions: “The table number\nand title always appear before the table.”\nThe table captions are also incomplete and missing, for instance, what the bold numbers represent and what can be drawn from the table.\nTable formatting is inconsistent. The style of Tables 1 and 2 is very different from that of Tables 3 to 6.\n\nThere are some odd hyperlinks, such as SigLIP 2, on lines 217 and 219. Also, why are the references to Sections, Tables, and Figures in red?\n\nSection 2 is missing a subsection on evaluation metrics.\n\nGiven the datasets you generate, you should consider (and cite!) https://arxiv.org/abs/2407.06581. Also, you should include the full prompts given to the models.\n\nThe findings, while useful, are largely confirmatory (text supervision helps, unfreezing trades off in/out-domain performance) rather than surprising. The cross-modality transfer analysis, though interesting, is hampered by methodological constraints.\n\nTypos etc.:\nLine 115: “which contains of 28 transformer layers” should be “which contains 28 transformer layers”\nLine 236: +5.3 is missing colour."}, "questions": {"value": "How do you conclude: “MoT with an unfrozen image tokenizer and frozen language layers delivers the best overall task performance?” \nThe joint decoder with everything unfrozen delivers 1.9pp higher in distribution performance with 3.2 pp lower out of distribution performance. However, depending on how you weigh the two metrics, the conclusion changes. Furthermore, if I looked at improvement over the baseline frozen model, then the conclusion changes yet again.\nPlease create a single table extension of Table 2 where you do not aggregate on the tokenizers. (It should include all tokenizers and not just the subset you used for Table 2).\n\nFor the start of Section 4, why do you create your own datasets rather than using existing ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmlncaI4rq", "forum": "xbc2l4NCzE", "replyto": "xbc2l4NCzE", "signatures": ["ICLR.cc/2026/Conference/Submission20919/Reviewer_tpUk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20919/Reviewer_tpUk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212578085, "cdate": 1761212578085, "tmdate": 1762999978405, "mdate": 1762999978405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper broadly evaluates existing vision-language model approaches, covering image tokenizers, model architecture, and frozen parameter settings.\nEach method is evaluated on several vision question answering datasets in terms of several perspectives including in-domain/out-of-domain settings and cross-modalirty transfer from image/text to text/image.\nThrough experimental evaluations, this paper provides some observations, e.g., the best approach for image tokenizers is to learn with text-alignment objectives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Readers easily grasp the claims of this paper.\nThe experimental evaluation setup is clearly organized. In addition, the conclusion of each experiment is well summarized in takeaways.\n\n- Broadly and fairly evaluating existing technologies provides useful information for practitioners.\nThis paper provides a comprehensive evaluation of key aspects in handling vision language models, including continuous/discrete image tokenizers and frozen/unfrozen approaches,\nfrom several perspectives including in-domain and out-of-domain scenarios.\n\n- Model architecture and image tokenizers are crucial elements in vision language models. \nIf the paper had provided a clearer and more promising research direction, it could have had a significant impact.\nHowever, as noted in Weakness, the claims are not sufficiently supported by the experimental evidence."}, "weaknesses": {"value": "- In the evaluation, the language backbone consists solely of one model, Qwen3-0.6B. This might cause some bias in experiments although I do not know the impact of language backbone.\n\n- The experimental evaluation mainly serves as a benchmark comparison across existing techniques and concludes without deeper evaluation based on hypotheses or analytical questions.\nSince no experiments were designed to substantiate the observations, the takeaways might not go beyond observations.\n Of course, benchmark evaluations are important, but the paper becomes stronger by presenting analytical experiments that go beyond observations,\n identifying open questions that should be addressed as a field, and outlining future directions for vision language models.\n\n- Regarding the above issue, I'm not sure if the conclusion in the takeaways is correct. \nTakeaway 1 claims \"Image tokenizers trained with text-alignment objectives are crucial for strong VLM performance.\".\nIts basis is that image tokenizers trained with text-alignment objectives (AIMv2, SigLIP 2, CLIP) outperform those trained for image reconstruction (TiTok, VAR).\nHowever, in Section 2.2, the distinction between AIMv2, SigLIP 2, CLIP and TiTok, VAR is whether they are continuous tokenizers or discrete tokenizers.\nThus, it is also possible to conclude that continuous tokenizers outperform discrete tokenizers.\nTakeaway 5 makes a similar claim regarding cross-modality transfer. However, since Table 3 and Table 1 show similar trends, it is also possible that TiTok and VAR simply have lower performance."}, "questions": {"value": "If there are any misunderstandings on my part in Weakness, could you point them out?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oTNUUVwQpf", "forum": "xbc2l4NCzE", "replyto": "xbc2l4NCzE", "signatures": ["ICLR.cc/2026/Conference/Submission20919/Reviewer_HxG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20919/Reviewer_HxG6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809908373, "cdate": 1761809908373, "tmdate": 1762999978723, "mdate": 1762999978723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic empirical study of the design space for vision-language models (VLMs). The authors investigate three key dimensions: image tokenization, architectural fusion mechanisms, and layer-freezing strategies. By training and evaluating over 50 VLM variants built on a Qwen3-0.6B backbone, the paper derives some takeaways about what design choices lead to better performance on in-domain and out-of-domain tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is exceptionally well-written and organized.\n\nThe experimental methodology is comprehensive."}, "weaknesses": {"value": "1. All experiments are conducted on a 0.6B parameter LLM backbone. The paper's takeaways are presented as general principles for VLM design, but without validation on larger, more capable models, they may simply be artifacts of a low-capacity regime.\n\n2. Several of the main takeaways are well-known, such as takeaway 1 & 2 & 3.\n\n3. I am not sure how to get a certain conclusion of this paper, it seems more like to be a blog & survey."}, "questions": {"value": "I'm curious what the author's highest priority takeaway is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GNPuoepse3", "forum": "xbc2l4NCzE", "replyto": "xbc2l4NCzE", "signatures": ["ICLR.cc/2026/Conference/Submission20919/Reviewer_Y52k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20919/Reviewer_Y52k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913187304, "cdate": 1761913187304, "tmdate": 1762938737767, "mdate": 1762938737767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a detailed survey of VLM design axes, focusing on image tokenizers, modality fusion architectures, and LLM backbone freezing strategies by altering many different configurations based on a single model (Qwen3-0.6B). Through a consistent three-stage training pipeline (pretraining, finetuning, and cross-modality reasoning transfer), the authors evaluate performance on both in-domain and out-of-domain benchmarks. They report several interesting findings regarding the impact of image tokenizer design, the effect of thawing the language backbone, and comparisons among modality fusion strategies. These results provide actionable insights, although their exclusive use of a very small sized model leaves some room for doubt about the generalization of the findings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper conducts a comprehensive and well-structured examination of previously underexplored VLM design axes (such as image tokenizers, fusion architectures, and layer-freezing strategies) within a consistent framework.\n\n- Their findings yield several actionable insights, which benefits future model development, e.g., the benefit of training image tokenizers with text-alignment objectives, the importance of selecting unfreezing strategies based on target tasks, and the effectiveness of the Mixture-of-Transformers (MoT) architecture.\n\n- In addition, the analysis of cross-modality transfer offers a valuable perspective that goes beyond conventional, often superficial, VQA style benchmarking."}, "weaknesses": {"value": "- All experiments are conducted on a very small 0.6B-parameter model (Qwen3-0.6B), which limits the overall impact of the work. A study of this depth and architectural scope would ideally include experiments on larger models. If training all variants is not feasible, testing a few representative configurations with larger models would help validate the trends observed with the 0.6B model.\n\n- While the paper mentions training \"over 50 variants\", these are essentially factorial combinations of a few design axes rather than truly distinct model architectures. The framing could better reflect that this is rather an extensive ablation study rather than a broad model comparison.\n\n- Over the years, I have seen several survey papers on the design space of multimodal LLMs (some of these papers are listed below), some of which might already have covered similar architectural and training aspects. These works would be valuable reference points, but the current related work section does not discuss them.\n\n- (Minor) The presentation could be improved. Some results are reported in aggregated form without a clear enumeration of all configurations. For example, the claim of “50+ variants” could be shown more explicitly through a summary table or schematic.\n\nLong, Siqu, et al. \"Vision-and-language pretrained models: A survey.\" arXiv preprint arXiv:2204.07356 (2022).\n\nDu, Yifan, et al. \"A survey of vision-language pre-trained models.\" arXiv preprint arXiv:2202.10936 (2022).\n\nYin, Shukang, et al. \"A survey on multimodal large language models.\" National Science Review 11.12 (2024): nwae403.\n\nZhang, Duzhen, et al. \"Mm-llms: Recent advances in multimodal large language models.\" arXiv preprint arXiv:2401.13601 (2024).\n\nMa, Xiaorui, Haoran Xie, and S. Joe Qin. \"Efficiently Integrate Large Language Models with visual perception: A survey from the training paradigm perspective.\" Information Fusion (2025): 103419."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQVnSLdi6V", "forum": "xbc2l4NCzE", "replyto": "xbc2l4NCzE", "signatures": ["ICLR.cc/2026/Conference/Submission20919/Reviewer_XpAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20919/Reviewer_XpAR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20919/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978546160, "cdate": 1761978546160, "tmdate": 1762938664260, "mdate": 1762938664260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}