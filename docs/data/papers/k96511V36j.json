{"id": "k96511V36j", "number": 3689, "cdate": 1757497165403, "mdate": 1759898074908, "content": {"title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce **A**daptive **M**ulti-Guidance **P**olicy **O**ptimization (**AMPO**), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This \"guidance-on-demand\" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a **4.3%** improvement on mathematical reasoning tasks and **12.2%** on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (*e.g.*, DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. *Our code is available at [https://anonymous.4open.science/r/7fBQd46C](https://anonymous.4open.science/r/7fBQd46C), which will be made public after double-blind review*.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Reasoning", "Exploration", "Multi-teacher Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e61b3046dd53caea57052bb26268ce5758452d33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "the paper introduces a new method that guides the learner dynamically, only when the learner cannot produce any correct traces. It also selectively learns what trace is best for the learner."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- relevant topic, writing is mostly clear"}, "weaknesses": {"value": "- the method has overall a lack of novelty, it seems to be combining findings existing works, also lacks motivation, justification and theoretical analysis\n- requires the teacher log probabilities to calculate the importance sampling term, might be too restrictive for certain scenarios\n- the method needs several teacher models, but was only compared to GRPO as the baseline for llama-8b and qwen 1.5b, which is unfair.\n\n[1] Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu,\nQingfu Zhang, and Dandan Tu. Ghpo: Adaptive guidance for stable and efficient llm reinforcement learning.\n[2]  Rlpr: Extrapolating rlvr to general domains without verifiers"}, "questions": {"value": "- what is the justification of using the shaping function, f(x)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F4J9E7R7Yv", "forum": "k96511V36j", "replyto": "k96511V36j", "signatures": ["ICLR.cc/2026/Conference/Submission3689/Reviewer_y1Nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3689/Reviewer_y1Nj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686375883, "cdate": 1760686375883, "tmdate": 1762916923783, "mdate": 1762916923783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AMPO (Adaptive Multi-Guidance Policy Optimization), a mixed-policy RL framework that leverages multiple teacher models to enhance reasoning capabilities in LLMs. The core contributions are: (1) an adaptive replacement mechanism that provides external guidance only when the on-policy model completely fails, and (2) a comprehension-based selection strategy that guides the student to learn from reasoning paths it is most likely to understand. Experiments on mathematical reasoning benchmarks show AMPO outperforms GRPO by 4.3% on in-distribution tasks and 12.2% on out-of-distribution tasks, while achieving comparable performance to single-teacher methods using significantly less data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies limitations of single-teacher approaches in mixed-policy RL and provides convincing motivation for multi-teacher strategies.\n\n2. The combination of adaptive triggering (guidance-on-demand) and comprehension-based selection is intuitive.\n\n3. The 12.2% improvement on OOD tasks and efficient use of 8.5k vs 46k samples demonstrate practical value. The Pass@k and entropy analyses convincingly show enhanced exploration."}, "weaknesses": {"value": "1. While empirically successful, the paper lacks theoretical analysis of why multiple peer-sized teachers should outperform a single stronger teacher. What are the theoretical conditions under which diversity trumps capability?\n\n2. Experiments focus heavily on mathematical reasoning. Generalization to other domains (code, creative writing, general reasoning) remains unclear. The paper would benefit from at least one non-math domain.\n\n3. I have some scalability concerns: Does the approach work with 10+ teachers? What about continually adding new teachers? How to select the optimal k0 automatically?\n\n4. Missing comparisons: No comparison with other recent multi-teacher or ensemble RL methods. The baseline is primarily GRPO and one single-teacher method (LUFFY)."}, "questions": {"value": "1. Can you provide any theoretical analysis (e.g., regret bounds, convergence guarantees) showing when multi-teacher diversity is provably beneficial?\n\n2. How does AMPO (4×7B teachers) compare against using DeepSeek-R1 or GPT-4 as a single teacher with the same 8.5k budget?\n\n3. Have you considered learned selection mechanisms (e.g., using a meta-learner to choose teachers) instead of comprehension-based scoring?\n\n4. When does AMPO underperform? Are there cases where single-teacher guidance is actually better?\n\n5. Can you quantify teacher diversity (e.g., solution agreement rate, trajectory distance) and correlate it with performance gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AttFgCig10", "forum": "k96511V36j", "replyto": "k96511V36j", "signatures": ["ICLR.cc/2026/Conference/Submission3689/Reviewer_dXdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3689/Reviewer_dXdg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408981653, "cdate": 1761408981653, "tmdate": 1762916923395, "mdate": 1762916923395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Multi-Guidance Policy Optimization (AMPO), a new reinforcement-learning framework designed to enhance the reasoning capability of large language models (LLMs) through multi-teacher guidance. Existing Reinforcement Learning with Verifiable Rewards (RLVR) methods—such as Group Relative Policy Optimization (GRPO)—often rely on a single off-policy teacher or pure self-exploration, limiting the diversity of reasoning paths explored. AMPO overcomes this limitation by (1) selectively invoking multiple teachers only when the model fails (the adaptive replacement mechanism), and (2) ranking external guidance using a comprehension-based selection metric derived from the student model’s likelihood of reproducing the teacher’s reasoning. The approach demonstrates consistent improvements—4.3% on in-distribution and 12.2% on out-of-distribution benchmarks—while maintaining training stability and higher reasoning diversity"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a well-motivated and clearly structured framework that generalizes the idea of mixed-policy RL with an elegant “guidance-on-demand” paradigm.\n\nAMPO improves both in-distribution (math reasoning) and out-of-distribution generalization, outperforming GRPO and matching or surpassing LUFFY with 5× less data.\n\nRobust performance across multiple model sizes (1.5B, 7B, 8B) and architectures (Qwen, LLaMA) underscores scalability."}, "weaknesses": {"value": "While AMPO is compared against LUFFY and GRPO, there is no direct comparison with multi-teacher SFT frameworks (e.g., TWT 2025 (https://arxiv.org/abs/2503.24198) or multi-distillation baselines). This makes it difficult to isolate the incremental benefits of AMPO’s adaptive mechanism over simple multi-teacher data aggregation.\n\nThe experiments show that mixing LongCoT and ShortCoT teachers affects both accuracy and reasoning length. However, the criteria for teacher selection and diversity quantification remain heuristic. A deeper ablation or diversity metric (e.g., representation distance, reasoning style variance) would strengthen the argument."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ecNBg3JGSp", "forum": "k96511V36j", "replyto": "k96511V36j", "signatures": ["ICLR.cc/2026/Conference/Submission3689/Reviewer_Lxdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3689/Reviewer_Lxdg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876070320, "cdate": 1761876070320, "tmdate": 1762916923087, "mdate": 1762916923087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm, Adaptive Multi-Guidance Policy Optimization (AMPO), which enhances reasoning in LLMs by leveraging multiple teacher models when the generated results are incorrect. A comprehension-based guidance selection further filters off-policy traces so that the student agent learns from reasoning paths it can better understand. The authors validate the method’s efficiency through extensive experiments and ablation studies. Results show that AMPO achieves consistent gains, particularly on out-of-distribution benchmarks, while maintaining higher entropy during training, indicating stronger exploratory behavior."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method consistently improves over GRPO, especially on OOD tasks, demonstrating stronger generalization capability. AMPO achieves competitive or superior results using multiple weaker teachers, compared to single stronger teachers or self-exploration, showing improved data efficiency. The paper includes detailed ablation studies on various components of AMPO, providing clear intuition on how adaptive replacement and comprehension-based selection jointly maintain exploration and learning efficiency."}, "weaknesses": {"value": "1. Constructing the 8.5k multi-teacher verified dataset appears computationally intensive. More detail on the associated cost or resources would help contextualize the efficiency and scalability claims.\n2. Baseline Inconsistency:\n For Qwen 2.5-1.5B and LLaMA 3.2-8B, only GRPO is used as a baseline, while the 7B model compares against SFT, SFT + GRPO, and LUFFY.  Including at least SFT or SFT + GRPO for the smaller models would give a fairer picture. Additionally, the 0 % GPQA-diamond result for LLaMA 3.2 8B seems unexpectedly low and should be re-checked.\n3. The same symbol \\hat{A} is used for both on-policy and off-policy adavantages, though they are computed over different sample sets. It would help to introduce explicit subscripts (e.g., \\hat{A}^{on}, \\hat{A}^{off}) to avoid ambiguity. \n4. Minor issue:\n   1. The numerical result reported around Line 341 does not match Table 2."}, "questions": {"value": "1. The ablation shows that larger $k_0$ leads to higher accuracy but also longer responses. Could the authors provide practical guidance or heuristics for selecting $k_0$, for example, balancing training variance against computational budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2wdsVGRiU", "forum": "k96511V36j", "replyto": "k96511V36j", "signatures": ["ICLR.cc/2026/Conference/Submission3689/Reviewer_kg4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3689/Reviewer_kg4R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965646391, "cdate": 1761965646391, "tmdate": 1762916921757, "mdate": 1762916921757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}