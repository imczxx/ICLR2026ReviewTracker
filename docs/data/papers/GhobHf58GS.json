{"id": "GhobHf58GS", "number": 8681, "cdate": 1758094887679, "mdate": 1759897769872, "content": {"title": "Task-Specific Adaptation with Restricted Model Access", "abstract": "Modern foundation models achieve state-of-the-art performance across diverse modalities, yet commonly require modification of internal weights or insertion of new layers during fine-tuning. Such modifications increase deployment complexity, hinder optimization for edge devices, and risk exposure of proprietary model parameters. In this paper we analyze for the first time existing fine-tuning paradigms in the context of these three axes. Within this context we introduce ``Gray-box'' fine-tuning: a lightweight and deployment-friendly framework that adapts frozen backbones without altering their architecture or internal parameters. Gray-box fine-tuning enables adaptation solely via compact, external input/output adapters trained with controlled gradient signals at predefined model entry points, preserving all internal components unchanged. We introduce two variants: DarkGray-Box Adaptation (DGA), restricting modifications strictly to input and output interfaces, and LightGray-Box Adaptation (LGA), allowing limited injection of learnable tokens at intermediate layers for enhanced adaptability. Extensive evaluations across tasks including text-to-image retrieval, video retrieval, image classification, sketch retrieval, and diffusion-based generation demonstrate that Gray-box methods achieve competitive performance relative to standard fine-tuning, despite significantly stricter constraints. By decoupling task-specific adaptation from internal model modifications, Gray-box fine-tuning provides an efficient, scalable, and secure alternative to conventional fine-tuning methods.", "tldr": "We introduce Gray-box fine-tuning, to adapt foundation models with lightweight input/output adapters that use only gradient access, never touching backbone weights or layers.", "keywords": ["Gray-box fine-tuning", "Foundation models", "Vision–language retrieval"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3eb47a214935e17a9f3906f7a1c759b637b12fd3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper formalizes a gray-box fine-tuning regime for adapting frozen backbones under strict access constraints. Two variants are proposed: DGA (input/output adapters only) and LGA (a few learnable tokens injected at selected layers without editing backbone weights). The pitch is deployment/IP friendliness, where one sealed backbone is reused across tasks via small per-task attachments, while remaining competitive (though not universally SOTA) on text–image/video retrieval and classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Deployment relevance: Clear, enforceable access model (sealed backbone; adapter/token attachments) aligns with multi-tenant serving and IP concerns.\n(2) Crisp constraint definition: “No weight edits; preserve computational flow” makes the scope precise and implementable.\n(3) Competitive under constraints: Meaningful gains over zero-shot and linear probes; often close to LoRA/last-layer FT despite stricter access limits. \n(4) Backbone diversity: Results include both ViT and CNN (e.g., CLIP RN101), supporting generality.\n(5) Ablations & design rationale: Decomposes input vs. output adapters and token count/placement; clarifies which components matter.\n(6) Transparency about trade-offs: Acknowledges cases where LoRA/FT win."}, "weaknesses": {"value": "(1) Missing closest SOTA baselines: Lacks comparison with CLIP-Adapter and Tip-Adapter and recent constrained CLIP prompt learners, with tuning parity and modern training recipes.\n(2) Non-SOTA outcomes under-justified: When underperforming LoRA/FT, the paper lacks causal analysis (e.g., capacity limits of I/O adapters, architectural locality, data regime effects) and a quantitative cost–benefit (accuracy vs. trainable params, GPU-hours, latency, memory, concurrency).\n(3) Reporting gaps: Lack of disclosure of trainable parameter counts, compute budgets, hyperparameter search spaces/budgets for all methods, and also a lack of broader distribution-shift tests.\n\nMinor\n(4) Editorial issues: Repetition of full forms (e.g., expanding “Dark-Gray-box Adaptation (DGA)” multiple times). Define it once, then use the acronym."}, "questions": {"value": "Along with the weaknesses raised, here are a few questions that needs to be addressed\n\n1. Closest baselines: Why are CLIP-Adapter, Tip-Adapter/Tip-Adapter-F, VPT (and recent constrained prompt learners) not included? Please add them with tuning parity and modern training recipes.\n2. Trade-off analysis: For cases where you trail LoRA/FT, provide a causal diagnosis (capacity limits, layer-wise adaptation needs, data regime) and a quantitative trade-off table (acc. vs. trainable params/GPU-hours/latency/memory/concurrency)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxrRlLWsz6", "forum": "GhobHf58GS", "replyto": "GhobHf58GS", "signatures": ["ICLR.cc/2026/Conference/Submission8681/Reviewer_Rr9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8681/Reviewer_Rr9e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557569993, "cdate": 1761557569993, "tmdate": 1762920494785, "mdate": 1762920494785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a paradigm for adapting large foundation models without modifying their internal weights or architecture. The core idea is to maintain a frozen, sealed backbone model and perform adaptation solely through lightweight, external modules. The authors propose two specific instantiations: DarkGray-Box Adaptation (DGA), which restricts modifications to input and output adapters, and LightGray-Box Adaptation (LGA), which additionally allows the injection of learnable tokens into intermediate layers of a transformer to influence attention mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Strong Empirical Validation: The paper is thorough in its experiments. Evaluating across multiple tasks (retrieval, classification, generation), modalities (image, text, video, sketch), and model architectures (ViT, CNN) provides convincing evidence for the generality and robustness of the proposed methods. The consistent trend showing DGA/LGA's competitiveness with LoRA is impressive.\n\n- Comprehensive Analysis: The inclusion of detailed ablation studies, the number of proxy tokens, layer selection for LGA) is commendable. It provides deep insights into what makes the method work and offers practical guidance for future implementations. The statistical significance tests add rigor to the claims."}, "weaknesses": {"value": "1. Limited theoretical or empirical analysis: While the paper's motivation heavily leans on security and IP protection, the evaluation of these aspects is relatively superficial. The authors correctly note that LoRA weights can be used to recover original weights, thus re-classifying it as white-box. However, they do not provide a similar security analysis for their own methods. Could the gradients exposed in DGA, or the intermediate activations and gradients in LGA, be exploited in a model extraction or data reconstruction attack? A discussion, even if preliminary, on the security bounds of the \"Gray-box\" is a significant missing piece.\n\n2. Weak performance in significant domain shifts: The results on the Sketchy dataset and ImageNet-Sketch clearly show that Gray-box methods struggle compared to white-box methods when the target domain is vastly different from the pre-training domain (e.g., natural images vs. sketches). This is an important limitation that should be more prominently discussed in the main text, as it delineates the boundaries of the method's applicability. It suggests that internal weight adjustments are still necessary for radical domain adaptation.\n\n3. Positioning against very recent black-box methods: The discussion in Appendix D is a good start, but the paper could more forcefully position itself against the latest \"prompt optimization\" or \"black-box tuning\" literature (e.g., methods that optimize only the input text prompt). The claim that these methods are limited to text is valid, but a more direct comparison on a shared task (e.g., 16-shot classification) would strengthen the argument that Gray-box access provides a tangible advantage over pure black-box approaches.\n\n4. Computation and efficiency analysis: The approach requires full gradient back-propagation through the frozen model for both DGA and LGA. While the number of trained parameters is low, the training time and memory footprint are likely closer to full fine-tuning than to inference-time prompt tuning methods. A brief discussion of training efficiency would provide a more complete picture."}, "questions": {"value": "As mentioned above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "otALVU56Pl", "forum": "GhobHf58GS", "replyto": "GhobHf58GS", "signatures": ["ICLR.cc/2026/Conference/Submission8681/Reviewer_ka9w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8681/Reviewer_ka9w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822707591, "cdate": 1761822707591, "tmdate": 1762920494351, "mdate": 1762920494351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Gray-box\" fine-tuning, a lightweight, deployment-friendly framework for adapting foundation models without modifying internal parameters or architecture. It aims to address the challenges of deployment complexity, edge device optimization, and the exposure of proprietary model parameters. The method uses compact, external input/output adapters, trained with controlled gradient signals at predefined model entry points. Two variants are proposed: DarkGray-Box Adaptation (DGA), which only modifies input and output interfaces, and LightGray-Box Adaptation (LGA), which allows limited injection of learnable tokens at intermediate layers. Extensive evaluations show that Gray-box fine-tuning achieves competitive performance while maintaining strict constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written.\n\n2. The paper introduces a novel setting, \"Gray-box fine-tuning,\" which lies between black-box tuning and white-box tuning.\n\n3. The experiments conducted are extensive and well-executed."}, "weaknesses": {"value": "1. I don’t fully understand the necessity of Gray-box fine-tuning. Is it really needed in real-world scenarios?\n\n2. If the setting of Gray-box fine-tuning holds, then the proposed method is essentially the simplest approach, which doesn’t seem particularly innovative.\n\n3. Since the method allows for the introduction of additional learnable tokens in the intermediate layers, existing techniques like LoRA and Adapter also fit within the proposed LightGray-Box Adaptation framework. From this perspective, the LightGray-Box Adaptation (LGA) method proposed seems meaningless.\n\n4. The paper mentions scalability, efficient edge deployment, and security, but how do DGA and LGA ensure these benefits? It appears that both introduce additional computational complexity, and LGA requires prior knowledge of the model, such as dimensions, for setting the fine-tuning locations. Doesn’t this impact the model’s privacy?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "72nWqc5oAA", "forum": "GhobHf58GS", "replyto": "GhobHf58GS", "signatures": ["ICLR.cc/2026/Conference/Submission8681/Reviewer_Rb73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8681/Reviewer_Rb73"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891272840, "cdate": 1761891272840, "tmdate": 1762920493880, "mdate": 1762920493880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Gray-box fine-tuning, a novel paradigm for adapting foundation models under restricted internal access, aiming to balance adaptability, deployment efficiency, and model security/IP protection.\nTwo variants are proposed:\n\nDarkGray-Box Adaptation (DGA): trains lightweight input/output adapters while keeping all backbone weights and architecture frozen.\n\nLightGray-Box Adaptation (LGA): further allows the injection of a few learnable tokens at intermediate layers without altering internal weights.\n\nExtensive experiments across multiple modalities — including text-to-image retrieval, video retrieval, image classification, sketch retrieval, and diffusion-based generation — demonstrate that the proposed Gray-box methods achieve reasonably competitive performance compared with conventional fine-tuning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a new adaptation concept, Gray-box adaptation, which aims to minimize structural and parametric modifications during fine-tuning. This design simplifies the adaptation pipeline and makes it particularly suitable for privacy-sensitive or proprietary model deployment scenarios, showing clear practical relevance.\n\n2. The experimental section is comprehensive. The authors evaluate the approach on multiple backbones and diverse datasets, presenting convincing results that strengthen the paper’s empirical soundness.\n\n3. The writing and organization are clear and logical, with well-defined sections and consistent argumentation throughout the paper."}, "weaknesses": {"value": "1. While the proposed Gray-box adaptation indeed reduces model access and helps mitigate issues like model thievery, the claimed simplification over existing approaches (e.g., Prefix/Prompt Tuning, LoRA, or Last-Layer Fine-Tuning) is not convincingly demonstrated. The paper lacks concrete comparisons in FLOPs, latency, or deployment complexity to support its efficiency claims.\n\n2. Methodologically, DGA mainly performs linear transformations on the input/output spaces, and LGA adds learnable tokens at intermediate layers. However, its mechanism closely resembles existing prompt or prefix-tuning approaches (e.g., MaPLe), making the degree of novelty somewhat questionable.\n\n3. The ablation study is limited. It only analyzes the effects of visual and textual adapters, without exploring other critical factors (e.g., number or position of learnable tokens, adapter capacity, or gradient access scope).\n\n4. In many experiments, DGA and LGA underperform compared with LoRA or full fine-tuning, suggesting that simple linear transformations at the input/output level might be insufficient for effective adaptation. It would be valuable to design more expressive or adaptive Gray-box modules to further enhance performance while maintaining restricted model access."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g5ah2cVyTk", "forum": "GhobHf58GS", "replyto": "GhobHf58GS", "signatures": ["ICLR.cc/2026/Conference/Submission8681/Reviewer_Q4xG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8681/Reviewer_Q4xG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907318160, "cdate": 1761907318160, "tmdate": 1762920493263, "mdate": 1762920493263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}