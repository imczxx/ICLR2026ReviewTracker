{"id": "DdYBGUySAK", "number": 21115, "cdate": 1758313922235, "mdate": 1759896941300, "content": {"title": "LaGEA: Language Guided Embodied Agents for Robotic Manipulation", "abstract": "Robotic manipulation benefits from foundation models that describe goals, but today’s agents still lack a principled way to learn from their own mistakes. We ask whether natural language can serve as feedback, an error-reasoning signal that helps embodied agents diagnose what went wrong and correct course. We introduce **LaGEA**, a framework that turns episodic, schema-constrained reflections from a vision language model (VLM) into temporally grounded guidance for reinforcement learning. LaGEA summarizes each attempt in concise language, localizes the decisive moments in the trajectory, aligns feedback with visual state in a shared representation, and converts goal progress and feedback agreement into bounded, step-wise shaping rewards whose influence is modulated by an adaptive, failure-aware coefficient. This design yields dense signals early when exploration needs direction and gracefully recedes as competence grows. On the Meta-World MT10 embodied manipulation benchmark, LaGEA improves average success over the state-of-the-art (SOTA) methods by 9.0\\% on random goals and 5.3\\% on fixed goals, while converging faster. These results support our hypothesis: language, when structured and grounded in time, is an effective mechanism for teaching robots to self-reflect on mistakes and make better choices. Code will be released soon.", "tldr": "LaGEA turns structured natural-language self-reflections into temporally grounded reward shaping for RL, improving embodied manipulation benchmark success with faster convergence.", "keywords": ["Embodied AI", "Vision Language Model", "Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/372b149df6bba46d7458981901e841b6ba4002ba.pdf", "supplementary_material": "/attachment/e16ce9906297ecf8819b9ef18aee13bcb7bb9e2f.zip"}, "replies": [{"content": {"summary": {"value": "The paper LAGEA: Language Guided Embodied Agents for Robotic Manipulation presents a framework that uses natural language as feedback to help robots learn from their own mistakes. Instead of treating language only as a goal description, LAGEA turns structured reflections from a vision-language model into feedback that identifies when and why an error occurred. This feedback is aligned with visual states and used to generate dense, step-wise reward signals that guide the robot through each stage of training. The system focuses on key moments in a trajectory and adjusts the reward intensity as the agent improves. Tested on the Meta-World MT10 benchmark, LAGEA shows faster convergence and higher success rates than other reinforcement learning methods like FuRL and SAC. The results demonstrate that structured, time-grounded language feedback helps robots understand their own failures and improve decision-making more effectively than conventional approaches based solely on sparse or visual rewards."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles a timely and relevant problem in embodied AI, namely how natural language feedback from vision-language models can guide reinforcement learning through structured error reasoning. The proposed LAGEA framework is clearly described, integrating keyframe weighting, feedback alignment, and adaptive reward shaping in a coherent and mathematically sound pipeline. The manuscript is well written and visually organized, with intuitive figures and clear methodology. Empirically, LAGEA shows modest yet consistent improvements over FuRL on Meta-World MT10, indicating that language-conditioned feedback may provide useful dense guidance signals under controlled simulation settings, even though the overall contribution remains incremental."}, "weaknesses": {"value": "1.All evaluations are confined to the Meta-World MT10 benchmark. The results neither test scalability to other simulation benchmarks (e.g., RLBench, or Maniskill3) nor robustness to domain variations, sensor noise, or camera shifts. Tasks like MT10 can already be solved by multi-task SAC or contextual based RL (e.g., CARE[1]) so the claimed improvements are marginal.\n\n2.Despite focusing on embodied manipulation, all experiments remain purely simulated. There is no evidence that the proposed language-feedback mechanism can handle in the real world. The absence of any real-world demonstration limits the practical value of the framework.\n\n3.The method builds directly upon FuRL with minor architectural additions (keyframe weighting and language delta rewards). Conceptually, it’s an incremental work rather than a fundamentally new paradigm, and the paper oversells the “language as feedback” idea without demonstrating that it outperforms simpler shaping heuristics.\n\n4.Figures 3, 4, 6, and Table 3 appear to report results from a single run without proper standard deviations or confidence intervals. This raises concerns about statistical bias and reproducibility, especially since the improvements are small and might be due to cherry-picking.\n\n5.The comparison includes only SAC, Relay, LIV, and FuRL. Missing strong modern baselines like LIFT, or diffusion-based reward models undermines the fairness of the evaluation. Without them, it is unclear whether LAGEA truly sets a new state of the art.\n\n6.The proposed ``structured reflection'' feedback is generated by a small VLM (Qwen-2.5-VL-3B), yet no analysis verifies whether the feedback quality actually correlates with learning improvements or if it simply provides additional random dense reward shaping.\n\nReference:\n\n[1] Sodhani S, Zhang A, Pineau J. Multi-task reinforcement learning with context-based representations[C]//International conference on machine learning. PMLR, 2021: 9767-9779."}, "questions": {"value": "1.Have the authors tested LAGEA on more complex or long-horizon benchmarks (e.g., CALVIN, RLBench) to confirm scalability? How does the system perform under visual noise, unseen textures, or viewpoint shifts?\n\n2.Since the work emphasizes embodied robotic manipulation, can the authors provide real-robot validation or at least simulation-to-real experiments to demonstrate real-world applicability?\n\n3.Figures and tables seem based on single runs. How many seeds were actually used, and what is the variance across runs? Are the reported results statistically significant or potentially cherry-picked?\n\n4.Why are recent strong baselines like RL-VLM-F (Wang et al., 2024), LIFT (Nam et al., 2023), or other reward-shaping methods excluded? Would including them change the reported ranking?\n\n5.Can the authors quantify the effect of language feedback quality itself? For example, by randomizing or ablating the feedback text to prove that improvements come from meaningful linguistic grounding rather than auxiliary dense rewards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QiLXDH43w8", "forum": "DdYBGUySAK", "replyto": "DdYBGUySAK", "signatures": ["ICLR.cc/2026/Conference/Submission21115/Reviewer_3zpo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21115/Reviewer_3zpo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704849798, "cdate": 1761704849798, "tmdate": 1762941290052, "mdate": 1762941290052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LAGEA (Language Guided Embodied Agents), a novel framework that uses natural language as a feedback and error-reasoning signal for reinforcement learning in robotic manipulation. The core idea is to have a VLM generate structured, episodic self-reflections to diagnose what went wrong in a trajectory. This includes few key steps: 1) Keyframe Selection: It first identifies \"causal moments\" in a trajectory using a deterministic heuristic based on goal-similarity dynamics. 2) Structured Feedback: It queries a VLM (Qwen-2.5-VL-3B) with these key frames and a \"schema-constrained JSON\" format, forcing the VLM to output reliable, auditable feedback (e.g., error codes, rationales) rather than free-form text. 3) Feedback Alignment: This feedback (encoded by GPT-2) is aligned with visual state embeddings in a shared space using a hybrid calibration and contrastive loss.4) Reward Shaping: The framework generates two potential-based rewards: a goal-delta reward (progress toward the goal) and a feedback-delta reward (alignment with the VLM's diagnosis). Crucially, the feedback reward is gated by the keyframe weights, concentrating the signal at the causal moments. The experimental results show that LAGEA improves the average success rate over SOTA methods by 9.0% on random goals and 5.3% on fixed goals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The use of language as a post-hoc causal reasoning and feedback mechanism: as an \"error-reasoning signal\"  to guide an RL agent's critic is interesting.\n* The framework is well-designed: to deal with VLM's \"unreliability, they use schema-constrained JSON protocol combined with a predefined error taxonomy to enforce the stability of the process.\n* The reward model is dynamic: the shaping signal only applied on the failed trajectories with an annealing strength controled by $\\rho$ as policy's success EMA improves.\n* The experiments are comprehensive: by systematically validating each design choice."}, "weaknesses": {"value": "* Ealy-stage training instability: in the early-stage training, an untrained policy executes mostly random or nonsensical trajectories. In this case, the effectiveness for VLM to diagnose primary error might be constrained. It is not fully analyzed how the VLM provides useful, \"actionable\" feedback during the initial phase of training.\n* The keyframe selection is heuristic: the selection process could be heuristic as it is based on the goal-similarity $s_t$\nand its temporal derivatives. This assumes the most critical moment of failure correlates with proximity to the goal or sharp changes in movement. However, in some cases, a critical failure (e.g. picking up the wrong object) could be subtle, occur far from the goal, and not involve rapid acceleration, causing the heuristic to miss the true causal moment.\n* The efficiency of the method: as it requires the sequential inference of several large models for each episode, it might affect the whole efficiency of the whole framework."}, "questions": {"value": "* I notice `peg-insert-side-v2` task is failed in all experiments and the authors' explanation is \"unable to produce a successful episode\". Is this a failure of the VLM feedback (e.g., it cannot diagnose precision errors)? A failure of the keyframe heuristic? Or purely due to the task complexity?\n* In real-world, the error propagations are complex and random, is the few-shot prompting sufficient to handle the high variance of initial exploratory failures?\n* The overall idea is interesting, if the authors can address my questions well, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3VB5PsXB3F", "forum": "DdYBGUySAK", "replyto": "DdYBGUySAK", "signatures": ["ICLR.cc/2026/Conference/Submission21115/Reviewer_y9EK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21115/Reviewer_y9EK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997339400, "cdate": 1761997339400, "tmdate": 1762941289330, "mdate": 1762941289330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the application of natural language as a guiding mechanism for embodied agents performing manipulation tasks. The authors propose a method where language serves as feedback, enabling agents to reflect on their own failures. Additionally, the study introduces a reward shaping technique designed to incentivize agents to navigate toward both the goal and the feedback directions indicated by the language feedback. The efficacy of the proposed approach is evaluated using the MT10 benchmarks. In comparison to baseline methods the proposed approach demonstrates a higher success rate across various tasks in both random-goal and fixed-goal settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a strong motivation. The ability to identify, learn from, and subsequently correct previous failures is a critical capability for embodied agents. This paper proposes a framework to address this challenge and show improvement empirically. \n\n2. While further clarification on some technical details is needed, the paper is generally well structured and easy to follow. \n\n3. The experimental results are comprehensive and look promising. The proposed approach achieves better results than competitive baselines such as FuRL and Relay across various tasks."}, "weaknesses": {"value": "1. The reviewer has concerns regarding the generalizability of the reward shaping mechanisms detailed in Sections 3.2 and 3.3. The proposed reward function appears to be heavily engineered, seemingly reliant on task-specific knowledge and heuristics. This reliance on hand-crafted components leads to uncertainty about the method's broader applicability. It is unclear whether this complex reward structure would remain effective, or if it would require significant re-engineering, when applied to new environments or tasks that differ substantially from the MT10 benchmark. \n\n2. In addition, do you observe any reward hacking when applying the dense rewards? \n\n3. The reviewer appreciated that all experiments were replicated using five distinct random seeds. However, a confidence interval was not included in the results. Given the potentially high variance of RL methods, the absence of confidence intervals makes it challenging to accurately assess and compare the different approaches."}, "questions": {"value": "1. Regarding the reward component introduced in lines 254 and 264, 'reward movement toward the feedback direction,' the reviewers would appreciate further elaboration on this concept. Could the authors please clarify the precise definition and representation of the 'feedback direction'?  A more detailed explanation of the intuition for how this specific reward term guides the agent's policy update would be beneficial. \n\n2. In table 2, all methods, including the proposed approach failed the task peg-insert-side-v2. Could you elaborate why the language feedback doesn’t help in this task? \n\n3. In Table 2, the meaning of the numbers in parentheses is ambiguous to the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8zkA0DyzMa", "forum": "DdYBGUySAK", "replyto": "DdYBGUySAK", "signatures": ["ICLR.cc/2026/Conference/Submission21115/Reviewer_sM6M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21115/Reviewer_sM6M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209035420, "cdate": 1762209035420, "tmdate": 1762941288479, "mdate": 1762941288479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LAGEA, a framework that integrates structured language feedback from a VLM for RL reward densification in the domain of vision-based robotic manipulation. The core idea is to transform text-based reflections into dense, temporally grounded rewards.\n\nThe method has 3 main stages:\n1. Language feedback extraction: After each rollout, Qwen-2.5-VL-3B model analyzes N uniformly sampled frames along with task instruction, error taxonomy, examples and history. The VLM outputs a schema-constrained JSON reflection. The reflection is then embedded by GPT-2 into a 768-dimensional feedback vector.\n2. Key frame selection: K keyframes are selected based on a) proximity to the goal, b) velocity and c) acceleration of image features. Then, dense per-frame weight is computed via a triangle kernel and normalization.\n3. Feedback alignment: the authors use small MLPs to project images, feedback and goals into the same dimension. They are trained to aligned in a shared space using both contrastive and classification losses.\n4. Reward shaping: The aligned embeddings are used to compute goal deltas and feedback deltas, which capture improvement or degradation between successive frames. These deltas produce bounded, step-level rewards that complement the sparse task reward. The dense reward is also weighted by learning progress: smaller dense reward as the policy performance improves (success EMA increases).\n\nExperiments are conducted on the Meta-World MT10 benchmark for simulated robotic manipulation. LAGEA achieves higher success rates and faster convergence compared to baselines such as LIV, FuRL, and Relay. The improvements hold in both fixed-goal and random-goal settings, with average gains of roughly +5–9% over prior work. The authors also conduct detailed ablation studies. Removing key-frame selection leads to slower learning and unstable gradients. Excluding goal-delta, feedback-delta or adaptive weighting also result in degraded performance.\n\nOverall, the results demonstrate that structured language reflections can provide meaningful, fine-grained supervision for reinforcement learning, improving both sample efficiency and final policy quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is well motivated: the paper addresses a concrete limitation of existing VLM-RL methods: language feedback is often ambiguous. LAGEA introduces temporally grounded, structured reward augmentation.\n- Most of the main algorithm components, including the structured JSON feedback, key-frame selection, and delta-based shaping, are well explained with intuitive justification.\n- Strong empirical results. LAGEA consistently outperforms relevant baselines across multiple tasks in simulation. It improves success rate and shows faster convergence, confirming that dense reward accelerates learning.\n- The authors conducted ablation studies to analyze each component’s contribution. The clear performance drops for these variants strengthen the causal claims.\n- The writing is organized, equations are well explained, and implementation details are included."}, "weaknesses": {"value": "- Limited evaluation scope. All experiments are conducted in simulation on Meta-World MT10. Moreover, it appears that each task require approximately a million environment steps to train. It is unclear how well this method would work on real robots in practice, especially when works like HIL-SERL and LaNE show sub-one-hour training on real hardware.\n- Dependence on frozen VLM and LLM. The system relies on one frozen vision-language model for reflection generation and another frozen LLM for embedding. There is no comparison across different VLMs or analysis of robustness to language model errors. Hallucinated feedback could mislead reward shaping.\n- The feedback is generated on uniformly sampled frames, risking missing important frames.\n- The core algorithm contains a lot of moving parts. The feedback alignment section especially appears ad-hoc. This stage fuses embeddings using BCE, InfoNCE and symmetric losses, but the paper does not visualize or quantify how important each of these components are or how well the feedback vector correlates with the state and goal embeddings. More analysis and ablation are needed to fully justify the complex design. The reward formulation also introduces several tunable factors: temperatures, discounts, and adaptive weighting schedules. It is unclear how easy or hard would it be to make LAGEA work in a new domain."}, "questions": {"value": "- The VLM takes uniformly sampled, but then later the authors select K key frames. Wouldn't it make sense to have the VLM process key frames instead? It appears to me that section 3.1.2's computation is possible before VLM.\n- In the NCE loss, what is $z^{(j)}$? Should it be $z_{f}^{(j)}$?\n- $L_{sym}$ needs substantially more explanation and justification: what is \"cross-entropies over cosine-similarity softmaxes\"? Isn't the alignment objective in conflict with the BCE loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8x5x2gcQgi", "forum": "DdYBGUySAK", "replyto": "DdYBGUySAK", "signatures": ["ICLR.cc/2026/Conference/Submission21115/Reviewer_oEmK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21115/Reviewer_oEmK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762400790211, "cdate": 1762400790211, "tmdate": 1762941286909, "mdate": 1762941286909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LAGEA, which leverages VLM to generate language feedback, thereby improving the training of embodied manipulation models. Experiments were conducted on the MT10 dataset, and the results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of integrating language feedback in RL training pipeline is interesting.\n2. The framework innovatively combines episodic VLM reflections with temporal localizationand reward shaping, extending self-reflection from text-based agents to visuomotor domains."}, "weaknesses": {"value": "1. The experiments are confined to a single benchmark (Meta-World MT10), which consists of 10 relatively simple manipulation tasks in simulation.\n2. Generating feedback requires querying VLM on N sampled frames per episode and embeding. In RL, where training involves thousands of episodes, this may adds significant overhead. It's unclear if LAGEA is practical for longer-horizon tasks.\n3. The feedback is constrained by a pre-defined error taxonomy, which appears task-specific, potentially limits scalability to new tasks or domains.\n4. Results depend on specific models: Qwen-2.5-VL-3B and GPT-2. No ablations test alternatives, which could affect reflection accuracy (e.g., hallucination rates) or embedding quality.\n5.  There's no empirical validation: are these key frames truly causal? No comparisons to oracle key frames (e.g., human-annotated), random selection, or attention-based alternatives.\n6. No qualitative examples of feedback (e.g., JSON outputs) or error cases (e.g., when VLM hallucinates)."}, "questions": {"value": "1. How does including history avoid compounding early errors?\n2. Could you provide computational benchmarks (e.g., training time vs. FuRL) and discuss mitigations for VLM overhead?\n3. How is the error taxonomy created/extended for new tasks?\n4. What happens with different VLMs/encoders?\n5. How do you validate key-frame causality? Consider comparing to human judgments.\n6. Provide failure case analyses: When does feedback mislead the policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sj7Nbr1NI0", "forum": "DdYBGUySAK", "replyto": "DdYBGUySAK", "signatures": ["ICLR.cc/2026/Conference/Submission21115/Reviewer_cyzP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21115/Reviewer_cyzP"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762421606107, "cdate": 1762421606107, "tmdate": 1762941285122, "mdate": 1762941285122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}