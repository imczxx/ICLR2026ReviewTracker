{"id": "5R11h5o44C", "number": 9413, "cdate": 1758121619311, "mdate": 1759897726289, "content": {"title": "Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms", "abstract": "Positive-unlabeled (PU) learning is a weakly supervised binary classification problem, in which the goal is to learn a binary classifier from only positive and unlabeled data, without access to negative data. In recent years, many PU learning algorithms have been developed to improve model performance. However, experimental settings are highly inconsistent, making it difficult to identify which algorithm performs better. In this paper, we propose the first PU learning benchmark to systematically compare PU learning algorithms. During our implementation, we identify subtle yet critical factors that affect the realistic and fair evaluation of PU learning algorithms. On the one hand, many PU learning algorithms rely on a validation set that includes negative data for model selection. This is unrealistic in traditional PU learning settings, where no negative data are available. To handle this problem, we systematically investigate model selection criteria for PU learning. On the other hand, the problem settings and solutions of PU learning have different families, i.e., the one-sample and two-sample settings. However, existing evaluation protocols are heavily biased towards the one-sample setting and neglect the significant difference between them. We identify the internal label shift problem of unlabeled training data for the one-sample setting and propose a simple yet effective calibration approach to ensure fair comparisons within and across families. We hope our framework will provide an accessible, realistic, and fair environment for evaluating PU learning algorithms in the future.", "tldr": "We propose the first positive-unlabeled learning benchmark to promote accessible, realistic, and fair evaluation of positive-unlabeled learning algorithms.", "keywords": ["Positive-unlabeled learning", "weakly supervised learning."], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e14832600204e2ac88ec18946336db5dd0a20d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates the experimental settings in PU learning.\nThe authors argue that using a validation set containing both positive and negative labeled samples for model selection is unrealistic, and therefore propose a proxy metric as a surrogate for this purpose.\nThey further discuss two sampling processes in PU learning, namely One-Sample (OS) and Two-Sample (TS), and introduce a calibration technique to mitigate the class-shift problem when transitioning from the TS to the OS setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed proxy metric is useful for model selection in cases where a validation set with both positive and negative samples cannot be obtained."}, "weaknesses": {"value": "1. The authors discuss two data generation processes, namely One-Sample and Two-Sample. However, it remains unclear why they do not consider benchmark settings with varying ratios of positive to unlabeled data, or different proportions of positive samples within the unlabeled set. By controlling the ratios, we can validate PU algorithms across varying settings to demonstrate their robustness. \n2. Regarding the proposed PA, although the authors argue that access to a fully labeled validation dataset is unrealistic, their assumption of knowing the class prior probability $\\\\pi$ is equally impractical. \n3. Although the authors theoretically prove that the proposed proxy metric preserves the partial order of the original metric, they provide no experimental evidence that the model selected by this proxy coincides with the one chosen using a validation set containing both positive and negative samples. \n4. The introduction of the calibration technique for the OS setting seems redundant, since the OS setting can be straightforwardly transformed into a TS setting by redefining the unlabeled training data to combine the original positive and unlabeled sets. Essentially, the proposed calibration achieves the same effect implicitly, as demonstrated below.\n\nWe start from Eq. 8:\n$$\nR = \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} ( \\\\ell( f(x_i), +1 ) + (c-1) \\\\ell (f(x_i), -1)) + \\\\frac{1 - c\\\\pi}{n_u} \\\\sum_{j=1}^{n_u}\\\\ell(f(x_i), -1).\n$$\nThis can be rewritten as\n$$\nR = \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell( f(x_i), +1)- \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell (f(x_i), -1) + \\\\frac{\\\\pi}{n_p} c \\\\sum_{i=1}^{n_p} \\\\ell (f(x_i), -1) + \\\\frac{1 - c\\\\pi}{n_u} \\\\sum_{j=1}^{n_u} \\\\ell(f(x_i), -1).\n$$\nSubstituting $c = n_p / (\\\\pi (n_p + n_u))$ gives\n$$\nR = \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell( f(x_i), +1) - \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell (f(x_i), -1) + \\\\frac{1}{n_p + n_u} \\\\sum_{i=1}^{n_p} \\\\ell (f(x_i), -1) + \\\\frac{1}{n_p + n_u} \\\\sum_{j=1}^{n_u} \\\\ell(f(x_i), -1).\n$$\nThus,\n$$\nR = \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell( f(x_i), +1) - \\\\frac{\\\\pi}{n_p} \\\\sum_{i=1}^{n_p} \\\\ell (f(x_i), -1) + \\\\frac{1}{n_p + n_u} \\\\sum_{i=1}^{n_u + n_p} \\\\ell(f(x_i), -1)\n$$\nThis derivation shows that the proposed calibration is equivalent to simply merging the positive and unlabeled sets into a single new unlabeled set."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fwmQmPPInJ", "forum": "5R11h5o44C", "replyto": "5R11h5o44C", "signatures": ["ICLR.cc/2026/Conference/Submission9413/Reviewer_WGLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9413/Reviewer_WGLx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758381810, "cdate": 1761758381810, "tmdate": 1762921018178, "mdate": 1762921018178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establish the first comprehensive benchmark for evaluating PU learning algorithms. Through extensive experiment with many state-of-the-art algorithms, the authors identify the internal label shift problem between OS and TS PU learning settings. The proposed model selection criteria doesn't rely on negative validation data, which contradicts PU learning's core premise. In addition, the authors proposed a calibration methods with theoretical guarantees to address the label shift issue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors conducted a thorough evaluation using four datasets with systematic data generation and consistent evaluation protocols. Three criteria are compared for model selection purposes. The experimental framework is particularly well-designed, addressing the unrealistic assumption that many existing evaluations make about having access to negative validation data."}, "weaknesses": {"value": "1. The authors use relatively small benchmark dataset, which may not reflect large-scale performance. \n2. Deeper exploration on how estimation errors in class prior affect performance is not presented."}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NbU9ypArC8", "forum": "5R11h5o44C", "replyto": "5R11h5o44C", "signatures": ["ICLR.cc/2026/Conference/Submission9413/Reviewer_BgWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9413/Reviewer_BgWp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945972431, "cdate": 1761945972431, "tmdate": 1762921017590, "mdate": 1762921017590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on empirical studies on PU learning methods. It systematically investigate model selection criteria to reduce the dependence on validation set, and designs a simple approach to ensure fair comparisons within and across one-sample and two-sample settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a realistic and fair evaluation framework of PU Learning, including model selection criteria and evaluation protocols. The framework is important to identify the effectiveness of components within existing methods and will promote the development of PU Learning.\n2. The paper is well-structured and written in a fluent manner, making it easy to follow and understand.\n3. The analysis is thorough and the experiments are comprehensive."}, "weaknesses": {"value": "1. Some evaluations over real-world datasets (e.g., from medical imaging, fraud detection, or recommender systems) are expected to sufficiently demonstrate the benchmark's effectiveness on larger-scale, more complex, or truly real-world PU datasets.\n2. The proposed calibration approach for the Internal Label Shift (ILS) problem is presented as \"simple yet effective.\" However, the paper lacks a discussion of its potential limitations."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dB0t6nfnZb", "forum": "5R11h5o44C", "replyto": "5R11h5o44C", "signatures": ["ICLR.cc/2026/Conference/Submission9413/Reviewer_c2Ap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9413/Reviewer_c2Ap"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989570275, "cdate": 1761989570275, "tmdate": 1762921017052, "mdate": 1762921017052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper performs an extensive benchmarking of many PU learning algorithms in a unified setting that is designed to be realistic and fair, and the results provide novel insights on the performance of those algorithms. In addition, through theoretical analysis, it identifies the problem of directly apply a two-sample PU learning algorithm to the one-sample setting, and provides a simple solution to this problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "As far as I know, this is the first rigorous and extensive benchmarking for PU learning algorithms. I believe the benchmarking provides interesting insights that can be useful for further research in the field, and the methodology can be adopted in future work for more rigorous comparison.\n\nI think the theoretical analysis is interesting and it seems to be sound.\n\nThe paper is well written and easy to follow."}, "weaknesses": {"value": "Currently the benchmarking is performed on four benchmarks while some other datasets are also commonly used, and it would be helpful to have benchmarking results on some of those datasets for comparison with results in the literature. Nevertheless, current benchmarking already requires substantial effort though.\n\nThe model details seem to be missing. Table 4 mention ResNet and MLP but there are no details.\n\nMinor comment: \"accessible\" doesn't seem needed as this is subsumed by \"realistic\"."}, "questions": {"value": "* How are the minibatch sizes for the P and U data set in Algorithm 1?\n* Why is SGD used while Adam is often better?\n* Training is done for 20,000 iterations. Is this generally large enough to ensure that the models are sufficiently well-trained, despite dataset size differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kAFcAoxx8F", "forum": "5R11h5o44C", "replyto": "5R11h5o44C", "signatures": ["ICLR.cc/2026/Conference/Submission9413/Reviewer_DKd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9413/Reviewer_DKd8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995210724, "cdate": 1761995210724, "tmdate": 1762921016769, "mdate": 1762921016769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}