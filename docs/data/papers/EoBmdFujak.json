{"id": "EoBmdFujak", "number": 12814, "cdate": 1758210497144, "mdate": 1759897483118, "content": {"title": "Train Once, Answer All: Many Pretraining Experiments for the Cost of One", "abstract": "Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a *single* training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget.", "tldr": "We show that it is possible to conduct multiple pretraining experiments during the training of a single LLM.", "keywords": ["large language models (LLMs)", "pretraining", "experiments", "memorization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abebab4d70423a870cbd08d41ed1f1d01e77b065.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work demonstrates that we could conduct multiple pre-training experiments in a single run while retaining the independence of the results of the different pre-training tests\n\nThe paper empirically shows this by conducting ten different tests in a single training run of an OLMo 2 model on 210 Billion tokens. They demonstrate that even while the experiments have been run on a single training run, they reproduce the results as independent runs of the same experiments.\n\nThe main contributions of this work are listed as:\n- The paper proposes a novel method to conduct multiple independent pretraining experiments within a single training run, dramatically reducing the computational cost of large-scale controlled pretraining studies.\n- The authors replicate results from five prior works (on contamination, poisoning, memorization, and forgetting) within a single training run, while also introducing three novel experiments on knowledge acquisition, mathematical reasoning, and pretraining data watermarking.\n- They introduce CPDT, a new method to test for dependencies between different experiments before training, and show that the simultaneous experiments have negligible interactions and minimal effect on overall model performance"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has covered a decent variety of experiments to test and targets an important problem statement. While most of the focus is on the final training costs of models, training experiments take a significant amount of resources and it is critical to drive down this cost to increase innovations.\n\nThe core strength of this work is the usefulness of the direction they've taken and their simple and clear approach towards demonstrating the possibility of multiple experiments in a single run."}, "weaknesses": {"value": "In general, I'm a bit worried about the lack of ablations or comprehensiveness of the tests done in this study. This leaves a lot of room to improve this study at best and at worst puts questions on some of the claims made. I'm list these gaps found as follows in no particular order:\n\n- While the work does a good job in conducting experiments with different tests and reporting their results, there has been no ablations or analysis by varying some of the test parameters such as the number of tokens modified. Since for most of the tests considered in this study, the only way to affect other tests are the tokens varied, I believe it should have been a core part of the study to identify the sensitivity of tests with increasing amount of modified tokens, such an approach might've helped to come up with a more general heuristic to determine if a test could be affected due to other tests during the training\n\n- The original OLMo models of the size used in the paper were trained using ~10x the tokens used in this paper. The paper trains the model from random initialization, hence the provided results are valid only for the early stages of pretraining. The placement of these modified tokens in different phases of tokens could potentially have large difference in the final results; This limitation could be explained by the training budget stated in appendix B3, but nonetheless this limitation makes the result unreliable in this respect\n\n- There is only a single test which changes a setting of training other than the tokens while in a real situation many tests might need more change than only the tokens. How such tests might influence each other has been unexplored in this study\n\n- No confidence values or other statistical methods used to back up the results\n\n- Many of the graphs and figures in general don't convey full information, for instance in Figure 4, there is no mention what is the difference that is being reported. Figure3 a & b don't show the corresponding results on the independent tests\n\n- Appendix B4, seems is too vague of a definition, especially when the original test results aren't reported in every case and there should have been a more rigid definition for when you consider two test results the same. This definition leaves too much room to make haphazard claims"}, "questions": {"value": "- Please clarify what exactly do the values in Figure 4 mean? Are they percentage changes?\n\n- Why were no statistical tests used to properly identify whether there has been a change in results or not?\n\n- If I missed something while writing the weaknesses section, please let me know, especially regarding the pointer on Appendix B4, which seems to be one of the biggest drawback of this study to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "joesx3dNzb", "forum": "EoBmdFujak", "replyto": "EoBmdFujak", "signatures": ["ICLR.cc/2026/Conference/Submission12814/Reviewer_MRar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12814/Reviewer_MRar"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760643789151, "cdate": 1760643789151, "tmdate": 1762923622335, "mdate": 1762923622335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the practice of pre-training-based experimentation where model behavior is assessed in response to pre-training data treatments. They argue that the traditional approach of pre-training experiments is challenging due to the computational cost of training modern day language models. They instead argue that multiple pre-training data treatments (i.e., experiments) can be applied to a single model. This saves computational cost and enables comprehensive studies of multiple behaviors at once. They raise a potential limitation of their approach: confounding effects between different experiments. To address this, they propose an interaction test for continuous pretraining experiments and find that interactions are negligible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is reasonable: pretraining-based experiments are really expensive to conduct in a one-at-a-time manner. By assessing if multiple treatments/outcome pairs could be assessed at the same time, the cost of pretraining experimentation is substantially reduced. Further, the authors do not emphasize this in the writing, but I observe that in real-world model training, datasets are rarely as controlled as they are in traditional pre-training datasets; datasets in the wild are more likely to resemble the proposed “train-at-once” pre-training paradigm rather than the traditional “one-at-a-time” paradigm.\n2. This paper was clear and easy to read. \n3. The paper comprehensively assesses its train-at-once experimental framework with a wide range of benchmarks (10), shown in table 1.\n4. The paper does touch on the challenge of teasing about controlling for confounders when multiple experiments are done at the same time."}, "weaknesses": {"value": "In section 5.2: You find that interactions are negligible based on limited dataset manipulation and limited training (roughly 1% of data). However, some previous work has shown that models demonstrate substantially different behavior as the size and time of training is scaled (for example: deep double descent [1]). How do you account for confounders/model behaviors which may not manifest unless that proportion of the data in the pretraining data is scaled or the length of training is scaled?\n\n[1] Deep Double Descent: Where Bigger Models and More Data Hurt. ICLR 2020."}, "questions": {"value": "In figure (a) and (b), you find a dependance between language modeling benchmarks but no dependance between experiments. What makes a “benchmark” different from an \"experiment\"? From what I understand, both a benchmark and experiment simply involve applying some specific treatment to a portion of the pre-training data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FbSKwsJJdI", "forum": "EoBmdFujak", "replyto": "EoBmdFujak", "signatures": ["ICLR.cc/2026/Conference/Submission12814/Reviewer_YL5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12814/Reviewer_YL5h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760917238504, "cdate": 1760917238504, "tmdate": 1762923621907, "mdate": 1762923621907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new experiment protocol to conduct multiple continual pre-training experiments, i.e., ones that intervene on training data to measure the effects on model behaviors, in a single training run. The main motivation is to reduce computation cost of running these experiments. The authors empirically show the validity of the proposed experiment protocol by conduct a single continue pre-training run using OLMo-1B on 210B tokens (~5% of the original pre-training data) with 10 general quality, memorization, and unlearning experiments, which replicate the effects of individual training run from the literature."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Aim to solve an important problem**: The problem that this work aims to tackle, i.e., high computation cost for continued pre-training experiment, is a real and pressing issue for researchers working on understanding pre-training. This work takes a different approach to solve this problem, instead trying to reduce the cost of an individual run, the authors explore the possibility to combine multiple experiments, which could offer a practical solution to a researcher/group who happens to be interested in doing multiple continued pre-training experiments.\n\n* **Replication study is a valuable contribution**: Apart from the experiment protocol, a valuable contribution of this work is the replication studies of existing work. It is reassuring to see that these five experiments from the literature can be replicated.\n\n* **Experiment covers a diverse set of model behaviors**: The single training run covers 10 tasks under three broad categories: general quality, memorization & privacy, and unlearning, which are relatively comprehensive given the types of continued pre-training experiment conducted in literature (however, see questions below on the specific choices of tasks)"}, "weaknesses": {"value": "* **Compute reduction and applicability is limited**:\n  * While reducing compute is the main motivation, the proposed experimental framework only reduces compute linearly to the number of tasks combined. Even under this protocol, it would not enable a researcher who can afford to train 1B models, but not 10 runs of 1B model to be able to conduct experiments on a 8B model (as under the Chinchilla compute-optimal scaling, it would require x8 tokens and x8 parameters, so x64 compute to train a 8B model). Yet, in practice, a question that lingers on researcher's mind is often what happens when the model gets larger.\n  * Practically, the use case that benefits the most from this protocol, which is a research work that requires running experiments from unrelated interventions on training data, e.g., a paper that studies both knowledge acquisition, watermarking, and unlearning, might not be the most common use case. Rather, a work would more likely require multiple runs of the same task, but intervening on slightly different pre-training data, which is not well discussed in this work.\n\n* **No comparison with alternative cost reduction methods**: If the goal is to save compute, there are many other established ways to consider: 1) fitting a scaling law on multiple smaller models to estimate performance on larger models; 2) Open sourcing and standardizing continued pre-trained models with different data intervention, e.g., the \"pythia-intervention\" models from Biderman et al. 2023. \n\n* **Unclear whether interference will occur in a new task**: Given it is unlikely that a researcher would need to run a subset of the ten tasks studied in this paper, it would be useful to understand and predict when two new tasks will interfere with each other. The authors propose a metric to empirically measure the task dependency after running the task (in Section 5.2), but this seems to be a post-hoc measure that would not allow one to predict the outcome ahead of time and save the cost of running tasks that interfere with each other. Does feature like ngram overlaps help us to predict task independence?\n\n* **Lack of justification on the choice of tasks**: While these ten tasks provide reasonable coverage of types of model behaviors, it is unclear why authors choose a specific task within each type.  Do these experiments cover a particularly effective method, a representative and well-known phenomenon, or just randomly chosen from recent ML conference publications? Just some examples,\n  * Why choose Gaussian watermarking but not other training-time watermark?\n  * Why choose Pagliardini et al, 2025's observation on forgetting curve, i.e., \"spikes immediately after observing the batch, but then decays quickly back to its original level\", especially given earlier work has contradictory findings when using continued pre-training experiment, e.g., Figure 1 in [Chang et al 2024](https://proceedings.neurips.cc/paper_files/paper/2024/file/6fdf57c71bc1f1ee29014b8dc52e723f-Paper-Conference.pdf) clearly shows that model can retain the information well and does not drop back to its original level? Or maybe the description of \"back to its original level\" is just inaccurate here, as neither Figure 19 nor Figure 20 shows that the loss drops back to the original (pre exposure).\n  * Why choose MUSE-news for unlearning? Does the domain of the unlearned texts matter for the experiment outcome?\n\n* **Experiment is conducted on a significantly undertrained model**: In the experiment, the authors trained OLMo-2-1B model on 210B tokens, this is only 5% of the original OLMo-2-1B training token count. As checkpoints from earlier part of the training typically do not have rich structures as later ones, tasks that rely on learning structures could be affected, e.g., conducting these experiments on a well-trained model might yield different results."}, "questions": {"value": "Please see weaknesses above.\n\nAdditionally, it would be great if authors could offer some thoughts on how the proposed protocol can be used practically. The proposed experimental framework only provides benefits when there are multiple pretraining experiments to run. Given the chance that a researcher wants to conduct all these experiments in a single research paper is relatively low, (how) should they disclose that their experiment run is potentially mixed with treatments from a completely irrelevant experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "grlY1uckt7", "forum": "EoBmdFujak", "replyto": "EoBmdFujak", "signatures": ["ICLR.cc/2026/Conference/Submission12814/Reviewer_Fc7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12814/Reviewer_Fc7F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906708198, "cdate": 1761906708198, "tmdate": 1762923621335, "mdate": 1762923621335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes running multiple controlled pretraining experiments within a single training run, arguing this amortizes compute while preserving experimental validity. The authors train a 1.5B-parameter OLMo-2 variant on 210B tokens and interleave ten interventions spanning knowledge acquisition, math reasoning, contamination, memorization (two styles), Gaussian watermarking, poisoning, forgetting curves, MUSE-News insertion, and i.i.d. token replacements. They also introduce Continual Pretraining Dependence Testing (CPDT) to check whether experiments interfere with one another before full training. Empirically, they (i) replicate several prior results on contamination, memorization, poisoning, and forgetting, (ii) show targeted gains on math reasoning with small in-distribution data and modest length generalization, and (iii) observe minimal deviation in loss curves and overall performance relative to a baseline run. The conceptual pitch: “train once, answer many.” (Abstract; contributions; Fig. 1 depiction of the idea ; experiments overview/table and scope ; key math/watermarking results ; independence testing intro ; training-dynamics similarity .)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Compute-amortizing paradigm with strong empirical grounding; shows many results can survive co-training without obvious interference. (Loss/accuracy parity; deviation minimal .)\nCPDT gives an actionable, pretraining-time interference check; convincingly shows benchmark–benchmark dependencies vs. minimal cross-experiment effects. (Dependence matrices and interpretation .)\nBreadth + Replication: Reproduces contamination scaling/forgetting, canary patterns, poisoning backdoors, and batch forgetting, inside one run. (Replicated findings list & plots .)\nNew experiments: (i) closed-loop knowledge acquisition with online control, (ii) math reasoning improvements and modest length generalization at tiny data ratios, (iii) Gaussian watermark detectability during pretraining. (Results & curves .)"}, "weaknesses": {"value": "External validity: One model scale (1.5B) and one data mixture; it’s unclear whether independence and negligible dynamics shift hold at 7B/13B+ or with different mixtures/optimizers. A small ablation at another scale would strengthen claims.\nInterference boundaries: CPDT measures pairwise/aggregate effects over short continual-pretraining bursts at ~1% intensity. A formal analysis of its power and false-negative rate—especially for subtle, higher-order interactions—would help. (Limits acknowledged; higher-order interactions noted .)\nMemV special-casing: The dependence method cannot address experiments defined by absence of data across the entire run; the paper notes this, but leaves open broader classes where CPDT won’t apply. (MemV limitation .)\nReasoning eval scope: Math reasoning improvements are promising, but limited to a synthetic GSM-like suite; adding a public math benchmark (e.g., GSM8K-style transfer) or open-ended CoT probes would bolster generality.\nSecurity/privacy framing: Watermark and poisoning results are neat; discussion could more squarely address safety trade-offs of co-training many interventions (e.g., increased attack surface vs. detection tools)."}, "questions": {"value": "Scaling: Do CPDT results and minimal dynamics drift persist at 7B/13B? Any pilot evidence or costed plan?\nCPDT power: Can you quantify sensitivity (effect sizes detectable vs. training steps/data intensity)? Any synthetic stress test where a known small interaction is planted and detected?\nInterference classes: Beyond MemV, which experiment types (e.g., instruction-tuning-like format shifts, toolformer-style tagging) are expected to defeat CPDT’s approximation?\nScheduling: Did you test whether when (early vs. late) an experiment’s data is inserted changes its measured outcome or interference (given the observed “recency bias” in watermarks)? (Recency hint .)\nReasoning generalization: Any transfer to non-synthetic math or reasoning suites (e.g., ARC-C, MATH subsections) without contamination?\nCompute ledger: What was the runtime/throughput overhead of logging, selective data routing, and periodic probing? Could this be packaged for community reuse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RmT05h8c62", "forum": "EoBmdFujak", "replyto": "EoBmdFujak", "signatures": ["ICLR.cc/2026/Conference/Submission12814/Reviewer_YRA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12814/Reviewer_YRA9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125770028, "cdate": 1762125770028, "tmdate": 1762923620852, "mdate": 1762923620852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}