{"id": "TzqUDJZ23Q", "number": 9836, "cdate": 1758143045070, "mdate": 1759897692674, "content": {"title": "Learning to Think Like a Cartoon Captionist: Humor Understanding With Multimodal Reasoning Models", "abstract": "Humor remains one of the most elusive challenges for artificial intelligence, demanding models to integrate visual perception, cultural knowledge, and creative reasoning. The New Yorker Cartoon Caption Contest (NYCC) offers a uniquely structured testbed for this problem, pairing images with thousands of captions, expert curation, and large-scale audience judgments. Prior work largely reduces humor to black-box classification or preference prediction,  overlooking the step-by-step reasoning processes employed by human captionists. We introduce a framework for teaching multimodal language models (MLLMs) to reason like professional captionists. Central to our approach are captionist reasoning traces that decompose humor into incongruity detection, resolution construction, and punchline evaluation. Models are first adapted through continual pretraining on humor-focused corpora, then trained with supervised fine-tuning on captionist-style traces, and finally aligned with  humor judgments using reinforcement learning with grounded perceptual rewards and stylistic rewards. Across NYCC-derived matching and ranking tasks, our models significantly outperform strong multimodal baselines. Beyond accuracy, they generate  explanations that align with expert strategies and audience preferences. These results highlight humor as a powerful frontier for multimodal reasoning and demonstrate that combining explicit reasoning supervision with preference alignment offers a scalable path toward computational humor.", "tldr": "", "keywords": ["multimodal large language models", "humor understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63a49bd608b9222e3cdf56f6c1a6fe2d1a7030e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of teaching multimodal language models to understand humor, specifically through the New Yorker Cartoon Caption Contest (NYCC) as a testbed. It points out that prior work largely treats humor as black-box classification or preference prediction, overlooking the step-by-step reasoning processes that professional captionists employ.\n\nTo bridge this gap, the paper proposes a three-stage framework: continual pretraining on humor-focused corpora, supervised fine-tuning with captionist reasoning traces, and reinforcement learning guided by perceptual and stylistic rewards. \n\nThe approach achieves improvements over baseline multimodal models in caption matching and ranking tasks. Beyond accuracy, the system produces explanations aligned with expert strategies and audience preferences, showing a scalable path toward computational humor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation is clear, the paper frames humor as a frontier for multimodal reasoning.\n2.\tThis staged pipeline is well-justified in the proposed, combining pretraining, explicit reasoning supervision, and RL with humor-specific rewards. \n3.\tExperiments are thorough, including comparisons against strong baselines, with improvements across tasks"}, "weaknesses": {"value": "1. The reliance on curated datasets in CPT may raise questions about scalability and generalization to real-world use, especially for diverse humor styles. \n2.\tThe evaluation of RL rewards (e.g., perception and style judges) still depends on other LLMs using LLM-as-judge, which may introduce biases or inconsistencies, a justification would be needed.\n3.\tIt would be beneficial if more insights and theories from a cognitive or cultural view could be involved. \n4.\tFrom the experimental results shown in Table 1, closed models still outperform the proposed method in many cases, leaving the question of how far this approach can achieve and scale.\n5.\tCurrent human evaluation only includes 1 expert, which may be insufficient. Including broader human studies, especially those from diverse backgrounds, would be helpful.\n6.\tFurther improvement of the experiment could be considered, such as further computational cost analysis and error analysis.\n7.\t(minor) Several typos exist (e.g., line 961 misses the reference of equation), proofreading is needed."}, "questions": {"value": "1. How scalable and generalizable is the proposed method, especially under different context (e.g., cultural background, different groups of people) ?\n2. Could the authors give a justification for the correctness of LLM judge quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mGldvpvkMM", "forum": "TzqUDJZ23Q", "replyto": "TzqUDJZ23Q", "signatures": ["ICLR.cc/2026/Conference/Submission9836/Reviewer_TNmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9836/Reviewer_TNmA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797795533, "cdate": 1761797795533, "tmdate": 1762921315977, "mdate": 1762921315977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the task of multimodal humor understanding and introduces a training framework designed to enhance the reasoning capabilities of MLLMs in interpreting cartoons. The proposed framework comprises three stages: (1) continued pretraining on humor-related corpora for domain adaptation; (2) supervised fine-tuning (SFT) using synthetic reasoning traces generated by DeepSeek-R1 and GPT-4o to cultivate reasoning ability; and (3) GRPO-based reinforcement learning with grounded perception and stylistic rewards to further ehance the model’s humor reasoning. Experiments conducted on two humor understanding benchmarks (both requiring the model to match captions with corresponding cartoon images) demonstrate that the proposed method significantly outperforms several strong MLLM baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper investigates an important task of humor understanding. I believe this an essential direction, as the ability of current large models to comprehend human creative expressions remains underexplored yet highly significant.\n- The proposed framework is well-motivated and systematically designed, incorporating continued pretraining, supervised fine-tuning, and RL stages. Experimental results demonstrate the model’s strong performance across two humor understanding benchmarks.\n- Overall, the paper is clearly written, well-organized, and easy to follow."}, "weaknesses": {"value": "- My main concern lies in the novelty of the paper. The proposed training framework largely follows a conventional large-model training pipeline, comprising continued pretraining, supervised fine-tuning, and reinforcement learning, and primarily applies it to a new domain, namely humor understanding. As a result, the overall contribution feels somewhat incremental rather than fundamentally novel;\n- Regarding continued pretraining, Table A.1 shows that the dataset contains only 4,123 instances (fewer than 5 million words). Fine-tuning on such a small dataset may risk overfitting to this specific domain, leading to limited generalization. Moreover, the pretraining corpus appears highly similar to the NYT Cartoon dataset, which closely overlaps with the evaluation benchmark (e.g., the NYT Cartoon Benchmark). This overlap raises concerns about potential data leakage or domain bias. Additionally, humor understanding encompasses diverse subdomains (e.g., various comic genres as in [1,2]); hence, the proposed pretraining approach may not generalize well to humor datasets with significant domain shifts.\n- Concerning reinforcement learning, the motivation for including the style reward ($R_s$) is unclear. Since the task involves matching an image with an existing caption rather than generating the caption itself, it is not evident how stylistic rewards contribute to improving the model’s understanding of humor or visual-semantic alignment. On the contrary, such rewards might introduce additional inductive biases.\n- The benchmark coverage is limited. The evaluation focuses only on two datasets, both closely related to NYT-style cartoons. It would strengthen the work to assess performance on more diverse humor understanding benchmarks, such as [1] and [2].\n\n\n[1] Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions, NeurIPS 2024\n\n[2] Can Large Multimodal Models Uncover Deep Semantics Behind Images? ACL Findings 2024"}, "questions": {"value": "Q1. For continue pretraining: As the corpora are textual data, how do you conduct model training of MLLMs? Do you freeze the ViT and only update the language model component?\n\nQ2. Missing Citations on some recent humor understanding works, such as [1] and [2] mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PgJ5jKBdvH", "forum": "TzqUDJZ23Q", "replyto": "TzqUDJZ23Q", "signatures": ["ICLR.cc/2026/Conference/Submission9836/Reviewer_93PD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9836/Reviewer_93PD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804747573, "cdate": 1761804747573, "tmdate": 1762921315630, "mdate": 1762921315630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper does something interesting: instead of just having AI guess what's funny, it tries to teach it to think like a New Yorker captionist. The main contribution is shifting humour from a black-box guessing game to an interpretable, step-by-step reasoning task. The technical pipeline is typical and standard."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper doesn't just stop at which is funnier? It originally proposes teaching AI to mimic the human thought process. \n2. This paper isn't just about jokes; it's about explainable AI and tackling other creative tasks that need cultural context and subjective judgment.\n3. The technical pipeline is standard. The two RL rewards ($R_p$ and $R_s$) are especially clever. They quantify a fuzzy concept like good humour into optimizable signals for visual grounding and stylistic taste. The ablation studies (Tables 2 and 3) are well-executed and clearly demonstrate the contribution of each part.\n4. The paper is very clearly written. The introduction and abstract are concise, and the figures provide a clear overview. The authors are also very candid about the limitations in the Appendix."}, "weaknesses": {"value": "1. The SFT traces are not from human experts, but are synthetically generated by GPT-4o. This means the model is merely distilling one AI's (potentially flawed) reasoning, rather than learning from genuine human captionists. The RL reward signal is also not from human preference, but from an LLM-as-judge. This creates a closed loop where an AI is trained on an AI's data and judged by another AI, with no validation of whether this aligns with genuine human humour.\n\n2. For all its complexity, the final results in Table 1 are weak. The proposed model (60.33 on Matching) is dramatically outperformed by the closed o3 baseline (83.33). This >20-point gap calls into question the practical utility of this pipeline. \n\n3. The paper's claim to align with expert humour is severely undermined by its evaluation. The only human baseline mentioned is a single Human expert. This N=1 evaluation is statistically meaningless and lacks diversity. Judging something as subjective and culturally specific as New Yorker humour requires, at the very least, a diverse panel of human evaluators, not a single data point."}, "questions": {"value": "1. The SFT stage relies entirely on teacher traces generated by GPT-4o. Was any human-led quality control or evaluation performed on these synthetic traces? How do you know that GPT-4o's generated reasoning is high-quality and free of plausible-sounding nonsense?\n2. You benchmark against only one professional captionist. Could you consider including more evaluators? Additionally, a deeper quantitative or qualitative analysis is needed, for instance, to examine the differences between human and VLM humour comprehension and to identify which specific aspects of humour VLMs fail to grasp.\n3. Why did your Perception Reward ($R_p$) fail to correct the fundamental Viking/Grim Reaper error? Does this mean your $R_p$—which relies on text anchors—can only check for text-to-text consistency, not fix actual visual perception errors?\n4. Is it possible to experiment on more of the latest 7B models to see if this could bring different potential improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XmwzNCeFp5", "forum": "TzqUDJZ23Q", "replyto": "TzqUDJZ23Q", "signatures": ["ICLR.cc/2026/Conference/Submission9836/Reviewer_fKK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9836/Reviewer_fKK3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901979833, "cdate": 1761901979833, "tmdate": 1762921315302, "mdate": 1762921315302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a three-stage framework. First, continual pretraining guides the model’s representation space toward a specific sense of humor by feeding it curated humorous corpora. Second, the authors perform supervised fine-tuning using a constructed reasoning traces dataset to encourage captionist-style reasoning in the model. Finally, they apply reinforcement learning, introducing two additional rewards—visual perception and style—to further enhance the model’s ability to discriminate humor. This framework shifts the large language model from merely understanding humor through surface patterns to truly engaging in creative reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The three-stage framework is both novel and well-designed, effectively mimicking the reasoning patterns of human captionists, thereby enabling multimodal large models to acquire similar capabilities.\n- The reasoning trace dataset constructed by the authors, along with the visual reference used for reinforcement learning, played a significant role in improving the model, making it a substantial contribution to the work."}, "weaknesses": {"value": "- Typo in line 40 \"thrieves\"\n- The motivation is not clearly stated. Why is fine-tuning necessary? (See Questions.)\n- The experimental results do not appear to be the best. (See Questions.)"}, "questions": {"value": "- The paper uses a large language model to construct both the reasoning trace dataset and the visual reference. Since the model already demonstrates the ability to generate high-quality humorous captions through carefully designed prompts, why is fine-tuning still necessary? It seems that simply designing a method to guide the model would already be sufficient.\n- As shown from the experimental results, the proposed method does not achieve the best performance. Since some closed-source models like o3 already perform well enough, the motivation of this work appears less compelling. Could the authors provide further clarification on this point?\n- A minor suggestion is, you could use special markers or colors to help readers quickly grasp the key points in the presented experimental results, such as Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bwNw9amrzd", "forum": "TzqUDJZ23Q", "replyto": "TzqUDJZ23Q", "signatures": ["ICLR.cc/2026/Conference/Submission9836/Reviewer_QC7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9836/Reviewer_QC7F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964715778, "cdate": 1761964715778, "tmdate": 1762921314766, "mdate": 1762921314766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}