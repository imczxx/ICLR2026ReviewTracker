{"id": "57q8qtE5Ih", "number": 7995, "cdate": 1758049897533, "mdate": 1759897816500, "content": {"title": "Robust Direct Preference Optimization via Variational form f-divergence", "abstract": "Direct Preference Optimization is commonly deployed to align Large Language Models (LLMs) with human preferences, while such a technique also suffers from noisily annotated human preference. Existing robust approaches often require the knowledge of transition between clean and noisy human preferences, or leverage additional architecture/models to perform noisy human preference correction. In this work, we investigate when $f$-divergence is immune to the imperfect human preference annotations, by maximizing the $f$-divergence between noisy preferred and unpreferred data distributions. Theoretically, we show that when the noise ratio is known, the Total Variation formulation can serve as a surrogate for the clean dataset. In contrast, the Jensen–Shannon formulation is invariant to noise, yielding identical results under both noisy and clean preferences, even without knowledge of the noise rate. Empirically, the variational form of the Jensen–Shannon divergence enhances the model’s ability to generate preferred responses under noisy conditions, while simultaneously improving the factual accuracy of its outputs.", "tldr": "We investigate the robustness of certain variational form f-divergence when learning with noisy preference text data.", "keywords": ["Direct Preference Optimization", "$f$-divergence", "aligning with preference noise"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14edff3608c12b82627542ee3af2132927c0b242.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Noisy annotations in preference data can lead to learning undesirable behavior. Current methods such as Robust DPO mitigate such effects but require knowing the noise rate and being able to distinguish noisy vs. clean samples. Dr.DPO is another method which does not require noise rate estimation and is effective on both clean and noisy datasets. The authors introduce maximizing the f-divergence between preferred and unpreferred distributions and use this formulation to modify the loss function. They demonstrate improvements compared to baseline methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novelty - The paper introduces an original perspective that aims to robustly learn from preference data by framing the objective as f-divergence maximization between the preferred and unpreferred distributions. Under this setup, the authors demonstrate that different f-divergences have varying behavior for noisy data and show that the Jensen-Shannon divergence is invariant to preference noise. \n\nEmpirical Performance - The experiments demonstrate that the method can perform better than existing methods on multiple tasks suggesting the method may lead to more robust preference learning. The tasks include both preference accuracy and generation-based metrics on MTBench."}, "weaknesses": {"value": "Experimental Setup - The experimental setup could be more consistent across evaluations and provide further details. In particular, while evaluations on MT-Bench are based on Pythia models trained on HH-RLHF while the evaluations on TruthfulQA are based on LLaMa-2 models trained on UltraFeedback. The experiments would be significantly more informative if all evaluations were based on the same starting model trained on the same dataset. One useful detail that could be provided is the preference accuracy/downstream performance of methods when the noise rate is 0 so that for each instance, there is a clear starting point that allows for a straightforward comparison of how much the performance drops once noise is introduced. Based on the current results, the generality of the method’s benefits seems unclear. \n\nTechnical Clarity - The technical ideas presented pass over significant details and also contains incorrect statements. In particular, parts of the derivation of the method and modified loss function should be further discussed. For example, in lines 196-198, it is unclear what is meant by the variational representation being an empirical estimate in contrast to a strict equivalence and why this is a reasonable choice to make. Some technical details that require correcting or are missing:\n- Lines 31-32: The reward model itself is not trained with PPO. \n- Lines 149-151: The Z’s are not defined. \n- Lines 191-192: Minimizing the DPO loss is not equivalent to point-wise maximizing the implicit reward and furthermore, the DPO loss is derived from reward maximization with KL regularization used to limit the extent to which reward is maximized.\n- Lines 202-203: D_l is not defined.\n\nImpact - The work aims to build on robust preference learning methods and in particular mentions Dr.DPO as an effective method that does not require noise rate estimation but lacks rigorous theoretical justification regarding its robustness. However, while the work demonstrates some improvements over Dr.DPO, the improvements are shown on limited settings and the work does not provide a rigorous theoretical justification for the objective. In particular, while the works demonstrates that the f-divergence itself is robust to noise, the work does not address properties of the final resulting objective such as explaining the choice of log-sigmoid, providing a gradient analysis, or deriving its optimal policy. \n\nActionable changes:\n- Providing further explanation on the derivation of the final objective and its properties\n- Clarifying definitions and steps in the derivation and updating incorrect statements\n- Providing experimental results under a consistent setup with the same base model and dataset\n- For results across noise rates, providing performance at a noise rate of 0 for a clear baseline"}, "questions": {"value": "Questions:\n- Could you explain the choice of having the objective be negative log sigmoid of beta times VF term?\n- A follow up question I have is I'm not certain that maximizing the f-divergence term necessarily means learning correct preferences. Couldn't it also be maximized by learning incorrect preferences? I would appreciate some clarification here as well. \n\nSuggestions are listed in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bVEMzE9BSi", "forum": "57q8qtE5Ih", "replyto": "57q8qtE5Ih", "signatures": ["ICLR.cc/2026/Conference/Submission7995/Reviewer_drKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7995/Reviewer_drKW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431762842, "cdate": 1761431762842, "tmdate": 1762919999049, "mdate": 1762919999049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to examine how the properties of f-divergence can improve the performance of DPO with noisy labels. The authors present a theoretical analysis of a DPO like loss function based on f-divergence. They analyze the properties of the variational form of the loss function and show that the Total Variation form of f-divergence is useful when the noise rate is known while the JS divergence is invariant to noise. In empirical studies, they show that the reformulated loss function based on DPO performs better on HH-RLHF and UltraFeedback datasets in comparison to several strong baselines"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a novel analysis - reformulating the DPO loss function based on f-divergence, it's theoretical properties under noisy conditions.\n2. The experiments show that this loss function shows better performance over other baselines at diverse noise levels"}, "weaknesses": {"value": "1. There is room for improvement in presentation of the theoretical proofs. Particularly in sections 2 and 4, the equations are not numbered and it is difficult to follow how different equations connect to one another. Furthermore, there is no intuition or analytical understandings given which explains why the reformulated loss function shows better performance in comparison to the baselines\n2. The authors do not discuss the additional computational needs from variational optimization which is not needed in the case of the baselines\n3. The experiments are limited to English-only text tasks with mid-sized models. Since different loss functions are being examined with respect to noisy labels, experiments with larger models and other tasks will solidify the paper."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "16SA9NybFq", "forum": "57q8qtE5Ih", "replyto": "57q8qtE5Ih", "signatures": ["ICLR.cc/2026/Conference/Submission7995/Reviewer_3Kse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7995/Reviewer_3Kse"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849668045, "cdate": 1761849668045, "tmdate": 1762919998707, "mdate": 1762919998707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies on direct preference optimization and proposes a novel strategy with the introduction of f-divergence. \n\nWhile the topic is important and the paper presents a method that is conceptually reasonable, I have significant concerns on the novelty, motivation and the experimental evaluation. As a result, I assign a score of 4."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThis work studies on an important problem.\n2.\tThe proposed method appears to be reasonable.\n3.\tExtensive experiments on real-world datasets have been conducted to verify the efficacy of the proposed method."}, "weaknesses": {"value": "1.\tMy major concern lies on the novelty. The use of f-divergence in DPO has already been extensively explored in recent studies, like [a1][a2]. This work does not adequately review these contributions, nor does it clearly explain how the proposed method differs from them. \n\n2.\tMy another concern lies on the unclear motivation: 1) While the paper claims benefits from introducing f-divergence, these benefits appear to arise mainly from the use of JS-divergence, which is also the only form implemented in the experiments. 2) Existing denoising DPO strategies are not sufficiently compared, and it remains unclear why and how the proposed method would outperform them. 3) The theoretical analysis relies heavily on the assumption that e_w$=$e_l$,  whose validity should be justified. \n\n\n3.\tThere are also some concerns on the experiments: 1) Some important baselines like [a1][a2] are missing  from the comparisons. 2) Experiments are conducted only on a relatively small LLM, Pythia-2.8B. Testing on larger and more diverse models would better demonstrate scalability and effectiveness. 3) Evaluation is limited to two benchmarks. Incorporating additional datasets would strengthen the empirical evidence.\n\n[a1] AISTATS’25: f-PO: Generalizing Preference Optimization with f-divergence Minimization\n[a2] iclr’25: beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lx1xTtPZZu", "forum": "57q8qtE5Ih", "replyto": "57q8qtE5Ih", "signatures": ["ICLR.cc/2026/Conference/Submission7995/Reviewer_5ZUj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7995/Reviewer_5ZUj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974405380, "cdate": 1761974405380, "tmdate": 1762919998082, "mdate": 1762919998082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper use f-DPO loss function (which is already accepted in ICLR 2024 [1]), to handles noisy human preference annotations when aligning Large Language Models. Standard DPO suffers from noisily annotated human preferences, and existing robust approaches either require knowledge of the noise transition rates or need additional models for correction.\nThe paper investigates when f-divergence is immune to imperfect preference annotations by maximizing f-divergence between noisy preferred and unpreferred data distributions. When noise ratio is known, the Total Variation formulation can be transformed to represent the clean dataset with a multiplicative factor (1-2ef). The Jensen-Shannon formulation is invariant to noise - it yields identical results under both noisy and clean preferences, even without knowing the noise rate\nExperiments on HH-RLHF and UltraFeedback datasets with MT-Bench and TruthfulQA benchmarks show that the JS variational form: 1) Enhances ability to generate preferred responses under noisy conditions; 2) Improves factual accuracy of outputs; 3. Consistently outperforms baselines (DPO, cDPO, rDPO, IPO, Dr.DPO) across various noise levels.\n\n[1] Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper makes a genuine theoretical contribution by analyzing the behavior of f-divergence variational forms under noisy preference data.\n2. Noisy human preference annotations are a real and costly problem in RLHF. The paper's finding that JS divergence naturally provides noise robustness without requiring noise rate estimation or auxiliary models is practically significant."}, "weaknesses": {"value": "## 1. Insufficient Credit to Wang et al. (2023)\n\n**Problem:** The paper adopts the f-divergence variational framework from Wang et al. (2023) but provides inadequate attribution.\n\n**Evidence of Equivalence:**\n- **Robust DPO (this paper):** $\\mathcal{L}_{JS}(\\theta) = -\\log \\sigma(\\beta \\cdot VF_{JS}(\\theta, g^*))$\n- **Beyond Reverse KL (Wang et al.):** $\\mathcal{L}(\\theta) = -\\log\\sigma\\left(\\beta f'\\left(\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}\\right) - \\beta f'\\left(\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right)$\n- **Proof:** $g^*(v) = f'(e^v) = \\log \\frac{2e^v}{1+e^v}$ → **Same loss function, different notation**\n\n## 2.  Undefined Symbols\n- Line 117: $\\beta, \\pi, \\theta$ not defined\n- Line 122: $\\beta$ definition missing; $r(x,y)$ needs explanation\n- Line 144: Clarify if $x$ is the same as Line 111\n\n## 3. Errors and Typos\n- Line 118: $(x, y)$ should be $(y_w, y_l)$\n- Line 119: \"SFT policy\" should be \"reference policy\"\n- Line 183: $\\log\\sigma$ is informal notation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bNLXHGL0o5", "forum": "57q8qtE5Ih", "replyto": "57q8qtE5Ih", "signatures": ["ICLR.cc/2026/Conference/Submission7995/Reviewer_ogr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7995/Reviewer_ogr5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990278721, "cdate": 1761990278721, "tmdate": 1762919997597, "mdate": 1762919997597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}