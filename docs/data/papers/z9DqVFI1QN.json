{"id": "z9DqVFI1QN", "number": 19265, "cdate": 1758294898589, "mdate": 1759897048729, "content": {"title": "VP-MonoMF: Visual Prompt Guided Monocular 3D Object Detection with Multiscale Fusion", "abstract": "Depth estimation from a single image remains a challenging task in monocular 3D object detection. Existing methods improve the detection accuracy by leveraging more precise 2D and 3D information. However, they simultaneously train 2D and 3D detection branches, which inevitably affect each other. Meanwhile, they often overlook the adverse effects caused by variations in camera pose. Furthermore, although they achieve satisfactory detection accuracy on large objects, their accuracy on small objects remains limited due to limited pixel areas. To address these issues, we propose a Visual Prompt Guided Monocular 3D Object Detection Method with Multiscale Fusion (VP-MonoMF). Specifically, we first develop a Multi-Depth Fusion (MDF) module as the 3D detection branch, which integrates multi-scale information from both global depth maps and local 3D depth information. Then, we train MDF in the first stage and the 2D Detector in the second stage to mitigate mutual interference. To minimize the impact of the camera pose variance, MDF utilizes a 3D Depth Reconstruction (3DR) module to correct depth map deviations. Furthermore, we introduce a Visual Prompt Fusion (VPF) module to enhance small object features by adaptively adjusting weights based on object size. We conduct experiments on the KITTI dataset. VP-MonoMF achieves state-of-the-art performance in monocular 3D object detection task. The code will be\nmade available upon acceptance of the paper.", "tldr": "Explores GT-based visual prompt for monocular 3D object detection with Multiscale Fusion.", "keywords": ["monocular 3D object detection", "multiscale fusion", "visual prompt", "depth fusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d94576fc467c90d686016a97791db5d149de736.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a monocular 3D object detection method, VP-MonoMF, which mitigate the mutual interference between 2D and 3D detection branches by training them in separate stages. In addition, the method incorporates a depth reconstruction module to correct camera pose. To further enhance the detection of small objects, it employs a visual prompting mechanism that adaptively adjusts feature weights based on object size. The proposed approach achieves competitive performance on the KITTI 3D detection benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The method demonstrates solid performance on the KITTI dataset.\n- It introduces a visual prompting mechanism that adjusts feature weighting based on object size, improving detection accuracy for small objects."}, "weaknesses": {"value": "1. **Questionable Motivation**  \n   - The claim that joint training of 2D and 3D detection branches leads to negative interference is not well justified.  \n   - Prior studies have shown that joint optimization can facilitate mutual benefits between 2D and 3D tasks.  \n   - The rationale for completely separating the two branches therefore appears unconvincing.\n\n2. **Unclear Methodology and Contradictory Logic**  \n   - Several parts of the paper are vague and difficult to follow.  \n   - **Figure 1** lacks clarity: if the camera pose changes, both the depth map and the 3D bounding boxes should vary accordingly. As a teaser figure, it fails to highlight the core problem or main innovation.  \n   - The term **“3D depth information”** (Lines 71 and 73) is ambiguous, it is unclear whether it refers to estimated depth, ground-truth depth, or a learned feature representation.  \n   - The workflow is logically inconsistent: although the authors claim that 2D and 3D branches interfere with each other, the second stage trains the 2D detector using enhanced features derived from 3D training.  \n   - The decision to train **3D first, then 2D** is not adequately justified. The alternative order (2D → 3D) is not explored.\n\n3. **Limited Novelty**  \n   - The paper reads more like a system description than a research paper. The overall design appears complicated rather than conceptually elegant. The proposed method primarily integrates several complex components (e.g., DLA, MDF, VPF) rather than introducing a fundamentally new concept.  \n\n4. **Poor Readability**  \n   - Numerous abbreviations (MDF, 3DR, VPF, etc.) are overused throughout the paper, reducing readability.  Subsection titles are vague and do not clearly convey their content.\n\n5. **Overclaimed Results**  \n   - The abstract asserts state-of-the-art performance, but Table 1 shows that some results are not superior to existing approaches.\n\n6. **Insufficient Evaluation**  \n   - Experiments are conducted solely on the KITTI dataset.  Evaluation on additional benchmarks such as nuScenes would be needed to validate the generalization capability."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kq4V9Mvnbo", "forum": "z9DqVFI1QN", "replyto": "z9DqVFI1QN", "signatures": ["ICLR.cc/2026/Conference/Submission19265/Reviewer_TRTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19265/Reviewer_TRTi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709400692, "cdate": 1761709400692, "tmdate": 1762931230705, "mdate": 1762931230705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VP-MonoMF, a novel two-stage framework designed to improve accuracy and robustness for monocular 3D object detection. The proposed method consists of three main modules: 3D Depth Reconstruction (3DR), Multi-Depth Fusion (MDF), and Visual Prompt Fusion (VPF). The first stage trains the 3D branch including MDF, which estimates global depth map and local 3D depth information per object. The subsequent second stage trains the 2D detector using VPF module, which enhances the small object features by adaptively adjusting attention weights based on GT object size."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Unfortunately, none."}, "weaknesses": {"value": "1. [Novelty] 3D object detection is a very mature task and field. There are many existing models and methods that perform much better than the proposed method, but they are neither cited nor referred.\n2. [Benchmark] KITTI benchmark is a small and outdated dataset and no longer used in the field of 3D object detection. nuScenes is the minimum standard benchmark for 3D object detection.\n3. [Performance Comparisons] The study does not compare with SOTA models.\n3. [Method] Uses a combination of outdated methods and models. A complex method with limited performance."}, "questions": {"value": "1. The introduced multi-task losses (equation 11, 15) appear to be a simple sum of their components, implying a uniform weight of 1.0 for each. Is this correct, or were different balancing weights (hyperparameters) used? If so, have you experimented with different weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RCsPJWwCer", "forum": "z9DqVFI1QN", "replyto": "z9DqVFI1QN", "signatures": ["ICLR.cc/2026/Conference/Submission19265/Reviewer_2uNn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19265/Reviewer_2uNn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925942832, "cdate": 1761925942832, "tmdate": 1762931230295, "mdate": 1762931230295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1 The paper proposes VP-MonoMF, a two-stage monocular 3D object detection method that integrates Multi-Depth Fusion (MDF), 3D Depth Reconstruction (3DR) for camera pose correction, and Visual Prompt Fusion (VPF) to enhance small-object features using GT-guided adaptive attention.\n\n2 It evaluates performance on the KITTI benchmark, reporting AP3D and APBEV at IoU=0.7 for Car under Easy/Mod/Hard settings, achieving SOTA results especially on Mod/Hard splits, and includes additional results on nuScenes.\n\n3 Ablation studies validate each component (MDF, 3DR, VPF), depth fusion strategies, and the two-stage training scheme, demonstrating consistent improvements; experiments appear comprehensive and well-structured.v"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated and technically sound approach to monocular 3D object detection, with clear strengths in originality—particularly through the novel integration of visual prompting guided by ground-truth object size, a two-stage training strategy to decouple 2D and 3D learning, and explicit handling of camera pose variation via 3D depth reconstruction. The method is thoroughly evaluated on KITTI and nuScenes, supported by comprehensive ablation studies that validate each component. The writing is clear, the problem formulation is realistic and addresses practical limitations of prior work, and the reported SOTA results—especially on small and hard objects—highlight its significance for real-world autonomous perception."}, "weaknesses": {"value": "1 The paper’s reliance on ground-truth object size to generate the visual prompt limits its practical applicability, as GT is unavailable during inference.\n\n2 The description of the Pose Detector and 3DR module lacks sufficient architectural and implementation details (e.g., how vanishing point estimation is trained or validated), making it hard to assess robustness or reproduce the camera pose correction.\n\n3 While KITTI results are strong, experiments on nuScenes are only in supplementary material and lack per-category or small-object-specific metrics, leaving uncertainty about whether gains truly generalize beyond KITTI’s Car class and moderate/hard splits."}, "questions": {"value": "Similar to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tICmjVcvzf", "forum": "z9DqVFI1QN", "replyto": "z9DqVFI1QN", "signatures": ["ICLR.cc/2026/Conference/Submission19265/Reviewer_BGc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19265/Reviewer_BGc9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937307756, "cdate": 1761937307756, "tmdate": 1762931229583, "mdate": 1762931229583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}