{"id": "Bx7qVi0aVC", "number": 5167, "cdate": 1757858991716, "mdate": 1759897990826, "content": {"title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "abstract": "Neuromorphic event cameras possess superior temporal resolution, power efficiency, and dynamic range compared to traditional cameras. However, their asynchronous and sparse data format poses a significant challenge for conventional deep learning methods. Existing solutions to this incompatibility often sacrifice temporal resolution, require extensive pre-processing, and do not fully leverage GPU acceleration. Inspired by word-to-vector models, we draw an analogy between words and events to introduce event2vec, a novel representation that allows neural networks to process events directly. This approach is fully compatible with the parallel processing and self-supervised learning capabilities of Transformer architectures. We demonstrate the effectiveness of event2vec on the DVS Gesture, ASL-DVS, and DVS-Lip benchmarks. A comprehensive ablation study further analyzes our method's features and contrasts them with existing representations. The experimental results show that event2vec is remarkably parameter-efficient, has high throughput, and can achieve high accuracy even with an extremely low number of events. Beyond its performance, the most significant contribution of event2vec is a new paradigm that enables neural networks to process event streams as if they were natural language. This paradigm shift paves the way for the native integration of event cameras with large language models and multimodal models. Code, model, and training logs are provided in https://anonymous.4open.science/r/event2vec_iclr-7B40.", "tldr": "", "keywords": ["Neuromorphic computing", "event camera", "event based vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55852272d8293dd41209917d5566504bfaeb2618.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Event2Vec, a novel representation that encodes asynchronous events into vector space. By analogizing events to words in NLP, the authors propose spatial and temporal embedding modules that capture local structure and temporal dynamics, facilitating direct processing of event data using Transformer architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The conceptual analogy between discrete language tokens and asynchronous neuromorphic events is both novel and compelling. It reframes event data not as irregular signals to be normalized, but as sequences to be understood in context.\n2. The separation of spatial and temporal embeddings with inductive biases (e.g., spatial locality via parametric functions, temporal convolution on $\\Delta t$) is methodologically solid.\n3. Event2Vec achieves high throughput and minimal preprocessing time (especially with random sampling), showing practical relevance for real-time systems."}, "weaknesses": {"value": "1. **Limited Task Diversity and Oversimplified Evaluation Benchmarks**:\nThe evaluation is confined to classification benchmarks. Given the strong NLP-inspired framing, it is disappointing that no sequence-centric assessments are included. Applications such as event-to-text generation, predictive modeling, or alignment with natural language would more fully test the paradigm. Moreover, the strong performance on simple datasets may thus reflect dataset limitations rather than model expressiveness. Additional experiments on more demanding benchmarks would be necessary to validate the model‚Äôs broader applicability.\n2. **Preprocessing bottlenecks**:\nWhile random sampling is fast, the K-Means variant‚Äîused for higher accuracy on DVS-Lip‚Äîincurs substantial latency. The trade-off undermines claims of efficiency, especially since no runtime adaptivity or approximations are proposed to mitigate the cost.\n3. **Limited Robustness Analysis**:\nThe paper lacks evaluation under domain shifts (e.g., varying sensor resolutions, noisy events, different motion patterns). Given the sparse and sensitive nature of event data, robustness is critical."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nQyAHURX4M", "forum": "Bx7qVi0aVC", "replyto": "Bx7qVi0aVC", "signatures": ["ICLR.cc/2026/Conference/Submission5167/Reviewer_fgEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5167/Reviewer_fgEv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844780433, "cdate": 1761844780433, "tmdate": 1762917924888, "mdate": 1762917924888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all reviewers"}, "comment": {"value": "# Fix Table 1\n\nIn Table 1, we wrongly filled in the accuracy and parameters of `ResNet-18, BiGRU + Frame`. And the params (MB) of the first two rows should multiply  4, because the original data from Table 1 in (Dampfhoffer & Mesquida, 2024\\) are `the number of params` rather than `params`. The relations between them are: `params (MB) = the number of params (MB) * 4` because a float32 value occupies 4 bytes. Thus, our method is only 0.16% lower than theirs, but it only uses 8.18% params of their. After fixing the errors, the parameter efficiency of the method proposed in this paper has been further verified.\n\n**Table R1: Comparison on the DVS Lip Dataset**\n\n| Dataset | Method \\+ Representation                                     | Accuracy (%)                             | Params (MB) |\n| ------- | ------------------------------------------------------------ | ---------------------------------------- | ----------- |\n| DVS-Lip | ResNet-18,BiGRU \\+ Frame (Tan et al., 2022\\)                 | 72.1                                     | 241.2       |\n|         | Spiking ResNet18,BiGRU \\+ Frame (Dampfhoffer & Mesquida, 2024\\) | 75.3                                     | 223.63      |\n|         | Linear Attention \\+ Event2vec (1024 Randomly Sampled Events) | 70.62¬±1.55                               | 18.3        |\n|         | Linear Attention \\+ Event2vec (1024 Cluster Events)          | 75.14, 75.38 (with normalized intensity) | 18.3        |\n\nIn addition, as suggested by Reviewer EEdR, we normalize the intensity in accordance, and our new model achieves an accuracy rate of 75.38%, outperforming the performance of `Spiking ResNet18,BiGRU + Frame (Dampfhoffer & Mesquida, 2024)`.  We also compare the training costs and find that our method only uses 41% of the memory consumption and 6.3% of the GPU training time of their method.\n\n# Fair comparison with data augmentation\n\nTransformers (including linear attention) lack inductive bias and thus require more data for learning. We used data augmentation methods to expand the data volume to a certain extent, thereby improving performance. Specifically, we did not use data augmentation on the ASL-DVS dataset because we found that SOTA (State-of-the-Art) performance could be achieved without it‚Äîthis is likely due to the sufficient scale of this dataset: the number of samples in its training set is approximately 80,640, while that of DVS Gesture is 1,176, and DVS Lip is 14,896.\n\nWe clearly provide all the details of data augmentation, as well as complete code, hyperparameter configuration files, and training logs to facilitate readers' reproduction. Most of the previous studies also used data augmentation. Since we have not proposed a plug-and-play module, such as a new network layer or learning algorithm, but rather a brand-new framework, we believe it is reasonable to use data augmentation and compare it with existing SOTA (State-of-the-Art) studies.\n\nThe following Table R2 are the data augmentation methods used in the articles in Table 1\\. It can be observed that all existing methods utilize data augmentation.\n\n**Table R2: Data augmentations of methods in Table 1**\n\n| Dataset     | Method \\+ Representation             | Data Augmentation                                            |\n| ----------- | ------------------------------------ | ------------------------------------------------------------ |\n| DVS Gesture | DVS Gesture Sparse GRU \\+ Frame      | Random crop, translation, and rotation                       |\n|             | SNN \\+ Frame                         | Random slice and integrate                                   |\n|             | FARSE-CNN \\+ Window Slicing          | Random coordinate translations                               |\n|             | Event MAE \\+ Point Cloud             | Point resampling from Point-BERT (CVPR 2022\\)                |\n|             | Linear Attention \\+ Event2vec        | Random resize, rotation, shear, translate, erase, and chunk dropout |\n| ASL-DVS     | GNN,CNN \\+ Graph                     | Random scale, flip, and rotation of  node positions          |\n|             | GNN,Transformer \\+ Image,Voxel Graph | Random scale and translate                                   |\n|             | Ours                                 | None                                                         |\n| DVS-Lip     | ResNet-18,BiGRU \\+ Frame             | Random crop and horizontal flip                              |\n|             | Spiking ResNet18,BiGRU \\+ Frame      | Random crop, horizontal flip, spatial masking, zoom, and temporal mask |\n|             | Linear Attention \\+ Event2vec        | Random resize, rotate, shear, flip, translate, and erase     |"}}, "id": "y0LBxlnrSQ", "forum": "Bx7qVi0aVC", "replyto": "Bx7qVi0aVC", "signatures": ["ICLR.cc/2026/Conference/Submission5167/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5167/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5167/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763761115353, "cdate": 1763761115353, "tmdate": 1763761890056, "mdate": 1763761890056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by word embeddings in NLP tasks, the authors designed event2vec to convert event data into vector representations, thereby introducing a novel way to represent event streams.\nThe proposed method embeds the event stream from both temporal and spatial dimensions, fully exploiting the characteristics of event flows. In Transformer-based experiments, the authors demonstrated that event2vec improves performance across multiple datasets while also providing an analysis of latency and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficiency and Parameter Compactness. Experimental results show that Event2Vec maintains high accuracy while keeping the number of model parameters extremely small.\n2. Temporal Generalization Ability. By employing a convolutional time embedding based on the relative time difference Œît, the model achieves time translation invariance, enabling it to generalize effectively across temporal shifts."}, "weaknesses": {"value": "1. Limited Cross-Dataset Generalization. The model still requires separate training or fine-tuning on different datasets (such as DVS Gesture, ASL-DVS, and DVS-Lip), indicating that its zero-shot transfer capability remains weak.\n2. Sensitivity to Clustering or Sampling Strategies. Because event stream lengths vary greatly, Event2Vec still relies on sampling or clustering to control input length. However, inappropriate strategy choices may lead to information loss or performance instability.\n3. Limited Generalization and Heavy Preprocessing Requirements. Event2Vec demands extensive preprocessing, making it less suitable for real-time event streams or DVS-based video data, where its adaptability and efficiency are significantly constrained."}, "questions": {"value": "See the section of weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bopqxkwD9k", "forum": "Bx7qVi0aVC", "replyto": "Bx7qVi0aVC", "signatures": ["ICLR.cc/2026/Conference/Submission5167/Reviewer_E7eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5167/Reviewer_E7eu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922378555, "cdate": 1761922378555, "tmdate": 1762917924607, "mdate": 1762917924607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the incompatibility between the asynchronous and sparse event streams output by event cameras and mainstream deep learning paradigms, this paper proposes a representation method named Event2Vec. It draws an analogy between events and words in natural language, enabling direct processing of events by embedding them into a vector space. The core of the method includes parametric spatial embedding (which incorporates neighborhood similarity bias) and convolutional temporal embedding based on temporal differences, supporting random sampling or k-means clustering sampling to fix the sequence length. Experiments on the DVS Gesture, ASL-DVS, and DVS-Lip datasets verify its advantages: high parameter efficiency, fast preprocessing, high throughput, and robustness with an extremely low number of events. Its accuracy is comparable to that of representative methods. The authors emphasize that this method provides a new paradigm for event stream processing, facilitating integration with Transformers and self-supervised learning, and paving the way for multimodal fusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel concept and clearly explains its motivation through an analogy between events and words. The spatial embedding employs a parametric network that explicitly injects neighborhood similarity bias, and the visualizations demonstrate smooth gradients. For temporal modeling, it uses differential convolution, which ensures time-shift invariance and local contextual consistency. From an engineering perspective, the approach offers extremely low preprocessing latency and high training and inference throughput, significantly outperforming voxel and point cloud-based methods. It also maintains reasonable accuracy even with very few events, showing strong robustness. In addition, the model supports self-supervised pre-training, and its performance further improves after fine-tuning on DVS-Lip, demonstrating good extensibility."}, "weaknesses": {"value": "The paper lacks sufficient comparisons with the strongest baselines under unified settings. For example, although the 99.91% accuracy on ASL-DVS is high, the accuracy on DVS-Lip only reaches 75.14% through clustering and pre-training, which is lower than the 75.3% achieved by the SNN + Frame method. There is no fair ablation study conducted under the same backbone and resource conditions. The k-means clustering sampling process also introduces high preprocessing time, which may limit its applicability in online scenarios. The temporal modeling relies solely on first-order differential convolution, and the verification for long-range dependencies remains insufficient. The task evaluation is primarily focused on classification, without evidence of effectiveness in multi-task scenarios such as detection or reconstruction. The ablation experiments are limited in scope and fail to thoroughly examine the robustness of the intensity factor ùúå, attention variants, or sampling strategies. Moreover, the transparency of reproducibility details, including training time and sources of variance, should be improved."}, "questions": {"value": "Does parameter sharing limit expressiveness, and what are the comparison results with bidirectional models that do not use parameter sharing? \n\nWhat is the sensitivity of the intensity factor $\\(\\rho\\)$ to distribution, and what impact would it have if $\\(\\rho\\)$ is treated as an additional channel or processed through normalization? \n\nIn real-time applications, what are the specific end-to-end latency and throughput of clustering and random sampling, and is it possible to integrate clustering into a trainable module? \n\nCan Event2Vec achieve zero-shot transfer to tasks such as optical flow or depth estimation, or align with large language models (LLMs) to implement multimodal tasks? \n\nWhy is data augmentation not used for ASL-DVS while multiple data augmentations are applied for DVS Gesture, and does this affect the fairness of comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4QooYgXUrw", "forum": "Bx7qVi0aVC", "replyto": "Bx7qVi0aVC", "signatures": ["ICLR.cc/2026/Conference/Submission5167/Reviewer_EEdR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5167/Reviewer_EEdR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979737080, "cdate": 1761979737080, "tmdate": 1762917923551, "mdate": 1762917923551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes event-2-vec for event-based cameras that convert each event into vectorized representations, added with positional encoding terms from the temporal side of the input, using temporal differences. Motivated from NLP based inputs to transformers which are vectorized with added positional encoding, the authors follow a similar structure here. The resulting approach is more parameter-efficient and experiments across three datasets show decent performance. Attention maps are shown to highlight the explainability of the method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is written well overall\n2. Attention maps of their approach are highlighted, which adds good explainability of the proposed approach. \n3. Parameter efficiency is good."}, "weaknesses": {"value": "Ambiguous use of words to describe the datasets and some of the stats (single stream vs samples; throughput etc.) Proper definitions would be good for ease of readability. Otherwise the reader will need to infer things themselves, as I had to. There are still some questions at large.  \n\nUltimately what matters is single-stream latency, as in the wild a camera won't be processing 1000 sequences at once when live, but just one. From that perspective, due to the transformer's bulk and large input context, I see that the authors' approach is actually slower than others except the FARSE-CNN approach. This significantly changes the narrative. The parameter efficiency is not useful if the actual deployment latencies are larger than other approaches, while the performance being worse (e.g. SNN+ Frame exhibits almost half latency while being more accurate on DVS-Gesture). Also, it is not clear if the single-stream latency also includes the pre-processing of those events within the stream. Even if it does not, I note that the results in Table 2 essentially suggest that the pre-processing time for the first four approaches are negligible, as these are \"total\" times across 1000+ total samples, which significantly reduces the pre-processing time per-sample. \n\nI think the temporal embedding here is a bit of an issue. I feel the authors' tried a bunch of encodings (which is demonstrated in Tab. 4) and the narrative seems to have been shaped around what empirically worked. I was not convinced by 3.3, as it seemed that due to the fact that time can be open-ended (and thus the risk of \"out-of-distribution\") to ensure it is not, the authors picked some range limiting approaches and went with what worked best. Their final choice also does not align well with the original NLP based motivation (lines 71-92). In the transformer context a \"position encoding\" is usually given to the information corresponding to a input's position, and usually uses sin/cosine embeddings or even learnable ones. So the natural choice for PEs in this context would also be sin and cosine (as the authors have indeed explored). Lines 71-92 motivates either a direct normalized map of times or a function imposed on them (sin/cosine). However, the authors go with a function of the time difference of consecutive events, and for me the motivation for this is lacking. Firstly, having worked in this space, I'm aware of the quite random nature of the arrival of events, and I'm not sure how interpretable or sensible the consecutive temporal difference of two events would be, as the events themselves could come from very different parts of the input. I suppose what is happening is that in such datasets (like DVS-Gesture), the temporal difference embedding is essentially just capturing the rate at which the events are happening, which causally links to the current \"speed\" of the motion. Faster motion will simply yield (on average) lower $\\Delta t$ and vice-versa. That to me makes sense more than what the authors' outline in 3.3, which is temporal shift invariance and the role of convolutions. I also do not see the actual utility of using a convolutional encoder, as the authors seem to be working with a single $\\Delta t$ input to the conv-encoder in Figure 2 anyway (please correct me if I'm wrong). Either way, I don't see the proper motivation behind using a convolutional encoder versus any other from the authors' motivations. The only way it makes sense to me is perhaps from the fact that conv architectures are more parameter efficient. The design choices in Figure 2 appear somewhat ad-hoc as well. \n\n\nFollowing up on the previous point, this leads to one of the core issues for me in this work. The authors' main motivation is to avoid dense event-to-frame representations which can lead to \"degradation or complete loss of high temporal resolution\" (line 124). However, given the relatively noisy nature of these cameras in capturing the events, and given the random nature of the consecutive temporal event differences other than just giving a rough indication of the degree of motion overall in the scene, it is not clear to me how the framework resolves the aforementioned issues without creating these new ones. Excessive dependence on temporal information with event-based cameras can be counter-productive, and that is why framing/aggregating has happened in the first place with most works in literature. Lastly, I note that even in this work, where not sacrificing temporal information is key, the authors inevitably end up using an aggregation approach. The motivation is cited as maintaining the fixed context length L, but I believe aggregation also yields the necessary invariance to noisy local temporal information. The clustering approach mentioned here essentially does that, while the random sampling approach seems to lose information as it is subsampling the events. \n\n\nI note that the authors indeed follow a very rigorous data augmentation procedure for all datasets, however, I don't see any references for the set of augmentations used, so it is not clear whether accuracy wise all the quoted methods are being compared on equal grounds, as data augmentation can significantly enhance accuracy. \n\n\nI feel the experiments would benefit from more baselines, as DVS-Gesture is one of the oldest datasets and there are many approaches out there. A survey from 2023 outlines higher sota accuracy (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10298106) than the methods discussed in the paper."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qAvLPErQew", "forum": "Bx7qVi0aVC", "replyto": "Bx7qVi0aVC", "signatures": ["ICLR.cc/2026/Conference/Submission5167/Reviewer_4MEw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5167/Reviewer_4MEw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982764935, "cdate": 1761982764935, "tmdate": 1762917923210, "mdate": 1762917923210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}