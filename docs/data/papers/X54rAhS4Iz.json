{"id": "X54rAhS4Iz", "number": 1603, "cdate": 1756896722753, "mdate": 1763683241449, "content": {"title": "Rate-Distortion Optimization for Transformer Inference", "abstract": "Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.", "tldr": "", "keywords": ["information theory", "learning theory", "lossy learnable compression", "transformers", "language models", "usable information", "neural networks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed65f3ea19ee13787f2b9372f062d1bd1e760a68.pdf", "supplementary_material": "/attachment/21920cd23174d46b86249bd3acf439a5326e9d1d.pdf"}, "replies": [{"content": {"summary": {"value": "The authors address the problem of rate–distortion optimization within the framework of transformer inference. They propose a new entropy model and introduce a theoretical framework based on the v-entropy gap and its bound."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The topic is interesting and relevant, and the theoretical analysis has potential."}, "weaknesses": {"value": "The paper suffers from several critical issues that need to be addressed before it can be considered for publication. \n- The motivation of the paper is not sufficiently grounded in real-world applications. The practical context in which the proposed problem formulation becomes relevant should be clarified. The authors should provide concrete examples of situations where rate–distortion optimization for transformer inference arises in practice. For instance, this topic appears closely related to federated learning and distributed inference, where lossy compression is often employed to reduce communication costs. The authors are encouraged to make this connection explicit and cite related references to position their contribution within this broader context. Another example: In Section 3.1, the paper states: \"We propose a transformer-based entropy model that does not have direct access to elements in Y, requiring more support from the hyper-prior.\" This point requires further explanation. Why is this assumption made? Does it model a more challenging or more realistic scenario? Clarifying the motivation and implications of this design choice would help readers better understand its practical relevance.\n- The connection between the theoretical framework and the experimental observations is currently weak. The authors report interesting empirical findings (e.g., the rate–distortion performance as a function of the split point) and hypothesize that these trends are due to the v-entropy gap and its generalization error. However, no concrete evidence or analysis is provided to substantiate this claim. The statement:\"As the source is further processed by non-linear functions in a high-dimensional space, the complexity of the intermediate representations increases, making their probability density more difficult to estimate\" is plausible but lacks empirical validation. Can the authors provide any proof or quantitative evidence that the complexity of intermediate representations indeed increases as hypothesized? Moreover, does this increase correlate with the measured rate–distortion performance? Establishing such a link would significantly strengthen the paper’s contribution and credibility.\n\nMinor remarks:\n- Related Work: The related work section needs substantial improvement. The authors mention learning-based image compression methods and claim that these approaches primarily rely on convolutional modules. However, many recent image compression architectures are transformer-based. The authors should clarify how their method relates to these transformer-based models and cite relevant works. Another example: the authors claim that, in the framework of image compression, the target representation follows a multivariate normal distribution with diagonal covariance. However, alternative distributions have been explored in the literature. For example, see:\nFu, H., Liang, F., Lin, J., Li, B., Akbari, M., Liang, J., & Han, J. (2023). Learned image compression with gaussian-laplacian-logistic mixture model and concatenated residual modules. IEEE Transactions on Image Processing, 32, 2063–2076."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d2JxT1fsnO", "forum": "X54rAhS4Iz", "replyto": "X54rAhS4Iz", "signatures": ["ICLR.cc/2026/Conference/Submission1603/Reviewer_yFGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1603/Reviewer_yFGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559410046, "cdate": 1761559410046, "tmdate": 1762915831354, "mdate": 1762915831354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learnable, lossy codec to compress transformer intermediate representations for split-device inference, optimizing an explicit rate–distortion trade-off. Its key design is a simple hyper-prior–centric entropy model: a transformer generates a compact hyper-prior that conditions a second transformer predicting factorized Gaussian parameters for the target activations, without direct access to previously coded targets; a deep factorized CDF models the hyper-prior. The authors introduce the V-entropy gap—the difference between achievable rate under a restricted predictive family and true entropy—show that the codec’s rate term minimizes this gap, and derive bounds linking it to covariance determinant, KL divergence, Lipschitz constants, and provide PAC-style generalization bounds via Rademacher complexity. Empirically, on GPT-2/Pythia (OpenWebText) and ViT/ResNet (ImageNet), the method achieves better RD than a Fourier density hyper-prior and a more complex direct-access entropy model (e.g., ~10.7% BD-rate gain vs direct-access), beats Deflate in bitrate and speed on activations, and can even improve task metrics at early splits. A key finding is that transformer bitrate increases with deeper splits—despite decreasing entropy—strongly correlating with covariance determinant and Rademacher complexity; for ResNet, the trend reverses. This supports a bias–variance-like explanation: increasing model complexity may reduce the gap but harms generalization. Limitations include factorized Gaussian assumptions, approximate bounds, and a non-optimized implementation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces the V-entropy gap as a unifying lens to explain why learned codecs operate above entropy in practice, bridging information-theoretic concepts (usable information, conditional V-entropy) with real transformer activation compression. The hyper-prior–centric design that conditions only on W (no direct access to past Y) challenges the necessity of complex autoregressive context models and is well-suited to streaming/split inference.\n2. Theory-practice alignment: formal definitions and bounds (covariance-determinant upper bounds, Lipschitz/PAC–Rademacher generalization bounds) with proofs, plus comprehensive experiments across architectures and modalities (GPT-2, Pythia, ViT, ResNet). Empirically outperforms more complex baselines in BD-rate, shows clear bitrate/speed gains over Deflate, and validates depth-dependent rate trends via high correlations with covariance determinant and Rademacher complexity."}, "weaknesses": {"value": "1. Limited impact of the derived bounds on empirical performance: While the covariance- and PAC/Rademacher–Lipschitz bounds are useful for explaining observed trends (e.g., higher bitrate at deeper layers, capacity–generalization trade-offs), they are loose and approximation-heavy, offering little prescriptive guidance for hyperparameters or architecture. \n\n2. Lack of real world evaluation: This paper does not demonstrate end-to-end gains in a true multi-GPU or cross-machine setup (e.g., wall-clock latency/throughput improvements under PCIe/NVLink/Ethernet, contention under batching, packetization/overheads). That is a genuine external validity gap."}, "questions": {"value": "- In Figure 5, the caption \"The rate is measured in bits-per-token (BPT) using the preprocessing target resolution of 224 × 224 pixels.\" is confusing. I suppose it's a figure for text tasks but why is pixels mentioned here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kcDzBV0SMT", "forum": "X54rAhS4Iz", "replyto": "X54rAhS4Iz", "signatures": ["ICLR.cc/2026/Conference/Submission1603/Reviewer_cubh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1603/Reviewer_cubh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942231597, "cdate": 1761942231597, "tmdate": 1762915831098, "mdate": 1762915831098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the compression of intermediate features in Transformers and identifies the counterintuitive phenomenon where deeper split points yield higher bitrates. A theoretical explanation is proposed, and preliminary comparative experiments on compression networks are conducted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tries to employs rigorous and detailed theoretical analysis, formalized through theorems."}, "weaknesses": {"value": "* The paper resembles the contributions from three theorems, without explaining how experimental results are connected to these contributions.\n* Experimental results are quite limited from the scope of image compression. It is suggested to add comparisons with SOTA methods and better model tuning. For the Hyperprior model, the authors explore factorized priors and Fourier bases, but the performance of the Fourier basis is significantly worse, suggesting potential inadequate tuning. Regarding the encoding of the latent `y`, an autoregressive model is used. While the proposed method performs better on the Perplexity metric, the autoregressive model achieves better results on the LAMBADA benchmark. Furthermore, experiments may include comparisons with state-of-the-art compression methods.\n* The information processing inequality suggests that deeper layers should have lower entropy. However, Section 4.4 attempts to argue that additional non-linear mappings push features into higher-dimensional spaces, increasing the complexity of entropy estimation and raising the upper bound of the generalization error.\n* Some results are not well-explained. The figures indicate that both Transformer and ViT models exhibit higher bitrates with deeper split points, whereas ResNet does not. This observation does not seem to be adequately explained by the proposed theory.\n* Baselines are also incomplete. The lossless compression comparison only uses Deflate; adding methods like zstd or tensor quantization is recommended.\n* There are several writing issues. 1) V-entropy Presentation: It is recommended to include Appendix Table 3 and Table 4 for better illustration. 2) Outdated References: Section 2's related work (latest 2018) requires updating. 3) Incorrect Title: Sec. 3.1 title should be \"Token-level Causal Hyperprior.\" 4) Unclear Terminology: \"Direct access\" and Fig. 2b's \"Direct-access entropy model\" lack immediate explanation. 5) Notation Error: Theorem 1's Gv(Y|Y) should likely be Gv(Y|W)."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gje3GhheQp", "forum": "X54rAhS4Iz", "replyto": "X54rAhS4Iz", "signatures": ["ICLR.cc/2026/Conference/Submission1603/Reviewer_upUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1603/Reviewer_upUu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981686019, "cdate": 1761981686019, "tmdate": 1762915830984, "mdate": 1762915830984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a lossy compression algorithm for encoding neural network outputs. They achieve this by selecting an appropriate hidden layer in the network architecture and quantising and entropy coding the network activations at that layer. To improve the efficiency of their codec, the authors propose encoding side information, for which they develop a hyperprior model.\n\nThey also develop a PAC bound to study the rate-distortion tradeoff of their method. Finally, they conduct experiments on compressing large language model (LLM) outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I found the experiments on the performance of the split points, i.e., which hidden layer to pick and quantise, interesting."}, "weaknesses": {"value": "The paper's most significant weakness is its lack of both novelty and insight.\n\nThe core idea of the paper, as I mentioned in the summary, is to encode neural network outputs by quantising and entropy coding hidden layer activations. This idea, along with the use of a hyperprior model, is not a novel or significant contribution.\n\nFurthermore, the authors' theoretical results follow easily from basic information-theoretic identities. This in itself is not an issue, but the final bound the authors obtain in Thm 3 is not directly useful: the authors do not compute it numerically, so it is difficult to judge its tightness, but based on my knowledge of similar bounds, I would imagine it is hopelessly loose. Indeed, it is not entirely clear what the authors' intention was in including the entire theoretical discussion in Sections 3.2 and 3.3, as these results do not yield any actionable insights.\n\nFurthermore, in certain places, the authors' language is highly imprecise, even contradictory, and potentially misleading. The main offenders are:\n - The authors refer to \"differentiable quantization functions.\" -- No quantization function is differentiable. To someone who doesn't know the works the authors cite, this is extremely misleading terminology. Something like \"quantizer with a differentiable training-time approximation\" would be appropriate.\n - \"This quantization (rounding) function discretizes the target representation so it can be coded and so gradients can flow through it during automatic differentiation\" -- this sentence is contradictory, a function with a discrete range cannot be differentiable. \n - \"Deflate operates at 0.672 milliseconds per token, whereas our proposed method operates at 0.348 milliseconds per token\" -- once again, this is a highly misleading sentence. In Appendix C, the authors state that: \"The hyper-prior and entropy model ran on an NVIDIA 2080 RTX Ti GPU. The arithmetic coder ran on a single core of an Intel Core i9-9900K CPU. The Deflate algorithm ran on the same system using a single CPU core.\" Unless the authors can demonstrate that the compute required to run the hyperprior model is insignificant compared to AC, the comparison of their method to Deflate is unfair.\n\nMore miscellaneous points:\n - The AE and AD acronyms in Figure 1 are not defined.\n - Eq 7: The notation $ (g[f(x)] \\circ f) (x)$ is really hard to read; the notation $g[\\cdot]$ is undefined, my assumption for reading was that $g[x] = g_x$.\n - Eq 8: $r \\in \\{ r \\mid r(y)=−\\log g(y), g\\in V\\}$ using $r$ for both the bound variable and a generic element of the set.\n - Accuracy is not a good y-axis label in Figure 3, since lower values are better in their plots.\n - The authors don't specify what distortion they used in their experiments; I presume they used the standard cross-entropy loss."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R5HhFLmWGz", "forum": "X54rAhS4Iz", "replyto": "X54rAhS4Iz", "signatures": ["ICLR.cc/2026/Conference/Submission1603/Reviewer_1DH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1603/Reviewer_1DH4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762430395373, "cdate": 1762430395373, "tmdate": 1762915830807, "mdate": 1762915830807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}