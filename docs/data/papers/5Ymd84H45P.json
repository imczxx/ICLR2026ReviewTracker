{"id": "5Ymd84H45P", "number": 9546, "cdate": 1758126983964, "mdate": 1759897712933, "content": {"title": "Assigning Confidence: K-partition Ensembles", "abstract": "Clustering is widely used for unsupervised structure discovery, but it offers no clear measure of how reliable each individual assignment is. While convergence or objective scores may reflect global quality, they do not indicate whether specific points are stably assigned, particularly in algorithms like $k$-means, which are sensitive to initialization and noise. This assignment-level instability can undermine clustering accuracy and robustness. Ensemble methods improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a unified framework that evaluates each point using two complementary statistics, assignment stability and consistency of local geometric fit, measured across a clustering ensemble. These are combined into a single interpretable confidence score in $[0,1]$. Theoretical analysis shows that CAKE scores are robust to noise and reliably distinguish stable from unstable points. Empirical results on synthetic and real-world datasets demonstrate that CAKE identifies both high- and low-confidence assignments, enabling targeted filtering or prioritization that improves clustering quality.", "tldr": "We propose CAKE, a method that estimates per-point confidence in clustering using ensemble agreement and geometric consistency, enabling robust, interpretable, and unsupervised assessment of cluster reliability.", "keywords": ["ensemble learning", "unsupervised learning", "clustering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cac8e55a0283d3cf3bca9196b348661c93b5d66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CAKE, a method to assign per-point confidence scores in clustering by combining assignment stability and geometric consistency from an ensemble of clusterings. While the problem is meaningful and the framework is intuitive, the paper in its current form has several limitations that prevent it from making a strong contribution. The core idea is a straightforward combination of existing concepts (ensemble agreement and Silhouette scores), and the empirical evaluation, though extensive, does not conclusively demonstrate a significant advantage over simpler baselines in many cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-Motivated Problem: Pointwise confidence estimation in clustering is a practical and under-explored area.\n\nComprehensive Evaluation: The paper is thorough in its empirical validation across multiple datasets and metrics.\n\nUnified Framework: It provides a clean, model-agnostic framework for combining two intuitive sources of confidence.\n\nTheoretical Analysis: The inclusion of non-asymptotic bounds adds rigor."}, "weaknesses": {"value": "Limited Novelty: The core components of CAKE (ensemble agreement via Hungarian algorithm and Silhouette scores) are well-established. The key contribution lies in their combination, which is a natural and straightforward idea that lacks conceptual breakthrough.\n\nInconsistent Empirical Advantage: A major weakness is that the full CAKE method does not consistently and significantly outperform its stronger individual component (often the $\\tilde{S}$ component) across the benchmark datasets. This raises questions about the necessity of the more complex, combined score. In many rows of Table 1, CAKE and ˜S component have identical or nearly identical values.\n\nHigh Computational Cost: The method requires running a clustering algorithm R times and computing pairwise label alignments ($O(R^2 k^3)$) and Silhouette scores ($O(Rn^2d)$ or $O(Rnkd)$ with approximation). This cost is not justified if a simpler method (like the $\\tilde{S}$ component alone) achieves similar performance.\n\nSuperficial Theoretical Analysis: The theoretical guarantees are direct applications of Hoeffding's inequality for U-statistics and do not offer novel theoretical insights into the clustering problem itself."}, "questions": {"value": "Given that the $\\tilde{S}$ component alone often performs as well as the full CAKE score, what is the concrete practical scenario where the additional complexity and cost of computing the stability component ($c_i$) is justified?\n\nThe ensemble is homogeneous (same algorithm, same k). Have the authors explored heterogeneous ensembles (e.g., different clustering algorithms), and if so, does the relative performance of CAKE change?\n\nThe method is evaluated primarily with k-means. How does CAKE's performance depend on the base clusterer's properties (e.g., stability, sensitivity to initialization)?\n\nThe computational complexity is non-trivial. For a very large dataset (e.g., n > 1M), is CAKE feasible? What are the concrete scalability limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E608d2P6Aj", "forum": "5Ymd84H45P", "replyto": "5Ymd84H45P", "signatures": ["ICLR.cc/2026/Conference/Submission9546/Reviewer_83qA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9546/Reviewer_83qA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532481369, "cdate": 1761532481369, "tmdate": 1762921105956, "mdate": 1762921105956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised confidence evaluation framework named CAKE (Confidence in Assignments via K-partition Ensembles), which is designed to measure the reliability of each data point in clustering algorithms. By integrating multiple clustering results and combining two metrics—clustering assignment stability and local geometric consistency—this method generates an interpretable confidence score for each sample. Theoretical analysis demonstrates that CAKE is robust to noise and can reliably distinguish between high-confidence and low-confidence samples. Experimental results show that CAKE improves clustering quality and interpretability across various synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. High method interpretability: The confidence level consists of two components—geometric structure and assignment stability—and its score range [0, 1] is intuitive and easy to understand.\n\n2. Significant empirical effects: It is verified on both synthetic and real-world datasets that CAKE can effectively improve the robustness and accuracy of clustering.\n\n3. Strong universality: The framework can be applied to any hard clustering algorithm (e.g., k-means) and supports integration of multiple algorithms or resampling."}, "weaknesses": {"value": "1. High computational complexity: Multiple clustering runs and pairwise label alignment are required, resulting in a computational complexity of $O(R²(n + k³))$.\n\n2. Obvious parameter dependence: Confidence results are significantly affected by the choice of ensemble size $R$ and clustering algorithm.\n\n3. Some experiments rely on manually set thresholds (e.g., 70% sample retention), and the strategy still needs optimization.\n\n4. Lack of theoretical verification under complex data distributions: The paper mainly assumes that clusters are separable (e.g., Gaussian clusters) and does not analyze the theoretical robustness in non-convex structures, manifold data, or high-dimensional sparse data, which limits the universal theoretical basis of the method.\n\n5. Relatively single experimental scenario: All experiments are mainly based on k-means clustering and a few standard datasets (e.g., Iris, Digits, FMNIST, 20News), and do not cover more complex or non-Euclidean space clustering tasks (e.g., graph clustering, spectral clustering, DBSCAN). Thus, the extrapolability of experimental verification is limited.\n\n6. Insufficient analysis of parameter sensitivity: Although the paper mentions that the number of ensemble runs $R$ affects the results, it does not systematically analyze the influence range of $R$, the deviation of cluster number $k$, or Silhouette approximation error on the final confidence, and lacks robustness or sensitivity experiments."}, "questions": {"value": "1. Can the confidence score be used for the adaptive selection of the number of clusters, k?\n\n2. Is the Silhouette approximation still reliable for high-dimensional or heterogeneous data?\n\n3. If the clustering algorithm itself is relatively stable (e.g., spectral clustering), can CAKE still bring significant gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IE6jM2awAG", "forum": "5Ymd84H45P", "replyto": "5Ymd84H45P", "signatures": ["ICLR.cc/2026/Conference/Submission9546/Reviewer_tVe6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9546/Reviewer_tVe6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553223497, "cdate": 1761553223497, "tmdate": 1762921105139, "mdate": 1762921105139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAKE, a method that assigns per-point confidence scores for clustering based on an ensemble of clusterings. The score combines assignment stability (via Hungarian-aligned label consistency) and geometric consistency (via silhouette statistics). The method is simple, modular, and empirically shown to filter low-confidence points to improve clustering quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a practical and lightweight method to quantify uncertainty in clustering assignments, requiring no changes to the underlying clustering algorithm. The design of the CAKE score is interpretable, and the decomposition into two complementary signals is intuitive. It also explores extensions to kernel methods and tests the framework on both synthetic and real-world data."}, "weaknesses": {"value": "1.Several confidence intervals in Table 1 are either incorrectly formatted (e.g., missing brackets) or exceed the valid range (e.g., ACC > 1). This raises concerns about numerical correctness or post-processing.\n\n2.The theoretical analysis only provides bounds for the assignment consistency component. The geometric component and final combined score are used in claims without corresponding guarantees, which creates a gap between theory and the stated robustness.\n\n3.The set of baseline methods is limited. Established alternatives like co-association-based measures (e.g., EAC), bootstrap-based stability, or GMM-based posterior scores are not compared. These should be included to place CAKE in context.\n\n4.The usage of fixed thresholds (e.g., retaining top 70% by CAKE) is ad hoc. The paper lacks methods for adaptive thresholding or visualization of coverage–risk tradeoffs, which limits the applicability in real deployments.\n\n5.Scalability evaluation is incomplete. While the runtime comparison in Figure 6 considers ensemble size, it does not assess performance across data size, number of clusters, or dimensionality, which are crucial in practice."}, "questions": {"value": "Can the authors clarify how the confidence intervals in Table 1 were computed? Were repeated trials performed, and were standard errors or bootstrapping used?\n\nAre there any assumptions under which the geometric component can be bounded, or are the authors planning to extend the analysis in future work?\n\nWould it be possible to incorporate co-association or posterior-based scores as baselines in the next revision?\n\nCan the authors provide a way to set the CAKE threshold automatically or present coverage–risk curves?\n\nDo runtime and memory usage grow linearly with n or k? If not, can that be made explicit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kAf2DBcmHu", "forum": "5Ymd84H45P", "replyto": "5Ymd84H45P", "signatures": ["ICLR.cc/2026/Conference/Submission9546/Reviewer_QzbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9546/Reviewer_QzbK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780885350, "cdate": 1761780885350, "tmdate": 1762921104663, "mdate": 1762921104663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for measuring per-point confidence in clustering ensembles. The proposed score is computed based on two statistical criteria: assignment stability and consistency of the local geometric fit, which are combined into a single scalar value ranging from 0 to 1. Experiments on both synthetic and real datasets show that the score helps identify both high- and low-confidence assignments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed per-point confidence score is both simple and interpretable, making it useful for downstream processing.\n\nIn addition to its practical applicability, the authors also provide theoretical guarantees for the score."}, "weaknesses": {"value": "The literature review is outdated. While one reference is from 2025, all others are from 2020 or earlier, and thus the discussion of related work does not reflect the current state of the art. For example: “Ensemble-based techniques like consensus clustering (Strehl & Ghosh, 2002) have also been explored to improve robustness by aggregating multiple partitions”. Although a few ensemble clustering papers are mentioned, the review neglects more recent developments, including progress in deep clustering. The survey papers referenced are also outdated. As a result, the novelty of the work remains unclear.\n\nThe selection of the silhouette score appears ad hoc and is not well motivated. Alternative metrics are neither discussed nor experimentally compared.\n\nThe proposed per-point confidence score is simple. Technically, constructing such scores is not difficult. The paper does not show concrete use cases to demonstrate its utility.\n\nThe authors focus exclusively on homogeneous ensembles, formed by repeated applications of a single clustering method. While this simplifies quantifying assignment-level confidence by avoiding other sources of variation, it limits the broader applicability of the approach. The potential for inhomogeneous ensembles is neither discussed nor demonstrated.\n\nAnother (implicit) limitation is that the ensemble members are required to have the same number of clusters, which excludes many clustering methods that adaptively determine the number of clusters. Extending the method to handle ensembles with different cluster counts (k₁ and k₂) may not be trivial, particularly for the label assignment process described in Section 3.3.\n\nFinally, only k-means is used in the experimental study. The authors justify this choice because k-means is widely used, easy to interpret, and sensitive to initialization, making it a natural candidate for assessing assignment-level confidence. However, this sensitivity also makes k-means an easy test case. More challenging is to test if the proposed score is able to deal with more discreet cases resulting from clustering methods that are more advanced than k-means."}, "questions": {"value": "What are the recent works related to per-point confidence scores?\n\nWhy is the silhouette score considered superior to other metrics?\n\nHow can this work be extended to the more general case of unequal numbers of clustering members in the ensemble?\n\nWhat potential challenges arise when dealing with inhomogeneous ensembles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DTxaYewOPG", "forum": "5Ymd84H45P", "replyto": "5Ymd84H45P", "signatures": ["ICLR.cc/2026/Conference/Submission9546/Reviewer_kpDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9546/Reviewer_kpDz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935350842, "cdate": 1761935350842, "tmdate": 1762921104285, "mdate": 1762921104285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}