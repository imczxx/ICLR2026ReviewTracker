{"id": "41JeFWdVFa", "number": 8948, "cdate": 1758103599057, "mdate": 1759897752277, "content": {"title": "LDP: A Lightweight Denoising Plugin Enhancing Generalization in Single-Image Super-Resolution", "abstract": "Current single-image super-resolution (SISR) models struggle to generalize to real-world degradations. To address this challenge, we propose LDP, an innovative lightweight denoising autoencoder~(DAE) plug-in. It improves the generalization ability of SR models via low-resolution (LR) images prediction-based cyclic regularization. LDP models the SISR degradation process within the DAE framework. It leverages a property of diffusion models, where after noise is added, high-resolution (HR) images and LR features become aligned, so that denoising noisy HR features is equivalent to denoising noisy LR features. During the corruption process, noise is added independently to each HR patch. During the denoising process, a convolutional denoiser uses learned filters to approximate blur kernels. \nIn addition, LR degradation is used to distinguish different LR from the same HR. LDP can be applied to SR models in two modes: as a training loss to improve reconstruction quality, or as an inference post-processing step to correct artifacts. Extensive experiments demonstrate that LDP substantially improves the generalization of existing SR models to unseen degradations.", "tldr": "we propose LDP, a lightweight denoising autoencoder plug-in.  It improves SR model generalization via LR-prediction–based cyclic regularization.", "keywords": ["low-level vision", "Single Image Super-Resolution", "degradation model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf719899aeff5d5f8dadc13eb5eab6479a357ebe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a general HR to LR lightweight network, which can regularize the training and act as an auxiliary task for test-time adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Lightweight and plug-and-play.\n2. Can be used as bot regularizer during training and corrector during test-time adaptation.\n3. General to most SR models."}, "weaknesses": {"value": "Still need paired data for training."}, "questions": {"value": "1. I noticed that there is a cone layer before the NAM in Fig 2(a), but the input of Fig 2(c) is still images, please clarify this.\n2. This LDP module is also trained on the paired data, how come it is capable of generalizing to the unseen degradation? Because noise makes HR and LR distributions comparable? Any more mathematical explanation of this?\n3. When the LR is poor, I think high-freq feature of it is almost zero. How can this zero-like map help.\n4. I believe there are some other HR to LR methods, can they help? Any comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HDdggMC6WM", "forum": "41JeFWdVFa", "replyto": "41JeFWdVFa", "signatures": ["ICLR.cc/2026/Conference/Submission8948/Reviewer_C4zd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8948/Reviewer_C4zd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797311539, "cdate": 1761797311539, "tmdate": 1762920686853, "mdate": 1762920686853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LDP, a lightweight denoising autoencoder plug-in designed to improve the generalization of single-image super-resolution (SISR) models to unseen degradations. The key idea is to predict the LR image from the SR/HR pathway and enforce cyclic LR consistency during training of any SR backbone. At test time, the same learned degradation model can guide diffusion SR via Diffusion Posterior Sampling (DPS) to reduce artifacts. Concretely, LDP corrupts HR patches with independent noise, uses a small CNN denoiser that approximates blur kernels, and aligns HR and LR features after noise injection, yielding a symmetric LR loss in the Fourier domain and a DPS guidance term that nudges samples toward LR-consistent solutions. The method is plug-and-play, can be used either as a training loss or an inference post-processing step, and shows consistent gains across diverse SR models in both synthetic and real-world settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a practical and coherent contribution that is original in its combination of ideas: a tiny conditional denoiser that learns HR to LR degradation to (i) enforce LR-cycle and frequency consistency during training and (ii) guide diffusion SR via DPS at inference, yielding a unified degradation prior across regimes. The design is simple, data-efficient, and plug-and-play. Experiments span multiple SR backbones and degradation types, yielding consistent gains, and ablations isolate the effects of key components (symmetric LR loss, frequency term, DPS). The figures and algorithms are easy to follow, limitations are stated, and implementation details (params, schedules, compute) support reproducibility. Significance is strong for the SR community: the module is a lightweight, architecture-agnostic, and improves robustness to unseen degradations with minimal engineering overhead."}, "weaknesses": {"value": "The paper would benefit from direct, same-protocol comparisons to earlier degradation-modeling methods (e.g., DRN, DualSR, SCL-SASR) to isolate LDP’s incremental value. As written, the related-work framing is stronger than the empirical head-to-head evidence.  Real-world results are mixed; on RealSR, some non-reference metrics worsen for certain backbones, so adding broader real-image datasets or RAW pipelines would better substantiate generalization claims.   Diffusion Posterior Sampling brings substantial runtime overhead with modest quantitative gains unless applied very frequently. Reporting FLOPs and latency, along with a speed-quality, would clarify deployability.    The guidance also interacts poorly with StableSR (a repeat-spot artifact) without an additional mitigation step, suggesting the need for a deeper analysis of when guidance helps or harms strong generative priors.  Evidence is concentrated at ×4 with limited sensitivity analysis. Multi-scale (×2/×3) results and parameter sweeps would improve external validity.    Finally, while LDP is lightweight, the training overhead is not fully characterized across backbones, and summarizing memory and performing an ablation of LDP capacity vs. benefit would make costs transparent."}, "questions": {"value": "Add an experiment against DRN, DualSR, and SCL-SASR under the same data/degradation protocols to isolate LDP’s advantage over prior degradation-modeling pipelines? This would strengthen Section 2.2’s positioning with empirical evidence.  \n\nReal-world results on RealSR/DPED/RealSRSet are promising but mixed for some models/metrics. Can you expand the real-image evaluation (e.g., additional datasets or RAW pipelines) and explain when LDP helps or hurts no-reference metrics like CLIPIQA/NIQE?   \n\nPlease report complete runtime/compute overhead for DPS guidance across models (wall-clock per image, FLOPs, peak memory), and include a speed–quality Pareto curve. Table 11 suggests significant slowdowns when guidance is frequent—how does this trade-off look across step counts and models? \n\nFor StableSR, LDP interacts with the repeat-spot artifact and requires a noise-subtraction mitigation. Can you analyze why the interaction occurs, quantify the extent to which the mitigation helps, and clarify whether similar interactions occur in LDM/ResShift/UPSR?  \n\nHyperparameters are largely fixed (e.g., τ=100; loss weights). Could you provide sensitivity curves for τ and the frequency-loss weight and discuss recommended defaults per backbone? An expanded ablation would help practitioners robustly tune LDP.   \n\nMost results emphasize ×4 SR. Can you report multi-scale (×2/×3/×4) performance and discuss whether the LR-HF conditioning or noise schedule needs scale-specific adjustments?  \n\nPlease characterize training overhead more fully across backbones (total wall-clock for fine-tuning, GPU type, batch sizes) and, if possible, include an ablation of LDP capacity (e.g., parameter count or depth) vs. benefit. Table 12 is a good start, but it focuses on SwinIR. \n\nIn the Limitations, note that LDP is not generative in posterior sampling and relies on LR high-frequency conditioning. Could discuss scenarios where this reliance risks information leakage or trivial solutions, and propose safeguards or diagnostics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kdCFJXgUXf", "forum": "41JeFWdVFa", "replyto": "41JeFWdVFa", "signatures": ["ICLR.cc/2026/Conference/Submission8948/Reviewer_hH62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8948/Reviewer_hH62"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046344859, "cdate": 1762046344859, "tmdate": 1762920686507, "mdate": 1762920686507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LDP, a lightweight denoising autoencoder (DAE) designed as a plug-in module to improve the generalization of single-image super-resolution (SISR) models. It addresses the challenge of poor performance on unseen real-world degradations by modeling the degradation process and enforcing low-resolution (LR) cyclic consistency."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed LDP introduces a lightweight, high frequency degradation concentrated degradation model that generates LR images from HR inputs.\n2. The proposed LDP can be applied both during training and inference, enhancing generalization across diverse SISR models without architectural changes."}, "weaknesses": {"value": "1. In the degradation prediction module, $LR_{hf}$ is obtained by subtracting the s’-fold downsampled-then-upsampled LR image from the original LR image. Although downsampled operation will corrupt a part of high frequency information, it also damaged the low frequency information. This approach to obtain high frequency component does not make sense.\n2. When fine-tuning SR models with LDP, this paper claims to minimize the symmetric loss, which measuring the consistency of high frequency componets between LR input and a reconstructed LR from the SR output. When the reconstruction model (LDP) is freezed, however, the reconstruction consistency should decrease as the SR quality increases because better SR should remove more high frequency degradations from LR. Therefore, the symmetric loss should be maximized instead of minimized during fine-tuning and the proposed LDP can work like a degradation discriminator for the pretrained SR models. \n3. In Table 1, there's only LDP's performance on synthetic degradation predictionon. Given no compared method existing, I cannot judge the effiency of the proposed method.\n4. In Table 2, the performance enhancement of LDP is not significant. The improvement is especially trivial for the most updated SOTA method MambaIR and even MambaIR was published over one and a half years.\n5. All visual comparisons are too small to distinguish the difference even after zoom in!"}, "questions": {"value": "1. What is the training purpose of the proposed LDP (Lightweight Denoising Plugin) framework? It seems like the authors want to predict a y' (or LR') with similar high frequency components of the input LR, but what is the necessity to train a network instead of using wavelet transformation or other frequency-based approaches to performe a high-pass filter on input LR? If LDP does not have LR as input and only use LR as supervision, this framework will be much more reasonable. \n2. What is the purpose of using HR in the proposed LDP framework? Since $LR_{hf}$ provides the high-frequency information, it seems to exploit HR to gain more low-frequency or content information. However, this paper apply a blur kernel and the noise adding schedule from diffusion models in the LDP, producing a super degraded HR before entering the denoiser. Note the proposed denoiser adopt a lightweight structure so it cannot provides a similar denoising ability compared to the U-Net in diffusion models. Therefore, I doubt how much content information from HR can be provided for predicting the LR'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RFuXg6U7mG", "forum": "41JeFWdVFa", "replyto": "41JeFWdVFa", "signatures": ["ICLR.cc/2026/Conference/Submission8948/Reviewer_ceoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8948/Reviewer_ceoH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251239445, "cdate": 1762251239445, "tmdate": 1762920686054, "mdate": 1762920686054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}