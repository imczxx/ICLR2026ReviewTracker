{"id": "MNVxrgRcJV", "number": 5699, "cdate": 1757927660216, "mdate": 1759897959882, "content": {"title": "Sample Reward Soups: Query-efficient Multi-Reward Guidance for Text-to-Image Diffusion Models", "abstract": "Recent advances in inference-time alignment of diffusion models have shown reduced susceptibility to reward over-optimization. However, when aligning with multiple black-box reward functions, the number of required queries grows exponentially with the number of reward functions, making the alignment process highly inefficient. To address the challenge, we propose the first inference-time soup strategy, named Sample Reward Soups (SRSoup), for Pareto-optimal sampling across the entire space of preferences. Specifically, at each denoising step, we independently steer multiple denoising distributions using reward-guided search gradients (one for each reward function) and then linearly interpolate their search gradients. This design is effective because sample rewards can be shared when two denoising distributions are close, particularly during the early stages of the denoising process. As a result, SRSoup significantly reduces the number of queries required in the early stages without sacrificing performance. Extensive experiments demonstrate the effectiveness of SRSoup in aligning T2I models with diverse reward functions, establishing a practical and scalable solution.", "tldr": "We propose the first inference-time soup strategy, named Sample Reward Soups (SRSoup), for Pareto-optimal sampling across the entire space of preferences.", "keywords": ["Diffusion model", "Text to Image", "Sample Reward Soups", "Training-free", "Black-box alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22da4edec5ea980549ee746a135fd8ec723f50ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper introduces SRSoup, a training-free and query-efficient diffusion-time alignment method for multiple rewards.\n- The core idea is to avoid combinatorial blow-up by decomposing multi-objective optimization into $M$ simpler single-objective problems.\n- The theory clarifies when the approximation is accurate: it holds when the objective’s curvature (second derivative) is small and the initialization is suitably chosen.\n- In text-to-image experiments, SRSoup attains higher rewards under comparable query budgets.\n- The empirical study also examines the roles of key hyperparameters.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed multi-reward alignment method is training-free and query-efficient.\n- Its query efficiency and accuracy are supported by both theory and experiments.\n- The authors prove they can approximate the search gradient efficiently and accurately under stated assumptions, and they evaluate the finite-sample approximation error.\n- The Pareto front is clearly visualized in experiments, and empirical Pareto near-optimality is demonstrated in Figures 3, 5, and 7.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "weaknesses": {"value": "- As noted in lines 327–335, the main concern is that the assumption for Proposition 3 may not hold, leading to inaccurate approximations. The authors suggest switching from SRSoup to the standard method at the K-th step; however, the computational cost will then approach that of the standard “WeightedSum” method.\n- For these reasons, I hope the computational cost as a function of K is clarified concretely in the main paper (I only found Table 4 in the appendix).\n- The authors empirically validated density overlap using the Bhattacharyya coefficient (BC), whereas their theory uses Total Variation (TV). In the appendix and Proposition 4, they provide only an upper bound on TV. I think a lower bound on TV is also needed to justify the choice of K. The true TV might be small if BC is small.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "questions": {"value": "- Could you extend your method to a per-subproblem optimization strategy with $M_2$ rewards per subproblem, where $M_2 < M$? I believe this could offer a favorable trade-off between quality and cost.\n- For example, with 10 reward functions in total, you could approximately divide them into 5 subproblems, each containing 2 reward functions.\n- Please note that I am NOT requiring any additional experiments for this.\n\nNote: I used ChatGPT for minor language editing and phrasing assistance; all technical assessments are my own."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N0CwERLNSm", "forum": "MNVxrgRcJV", "replyto": "MNVxrgRcJV", "signatures": ["ICLR.cc/2026/Conference/Submission5699/Reviewer_5nQC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5699/Reviewer_5nQC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377563951, "cdate": 1761377563951, "tmdate": 1762918204276, "mdate": 1762918204276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sample Reward Soups (SRSoup), a training-free, query-efficient multi-reward alignment method for text-to-image diffusion models.\nThe core idea is to pre-compute per-reward gradients (“search gradients”) and to linearly interpolate them to approximate the gradients for arbitrary reward weightings.\nBy sharing reward evaluations across weights during the early diffusion steps—when the sample distributions are still overlapping—the method aims to achieve efficient Pareto-optimal sampling under multiple rewards.\nExperiments on Stable Diffusion 1.5 and SDXL demonstrate similar or slightly better image quality compared to fine-tuning methods (e.g., DDPO, TDPO) while reducing reward queries by ~1.8–2.7×."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles the important problem of reducing query computation cost in multi-objective preference optimization.\n\n2. The number of queries can be explicitly calculated (as shown in Section C.3), and each parameter can be set according to the desired reduction target.\n\n3. The proposed method is clearly formulated and easy to understand; unifying the Gaussian initialization to achieve a first-order gradient approximation is an interesting design."}, "weaknesses": {"value": "1. The paper does not discuss the impact of varying K on performance.\n\n2. There is no comparison of processing time with existing methods other than weighted-sum."}, "questions": {"value": "1. How does the performance change when K is varied?\n\n2. Would it be possible to compare the inference time with existing methods such as Rewarded Soups, AlignProp, TDPO, and DDPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "moG2hgAGES", "forum": "MNVxrgRcJV", "replyto": "MNVxrgRcJV", "signatures": ["ICLR.cc/2026/Conference/Submission5699/Reviewer_k88a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5699/Reviewer_k88a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895825462, "cdate": 1761895825462, "tmdate": 1762918203935, "mdate": 1762918203935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free multi-reward optimization framework that improves generated images of text-to-image diffusion models. In particular, the authors proposed a multi-reward alignment approach that optimize the denoising distribution at each sampling step using black-box optimization strategy. Additionally, they also proposed Sample Reward Soups, a mechanism that combines multiple reward objective that interpolates reward-guided search gradients. Experiment results show that the proposed inference-time framework achieves competitive performance compared to RL and other finetuning methods. They also demonstrated that the proposed Sample Reward Soups is more preferable than naive weighted sum of multiple rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is sufficiently novel in that it extends the idea of Reward Soap that interpolates multiple finetuned diffusion models to a training-free setup, making it more accessible while provided meaningful insight about the nature of this type of interpolation methods.\n2. The author provided a detailed theoretical analysis of the characteristics of the proposed method, supported by toy experiments on mixture of Gaussian, which are informative.\n3. The experiments are thorough. I especially appreciate the additional experiments and analysis in the supplementary material."}, "weaknesses": {"value": "1. The author only showed experiments on SD-series U-Net, which is quite outdated at this time. I understand that many of the RL literature still focuses on SDv1.5 and SDXL due to computation constraints. However, as the proposed method is training free, it would be good to show results on state-of-the art rectified flow models based on diffusion transformers (DiT), such as Sana, SD3, Flux, etc. Such inclusion would make the paper more relevant.   It would also be interesting to see how the proposed method compared with the latest RL literature, such as Flow-GRPO. These latest works should also be discussed in the related works.\n\n2. offline policy learning methods such as DPO are discussed but not included in the comparison. these model learns directly from human preference dataset which encompasses a diverse set of preference. It would be interesting to see however this method compare. While it is hard to apply soup to these models as they cannot be directly trained for different rewards, the author can still use other inference-time methods such as best-of-N sampling by matching the inference time compute.\n\n3. for SDXL comparison in appendix, an RL baseline is missing, such as D3PO[2].\n\n\n[1] Liu, Jie, et al. \"Flow-grpo: Training flow matching models via online rl.\" arXiv preprint arXiv:2505.05470 (2025).\n[2] Yang, Kai, et al. \"Using human feedback to fine-tune diffusion models without any reward model.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "I currently recommend this paper for weak acceptance. \nI'm willing to increase my score based on author's responses. In particular, I hope the author can provide missing baselines and related literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ajzXCewWBT", "forum": "MNVxrgRcJV", "replyto": "MNVxrgRcJV", "signatures": ["ICLR.cc/2026/Conference/Submission5699/Reviewer_itLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5699/Reviewer_itLM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904244178, "cdate": 1761904244178, "tmdate": 1762918203644, "mdate": 1762918203644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper Sample Reward Soups (SRSoup) addresses the challenge of multi-objective optimization in Text-to-Image diffusion models by proposing the first query-efficient, inference-time \"soup\" strategy. The core problem is that standard methods for balancing multiple black-box rewards (like aesthetic quality and compressibility) require an exponentially large number of expensive reward queries across the preference space. SRSoup solves this by independently calculating a reward-guided search gradient for each individual objective at every denoising step, and then linearly interpolating these gradients (not the model weights or rewards themselves). This allows the model to efficiently share and reuse sample rewards across different weighted objective combinations, drastically reducing the number of required queries and enabling superior or comparable performance to the Weighted Sum baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty of Inference-Time Gradient Interpolation: The paper introduces the first inference-time “soup” strategy (SRSoup) for diffusion process, effectively adapting the “Model Soups” concept not to model weights but to search gradients. This approach achieves Pareto-optimal sampling across a preference space without the risks of reward over-optimization often seen in fine-tuning methods.\n2. Addresses a Scalability Bottleneck with High Efficiency: The paper addresses a limitation in aligning with multiple reward functions, which requires exponentially growing number of reward queries, which makes the process highly inefficient. The proposed SRSoup resolves this by significantly reducing the number of queries in the early denoising stages, establishing a more scalable solution for multi-objective T2I alignment.\n3. Strong Empirical Results and Practical Impact: The method achieves significant query efficiency (e.g., up to 2.7x speedup) over the weighted-sum baseline, as demonstrated by the Hypervolume (HV) versus reward query plots."}, "weaknesses": {"value": "1. Lack of Cost-Benefit Justification: While the paper shows a reduction in the number of reward queries, it fails to provide a comprehensive cost-benefit analysis for the entire inference pipeline. The reward query computation time, which is model-dependent, must be weighed against the overhead of running multiple parallel single-reward guidance steps required by SRSoup's gradient estimation. \n2. Insufficient Analysis of Multi-Reward Trade-offs and Stability: The analysis, primarily relying on Hypervolume, does not rigorously demonstrate that SRSoup maintains a better balance or stability across the Pareto front compared to weighted-sum methods, especially with non-convex reward functions. This omission limits validation of the method's robustness against potential reward over-optimization.\n3. Sensitivity and Justification of the Hybrid Strategy: The reliance on a hybrid strategy using SRSoup only for the first K steps lacks rigorous investigation into the determination and sensitivity of the boundary K."}, "questions": {"value": "1. Please provide a detailed breakdown of the wall-clock time and GPU memory consumption across the full inference pipeline, explicitly weighing the reduced reward query time against the overhead of running M parallel guidance steps.\n2. Given the risk of sub-optimal configurations in multi-objective optimization, how does SRSoup compare to simple weighted-sum methods in terms of output balance (i.e., avoiding cases where one reward is disproportionately maximized)?\n3. Please provide a quantitative ablation study showing the performance (Hypervolume) and efficiency (Query Reduction) trade-offs across a range of K values (e.g., K=20, 40, 60)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aVpQcmx5ch", "forum": "MNVxrgRcJV", "replyto": "MNVxrgRcJV", "signatures": ["ICLR.cc/2026/Conference/Submission5699/Reviewer_GEV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5699/Reviewer_GEV9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050893299, "cdate": 1762050893299, "tmdate": 1762918203381, "mdate": 1762918203381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}