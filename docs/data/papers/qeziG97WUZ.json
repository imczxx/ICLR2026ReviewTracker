{"id": "qeziG97WUZ", "number": 14166, "cdate": 1758229536195, "mdate": 1759897386386, "content": {"title": "lmgame-Bench: How Good are LLMs at Playing Games?", "abstract": "Playing video games requires perception, reasoning, memory, and long-horizon planning—exactly the faculties expected of modern large language and vision–language models (LLMs/VLMs). We introduce LMGame-Bench, a benchmark built on six popular games spanning platformer, puzzle, and narrative games through a unified Gym‑style API. Unlike prior game benchmarks that entangle multiple skills, LMGame-Bench employs a modular harness—including perception, memory, and reasoning modules—that can be toggled to selectively probe distinct capabilities. The benchmark further improves robustness through prompt standardization and contamination mitigation. Evaluation of 13 state-of-the-art models demonstrates that LMGame-Bench remains challenging yet effectively discriminates among models. Correlation analysis reveals that individual games align with core LLM capabilities, providing a quantitative framework for interpreting performance. Finally, LMGame-Bench exposes models’ limitations in visual state extraction, reflection, spatiotemporal reasoning, and long-context reasoning, pointing to concrete directions for model improvement.", "tldr": "", "keywords": ["LLM", "VLM", "Agents", "Benchmark", "Games"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a6db93f6411024606f51e82cb77e0042b83aadf.pdf", "supplementary_material": "/attachment/7c5ff194c74d9ece15998861bbf91557e089c27a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmark for LLMs on six game environments, they evaluated a variety of popular LLMs. The paper also studies contamination and attempts to create standardize prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Having the human average for each of the games and random baselines is good. The study on contamination detection is interesting, and so is the attempt at standardizing the prompting with  DSPy standardization"}, "weaknesses": {"value": "I would argue that testing LLM without harness doesn’t make much sense at all. In the context of LLM Agents, the LLM is the brain (without memory module), the agentic scaffolding, or so called “harness” in the paper, is equivalent to affordances and tools such as arms, legs and eyes, memory. It’s not surprising to me at all how a naked LLM without these affordances (scaffolding or harness) often doesn’t outperform random. Memory particularly is very important for partially observable MDPs. \n\nThe benchmark could have been valuable around a year prior to submission, but now comes in a much more crowded space of benchmarks for LLM/VLM Agents, and the the papers struggles to find novelty both in the methodology as well as in insights provided.  The variances in most of the results are extremely large, and it’s difficult to disentangle real results from noise. The paper attempts to do something very similar to existing benchmarks specifically Balrog [Paglieri et al. ICLR 2025], and given the many similarities a more thorough comparison would be needed to explain what novelty (whether methodological or new insights) this paper brings. \n\nUsing o3 to generate perception traces also seems like a big methodological mistake as the rest of the models' capabilities will be influenced by another model."}, "questions": {"value": "Could the author better argue what’s the difference between their “harness” compared to typical scaffoldings used in most related benchmarks?\nCould the author try to better argue where the novelty of the benchmark comes from, and what insights are actually new, especially when compared to the many existing benchmarks in the area?\nWhy using o3 for perception? This feels like a methodological flaw.\nHow many seeds were tested for each game?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tMfKSZKpN4", "forum": "qeziG97WUZ", "replyto": "qeziG97WUZ", "signatures": ["ICLR.cc/2026/Conference/Submission14166/Reviewer_tEMA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14166/Reviewer_tEMA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506601525, "cdate": 1761506601525, "tmdate": 1762924627525, "mdate": 1762924627525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LMGame-Bench, a new benchmark designed to evaluate large language\nand vision–language models (LLMs/VLMs) through six popular video games: Super Mario Bros.,\nTetris, Sokoban, 2048, Candy Crush, and Ace Attorney. The benchmark uses additional modular\ngaming harness composed of Perception, Memory and Reasoning modules, which are used to\nassess specific model capabilities, and allow the models to perform at the level of human\nplayers. The benchmark also takes advantage of the DSPy’s SIMBA optimizer to standardize\nthe prompts.\nLMGame-Bench allows for a controlled evaluation of LLMs on video games and offers a\ncomprehensive evaluation of 13 state-of-the-art models, while relying on robust techniques such\nas prompt standardization and contamination detection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Gaming Harness: The design of the gaming harness is highly effective and it represents a\nrobust method for isolating and accurately evaluating specific model capabilities and skills.\n2. Data Contamination Detection: The inclusion of an explicit mechanism for detecting data\ncontamination is a significant strength\n3. Prompts Standardization: The decision to standardize prompts by leveraging the DSPy\nframework is highly commendable. This approach ensures consistency and reproducibility\nacross experiments."}, "weaknesses": {"value": "The primary weakness lies in the overall presentation and clarity of the paper. I recommend\nrepositioning the Related Work section to immediately follow the Introduction. This structural\nchange would more effectively contextualize the work and highlight the paper's novel\ncontributions earlier. Also, given that this is a benchmark paper, the explanation of the Metrics\n(specifically the Raw and Aggregated Scores) should be expanded, possibly reserving one\nsubsection just for this point.\n\nAlso, there are some typos in the main paper. For example line 53:\n\"To address this issue and also enable controlled evaluation, we enriches our evaluation\nsettings by developing gaming harness\". \"we enriches\" should be \"we enrich\".\n\nAnd line 425:\n\"Super Mario Bros. is excluded\" should be \"Super Mario Bros is excluded\"."}, "questions": {"value": "Regarding the gaming harness. It is not clear to me if you can \"toggle on or off\" them selectively.\nFor example, is it possible to only activate the Perception Module without activating the Memory\nModule, or viceversa?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6JXFnb8cWw", "forum": "qeziG97WUZ", "replyto": "qeziG97WUZ", "signatures": ["ICLR.cc/2026/Conference/Submission14166/Reviewer_zKTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14166/Reviewer_zKTd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733989333, "cdate": 1761733989333, "tmdate": 1762924627127, "mdate": 1762924627127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark for evaluating LLMs on game-playing tasks, testing 13 state-of-the-art models using a direct screenshot-to-action setup. The authors find that these models perform poorly, often close to random, highlighting their weaknesses in visual perception and long-horizon decision-making. The study also explores where and how these models fail as task difficulty increases and demonstrates the effective use of the proposed harness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. A new benchmark consisting of complex goal-driven games is introduced in this study. \n2. An extensive suite of models is evaluated, covering 13 state-of-the-art architectures.\n3. The problem statement and the experimental framework are well designed and presented.\n4. The authors perform detailed and consistent evaluations across difficulty levels, revealing how and where models fail."}, "weaknesses": {"value": "While the work is interesting and systematically executed, many of its findings align with prior studies that have already established similar limitations of LLMs and explored methods to overcome them (e.g., Chain-of-Thought reasoning, embedding API calls, or memory modules/database access). The novelty and contribution of this work, therefore, feel limited unless the authors can better justify what new insights their benchmark offers.\n\nAdditionally, while the authors show that adding different modules enhances performance, they do not explain why these modules lead to improvement. If such reasoning is provided, the authors should indicate the corresponding line numbers."}, "questions": {"value": "1. How does this benchmark fundamentally differ from existing benchmarks that also test LLMs or VLMs on interactive or game-based tasks? Specifically, how are these selected games different in terms of difficulty and multi-modality from the other existing benchmarks/games?\n\n2. Is there any difficulty level or stage where even the module-based models fail to improve? The authors should clarify how far these modules can enhance performance and at what point their effect saturates or diminishes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NnFDhmaglZ", "forum": "qeziG97WUZ", "replyto": "qeziG97WUZ", "signatures": ["ICLR.cc/2026/Conference/Submission14166/Reviewer_2PLk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14166/Reviewer_2PLk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886618305, "cdate": 1761886618305, "tmdate": 1762924626739, "mdate": 1762924626739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LMGame-Bench, a benchmark for evaluating large language models (LLMs) and vision-language models (VLMs) on six popular video games (Super Mario Bros., Tetris, Sokoban, Candy Crush, 2048, Ace Attorney) via a unified Gym-style API. Unlike prior game benchmarks that entangle multiple skills, LMGame-Bench uses a modular harness (perception, memory, reasoning modules) to isolate specific capabilities, supports both scaffolded and unscaffolded evaluations, and enhances robustness through data contamination mitigation and prompt standardization. Evaluations of 13 state-of-the-art models show the benchmark effectively discriminates performance (o3 and o1 lead), reveals correlations between games and core LLM capabilities (e.g., Sokoban aligns with math/coding, Ace Attorney with language understanding), and identifies model limitations in visual state extraction, spatiotemporal reasoning, and long-context processing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Modular Harness Design: Addresses a key limitation of prior game benchmarks (entangled skills) by enabling selective activation of perception, memory, and reasoning modules. This allows fine-grained diagnosis of model strengths/weaknesses (e.g., separating perception failures from planning gaps) that was previously unachievable.\n2. Rigorous Experimental Design: Evaluates 13 models across 6 diverse games (platformer, puzzle, narrative) with standardized metrics (progression/long-horizon rewards) and statistical validation (paired-sample t-tests, Glass’s δ, coefficient of variation). Results are consistent and reproducible, with detailed ablation studies for harness modules.\n3. Insights for Model Improvement: By identifying specific failure modes (e.g., VLMs struggle with board state extraction from images, non-reasoning models lack self-correction), the paper guides concrete advancements in model architecture and agentic design."}, "weaknesses": {"value": "1. Limited Game Diversity: While the 6 games cover 3 genres, they lack representation of real-time strategy (RTS), open-world, or multiplayer games—domains that test collaboration, dynamic resource management, or complex opponent adaptation. This limits the benchmark’s generalizability to broader game-based agentic tasks.\n2. Computational Cost Opacity: While the paper mentions high computational costs (Appendix B.4), it does not provide concrete guidance for scaling evaluations (e.g., cost-saving strategies beyond vague suggestions like \"bounding trajectories\"). This may limit accessibility for smaller research teams.\n3. Perception Module Efficacy: For games like Super Mario Bros., how accurate is the textual representation generated by the perception module in capturing dynamic spatiotemporal cues (e.g., enemy speed, jump physics)? Could gaps in this representation explain why the harness provides limited gains for this game?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mcTtZFyYSL", "forum": "qeziG97WUZ", "replyto": "qeziG97WUZ", "signatures": ["ICLR.cc/2026/Conference/Submission14166/Reviewer_uZi7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14166/Reviewer_uZi7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965881462, "cdate": 1761965881462, "tmdate": 1762924626312, "mdate": 1762924626312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "## Summary  \nThis paper introduces **LMGame-Bench**, a modular, Gym-style benchmark constructed around six popular games (Super Mario Bros., Tetris, Sokoban, Candy Crush, 2048, and Ace Attorney). A core design feature is a **toggleable “gaming harness”** (comprising perception, memory, and reasoning modules) that isolates distinct capabilities and expands performance headroom. Complementing this, the benchmark integrates **contamination checks** and **prompt standardization** to reduce evaluation variance. Experiments conducted on 13 state-of-the-art models demonstrate that the harness significantly improves the benchmark’s ability to discriminate between models while uncovering key failure modes—including limitations in visual state extraction, spatiotemporal control, self-reflection, and long-context reasoning. The paper further employs **correlation and low-rank decomposition analyses** to link game-specific performance to broader clusters of LLM capabilities.  \n\n\n## Strengths  \n1. **Original and Impactful Contribution**: LMGame-Bench addresses a critical limitation of existing game benchmarks—their tendency to entangle multiple skills—by introducing a modular harness that isolates distinct LLM/VLM capabilities (perception, memory, reasoning). This design enables fine-grained diagnosis of models’ strengths and weaknesses, making it a valuable tool for guiding model development.  \n2. **Rigorous Benchmark Design**: The benchmark strikes a balance in difficulty (avoiding both premature saturation and excessive hardness) and covers diverse game genres (platformers, puzzles, narrative games), ensuring it effectively discriminates between state-of-the-art models. The inclusion of contamination mitigation (e.g., entity masking, paraphrasing) and prompt standardization (via DSPy’s SIMBA optimizer) further enhances its robustness—a key prerequisite for reliable LLM evaluation.  \n3. **Comprehensive Experimental Design**: The evaluation of 13 models (under both harnessed and unharnessed settings) combines quantitative methods (paired-sample t-tests, Glass’s δ effect sizes, correlation analysis) and qualitative insights (failure mode analysis). By linking game performance to core LLM capabilities through low-rank factorization, the work delivers actionable insights beyond mere raw score comparisons.  \n4. **Candid Limitation Identification**: The paper openly highlights models’ weaknesses—such as challenges in visual state extraction, spatiotemporal reasoning, and long-context retrieval—and proposes concrete directions for improvement. This avoids the common pitfall of overemphasizing benchmark performance without addressing actionable gaps in model capability.  \n\n\n## Weaknesses  \n1. **Lack of Quantitative Support for Qualitative Analysis**: While Section 3.2 presents a qualitative analysis of model failures, it lacks accompanying quantitative validation. For instance, the paper could address this gap by using LLMs to annotate a subset of game trajectories, identifying the key failure reasons (e.g., “incorrect visual state parsing” vs. “poor long-horizon planning”) for each episode, and calculating the statistical proportion of failures attributed to each cause. Conducting this ablation experiment on a small scale would significantly enhance the credibility of the benchmark’s diagnostic claims.  \n2. **Limited Diversity in Evaluation Metrics**: The paper relies primarily on raw scores to evaluate model performance. While raw scores effectively reflect game progress from a human perspective, they provide limited procedural feedback for LLMs—failing to capture nuanced capabilities like reaching critical game nodes (e.g., accessing a bonus area in Super Mario Bros.) or acquiring key information (e.g., identifying critical evidence in Ace Attorney). Incorporating such procedural metrics would offer a more holistic view of model capabilities.  \n\n\n## Questions  \nQ1: Could the authors supplement the qualitative analysis with small-scale quantitative validation (e.g., trajectory annotation and failure reason statistics) as suggested in Weakness 1?  \nQ2: Could the authors propose additional, more diverse evaluation metrics—including procedural feedback indicators—to better capture nuanced model capabilities, as outlined in Weakness 2?"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "See Summary"}, "questions": {"value": "See Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cia5UuE5ok", "forum": "qeziG97WUZ", "replyto": "qeziG97WUZ", "signatures": ["ICLR.cc/2026/Conference/Submission14166/Reviewer_GmFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14166/Reviewer_GmFK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762908428939, "cdate": 1762908428939, "tmdate": 1762924625521, "mdate": 1762924625521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}