{"id": "QDmoLEJifR", "number": 24650, "cdate": 1758358959164, "mdate": 1759896757076, "content": {"title": "Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction", "abstract": "Learning effective numerical representations, or embeddings, of programs is a fundamental prerequisite for applying machine learning to automate and enhance compiler optimization. Prevailing paradigms, however, present a dilemma. Static representations, derived from source code or intermediate representation (IR), are efficient and deterministic but offer limited insight into how a program will behave or evolve under complex code transformations. Conversely, dynamic representations, which rely on runtime profiling, provide profound insights into performance bottlenecks but are often impractical for large-scale tasks due to prohibitive overhead and inherent non-determinism. This paper transcends this trade-off by proposing a novel quasi-dynamic framework for program representation. The core insight is to model a program's optimization sensitivity. We introduce the Program Behavior Spectrum, a new representation generated by probing a program's IR with a diverse set of optimization sequences and quantifying the resulting changes in its static features. To effectively encode this high-dimensional, continuous spectrum, we pioneer a compositional learning approach. Product Quantization is employed to discretize the continuous reaction vectors into structured, compositional sub-words. Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to learn the deep contextual grammar of these behavioral codes. Comprehensive experiments on two representative compiler optimization tasks---Best Pass Prediction and -Oz Benefit Prediction---demonstrate that our method outperforms state-of-the-art static baselines. Our code is publicly available at https://anonymous.4open.science/r/PREP-311F/.", "tldr": "", "keywords": ["Program Representation", "Compiler Optimization", "Behavioral Embedding"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fde646cd72e9445e8cd4e20aa28b9596f0ddb52.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce the Program Behavior Spectrum, a new representation generated by probing a program's IR with a diverse set of optimization sequences and quantifying the resulting changes in its static features. To effectively encode this high-dimensional, continuous spectrum, the authors  pioneer a compositional learning approach. Product Quantization is employed to discretize the continuous reaction vectors into structured, compositional sub-words. Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to learn the deep contextual grammar of these behavioral codes. Comprehensive experiments on two representative compiler optimization tasks---Best Pass Prediction and -Oz Benefit Prediction---demonstrate that our method outperforms state-of-the-art static baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method has some potential to overcome the limitation of static/dynamic representation method and encode the semantic of a program.\n2. The evaluation is thorough. Ablation studies show that PQ, scale-invariance, and Transformer components each contribute meaningfully.\n3. Open-source implementation is provided."}, "weaknesses": {"value": "1. The proposed method is limited to compiler optimization tasks, which, while technically meaningful, may be too specialized and fall outside the primary scope of interest for most ICLR readers.\n2. Missing references/comparison to several work on leveraging IR/compiler optimization for improving program embedding:\n    - Unleashing the power of compiler intermediate representation to enhance neural program embeddings ICSE 2022\n    - Ircoder: Intermediate representations make language models robust multilingual code generators ACL 2024\n    - ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding ICLR 2025\n3. Building on the point above, the choice of baselines appears outdated. All compared models are from before 2021, while many modern program embedding and code representation models have demonstrated stronger performance and broader applicability. The authors justify their baseline selection by requiring that each method be “pre-trained on the same algorithm classification dataset.” However, this constraint seems unnecessary in the current landscape, as most recent code embedding models are pre-trained on large-scale, diverse corpora. The paper should clarify the practical incentive for adopting the proposed model instead of fine-tuning these stronger, pre-trained representations on the target optimization tasks.\n\nMinor comments:\n1. Autophase should be explained in more details."}, "questions": {"value": "Please jutify the benefit of the proposed method over pre-train+fine-tuning paradigm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sbUrAg2rIT", "forum": "QDmoLEJifR", "replyto": "QDmoLEJifR", "signatures": ["ICLR.cc/2026/Conference/Submission24650/Reviewer_S2NV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24650/Reviewer_S2NV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575926157, "cdate": 1761575926157, "tmdate": 1762943146452, "mdate": 1762943146452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the trade-off between static and dynamic program representations by proposing a quasi-dynamic framework modeling a program’s optimization sensitivity. It introduces the Program Behavior Spectrum, a novel representation constructed by probing program IR using multiple optimization sequences and measuring the resulting changes in static features. These high-dimensional behavioral change vectors are then discretized via Product Quantization, and a transformer model (PQ-BERT) is pre-trained to learn compositional behavioral embeddings. The method is evaluated on two optimization prediction tasks: Best Pass Prediction and -OZ Benefit Prediction, and demonstrates improvements over static baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of deriving representations from optimization-induced behavioral variation is novel and addresses a meaningful gap between static embeddings and full dynamic profiling.\n- The authors validate the contribution of each component in the framework through ablation studies. \n- The authors go beyond downstream accuracy and examine the embedding space, demonstrating an effort towards enhancing interpretability and supporting the utility of their method. \n- The authors have provided the complete code for their experiments."}, "weaknesses": {"value": "- Limited Evaluation and Generalization: The evaluation is limited to two downstream tasks, which are narrow in scope. \n- No comparison with dynamic baselines. The approach is motivated as more efficient than dynamic profiling, but since it relies on generating multiple optimized variants per program, it would be helpful to include an analysis of the associated computational cost and scalability.\n- Minor fixes:\n- Line 145,146: horig and hopt -> h_orig and h_opt\n- Line 179: ..may suffers from -> may suffer from"}, "questions": {"value": "1. What is the computational cost of running the entire pipeline? \n2. Can the learned embeddings support optimization tasks beyond code-size improvement, such as runtime performance optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LWFkEAnSAa", "forum": "QDmoLEJifR", "replyto": "QDmoLEJifR", "signatures": ["ICLR.cc/2026/Conference/Submission24650/Reviewer_ijRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24650/Reviewer_ijRG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945196297, "cdate": 1761945196297, "tmdate": 1762943146134, "mdate": 1762943146134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a quasi dynamic program embedding that encodes how a program’s static features change in response to compiler optimization probes. The resulting Behavioral Spectrum is discretized via Product Quantization into compositional codes and modeled with a multi task Transformer (PQ BERT). On CompilerGym tasks, Best Pass Prediction and -Oz benefit regression, the approach improves substantially over static baselines, with informative ablations and embedding space analyses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tNew perspective: optimization sensitivity as representation. \n\n•\tStrong gains vs. static baselines on two tasks. \n\n•\tAblations and qualitative analyses add insight."}, "weaknesses": {"value": "1.\tThe paper’s central claim is that behavioral embeddings improve optimization prediction. However, neither downstream task measures runtime speedup or other practical metrics (e.g., energy, compile time). This weakens the argument that the representation is task-relevant beyond instruction count reduction. Adding runtime benchmarks or end-to-end pipelines would substantially strengthen the generality of the proposed representation. \n\n2.\tThe work frames itself between static and dynamic profiling, yet no dynamic baselines are included. Including one such baseline would help quantify the “best of both worlds” claim. \n\n3.\tIs the representation created specific to the task of instruction reduction? Or is it general? The representations that the authors compare to (e.g., IR2vec) are general representations that can be used for many tasks. If the proposed representation is specific to the task of instruction reduction, then it is natural that it would be better. Can the authors clarify how general their representation? i.e., how would it perform on other tasks (other than instruction reduction)."}, "questions": {"value": "•\tCan you include a dynamic baseline?\n\n•\tCan you use your representation for other tasks? Other than instruction reduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzueLFGr5y", "forum": "QDmoLEJifR", "replyto": "QDmoLEJifR", "signatures": ["ICLR.cc/2026/Conference/Submission24650/Reviewer_JrYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24650/Reviewer_JrYV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989685755, "cdate": 1761989685755, "tmdate": 1762943145894, "mdate": 1762943145894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}