{"id": "N6G2Mmz8qs", "number": 7872, "cdate": 1758040209102, "mdate": 1763645713502, "content": {"title": "ESNv2: Resurrecting Reservoir Computing in the Deep Learning era", "abstract": "Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing, yet its scalability remains severely constrained by the necessity of processing temporal data sequentially. In this work, we revisit RC through the lens of structured operators and state space modeling, introducing Parallel ESN (ParalESN), a framework that enables the construction of efficient reservoirs with diagonal linear recurrence in the complex space that can be parallelized during training. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of classical Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN attains comparable predictive accuracy to traditional RC on memory and forecasting benchmarks, while delivering substantial gains in training efficiency. On 1-D pixel-level classification tasks, the model achieves competitive accuracy with fully trainable networks, reducing computational costs and energy consumption. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.", "tldr": "Introducting a novel framework for the construction of efficient, randomized RNNs based on diagonal linear recurrence in the complex space.", "keywords": ["reservoir compting; echo state networks; recurrent neural networks;"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c742f31f1d5008d8ca4bf49a92c4508cd0ed8c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces ESNv2, revisits ESN with a diagonal complex-valued recurrence to enable parallelism, but offers limited real innovation beyond reinterpreting existing state-space and recurrent ideas. \n\nThe main contribution includes: 1. using a diagonal complex-valued linear recurrence that allows parallel computation. 2. adding a nonlinear mixing layer for expressivity while keeping the readout as the only trainable part."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work has a clear motivation. The author figure out the the lack of parallelism in RCs and attempt to address it.\n\n2. This work is well structured and easy to follow."}, "weaknesses": {"value": "Although the method achieves faster training by eliminating sequential dependencies, this improvement comes at the expense of reduced model expressiveness, since the diagonal recurrence structure limits the network’s ability to capture complex temporal interactions.\n\n\nThe main problem is that the experiments only demonstrated mainly on simplistic synthetic benchmarks such as memory and forecasting tasks; moreover, comparisons with modern deep models like Transformers or LRUs appear shallow and unconvincing, as ESNv2 lacks the flexibility and scalability required for real-world applications. \n\nSince the author has been benchmarking against the concept of SSM, I suggest the author test the real capability on text datasets.\n\nTherefore, I believe this work does not meet the acceptance criteria"}, "questions": {"value": "1.  Please use more solid and credible experiments to demonstrate the effectiveness of the proposed solution. The currently used dataset is too old and too small. It's even a benchmark Lorenz96 published in 1996. While the largest classification dataset is MINST. This cannot represent the latest research progress at all.\n\n2. These mini datasets cannot demonstrate the efficient and parallelized effects that the author claims to have proposed. In the current situation, I suggest the author consider ImageNet[1] as a baseline for classification task and PILE[2] for sequence modelling. \n\n\n[1] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009.\n\n[2] Gao, Leo, et al. \"The pile: An 800gb dataset of diverse text for language modeling.\" arXiv preprint arXiv:2101.00027 (2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dZwlwiLxIP", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Reviewer_FkkR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Reviewer_FkkR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533817929, "cdate": 1760533817929, "tmdate": 1762919906885, "mdate": 1762919906885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new type of RC architecture that can have multiple layers and can process sequence data in parallel. They show the new architecture, ESNv2, can achieve comparable performance to ESN on many tasks while being computationally more efficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of combining classical dynamical systems-inspired learning and contemporary large-scale modeling to bring reservoir computing into the deep learning regime is intriguing. If successful, it has the potential to scale up reservoir computing to tackle tasks that were previously out of reach. The problem is well motivated and the paper is well written."}, "weaknesses": {"value": "* ESN and RC are most commonly used in time-series forecasting tasks. On this crucial task, ESNv2 does not improve on existing architectures in terms of performance.\n* The benchmark on forecasting focuses on three very simple systems (Lorenz96, Mackey-Glass, NARMA), which may not be representative enough to draw robust conclusions about the forecasting capability of ESNv2 in general.\n* The theoretical results seem to be direct applications of existing results.\n* Things like the fading memory property are not defined."}, "questions": {"value": "* Why is ESNv2 better than ESN at certain tasks (e.g., 1-D pixel-level classification) but not other tasks (e.g., forecasting chaotic systems)?\n* One fundamental limitation of the traditional ESN is that the size of the reservoir cannot be scaled to billions of parameters due to the poor scaling of matrix inversion used in the training process (e.g., Ridge regression). Does ESNv2 address this limitation in any way? Is it possible to have a much larger reservoir in ESNv2 than in ESN?\n* Figure 5 seems to be partially incompatible with Figure 3. Why do ESN and ESN (deep) have high efficiency in Figure 5 but low efficiency in Figure 3?\n* It was mentioned that \"Observe that even ESNv2 (deep), despite consisting of multiple reservoir layers, trains faster than a traditional, shallow ESN consisting of just one layer.\" My understanding is that both ESNv2 and ESN train through Ridge regression. So how does ESNv2 train faster than ESN with the same number of trainable parameters (i.e., when the dimension of the regression problem is the same)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0SDwY5lmRe", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Reviewer_bxxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Reviewer_bxxD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760726809121, "cdate": 1760726809121, "tmdate": 1762919906414, "mdate": 1762919906414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The present manuscript proposes a structured method of Reservoir Computing, called ESNv2, that parallelize the input processing maintaining the Echo State Property, provided that the spectral radius of the diagonal weights is bounded, as proved in Theorem 1. \nMoreover, for every ESN an equivalent ESNv2 in terms of expressivity can be found. \nEvaluation on benchmarks is provided, comparing the proposed model with the traditional ESN and with popular recurrent models such as LSTM and Transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well organized and easily readable.\nThe proposed model is supported by a sound, although simple, theoretical characterization (Theorem 1 and Proposition1). \nThe metrics evaluated in the experimental framework give a broad overview of the performance of the proposed model, also comparing with deep learning alternatives such as LSTMs and Transformers."}, "weaknesses": {"value": "- Structured transforms in Reservoir Computing have already been proposed and investigated [1,2], but the present manuscript is totally missing a part of literature review in this regard, and therefore it lacks of a consequent comparison , in terms of performances and computational cost, with these structured methods. \n- at line 99 it is claimed that the weight matrices \"are generally sampled from a uniform distribution\"; the claim lacks of bibliography references; moreover, in earlier works [3], it is suggested that the weights are initialized from Gaussian distributions;\n- A reference and a consequent discussion to standard Deep Reservoir Computing is missing [4]\n- The title, in my opinion, is disregarding the huge recent literature of works that are considering reservoir computing as a sound recurrent alternative to deep (backpropagation-trained) learning models for what concern hardware implementations [5]. \nMinor comments:\n- In the proof of Theorem 1, there is a B in place of what it should be W_{in}; I believe it is so, because otherwise the proof wouldn't work.\n\n[1] Dong, J., Ohana, R., Rafayelyan, M., & Krzakala, F. (2020). Reservoir computing meets recurrent kernels and structured transforms. Advances in Neural Information Processing Systems, 33, 16785-16796.\n[2] D’Inverno, G. A., & Dong, J. (2025). Comparison of Reservoir Computing topologies using the Recurrent Kernel approach. Neurocomputing, 611, 128679.\n[3] Verstraeten, D., Schrauwen, B., d’Haene, M., & Stroobandt, D. (2007). An experimental unification of reservoir computing methods. Neural networks, 20(3), 391-403.\n[4] Gallicchio, C., Micheli, A., & Pedrelli, L. (2017). Deep reservoir computing: A critical experimental analysis. Neurocomputing, 268, 87-99.\n[5] Gallicchio, C., & Soriano, M. C. (2025). Hardware friendly deep reservoir computing. Neural Networks, 108079."}, "questions": {"value": "In view of what already said, my suggestion for the authors are listed as follows:\n- the authors should integrate a substantial comparison with already existing structured transforms for RC in terms of formulation, theoretical guarantees and experimental validation;\n- the authors may try to validate experimentally the performances in correspondence of Gaussian weight initialization;\n- I would suggest either to revise the title, or to integrate the manuscript with a convincing argument to support the current title;\n- all the missing references must be integrated in the manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hdcrPaYoWz", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Reviewer_KJoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Reviewer_KJoa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744256337, "cdate": 1761744256337, "tmdate": 1762919904806, "mdate": 1762919904806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework, ESNv2, which introduces the diagonal linear recurrence in the complex space into traditional RC systems. The paper also provides a theoretical analysis and empirical validation, demonstrating the model's efficiency and competitive performance compared to both classical RC and some deep learning models (LSTM, Transformer, LRU) on sequential MNIST tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of diagonal linear recurrence in the complex space is a contribution to RC that allows for parallelization during training.\n2. The paper includes a solid theoretical foundation, proving that ESNv2 preserves the Echo State Property (ESP) and universality guarantees, which strengthens its credibility.\n3. The paper is well-structured, with clear explanations and logical flow from theoretical concepts to practical experimentation."}, "weaknesses": {"value": "1. The name \"ESNv2\" does not adequately reflect the core innovations of the model. A more descriptive name that captures the essence of the diagonal linear recurrence and its parallelization capabilities would enhance clarity and impact.\n2. While the paper compares ESNv2 with various models, a more detailed comparative analysis with a wider range of state-of-the-art deep learning models, particularly in terms of specific applications, could enhance the discussion.\n3. The work does not deeply survey and discuss related works on ESN variants and Deep ESN, which could provide valuable context and highlight the novelty of ESNv2.\n4. The absence of a comparison with Mamba and related state space models is a notable gap. Including this comparison would enrich the findings and establish ESNv2's position more clearly within the landscape of contemporary models.\n5. The paper does not discuss how key parameters from traditional ESNs, such as input scaling and spectral radius, are set and their influence on the new model (missing in the methodology section). A detailed examination of these parameters would improve understanding of ESNv2's behavior and performance.\n6. The paper lacks a comparison of ESNv2's performance on real-world time series prediction tasks. Adding such analysis would provide practical insights into the model's applicability and effectiveness in real-world scenarios.\n7. The method of hyperparameter tuning is mentioned, but more details on the specific impact of different hyperparameters on performance could be beneficial for reproducibility and practical implementation."}, "questions": {"value": "In the part of Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yO52RIrTAK", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Reviewer_prv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Reviewer_prv8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971142359, "cdate": 1761971142359, "tmdate": 1762919904380, "mdate": 1762919904380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Message to all Reviewers"}, "comment": {"value": "Thank you for the comments, questions, and suggestions. We particularly appreciate your recognition that our work addresses a key limitation of reservoir computing, the lack of parallelism, by introducing diagonal linear recurrence to enable parallel training. We are also grateful that you found our paper well-structured with clear motivation, and that you noted the theoretical analysis strengthens our claims.\n\nWe have responded to individual reviewer comments.\n\nTo avoid confision, **we point out that our model \"ESNv2\" has been renamed to \"Parallel ESN (ParalESN)\"**, following Reviewer prv8's feedback. **We will refer to our approach as ParalESN going forward**. The manuscript has been updated accordingly.\n\n---\n\nAll in all, we believe that this work represents a novel contribution to reservoir computing and provides robust empirical results complemented by a sound theoretical analysis. We also believe that this work would benefit the ICLR community in this venue. We hope that the Reviewers agree, particularly given the new experiments, improved rewriting, and clarifications, and see fit to raise their scores."}}, "id": "BADRrW1YS4", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647072513, "cdate": 1763647072513, "tmdate": 1763653403502, "mdate": 1763653403502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Message to all Reviewers"}, "comment": {"value": "Thank you for the comments, questions, and suggestions. We particularly appreciate your recognition that our work addresses a key limitation of reservoir computing, the lack of parallelism, by introducing diagonal linear recurrence to enable parallel training. We also appreciate that you found our paper well-structured and with clear motivations, and that you noted the theoretical analysis strengthens our claims.\n\nWe have responded to individual reviewer comments.\n\nTo avoid confusion, **we point out that our model \"ESNv2\" has been renamed to \"Parallel ESN (ParalESN)\"**, following Reviewer prv8's feedback. **We will refer to our approach as ParalESN going forward**. The manuscript has been updated accordingly.\n\n---\n\nAll in all, we believe that this work represents a novel contribution to reservoir computing and provides robust empirical results complemented by a sound theoretical analysis. We also believe that this work would benefit the ICLR community in this venue. We hope that the Reviewers agree, particularly given the new experiments, improved rewriting, and clarifications, and see fit to raise their scores."}}, "id": "BADRrW1YS4", "forum": "N6G2Mmz8qs", "replyto": "N6G2Mmz8qs", "signatures": ["ICLR.cc/2026/Conference/Submission7872/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7872/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission7872/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647072513, "cdate": 1763647072513, "tmdate": 1763674833832, "mdate": 1763674833832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}