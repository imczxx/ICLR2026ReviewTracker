{"id": "h4DNjtyRRA", "number": 5583, "cdate": 1757921234417, "mdate": 1763724600457, "content": {"title": "ReCode: Unify Plan and Action for Universal Granularity Control", "abstract": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action.\nHowever, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities.\nThis limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization.\nWe propose **ReCode** (**Re**cursive **Code** Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation.\nIn this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions.\nThis recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes.\nExtensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control.", "tldr": "", "keywords": ["Language agent", "decision making", "planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4010b87a0130cdcb44563ac96e5b1c33dbdfa0a0.pdf", "supplementary_material": "/attachment/abb5d87320d3afc8dece591d4802e57f1292a1b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ReCode, a recursive code generation paradigm for LLM-based agents that unifies planning and action within a single executable code framework. The key idea is to treat planning as high-level action, **recursively** decomposing abstract placeholder functions into executable API calls until primitive actions are reached. This approach aims to enable dynamic granularity control, eliminate the need for predefined action spaces, and generate rich hierarchical training data. The authors evaluate ReCode across three text-based environments (ALFWorld, WebShop, ScienceWorld) and demonstrate improvements in inference performance, generalization, and training efficiency compared to strong baselines like ReAct and CodeAct."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This approach of initially employing placeholders for complex actions and iteratively refining them into atomic actions or more detailed plans is insightful.\n2. Both plans and actions are represented as Python function calls (placeholders, atomic), which are generated and executed by a single policy $\\pi$, thereby eliminating the conventional separation between planning and execution. This is conceptually elegant and practically implementable.\n3. Figure 2 offers an intuitive illustration. Once you see it, you pretty much get how ReCode works.\n4. The Cost Analysis effectively demonstrates the cost-efficient nature of the ReCode framework."}, "weaknesses": {"value": "1. About the writing, it would be better to just clearly list out the specific contributions in the Introduction.\n2. Algorithm 1 mentions using a HeuristicConvert function to turn task instruction into a root. I feel like this part isn't really explained—it's not clear how it actually works.\n3. All three are text-world benchmarks. This seems a bit limited. I'm not totally convinced it would work in more complex settings. I'd like to see how it performs on more diverse and harder benchmarks, like EXP-Bench [1], or RExBench [2].\n\n[1] Kon, P. T. J., Liu, J., Zhu, X., Ding, Q., Peng, J., Xing, J., ... & Chen, A. (2025). EXP-Bench: Can AI Conduct AI Research Experiments?. arXiv preprint arXiv:2505.24785.\n\n[2] Edwards, N., Lee, Y., Mao, Y. A., Qin, Y., Schuster, S., & Kim, N. (2025). RExBench: Can coding agents autonomously implement AI research extensions?. arXiv preprint arXiv:2506.22598."}, "questions": {"value": "1. When a placeholder expansion fails (invalid code, violated state assumptions), is there any recovering or replanning mechanism? Can you quantify the recovery rate and its extra cost?\n2. About the HeuristicConvert part, could you share some insights into how it was designed? It would be great to see an ablation study testing if different heuristics would change the results.\n3. Can you conduct experiments on EXP-bench, or RExBench? \n4. For Table 2, could you also add the results for AdaPlanner and ADaPT using models like Gemini 2.5 Flash and DeepSeek-V3.1 on these three environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZF3gDbiZBR", "forum": "h4DNjtyRRA", "replyto": "h4DNjtyRRA", "signatures": ["ICLR.cc/2026/Conference/Submission5583/Reviewer_VZQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5583/Reviewer_VZQm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487738342, "cdate": 1761487738342, "tmdate": 1762918147182, "mdate": 1762918147182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Public Comment 1"}, "comment": {"value": "We thank the reviewers for their helpful feedback.\nWe have substantially improved the paper, and all important revised content is marked in **blue**.\n**Below is a concise summary of the key updates**:\n1. (For Reviewer VZQm) We rewrote the contribution statements at the end of **Section 1 Introduction** to clearly articulate ReCode’s key novelty: unifying planning and action, introducing recursive code generation, and enabling multi-granularity training data.\n2. (For Reviewer SKxJ, 23Bv) We expanded **Section 2 Related Work** by adding missing discussions on recursive agents, hierarchical planning, and code-as-policy paradigms. Particularly, we clarified how ReCode differs fundamentally from THREAD and REPL-Plan.\n3. We redesigned **Figure 2** to explicitly separate the generation and execution flows, and added missing implementation details in **Section 3.3**, including task initialization, context management, error handling, and depth control.\n4. (For Reviewer wfpU, VZQm) We added **AdaPlanner** and **ADaPT** in **Table 2** using Gemini 2.5 Flash and DeepSeek-V3.1 in addition to GPT-4o-mini, demonstrating that ReCode’s gains hold consistently across all baselines with different model families.\n5. (For Reviewer SKxJ, 23Bv, wfpU) We added an ablation study on maximum recursion depth in **Section 4.2**, showing an inverted-U trend and justifying the chosen depth setting.\n6. We added experiments analyzing data efficiency during supervised fine-tuning in **Section 4.3**, demonstrating that ReCode produces more compact yet more informative trajectories than ReAct.\n7. (For Reviewer SKxJ, wfpU) We moved the original case-study examples to **Appendix A.5** and replaced **Section 4.4** with new experiments conducted in the **ManiSkill embodied manipulation environment**. These experiments verify that ReCode maintains advanced performance and robust decision-granularity control in continuous actions settings.\n8. (For Reviewer SKxJ, 23Bv) We added runtime statistics in the appendix to better document ReCode’s runtime behavior and computational cost."}}, "id": "J8n1PTvi8C", "forum": "h4DNjtyRRA", "replyto": "h4DNjtyRRA", "signatures": ["ICLR.cc/2026/Conference/Submission5583/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5583/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5583/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724614593, "cdate": 1763724614593, "tmdate": 1763724614593, "mdate": 1763724614593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReCode, a novel paradigm for LLM-based agents that unifies planning and action through recursive code generation. The key insight is that planning and action are not fundamentally different but represent decisions at different levels of abstraction. ReCode represents both plans and actions as executable code, with high-level plans as placeholder functions that are recursively refined into primitive executable actions. The authors demonstrate that this approach achieves superior performance across three benchmark environments (ALFWorld, ScienceWorld, WebShop) with over 20.9% improvement in inference and remarkable training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated and easy to read\n- ReCode achieves remarkable cost efficiency and training efficience. This is a significant practical advantage."}, "weaknesses": {"value": "- While the three environments are diverse, they are all text-based simulation environments. The approach needs validation on more complex, real-world tasks or environments with continuous action spaces. According to the paper's content, it seems possible to have primitive actions at much lower levels of API (for example, continuous actions like moving forward 3.5m). Experiments on this aspect are needed.\n- The paper doesn't discuss how the system handles errors in code generation or execution failures. If errors occur in some parts of the code, it's questionable whether the task can be performed normally. If not, can this methodology effectively respond when LLM performance decreases?"}, "questions": {"value": "- In Section 4.2, we can see that AdaPlanner has the second-best performance. Can we see the results of testing AdaPlanner based on Gemini-Flash or DeepSeek?\n- What is the maximum recursion depth observed in practice, and how does performance degrade with increasing task complexity?\n- Can you provide analysis on failure modes? When does the recursive expansion fail to converge to executable actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oEozY4aL2P", "forum": "h4DNjtyRRA", "replyto": "h4DNjtyRRA", "signatures": ["ICLR.cc/2026/Conference/Submission5583/Reviewer_wfpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5583/Reviewer_wfpU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727556156, "cdate": 1761727556156, "tmdate": 1762918146910, "mdate": 1762918146910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes ReCode, a single-policy, recursive “code as plan and action” framework that refines unimplemented placeholders into executable steps, enabling flexible control over reasoning granularity. Across multiple simulated environments, it outperforms code-reasoning baselines and its hierarchical trajectories further strengthen supervised fine-tuning; however, the novelty over prior recursive code generation works (e.g., REPL-Plan) remains unclear without head-to-head comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*  **Simple, unified mechanism.** Both plans  and actions are written as code, and the system only recurses when something isn’t executable, easy to reason about and implement. \n* **Gains at lower cost.** Across benchmarks, the method outperforms baselines at lower cost."}, "weaknesses": {"value": "* The novelty over prior similar recursive code work is unclear; it could be better to add a brief comparison to previous works(e.g., REPL-Plan, Code-as-Policies) to make the contribution explicit.\n*  No granulity measurement and analysis."}, "questions": {"value": "* Could you share distributions of recursion depth and the number of expanded placeholders per episode? \n\n* Could you add a brief, explicit paragraph contrasting ReCode with other recursive code generation papers, i.e., REPL-Plan and Code-as-Policies? A small comparison table would make the distinctions especially clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c0UjFbA2iM", "forum": "h4DNjtyRRA", "replyto": "h4DNjtyRRA", "signatures": ["ICLR.cc/2026/Conference/Submission5583/Reviewer_23Bv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5583/Reviewer_23Bv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948237476, "cdate": 1761948237476, "tmdate": 1762918146623, "mdate": 1762918146623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReCode, a framework that represents an policy as a recursive code generator, unifying planning and action within a single paradigm. High-level tasks are expressed as abstract placeholder functions that the LLM recursively expands into finer-grained subfunctions and primitive actions, enabling dynamic control over decision granularity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "ReCode unifies high-level planning and low-level action within a single code-based framework, allowing LLM agents to dynamically adjust decision granularity."}, "weaknesses": {"value": "1. The proposed approach is closely related to recent advances in code-as-policies paradigms that leverage code-generation LLMs (e.g., [1–5]). However, the paper does not sufficiently analyze or position ReCode against these methods in the related work section.\n\n[1] Code as Policies: Language Model Programs for Embodied Control. ICRA 2023.\n\n[2] RoboCodex: Multimodal Code Generation for Robotic Behavior Synthesis. ICML 2024.\n\n[3] PoAct: Policy and Action Dual-Control Agent for Generalized Applications. arXiv  2025.\n\n[4] Executable Code Actions Elicit Better LLM Agents. ICML 2024\n\n[5] Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning, arxiv 2025\n\n2. The recursive expansion and execution of functions or subtasks in ReCode resemble hierarchical code-as-policies frameworks previously explored in works [6, 7]. Since these studies also pursue recursive, code-centric planning structures, the paper needs to articulate more clearly what distinguishes ReCode’s recursive granularity control from prior hierarchical code-generation methods. Moreover, given the conceptual similarity, it would be valuable to include empirical comparisons with particularly [7], one unifying reasoning and acting through executable code with adaptive feedback during execution. Technical novelties should be clearly specified. \n\n[6] Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought. NeurIPS 2023.\n\n[7] Interactive and Expressive Code-Augmented Planning with Large Language Models.  arXiv 2024.\n\n3. While the paper claims that flexible control of decision granularity leads to superior adaptability and efficiency, the current experiment results do not isolate granularity as a directly measurable factor. To substantiate this claim, ablation experiments varying the recursive depth could demonstrate how granularity impacts reward, cost, and performance–efficiency trade-offs.\n\n4. Beyond qualitative case studies, the paper would present quantitative analyses that reveal how granularity dynamically changes during execution. Statistics such as the average depth of generated decision trees, branching factors, and the ratio of placeholder functions to primitive actions across different tasks would provide stronger empirical support for the proposed mechanism.\n\n5. While ReCode emphasizes the unification of hierarchical decision-making within a single code-based framework, the chosen benchmarks and baselines primarily focus on high-level planning tasks. These environments do not inherently require dynamic adjustment of decision granularity, as most decisions occur at task-planning level. For instance, the ALFWorld case study in Appendix Figure 3 represents a classic high-level planning scenario where expressing actions as executable code offers limited additional benefit over traditional reasoning.\n\n6. To substantiate the claimed benefits of unified decision granularity, ReCode would be evaluated in environments where both high-level planning and low-level control are essential such as robotic manipulation or embodied control tasks. In such settings, policies must integrate perception, planning, and motor control, providing a more realistic test of ReCode’s ability to adjust decision granularity. Moreover, conducting quantitative comparisons with recent frameworks like Code-as-Policies or Vision-Language-Action (VLA) models would more convincingly demonstrate the distinct contribution and practical value of ReCode’s recursive, code-centric decision mechanism."}, "questions": {"value": "How is the mechanism for learning and controlling decision granularity concretely implemented and trained in ReCode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m49Nhw7QgR", "forum": "h4DNjtyRRA", "replyto": "h4DNjtyRRA", "signatures": ["ICLR.cc/2026/Conference/Submission5583/Reviewer_SKxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5583/Reviewer_SKxJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995771533, "cdate": 1761995771533, "tmdate": 1762918146371, "mdate": 1762918146371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}