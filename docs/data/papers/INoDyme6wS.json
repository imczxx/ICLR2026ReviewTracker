{"id": "INoDyme6wS", "number": 6211, "cdate": 1757958661776, "mdate": 1759897929730, "content": {"title": "Generalization in Online Reinforcement Learning for Mobile Agents", "abstract": "Graphical user interface (GUI)-based mobile agents automate digital tasks on mobile devices by interpreting natural-language instructions and interacting with the screen. While recent methods apply reinforcement learning (RL) to train vision-language-model(VLM) agents in interactive environments with a primary focus on performance, generalization remains underexplored due to the lack of standardized benchmarks and open-source RL systems. In this work, we formalize the problem as a Contextual Markov Decision Process (CMDP) and introduce AndroidWorld-Generalization, a benchmark with three increasingly challenging regimes for evaluating zero-shot generalization to unseen task instances, templates, and applications. We further propose an end-to-end RL framework that integrates Group Relative Policy Optimization (GRPO) with a scalable rollout collection system, consisting of containerized infrastructure, asynchronous execution, and error recovery to support reliable and efficient training. Experiments on AndroidWorld-Generalization show that RL enables a 7B-parameter VLM agent to surpass supervised fine-tuning baselines, yielding a 26.1\\% improvement on unseen instances but only limited gains on unseen templates (15.7\\%) and apps (8.3\\%), underscoring the challenges of generalization. As a preliminary step, we demonstrate that few-shot adaptation at test-time improves performance on unseen apps, motivating future research in this direction. To support reproducibility and fair comparison, we open-source the full framework, including environment, tasks, models, prompts, and training infrastructure.", "tldr": "", "keywords": ["agentic models", "Reinforcement Learning", "Mobile Use", "GUI", "Vision Language Model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad78e273c220d7f3fba3f8b907a2ad6e83bc88d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates generalization in online reinforcement learning for GUI-based mobile agents. The authors formalize the problem as a Contextual Markov Decision Process (CMDP) and introduce AndroidWorld-Generalization, a benchmark with three progressive difficulty regimes: Unseen Instance, Unseen Template, and Unseen Application. They develop a fully open-source end-to-end RL framework integrating Group Relative Policy Optimization (GRPO) with a scalable rollout collection system featuring containerized infrastructure, asynchronous execution, and error recovery. Experiments show that RL enables a 7B-parameter VLM agent to achieve good improvement on unseen instances over supervised fine-tuning baselines, but limited gains on unseen templates and apps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel benchmark design: **Generalization is a big issue and bottleneck under this topic.** The three-tiered generalization benchmark (Unseen Instance/Template/App) provides a systematic framework for evaluating zero-shot transfer, addressing a significant gap in existing mobile agent research\n\n- Practical system design: The scalable rollout collection system with Docker containerization, asynchronous execution, and error recovery addresses real engineering challenges in RL for mobile environments\n\n- Insightful case studies: The analysis of transferable skills in Unseen Template provides a valuable understanding of what enables generalization"}, "weaknesses": {"value": "- Limited scale: The benchmark covers only 20 applications and 116 templates, which the authors acknowledge constrains generalization evaluation and training diversity\n\n- Poor generalization to harder regimes: The dramatic performance drop on Unseen Template (15.7%) and especially Unseen App (8.3%) suggests fundamental limitations that aren't adequately addressed\n\n- Few-shot adaptation is underdeveloped: The test-time adaptation experiments (Section 5.1, Q3) are preliminary and don't explore important questions like optimal adaptation strategies, data efficiency, or when to apply adaptation"}, "questions": {"value": "- Why GRPO specifically? What motivated choosing GRPO over other online RL algorithms like PPO or actor-critic methods? Have you experimented with alternatives?\n\n- Trajectory-level advantages: In Equation 3, you broadcast the trajectory-level advantage uniformly to all timesteps. How does this compare to using per-step advantages or other credit assignment strategies?\n\n- Curriculum learning: What criteria determine progression through curriculum stages (Easy → Easy+Medium → All)? Is this transition automated or manual?\n\n- Adaptation strategy: Why only 50 fine-tuning steps for adaptation? How sensitive is performance to this choice?\n\n- Per-app vs. All-app: The Per-App adaptation outperforms All-App by 6.3%. Does this suggest fundamental limits to cross-app generalization, or could All-App improve with more data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hqp7R3tO3u", "forum": "INoDyme6wS", "replyto": "INoDyme6wS", "signatures": ["ICLR.cc/2026/Conference/Submission6211/Reviewer_yrmE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6211/Reviewer_yrmE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775155032, "cdate": 1761775155032, "tmdate": 1762918547336, "mdate": 1762918547336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark, AndroidWorld-Generalization, that evaluates the RL-trained GUI-based mobile agents' capability on generalizing to unseen tasks. They also open-source an RL framework for mobile agent RL training. In this framework, they implement the GRPO algorithm, and leverage asynchronous simulation for better scalability. Finally, they conduct experiments to show the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clearly written and easy to follow\n2. This paper studies generalization to new tasks, which is a very important question about algorithm evaluation.\n3. The benchmark and training framework is going to be open-sourced"}, "weaknesses": {"value": "My main criticism about this paper is that I do not follow the overall logic of the paper. The paper starts with a benchmark, highlighting that it can test the model's performance on unseen tasks. However, they use the environments from AndroidWorld, which limits the novelty. Next, the paper proposes an RL training framework. A framework should facilitate the implementation of many algorithms, whereas the paper only implement GRPO. The paper claims that the the agent trained under this framework outperforms prior works, but I do not see where the improvements come from. See questions for details"}, "questions": {"value": "1. To my understanding, for any benchmark that includes multiple tasks, we can group the tasks manually into training set, test set, and validation set. Besides this, what is the novelty of AndroidWorld?\n2. How does the RL training framework facilitate the implementation of algorithms besides GRPO? Why are current RL training frameworks not suitable for mobile agent training? How does the scalability of the existing frameworks compare to the framework you proposed?\n3. How are the prior works in Table 2 trained? Where is the improvement of this paper coming from over prior works? Training agents using RL and the GRPO algorithm is not new. Are the authors trying to say this paper is the first to use RL on mobile agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gLCCgi51Vq", "forum": "INoDyme6wS", "replyto": "INoDyme6wS", "signatures": ["ICLR.cc/2026/Conference/Submission6211/Reviewer_UZoD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6211/Reviewer_UZoD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029556400, "cdate": 1762029556400, "tmdate": 1762918546625, "mdate": 1762918546625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the generalization capabilities of device control agents, and introduces a new benchmark that specifically tests generalization in terms of task instances, templates (tasks), and apps. They also experiment with RL-based fine-tuning and test-time adaptation as approaches for improving generalization. The paper's core contributions are to introduce a new benchmark focused on such generalization, and present experiments using RL and test-time adaptation in this generalization problem. In conducting the RL experiments, they also develop software that supports future work in RL training for device control agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The formalization of generalization as CDMP is nice.\n* Results demonstrate that, like prior work  has found, RL-based fine-tuning can improve agent performance, even in difficult generalization problems."}, "weaknesses": {"value": "* The intro has a ton of citations in it, these should really only be in the related work unless they are central to the core argument of the paper\n* The test sizes are very small. What is prohibiting you from generating even more held-out test data, since it's all generated anyway? Especially because in Section 5.1 you are already expanding it to even more templates for this experiment. Perhaps I am wrong about this estimate because the format of Table 1 is confusing, but computing accuracy over just 69 examples (in unseen app) is not going to result in a statistically significant difference when performance gaps are just ~8%.\n* Test data should not be used for analyzing training dynamics, e.g., in Fig 3, 4. This serves as a form of information leakage in the experiment, where we are no longer using the test data to evaluate true generalization of the approach to held-out, IID data."}, "questions": {"value": "* I don't understand how Table 1 is formatted. Why are there training examples for unseen instance/template/app? Shouldn't these be held-out until test only? And why do each of the counts also include instance/template/app breakdowns? In general, I'm confused on what the training/testing setup is like in terms of data available at each stage wrt. generalization dimension being tested.\n* How is task difficulty determined? This is coming from AndroidWorld, but how do they define it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9wJVDVICUd", "forum": "INoDyme6wS", "replyto": "INoDyme6wS", "signatures": ["ICLR.cc/2026/Conference/Submission6211/Reviewer_pnBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6211/Reviewer_pnBC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123890477, "cdate": 1762123890477, "tmdate": 1762918545796, "mdate": 1762918545796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}