{"id": "9syxIziMpv", "number": 5043, "cdate": 1757838614224, "mdate": 1759897998514, "content": {"title": "Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression", "abstract": "Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image—particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction.", "tldr": "", "keywords": ["dataset pruning", "dataset compression", "quantization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/714033f174b2fcc27dee46a63af1fd7262de460d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Dataset Color Quantization (DCQ), a training-oriented framework that compresses entire training sets by reducing color-space redundancy while attempting to preserve semantic content and texture. The method involves three main stages:\n1. Chromaticity-Aware Clustering (CAC): Images are clustered based on their CNN features. A shared color palette is then generated for all images within the same cluster to enforce cross-image color consistency.\n2. Attention-Guided Palette Allocation: It uses Grad-CAM++ attention maps from a pre-trained model to identify semantically important regions. This guides the palette allocation to prioritize \"High-Impact Palettes\" crucial for model performance.\n3. Texture-Preserved Palette Optimization: Finally, it performs a differentiable palette refinement using a Sobel operator-based edge loss to minimize texture loss and preserve structural details.\n\nExperiments on CIFAR-10/100, Tiny-ImageNet, and ImageNet-1K report strong accuracy at extremely low color bit-depths (e.g., 2-bit) and favorable comparisons against both image-wise Color Quantization (CQ) baselines and dataset-pruning methods"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental gains are significant. For example, on CIFAR-10 at 2 bits (4 colors), DCQ achieves 89.15% accuracy, while the ColorCNN baseline (adapted for training) achieves 59.15%. The method also consistently outperforms all dataset pruning baselines at high compression ratios, such as a 79.9% accuracy on CIFAR-10 at a 96% pruning ratio (1-bit), far exceeding the next-best CCS method.\n\n2. The authors provide a thorough set of ablation studies to justify their design choices, including the optimal number of clusters (20), the superiority of shallow-layer features for clustering, the choice of attention mechanism (Grad-CAM++), the positive impact of the texture-preservation loss, and the optimal percentage of pixels to retain via attention (50%).\n\n3. The paper demonstrates that DCQ is orthogonal to other compression techniques like dataset pruning. It can be combined with pruning (e.g., CCSAUM) to achieve extreme compression ratios (up to 99.2%)  while maintaining high accuracy."}, "weaknesses": {"value": "1. Baseline Fairness:\nCQ Baselines: The paper adapts several image-wise CQ methods (like ColorCNN) that were originally designed for inference (quantizing the test set) to a new training task (quantizing the train set). While the authors acknowledge this protocol difference, it's unclear if these baselines were properly tuned for this new \"quantize-then-train\" setting, which could overstate DCQ's relative performance.\n2.  The paper defines its compression ratio solely based on the reduction in color bits (e.g., $q_r = 1 - q/24$). This is optimistic as it only accounts for the per-pixel index map storage and ignores the significant overhead of storing the shared palettes (e.g., $3 \\times K \\times 2^q$ bits) and any metadata for cluster assignments. A true evaluation of compression would also require reporting the total on-disk file size (palettes + indices) and comparing it against standard codecs (like PNG, JPEG, or WebP) or learned image compressors at a similar quality level.\n3. The \"Attention-Guided Palette Allocation\" mechanism uses Grad-CAM++ generated from a \"task-specific pre-trained model\". This means information from the ground-truth labels (which trained the pre-trained model) is used to guide the compression of the training images. This is a form of information leakage that mixes supervision with the data representation itself, which could be a confounding factor in the results."}, "questions": {"value": "See weakness. I wish the authors could clarify these questions, and I am willing to reevaluate the paper based on the reply."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IUZG7yPpWb", "forum": "9syxIziMpv", "replyto": "9syxIziMpv", "signatures": ["ICLR.cc/2026/Conference/Submission5043/Reviewer_Jn5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5043/Reviewer_Jn5n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666372178, "cdate": 1761666372178, "tmdate": 1762917839794, "mdate": 1762917839794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper investigates color quantization via the usage of network features and clustering methods. specifically, it does this from a perspective of training dataset compression, as opposed to the per-image compression for model inference in existing research [ColorCNN,  ColorCNN+]. first, it splits the images in a dataset into groups according to their neural network features, forming clusters of similar colors. then, for images within each cluster, it divides each image into the high and low importance regions according to Grad-CAM attention map. finally, the quantized color map is optimized through a differentiable palette optimization model. on several benchmarks, the authors reported competitive results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ using color quantization to compress training datasets seems an interesting direction.\n+ good presentation overall, with clear motivation and easy-to-read language. \n+ the bi-level compression method achieves good results, plus the visual results seem good.\n+ ablation study verifies the effectiveness of the proposed method."}, "weaknesses": {"value": "- some clarity issues since there are two levels of k-means clustering. the reviewer recommends highlighting the cluster's level when introducing them, e.g., change the title for Table 2b and 2c.\n- please consider including more visual comparisons on different color spaces sizes."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HY8kdDY0WI", "forum": "9syxIziMpv", "replyto": "9syxIziMpv", "signatures": ["ICLR.cc/2026/Conference/Submission5043/Reviewer_NGTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5043/Reviewer_NGTe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819100578, "cdate": 1761819100578, "tmdate": 1762917839391, "mdate": 1762917839391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a training-oriented dataset color quantization framework that learns shared, attention-guided, texture-preserving color palettes at the dataset or cluster level to reduce storage while keeping the compressed dataset effective for training. The approach demonstrates strong classification accuracy under aggressive color-bit constraints and compares favorably to pruning and distillation methods at matched compression ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work focuses on color-level redundancy as a distinct dimension of dataset compression, rather than only reducing samples or resolution. This makes the problem practically relevant for scenarios where storage and bandwidth are constrained but training quality must be preserved.\n\n2. The proposed framework combines chromaticity-aware clustering, attention-guided palette allocation, and differentiable, texture-preserving refinement. The components are mutually consistent and clearly aligned with the training objective.\n\n3. The method achieves strong results when colors are heavily quantized (e.g., 1–6 bits) and remains competitive against dataset pruning or distillation when the comparison is made at the same overall compression budget.\n\n4. Experiments on large-scale image classification settings indicate that the method can, in principle, be applied beyond toy scenarios."}, "weaknesses": {"value": "1. Most individual elements (clustering, attention guidance, STE-based refinement) are known in the literature; the central contribution lies in integrating them for dataset-level color compression. Readers may perceive this as a strong system design rather than a conceptual breakthrough.\n\n2. The paper does not fully quantify the training-time overhead of palette learning, attention computation, and differentiable refinement,  particularly on large datasets and modern backbones.\n\n3. The evaluation is centered on image classification. It remains unclear how well the approach generalizes to tasks that are more sensitive to local or fine-grained color cues, such as fine-grained recognition, detection, or segmentation.\n\n4. While comparisons with pruning and distillation are useful, some aspects of the evaluation protocol (training schedules, augmentation ranges, and budget alignment) could be specified more tightly to eliminate possible confounding factors."}, "questions": {"value": "1. What is the end-to-end compute and memory overhead of the proposed DCQ pipeline compared with pruning or distillation methods at the same compression ratio, especially on large-scale datasets?\n\n2. How sensitive is the method to the choice of cluster count and attention mechanism, and is there a recommended default or auto-tuning strategy that can be applied across datasets and resolutions?\n\n3. How does the method behave on tasks where discriminative information is carried by subtle color patterns (e.g., fine-grained species classification)?\n\n4. Can the proposed palette sharing introduce harmful color aliasing or loss of spatial cues in detection/segmentation settings, and are there preliminary results in that direction?\n\n5. Does the attention-guided palette allocation amplify spurious or dataset-specific artifacts learned by the proxy model, and how robust is the method under label noise or distribution shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WVnvikB2lL", "forum": "9syxIziMpv", "replyto": "9syxIziMpv", "signatures": ["ICLR.cc/2026/Conference/Submission5043/Reviewer_KUXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5043/Reviewer_KUXK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825011947, "cdate": 1761825011947, "tmdate": 1762917839047, "mdate": 1762917839047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dataset Color Quantization (DCQ), a framework for compressing visual datasets by reducing color-space redundancy while preserving information critical for model training. Unlike existing methods that prune samples or perform image-wise quantization, DCQ introduces shared palettes across chromatically similar images via clustering, guided by shallow-layer features of a pre-trained network. It further enhances semantic preservation through attention-guided bit allocation and optimizes palette quality using edge-preserving differentiable quantization. Experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly outperforms both traditional color quantization and dataset pruning methods under aggressive compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  This work proposes a dataset-level color quantization framework tailored specifically for training tasks, overcoming the limitations of traditional color quantization methods. It effectively balances the trade-off between storage compression and model trainability.\n\n-  This paper introduces the training-oriented dataset color quantization framework that integrates (1) a shared clustering-based palette, (2) attention-guided bit allocation, and (3) edge-preserving optimization. \n\n- The proposed method significantly outperforms existing color quantization and dataset pruning approaches across multiple benchmarks, including CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K. \n\n-  The experimental evaluation is comprehensive, covering multiple benchmark datasets and network architectures (e.g., ResNet, ShuffleNet, MobileNet-v2, ViT)."}, "weaknesses": {"value": "1. The method proposed in this paper reduces dataset storage by compressing the color space, whereas traditional dataset pruning achieves this goal by removing data samples. However, directly comparing these two approaches is somewhat unfair, as the method in this paper does not reduce the actual number of training samples. In other words, the proposed approach may not necessarily decrease the total number of training iterations required for the model training, leading to have some questions about the paper’s claim of improving model training efficiency. Therefore, the authors should analyze the relationship between the number of training iterations and accuracy.\n2. Some recent state-of-the-art or more related methods are missing, such as [1,2,3,4,5].\n3. Color quantization, in a sense, functions similarly to data augmentation. What if dataset pruning is also combined with a similar data augmentation strategy? If such a combination also yielded strong performance, the contribution of this paper would be somewhat diminished, because dataset pruning plus data augmentation would not only reduce storage requirements but also decrease the number of training iterations while maintaining performance.\n\n[1] Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty. ICML 2025.\n[2] Dataset Quantization. ICCV 2023\n[3] Dataset quantization with active learning based adaptive sampling. ECCV 2024.\n[4] Color-Oriented Redundancy Reduction in Dataset Distillation, NeurIPS 2024\n[5] Adaptive Dataset Quantization, AAAI2025"}, "questions": {"value": "See weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5WhS3DB7Tc", "forum": "9syxIziMpv", "replyto": "9syxIziMpv", "signatures": ["ICLR.cc/2026/Conference/Submission5043/Reviewer_FTHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5043/Reviewer_FTHp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967483881, "cdate": 1761967483881, "tmdate": 1762917838765, "mdate": 1762917838765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel method, Dataset Color Quantization (DCQ), that reduces dataset storage by compressing color information rather than discarding samples. Unlike traditional pruning or distillation methods that lower dataset size by removing images, DCQ targets color-space redundancy within images to achieve compression while preserving critical training information. The proposed method does not offer significant training or inference time savings. The experiments are based on classification tasks over CIFAR-10/100, TinyImageNet, and ImageNet-1k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's method is clearly stated, and the paper is easy to follow. \n\n2. The proposed method is intuitive and proven to be effective under the defined settings."}, "weaknesses": {"value": "1. I am not very clear about the comparison with direct image quantization, especially as the size of the dataset increases. For example, if extending the experiment to ImageNet 22k, will the proposed method still outperform the direct image quantization?\n\n2. As the classification task is highly abstract, the proposed method could work. However, it is not clear if the method still works for dense prediction tasks such as image segmentation and object detection. It would make the paper stronger if there were some experiments on generative tasks. Besides, it is also interesting to see if the proposed method could work together with model quantization, which could lead to significant model training and inference speed-up."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Edj8NIgQYW", "forum": "9syxIziMpv", "replyto": "9syxIziMpv", "signatures": ["ICLR.cc/2026/Conference/Submission5043/Reviewer_ivQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5043/Reviewer_ivQG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101363147, "cdate": 1762101363147, "tmdate": 1762917838364, "mdate": 1762917838364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}