{"id": "uXmbrTlko7", "number": 5793, "cdate": 1757935485455, "mdate": 1759897952930, "content": {"title": "ScalingCache: Extreme Acceleration of DiTs through Difference Scaling and Dynamic Interval Caching", "abstract": "Diffusion Transformers (DiTs) have emerged as powerful generative models, but their iterative denoising structure and deep transformer blocks incur substantial computational overhead, limiting the accessibility and practical deployment of high-quality video generation. To address this bottleneck, we propose ScalingCache, a training-free acceleration framework specifically designed for DiTs. ScalingCache exploits the inherent redundancy in model representations by performing lightweight offline analysis on a small number of samples and dynamically reusing previously computed activations during inference, thereby avoiding full computation at certain denoising steps. Experimental results demonstrate that ScalingCache achieves significant acceleration in both image and video generation tasks while maintaining near-lossless generation quality. On widely used video generation models including Wan2.1 and HunyuanVideo, it achieves approximately 2.5$\\times$ acceleration with only 0.5$\\%$ drop in VBench scores; on FLUX, it achieves 3.1$\\times$ near-lossless acceleration, with human preference tests showing comparable quality to original outputs. Moreover, under similar acceleration ratios, ScalingCache outperforms prior state-of-the-art caching strategies, achieving a 45$\\%$ reduction in LPIPS for text-to-image generation and 20$-$30$\\%$ reduction for text-to-video generation, highlighting its superior fidelity preservation.", "tldr": "This paper introduces ​​a method that drastically speeds up Diffusion Transformer (DiT) inference by ​​reusing the feature cache​​ from previous denoising steps.", "keywords": ["​​Diffusion Transformer", "Image generation", "Video generation", "Model Acceleration", "Feature Cache"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4ceb786f5ea86d02304ad48da5f28b8552a40fb.pdf", "supplementary_material": "/attachment/aa7b0d788b396755d8c3dc15a1bd1bac5ffc3d7d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ScalingCache, a training-free acceleration framework for Diffusion Transformers (DiTs), focused on visual generative models for both image and video generation. The approach introduces an adaptive dynamic caching mechanism, leveraging block-wise differential scaling coefficients (precomputed offline) and runtime error-adaptive cache interval selection to reduce redundant computation during denoising inference. Experimental results on multiple state-of-the-art image and video DiTs show substantial speedup (up to 3.1×) with minimal loss in visual quality, outperforming previously established caching baselines in both fidelity and efficiency on several standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a pressing bottleneck in generative modeling—accelerating the slow inference of DiTs—through an approach requiring no retraining.\n2. The method section provides a systematic derivation of the prediction formula (Eq. for $\\hat{\\boldsymbol{y}}{t}^{l}$, Page 4), the estimation of scaling coefficients (Eq. for $\\alpha{t}^{l}$), and the adaptive error-based update rule. Full derivations and algorithmic steps are offered in the main text and appendices.\n3. Results across major models (Wan2.1, HunyuanVideo, FLUX) and diverse settings show consistently strong speedups and lower LPIPS/SSIM drops versus all baselines."}, "weaknesses": {"value": "1. Several directly related and competitive caching acceleration methods—TokenCache (Lou et al., 2024) [1], Gradient-Optimized Cache (Qiu et al., 2025) [2], FastCache (Liu et al., 2025) [3], SpeCa (Liu et al., 2024) [4], DiTFastAttn (Kim et al., 2024) [5], and Dynamic Diffusion Transformer (Wang et al., 2024) [6]—are not cited, discussed, or included as baselines. While the paper does compare to prominent recent caching strategies, this omission leaves a gap in situating ScalingCache's novelty and advancement versus the current best practices.\n2. The derivation of the dynamic error threshold $\\delta_s$ in Algorithm 1 (Page 6) could benefit from further theoretical and empirical justification—currently, it is set based on an empirical mean of prior errors, and may be sensitive to outliers or sample diversity. The implications for worst-case quality loss (e.g., video flicker) as a function of $\\delta_s$ are left unexplored.\n3. While near-lossless acceleration is highlighted, the methodology is not extensively challenged at higher speedup factors, nor is there a rigorous exploration of when the trade-off between speed and quality breaks down (e.g., in especially long sequences, rare prompts, or highly dynamic scenes).\n4. In some equations, notation such as $\\Delta \\boldsymbol{y}_{\\tau}^{l}$ is used before being defined, and formatting is at times inconsistent (e.g., parameter lists in Algorithm 1). In Section 3.2 (Page 4), certain variables and indices are introduced abruptly, which may cause confusion for readers less familiar with blockwise DiT architectures.\n\n[1] Lou J, Luo W, Liu Y, et al. Token caching for diffusion transformer acceleration[J]. arXiv preprint arXiv:2409.18523, 2024.\n\n[2] Qiu J, Liu L, Wang S, et al. Accelerating diffusion transformer via gradient-optimized cache[J]. arXiv preprint arXiv:2503.05156, 2025.\n\n[3] Liu D, Yu Y, Zhang J, et al. Fastcache: Fast caching for diffusion transformer through learnable linear approximation[J]. arXiv preprint arXiv:2505.20353, 2025.\n\n[4] Liu J, Zou C, Lyu Y, et al. Speca: Accelerating diffusion transformers with speculative feature caching[J]. arXiv preprint arXiv:2509.11628, 2025.\n\n[5] Yuan Z, Zhang H, Pu L, et al. Ditfastattn: Attention compression for diffusion transformer models[J]. Advances in Neural Information Processing Systems, 2024, 37: 1196-1219.\n\n[6] Zhao W, Han Y, Tang J, et al. Dynamic diffusion transformer[J]. arXiv preprint arXiv:2410.03456, 2024."}, "questions": {"value": "1. Can the authors provide a more systematic analysis of scaling/failure scenarios? For instance, under what prompt or video conditions does the method’s error substantially increase, and what diagnostic measures could be advised in practice?\n2. How sensitive is the method to the quality and diversity of prompts used for offline estimation of $\\alpha$ coefficients? Figure 6 shows convergence, but quantitative analysis across tasks would clarify real-world robustness.\n3. Given the omission of several related works (see above), how does ScalingCache’s performance and computational overhead compare to FastCache, TokenCache, and GOC, both qualitatively and in speedup/fidelity metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7MCLxMzTCK", "forum": "uXmbrTlko7", "replyto": "uXmbrTlko7", "signatures": ["ICLR.cc/2026/Conference/Submission5793/Reviewer_BDzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5793/Reviewer_BDzQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294758750, "cdate": 1761294758750, "tmdate": 1762918266276, "mdate": 1762918266276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ScalingCache, a training-free acceleration framework tailored for DiTs. By synergizing differential-scaling-based prediction with runtime-adaptive caching intervals, ScalingCache delivers significant speed-ups on both image and video generation while retaining near-lossless quality. Extensive experiments on Wan2.1, HunyuanVideo and FLUX show 2.3–3.1× acceleration with only 0.3–0.5 % VBench drop, and outperform prior state-of-the-art caching methods in LPIPS and other fidelity metrics, demonstrating superior fidelity-efficiency trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed algorithm is clearly described, with a well-defined formulation and solid explanation.\n2.\tThe manuscript is clearly structured and well-articulated, making it easy for readers to follow."}, "weaknesses": {"value": "1.\tThe related work section overlooks the discussion of cache acceleration methods for UNet-based models, even though the cache acceleration technique for DiT-based models is an extension of and inspired by the earlier approaches developed for UNet-based models.\n2.\tIt would be great if the proposed method could further improve the sampling speed of the distilled models.\n3.\tIt's better to provide a user study to verify, through human evaluation, whether the generative performance of the method is close to the baseline.\n4.\tIs using 50 prompts sufficient to determine the appropriate scale?"}, "questions": {"value": "The authors are encouraged to further explore the applicability of the proposed approach to few-step distilled models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "75N1jpTQqs", "forum": "uXmbrTlko7", "replyto": "uXmbrTlko7", "signatures": ["ICLR.cc/2026/Conference/Submission5793/Reviewer_6bkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5793/Reviewer_6bkW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664623307, "cdate": 1761664623307, "tmdate": 1762918265318, "mdate": 1762918265318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ScalingCache, a training-free inference acceleration framework specifically designed for Diffusion Transformers (DiTs), targeting image and video generation tasks. The core idea leverages the temporal redundancy in the hidden states during the denoising process in DiT. It conducts lightweight offline analysis on a small number of samples to precompute differential scaling factors and dynamically reuses previously computed activations during inference to bypass certain computation steps. This method achieves significant speedup while maintaining near-lossless generation quality, outperforming existing caching strategies. It particularly demonstrates better robustness in complex video generation scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Important and Practical Problem: The high computational cost of DiT significantly limits its deployment in real-world scenarios like video generation. The proposed training-free acceleration approach holds clear practical value.\n\n2. Sophisticated Technical Design:\nThe introduction of the differential scaling factor α effectively combines zero-order and first-order predictions, addressing the issue of large prediction errors in certain layers seen with methods like Taylorseer.\n\n3. Comprehensive Experiments with Outstanding Results:\nCovers multiple SOTA models (Wan2.1-1.3B/14B, HunyuanVideo, FLUX);\nEvaluates both image and video tasks with metrics including PSNR/SSIM/LPIPS/VBench/CLIP Score and human preference.\n\n4. Engineering-Friendly: Only requires tuning one hyperparameter (Sf, i.e., number of initial full computation steps), without the need for training, fine-tuning, or complex scheduling logic, making it easy to integrate."}, "weaknesses": {"value": "1. Generalization of α: The α coefficients need to be estimated offline using a small number of prompts (~50 prompts). While the paper claims convergence and low variance (Figure 6), it doesn’t verify its generalization to out-of-distribution prompts (e.g., extreme styles or rare objects). If α is sensitive to prompts, frequent re-estimation may be necessary for deployment.\n\n2. (Section 3.3) The authors acknowledge that their strategy may fail for \"static-to-dynamic\" videos (e.g., a scene suddenly transitioning from stillness to high-speed movement). Such scenarios are not uncommon in real-world videos. Furthermore, as the denoising process can theoretically access tokens from all frames at every step, this raises the question of why such scenarios would significantly impact this strategy and whether a reasonable threshold can still be estimated.\n\n3. While the authors claim \"no additional inference overhead,\" the calculation of dynamic errors (Equation 7) and all-reduce operations (Appendix F) under sequence parallelism still involve communication and computational overhead.\nThe authors fail to report the storage cost of caching (storing y and Δy per module) and do not discuss the potential impact on devices with limited memory.\n\n4. Lack of Comparisons: Why didn’t the authors compare their method with other acceleration approaches, such as Sparse VideoGen?"}, "questions": {"value": "1. In Table 1, why does MixCache achieve the highest score on the 14B WAN2.1 model? Could the authors explain this anomaly?\n\n2. Concerns remain regarding the generalization of α. How does α perform in out-of-distribution prompts? If the selected prompts are not diverse enough, could this lead to suboptimal results?\n\n3. Regarding the human evaluation experiments, were the participants professionals or anonymous general users? Could this introduce bias?\n\n4. In the real-world deployment of 14B models, how much does the cache increase memory consumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z1nedzdQ0S", "forum": "uXmbrTlko7", "replyto": "uXmbrTlko7", "signatures": ["ICLR.cc/2026/Conference/Submission5793/Reviewer_PyFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5793/Reviewer_PyFd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812523909, "cdate": 1761812523909, "tmdate": 1762918264843, "mdate": 1762918264843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ScalingCache, a training-free method to accelerate Diffusion Transformers (DiTs). It improves upon standard feature caching by introducing a pre-computed scaling factor (alpha) for more accurate feature prediction and a dynamic caching strategy to adaptively skip computation steps. The method achieves significant speedups (2.5-3.1x) with minimal quality loss on major text-to-video and text-to-image models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Effective: The differential scaling with alpha is a lightweight approach to improve feature prediction accuracy.\n2. Strong Results: The method delivers impressive speedups while preserving high visual fidelity, outperforming prior methods in key metrics.\n3. Practical: As a training-free solution, it is easy to apply to existing models without expensive retraining."}, "weaknesses": {"value": "1. Robustness of Alpha: Is calculating the alpha coefficient from only 50 prompts sufficient for generalization across diverse inputs? The paper should discuss the method's robustness and show potential failure cases.\n2. Analysis of Dynamic Caching: The ablation study confirms the dynamic caching strategy is useful, but lacks a deeper analysis. How does it adaptively change intervals for different content (e.g., static vs. dynamic scenes)?\n3. VBench Score Breakdown: The analysis of VBench scores is too general. A breakdown by dimension (e.g., image quality, temporal consistency) is needed to clarify where the method truly excels. An explanation for why it doesn't achieve top scores on all models would also be helpful."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BdUTY5ppDn", "forum": "uXmbrTlko7", "replyto": "uXmbrTlko7", "signatures": ["ICLR.cc/2026/Conference/Submission5793/Reviewer_mevD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5793/Reviewer_mevD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921128730, "cdate": 1761921128730, "tmdate": 1762918264406, "mdate": 1762918264406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}