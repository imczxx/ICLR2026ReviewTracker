{"id": "GNo1qMqgPD", "number": 15275, "cdate": 1758249692608, "mdate": 1763708745173, "content": {"title": "VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models", "abstract": "As Speech Language Models (SLMs) transition from personal devices to shared, multi-user environments such as smart homes, a new challenge emerges: the model is expected to distinguish between users to manage information flow appropriately. Without this capability, an SLM could reveal one user’s confidential schedule to another—a privacy failure we term **interactional privacy**. Thus, the ability to generate speaker-aware responses becomes essential for SLM safe deployment. Current SLM benchmarks test dialogue ability but overlook speaker identity. Multi-speaker benchmarks check who said what without assessing whether SLMs adapt their responses. Privacy benchmarks focus on globally sensitive data (e.g., bank passwords) while neglecting contextually sensitive information (e.g., a user’s private appointment). To address this gap, we introduce **VoxPrivacy**, the first benchmark designed to evaluate interactional privacy in SLMs. VoxPrivacy spans three tiers of increasing difficulty, from following direct secrecy commands to proactively protecting privacy. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals a widespread vulnerability: most open-source models perform close to random chance (around 50\\% accuracy) on conditional privacy decisions, while even strong closed-source systems still fall short on proactive privacy inference. We further validate these findings on Real-VoxPrivacy, a human-recorded subset, confirming that the failures observed on synthetic data persist in real speech. We also demonstrate a viable path forward: by fine-tuning on a new 4,000-hour training set, we improve the model’s privacy-preserving capabilities while achieving fair robustness. To support future work, we are releasing the VoxPrivacy benchmark, the large-scale training set, and the fine-tuned model to help the development of safer and more context-aware SLMs.", "tldr": "", "keywords": ["Benchmark", "Speech Language Model", "Interactional Privacy"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2edfd62dbc85c0a9fd70b0bfda4a8b9e26fc95a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "VoxPrivacy is the first benchmark that evaluates interactional privacy for Speech Language Models (SLMs) in multi-user, spoken settings. It tests whether a model can keep user-specific secrets across three escalating tiers: (1) obeying explicit non-disclosure commands, (2) using the speaker’s voice as a key to disclose only to the original owner, and (3) proactively protecting privacy with no instruction by inferring sensitivity from content and context. Built from 7,107 utterances (32.86 hours, English/Chinese balanced) of high-quality synthetic audio with diverse speakers, VoxPrivacy pairs objective LLM-as-judge scoring with human validation. Across nine SLMs, most open-source systems hover around chance on conditional privacy decisions, revealing a core weakness in speaker-aware reasoning, not basic conversation. The authors also show a practical path forward: fine-tuning on a 4,000-hour training set substantially improves privacy compliance while preserving general abilities, though proactive, common-sense privacy remains challenging and vulnerable to spoofing attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel and interesting research questions\n- Detailed and complete experiment\n- Well-written paper"}, "weaknesses": {"value": "- Lack of practical motivations\n- Data authenticity and generalization issues"}, "questions": {"value": "This paper presents VoxPrivacy, the first benchmark designed to evaluate interactional privacy for speech language models (SLMs) in realistic multi-user spoken scenarios. The authors construct a bilingual (English/Chinese) dataset containing 7,107 utterances (32.86 hours) of high-quality synthetic audio from diverse speakers and evaluate nine SLMs, including both open-source and proprietary systems. The benchmark employs a hybrid evaluation protocol combining LLM-as-judge scoring and human validation. Results show that open-source SLMs perform near random on speaker-conditioned privacy tasks, while closed-source models and fine-tuned versions achieve better compliance. Additional analyses reveal that the main bottleneck lies not in conversational ability but in speaker-aware reasoning and contextual privacy understanding. The paper is well-written in general. However, I do have the following concerns:\n\n- Lack of practical motivation. \nAlthough it sounds fancy to leverage models' internal ability to conduct the permission verification, it is still common sense that models' responses are unreliable and random. Therefore, it may be possible to construct the permission systems instead of relied solely on large models to solve the questions. The authors also show in the paper that some jailbreak method can break the model recognition ability. Therefore, I am wondering given the current model architecture and model design, is it worth doing such a test? Should all permission-related issues be handed over to a dedicated permission system?\n\n- Data authenticity and generalization issues\nAll speech data in VoxPrivacy are synthetic. WHile this choice ensures ethical safety, it limits the acoudstic and semantic diversity presnet in real spoken interactions, such as bacground noise, accent variation, and spontaneous interruptions. As a result, the benchmark may overestimate the model performance on the more realisitic data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CClqEb5AnF", "forum": "GNo1qMqgPD", "replyto": "GNo1qMqgPD", "signatures": ["ICLR.cc/2026/Conference/Submission15275/Reviewer_G4Sy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15275/Reviewer_G4Sy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639129071, "cdate": 1761639129071, "tmdate": 1762925578147, "mdate": 1762925578147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Privacy studies have, up to now, mainly focused on individual users, whereas many problems of privacy occur only within the context of an interaction. This paper proposes a new dataset for evaluating interactional privacy when interacting with speech agents. This is an important contribution and a problem that has been overlooked in past works. \nThe dataset is synthetic speech audio. This is a reasonable starting point, though obviously, actual recordings of human speech would be better. Recording sufficient amounts of such data, however, requires significant resources. Synthetically created speech is thus the obvious and reasonable simplification. \nOverall, I like the paper rather a lot and have only minor comments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- High novelty; The addressed research problem is novel, and this is, as far as I know, the first dataset and methodology for evaluating interactional privacy.\n- High quality; The proposed dataset is designed following principles of good design, the validation tests for the dataset are good, and the analysis of results is insightful. \n- Good clarity; Writing and argumentation are clear, with only minor blemishes.\n- High significance: As this work addresses an important problem that has not been studied before, I believe that this can have a significant impact."}, "weaknesses": {"value": "Main weaknesses:\n- Argumentation: Building a dataset for SLMs was motivated by the fact that spoken dialogues have plenty of contextual information that is not available in the text only. This is true; speech is a much more informative representation than text, and my informed guess is that much of the information related to interactional privacy is available only in the voice (not in text). That said, as data is here created through synthesis from text, there is no way to confirm that information related to interactional privacy (beyond text) is included in the dataset. The question is thus whether the audio representation has any added value in comparison to text only, as long as the data is synthetic? I acknowledge that this is a difficult question that probably cannot be solved in a single paper and probably not even in a single doctoral thesis, but I would request a discussion about this issue in the paper.\n- Data representativeness: A variation of the above argument is that synthetic data is always a proxy for real data, and special care must be taken to ensure or verify that it represents the true population adequately. This could be solved, for example, by adding audio from real human speakers to the test set. If the performance with synthetic and authentic samples are similar, then the synthetic data is sufficient. Again, this is not a demand but a proposal, given that it this modification can require significant resources, time and effort.\n\nMinor comments:\n- Fig 2, stage 3; spelling of \"Instruction\"\n- Fig 2 is very dense, packed with information. I don't have a better solution to propose, but I just want to acknowledge that understanding the figure requires some effort."}, "questions": {"value": "My suggestions and questions were included in the weaknesses box."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OxYxoUEK7T", "forum": "GNo1qMqgPD", "replyto": "GNo1qMqgPD", "signatures": ["ICLR.cc/2026/Conference/Submission15275/Reviewer_KeNS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15275/Reviewer_KeNS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915392168, "cdate": 1761915392168, "tmdate": 1762925577505, "mdate": 1762925577505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VoxPrivacy, a benchmark for evaluating a privacy failure mode (termed \"interactional privacy\") in multi-user conversational context for speech language models. One user discloses secret information to the SLM, explicitly or implicitly, and the SLM needs to ensure this secret information is not disclosed when another user queries it. VoxPrivacy consists of three tiers of privacy evaluation, ranging from the most explicit (directly indicating secrecy) to the most implicit (inferring protection needs based on commonsense). Their evaluation found current SLMs, especially open-source ones, struggling with these tasks. Their fine-tuned model improved the privacy preservation capabilities without compromising the general capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper examines contextual privacy leakage issues in speech language models and engages with the unique capabilities of SLM to process the voice which can uniquely identify a person. Hence, it makes sense to evaluate the end-to-end privacy protection for SLM.\n- The paper develops a benchmark covering both direct and indirect indicators of privacy information to perform a thorough evaluation of the privacy protection capabilities of closed-source and open-source models.\n- The evaluation reveals substantial gaps in the open-source models' privacy preservation capabilities.\n- The fine-tuned model showed promising results improving the privacy capabilities while preserving the general capabilities."}, "weaknesses": {"value": "- I can't find a realistic grounding for the privacy violations in the benchmark. The benchmark assembles the specification, instruction, and probing queries into a multi-turn dialogue, which corresponds to the situation when multiple users converse with the SLM in the same session. In these cases, people already have equal access to the output of the model, which means the sensitivity of information in the output should be determined by everyone present in the conversation, rather than just the speaker (or even people who are co-present in the context with access to the model output, regardless of whether they participate in the conversation). Also, in this case it's not natural for one speaker to describe their secret in front of another. This scenario hence feels contrived.\n- In the Tier 3 example, I don't understand why \"I'm worried about my medical results\" implies that \"medical results\" should be considered a secret. I feel the bar might be set too rigidly and doesn't align with common sense.\n- Gaps are exaggerated in the abstract \"most models perform near random chance, about 50% accuracy on binary privacy decisions\" — In fact, the closed models performed well in many of the tier 1 and 2 tasks, sometimes even better than the fine-tuned model.\n- I feel it's inappropriate to call the LLM-as-a-Judge evaluation as objective evaluation and the human evaluation as subjective, because they are following the same criteria."}, "questions": {"value": "- Can you discuss the validity of your threat model, and why the benchmark design appropriately reflects it?\n- Can you explain how you determine the test cases in Tier 3, specifically what procedure did you follow to ensure they properly reflect social norms and commonsense?\n- Are there any considerations about the potentially different cultural norms between English vs. Chinese speaking contexts? \n- How were the human annotations used to validate the LLM-as-a-Judge labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C0Yq2egy7D", "forum": "GNo1qMqgPD", "replyto": "GNo1qMqgPD", "signatures": ["ICLR.cc/2026/Conference/Submission15275/Reviewer_muR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15275/Reviewer_muR9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971923502, "cdate": 1761971923502, "tmdate": 1762925577023, "mdate": 1762925577023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VoxPrivacy, the first benchmark for assessment of interactional privacy in SLMs operating in shared, multi-user environments. The authors propose an evaluation framework that tests (1) direct command secrecy, (2) speaker-verified access, and (3) proactive privacy — reflecting practical privacy expectations in systems like smart home assistants. The benchmark consists of 7107 synthetic utterances (32.86 hours, English and Chinese). It covers a variety of privacy-sensitive categories constructed with a multi-stage pipeline and verified for quality and linguistic diversity. 9  SLMs are evaluated, showing that most open-source models perform no better than random chance on speaker-aware privacy tasks. Closed-source and fine-tuned models perform better but still show significant vulnerabilities. Analysis highlights crucial challenges in contextual integration and adversarial robustness. The authors release all resources, including the benchmark, a 4000-hour mixed-task training set, and a baseline model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a novel and important problem in SLMs, interactional privacy in multi-user environments, which is underexplored. The introduction of the VoxPrivacy benchmark, based on a theoretically grounded definition of interactional privacy using Nissenbaum's Contextual Integrity.\n\n2. The quality of the paper is good, the authors constructed a large-scale bilingual dataset with data synthesis, filtering, multi-model LLM generation, and human verification processes. The benchmark includes well-designed three-tiered tasks isolating distinct privacy capabilities, with comprehensive evaluations of nine state-of-the-art models, including open- and closed-source systems.\n\n3. The paper is very well written.\n \n4. The experimental results, based on a carefully constructed benchmark and clear evaluation methods, support the claim that current SLMs (especially open-source) struggle to reliably enforce speaker-based privacy.\n\n5. The benchmark addresses critical safety and privacy challenges faced by SLMs in realistic shared environments such as smart homes. This benchmark will foster further research and development of practical solutions for privacy-preserving SLMs."}, "weaknesses": {"value": "1. Synthetic dataset limitations (also acknowledged by the authors). The use of only synthetic, LLM-generated dialogues for privacy-sensitive utterances may reduce real-world relevance. The paper lacks user studies or comparisons with real data to confirm if synthetic secrets match actual privacy concerns.\n\n2. Artificial dialogue structure.The fixed 3-turn dialogue pattern (secret statement → privacy instruction → probe) may not fully capture the richness and variability of natural conversations, including interruptions, multi-party interplay, and temporal gaps.\nSpeaker verification analysis and metric. Some details of speaker verification analysis are missing, also more conventional automatic speaker verification metric, i.e. equal error rate,  would be more appropriate.\n\n3. Limited fine-tuning analysis. The construction of the 4000-hour mixed-task dataset for fine-tuning, including mixtures of tasks and proportions, is not fully justified. Ablation studies exploring the impact of different auxiliary tasks and the balance between privacy enhancement and general capability preservation are missing.\n\n4. Cross-lingual performance gaps. The underperformance on Chinese w.r.t. English is observed but is not sufficiently analyzed, leaving open questions about multilingual robustness."}, "questions": {"value": "1. Have the authors conducted user studies to validate that humans perceive the synthetic secrets as privacy-sensitive and expect SLMs to protect them?\n\n2. Can the authors provide error analyses by secret categories and instruction phrasing to clarify which types of secrets leak most frequently?\n\n3. Can the authors elaborate on the acoustic features or embeddings models use for speaker verification and their limitations, particularly regarding spoofing attacks?\n\n4. Can the authors discuss potential extensions of the benchmark to more realistic, interactive multi-turn dialogues?\nRegarding the multilingual aspect, do the authors have hypotheses or insights on why models underperform for Chinese?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzB03meUbI", "forum": "GNo1qMqgPD", "replyto": "GNo1qMqgPD", "signatures": ["ICLR.cc/2026/Conference/Submission15275/Reviewer_m294"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15275/Reviewer_m294"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762758552972, "cdate": 1762758552972, "tmdate": 1762925576745, "mdate": 1762925576745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive feedback and high-level engagement with our work. We are encouraged that there is broad agreement that **interactional privacy for SLMs in shared, multi-user environments is both novel and important, and that VoxPrivacy fills a clear gap not addressed by existing speech, multi-speaker, or safety benchmarks.** Our goal in the rebuttal and revision was to (i) sharpen the practical motivation and threat model, (ii) strengthen the realism and validity of the benchmark with new human studies and real speech, and (iii) deepen the diagnostic analysis of where and why current SLMs fail.\n\n### **Core contributions and what is new in the revision**\n\nTo the best of our knowledge, **VoxPrivacy is the first benchmark that directly evaluates interactional privacy in multi-speaker SLM dialogues**, with a **three-tier task structure** that spans explicit secrecy commands, speaker-verified access, and proactive privacy inference, plus a 4000 hour training set and a fine-tuned SLM baseline. In the revision, we substantially reinforce this contribution along three main axes:\n1. **Realism of the threat model and motivation.**\n    \n    We clarify that VoxPrivacy targets **asynchronous, time-separated interactions on shared devices with memory**, not just concurrent multi-party chat. The three turns in our benchmark compress a realistic sequence: (T₀) user A discloses a secret, (T₁) sets an explicit or implicit privacy preference, (T₂) later user B queries the same assistant. We argue more clearly why **external permission systems (account logins, voice match, device locks) are insufficient and inadequate** in this setting: they either require impractical pre-enrollment of every possible speaker, or they enforce hard isolation between user histories, which breaks collaborative use cases (e.g., “What did Alex say about the meeting time?”). This strengthens the case that **speaker-aware, context-sensitive privacy reasoning inside the SLM is necessary, not a “nice-to-have”**.\n2. **Validation of synthetic data via human studies and real recordings.**\n    \n    Multiple reviewers questioned whether synthetic, LLM-generated secrets and TTS audio truly reflect real-world privacy concerns and acoustic conditions. We therefore added two new validation components:\n    \n    - A **human perception study** where 5 bilingual annotators rated 200 randomly sampled “secrets” on a 5-point Likert scale. **92% of secrets received an average sensitivity rating ≥4**, showing that humans indeed treat these scenarios (including Tier-3 examples like “I’m worried about my medical results”) as information they expect an assistant not to reveal to others.\n    - A **Real-VoxPrivacy evaluation** in which **18 bilingual volunteers recorded 586 utterances** (balanced across EN/ZH). The new Table 4 shows that **model rankings, the Tier2 to Tier3 difficulty gap, and the near-random behavior of most open-source SLMs all persist on real human speech**. In some cases, performance on real audio is even worse due to accents and noise, confirming that our synthetic benchmark is a **clean “best-case” upper bound rather than an overestimate of real-world difficulty**."}}, "id": "OJXjzjETYc", "forum": "GNo1qMqgPD", "replyto": "GNo1qMqgPD", "signatures": ["ICLR.cc/2026/Conference/Submission15275/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15275/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission15275/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763717048690, "cdate": 1763717048690, "tmdate": 1763717048690, "mdate": 1763717048690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}