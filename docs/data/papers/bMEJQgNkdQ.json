{"id": "bMEJQgNkdQ", "number": 21535, "cdate": 1758318625458, "mdate": 1759896917144, "content": {"title": "Select the Right Agent: Data-Driven Online Model Selection in Reinforcement Learning", "abstract": "We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating data-driven model selection methods into reinforcement learning training procedures.  We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Stabilized training, 3) Adaptation under non-stationary dynamics. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self-model selection.", "tldr": "", "keywords": ["Reinforcement Learning", "Model Selection"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2587e3991197b2ef953829cf630bba4ea5ef631b.pdf", "supplementary_material": "/attachment/1a3d88dfb5f68599f9ba0a5012d75d39dc1a1abc.zip"}, "replies": [{"content": {"summary": {"value": "The idea is to use a selector meta algorithm that adaptively choose from a set of base RL agents during a training run."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* it shows strong empirical valiadtion in section 4 \n*  The paper is well-written and easy to follow. \n* The paper does an excellent job of connecting theory to practice. Theorem 4 provides a clean, intuitive theoretical justification for why the the selector works: it adaptively allocates more \"pulls\" (training episodes) to agents that demonstrate a better (lower) regret coefficient."}, "weaknesses": {"value": "* the authros didn't provide the evidence that the model is scalable\n*  The paper relies heavily on D³RB but didn't express clearly why not ED²RB"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WLFvuc4cbT", "forum": "bMEJQgNkdQ", "replyto": "bMEJQgNkdQ", "signatures": ["ICLR.cc/2026/Conference/Submission21535/Reviewer_VTaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21535/Reviewer_VTaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442603174, "cdate": 1761442603174, "tmdate": 1762941823955, "mdate": 1762941823955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies online model selection in RL. It integrates a data-driven model selction method D3RB into standard RL loops. The paper shows the efficiency compute allocation, adaptation to non-stationary envs, and improved stability through a mix of theoretical restatements and small-scale experiments on Atari and MuJoCo tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear motivation, and the targeted problem, RL algos often require many hyperparameters and are sensitive to misspecification,  is critical."}, "weaknesses": {"value": "1. Lack of novelty. The primary algorithmic component (D3RB) and its theoretical guarantees are directly borrowed from previous works. \n2. Even the central theorems are re-statements of known results. There is no new analysis specific to RL, such as, How model selection interacts with stochastic policy gradients? Whether the selector biases learning or affects convergence? Theoretical treatment of non-stationary dynamics or exploration coupling?\n3. Related work discussion omits recent AutoRL and hyperparameter optimization literature."}, "questions": {"value": "1. In the main paper, the theorems are derived under the episodic RL setting without discount factor $\\gamma$, but in Appendix A.4, it uses discount factor, here exists mathematically inconsistency.\n2. Experiments use only three random seeds which is unacceptable for RL where variance is large.\n3. No numerical variance, confidence intervals, or significance tests are provided. The error bars on Base Agent 2 are nearly invisible.\n4. The paper claims “better resource allocation” but provides no runtime, compute usage, or sample efficiency data.\n5. The authors repeatedly claim to “improve efficiency” and “stabilize training,” but the evidence only shows that D3RB eventually matches the best base agent. There is no demonstration of faster convergence or less compute spent for the same return.\n6. Can D3RB outperform all base agents by better combining partial learning progress, or does it only track the best one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IzjBxu1bEG", "forum": "bMEJQgNkdQ", "replyto": "bMEJQgNkdQ", "signatures": ["ICLR.cc/2026/Conference/Submission21535/Reviewer_MRCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21535/Reviewer_MRCK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817242831, "cdate": 1761817242831, "tmdate": 1762941823738, "mdate": 1762941823738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies online model selection for RL by embedding D$^3$RB (or any model selection algorithm following similar interface to D$^3$RB) directly into the RL training loop.\nConcretely, the proposed method adopts D$^3$RB’s balancing potential $\\phi\\_t^i=\\hat{d}\\_t^i\\sqrt{n\\_t^i}$, regret and regret coefficient, and the misspecification test, and then provides a minimal Selector+RL integration that replaces the (contextual) bandit value definitions with episodic RL returns.\nTheoretical guarantees are inherited from D3RB (i.e., the selector’s high-probability regret bound is adopted rather than strengthened).\nBeyond this transfer, this paper analyzes how compute is allocated across agents within RL and proves a relationship between the allocated compute and regret coefficient of each base learner (i.e., $\\alpha_i\\propto 1/d\\_i^2$) clarifying why better-performing agents get more resource.\nOverall, relative to D³RB, the main contribution is an RL-oriented integration and interpretation of the selector’s behavior in episodic settings.\n\nEmpirically, the paper evaluates three tasks: (1) neural architecture selection task in DQN agents for four Atari games; (2) step-size selection task in RL (especially, PPO agents in three different MuJoCo environments); and (3) self-model selection across different random seeds. \nThese case studies support the claims that data-driven selection adapts under non-stationarity and stabilizes training without requiring agent-specific structure or regret bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes applying D$^3$RB as a selector within the RL training loop and clearly describes a minimal integration recipe that reuses D$^3$RB’s interface (balancing potential, misspecification test, etc.), which makes the approach easy to drop into existing pipelines.\n- Provides empirical comparisons across multiple selector baselines (e.g., bandit-style selectors), illustrating adaptation under non-stationarity and improved training stability.\n- Does not require agent-specific structure or per-agent regret bounds, which increases applicability across RL algorithms.\n- Offers an interpretive analysis of compute allocation and proves a proportionality result $\\alpha_i\\propto 1/d\\_i^2$, giving a useful explanation for why stronger agents receive more updates."}, "weaknesses": {"value": "- Theoretical novelty beyond D$^3$RB is minimal. Core guarantees are inherited without non-stationary (e.g., switching-regret) results.\n- Evaluation scope is narrow (few Atari/MuJoCo tasks, limited seeds), making generalization claims tentative.\n- Ablations (e.g., sensitivity to $d\\_{\\min}$​, selector variants like ED$^3$RB) and baselines (e.g., meta learning algorithms for adaptive hyperparameter selection in RL, mentioned in line 65) are thin, limiting insight into when the approach helps or fails."}, "questions": {"value": "- In Fig. 3, why does the Selector (D³RB) performance curve differ from any mixture/combination of the base agents’ learning curves, even though the selector is claimed not to affect each base algorithm’s training curve?\n- Why do Fig. 3 and 4 use different environments, even though it seems feasible to include the same baselines in Fig. 3 and to report the base agents’ standalone performance in Fig. 4? Could you harmonize the setups so that (1) Fig. 3 includes the baselines shown in Fig. 4 and (2) Fig. 4 also plots the standalone base-agent curves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PpoRArEPmM", "forum": "bMEJQgNkdQ", "replyto": "bMEJQgNkdQ", "signatures": ["ICLR.cc/2026/Conference/Submission21535/Reviewer_BbGC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21535/Reviewer_BbGC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018392594, "cdate": 1762018392594, "tmdate": 1762941823481, "mdate": 1762941823481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies online model-selection to deep reinforcement learning by using D³RB (Dann et al. 2024) to adaptively select between different RL agents (with different architectures, hyperparameters, or seeds) to update after each episode. The goal is to allocate resources to promising agents, improving stability and avoiding committing to suboptimal configurations. The method is evaluated on Atari (DQN) and MuJoCo (PPO) and generally matches or improves the performance of the best agent in hindsight."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Achieving stability across seeds and hyperparameters remains a real issue in deep RL practice. Wrapping a model-selection algorithm around existing RL agents is appealing and broadly applicable. The D^3RB is a provably-sound approach that is shown to improve training robustness across a range of settings (architecture choice, learning rate, seed selection) in deep RL contexts where algorithm selection is typically heuristic."}, "weaknesses": {"value": "The theoretical novelty is limited/unclear. Section 3 restates prior results from Dann et al. (2024), and while Section 4 adds some analysis, the contribution beyond prior theory is not fully clear. For instance, Theorem 5 appears as a simple corollary, and Theorem 4 appears more as a sanity-check. This makes it difficult to appreciate the paper since the positioning is primarily theoretical. \n\nOn the experimental side, the results are promising and I feel that, if anything, the paper should be positioned as more of an experimental paper rather than a theoretical paper as it currently stands. However, if it was actually an experimental paper, one would expect the experimental scope to be broader. While Atari and MuJoCo are good starting points, the real impact of model selection should be settings where we could not simply afford to just run the M agents in parallel. How would these algorithms fare e.g. in LLM post-training settings, where compute is comparatively more valuable. Regarding the baselines, the authors mainly compare to theoretical model-selection algorithms and do not consider more empirical baselines for hyperparameter-tuning (e.g. hyperband/meta-learning algorithms), which would significantly strengthen the case.  Lastly, the compute overhead of the model-selection algorithm (while it appears at a distance to be somewhat minimal) should be tracked.\n\nOverall, it appears the work sits between theory and practice, but the contributions on the theoretical side are somewhat minimal. If framed primarily as a practical contribution, it needs deeper empirical analysis; if framed as theory, the novelty is less clear. Still, the contribution is a promising an empirical demonstration that D³RB can be usefully integrated into deep RL pipelines."}, "questions": {"value": "1.\tCan you clarify the precise theoretical novelty beyond Dann et al. (2024)?\n\t2.\tIs there a reason not compare against more empirical tuning strategies such as meta-learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FKDvye7gi8", "forum": "bMEJQgNkdQ", "replyto": "bMEJQgNkdQ", "signatures": ["ICLR.cc/2026/Conference/Submission21535/Reviewer_9SkD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21535/Reviewer_9SkD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762370283125, "cdate": 1762370283125, "tmdate": 1762941823270, "mdate": 1762941823270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}