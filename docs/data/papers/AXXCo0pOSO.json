{"id": "AXXCo0pOSO", "number": 8745, "cdate": 1758096807925, "mdate": 1759897766084, "content": {"title": "Scalable Supervising Software Agents with Patch Reasoner", "abstract": "While large language model agents have advanced software engineering tasks, the unscalable nature of existing test-based supervision is limiting the potential improvement of data scaling. The reason is twofold: (1) building and running test sandbox is rather heavy and fragile, and (2) data with high-coverage tests is naturally rare and threatened by test hacking via edge cases. In this paper, we propose R4P, a patch verifier model to provide scalable rewards for training and testing SWE agents via reasoning. We consider that patch verification is fundamentally a reasoning task, mirroring how human repository maintainers review patches without writing and running new reproduction tests. To obtain sufficient reference and reduce the risk of reward hacking, R4P uses a group-wise objective for RL training, enabling it to verify multiple patches against each other's modification and gain a dense reward for stable training. R4P achieves 72.2\\% Acc. for verifying patches from SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we design and train a lite scaffold, Mini-SE, with pure reinforcement learning where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2\\% Pass@1 on SWE-bench-verified, showing a 10.0\\% improvement over the original Qwen3-32B.\nThis can be further improved to 33.8\\% with R4P for test-time scaling. The stable scaling curves in both RL test rewards and test-time accuracy reflect R4P's practical utility for scalable supervision on software agents.", "tldr": "A reasoning-based patch verifier for test-free scalable supervision on software agents.", "keywords": ["LLM", "Software Engineering", "Agent"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/559f04da6800e2dec4bdac71d08cb1ce0d9a16af.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies an interesting problem of learning \"execution free patch verifiers\" for SWE agents. Currnent test time scaling agents fall into 3 categories: execution-free verifiers, execution-based verifiers and hybrid verifiers. Current execution-free verifiers require access to agent trajectory which can bias the final response of the patch verifier. The paper simply considers learning better \"exec ution free patch verifiers\" as a reasoning problem which can be solved with RL. The proposed approac R4P achieves 72% \"verification accuracy\" while also help improve TTS (test time scaling) performance by ~10% on SWE-Bench-Verified. (Its important to note that 72% is verification accuracy, not accuracy after TTS)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of using RL for training patch verifiers is interesting.\n\n* \"the binary outcome reward is very easy to hack, making the training unstable.\" This is a very insightful observation especially for training patch verification agents with RL.\n\n* The final results show improvements for patch verification and TTS performance."}, "weaknesses": {"value": "* The paper mentions that \"The Pass@1 resolution rate on SWE-bench-verified steadily improves with more training data and finally reaches 26.2%, outperforming Lingma Agent +\nLingma SWE-GPT-72B (Ma et al., 2024).\" Have the authors also tried their approach on closed source models like Claude 4.5 Sonnet and best open source models like R2E-Agent, SWE-Smith, DeepSWE, etc.?\n\n* The paper considers group verification for training the RL model (sec. 3). Have the authors tried training a patch verifier using RL for individual patches given the input problem statement and repository sandbox?\n\n* The authors mention that for training they use \"2,438 issue instances\". Have the authors explored impact of number of issues on the performance of the patch verifier? Is 2500 issues enough for RL training?\n\n\n* For fig. 2, I believe r2e-gym also has an execution based verifier. I will be curious on how the proposed verifier compares to execution-based verifiers?\n\n* Also for Tab. 1, are all values reported for execution-free verification given just the problem statement and final patch?\n\n* Using RL for training the patch verifier is interesting. Can the authors please also share some outputs from the patch verifier to help understand how RL shapes the reasoning process as compared to closed-source patch verifiers like o3, gpt-5 etc?\n\n* Finally, while not a major concern, have the authors tried comparing R4P with recent pretrained closed-source patch verifiers like Claude 4.5 Sonnet, gpt-5 etc?"}, "questions": {"value": "Please see the weaknesses section for some additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ltmpCNDxxw", "forum": "AXXCo0pOSO", "replyto": "AXXCo0pOSO", "signatures": ["ICLR.cc/2026/Conference/Submission8745/Reviewer_dq9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8745/Reviewer_dq9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940954177, "cdate": 1761940954177, "tmdate": 1762920536550, "mdate": 1762920536550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces R4P (Reasoning-for-Patch) — a scalable, test-free reward model for supervising software engineering (SWE) agents via reasoning rather than traditional test execution.\nIt addresses the scalability bottleneck of test-based verification, which is heavy, fragile, and limited by test coverage.\nR4P formulates patch verification as a group-wise reasoning task, comparing multiple candidate patches to produce dense, stable rewards during reinforcement learning (RL).\nExperiments show that R4P achieves 72.2% patch verification accuracy, surpassing proprietary models like OpenAI o3. The authors further train a lightweight agent, Mini-SE, purely under R4P supervision, which achieves 26.2% Pass@1 on SWE-bench-verified—+10% over Qwen3-32B—and 33.8% when combined with R4P at test time.\nThe paper argues that R4P enables scalable supervision for SWE agents without dependence on sandbox testing"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The group-wise reasoning objective transforms sparse binary verification into a dense, stable reward signal\n- The paper provides ample evidence showing the advantage of R4P, along with nice ablation studies to analyze the behavior of R4P model"}, "weaknesses": {"value": "- The reward model is fixed post-training, leading to potential reward drift as agents improve. It will be interesting to understand the RL behavior when you overtrain the model with such a static reward model model\n\n- In Fig. 9, it will be good to draw the confidence interval to see if the trend is significant. The bins to the right have too few samples, which makes the conclusion that \"verification accuracy positively correlates with/ number of edited lines\" a bit ungrounded\n\n- Despite the two challenges of applying R4P directly to existing agent scaffolds via RL, it'd be interesting to demonstrate R4P's ability to provide supervision for training models to work on general agent scaffolds. e.g., you can use R4P to re-rank patches/trajectories generated on training datasets like SWE-Gym+OpenHands and SFT on the top 10% trajectories and measure performance improvements on OpenHands vs. random sampling. This could demonstrate R4P's ability to generalize across scaffolds."}, "questions": {"value": "> As the agent’s policy improves, the static reward model may become misaligned with true answer quality \nI wonder whether the authors have tried to overtrain the policy (i.e., training the model longer in Figure 3a). I'd be interested in understanding the R4P approach's bottleneck, e.g., whether it will saturate at a fixed performance on SWE-Bench or degrade performance if overtrained.\n\n- I would be helpful if the authors could share more details about how the Acc/F1/EM was calculated, as well as the exact reward function that was used to perform RL on Mini-SE LM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tsN40Pys3N", "forum": "AXXCo0pOSO", "replyto": "AXXCo0pOSO", "signatures": ["ICLR.cc/2026/Conference/Submission8745/Reviewer_N1rv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8745/Reviewer_N1rv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953698160, "cdate": 1761953698160, "tmdate": 1762920536156, "mdate": 1762920536156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a reasoning-based framework for supervising large language model (LLM) agents in software engineering tasks without relying on computationally expensive or fragile testing environments.\nTo overcome these challenges, the authors introduce R4P, a reasoning reward model that performs group-wise patch verification—evaluating multiple code patches for a given software issue to determine correctness via reasoning rather than execution. This design produces dense, stable rewards and mitigates reward hacking. Built upon Qwen2.5-Coder-32B-Instruct, R4P achieves 72.2% accuracy on the SWE-bench-verified dataset, outperforming OpenAI’s o3 model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative Test-Free Supervision Paradigm.\nR4P redefines software agent supervision as a reasoning task, eliminating the dependency on sandbox testing. This shift addresses scalability, cost, and fragility in existing solutions.\n2. The reward model design is novel."}, "weaknesses": {"value": "1. Scope of this work can be better elaborated.\n2. The evaluation can be more comprehensive.\n\nMy major concen of this work is clarity and evaluation, I believe these shortcomings can be overcame before submitting the camera-ready version.\n\nWhy the reward design is technically sound and how it affects the learning? I think this is important when designing a reward function for reinforcement learning, and maybe it is better to elaborate that it is aligned with your objective to avoid reward hacking.\nEquation (3) can be better explained, and similar problem happened in other places, please define and explain each symbol carefully.\n\nMaybe it is better to include some real data if possible, the current tests are completely on sythetic data.\nI'm curious about the results if the correctness ratio is imbalanced, seems this is more natural in real-world problems."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IsBpHya3PX", "forum": "AXXCo0pOSO", "replyto": "AXXCo0pOSO", "signatures": ["ICLR.cc/2026/Conference/Submission8745/Reviewer_sfSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8745/Reviewer_sfSF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980505993, "cdate": 1761980505993, "tmdate": 1762920535784, "mdate": 1762920535784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to automatically evaluate software repository patches proposed by SWE agents via a verifier model that reasons explicitly about the proposed patches. They propose this as an explicit alternative to simple reward modeling that provides a sparse scalar estimate of quality of proposed patches. The proposed model, R4P, can be used to evaluate SWE agents and to train them in cases where a repository doesn't have ground-truth test cases. Experiments focus on evaluating R4P's quality as a verifier, and on the application of R4P for training SWE agents via RL and improving performance via test-time scaling. As far as I can tell, the main contribution is that R4P is trained to not only provide judgments over patch quality, but to also provide reasoning about its judgments. Augmenting models with reasoning capabilities has been a popular approach for improving model outcomes in difficult tasks, so it makes sense that it works well here too."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The analysis in 5.3 is really comprehensive\n* Experiments prove the efficacy of R4P on a simple agent scaffold, and strong performance as a verifier when compared with existing non-reasoning verifiers"}, "weaknesses": {"value": "* Figure 3 shows rewards for test data. Test data should be used in experiments very sparingly, to avoid compromising integrity of conclusions about model generalization\n* Evaluation is only performed on the mini-SE scaffold, rather than other scaffolds that achieve stronger base performance on SWE-bench-verified. Would R4P still be useful in these other scaffolds? Do these scaffolds make R4P more difficult because they include much longer trajectories, which are more difficult to reason about (although R4P is trained on OpenHands trajectories, so it should be in-distribution to apply it to the OpenHands scaffold)?"}, "questions": {"value": "* What is the difference between DeepSWE-Verifier and DeepSWE-Test?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v0KRarGvKX", "forum": "AXXCo0pOSO", "replyto": "AXXCo0pOSO", "signatures": ["ICLR.cc/2026/Conference/Submission8745/Reviewer_ueZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8745/Reviewer_ueZU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125638360, "cdate": 1762125638360, "tmdate": 1762920535367, "mdate": 1762920535367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}