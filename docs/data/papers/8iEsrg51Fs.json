{"id": "8iEsrg51Fs", "number": 16823, "cdate": 1758269090510, "mdate": 1759897217269, "content": {"title": "SciNav: A Principled Agent Framework for Scientific Coding Tasks", "abstract": "Autonomous science agents, built on large language models (LLMs), are increasingly being investigated to generate hypotheses, design experiments, and produce reports. Prior science agents primarily focus on open-ended scientific problems, where such outputs—hypotheses, experiments, or analyses are inherently subjective and thus difficult to evaluate rigorously. In contrast, existing scientific coding benchmarks provide tasks with clearly defined, executable outputs that enable objective assessment. However, current agent-based approaches to these benchmarks remain engineering-driven pipelines, lacking principled framework design. This mismatch exposes a gap: the absence of end-to-end, principled science agent frameworks for scientific coding tasks. We address this gap by focusing on scientific coding tasks, where evaluation can be made rigorously, and introducing an agent framework SciNav (Scientific Navigator) that enables more effective solution exploration. Our framework is designed to operate efficiently under constrained search budgets, moving beyond reliance on pre-defined success metrics and prolonged search cycles. Inspired by findings that comparative judgments often reveal finer-grained quality differences and therefore provide greater discriminative power than absolute scoring, our framework leverages pairwise relative judgments within a tree search process to select top-K promising solution branches, prune low-potential ones, and progressively narrow down the solution candidates on the selected branches guided by relative comparisons. We demonstrate our agent's effectiveness across different types of tasks on two benchmarks. Experiments show that SciNav significantly outperforms direct prompting and prior agents like OpenHands and Self-Debug across different base models, task types, and difficulty levels, and exceeds different frontier comparators such as random selection and LLM absolute scoring. These results confirm the strength of our agent design and highlight the effectiveness of relative judgment–guided top-K search for high-quality scientific coding, marking a step toward more practical science agents.", "tldr": "", "keywords": ["science agents; test-time scaling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd6b6fc49671ce36f6d97614aa04a7f2ea4c320e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study introduces SciNav, an LLM-based agent framework designed for scientific coding tasks. SciNav employs Top-K Comparative Tree Search to address these tasks under constrained computational budgets. Experimental results demonstrate that SciNav surpasses baseline methods, achieving up to a 24% increase in success rate and a 7.8-point absolute improvement in valid execution rate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThis paper addresses a clearly defined and underexplored problem: scientific coding tasks, which connect scientific reasoning with executable code generation and allow for objective evaluation.\n-\tThe proposed Top-K Comparative Tree Search introduces a novel use of relative LLM judgments for iterative code refinement. It shows consistent empirical improvements over strong baselines.\n-\tThe paper is well-organized with detailed component analysis, ablation studies, and transparent experimental setup across multiple benchmarks and LLMs."}, "weaknesses": {"value": "-\tThe primary concern with this study is the gap between the authors’ claims and their actual contributions: the paper frames SciNav as a general “principled agent framework,” but the work focuses narrowly on code-generation heuristics without formal theoretical grounding or demonstration of broader scientific reasoning.\n-\tThe use of the term “principled” in the title and narrative is inappropriate, as the method is not derived from explicit first principles or formal justification. Currently it is a structured heuristic rather than a principled framework in the scientific sense. I highly recommend the authors to use more appropriate terms.\n-\tThe paper lacks quantitative analysis of computational cost and statistical significance of performance gains brought by the proposed method, making claims of efficiency and reliability only partially supported."}, "questions": {"value": "Please see comments above. One additional question: can scientific coding serve as a valid representation of scientific reasoning tasks? How does the proposed system enhance scientific reasoning in open-ended environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DLgVpVsIsB", "forum": "8iEsrg51Fs", "replyto": "8iEsrg51Fs", "signatures": ["ICLR.cc/2026/Conference/Submission16823/Reviewer_2Rev"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16823/Reviewer_2Rev"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880160032, "cdate": 1761880160032, "tmdate": 1762926852830, "mdate": 1762926852830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SciNav, an agent for scientific coding problems that frames solution search as a Top-K Comparative Tree Search (TKCTS). Instead of relying on absolute LLM scores or task-specific metrics during exploration, SciNav repeatedly performs pairwise (relative) judgments among candidate programs, prunes low-potential branches, and refines the top-K trajectories via self-debug and self-improvement loops. The system is evaluated on ScienceAgentBench and DA-Code, claiming consistent gains over Direct Prompting, Self-Debug, and OpenHands, with ablations suggesting relative judgments beat random or absolute scoring for frontier selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem focus: Targets scientific coding where outputs are executable and evaluable, avoiding the fuzziness of “end-to-end science agents.” \n\n2. Methodical framing: TKCTS with self-debug/self-improve is a clean, modular agent design; Algorithm 1 is easy to follow and implement. \n\n3. Relative judgments: Sensible use of pairwise comparisons; the frontier-comparator ablation shows consistent benefits over random and absolute scoring. \n\n4. Cross-dataset evaluation: Evidence on ScienceAgentBench and DA-Code, with task-type and difficulty breakdowns and an informative error analysis."}, "weaknesses": {"value": "1. Compute fairness & unclear budgets.\nThe paper fixes step counts but does not report token, runtime, or dollar budgets across methods. Without normalized compute, it’s unclear if performance gains stem from the algorithm or simply more compute. Reporting tokens / wall-clock / $ per task and re-running under a fixed compute budget is necessary.\n\n2. Small absolute gains and low success-rate regime.\nImprovements are modest (e.g., \\~2–4% absolute SR gains) and overall SR remains low (\\~15–19%), as seen in Table 2. The practical significance is unclear without confidence intervals, per-task breakdowns, or hypothesis testing.\n\n3. LLM-as-judge bias / circularity risk.\nIf the same model family both generates and judges, relative scoring may reflect stylistic familiarity rather than correctness signal. This risks overestimating benefit of pairwise comparison. Cross-model judging or position-randomization baselines are missing.\n\n4. Baselines not fully representative.\nThe paper omits competitive selection/reranking baselines such as tournament selection over best-of-N, MCTS with learned value, or verification-guided heuristics. These are relevant comparisons that could challenge the novelty claim.\n\n5. Sparse statistical reporting.\nOnly 2–3 runs per setting, no reported CIs, no significance tests, and unclear sampling protocol for DA-Code. More rigorous variance reporting is needed, especially given the stochastic nature of LLM benchmarking.\n\n6. Under-leveraged partial execution signals.\nThe paper emphasizes “no task-specific metric at run time,” but many tasks allow cheap checks (import/compile, partial test subsets, lints). A hybrid relative-judgment + lightweight execution signal could materially improve the agent — and the omission feels like an avoidable limitation rather than a principled choice.\n\n7. Reproducibility gap.\nPrompts are included, but code is “will be released upon acceptance.” Given the importance of queueing, frontier selection, and Elo parameters, anonymized code or pseudocode for comparator internals would improve credibility."}, "questions": {"value": "1. Compute parity:\nCan you report tokens, wall-clock time, and $ cost per task for each method? Do results hold under strict compute-budget matching?\n\n2. Judge/model decoupling:\nDid you evaluate cross-model judging (e.g., Claude evaluates GPT solutions and vice versa)? If not, please include — this is crucial to rule out style bias.\n\n3. Pair selection policy:\nHow exactly are candidate pairs chosen for comparison? Uniform random? Score-based? Uncertainty-based? Please add an ablation isolating this choice.\n\n4. Comparator stability:\nHow sensitive performance is to the Elo update parameters and number of comparison calls? A plot of success-rate vs. comparison budget would help.\n\n5. DA-Code sampling clarity:\nHow were the 100 DA-Code tasks selected and stratified? Please release task IDs and sampling seeds to enable exact replication.\n\n6. Hybrid signal experiment:\nHave you tested combining pairwise judgments with cheap verification signals (static checks, partial tests)? This seems easy to add and directly addresses observed failure modes.\n\n7. Baseline strengthening:\nCan you add tournament-selection, majority-vote ranking over best-of-N, or MCTS-style search? If not, please justify why these are not relevant or already covered by TKCTS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hVZA51yqV8", "forum": "8iEsrg51Fs", "replyto": "8iEsrg51Fs", "signatures": ["ICLR.cc/2026/Conference/Submission16823/Reviewer_9HVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16823/Reviewer_9HVs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884491894, "cdate": 1761884491894, "tmdate": 1762926852369, "mdate": 1762926852369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose and evaluate SciNav, a \"framework\" (agent) that performs scientific coding. The agent performs search taking into account a constrained budget, and uses pairwise (comparative) jugements rather than absolute judgements to guide the search. They show SciNav outperforms prior agents like OpenHands."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical result of outperforming OpenHands and Self-Debug is quite compelling\n - nice to see search budgets taken into account in the framework\n - nice to see you leveraging existing benchmarks rather than creating a new one\n - ablation that suggests relative judgements are helping (a little bit)"}, "weaknesses": {"value": "- Gains are somewhat modest (~2%-3%), so the impact of the work seems a little limited\n - Comparison with genetic algorithm approaches to coding (e.g., in AI Scientist) would be useful \n\nMinor:\n - Abstract takes way to long too get to the goal and contribution - should be stated in first or second sentence. (The abstract gives the impression at first you're going to propose a benchmark)\n - Would be worth expanding on use of relative judgements in AI, e.g., it's the basis of A/B testing, preference optimization (e.g., DPO), and other methods."}, "questions": {"value": "- A common approach in other agent-based coding tasks is to use genetic algorithms to merge different coding ideas (e.g., AIScientist), rather than expanding a single parent. It'd be nice to know how your \"single parent\" approach would compare. Do you have any intuitions about this?\n - It seems that your contribution is more proposing and evaluating TKCTS as a great framework for agent coding, rather than the (narrower) use of comparative judgements. Is that a reasonable reframing of the contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z94W05wcpO", "forum": "8iEsrg51Fs", "replyto": "8iEsrg51Fs", "signatures": ["ICLR.cc/2026/Conference/Submission16823/Reviewer_Eeg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16823/Reviewer_Eeg5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939003012, "cdate": 1761939003012, "tmdate": 1762926851909, "mdate": 1762926851909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on improving scientific coding agents by proposing SciNav, a framework that treats problem-solving as a structured search guided by relative evaluation. They introduce Top-K Comparative Tree Search (TKCTS), which allows the agent to explore, compare, and refine code solutions through pairwise relative judgments rather than absolute scoring. SciNav integrates components for planning, self-debugging, self-improvement, and frontier selection using an Elo-based ranking mechanism. The authors evaluate SciNav on scientific coding benchmarks to demonstrate that this principled, comparison-driven approach leads to more effective and reliable solution generation than the baseline agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper is well-motivated and clearly defines the need for principled frameworks for scientific coding tasks with verifiable outputs.\n\n(2) It presents a structured search method combining relative judgments and iterative refinement, supported by consistent quantitative improvements over existing agent baselines."}, "weaknesses": {"value": "* Evaluation is limited to two controlled benchmarks, leaving uncertainty about generalization to real-world or open-ended scientific tasks.\n* Reliance on LLM-as-judge comparisons may introduce bias, as the same models both generate and evaluate solutions.\n* The fixed and narrow search budget restricts exploration, and scalability to more complex tasks remains unclear."}, "questions": {"value": "* Was a cost or runtime comparison performed to quantify the additional computation introduced by pairwise judgments and iterative search relative to baselines?\n* The TKCTS relies on relative judgments by an LLM-as-judge. How consistent are these judgments across multiple runs or judging models? Would cross-model evaluation (e.g., using a different LLM as the judge) yield stable rankings?\n* The framework uses a fixed budget (five initial solutions, three debug steps, ten total exploration steps). Why were these values chosen, and have the authors tested sensitivity to these parameters?\n* Given that relative judgments guide the search process, was any validation (e.g., human evaluation or ground-truth correctness checks) performed to verify that the LLM-judge’s preferences align with actual code quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J7Vh77J3xh", "forum": "8iEsrg51Fs", "replyto": "8iEsrg51Fs", "signatures": ["ICLR.cc/2026/Conference/Submission16823/Reviewer_2AeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16823/Reviewer_2AeN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942676855, "cdate": 1761942676855, "tmdate": 1762926851427, "mdate": 1762926851427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SciNav (Scientific Navigator), a framework for autonomous science agents designed to tackle scientific coding tasks. The core contribution is the Top-K Comparative Tree Search (TKCTS) algorithm, which replaces absolute scoring with pairwise relative judgments during solution exploration. SciNav integrates several components: initial multi-plan generation, self-debug, iterative self-improvement, and a frontier comparator based on relative LLM judgments, to progressively refine code solutions under constrained computational budgets.\n\nExperiments on ScienceAgentBench and DA-Code show that SciNav performs best compared to baselines such as OpenHands and Self-Debug. Ablations also show that relative comparison helps compared to random selection and absolute scoring."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The relative judgment–guided Top-K search is a well-motivated methodological idea that builds on prior insights about the reliability of pairwise evaluation and applied in an agentic setting.\n\nS2. The experiments are reasonable, covering two benchmarks, several LLM backbones, and detailed component ablations. The experiments for the contributions of each component, including initial plan diversity, self-improvement, and the comparator strategy are appreciated."}, "weaknesses": {"value": "While the results and experiments are good, my main concerns center around how much we can interpret from them which I'm happy to change with some clarification.\n\nFirst, the paper does not report error bars or statistical significance. This makes it hard to assess whether observed performance differences are meaningful or consistent across runs.\n\nSecond, it is important, especially when we consider deployment to also compare the cost of each agent/ablation involved. How many extra LLM calls/tokens are used for SciNav vs. Self-debug to obtain the performance increases? What is the cost of inference time or $ cost to have extra LLM calls? \n\nWithout this information, it is hard to assess the tradeoff/value of SciNav. For instance, how much of this performance gain is just a result of scaling test-time compute."}, "questions": {"value": "What is the K in top-K and the comparison budget for the experiments?\nHow does changing this impact the performance? Given the main contribution is the comparator it would be helpful to see how much performance is impacted by these hyperparamter choices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MAULRrJWPJ", "forum": "8iEsrg51Fs", "replyto": "8iEsrg51Fs", "signatures": ["ICLR.cc/2026/Conference/Submission16823/Reviewer_2cow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16823/Reviewer_2cow"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950582421, "cdate": 1761950582421, "tmdate": 1762926851019, "mdate": 1762926851019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}