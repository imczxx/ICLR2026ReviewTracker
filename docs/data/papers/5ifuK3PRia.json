{"id": "5ifuK3PRia", "number": 18252, "cdate": 1758285661393, "mdate": 1759897116313, "content": {"title": "RaceCLIP: Enhancing medical vision-language representation learning via retrieval augmented caption enrichment", "abstract": "Contrastive Language-Image Pre-training (CLIP) has demonstrated strong potential in learning transferable visual models by aligning paired images and textual descriptions. However, the quality of training data remains a significant bottleneck. In many real-world scenarios, image-text pairs are often noisy or accompanied by captions that are too short or generic to convey key visual attributes. For example, in medical imaging, most available data come from illustrative figures in public literature instead of detailed clinical reports, resulting in captions that lack the precision and context provided by expert annotations. Recent efforts to improve caption quality using Large Language Models (LLMs) have largely focused on natural images and overlook the integration of domain-specific knowledge. In this study, we propose a Retrieval-Augmented Generation (RAG) framework guided by expert semantic knowledge to enrich image captions in the medical context. We further introduce a multi-text training strategy that effectively incorporates these enriched descriptions into CLIP training. Our approach, demonstrated in the medical domain as a proof of concept, achieves state-of-the-art performances on multiple downstream tasks, highlighting its broader potential for vision-language pretraining in specialized domains. Our code is available at https://anonymous.4open.science/r/RaceCLIP-D4C5.", "tldr": "", "keywords": ["CLIP", "RAG"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a89fe5d9c7b4504ebf213f35572419bd95d91a6.pdf", "supplementary_material": "/attachment/dc1c1068f101a0fe447f4c47421e85c52ce5ba34.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RaceCLIP, a framework that combines Retrieval-Augmented Generation (RAG) with CLIP training for medical vision-language representation learning. The authors use LLaVa-Med with UMLS knowledge retrieval to generate enriched captions for the ROCO dataset (81K image-text pairs), creating ROCOcap, and introduce a multi-text contrastive loss to align images with multiple augmented captions simultaneously. The method is evaluated on image retrieval and zero-shot classification tasks across multiple medical imaging datasets, showing improvements over baselines including CLIP, MedCLIP, LaCLIP, and VeCLIP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic engineering approach. The paper demonstrates careful integration of RAG with medical image captioning, thoughtfully combining UMLS knowledge (definitions and relations) with multimodal LLMs to generate enriched descriptions.\n2. Comprehensive experimental validation. The authors provide detailed ablation studies (Table 3) showing the contribution of different retrieval components and evaluate their method across diverse downstream tasks including image-to-image retrieval, cross-modality retrieval, and zero-shot classification on multiple datasets (ROCO, MEDICAT, MURA, IRMA).\n3. Reproducibility commitment. The authors promise to release both the source code and the ROCOcap dataset upon acceptance, which would benefit the research community.\n4. Domain-specific knowledge integration. The paper demonstrates how to systematically incorporate expert semantic knowledge (UMLS) into the caption generation process, which is particularly relevant for specialized domains like medical imaging."}, "weaknesses": {"value": "1. The paper claims medical datasets are \"orders of magnitude smaller\" than natural image datasets due to privacy constraints, but this is demonstrably false as of 2025. Multiple large-scale medical image-text datasets now exist, including BIOMEDICA (24M+ pairs), PMC-OA (1.6M pairs), PMC-15M (15M pairs). The ROCO dataset (81K pairs) used in this paper represents only very few portion of available medical imaging data. \n\n2. The authors train exclusively on ROCO (81K low-quality figure captions) without providing any scientific justification for why they ignore vastly larger and higher-quality alternatives like BIOMEDICA or PMC-15M. Spending 360 GPU hours to generate 81K synthetic captions (which may contain hallucinations) instead of using 24M+ real captions is questionable, and the paper provides zero rationale beyond implicit convenience.\n\n3. The paper compares against methods from 2021-2024 but ignores modern medical CLIP models trained on large-scale data (PMC-OA, PMC-15M, BIOMEDICA). A fair comparison requires evaluating whether RAG on 81K samples outperforms simple scaling to millions of real samples, and without comparing \"ROCO (81K) + RAG\" versus \"BIOMEDICA (24M) baseline,\" we cannot assess whether RAG adds value or if data scaling is simply more effective.\n\n4. The paper provides no quantitative hallucination analysis, no expert validation of generated captions, and no comparison of error rates between synthetic and real clinical reports.\n\n5. Limited novelty as incremental combination of existing techniques like RAG, LLM-based recaptioning from LaCLIP/VeCLIP, multi-text contrastive learning. \n\n6. The paper provides no cost-benefit analysis comparing performance per GPU hour against simply training on larger existing datasets, and cherry-picks metrics while dismissing cases where RaceCLIP underperforms (e.g., IRMA Top-1: 54.77 vs LaCLIP 54.83).\n\n7. All experiments are conducted only on small datasets (ROCO test 10K, MEDICAT 8K) without evaluation on large-scale benchmarks, and it's unclear whether the RAG approach scales to millions of images or how the 3% training overhead increases with more augmented texts."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xHJyJ3Dkz0", "forum": "5ifuK3PRia", "replyto": "5ifuK3PRia", "signatures": ["ICLR.cc/2026/Conference/Submission18252/Reviewer_kptq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18252/Reviewer_kptq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612953722, "cdate": 1761612953722, "tmdate": 1762927980132, "mdate": 1762927980132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RACECLIP introduces a UMLS-based retrieval mechanism combined with multi-text contrastive learning to address well-known limitations of standard CLIP models in the medical domain, particularly their limited domain knowledge and reliance on noisy captions. The method shows promise in improving medical understanding and overall performance.\n\nHowever, the pipeline lacks sufficient rigor and transparency in several key stages. The concept extraction and CUI matching steps are not clearly explained, the clinical validity of retrieved relationships is not systematically verified, and the risks of semantic drift or hallucination in generated captions are not thoroughly analyzed. Strengthening these aspects with more detailed evaluation and failure case analysis would enhance the soundness and practical applicability of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduce RACECLIP, which integrates UMLS-based retrieval and a multi-text contrastive learning strategy to address known limitations of conventional CLIP models in medical settings, such as limited medical knowledge and noisy captions."}, "weaknesses": {"value": "Overall, more rigorous experiments and clearer step-by-step pipeline descriptions are needed.\n\n1.\tInsufficient clarity in concept extraction and CUI matching: The ScispaCy-based mapping process lacks concrete details on similarity metrics, thresholds, and synonym resolution strategies. Without quantitative error analysis, it is difficult to assess how mapping errors propagate into downstream components.\n\n2.\tLimited validation of UMLS-based relationship retrieval: Relations not explicitly encoded in UMLS are filtered with PubMedBERT, yet there is no systematic validation that these inferred relations align with clinical context, raising concerns about semantic drift. Human review and more thorough analyses are needed.\n\n3.\tInadequate verification of generated captions: Errors introduced in retrieval can yield semantic distortion or hallucination during caption generation. While this possibility is acknowledged, the paper does not provide deeper failure analyses or concrete mitigation mechanisms."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jcskLXVmSr", "forum": "5ifuK3PRia", "replyto": "5ifuK3PRia", "signatures": ["ICLR.cc/2026/Conference/Submission18252/Reviewer_Q3WE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18252/Reviewer_Q3WE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635318433, "cdate": 1761635318433, "tmdate": 1762927979760, "mdate": 1762927979760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RaceCLIP, a medical vision-language framework using retrieval-augmented generation (RAG) and domain knowledge (UMLS) to enrich image captions and train CLIP with multi-text contrastive learning. Experiments on multiple medical datasets show consistent gains over existing CLIP baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Integration of RAG with domain-specific knowledge is technically sound.\n2. Clear motivation on improving noisy medical captions, addresses the common issue of poor-quality captions in medical datasets.\n3. Releases an enriched ROCO dataset for community use.\n4. Private code sharing is considered as a merit."}, "weaknesses": {"value": "1. Introduction is somewhat messy wth very long paragraph, clearer structure on problem, challenge, and motivation needed.\n2. Incremental combination of RAG and MLLM leads to limited methodological novelty.\n3. Experiments are relatively narrow with no comparison to recent RAG-based works, and omits non-medical domains, weakening generalization claims."}, "questions": {"value": "Can the authors clarify the key novelty and how it differs from recent RAG-based medical VL frameworks (e.g., HeteroRAG, MMED-RAG)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qIzqR5t4AH", "forum": "5ifuK3PRia", "replyto": "5ifuK3PRia", "signatures": ["ICLR.cc/2026/Conference/Submission18252/Reviewer_TYM2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18252/Reviewer_TYM2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870556305, "cdate": 1761870556305, "tmdate": 1762927979288, "mdate": 1762927979288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a technique for harnessing synthetic data for data augmentation to train CLIP models in the medical domain.\nIn particular, it uses RAG over knowledge resources (UMLS) for data augmentation to obtain additional textual descriptions.\nThis extended data is then used to train an improved CLIP model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Improvements over some prior models"}, "weaknesses": {"value": "- The motivation provided in the introduction does not align that well with the developed solution. The authors rightly point out that \"the distinction between “normal” and “abnormal” in medical imaging often exhibits minimal visual differentiation.\" However, it is not clear why data augmention from a knowledge store such as UMLS is the most appropriate solution in this case.\n\n- The paper also presents a loss functon for handling multiple texts per image in Section 3.3. Howvever, the paper does not properly acknowledge the numerous instances of prior work exploring these sorts of improved loss function.\n\n- The set of baselines is very limited.\n\n- There does not seem to have been any proofreading of the resulting PDF, as all references are ill-formatted.\n\n- Several of the tables are poorly formatted, with fonts squashed or stretched."}, "questions": {"value": "- LLMs likely already possess knowledge from UMLS. Would it be possible to develop an approach that better leverages the intrinsic knowledge of LLMs instead of depending on extra training examples for each piece of knowledge in UMLS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hH72BL9N6j", "forum": "5ifuK3PRia", "replyto": "5ifuK3PRia", "signatures": ["ICLR.cc/2026/Conference/Submission18252/Reviewer_kdfY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18252/Reviewer_kdfY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134607184, "cdate": 1762134607184, "tmdate": 1762927978792, "mdate": 1762927978792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}