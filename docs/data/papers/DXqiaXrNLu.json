{"id": "DXqiaXrNLu", "number": 13739, "cdate": 1758221829541, "mdate": 1759897416492, "content": {"title": "Old Memories Die Hard: Understanding Challenges of Privacy Unlearning in Large Language Models", "abstract": "Large language models (LLMs) often memorize private information during training, raising serious privacy concerns. While machine unlearning has emerged as a promising solution, its true effectiveness against privacy attacks remains unclear.\nTo address this, we propose Prileak, a new evaluation framework that systematically assesses unlearning robustness through three-tier attack scenarios: direct retrieval, in-context learning recovery, and fine-tuning restoration; combined with quantitative analysis using forgetting scores, association metrics, and forgetting depth assessment.\nOur study exposes significant weaknesses in current unlearning methods, revealing two key findings: 1) unlearning exhibits ripple effects across gradient-based associated data, and 2) most methods suffer from shallow forgetting, failing to remove private information distributed across multiple model layers.\nBuilding on these findings, we propose two key strategies: association-aware core-set selection that leverages gradient similarity, and multi-layer deep intervention by progressive learning rates and representational constraints. These strategies represent a paradigm shift from shallow forgetting to deep forgetting.", "tldr": "", "keywords": ["LLM Unlearning", "Relearning Attack", "Privacy Leakage", "CKA"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d7cce19a84eb927ebab91a46128a715112c3364.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the robustness of privacy unlearning methods for large language models (LLMs). It introduces PriLeak, a new evaluation framework that tests unlearning effectiveness against active attacks: direct retrieval, in-context recovery, and fine-tuning restoration. Using the Enron dataset and LLaMA-3.2-3B, the authors benchmark 19 existing unlearning methods and find that many approaches only achieve shallow forgetting, with sensitive information quickly recoverable. Their analysis points to two core issues: forgetting ripples across associated data and fails to penetrate deeper network layers. Based on these insights, they propose strategies including association-aware core-set selection and multi-layer intervention to strengthen privacy forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The motivating problem is important and timely, and the paper demonstrates clear originality by shifting the evaluation of unlearning into more realistic active attacker scenarios. The PriLeak benchmark is a meaningful contribution to the community, offering nuanced and multi-tiered measurements of privacy persistence that go beyond passive-output testing. The identification of ripple effects and shallow forgetting shows careful empirical analysis, revealing mechanisms that prior work did not clearly articulate. The proposed strategies are incremental but directionally interesting, suggesting a practical path toward deeper and more resilient forgetting. The scope of the evaluation (19 methods, both known/unknown private data, multiple datasets) supports the value of the empirical findings."}, "weaknesses": {"value": "The technical novelty of the proposed strategies feels modest relative to the strong emphasis on negative benchmarking results. The empirical section is heavily overloaded with tables and metrics, making key insights harder to follow; clearer narrative summarization would help. The benchmark relies on a single primary dataset for privacy testing, limiting the generalizability of the findings in real-world PII contexts. Some terminology such as “deep forgetting” remains conceptual without rigorous formalization or theoretical insight. The study of known vs. unknown private data is compelling but deserves deeper exploration: why does forgetting propagate similarly, and what constraints does this impose on future algorithm design? Finally, while the experiments are extensive, clear ablation results are needed to isolate where improvements truly come from in the proposed approach."}, "questions": {"value": "One concern is whether the strong performance gaps between different methods are sensitive to hyperparameter choices. Is PriLeak intended to be a fixed evaluation standard or a benchmark whose scores vary significantly depending on tuning choices? The ripple-effect analysis suggests that privacy removal conflicts with utility preservation, yet the proposed method still appears vulnerable under P3. How do you envision closing the remaining >30% recovery gap? It would be helpful to clarify whether the benchmark could incorporate adaptive adversaries rather than predefined fixed attacks. The current selection of PII types from Enron seems narrow; could the proposed metrics generalize to more complex private attributes such as implicit identifiers? Finally, the proposed representation-anchoring loss uses noise-perturbed base states. Could you justify this choice more rigorously or compare with alternative anchoring methods (e.g., teacher-student consistency with privacy-filtered representations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XXq18RKbln", "forum": "DXqiaXrNLu", "replyto": "DXqiaXrNLu", "signatures": ["ICLR.cc/2026/Conference/Submission13739/Reviewer_Kfzc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13739/Reviewer_Kfzc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657181985, "cdate": 1761657181985, "tmdate": 1762924277858, "mdate": 1762924277858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PriLeak, a new evaluation framework that assesses unlearning robustness through three-tier attack scenarios: direct retrieval, in-context learning recovery, and fine-tuning restorationn; combined with quantitative analysis using forgetting scores, association metrics, and forgetting depth assessment. Empirical studies expose weaknesses in current unlearning methods --- ripple effects across gradient-based associated data and shallow forgetting. The paper then proposes  association-aware core-set selection based on gradient similarity and multi-layer deep intervention as two strategies to mitigate the issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** The new benchmark is well motivated with principled designs.\n\n**S2.** Empirical studies cover extensive unlearning methods and present interesting insights.\n\n**S3.** The proposed two strategies effectively improve the unlearning.\n\n**S4.** The paper is well presented."}, "weaknesses": {"value": "**W1.** The empirical analyses are constrained to relatively small LLMs (LLaMA-3.2-3B and GPT-2).\n\n**W2.** The paper's presentation may be further improved by highlighting the findings previous benchmarks did not yield in empirical study analyses.\n\n**W3.** While I like the idea of fine-tuning-based recovery, I think it will be more interesting to check if fine-tuning on related but non-private data restores unlearned private data, e.g., email addresses of public figures or organizations.\n\n**W4.** Minor Presentation Issues.\n- The full name of CKA should be provided at its first appearance.\n- Notations $\\mathbb{D}_{uk}$ and $\\mathbb{D}_k$ should be explicitly defined at their first appearance (L207) despite analogous definitions at L107.\n- There are two consecutive \"with\" at the end of L257.\n- The presentation of Table 1 can be potentially improved by using different colors to group numbers in different ranges."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "POMlWcBSU8", "forum": "DXqiaXrNLu", "replyto": "DXqiaXrNLu", "signatures": ["ICLR.cc/2026/Conference/Submission13739/Reviewer_FRQq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13739/Reviewer_FRQq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952399742, "cdate": 1761952399742, "tmdate": 1762924277502, "mdate": 1762924277502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of machine unlearning in large language models (LLMs), aiming to evaluate how effectively models can remove specific knowledge while maintaining general utility. The authors propose a unified benchmark that measures residual memorization through multiple levels of privacy leakage and a utility metric capturing general performance retention. They conduct large-scale experiments on the Enron and MUSE datasets, benchmarking various representative unlearning methods. The results reveal that existing methods still struggle to achieve thorough and reliable forgetting, highlighting the challenge of ensuring complete unlearning in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper conducts extensive experiments, covering a wide range of unlearning methods.\n\n\n2. The writing is clear and well organized, making the experimental setup and findings easy to follow.\n\n\n3. The experimental design is generally complete, with systematic evaluation across multiple models, datasets, and metrics."}, "weaknesses": {"value": "1. My main concern is the novelty of the paper. The core idea that unlearning certain knowledge propagates to semantically related facts via shared representations closely parallels prior studies on ripple effects in knowledge editing [1-5]. Similar analyses of entanglement and edit locality have been thoroughly explored, making the framing here appear incremental rather than conceptually new.\n\n\n2. The proposed measurements resemble established notions such as locality, retention, and causal entailment used in both knowledge editing and unlearning literature. Prior works including RippleEdits [4], MUSE [9], WMDP [10], and Deep Unlearning [11] have already operationalized comparable metrics for quantifying cross-fact interference. The paper does not convincingly justify why its definitions capture fundamentally different or deeper dynamics [6–11].\n\n\n3. The paper’s setup is closely related to knowledge editing frameworks. It is better to also include knowledge editing methods as well.\n\nReferences:\n\n [1] Locating and Editing Factual Associations in GPT Models.\n\n [2] Mass-Editing Memory in a Transformer.\n\n [3] Editing Factual Knowledge in Language Models.\n\n [4] Evaluating the Ripple Effects of Knowledge Editing in Language Models.\n\n [5] Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering.\n\n [6] TOFU: Benchmarking Factual Unlearning in LLMs.\n\n [7] Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models.\n\n [8] Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness.\n\n [9] MUSE: A Benchmark for Evaluating Unlearning in LLMs.\n\n [10] WMDP: Unlearning Harmful Knowledge in LLMs.\n\n [11] Evaluating Deep Unlearning in Large Language Models."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dDBL1yXKe7", "forum": "DXqiaXrNLu", "replyto": "DXqiaXrNLu", "signatures": ["ICLR.cc/2026/Conference/Submission13739/Reviewer_21E2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13739/Reviewer_21E2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015242697, "cdate": 1762015242697, "tmdate": 1762924276995, "mdate": 1762924276995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the limits of machine unlearning for large language models and introduces PriLeak, a benchmark for evaluating how well different unlearning methods resist privacy-related attacks. The framework tests models under three settings: direct extraction, recovery through in-context prompts, and recovery after fine-tuning. This goes beyond the usual unlearning benchmarks by allowing the attacker to access and modify model weights. The study compares 19 existing approaches and shows that many achieve only superficial removal of memorized information. \n\nThe authors identify two main issues: (1) cross-sample “ripple effects,” where unlearning one example inadvertently alters or forgets related data through shared gradient directions, and (2) “shallow forgetting,” where parameter changes remain concentrated in higher network layers while earlier representations continue to encode sensitive content. To address these, the paper proposes two strategies: an association-aware core-set selection method that identifies the most influential samples to forget based on gradient similarity, and a multi-layer intervention approach that adjusts learning rates and representational constraints across network depth to promote more complete forgetting without excessive performance loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Focuses on adaptive / active privacy attack via fine-tuning, whereas prior papers are focused on the case where the user only has API access to the model (can attack via ICL or via QA).\n2. Identifies additional insights into how/where the model has stored the data that is meant to be unlearned, which allows them to improve the unlearning procedure to ensure information is removed throughout all the layers. The insights are consistent across two model architectures which makes them more convincing.\n3. Experiments are thorough and provide interesting information on the existing unlearning algorithms."}, "weaknesses": {"value": "1. The fine-tuning–based attack scenario assumes that an adversary has access to modify model weights. This setting is not clearly justified and may not reflect realistic deployment conditions, where most users interact only through APIs. Clarifying when and why this is a threat model we would care about is important to make the arguments of th epaper.\n2. The paper does not sufficiently connect its metrics and analysis to existing work on memorization and knowledge localization in language models. There is a rich literature on tracing and diagnosing memorized content (e.g., influence functions, causal mediation, or representation probing), and the relationship between those approaches and the proposed metrics is not discussed. This makes it difficult to assess the conceptual novelty of the diagnostic tools. Moreover, the proposed evaluation metrics and the “deep forgetting” interpretation rely on several design choices (layer selection, gradient normalization, metric thresholds) that are not analyzed for robustness. Without sensitivity studies, it is unclear how stable these results are across architectures or training configurations.\n4. The link between representational change and actual privacy protection remains partly qualitative. The evidence that deeper layer modification corresponds to stronger unlearning is plausible but not rigorously demonstrated. Can the authors provide more direct evidence that these changes correspond to actual reductions in recoverable private information, rather than general representational drift?\n5. Enron and MUSE News contain highly structured PII as I understand it. So would the results hold in cases where PII is more diffuse? And what about forgetting more general knowledge?\n6. Recent literature has drawn into question the utility of the traditional unlearning definition (matching re-training from scratch). The authors do not comment on this at all, and this relates to my first question about why looking at FT attacks in this setting is even interesting in the first place.\n\n\nSeparately from this specific paper's methods, I think there are a lot of unlearning papers flooding the space without making meaningful improvements or insights. The lack of true, sociotechnical motivation for the new setting in this paper is just further evidence that the authors may not have thought through what is really important for unlearning from a social / legal standpoint. There is little to no discussion beyond the usual GDPR citation. I am also concerned, on the technical side, that the authors made little effort to connect to any literature that does not directly discuss unlearning (eg memorization literature)."}, "questions": {"value": "The weaknesses above contain the questions that I have about the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eq39zXnj6v", "forum": "DXqiaXrNLu", "replyto": "DXqiaXrNLu", "signatures": ["ICLR.cc/2026/Conference/Submission13739/Reviewer_E2YZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13739/Reviewer_E2YZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099606713, "cdate": 1762099606713, "tmdate": 1762924276458, "mdate": 1762924276458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}