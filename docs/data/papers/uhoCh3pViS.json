{"id": "uhoCh3pViS", "number": 17302, "cdate": 1758274444696, "mdate": 1759897183708, "content": {"title": "Dream-MPC: Gradient-Based Model Predictive Control with Latent Imagination", "abstract": "State-of-the-art model-based Reinforcement Learning (RL) approaches either use gradient-free, population-based methods for planning, learned policy networks, or a combination of policy networks and planning. Hybrid approaches that combine Model Predictive Control (MPC) with a learned model and a policy prior to efficiently leverage the benefits of both paradigms have shown promising results. However, these hybrid approaches typically rely on gradient-free optimization methods, which can be computationally expensive for high-dimensional control tasks. While gradient-based methods are a promising approach, recent works have empirically shown that gradient-based methods often perform worse than their gradient-free counterparts due to the fact that gradient-based methods can converge to suboptimal local optima and are prone to exploding or vanishing gradients. We propose Dream-MPC, a novel approach that generates few candidate trajectories from a rolled-out policy and optimizes each trajectory by gradient ascent using a learned world model. We incorporate uncertainty regularization directly into the optimization objective and amortize optimization iterations over time by reusing previously optimized actions. We evaluate our method on multiple continuous control tasks from the DeepMind Control Suite, Meta-World and HumanoidBench and show that gradient-based MPC can significantly improve the performance of the underlying policy and can outperform gradient-free MPC and state-of-the-art baselines. To facilitate further research on gradient-based MPC, we will open source our code and more at https://dream-mpc.github.io.", "tldr": "", "keywords": ["model-based reinforcement learning", "model predictive control", "world models", "gradient-based trajectory optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3161cf5c8691aa44d8a6eec272ec5762190e118.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces Dream-MPC, a gradient-based planning framework that integrates a learned policy prior and world model for trajectory optimization. The method builds on TD-MPC2 (Hansen et al., 2024) by replacing the MPPI planner with a gradient-based differentiable planner commonly found in classical receding horizon MPCs. To stabilize optimization, the authors introduce (1) an uncertainty regularization term and (2) amortization of optimization iterations across time by warm-starting from previous solutions. The authors do extensive evaluations on 24 continuous control tasks from different simulation suites and compare their proposed approach against SAC, DreamerV3, TD-MPC2, and BMPC. The authors report that their method can match or outperform gradient-free methods, particularly when initialized from a strong policy prior."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. **Readable and well-organized**: the paper is clearly well written with effective figures and consistent notation with prior works. The motivation for gradient-based MPC is framed coherently within prior literature of world models (Hafner et al., 2020; Hansen et al., 2022).\n2. **Extensive empirical evaluation**: the experiment section is broad and well-presented, covering a variety of domains and considered image-based tasks. The baseline methods are well chosen, and the presented metrics from Agarwal et al., 2021, lend credibility to the reported performance."}, "weaknesses": {"value": "1. **Limited novelty**: the main algorithmic structure is largely inherited from Grad-MPC (S V et al., 2023) and TD-MPC2 (Hansen et al., 2024). The paper's only conceptual addition is the uncertainty penalty (Eq. 5) and minor implementation details. These are relatively minor modifications that do not constitute a substantial advancement in the field.\n2. **Questionable motivation for the uncertainty term**: the proposed regularization term $u_t = mean(q_{1:M}) std(q_{1:M})$ is problematic. Maximizing $-u_t$ effectively drives $Q(s,a) \\rightarrow 0$, which conflicts with the goal of maximizing $Q(s,a)$. Unlike prior methods (e.g., CQL for uncertainty exploration), this term is not theoretically justified for a general RL algorithm.\n3. **Unclear experiment setup**: As it stands, I would not feel confident reproducing Section 5 given the details provided. For example, it is not clear to me which parts of this section are in an offline or online RL setting. While the appendices provide detailed hyperparameters, the authors should consider expanding on the experiment setup and moving some details from the appendix into Section 5.\n4. **Weak ablations and inconclusive results**: while I was very excited about this section, I found the number of environments limited and the results mixed. The paper asserts that uncertainty regularization and action reuse are beneficial, yet the shown results do not clearly support that. Critically, there is also missing analysis on the importance of warm starting their planning."}, "questions": {"value": "1. **Sample count**: why do you use only 5 samples for planning? To the best of my knowledge, samples are just bound by available memory (and the gradient buffering required). Personally, I have used 100+ samples in similar settings.\n2. **Stochastic optimization**: why don't you treat your planning as a stochastic optimization process and choose to optimize each sampled trajectory independently? \n3. **Policy initialization**: how well does your method work if you do not warm-start your planning from a feedback policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YlPibdBw17", "forum": "uhoCh3pViS", "replyto": "uhoCh3pViS", "signatures": ["ICLR.cc/2026/Conference/Submission17302/Reviewer_paQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17302/Reviewer_paQ4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534556578, "cdate": 1761534556578, "tmdate": 1762927240320, "mdate": 1762927240320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Model Predictive Control (MPC) is a method that uses a world model (WM) or simulator to plan actions before taking them in a target environment (like the real world).  MPC looks $k$ steps into the future, and finds the trajectory which has the highest reward, then generally only takes the first action.  Finding good actions in the WM is normally either done by sampling from a policy, a gradient free populations based methods, or some hybrid of the 2.  \n\nThe WM can used to learn a reactive policy, as done in Dreamer.  However, purely reactive policies can have limited generalization.  Gradient free model free optimization methods like Cross Entropy Method (CEM) scale can poorly as you increase the number of dimensions.  In contrast, gradients perform very well in optimizing very high dimensional spaces and thus should be a natural fit for optimizing the trajectories.\n\nPast approaches to gradient based MPC have performed poorly.  The authors introduce a novel methods called Dream-MPC that uses gradient ascent to optimize trajectories that overcomes some of these shortcomings.  They start by sampling a small number of trajectories from the policy, then proceed to optimize the actions using gradient ascent, in each iteration recomputing the latent states from the rolled out actions.  In order to not overfit, uncertainty regularization is added to the gradient target as well. \n\nMPC works by replanning to the end of the full window, but then only taking a first (few) actions.  In later iterations, the old \"tails\" from the previous trajectories are mixed in with the new samples from the polices, amortizing the optimization iterations over time. \n\nThey integrate their gradient based action optimization method into TD-MPC2 and BMPC - two existing model based reinforcement learning algorithms. They go on to show that they significantly outperform existing model based reinforcement learning algorithms on a wide variety of tasks from deepmind control, humanoid bench and meta world.  \n\nFinally, they perform ablation studies, demonstrating the importance of warm starting with policy samples,  uncertainty regularization, and even the benefit of using gradient ascent at all, showing each part is important for the final performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "## Originality\nWhile gradient optimization on top of MPC is definitely not a novel idea, the authors combined many pieces to actually get a SOTA system, which in itself is an original contribution.  \n\n## Quality\nThe model is well formulated, and motivated, and the differences from the existing methods in the filed are clear.  Also, the experiments are well run, rigorously defined, and clearly demonstrate the superiority of their method.  The ablations clearly motivate each of their model decisions.  They are careful in how they compare RL performance between the different methods as well. \n\n## Clarity\nThe problem to solve is clearly laid out, and are good at motivating the approach taken.  It should be possible to replicate the results from the descriptions in the paper. \n\n## Significance\nThe results clearly show their method is better.  Unlocking gradient based optimization in combination with world models is also likely to result in further research and impacts as other researches seek to improve upon their innovations."}, "weaknesses": {"value": "Nothing specific comes to mind."}, "questions": {"value": "Did you consider using more powerful optimizers like ADAM, etc?  Why did you choose to only use plain gradient ascent?  Alternatively, as the state spaces are small, do natural gradient style methods help the optimization process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKQnVE5Udk", "forum": "uhoCh3pViS", "replyto": "uhoCh3pViS", "signatures": ["ICLR.cc/2026/Conference/Submission17302/Reviewer_QH6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17302/Reviewer_QH6V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950840041, "cdate": 1761950840041, "tmdate": 1762927239822, "mdate": 1762927239822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Dream-MPC, a gradient-based model-predictive controller that warm-starts each optimisation episode with a **stochastic policy prior, re-uses past optimised actions, and penalises uncertainty**. Empirical results on 24 tasks (DM-Control, HumanoidBench, Meta-World) and four image-based settings show that the method improves the underlying policy and, when coupled with only BMPC, matches or exceeds MPPI.\n\n## Main Experiments\n\n- Offline checkpoints: Replace MPPI in **TD-MPC2/BMPC with Dream-MPC** at test time (1 it × 5 candidates, H = 3).\n- Online training: Learn world-model + actor with Dream-MPC planner on four DM-Control tasks; compare against Dreamer, PlaNet, SAC+AE, Grad-MPC, hybrid Grad-MPC.\n- Ablations: Remove **action-reuse, uncertainty term, and policy prior**. sweep over horizon/iterations/candidates.\n- Runtime: Inference latency on RTX-4090."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n\n- Dream-MPC is designed as a **drop-in module** that slots into popular world-model agents (Dreamer-v3, TD-MPC2, BMPC) without altering their training loops. This practical compatibility lowers the barrier to adoption in both research and real-time robotics deployments.\n\n-  The authors include a thorough **gradient-variance and ESNR analysis across horizons**, an insight likely to inform future gradient-based MPC work.\n\n- Low Sensitivity to Candidate Count : An ablation shows that performance saturates with as few as five candidate trajectories, indicating robustness to the primary computational knob and making the method attractive for resource-constrained settings."}, "weaknesses": {"value": "## Weaknesses\n\n- **Generality of the Method**: The paper's framing of the method's effectiveness could be moderated. The empirical results suggest the performance improvements are not uniform. For instance, the gains when integrating with TD-MPC2 appear to be minimal or, in some cases, negative , while the improvement over the **strong BMPC** baseline is modest. This selective efficacy suggests the method is more of a targeted enhancement for specific model-based architectures (like BMPC) rather than a universally applicable solution for all world models. As confirmed in **Figure 3 and 4**.\n\n- Confounded Attribution of Gains: The attribution of performance gains is somewhat confounded by the experimental design. Key parameters, such as learning-rate schedules and candidate budgets (e.g., **5 for Dream-MPC vs. 512 for MPPI**), differ between the proposed method and the baselines. This makes it challenging to isolate the specific contribution of the novel components, such as the uncertainty penalty, from the effects of other hyperparameter choices.\n\n- Incremental novelty: The present work mainly extends Hybrid Grad-MPC with the horizon and adds a small regularizer. Functional and performance overlap is acknowledged in **Table 14 and 15**. Methodological delta over Hybrid Grad-MPC may be modest (longer horizon + variance penalty)."}, "questions": {"value": "## Read Weaknesses \n\n- TD-MPC2 integration – Why does substituting Dream-MPC for MPPI yield clear gains in BMPC, yet provide little or no improvement when plugged into TD-MPC2? Could the authors elaborate on where this problem comes from?.\n\n- Uncertainty penalty – Why is **λ_unc fixed to 0.01, Action-reuse coefficient to ρ=0.1** chosen heuristically, no sensitivity study.\n\nAdditional Remarks\n\n- Hybrid Grad-MPC equivalence: The manuscript states that “Hybrid Grad-MPC is equivalent to Dream-MPC with a single candidate trajectory and a horizon of one,” yet immediately notes that the results could not be reproduced. I think, Hybrid Grad-MPC treat **candidate count and horizon** as tunable hyper-parameters and evaluate primarily on **sparse-reward tasks**. the current framing may inadvertently misrepresent that work's contribution. A clearer explanation (or removal) of the equivalence claim would avoid confusion.\n\n- Hyper parameter Accessibility: The paper's readability would be improved by moving essential hyper parameters from the appendix into the main text. Key settings, such as the λ_unc coefficient and the planning/optimization parameters, are critical to understanding the method. A **concise table in the main body would allow readers to grasp the method's essential configuration** without needing to frequently reference the appendix, like PPO table listed in Dreamer V3 paper.\n\n\nGiven the method’s limited generality, incremental gains & novelty, unclear attribution of improvements, it does not meet the significance threshold for acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s0O9W6vejE", "forum": "uhoCh3pViS", "replyto": "uhoCh3pViS", "signatures": ["ICLR.cc/2026/Conference/Submission17302/Reviewer_xthc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17302/Reviewer_xthc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980278996, "cdate": 1761980278996, "tmdate": 1762927239188, "mdate": 1762927239188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dream-MPC, a novel gradient-based Model Predictive Control (MPC) method, aiming to address the issues of high computational cost and low efficiency in high-dimensional tasks of traditional hybrid MPC (which relies on gradient-free optimization), as well as the problems of traditional gradient-based methods being prone to falling into local optima and gradient anomalies. It generates a small number of candidate trajectories (5 by default) from a policy network, optimizes them using gradient ascent with a learned world model, incorporates uncertainty regularization (estimating uncertainty based on an ensemble of Q-functions and penalizing trajectories with high uncertainty in the objective), and realizes the amortization of optimization iterations by reusing previously optimized actions. Validated on continuous control tasks, it improves the performance of the underlying policy (e.g., achieving the best average score when based on BMPC) and features fast inference speed. It can be flexibly integrated into frameworks such as TD-MPC2, BMPC, and Dreamer."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experiments in the paper are sufficiently comprehensive and yield rich results, illustrating the effectiveness and limitations of the proposed Dream-MPC method from multiple perspectives, with a clear overall presentation. Grad-MPC’s effectiveness is validated and its applicable scenarios are analyzed through targeted experiments."}, "weaknesses": {"value": "In the context of the document, the research exhibits clear merits: its experimental scope is extensive, encompassing a diverse set of continuous control tasks across multiple domains, and the results effectively validate the proposed Dream-MPC method’s performance advantages. At the same time, there are subtle areas where the presentation could be further polished. For one, the distinctions between Dream-MPC and configurations like \"policy + Grad-MPC\" are not fully and explicitly articulated, which may leave room for readers to desire more clarity on this comparative aspect. Additionally, while the abstract and conclusion position Dream-MPC as a solution to the limitations of earlier gradient-based MPC methods, the detailed content explaining how it specifically addresses these prior shortcomings is mostly contained within the appendix rather than the main text—where the narrative focus is more heavily placed on showcasing Dream-MPC as a new high-performing approach through comparisons with existing baselines. Refining these aspects slightly could help strengthen the document’s coherence in linking its innovative points to the gaps of previous methods."}, "questions": {"value": "- What are the differences between the method in this paper and the \"Policy + Grad-MPC\" approach described in Section 6 of the paper Gradient-based Planning with World Models (https://arxiv.org/pdf/2312.17227)? The latter mentions that \"The Policy+Grad-MPC method operates in a manner similar to the Grad-MPC method explained in previous sections. However, in this approach, trajectories are initialized from the output of the policy network.\" Do the innovations of this paper overlap with this description?\n\n- If the method in this paper is an improvement over Grad-MPC, it is recommended to add the definition and introduction of Grad-MPC in the \"PRELIMINARIES\" section. Then, highlight the differences between the proposed method and the original Grad-MPC in the \"METHOD\" chapter, or add a section in the main text to summarize the differences from Policy+Grad-MPC.\n\n- Figure 2: Aggregate performance metrics, why are the results of Dream-MPC (TD-MPC2) not included here, but only placed in the appendix? Overall, a high-quality policy is crucial for the method in this paper; it is recommended to emphasize this point in the main text rather than in the appendix.\n\n- Lines 416 to 424 state: \"We further evaluate the performance of fully trained TD-MPC2 agents with gradient-based MPC when varying the number of candidates, the number of optimization iterations, and the planning horizon\". However, the BMPC version is shown in the result figure. Is there a wrong image attached or a typo?\n\n- In the CONCLUSION, it is stated that \"Our empirical evaluation shows that Dream-MPC can not only outperform the baselines, but is also more robust to hyperparameters and faster compared to previously proposed gradient-based MPC methods.\" Could you please inform which table or figure illustrates the claim that Dream-MPC is \"more robust to hyperparameters compared to previously proposed gradient-based MPC methods\"? Is it Section D.2? If so, what are the differences between Grad-MPC and Dream-MPC in the experiments conducted here, and could you please explain them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGYBxn04Ss", "forum": "uhoCh3pViS", "replyto": "uhoCh3pViS", "signatures": ["ICLR.cc/2026/Conference/Submission17302/Reviewer_1niH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17302/Reviewer_1niH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080361518, "cdate": 1762080361518, "tmdate": 1762927237560, "mdate": 1762927237560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}