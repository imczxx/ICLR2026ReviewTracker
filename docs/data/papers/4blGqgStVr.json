{"id": "4blGqgStVr", "number": 15225, "cdate": 1758249104656, "mdate": 1759897320343, "content": {"title": "PDFBench: A Benchmark for De novo Protein Design from Function", "abstract": "Function-guided protein design is a crucial task with significant applications in drug discovery and enzyme engineering. However, the field lacks a unified and comprehensive evaluation framework. Current models are assessed using inconsistent and limited subsets of metrics, which prevents fair comparison and a clear understanding of the relationships between different evaluation criteria. To address this gap, we introduce PDFBench, the first comprehensive benchmark for function-guided de novo protein design. Our benchmark systematically evaluates eight state-of-the-art models on 16 metrics across two key settings: description-guided design, for which we repurpose the Mol-Instructions dataset, originally lacking quantitative benchmarking, and keyword-guided design, for which we introduce a new test set, SwissTest, created with a strict datetime cutoff to ensure data integrity. By benchmarking across a wide array of metrics and analyzing their correlations, PDFBench enables more reliable model comparisons and provides key insights to guide future research.", "tldr": "", "keywords": ["De novo Protein Design", "Evaluation Metric", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8a4a113786fa252073c98132cea9cef40cafe60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "PDFBench introduces a unified, reproducible benchmark for function-guided protein design across two settings: description-guided generation (natural-language functional descriptions) and keyword-guided generation (InterPro and/or GO terms). It evaluates eight modern systems (ProteoGAN, ESM3, CFP-Gen, Chroma, ProteinDT, PAAG, Pinal, ProDVa) on 16 metrics organized into six dimensions: Plausibility, Foldability, Language Alignment, Similarity, Novelty, and Diversity. The description-guided split repurposes Mol-Instructions to enable quantitative assessment, while the keyword-guided split is a new test set, built with a strict datetime cutoff to mitigate contamination. The paper also studies metric-metric relationships. Overall, PDFBench fills a gap by normalizing task definitions and metrics so competing model families can be compared fairly and by summarizing empirical regularities that can guide method design. Generally, I believe it is a novel and interesting benchmark in a wider range of protein design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- PDFBench consolidates multiple factors into a single framework: two canonical input regimes, six evaluation dimensions, and 16 concrete metrics that span feasibility and goal-matching. This is a long-standing comparability gap where prior works selected disjoint metric subsets and left cross-family conclusions ambiguous.\n\n-  The description-guided track systematically assesses Mol-Instructions; the keyword-guided track (SwissTest) applies a strict post-2025 annotation cutoff. The paper explicitly discusses overlap analyses and the rationale behind the splits, which strengthens confidence in leakge avoidance.\n\n- The authors quantify relationships such as lower perplexity co-occurring with higher pLDDT and lower PAE and show retrieval scores can swing dramatically under different negative-sampling strategies underscoring why metric details matter as much as the model. I think it is a very meaningful try.\n\n- On keyword-guided design, CFP-Gen, Pinal, and ProDVa frequently yield foldable sequences with strong language alignment under the chosen proxies, but show trade-offs against local repetition and novelty/diversity."}, "weaknesses": {"value": "- Novelty and diversity are sensitive to database coverage and thresholds. Sequence/structure dissimilarity against large corpora is an intuitive starting point, but the conclusions can drift with database freshness, taxonomy composition, and cutoff conventions. Because the benchmark observes that stronger alignment often coincides with higher repetition and lower novelty, it would be valuable to report the sensitivity of novelty/diversity to alternative databases and thresholds and to visualize the trade-off among alignment, foldability, and novelty. For example, in structure-based protein design, papers tend to compare the effect on multiple datasets (cath4.3, casp, pdb...even AF2)\n\n- The paper documents sizable gaps across strategies, which means absolute retrieval numbers are fragile and method rankings can change with protocol choices. A standardized retrieval protocol with fixed public seeds, paired with confidence intervals and significance tests, would reduce measurement noise and make cross-paper comparisons more robust.\n\n- Descriptions, GO terms, and InterPro entries encode function at different levels of specificity and with different structural implications. Dose-response studies that vary input constraint strength (e.g., number and granularity of keywords, cross-level GO) and measure the impact across all six dimensions would clarify how methods adapt to tight versus loose guidance.\n\n- Thresholds and binning rules need stronger statistical grounding. Practical heuristics like repetition cutoffs or high-confidence bands for structural scores improve readability but can be brittle across datasets. ROC/PR analyses, bootstrap confidence intervals, and sensitivity curves around chosen thresholds would make the findings more portable.\n\n- Computational cost and evaluation throughput are not quantified. Some metrics can be computationally expensive at the benchmark scale. Summarizing per-metric runtime and resource footprints and suggesting cost-aware evaluation recipes (for example, perplexity filtering before foldability checks) would help teams plan and reproduce studies efficiently.\n\n\n- Many tables present point estimates without variance bands over seeds or resamples, making it hard to judge whether narrow score gaps reflect real differences. As a solid benchmark, providing mean and standard deviation or confidence intervals, along with simple non-parametric tests on key comparisons, would make the takeaways sturdier. If the authors already have those results, I do encourage them to report it in the later version, but if not, I do not think it is required to conduct experiments during the rebuttal phase."}, "questions": {"value": "I have a question about the contents in Table 1, which mentions that some baseline models don’t support the same kinds of keyword inputs, so comparing them in one unified “keyword” task can be unfair: a model forced to handle an input type it doesn’t really support will look worse than it truly is. For example, in the paper’s capability table, ProteoGAN supports GO terms but not InterPro, while ESM3 supports InterPro but not GO, so in an InterPro-only setting ProteoGAN is handicapped, and in a GO-only setting ESM3 is. Maybe the fix is to clearly label each method’s capabilities in the results tables and report two numbers per method: one under its own native, fully supported pipeline (to show its true ceiling) and one under the unified PDFBench pipeline. But as I did not run the model myself, it might be not doable. Hope the authors can provide further elaborations on this issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0sN7VVPUCs", "forum": "4blGqgStVr", "replyto": "4blGqgStVr", "signatures": ["ICLR.cc/2026/Conference/Submission15225/Reviewer_gWpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15225/Reviewer_gWpu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498822953, "cdate": 1761498822953, "tmdate": 1762925521748, "mdate": 1762925521748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDFBench, a new benchmark aimed at providing a unified evaluation framework for function-guided de novo protein design. While the initiative to consolidate evaluation in this area is appreciated, it is questionable whether a text-guided approach is useful in real-world protein design. Also, the design choices of the benchmark itself are a major concern. Specifically, the metrics, comparison set, and the practical relevance of the structural results presented lead to significant concerns regarding the paper's contribution to the field. I do not currently believe that text-guided protein design offers a realistic path forward for practical protein engineering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper's primary strength is the attempt to address a clear gap in the field: the lack of a unified and comprehensive evaluation framework for function-guided (text-guided) protein design models. The systematic effort to evaluate multiple state-of-the-art models across a variety of metrics is a valuable starting point."}, "weaknesses": {"value": "The proposed benchmark suffers from several critical weaknesses that undermine its conclusions and practical relevance:\n1. The core motivation of text-guided design is questionable. Coarse-grained descriptions like Gene Ontology (GO) terms, EC numbers, or keywords are fundamentally insufficient for specifying novel, complex functions (e.g., designing a high-affinity binder for a specific, newly discovered target). This limits the real-world utility of the entire methodology.\n2. The benchmark utilizes existing models for evaluating the \"language alignment score.\" However, the accuracy of existing GO annotation algorithms is notoriously problematic, meaning the core metric used to assess the models' alignment with the text prompt is potentially based on a flawed, inaccurate standard.\n3. The benchmark significantly diverges from the state-of-the-art methods currently employed by biologists for de novo protein design. The paper doesn't include comparisons with highly successful and practically proven structure-based methods like RFDiffusion and ProteinMPNN. This makes it impossible to compare the performance of text-guided methods against realistic, successful baselines.\n4. The reported plddt values for the designed proteins are far too low. With baseline models consistently achieving scores below 80, the generated sequences lack the predicted foldability required for practical use in biology or engineering."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4CEq2mmZql", "forum": "4blGqgStVr", "replyto": "4blGqgStVr", "signatures": ["ICLR.cc/2026/Conference/Submission15225/Reviewer_ixQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15225/Reviewer_ixQU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554927478, "cdate": 1761554927478, "tmdate": 1762925521024, "mdate": 1762925521024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PDFBench set up a new benchmark for text-guided protein design task. This task went popular these years, but there is not comprehensive benchmark for this task. Authors have considered several aspects of protein-related metrics, most of which are wildly used in protein design benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Authors have considered \"Pausibility\", \"Foldability\", \"Language Alignment\", \"Novelty\" and \"Diversity\", most of which are well-established metrics and are wildly accepted in different works. The text-guided protein design is a relatevly a new task without current benchmark. The design of the benchmark is good, divide tasks into \"text-guided\" and \"keyword-guided\" is practical and useful."}, "weaknesses": {"value": "Generally, the task that using function to design protein is not well-accepted because of low controability of the design process, and using traditional pipeline to design protein using RFdiffusion and ProteinMPNN can also achieve similar task (describe function using structure, or using additional models). Add more discussion that comparing traditional workflow with the function2protein workflow is useful in this paper."}, "questions": {"value": "1. Is it possible that the protein design metrics is different in different kinds of protein functions? Like the well-studied or well-documented function can have much better performance than those rarely discovered function. Can you add a comparison in keyword guided part?\n2. Using large language model like ChatGPT to design protein can also be an interesting baseline for this task"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qc0l9EfAQs", "forum": "4blGqgStVr", "replyto": "4blGqgStVr", "signatures": ["ICLR.cc/2026/Conference/Submission15225/Reviewer_xuQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15225/Reviewer_xuQj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901877414, "cdate": 1761901877414, "tmdate": 1762925520463, "mdate": 1762925520463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PDFBench, a comprehensive benchmark for de novo protein design conditioned on function. It systematically evaluates eight representative models under both description-guided and keyword-guided settings, using metrics across six key dimensions: Plausibility, Foldability, Language Alignment, Similarity, Novelty, and Diversity.\nThe benchmark introduces two dedicated test sets and provides a unified framework for fair, reproducible comparison. The results offer deeper insights into the relationship between sequence design, structure formation, and functional alignment in generative protein modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark fills an important gap in functional protein design and provides a unified, reproducible evaluation framework.\n2. The paper is well-organized, and the rethinking section is particularly informative and insightful."}, "weaknesses": {"value": "1. The dataset and code links are currently inaccessible (“The requested file is not found”), which limits reproducibility.\n2. Many evaluation metrics overlap with those used in ProDVa, raising concerns about the degree of novelty beyond benchmark integration."}, "questions": {"value": "1. The link to the codebase is inaccessible. Could the authors verify and restore the dataset/code access links to ensure reproducibility?\n\nP.S. I also wonder whether this paper might better fit the Dataset and Benchmark Track, given its focus on standardization rather than methodological innovation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w2gqnbVbs2", "forum": "4blGqgStVr", "replyto": "4blGqgStVr", "signatures": ["ICLR.cc/2026/Conference/Submission15225/Reviewer_xTBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15225/Reviewer_xTBi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932644865, "cdate": 1761932644865, "tmdate": 1762925520108, "mdate": 1762925520108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}