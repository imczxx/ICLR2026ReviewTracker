{"id": "riWQNV1LOC", "number": 14765, "cdate": 1758243254915, "mdate": 1759897350460, "content": {"title": "Effectiveness of Local Steps on Heterogeneous Data: An Implicit Bias View", "abstract": "In distributed training of machine learning models, gradient descent with local iterative steps is a very popular method to mitigate communication burden, commonly known as Local (Stochastic) Gradient Descent (Local-(S)GD). In the interpolation regime, Local-GD can converge to zero training loss. However, with many potential solutions corresponding to zero training loss, it is not known which solution Local-GD converges to. In this work we answer this question by analyzing implicit bias of Local-GD for classification tasks with {\\em linearly separable data}. In the case of highly heterogeneous data, it has been observed empirically that local models can diverge significantly from each other (also known as \"client drift''). However, for the interpolation regime, our analysis shows that the aggregated global model resultant from Local-GD with arbitrary number of local steps converges exactly to the model that would result in if all data were in one place (centralized trained model) in direction. Our result gives the exact rate of convergence to the centralized model with respect to the number of local steps. We also obtain this same implicit bias with a learning rate independent of number of local steps with a Modified Local-GD algorithm. Our analysis provides a new view to understand why Local-GD can still work very well with a very large number of local steps even for heterogeneous data. Lastly we also discuss the extension of our results to Local SGD and non-separable data.", "tldr": "We analyze the implicit bias of Local-GD and show that the aggregated global model resultant from Local-GD with arbitrary number of local steps converges exactly to the centralized model in direction.", "keywords": ["local steps", "distributed learning", "implicit bias", "overparameterization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf2ff4329dd10cba59da8be10ed204bb52d2e13d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the implicit bias of local gradient descent for certain machine learning problems in the interpolation regime, where there are many optimal solutions with zero training loss. The paper analyzes how the number of local steps affect the convergence rate to the centralized model. Another algorithm, Modified Local-GD, is discussed, which can get the same implicit bias but with a learning rate independent of the number of local steps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper considers the implicit bias of local gradient descent for overparameterized linear regression and logistic regression with separable data. To the best of my knowledge, these results are new in the literature."}, "weaknesses": {"value": "1. I am concerned about the importance of the result. Understanding implicit bias of local GD was extensively studied in the literature and went beyond the simple linear models [r1, r2]. The authors should discuss these works and compare with them.\n\n[r1] Bao, Yajie, Michael Crawshaw, and Mingrui Liu. \"Provable benefits of local steps in heterogeneous federated learning for neural networks: A feature learning perspective.\" In Forty-first International Conference on Machine Learning. 2024.\n\n[r2] Sefidgaran, Milad, Romain Chor, Abdellatif Zaidi, and Yijun Wan. 2024. “Lessons from Generalization Error Analysis of Federated Learning: You May Communicate Less Often!” In Forty-first International Conference on Machine Learning. 2024.\n\n2. The presented results do not show any advantage of using local updates versus without local updates (i.e., minibatch SGD) [Woodworth et al. 2020]. Does the generalization decrease faster than minibatch SGD (which has the same computation and communication structure as local SGD)?\n\n3. The algorithm design and the proof of Modified Local-GD is very similar to [Evron et al. 2023], so the technical contribution is not significant."}, "questions": {"value": "See the weaknesses section. I am happy to increase my score if the authors can address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtmO1WFYBf", "forum": "riWQNV1LOC", "replyto": "riWQNV1LOC", "signatures": ["ICLR.cc/2026/Conference/Submission14765/Reviewer_p4kZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14765/Reviewer_p4kZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760916432074, "cdate": 1760916432074, "tmdate": 1762925120983, "mdate": 1762925120983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes Local GD for linear regression and logistic regression, with the goal of characterizing the implicit bias of Local GD. For overparameterized linear regression, it is proven that Local GD converges to the same solution as GD. For logistic regression, it is proven that Local GD converges in direction to the max margin solution on the global dataset, which is the same as GD, though I have some issues with the rates of convergence here (see weaknesses below). For a regularized version of logistic regression, it is proven that Local GD with a modified aggregation step converges to the maximum margin solution, under the assumption that the local regularized problem can be exactly solved at every round.\n\nNotice: I have given this paper a reject due to some important technical issues around the convergence rate of Theorem 2 (see weakness #1). If this problem is fixed and the claims around it are fixed, then I am happy to recommend acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper solves an important problem, namely to characterize the implicit bias of Local GD. This is a solid contribution to the literature in my opinion.\n2. The paper analyzes a large number of different settings and thoroughly characterizes algorithm behavior in each one."}, "weaknesses": {"value": "1. There are some significant issues with the derivation of convergence rates in Section 4 due to erroneous use of big $O$ notation. Essentially, the authors use $O$ to omit not only universal constants, but also problem-dependent parameters like margin $\\gamma$, number of clients $M$, etc. This can be fine as long as you are consistent about which parameters are omitted, but the real issue comes when $O$ omits SOME occurrences of $L$ (number of local steps) but not all of them, so that the convergence rate of $f(w_0^k) \\leq O(1/(Lk))$ from Theorem 2 is possibly inaccurate in terms of $L$. This leads to incorrect claims about the impact of local steps (lines 232 to 245). Let me elaborate:\n\n    Line 244 claims that the number of rounds can be $\\epsilon^{-a}$ for any $a \\in (0, 1)$ by choosing $L = \\Theta(\\sqrt{M} \\epsilon^{1-a})$. This claim requires a lot of clarification. First of all, the $O$ in Theorem 2 is omitting more than just universal constants. Suppose I fix $k=1$ and let $L \\rightarrow \\infty$. Then Theorem 2 implies that $f(w_0^1) \\leq O(1/L) \\rightarrow 0$, which is definitely not true. Of course, the reason this happens is that $f(w_0^k) \\leq O(1/(Lk))$ is an asymptotic rate, so it only holds for sufficiently large $k$. How large does $k$ need to be? It is unclear. And without a concrete bound on $k$, there is no finite-time result. This has an important impact on the claim of $\\epsilon^{-a}$ complexity, since the bound $f(w_0^k) \\leq \\epsilon$ may only hold for $k \\geq \\epsilon^b$ for some $b \\in (0, 1)$ which is larger than $a$. In that case, the authors claim of $\\epsilon^{-a}$ complexity is wrong.\n\n    To see where this comes from in the proof, consider the step from Equation (74) to Equation (75). The big O in Equation (75) appears to omit the coefficient $\\|\\rho^k\\|$, which is justified on the grounds that $\\|\\rho^k\\|$ is bounded (Line 1510). However, the big O here is omitting more than just universal constants: it may also be omitting dependence on $L$. Line 1510 says that $\\|\\rho^k\\|$ is bounded, but bounded by what as a function of $L$? That is not clear, and it is simply ignored by the current proof. If, for example, it was the case we only know $\\|\\rho^k\\| \\leq L$, then the RHS of Equation (75) would have to be clarified to $O(L/\\log(Lk))$, which is significantly worse than what the authors claimed. I'm not saying that this specific bound $\\|\\rho^k\\| \\leq L$ holds, just that the proof needs to keep track of the dependence on all parameters (especially $L$). You cannot use $O$ to omit some occurences of $L$ and not others.\n\n    From the proof, you can also see how this issue affects the claimed convergence rate of $f(w_0^k) \\leq O(1/(Lk))$. Consider the step from Equation (83) to the convergence rate $f(w_0^k) \\leq O(1/(Lk))$. Again, many problem dependent parameters are dropped here, such as $M$ and $|V|$, and whatever parameters $\\lVert \\rho^k \\rVert$ depends on (possibly $L$). This is especially problematic here because the dropped coefficient is potentially exponential in $\\lVert \\rho^k \\rVert$, which means that the big O here may actually be omitting a term like $\\exp(L)$, which would destroy the claim that larger $L$ leads to faster convergence.\n\n    **TLDR**: The authors use $O$ to sometimes drop the number of local steps $L$, which yields a possibly incorrect dependence on $L$ in the final convergence rate of Theorem 2. This invalidates the authors' claim of the benefit of local steps to achieve $\\epsilon^{-a}$ complexity on lines 232 to 245. I think that this error must be resolved before I can recommend acceptance. If the proof and claims are corrected, the paper is definitely worthy of publication.\n\n2. I disagree with some claims made by the authors about their contribution. Some of these claims need to be clarified in order to be accurate.\n\n    2a. Line 66 says: \"The meaning of this work lies in: 1). providing a theoretical explanation to the phenomenon that Local-GD can work well with a very large number of local steps in practice; 2). showing the local steps can benefit the convergence rate for smooth, convex functions (such as, logistic loss); this could not be derived from previous analysis in vanilla Local-GD.\" For (1), I don't think that this work really shows what is claimed; Sections 2 and 4 don't actually analyze Local GD, they analyze an idealized version in which local problems are exactly solved at every iteration, and actually the algorithms don't even have any concept of learning rate or number of local steps. In Section 3, the analysis depends on the standard condition $\\eta \\leq 1/L$, so large number of local steps only works with small learning rate, which is the same as prior work. I think that the authors missed the reference [1], which already proved that Local GD for logistic regression can converge for any learning rate and any number of local steps, without any regularization or modification to the Local GD algorithm. For (2), the claim of improvement from local steps is incorrect (see my weakness #1). I want to clarify that I still appreciate the authors' contribution of characterizing the implicit bias of Local GD.\n\n    2b. Section 4 is described as analyzing Local GD with learning rate independent of $L$. In fact, the algorithm analyzed in Section 4 has no concept of learning rate or number of local steps, so I don't think that this description really makes sense. I do think Section 4 is interesting and informative, but it is inaccurate to say that this section analyzes Local GD without the condition $\\eta \\leq 1/L$. I recommend that the authors clarify this description, for example on line 72.\n\nSome small suggestions:\n- Line 89: \"In the existing convergence analysis of Local-GD, the number of local steps L should not be very large for heterogeneous data\". I disagree with this claim. The more recent analysis (e.g. [2]) works for any number of local steps as long as $\\eta \\leq 1/L$.\n- Line 221: The conclusion $\\lVert \\rho^k \\rVert < \\infty$ is trivial. Should this say something like there is some $B$ such that $\\lVert \\rho^k \\rVert < B$ for every $k$?\n- Use \\citep{...} when you want a citation that doesn't interrupt the sentence, e.g. \"It can happen in a data center with thousands of connected compute nodes (Sergeev & Del Balso, 2018).\" instead of \"It can happen in a data center with thousands of connected compute nodes Sergeev & Del Balso, 2018.\"\n- Assumption 2: $\\limsup_{u \\rightarrow \\infty} g'(u) < 0$. Should this say $= 0$? The limit equals 0 for logistic loss.\n\n[1] Crawshaw, Michael, Blake Woodworth, and Mingrui Liu. \"Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability.\" ICML 2025.\n\n[2] Woodworth, Blake E., Kumar Kshitij Patel, and Nati Srebro. \"Minibatch vs local sgd for heterogeneous distributed learning.\" Advances in Neural Information Processing Systems 33 (2020): 6281-6292."}, "questions": {"value": "1. What happens in Theorem 1 if $\\theta_{\\min} > 2$?\n2. After fixing the dependence on $L$ in Theorem 2, can you still achieve $\\epsilon^{-a}$ communication for $a \\in (0, 1)$?\n3. For Sections 2 and 4, the analysis doesn't really consider Local GD, rather an idealized version of Local GD that exactly solves local problems at every round. Could this be extended to Local GD by accounting for the error of Local GD for solving local problems at every round?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bzt4Kvi1Dg", "forum": "riWQNV1LOC", "replyto": "riWQNV1LOC", "signatures": ["ICLR.cc/2026/Conference/Submission14765/Reviewer_5YeV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14765/Reviewer_5YeV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674416813, "cdate": 1761674416813, "tmdate": 1762925120116, "mdate": 1762925120116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the convergence of Local Gradient Descent for distributed optimization. The setting is that we have M different clients that each possess a different dataset and we want to find the minimizer of the average loss over all of these datasets together. The main question the paper asks is this: does the model learned by Local GD converge to the model that would be learned if all the datasets were centralized? The authors first motivate this question by considering linear regression, where one can show Local GD converges to the centralized model under overparameterization (Theorem 1). They then move on to binary classification with linear models, where they also assume overparameterization, and show (Theorem 2) that the loss function converges to 0 at the rate $1/(Lk)$ where L is the number of local steps and $k$ is the number of rounds, and the Local GD model converges to the centralized model at the rate $1/\\log L k$, provided the learning rate is small enough. Finally, the last result in the paper concerns a regularized variant of Local GD with/without weighted aggregation, which shows this result can be obtained even if the learning rate isn't very small."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The people is written clearly and the problem is well motivated.\n- The result under regularization is nice, but that's not really Local GD anymore it's more like FedProx. I would find it better to call that an analysis of FedProx instead."}, "weaknesses": {"value": "- There is no novelty in the linear regression section, this observation has already been made before. See for example [1], where placing $\\epsilon = 0$ and $\\sigma =0$ in their result immediately recovers this one and in fact it's more general.\n- The conclusions of the second section (on classification) are also not very novel; We already know that under overparameterization, Local GD/SGD converges at the rate O(1/Lk), see e.g. [2] or at the very least O(1/k), e.g. put $\\sigma_{\\mathrm{dif}} = 0$ in [4, Thm. 5]. It seems that overparameterization just trivializes this problem-- surely it's no surprise they all converge together if all of their data can be fit by the same model?\n- Even if we argue the iterate convergence is the most important thing here, that rate is $1/\\log Lk$! This is *not* a realistic convergence rate for any practical problem.\n\n[1] Sadchikov, Andrey, et al. \"Local SGD for near-quadratic problems: Improving convergence under unconstrained noise conditions.\" arXiv preprint arXiv:2409.10478 (2024).\n[2] Qin, Tiancheng, S. Rasoul Etesami, and Cesar A. Uribe. \"Faster convergence of local sgd for over-parameterized models.\" arXiv preprint arXiv:2201.12719 (2022).\n[3] Woodworth, Blake E., Kumar Kshitij Patel, and Nati Srebro. \"Minibatch vs local sgd for heterogeneous distributed learning.\" Advances in Neural Information Processing Systems 33 (2020): 6281-6292.\n[4] Khaled, Ahmed, Konstantin Mishchenko, and Peter Richtárik. \"Tighter theory for local SGD on identical and heterogeneous data.\" International conference on artificial intelligence and statistics. PMLR, 2020."}, "questions": {"value": "- The proof of Claim 3 is so unclear, it does not seem like this actually proves a $1/(Lk)$ rate but a (polynomially) worse one due to the big-O term in equation (83).  Can you please elaborate on this proof?\n- Please address what I wrote in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rB0EMRQlcw", "forum": "riWQNV1LOC", "replyto": "riWQNV1LOC", "signatures": ["ICLR.cc/2026/Conference/Submission14765/Reviewer_4GCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14765/Reviewer_4GCD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006733185, "cdate": 1762006733185, "tmdate": 1762925119662, "mdate": 1762925119662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the implicit bias of Local-GD in the overparameterized regime, focusing on classification tasks with linearly separable data. The work provides a theoretical explanation for the empirical observation that Local-GD can perform well with a large number of local steps (L).\nThe primary contribution (Theorem 2) demonstrates that for standard Local-GD (using a learning rate $\\eta = \\mathcal{O}(1/L)$), the aggregated global model converges *in direction* to the *exact* global max-margin solution. This bias is identical to that of centralized Gradient Descent, even with arbitrary L and data heterogeneity. The analysis shows that while the model's magnitude diverges, its direction aligns with the centralized solution.\nBuilding on this insight, the authors propose a \"Modified Local-GD\" algorithm that achieves this same implicit bias but with a learning rate that is independent of the number of local steps."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical analysis is sound and provides a new perspective on Local-GD.\n2. The writing is exceptionally clear, and the related work section is well-discussed, placing the contributions in a sharp context. The explanation of the theoretical results is thorough.\n3. The core finding that in the overparameterized regime, local steps do not alter the *direction* of the final solution (the implicit bias) is a significant and interesting result.\n4. A major practical strength, based on the theoretical results, is the design of the Modified Local-GD algorithm, which can achieve a learning rate independent of L."}, "weaknesses": {"value": "1.  A primary concern is that the main claim (\"the global model converges to the centralized model with any arbitrary number of local steps\") is asymptotic in the number of communication rounds (K). This claim, while asymptotically correct, might not fully align with practical constraints. In practice. K and L trade-off under a fixed computational budget (e.g., K*L = constant). If L increases, K should decrease. The analysis in this paper does not clarify what happens to the implicit bias in this fixed-budget regime, which is arguably the more practical scenario.\n2.  The paper relies on the overparameterized (interpolation) regime. However, it also uses the training of large language models (LLMs) as a motivation, where training is often in an *underparameterized* regime (i.e., not interpolating the whole data). The paper's introduction notes that in the underparameterized regime, a large number of local steps can hurt convergence to the centralized model. This may leave a gap: the paper's theory explains the LLM phenomenon *only if* we assume LLMs operate in the overparameterized setting (e.g., last-layer fine-tuning, as in the experiment), but not necessarily in the large-scale pre-training regime.\n3.  (Minor) The analysis is focused on Local-GD (full batch). While an extension to Local-SGD is briefly discussed (Sec 3.3), a more thorough analysis of the impact of stochastic noise on the implicit bias would strengthen the practical relevance of the findings."}, "questions": {"value": "The paper claims to hold for \"heterogeneous data,\" but Assumption 1 (global linear separability) seems to be a relatively mild condition. This assumption ensures a common global solution exists, but it doesn't capture the *degree* of heterogeneity (e.g., the severity of \"client drift\"). How does the *level* of data heterogeneity (e.g., measured by the diversity of local max-margin solutions) impact the *rate* of convergence *in direction* to the global max-margin solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s8SO4mtOSH", "forum": "riWQNV1LOC", "replyto": "riWQNV1LOC", "signatures": ["ICLR.cc/2026/Conference/Submission14765/Reviewer_yw2R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14765/Reviewer_yw2R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14765/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762409286503, "cdate": 1762409286503, "tmdate": 1762925119045, "mdate": 1762925119045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}