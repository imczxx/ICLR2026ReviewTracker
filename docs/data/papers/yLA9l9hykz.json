{"id": "yLA9l9hykz", "number": 16349, "cdate": 1758263518691, "mdate": 1759897246164, "content": {"title": "Towards Understanding Gated Linear Recurrent Neural Networks", "abstract": "Linear Recurrent Neural Networks (RNNs) have attracted attention for their memory and computational efficiency.\nIn particular, gated linear RNNs enable nonlinear transformations through gating mechanisms while still maintaining linear time complexity by removing hidden states from them.\nHowever, the impact of the gate mechanisms and such removal of hidden states from them remains unexplored.\nHere we empirically investigate the impact of these gating mechanisms and find that gate values near zero or one highly depend on hidden states, leading to unintended distribution shifts of gate values when hidden states are removed in gated linear RNNs.\nBased on our findings, we propose an algorithm to mitigate the distribution shifts, which empirically improves performance on long-sequence modeling tasks.", "tldr": "", "keywords": ["Recurrent Neural Network", "Gate mechanism"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c867377d485b434a4d01956260ae8f115aba2d7b.pdf", "supplementary_material": "/attachment/eabf0830918567b555bcceb1949f51417a439308.zip"}, "replies": [{"content": {"summary": {"value": "This paper discusses the compromise in gated linear RNNs, which remove the dependency of the gate projection on the previous hidden state to maintain linear complexity. While it's intuitive that removing this dependency leads to a performance drop, the paper provides a new perspective by analyzing the distribution shift of gate values. The authors uncover that after removing this dependency, the first layer of the model shows a significant distribution shift in its gate values, while subsequent layers exhibit a more moderate shift. Building on this, the paper introduces a trick that applies Gumbel-Softmax initialization to the first layer, forcing the model to output gate values close to either 0 or 1, thus enhancing performance. The experiments verify that this trick is effective for the synthetic recall tasks designed by the authors. However, for more complex language tasks, the trick is not as effective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates the intuitive performance drop caused by removing the gate's dependency on $h_{t-1}$. It empirically finds that this removal causes a \"distribution shift\" in the gate values. The discovery that $h_{t-1}$ is the primary driver for pushing gate values towards 0 or 1 in nonlinear RNNs is useful and inspirational.\n2. The solution of applying Gumbel-Softmax initialization only to the first layer based on the diagnosis that this is where the shift is most clear, which is a targeted approach."}, "weaknesses": {"value": "1. While the Gumbel-Softmax initialization shows outstanding performance on synthetic tasks, it demonstrates almost no improvement on the more important, complex real-world task of WikiText-103 language modeling (Table 1).\n2. All analyses and experiments are conducted on a minimal gated linear RNN or a simple 6-layer version. The paper completely lacks validation on any modern, state-of-the-art Linear RNN architecture (e.g., Mamba, RWKV). And the research can be expanded to modern architectures that adopt the matrix-formed memory (e.g., Gated Deltanet, TTT).\n3. The benchmark is relatively insufficient. Some basic widely used language tasks such PIQA, ARC are not included. This may due to that model is too small that cannot handle even a bit complex tasks. However, the results on small models may not be persuasive."}, "questions": {"value": "1. The method excels on copying tasks but shows almost no improvement on WikiText-103 language modeling . Does this imply the method fails to generalize to complex, real-world tasks?\n2. The analysis uses a minimal model. How do we know this distribution shift problem even exists in more complex SOTA architectures like Mamba or GLA, which were not tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GCOFtnovg1", "forum": "yLA9l9hykz", "replyto": "yLA9l9hykz", "signatures": ["ICLR.cc/2026/Conference/Submission16349/Reviewer_wM4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16349/Reviewer_wM4S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570248458, "cdate": 1761570248458, "tmdate": 1762926481873, "mdate": 1762926481873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the gating dynamics of gated linear RNNs, focusing on the impact of removing hidden states from the gate computation. The authors propose a bias initialization method based on the Gumbel-Softmax trick. The proposed method is evaluated on synthetic and real-world sequence modeling tasks with improvements over standard and prior initialization strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is somewhat clear and easy to follow."}, "weaknesses": {"value": "However, I think the draft has severe drawbacks:\n\n1. I think the main claim of this paper is somewhat not very convincing. The author uses synthetic tasks to show the findings, and only performs it with 3/6 layer models. I don't think that is convincing. For real-world results, the author uses the WikiText-103 dataset with 2048 sequence length, which is somewhat too short in the recent LLM era. Will the proposed findings still occurs in larger models? If the model/data got deeper/larger, will the distribution shift be solved by scaling? Since the results with the proposed method on WikiText-103 is similar to previous results, I don't think it may become a critical issue.\n\n2. The proposed method is somehow not novel. It just changes the bias initialization method with Gumbel-Softmax, which has occurred in previous methods. The change is minor, while the performance gain over previous methods is also minor."}, "questions": {"value": "I think the author should revise the draft to enhance the importance of solving so-called distribution in toy-data and toy datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RZzdgDVGg9", "forum": "yLA9l9hykz", "replyto": "yLA9l9hykz", "signatures": ["ICLR.cc/2026/Conference/Submission16349/Reviewer_vbjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16349/Reviewer_vbjC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846954959, "cdate": 1761846954959, "tmdate": 1762926481510, "mdate": 1762926481510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper observes the changes in the distribution of forget gate values in gate linear/nonlinear RNNs before and after training on two small-scale synthetic tasks, i.e., Copying and Selective Copying (Figures 1 & 2). Specifically, the distribution shifts from a relatively broad and uniform initialization to a state where most values cluster near the two extremes, 0 or 1. Based on this, the paper proposes using the Gumbel-Softmax trick to initialize the gate values to distributions closer to these two extremes. Experimental results on different gate initialization methods show that the proposed initialization approach can accelerate performance improvement and convergence speed for the copying/selective copying tasks (Figures 4 & 6). The paper also compares the language modeling perplexity loss of different gate initialization methods on the WikiText-103 dataset (Table 1), and conducts hyperparameter fine-tuning experiments for the proposed initialization method (Figure 8)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The authors observe that in the two small-scale synthetic tasks, Copying and Selective Copying, the forget gate values in gated linear/nonlinear RNN models tend to cluster near the extreme values of 0 and 1 after training, and this phenomenon is analyzed and explored to some extent.\n\n(2) This paper proposes a method for initializing the forget gates using the Gumbel-Softmax trick, which can accelerate both the performance improvement and convergence speed of the models on these two small-scale synthetic tasks."}, "weaknesses": {"value": "**(1) The core contribution of this paper may lack generalizability to real-world larger-scale (with parameter size not less than 1B) pre-training and multi-task reasoning scenarios.** \n\nThe gate value distributions observed only from the Copying/Selective Copying tasks may differ significantly from those in large-scale, general language modeling settings in real-world applications. As seen in Figures 1 and 2, there is a substantial proportion of forget gates close to 1 (corresponding to input gates close to 0) after training. I believe this is because in the copying/selective copying tasks, only the segments that need to be copied require memorization, while all other segments are treated as noise and can be largely forgotten. However, in practical applications, the model not only performs retrieval operations but also needs to form summarizing memories of factual information from a large number of segments. Therefore, the extreme situation where the forget gate is close to 1 and the input gate is close to 0 should not be as common as observed in copying/selective copying tasks.  \n\n**(2) Purely from the perspective of controlling gate value distribution, the initialization method proposed in this paper is not very meaningful in deep networks (e.g., 24 layers or more).** \n\nIn shallow networks (such as the 3-layer and 6-layer models discussed in this paper), the token mixing range that is achieved by gate values close to 0 or 1 could, in deeper networks, be equivalently modeled by progressively increasing the lower bound of the forget gate across layers. Mathematically, this approach (which was proposed in the HGRN paper[2]) should be able to fit the same effect.  \n\n**(3) The experimental evaluation is apparently not comprehensive enough.** \n\nRecent studies (for example, Table 1 in the DeltaNet paper [1]) have conducted comprehensive evaluations of model performance at scales such as 340M/1.3B, across more than a dozen commonsense reasoning and retrieval tasks. The persuasiveness of these results is much higher than the language modeling experiments in this paper, which are limited to Wikitext-103.  \n\nMinor weaknesses: \n\n(1) The phrase \"removing the hidden states\" may cause ambiguity; it might be better to use \"decay\" or \"forget\" instead.\n\nReferences:  \n[1] https://arxiv.org/pdf/2406.06484  \n[2] https://arxiv.org/abs/2311.04823"}, "questions": {"value": "My questions have been presented in the WEAKNESSES section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gh0gsjxsnC", "forum": "yLA9l9hykz", "replyto": "yLA9l9hykz", "signatures": ["ICLR.cc/2026/Conference/Submission16349/Reviewer_eQho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16349/Reviewer_eQho"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923904441, "cdate": 1761923904441, "tmdate": 1762926481092, "mdate": 1762926481092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}