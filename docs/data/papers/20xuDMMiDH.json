{"id": "20xuDMMiDH", "number": 3229, "cdate": 1757382700568, "mdate": 1759898101054, "content": {"title": "On the Universality of Self-Supervised Learning", "abstract": "In this paper, we investigate what constitutes a good representation or model in self-supervised learning (SSL). We argue that a good representation should exhibit universality, characterized by three essential properties: discriminability, generalizability, and transferability. While these capabilities are implicitly desired in most SSL frameworks, existing methods lack an explicit modeling of universality, and its theoretical foundations remain underexplored. To address these gaps, we propose General SSL (GeSSL), a novel framework that explicitly models universality from three complementary dimensions: the optimization objective, the parameter update mechanism, and the learning paradigm. GeSSL integrates a bi-level optimization structure that jointly models task-specific adaptation and cross-task consistency, thereby capturing all three aspects of universality within a unified SSL objective. Furthermore, we derive a theoretical generalization bound, ensuring that the optimization process of GeSSL consistently leads to representations that generalize well to unseen tasks. Empirical results on multiple benchmark datasets demonstrate that GeSSL consistently achieves superior performance across diverse downstream tasks, validating its effectiveness.", "tldr": "", "keywords": ["Self-Supervised Learning", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9263d603602c7d65a2b6e48747b95d144944fa65.pdf", "supplementary_material": "/attachment/d4497773ce01d9391aeacc678aba48985ea43520.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes General SSL (GeSSL) which integrates a bi-level optimization structure for self-supervised learning. The authors conduct experiments on various benchmarks using different SSL methods. Experimental results show that the proposed GeSSL method achieves improvements when integrated with different SSL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-presented and easy to follow. \n\n2. The authors also provide theoretical analysis. \n\n3. The authors selected multiple different SSL methods as baselines, and improvements can be observed across all of them with the proposed GeSSL."}, "weaknesses": {"value": "1. The results in Table 1 are not consistent with the original papers. For instance, In the original paper, Barlow Twins achieves Top-1 and Top-5 accuracy of 73.2% and 91% respectively on ImageNet using ResNet-50. However, in Table 1 of this paper, the Top-1 accuracy of Barlow Twins is 69.94%, and with GeSSL it reaches 72.84%. These results are significantly lower than those reported in the original paper.\n2. Additional results are needed to demonstrate the necessity of bi-level optimization.\n3. Lack definition of $f$, $f'$ and $g$. \n4. line 204: $L_{disc}(g, X)$ should be $L_{disc}(f, g, X)$?"}, "questions": {"value": "Please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QWCdAC9cck", "forum": "20xuDMMiDH", "replyto": "20xuDMMiDH", "signatures": ["ICLR.cc/2026/Conference/Submission3229/Reviewer_gP8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3229/Reviewer_gP8E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376191224, "cdate": 1761376191224, "tmdate": 1762916613825, "mdate": 1762916613825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new framework for improving universality in self-supervised learning (SSL). \nThe authors argue that a good representation should exhibit discriminability, generalizability, and transferability. \nTo achieve this, they propose General Self-Supervised Learning (GeSSL) — a unified framework that explicitly models these three dimensions through bi-level optimization integrating task-specific adaptation and cross-task consistency. Theoretical analysis provides a generalization bound, while extensive experiments on diverse benchmarks (ImageNet, COCO, VOC, miniImageNet, etc.) demonstrate significant performance gains over SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposed the unified concept of “universality,” systematically integrating discriminability, generalizability, and transferability\n2. Implemented comprehensive validation across unsupervised, semi-supervised, transfer, and few-shot learning tasks.\n3. GeSSL has strong adaptability by integrating with existing SSL paradigms (SimCLR, MoCo, BYOL, MAE, etc.)"}, "weaknesses": {"value": "1. Lack of the quantified metric to the defination of what is actually the \"Universality\"\n2. Experiments are mainly on vision domain, lacking validation on multimodal or language-based SSL tasks. \n3. Limited direct comparison with recent meta-SSL frameworks such as MCR [1]. From my poingt of view, MCR has excatly similar structure with this work. What is the technical improvement of this work compared to MCR?\n\n\n[1] Guo, Huijie, et al. \"Self-supervised representation learning with meta comprehensive regularization.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 3. 2024."}, "questions": {"value": "Q1: How can “universality” be quantitatively measured or benchmarked across models?\n\nQ2: Would the GeSSL structure generalize effectively to multimodal or temporal tasks?\n\nQ3: What is the most technical improvement of this work compared to MCR? Compared to MCR or other meta-regularization methods, is GeSSL more prone to local minima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lalueKBavS", "forum": "20xuDMMiDH", "replyto": "20xuDMMiDH", "signatures": ["ICLR.cc/2026/Conference/Submission3229/Reviewer_2Hsd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3229/Reviewer_2Hsd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557876481, "cdate": 1761557876481, "tmdate": 1762916613500, "mdate": 1762916613500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of universality in self-supervised learning (SSL) as a unifying criterion for good representations, defined by three properties: discriminability, generalizability, and transferability. To explicitly model these qualities, the authors propose a framework called General SSL (GeSSL) which incorporates a bi-level (two-tier) optimization approach inspired by meta-learning (MAML) to simulate training and testing within each mini-batch episode. GeSSL adds an alignment-based discriminative loss to improve class separation, uses a bi-level “update-then-evaluate” mechanism on split support/query sets to directly model generalization. This unified objective is designed to produce “universal” representations that perform well on the training data, generalize to unseen in-distribution data, and transfer to new tasks. The paper provides a theoretical generalization bound showing that under certain smoothness and boundedness assumptions, GeSSL’s training yields representations with bounded error on novel tasks. Empirically, GeSSL is evaluated by integrating it with various SSL methods (e.g. contrastive, distillation, and generative paradigms) and tested on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper clearly defines what constitutes a good SSL representation (discriminability, generalizability, transferability) and consolidates these into a single goal. This brings conceptual clarity and a new perspective to SSL, shifting focus from “which tricks yield good features” to “what properties should good features have”. By explicitly modeling these properties in the loss, the approach directly targets the end-goal of SSL rather than relying on indirect proxies.\n\nS2. The proposed GeSSL framework is a principled integration of meta-learning techniques with SSL. By employing a bi-level optimization (inner support set update, outer query set evaluation) and episodic training, GeSSL explicitly simulates the train-test process within SSL training. This is reminiscent of meta-learning for few-shot learning but effectively applied in a fully self-supervised context. This design is novel in SSL and provides a unified way to enforce that learned features not only fit the training data but also generalize to new data/tasks. The idea of using an episodic bi-level objective to improve unsupervised representation learning is an innovative contribution, distinguishing GeSSL from standard SSL methods that optimize a single-level loss.\n\nS3. The paper includes a theoretical generalization bound for the learned representation. Providing such a generalization guarantee is a strength, as many SSL works remain empirical. The authors prove (under certain regularity conditions) that GeSSL’s learning objective leads to a bounded generalization error on new tasks. This result lends credence to the claim that GeSSL can produce representations that transfer well, and it grounds the method in learning theory. Having a theory-backed discussion of why the method should work (beyond intuitive arguments) is commendable and relatively rare in SSL literature.\n\nS4. The experimental evaluation is broad in scope. GeSSL is shown to be compatible with multiple SSL paradigms – the paper augments several diverse methods (e.g. contrastive methods like SimCLR, knowledge-distillation methods like BYOL, redundancy-reduction methods like Barlow Twins, etc.) with the GeSSL components and demonstrates consistent performance gains. For example, models such as SimCLR+GeSSL or BYOL+GeSSL outperform their vanilla counterparts on image classification benchmarks, indicating the framework’s versatility."}, "weaknesses": {"value": "W1- Theoretical Analysis for Multi-Paradigm Generality: The paper should include a deeper analysis or explanation of why GeSSL can integrate with different SSL losses. My suggestion is to formalize the idea that each mini-batch forms a pseudo-task where one view acts as a “class prototype” (as hinted in the paper’s discussion of anchors and clustering). If this holds, one could argue that contrastive, distillation, and generative SSL all enforce some form of pairwise consistency that GeSSL leverages by treating each pair as a class/task. Making this argument explicit would help readers understand the compatibility.\n\nW2 - Benchmark on Distribution Shifts: To address the first weakness, the authors should evaluate GeSSL-pretrained models on dedicated robustness benchmarks. For example, running experiments on ImageNet-C [a] (common corruptions) will show how the universal representations handle noise, blur, etc., and testing on ImageNet-PD [b] (perspective distortion) will assess robustness to geometric changes.\n[a] - Hendrycks, Dan, and Thomas Dietterich. \"Benchmarking neural network robustness to common corruptions and perturbations.\" arXiv preprint arXiv:1903.12261 (2019).\n[b] - Chhipa, P. C., Chippa, M. S., De, K., Saini, R., Liwicki, M., & Shah, M. (2024, September). Möbius transform for mitigating perspective distortions in representation learning. In European Conference on Computer Vision (pp. 345-363).\n\nW3 -  Related work and comparisons. The paper is close in spirit to unsupervised meta-learning and episodic SSL; these are not positioned or compared. Include discussion and, if feasible, a baseline drawn from unsupervised MAML-style training. Also connect to robustness literature when claiming generalizability, and compare to strong non-GeSSL SSL baselines on the same downstream tasks to contextualize absolute performance.\n\nW4 - Practical overhead and stability not characterized. Bi-level training adds inner-loop computation and sensitivity to step sizes, number of inner steps, and support/query split. The paper should report training cost, memory footprint, and stability measures, plus ablations on inner-loop depth and first- vs second-order updates, so practitioners can judge feasibility at scale.\n\nMinor points - \n\n1. Give a one-figure overview: data flow, inner update, outer update, and where each loss is applied. Caption should explain every symbol used in the figure.\n\n2. Add a small “Notation and terms” table early (one column for term, one for meaning). Keep it to 10–12 entries max.\n\n3. Put the main theorem as a boxed statement with a one-paragraph proof sketch. Move long proofs to appendix with clear pointers.\n\n4. Consolidate training details in one place: datasets, image size, augmentations, batch size, optimizer, schedules, inner steps, outer steps. Include compute (GPU type, hours).\n5. I don't see source code repo as of now."}, "questions": {"value": "Weakness section includes the questions part as well. I am open raise my score upon getting satisfactory response on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QSI3PmsAX3", "forum": "20xuDMMiDH", "replyto": "20xuDMMiDH", "signatures": ["ICLR.cc/2026/Conference/Submission3229/Reviewer_z4r6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3229/Reviewer_z4r6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648693235, "cdate": 1761648693235, "tmdate": 1762916613081, "mdate": 1762916613081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates what properties a good representation should posess, for the purpose of designing a principled Self-supervised Learning (SSL) from first principles. The authors define the concept of Universality, consisting of discriminability (good training set performance), generalizability (good performance on the test set of the given task), and transferability (good performance on other tasks). \nThe authors then introduce “General SSL” (GeSSL) - a framework which models Unversality via a bi-level optimization scheme. The authors provide theoretical performance guarantees of GeSSL, as well as benchmark their approach in a number of downstream tasks, showcasing improved performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Developing SSL approaches from first principles is a crucial and ambitious research direction.\n2. The proposed bi-level optimization scheme is an intriguing and compelling approach to modeling generalizability. \n3. The proposed GeSSL approach possesses strong theoretical foundations. \n4. GeSSL improves the performance of the respective methods it is applied to, while reducing the training time.\n5. The breadth of the experimental evaluations is impressive."}, "weaknesses": {"value": "**Major**\n\n1. The paper applies episodic sampling within a single dataset, with downstream tasks drawn from the same domain. This does not validate cross-task generalization in a meaningful sense, especially compared to standard meta-learning or multi-dataset pretraining studies. The core claim that the method models Transferability remains substantially unproven under the current experimental design.\n2. The formulation still depends on conventional design choices (e.g., L_ssl, L_disc), which were originally justified empirically. Thus, the objective is not fully derived from the proposed theoretical principles.\n3. The statement “learning process within a single mini-batch can be viewed as performing a classification task” (L 120) is an overclaim, in my opinion. This is true for contrastive SSL (MoCo, SimCLR), but not necessarily for self-distillation (BYOL, SimSiam) or generative methods (like MAE) which do not rely on negative samples and are more akin to regression. The authors should correct this claim, or provide evidence or literature which supports it for generative and self-distillation methods.\n5. The experimental section lacks the evaluation with Vision Transformers, which are the most common backbones for SSL nowadays. The authors should provide the results of methods such as DINO and MAE with ViT-B (which is similar in size to ResNet-50).\n\n**Minor**\n\n1. References need a careful revision. For example: Liu et al. 2022a/b, Lin et. al. 2014a/b, Finn et al. 2017a/b, Deng at al 2009a/b seem to point at the same papers.\n2. I believe the MAE method is mis-cited - the correct citation is [1], whereas the paper cites [2].\n3. The readability of the paper would be greatly improved, if the citations were in brackets (\\citep{} command), instead of directly inline.\n4. The readability of the tables would be improved and the advantages of GeSSL better pronounced if the authors compared respective methods with / without GeSSL, instead of grouping different methods together.\n5. The writing could be tightened - Discriminability, Generalizability and Transferability are explained several times in the manuscript, including at least twice in the introduction (L060 & L066).\n\n[1] Masked Autoencoders Are Scalable Vision Learners Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick https://arxiv.org/abs/2111.06377\n\n[2] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 594–604, 2022."}, "questions": {"value": "1. L_disc is interesting on its own as a concept. Can we see an ablation study of simply training models with L_disc (without bi-level optimization) to better understand its effect?\n2. The authors claim that GeSSL “differs fundamentally from approaches that directly transplant meta-learning paradigms into SSL”. Could the authors should provide some examples of such approaches and explain the differences?\n3. Given that the main experiments are conducted with ResNet-50 (and other convolutional nets), how did the authors adapted MAE to that backbone? MAE strongly relies on its backbone being a ViT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y4ZhF5Q6NP", "forum": "20xuDMMiDH", "replyto": "20xuDMMiDH", "signatures": ["ICLR.cc/2026/Conference/Submission3229/Reviewer_2MiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3229/Reviewer_2MiX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687928539, "cdate": 1761687928539, "tmdate": 1762916611922, "mdate": 1762916611922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the lack of an explicit definition and direct modeling of what constitutes a good representation for self-supervised learning. \nThe authors define \"Universality\" by three key properties: discriminability, generalizability, and transferability, and propose GeSSL (General SSL). GeSSL explicitly models these properties through a bi-level optimization mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The universality of SSL is theoretically defined, including distinguishability, generalization, and portability.\n2. The idea behind GeSSL, which models generality through a two-layer learning paradigm, is interesting.\n3. Theoretical and empirical evaluations on benchmark datasets demonstrate the advantages of GeSSL."}, "weaknesses": {"value": "1. The baselines compared in this paper mainly focus on classic or state-of-the-art (SOTA) models from 2020, 2021, and 2022, while there are fewer models from 2023 and later, which cannot accurately reflect the true situation of the latest models.\n2. The computational efficiency is relatively high, which might hinder the necessity of the proposed method."}, "questions": {"value": "1. Please check the weakness 1. The comparison over recently-published methods is required. Besides, the paper demonstrates the effectiveness of the model through numerous experiments, but the comparisons are all based on the SOTA framework, without comparing it with other improvements to the SOTA framework.\n2. The GeSSL aims to optimize the objective through complex mechanisms. If I can directly use a pre-trained framework of a large model as my backbone, what is the necessity of using this model? Besides, the model's framework uses complex mechanisms to optimize the objective and has good data efficiency, but its computational complexity is high. \n3. The model was primarily pre-trained on the ImageNet-1K dataset, and also independently pre-trained on some other smaller datasets, but no pre-training experiments were conducted on other larger datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2fzcBDZTb", "forum": "20xuDMMiDH", "replyto": "20xuDMMiDH", "signatures": ["ICLR.cc/2026/Conference/Submission3229/Reviewer_ezAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3229/Reviewer_ezAa"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3229/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988592439, "cdate": 1761988592439, "tmdate": 1762916611211, "mdate": 1762916611211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}