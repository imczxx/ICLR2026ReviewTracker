{"id": "bVyCzNDNX8", "number": 21958, "cdate": 1758324094340, "mdate": 1759896893882, "content": {"title": "MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models", "abstract": "We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) $\\textit{Traditional MoE}$, which uses a single router per transformer block to select experts, (ii) $\\textit{BTX}$ (Branch-Train-miX), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) $\\textit{BTS}$ (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.\nThe library is accessible at: $\\textit{Link will be provided upon acceptance}$.", "tldr": "", "keywords": ["Mixture of Experts", "Large Language Models", "Model Merging", "Natural Language Processing", "Deep Learning", "Artificial Intelligence"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7eeabca0419e2bd1e11385750f74d223f2fa272.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents MixtureKit, an open-source (not open-sourced yet, and authors claim that they will release the code upon acceptance) framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. This codebase support not only traditional MoE but also some advanced versions like BTX and BTS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) MoE is widely used in most frontier open-sourced LLMs. It is important to have good codebase/infra to support MoE training and inference.\n2) This codebase seems easy to use. Based on the examples shown in the submission, it seems easy and fast to setup the experiments and visualizations."}, "weaknesses": {"value": "1) I'm not very sure about how useful it is to support the BTX adn BTS model architectures. To my best knowledge, there are very few frontier models using these designs.\n2) There seems no further research-focused efficiency optimization for the MoE models in this codebase, although author also mentioned the expensive cost of MoE models is the pain point. \n3) I understand training MoE and doing visualization is good. But there are many codebases can to some extent achieve this already. \n4) It is not clear how to integrate this codebase with other LLM training codebases easily. For example, if we are using megatron to train an MoE model, how can we use this codebase's feature?\n5) If this codebase can not be combined with existing codebase easily, how is the training/inference efficiency of the MoE implementation. Can it beat megatron or some other MoE-based LLM training frameworks?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vWf3p2FUiq", "forum": "bVyCzNDNX8", "replyto": "bVyCzNDNX8", "signatures": ["ICLR.cc/2026/Conference/Submission21958/Reviewer_fZkE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21958/Reviewer_fZkE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777765725, "cdate": 1761777765725, "tmdate": 1762941998474, "mdate": 1762941998474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper **\"MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models\"** introduces an open-source framework that enables users to easily build, combine, and interpret Mixture-of-Experts (MoE) architectures. Instead of pretraining large MoE models from scratch, MixtureKit allows the reuse of pretrained or fine-tuned models (experts) and merges them into a unified model through three methods: **Traditional MoE**, **Branch-Train-miX (BTX)**, and **Branch-Train-Stitch (BTS)**. The system automatically patches model configurations, integrates load balancing and gating mechanisms, and offers a visualization interface for analyzing token routing and expert utilization.\n\nA practical case study demonstrates MixtureKit’s efficiency on **Egyptian Arabic**, combining Arabic- and Latin-script experts into a multilingual MoE that achieves competitive or superior results to dense baselines. The framework proves flexible, modular, and extensible, supporting future MoE strategies and integration with inference systems like vLLM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**Originality**\n\nThe paper presents an open-source toolkit, **MixtureKit**, for composing and training Mixture-of-Experts (MoE) models from existing pretrained or fine-tuned checkpoints.  \n\n\n**Quality**\n- The implementation appears **technically sound and modular**, providing a clear merging and patching pipeline compatible with the HuggingFace ecosystem.  \n- The inclusion of a **visualization interface** for analyzing token routing and expert utilization adds usability value.  \n\nHowever, there is no benchmarking against **state-of-the-art MoE systems** (e.g., Mixtral, DeepSeekMoE, Gemini-MoE), making it difficult to assess the real quality of the approach.\n\n**Clarity**\n- The writing is **easy to follow**, with clear explanations of each supported method and step-by-step workflow examples.  \n- Figures and configuration examples improve readability, and the paper succeeds in documenting how MixtureKit works in practice.  \n\n\n**Significance**\n- As an **engineering contribution**, the framework may be useful to practitioners who wish to experiment with MoE model recycling or visualization.  \n\n\n**Overall Assessment:**  \nThe paper demonstrates solid engineering effort and contributes a usable open-source tool for MoE experimentation. Yet, it offers little in terms of conceptual or empirical advancement. Its contribution aligns more with a *software release paper* rather than the type of **theoretical or methodological innovation** typically expected at ICLR."}, "weaknesses": {"value": "1. **The motivation lacks depth and practical justification.**  \n   The paper positions MixtureKit as a “general framework for composing and training MoE models,” but fails to explain why such a framework is necessary beyond existing open-source systems (e.g., DeepSpeed-MoE, FairScale, Megatron-LM). There is no demonstrated bottleneck or pain point that MixtureKit specifically solves. As a result, the motivation appears weak and insufficiently grounded in practical or scientific needs.\n\n2. **The contribution is primarily engineering integration rather than scientific innovation.**  \n   MixtureKit mainly combines existing MoE routing variants (Traditional, BTX, BTS) into a unified API and configuration structure. While the integration is convenient, it does not propose any new algorithmic insight, theoretical formulation, or training paradigm. The framework reuses well-established components (routers, experts, stitching layers) without extending their capabilities, leading to minimal conceptual increment.\n\n3. **Experimental validation is extremely limited and unconvincing.**  \n   The evaluation focuses only on small-scale multilingual code-switching tasks (e.g., Arabic-Latin), without quantitative comparisons to stronger baselines such as Mixtral, DeepSeekMoE, or Switch Transformers. No large-scale or cross-domain results are reported, and there is no measurement of computational efficiency, scalability, or convergence stability. Consequently, the claims of generality and performance improvement are not empirically substantiated.\n\n4. **The framework introduces non-trivial engineering cost with limited benefit.**  \n   The process of merging multiple pretrained experts into an MoE model requires substantial training and parameter alignment overhead. The paper does not discuss the additional compute cost or compatibility issues (e.g., tokenizer mismatch, architecture heterogeneity). Given the small gains shown in experiments, the practical benefit of using MixtureKit over existing fine-tuning methods appears marginal.\n\n5. **Lack of comparison with existing model-merging and modular training frameworks.**  \n   Several recent frameworks—such as AdapterFusion, LoRAHub, and ModelSoup—already support model composition and knowledge integration. The paper fails to situate MixtureKit within this broader context, nor does it show advantages in efficiency, extensibility, or interpretability compared to these systems. Without such positioning, its originality and contribution to the field remain unclear.\n\n6. **Evaluation metrics and visualization claims are superficial.**  \n   Although the paper highlights MixtureKit’s visualization interface, the provided analyses are descriptive rather than analytical. There are no quantitative insights into routing entropy, expert utilization balance, or load distribution variance. The visualization results serve as demonstrations, not as scientific evidence supporting the framework’s design."}, "questions": {"value": "> I encourage the authors to thoroughly address the weaknesses and questions raised in this review. If the authors can provide detailed explanations and in-depth clarifications during the rebuttal, and if the revised version demonstrates substantial progress in both clarity and improvement, I will be willing to reassess the manuscript and **adjust my overall rating** accordingly, based on the quality and depth of the revision.\n\n---\n\n1. **On the motivation and positioning of MixtureKit:**  \n   - The paper claims to address a gap in the accessibility and flexibility of MoE research, but it remains unclear what concrete limitation in current MoE toolchains (e.g., DeepSpeed-MoE, Megatron-LM, Tutel, FairScale) MixtureKit resolves.  \n   - Could the authors provide a clear comparison table or discussion showing (a) which functionalities are missing in prior systems, and (b) how MixtureKit explicitly fills those gaps?  \n   - Is there any specific use case (e.g., domain adaptation, cross-lingual MoE composition, or model interpretability) where MixtureKit enables something *previously impossible or significantly easier*?  \n   - Without such clarification, the contribution risks appearing as an engineering duplication rather than a research advancement.\n\n2. **On the claimed novelty and framework design:**  \n   - The integration of Traditional, BTX, and BTS routing strategies is technically convenient, but conceptually derivative. Are there any new routing mechanisms or training procedures introduced?  \n   - How does MixtureKit ensure stable optimization when combining heterogeneous experts (with different data distributions or pretraining domains)?  \n   - Are there novel algorithmic elements—such as adaptive router regularization, inter-expert consistency constraints, or new stitching layer formulations—that go beyond existing MoE literature?  \n   - If not, could the authors justify why this framework constitutes a scientific contribution rather than a software engineering release?\n\n3. **On experimental design and empirical adequacy:**  \n   - The experiments focus solely on small multilingual code-switched datasets (Arabic–Latin). Why were these chosen as the main testbed, and how do they generalize to other domains such as programming or medical text?  \n   - The paper lacks a systematic analysis of model behavior (e.g., expert utilization rates, token routing distribution, or convergence stability). Could the authors include quantitative visualizations of these dynamics?  \n   - Can MixtureKit reproduce or surpass established MoE baselines like Mixtral, DeepSeekMoE, or Switch Transformers under equivalent compute budgets?  \n   - Without strong empirical grounding, how should readers interpret the claimed generality and performance improvements?\n\n4. **On computational cost, scalability, and resource implications:**  \n   - Merging multiple pretrained experts into a single MoE model can introduce parameter redundancy and routing overhead. What is the actual compute footprint (FLOPs, GPU-hours, or memory consumption) compared to dense or LoRA-based fine-tuning?  \n   - How does MixtureKit scale when combining 10+ experts or larger backbones (e.g., 13B–70B models)? Are there known performance bottlenecks in the current implementation (e.g., routing latency, checkpoint merging time)?  \n   - Does the framework support partial expert loading or lazy routing to reduce inference cost?  \n   - Have the authors evaluated how the mixture composition affects downstream latency or deployment efficiency?\n\n5. **On comparison with other model-merging and modular adaptation frameworks:**  \n   - Several recent systems—such as AdapterFusion, ModelSoup, LoRAHub, and MergeKit—already provide mechanisms for checkpoint merging and modular fine-tuning.  \n   - Could the authors clarify how MixtureKit differs in architecture abstraction, efficiency, and extensibility?  \n   - In particular, what advantages does MixtureKit offer in cases involving *heterogeneous* model architectures or tokenizers, where existing systems already struggle?  \n   - A formal benchmarking comparison or ablation (e.g., merging time, performance gain per parameter) would strengthen the claim of contribution.\n\n6. **On visualization and interpretability of expert behavior:**  \n   - The visualization interface is presented as a major feature, yet the paper only provides qualitative screenshots rather than analytical evidence.  \n   - Could the authors provide quantitative interpretability metrics—such as routing entropy, expert sparsity ratio, or per-layer specialization index—to substantiate the claim that MixtureKit helps diagnose model behavior?  \n   - Are these visual tools integrated into the training pipeline (e.g., for adaptive routing adjustments), or are they purely post-hoc inspection utilities?  \n   - How reproducible and generalizable are these visualization outputs across models of different architectures or token vocabularies?\n\n7. **On future development and community impact:**  \n   - The authors mention ongoing support for more model versions and open-source collaboration. What governance or maintenance plan exists to ensure MixtureKit remains compatible with rapidly evolving LLM ecosystems (e.g., Llama 3, Mistral, Gemma)?  \n   - How can other researchers extend or plug in new router designs or expert composition methods into MixtureKit without modifying the source code?  \n   - Finally, does MixtureKit aim to serve primarily as a **research tool (for probing MoE behavior)** or as a **production framework (for model deployment and merging)**? The intended scope is currently ambiguous."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4BzX0fEjJW", "forum": "bVyCzNDNX8", "replyto": "bVyCzNDNX8", "signatures": ["ICLR.cc/2026/Conference/Submission21958/Reviewer_npR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21958/Reviewer_npR5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847278951, "cdate": 1761847278951, "tmdate": 1762941998164, "mdate": 1762941998164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a toolkit that allows researchers to use pre-trained Mixture-of-Expert (MoE) models and combine them together for stronger systems. Overall, this seems to be an interesting tool that might help people in the community. The paper has a good background and related works section. It is an engineering heavy paper that has limited experimentation – though would be perfectly appropriate as a system description or demo paper. There are significant novel advancements that this paper makes, but they are less empirical and more engineering (which is not necessarily a negative)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This appears to be a potentially very useful toolkit. Being able to use pre-trained experts and combine them in a much broader way than the current literature is very appealing."}, "weaknesses": {"value": "I am not sure that the proposed toolkit actually accomplishes what it says. A bit more experimentation could be useful. For instance, in Table 1 the authors try to reproduce another paper using their method, but it is unclear to me whether or not the actually ever do. It would be nice to see this done, and perhaps one more paper as well using a different method and part of their toolkit to show it actually does everything they claim. This is not a paper that needs to beat SOTA results, but only replicate them - so experiments should be straightforward to run if the toolkit is as easy as they claim."}, "questions": {"value": "This might be lack of awareness on my part, but are many MoE methods actually “experts” in a specific domain, or is it more of an abstraction where we do not know how a particular expert is getting a vector routed to it? For instance, in Figure 3, how common is it to have an actual Arabizi vs Arabic script expert? You mention routing collapse, but would that be a problem for something in different scripts like this?\n\nIn table 1, you try to replicate a paper and show a lot of results. However, it is unclear to me if you actually do replicate it. Do you? Or if not, how close are they? I’m not sure what row is your implementation and which row is the paper you are trying to reproduce."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WNyOFcVuq1", "forum": "bVyCzNDNX8", "replyto": "bVyCzNDNX8", "signatures": ["ICLR.cc/2026/Conference/Submission21958/Reviewer_W5GP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21958/Reviewer_W5GP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762280784637, "cdate": 1762280784637, "tmdate": 1762941997819, "mdate": 1762941997819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mixture of Expert Models (MoEs) have shown significant promise, especially in recent years as it seems many large industry research organizations have moved towards MoE architectures to maximize total parameters while keeping active parameters (and therefore compute budgets) low. However, MoEs are complex to implement, generally requiring significant engineering investment. Though modern MoE LLMs have been regularly studied in larger research organizations, there is yet to be a single unified open-source library enabling independent researchers to investigate the range of MoE architectures with ease. \nThe primary contribution of this paper is a software library dedicated to training of Mixture of Experts (MoE) models with a variety of architecture options (vanilla MoE, BTX, BTS, etc) and additional visualization features to aid in analysis. The software enables easy usage of checkpoints pretrained from different libraries, merged as experts into a single model. The authors additionally include an example with experiments that they conduct in their framework, which comprise an implementation of Arabic and Latin script experts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The difficulty of training MoE models has been a longstanding problem, and there is indeed a need for a library to simplify the MoE research process. The software, as described by the authors, seems to have a fairly simple and intuitive interface, created to be compatible with the HuggingFace library of models, and includes useful routing visualizations to support research."}, "weaknesses": {"value": "In general, it's difficult to assess the strength of a software contribution. As a reviewer, it's impossible to know whether the implementation is truly as usable or intuitive as the authors describe. In line 381, the authors say \"[t]his tool has proven valuable for…\" but we do not receive any details about these scenarios where the tool has proven valuable\nWhile the paper includes one set of experiments from the authors, it lacks any other point of comparison to support its implementation quality. An example of such a comparison would be a reimplementation of a subset of the exact experiments in the original BTX paper, BTS paper, or other notable papers using similar methods, demonstrating the MixtureKit reimplementation's evaluation results, as well as the configs, and any other code a user would need to write in MixtureKit to generate this reimplementation. As the paper is right now,, a reviewer can't assess whether MixtureKit is truly usable.\nThe authors call the implementation in S4 a \"reimplementation.\" However, their results are exactly those of the paper they claim to reimplement, and it seems as though this is not a reimplementation from scratch, but rather the original implementation, or an implementation heavily informed by insider knowledge from the original authors, and thus does not qualify as evidence of the usability and accuracy of MixtureKit. \nThere is also a general lack of information about details in usage. Though the authors include one example config, there is no comprehensive list of available options.\nThe authors also say that \"[a]dding a new MoE variant only requires implementing a small adapter module\" but do not include an example file."}, "questions": {"value": "1. Is Section 4 truly a reimplementation, from scratch, entirely separate from the original implementation of the paper?\n2. Can you provide examples of usage? E.g., using your framework to do a true reimplementation of an existing paper, and an example of extension to new MoE variants?\n3. How different of an MoE variant can be easily included? I.e. what assumptions does your framework make, which would actually be very difficult to override?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cd6HLm56vp", "forum": "bVyCzNDNX8", "replyto": "bVyCzNDNX8", "signatures": ["ICLR.cc/2026/Conference/Submission21958/Reviewer_8cDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21958/Reviewer_8cDM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762321385474, "cdate": 1762321385474, "tmdate": 1762941997496, "mdate": 1762941997496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}