{"id": "dojxn0hE8m", "number": 16990, "cdate": 1758270973786, "mdate": 1759897205980, "content": {"title": "Rolling Ball Optimizer: Learning by ironing out loss landscape wrinkles", "abstract": "Training large neural networks (NNs) requires optimizing high-dimensional data-dependent loss functions. The optimization landscape of these functions is often highly complex and textured, even fractal-like, with many spurious (sometimes sharp) local minima, ill-conditioned valleys, degenerate points, and saddle points. Complicating things further is the fact that these landscape characteristics are a function of the training data, meaning that noise in the training data can propagate forward and give rise to unrepresentative small-scale geometry. This poses a difficulty for gradient-based optimization methods, which rely on local geometry to compute their updates and are, therefore, vulnerable to being derailed by noisy data. In practice, this translates to a strong dependence of the optimization dynamics on the noise in the data, i.e., poor generalization performance. To remediate this problem, we propose a new optimization procedure: Rolling Ball Optimizer (RBO), that breaks this spatial locality by explicitly incorporating information from a larger region of the loss landscape in its updates. We achieve this by simulating the motion of a rigid sphere of finite radius $\\rho>0$ rolling on the loss landscape, a straightforward generalization of Gradient Descent (GD) that simplifies into it in the *infinitesimal* limit $(\\rho\\to0)$. The radius serves as a hyperparameter that determines the scale at which RBO \"sees\" the loss landscape, allowing control over the granularity of its interaction therewith. We are motivated in this work by the intuition that the large-scale geometry of the loss landscape is less data-specific than its fine-grained structure, and that it is easier to optimize. We support this intuition by proving that our algorithm has a smoothing effect on the loss function. Evaluation against SGD, SAM, and Entropy-SGD,  on MNIST and CIFAR-10/100 demonstrates promising results in terms of convergence speed, training accuracy, and generalization performance.", "tldr": "", "keywords": ["Loss landscape", "DeepLearning", "Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68d356b26c77e44ed4079cce8b81102367ff3636.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a new optimizer: the rlling ball optimizer. It consists in simulating a rolling ball over the loss landscape. This work then present several theoretical evidences of the benefits of this approach, and empirically asses its performance."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm is novel and interesting. The theoretical analysis serves as a solid foundation for the proposed method."}, "weaknesses": {"value": "### Presentation\nLack of details for the algorithm: all the crucial steps of the algorithms 5-6-7 (Alg.1) need more explanations (i.e. for now I could not reproduce the algorithm from the description provided).\n\n### Experiments\nThe experimental section is quite weak: only small-scale vision datasets are used with small models. Moreover, it seems that hyperparameter tuning is not done for baselines and it is not clear how it is done for the proposed method.\n\n### Minor\n- l280: \"fromal\" -> \"formal\"\n\nTo sum up, though the idea is interesting and the theoretical analysis solid, the presentation and experimental section need significant improvements."}, "questions": {"value": "- Coud you make the relationship between the notion of \"Unreachable point\" and the trajectory of the algorithm more clear? Is it guaranteed that the algorithm will not visit those points?\n- Address the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jcXYCiUW8k", "forum": "dojxn0hE8m", "replyto": "dojxn0hE8m", "signatures": ["ICLR.cc/2026/Conference/Submission16990/Reviewer_T7n4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16990/Reviewer_T7n4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662219485, "cdate": 1761662219485, "tmdate": 1762927008996, "mdate": 1762927008996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-step (descent step and constraint projection) optimization method called RBO which is designed to consider the local geometry. The RBO simulates the motion of a rigid $\\rho$-sphere over the loss landscape. The authors argue that it improves generalization performance and optimization convergence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a novel optimization method with intuitive idea of \"rolling ball\" which can be beneficial not only for optimization and also for generalization."}, "weaknesses": {"value": "- The crucial weakness is the experiment parts.\n    - ResNet-6 and VGG-9 are too small. It would be much better if it is scalable to larger neural networks (e.g., WRN-28-10).\n    - The accuracy reported in Table 1 is very far from the state-of-the-arts. It doesn't need to be the state-of-the-arts, but at least, CIFAR-10 performance should be over/around 90% (SAM achieved >97% performance according to the SAM paper). It's unclear whether the hyperparameters of SAM are well-tuned for the small neural networks.\n    - What is the computational cost (seconds) and time complexity (with respect to the number of parameters) for the RBO compared to SGD? Where is the bottle neck for the RBO computation?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i965upERwR", "forum": "dojxn0hE8m", "replyto": "dojxn0hE8m", "signatures": ["ICLR.cc/2026/Conference/Submission16990/Reviewer_AuYt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16990/Reviewer_AuYt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903826942, "cdate": 1761903826942, "tmdate": 1762927004557, "mdate": 1762927004557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Rolling Ball Optimizer (RBO), a \"non‑local\" optimization procedure that simulates a rigid sphere of radius $\\rho > 0$ rolling on the graph of the loss $f(\\theta)$. Each iteration takes a descent-like step for the ball’s center and then projects back to an offset of the loss surface by finding the closest point on the graph to the provisional center; the new center is set to the contact point plus the outward normal scaled by $\\rho$ (Algorithm 1, Fig. 1, pp. 3–4). The intuition is that a finite‑radius body is insensitive to small‑scale “wrinkles,” so RBO “irons out” fine geometry while respecting large‑scale structure. This is formalized via the offset manifold $S(\\Gamma,\\rho)$ and a sequence of results: a weak smoothing (“ironing”) lemma for bounded functions, a “linear ironing” proposition for affine trends perturbed by bounded noise, and an unreachability result showing sharp local minima become inaccessible once \\rho exceeds a curvature‑dependent threshold (pp. 4–6, 12–13). Empirically, on MNIST, CIFAR‑10, CIFAR‑100 with small networks, RBO is compared to SGD, Entropy‑SGD, and SAM. Table 1 (p. 7) reports that RBO often attains higher validation accuracy than the baselines (not always lowest loss), and Figures 3–4 (pp. 7–8) show faster training‑loss reduction. A heat map (Figure 5, p. 8) suggests wide stability regions at large learning rates when $\\rho$ is larger."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality (concept): Replaces point‑particle dynamics with finite‑radius body dynamics; non‑locality emerges from a projection onto the graph’s offset. This is a clean, physically motivated design space distinct from SAM/Entropy‑SGD. Fig. 2 (p. 4) compellingly visualizes multi‑scale smoothing as $\\rho$ increases.   \n- Quality (math framing): The offset‑manifold viewpoint and the weak/linear ironing results formalize the smoothing intuition; the unreachability proposition links sharpness to curvature via $\\|\\nabla^2 f\\|$ (pp. 5–6, 12–13).  \n- Clarity (algorithmic idea): Algorithm 1 (p. 4) clearly separates the descent move and the projection; Fig. 1 helps build geometric intuition.  \n- Significance (potential): If made practical, a radius‑controlled, non‑local optimizer that is stable at larger step sizes could be impactful. Early results (Table 1, Figs. 3–5) indicate faster training and competitive or better validation accuracy on small benchmarks."}, "weaknesses": {"value": "1. **Metric & scaling are not specified or analyzed.** The projection minimizes Euclidean distance in $\\mathbb{R}^{d+1}$ between $\\tilde c_{t+1}$ and points on the graph $\\{(\\theta,f(\\theta))\\}$ (Eq. (3)), which implicitly equates horizontal parameter units and the vertical loss scale. Without a scaling parameter$\\lambda$ to balance $\\|\\theta-\\theta_e\\|^2 + \\lambda^2 (f(\\theta)-y_e)^2$, behavior can change drastically under simple transformations (e.g., multiplying the loss by a constant) or parameter re‑scalings. The paper does not discuss scale invariances, sensitivity to reparameterization, or how \\rho interacts with loss magnitude. Eq. (4) also reveals this coupling explicitly.  \n2. **Algorithmic details missing.** The inner projection solver (Eq. (4)) lacks stopping criteria, step size $\\gamma$, iteration budget, warm‑start strategy, and failure handling when multiple contact points exist (footnote 4 acknowledges non‑uniqueness for large $\\rho$). These choices materially affect stability and cost.   \n3. **No convergence or complexity guarantees.** Beyond ironing and unreachability, there is no analysis of convergence of the outer loop (even to stationary points of a smoothed objective), nor cost bounds for the inner projection.\n4. **Empirical evaluation is too thin to support claims.** \n     - Only 10 epochs on CIFAR‑10/100; these regimes underfit typical recipes and can invert method rankings.  \n     - No wall‑clock or FLOP accounting; RBO performs an inner optimization each step, so “faster to converge in epochs” may still be slower in time.  \n     - Baselines and tuning: SGD uses a single LR (0.01) with no momentum/decay/schedule; SAM/Entropy‑SGD hyperparameters are “recommended defaults” but learning rates and schedules are unclear, and a single set is used across tasks. This is not competitive practice and likely underestimates strong baselines.  \n5. **Missing ablations:** No study of inner‑loop iterations/tolerance, projection errors, or radius–learning‑rate couplings beyond a single heat map on MNIST (Fig. 5).  \n6. **Theory is partial and special‑case.** The main smoothing effect is proven for bounded functions (Lemma 1) and affine trends (Proposition 2). The central “strong ironing” conjecture is left open (p. 5, p. 13), so the key claim remains unproven in the settings that matter operationally."}, "questions": {"value": "1. Metric/scaling: What metric do you intend for Eq. (3)? As written, it is Euclidean in $\\mathbb{R}^{d+1}$. Have you explored a scale parameter \\lambda so the projection minimizes $\\|\\theta-\\theta_e\\|^2 + \\lambda^2 (f(\\theta)-y_e)^2$? How sensitive is RBO to multiplying the loss by a constant (e.g., switching from CE to CE×10)?   \n2. Exact formulas for $\\tau(p)$ and $\\nu(p)$ would be nice: Please specify the closed forms for a graph $\\Gamma=\\{(\\theta,f(\\theta))\\}$ and show the $\\rho\\to 0$ limit rigorously reduces to standard GD. (The abstract states this, but I did not find a proof.)   \n3. Projection step details: How many inner iterations are used, with what \\gamma, tolerance, and warm‑start? How often does the projection fail or find different contact points when multiple exist? Please include an ablation of inner iterations vs. accuracy, and wall‑clock costs per step/epoch.   \n4. How does it compare against momentum methods, such as the most widely used Adam(W)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "myE1r0FPp3", "forum": "dojxn0hE8m", "replyto": "dojxn0hE8m", "signatures": ["ICLR.cc/2026/Conference/Submission16990/Reviewer_PTNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16990/Reviewer_PTNo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957326932, "cdate": 1761957326932, "tmdate": 1762927003341, "mdate": 1762927003341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Rolling Ball Optimizer, which simulates rolling a voluminous object down the loss landscape rather than a point mass. It does this using a projected gradient descent approach. The authors show that under some idealized conditions, this has a nice \"ironing\" property on the loss landscape, show empirical comparisons to methods like SAM, and investigate the role of the radius hyperparameter."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea, or especially its implementation, seem novel and yet intuitive.\n\nThe explanations for why it might work also seem to pass muster (learning rate and the radius phase transition)."}, "weaknesses": {"value": "Some of the motivation in the abstract and intro feels like overselling the problem, e.g., for a while it was believed that local minima might simply not exist in neural networks; see https://arxiv.org/pdf/1910.00359.\n\nI would like to know _how_ much more computationally expensive this is; my intuition about doing the projections says \"much more than SAM\", which doesn't bode too well given they were trading blows in Table 1. In any case, that concern makes me want for some compute-matched experiments, to apply to settings where compute is the constraint rather than data, such as pretraining foundation models. Figure 4 gives the impression that RBO might be worse on such a basis unless it's only ~2x the cost of SAM.\n\nThe settings also seem impoverished, only going up to CIFAR-100 and ResNet-8s/VGG-9s, needlessly so, I suspect, if this could all be done with an RTX 4050."}, "questions": {"value": "What's stopping the projection from creating another intersection with the landscape? Why is it safe to assume Equation (1) for the proofs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zQV1WDO1yr", "forum": "dojxn0hE8m", "replyto": "dojxn0hE8m", "signatures": ["ICLR.cc/2026/Conference/Submission16990/Reviewer_VZaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16990/Reviewer_VZaA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986147383, "cdate": 1761986147383, "tmdate": 1762927002287, "mdate": 1762927002287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}