{"id": "Amx04hq2fZ", "number": 7113, "cdate": 1758008398432, "mdate": 1759897872027, "content": {"title": "On the Dynamics & Transferability of Latent Generalization during Memorization", "abstract": "Deep Networks have been known to have extraordinary generalization abilities, via mechanisms that aren't yet well understood. It is also known that upon shuffling labels in the training data to varying degrees, Deep Networks, trained with standard methods, can still achieve perfect or high accuracy on this corrupted training data. This phenomenon is called memorization, and typically comes at the cost of poorer generalization to true labels. Recent work has demonstrated, surprisingly, that such networks retain significantly better latent generalization abilities, which can be recovered via simple probes on their layer-wise representations. However, the origin and dynamics over training of this latent generalization is not well understood. Here, we track the training dynamics, empirically, and find that latent generalization abilities largely peak early in training, with model generalization, suggesting a common origin for both. However, while model generalization degrades steeply over training thereafter, latent generalization falls more modestly & plateaus at a higher level over epochs of training. Next, we design a new linear probe, in contrast with the quadratic probe used in prior work, and demonstrate that it has superior generalization performance in comparison to the quadratic probe, in most cases. Importantly, using the linear probe, we devise a way to transfer the latent generalization present in last-layer representations to the model by directly modifying the model weights. This immediately endows such models with improved generalization, i.e. without additional training. Finally, we use the linear probe to design initializations for Deep Networks, which, in many cases, turn out to be memorization-resistant, without using regularization. That is, Deep Networks with such initializations tend to evade memorization of corrupted labels, which is often accompanied by better generalization, when used with standard training methods alone. Our findings provide a more detailed account of the rich dynamics of latent generalization during memorization, and demonstrate the means to leverage this understanding to directly transfer this generalization to the model & design better model-weight initializations in the memorization regime.", "tldr": "", "keywords": ["Memorization", "Generalization", "Deep Neural Network"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ef8e2c6fdd2b7d15a5df5417b1032073bad1028.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper shows that latent generalization skills persist in models even when they start to memorize corrupted label. The authors propose a new linear probe called Velcip which is then used to identify latent generalization skills in intermediate layers for DNNs. The authors also propose a new initialization scheme which preserves the model's generalization scheme."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The experiments in the paper are extensive and support the claims made in the paper.\n2) The new initialization scheme proposed demonstrates practical utility of the empirical understanding of latent generalization skills."}, "weaknesses": {"value": "My main concern is that the method proposed draws heavily from the prior work [1]. In particular, the variant proposed is just a simple modification of MASC proposed in [1]. While MASC corresponds to using a subspace with the class-specific top-$m$ principal components (where $m$ varies and according to [1] is set as a hyperparameter choice), Velpic is equivalent to using $m=1$. This is because in the first case the quantity being maximized is $(x.p_1)^2$, while Velpic maximizes $x.p_1$, which are equivalent upto a sign (an ambiguity anyways present in PCA). In fact, as the authors notes, the former allows natural sign-invariance, eliminating the need for the mean projection-based sign reversal proposed in the current work.\n\nWhile it is interesting to note that just using a single component is enough to grant non-trivial improvements in generalization skills, it is necessary to perform a fairer comparison by comparing Velpic with MASC with the top-component (the subspace spanned by $p_1$). This also warrants some more investigations into the statistical properties of the latent representations, eg. whether the top-component already captures a significant portion of the total variance.\n\nAlso, alignment preserves accuracy (a discontinuous function) but not loss. The paper does not discuss what happens to the loss at all, restricting the utility of the method to classification tasks only. \n\n[1] Simran Ketha and Venkatakrishnan Ramaswamy. Decoding generalization from memorization in deep neural networks. arXiv preprint arXiv:2501.14687, 2025"}, "questions": {"value": "1) Clarification on Fig 2 - is the single curve for the Velpic accuracy the average accuracy across all the layers? Also for completeness the training accuracy should also be plotted.\n2) For section 7 (figure 4), the plots end at around epoch 100, as opposed to >200 epochs for the experiments done in Fig. 2. What happens when the model is trained longer? Does the generalization ability persist all throughout?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YhtaTMUG4k", "forum": "Amx04hq2fZ", "replyto": "Amx04hq2fZ", "signatures": ["ICLR.cc/2026/Conference/Submission7113/Reviewer_1JNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7113/Reviewer_1JNd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610595833, "cdate": 1761610595833, "tmdate": 1762919284289, "mdate": 1762919284289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the dynamics of the generalization power of the learned representation when memorization occurs. It suggests that the generation power in the latent representation persists even under memorization. On top of validating this dynamics using an existing probe (MASC), the paper proposes a new probe (VELPIC) that recovers the generalization ability. Last, it empirically shows the effectiveness of the new probe in variou settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper dives into an interesting problem: the dynamics of learning under potential memorization and what we can still gain from it. It touches quite a few sub-directions in this topic and contributes at various angles."}, "weaknesses": {"value": "My main concern for this work is its lacking of focus. There at least four items of investigation: 1) the dynamics of generalization power in latent representation vs. training epochs, 2) the underlying math nature of an existing probe, 3) a new probe, and 4) how the new probe can be used to transfer the generalization power to another models. Given that all these problem settings are relatively new, the authors may be putting too many good things in one paper: none of the problem settings is introduced or motivated well. See my questions for potential sources of confusion.\n\nThe current draft is extremely text-heavy. For some of the background concepts, a good picture can tell a thousand words."}, "questions": {"value": "1) Why do we want to transfer the latent representation to another task/model? What's the common example we have in literature, and does it match the experiment set up? Asking because the experiments are using MNIST, which doesn't really need transferring from larger models for good performance.\n\n2) I assume we have a base model that suffers from certain degree of memorization. Are MASC/VELPIC modifications to the base model that makes it useful for other tasks?\n\n3) Following 2), why is transferability the primary concern? Can MASC/VELPIC be used to salvage the model for its original task?\n\n4) What is the limitation of MASC, or in other words, what motivates us to find a new problem like VELPIC?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yoFwRAD0AJ", "forum": "Amx04hq2fZ", "replyto": "Amx04hq2fZ", "signatures": ["ICLR.cc/2026/Conference/Submission7113/Reviewer_iZq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7113/Reviewer_iZq6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916887680, "cdate": 1761916887680, "tmdate": 1762919283958, "mdate": 1762919283958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the training dynamics of neural networks when the data is noisy and analyzes the phenomenon that the direct predictions of the model get worse throughout training, while classifiers built on top of the hidden states of the network remain more robust. The authors track the performance of such classifiers and observe that initially, their performance is closely aligned with the original model, but after a certain point, the model starts to perform significantly worse, whereas the classifiers remain more robust. \nThe authors then develop a new classifier coined Velpic, essentially a linear probe that can outperform previously employed probes of quadratic nature. The new linear probe can directly be incorporated into the networks weights, and also serves as a more robust initialization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow. The phenomenon studied is interesting and a better understanding of it would reveal a lot about the training dynamics of neural networks and their surprising robustness to noise.\n2. The results on the network being more robust to noise when initialized with the Velpic vectors is interesting. It does depend on the training data used for Velpic, as I'm asking later in this review, but I find it surprising that such an intervention before training can help steer the model away from overfitting."}, "weaknesses": {"value": "1. The contributions of this work are not very clear to me. The work largely builds upon the previous work [1] that introduced the MASC classifier, and the initial set of experiments seem to largely obtain the same insights. What seems maybe novel is the dynamics aspect by tracking the performance throughout training but the behavior is not very surprising to me. \nThe new classifier, Velpic, seems like a more complicated way of training a linear classifier on top of the hidden states instead of just using the cross entropy? What I however did not understand is the training data for it: are you using the same noisy data or do you assume access to the ground truth labels? \nUsing linear probes on top of neural networks has by now a very long tradition in self-supervised learning and other areas, while Velpic seems to use another algorithm to find the linear classifier, I highly doubt that results would vary much if cross entropy were used. \n2. I'm also confused by the section on weight transfer to the original model, where essentially the head of the model is replaced by the Velpic vectors. Given that this is a linear classifier, isn't it by definition going to be the exact same predictions? I'm not sure if I'm missing something but this seems very obvious and not worth writing an entire section for. Even the plots in Figure 2 and Figure 3 look exactly identical, I don't think it is worth repeating the exact same plot for this observation. \n\n[1] Ketha et al., Decoding generalization from memorization in deep neural networks, 2025"}, "questions": {"value": "1. Do you use data augmentation during training? Previous work has observed very strong generalization even under perfect label noise when training with heavy data augmentation and subsequently training a linear classifier with the clean data [1]. Some of your observations might relate to those results, or might be amplified if data augmentation is used.\n2. According to the plots regarding robustness to noise when initializing with Velpic weights, it seems that MLPs benefit a lot but CNNs way less, it seems to even hurt them to some degree according to Figure 4. Do you have an intuition on this discrepancy?\n\n[1] Anagnostidis et al, The curious case of benign memorization, 2022"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XwuvsaMceP", "forum": "Amx04hq2fZ", "replyto": "Amx04hq2fZ", "signatures": ["ICLR.cc/2026/Conference/Submission7113/Reviewer_A95x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7113/Reviewer_A95x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947572820, "cdate": 1761947572820, "tmdate": 1762919283383, "mdate": 1762919283383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tracks how “latent generalization” emerges and persists in networks trained to memorize corrupted labels, argues MASC is a quadratic probe, proposes a new linear probe (VeLPIC) that often scores higher, and claims one can transfer the latent signal into the model’s weights and craft “memorization-resistant” initializations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* This paper tries to address a timely problem in machine learning"}, "weaknesses": {"value": "* Latent generalization is not well defined, no evidence for this is phenomena provided (or well cited reference).\n* The MASC-is-quadratic proof feels orthogonal to the central empirical story; the paper doesn’t explain why that nonlinearity matters for practice or theory\n* Evaluation is limited to small datasets/older architectures; there’s no evidence that claims hold on modern large-scale setups (e.g., ResNets/ViTs on ImageNet-1k, language models).\n* Writing/organization are rough, making it hard to follow; definitions and motivation need tightening."}, "questions": {"value": "1. What is MASC?\n2. How is it trained? \n3. Why does MASC help and not linear probe?\n4. Why do we care that MASC is quadratic?\n5. Can you provide evidence for latent generalization on large scale image/language datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VmOcM1jxuc", "forum": "Amx04hq2fZ", "replyto": "Amx04hq2fZ", "signatures": ["ICLR.cc/2026/Conference/Submission7113/Reviewer_W8Qc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7113/Reviewer_W8Qc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971089672, "cdate": 1761971089672, "tmdate": 1762919283016, "mdate": 1762919283016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}