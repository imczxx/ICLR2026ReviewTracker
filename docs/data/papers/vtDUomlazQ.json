{"id": "vtDUomlazQ", "number": 12424, "cdate": 1758207692053, "mdate": 1759897510775, "content": {"title": "Any-Order Any-Subset AutoRegressive Model", "abstract": "We propose Any-order Any-subset Autoregressive modeling (A3), a novel sequence generation framework that generalizes standard autoregressive (AR) factorization to support the prediction of arbitrary token groups in any order. A3 overcomes the limitations of conventional left-to-right decoding by enabling flexible groupwise generation while preserving probabilistic rigor and training stability. Our design combines a two-stream attention architecture with a progressive training strategy, allowing both efficient parallel decoding and robust modeling of diverse dependency structures. Empirical results demonstrate that A3 achieves a superior trade-off between generation speed, flexibility, and quality compared to state-of-the-art AR and diffusion-based methods. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.", "tldr": "", "keywords": ["Large language model", "permutation language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b9f23c5b3cabf30645b0c70b98448e064910314.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes A3, a sequence modeling framework that generalizes AR factorization to predict arbitrary groups of tokens in any order, aiming to combine AR’s likelihood-faithfulness with the flexibility/parallelism of diffusion and semi-AR methods. The core is a two-stream attention design (content vs. query streams) plus a progressive curriculum (AR init → group expansion → order permutation). Experiments on QA/commonsense/story-infilling show A3 beats several diffusion LMs and scales with model size, though it still trails an AR baseline at equal parameter counts; the authors attribute this to smaller pretraining budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Neat architectural idea. The two-stream attention extends XLNet-style permutation to groups: content attends to ≤ current group; query attends only to prior groups, yielding a clean predictive head for the next group. The masks and equations are well specified.\n- Thoughtful training curriculum. The three-stage progression (singleton AR → contiguous groups → permuted grouping) offers a practical path from a pretrained AR model to any-order behavior.\n- Flexible inference recipes. The paper describes groupwise AR sampling and dynamic resampling, articulating a speed–quality trade-off and illustrating it empirically.\n- Empirical signal vs. diffusion LMs. On TriviaQA/PIQA/etc., A3 (1B–8B) outperforms diffusion baselines and shows sensible scaling; the table also situates an AR baseline."}, "weaknesses": {"value": "- Missing baselines from the semi-AR family. The related-work discussion notes multi-token prediction and insertion models, but the experiments do not include strong semi-autoregressive baselines (e.g.,  speculative-with-MTP) that directly target speed-ups while preserving AR quality. This makes it hard to attribute A3’s benefits to grouping vs. other established accelerations.\n- Latency reporting is proxy-based. The speed-quality trade-off uses per-sequence decoding time and Llama-measured perplexity for unconditional generation; end-to-end wall-clock latency under realistic batching and KV-cache reuse is not reported for the task benchmarks. More concrete throughput/latency numbers (tokens/s, ms/sample) would help.\n- Ablation depth. The method depends on group size schedules, permutation schemes, and the two-stream mask design. The paper lacks sensitivity analyses for (i) curriculum choices (stage lengths, s progression), (ii) group selection criteria during dynamic resampling (beyond brief entropy vs. confidence), and (iii) robustness across context lengths."}, "questions": {"value": "- Compute-normalized comparison. Can you provide training FLOPs and data tokens for A3 vs. AR and diffusion baselines, and where possible train the AR baseline on the same 2B tokens to isolate the architectural effect? \n- Semi-AR baselines. How does A3 compare against modern multi-token-prediction methods under matched setups? A small-scale controlled study would be informative. \n- Curriculum sensitivity. What happens if you skip Stage 2 or change the group-size schedule? Is the two-stream architecture alone sufficient to get most of the gains? \n- Inference consistency. For dynamic resampling, does there exist a well-defined induced factorization (e.g., an ordering over committed sets) that preserves AR semantics, or is this best viewed as a heuristic? Any failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7AZsy9xQre", "forum": "vtDUomlazQ", "replyto": "vtDUomlazQ", "signatures": ["ICLR.cc/2026/Conference/Submission12424/Reviewer_ky8C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12424/Reviewer_ky8C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959658681, "cdate": 1761959658681, "tmdate": 1762923313793, "mdate": 1762923313793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes A3 (Any-order Any-subset Autoregressive), a sequence modeling framework that keeps the probabilistic rigor and training stability of standard autoregressive (AR) models but relaxes the decoding order to arbitrary groups of tokens. Concretely, A3 factorizes a sequence into groups and trains the model to predict any group conditioned on previously generated groups, using a two-stream attention layout to separate “what has been generated” from “what is now being predicted.” At inference, A3 can decode group-by-group (for speed) or do dynamic re-sampling of uncertain positions (for quality), so it can handle infilling and other non-left-to-right scenarios in a single model. Empirically, on common LM benchmarks (QA, commonsense, story cloze), A3 outperforms discrete-diffusion / masked-iterative baselines at similar or larger compute, but still trails a comparable AR model that was trained on far more tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear target: AR-level stability + arbitrary-order generation. The paper correctly identifies a real and currently active gap: classic AR models are stable and likelihood-faithful but order-rigid, while discrete diffusion / iterative masked LM are order-flexible but multi-step, hyperparameter-sensitive, and sometimes harder to train. Proposing a single framework that “looks like AR to the optimizer” but “behaves like an infiller” is a sensible and timely goal.\n2. Groupwise factorization is a clean, composable idea. Writing ($p(x)=\\prod_k p(x_{G_k}\\mid x_{G_{<k}})$) and randomizing the grouping/order during training is conceptually simple, keeps likelihood well-defined, and reuses the well-understood AR training pipeline. Compared with diffusion-style schedules, this reduces the number of design knobs while still giving order flexibility.\n3. Two-stream attention is a reasonable architectural choice. Reusing an XLNet-style separation between “content” and “query” to implement “see only past groups but write to current group” is a sensible reuse of an established mechanism, so the method is not just algorithmic but also implementable on existing Transformer stacks."}, "weaknesses": {"value": "1. Parallel generation is only *heuristically* correct, not *distributionally* correct. The paper’s decoding story (“decode some groups, resample the uncertain ones”) is an engineering compromise, but it does not give the kind of provable, joint-distribution-correct parallel sampling that very recent any-subset AR work is starting to provide (e.g. ASSD in Guo & Ermon 2025) — those works explicitly address the mismatch between parallel predictions and the target joint, while A3 largely sidesteps it. This makes the “any-subset” claim weaker on the sampling-theoretic dimension.\n2. Comparisons are not load-bearing for the main claim. The paper leans on the fact that it beats discrete/diffusion-style baselines, but those baselines are known to be data/step/hyperparameter hungry; meanwhile A3 is compared to a much better funded AR line only anecdotally (“we used fewer tokens, so we lag”). Without an equal-data, equal-backbone, equal-context comparison against a strong AR infiller (e.g. a repurposed XLNet / permutation LM, or an AR+speculative multi-token decoder), it is hard to measure how much of the gain comes from the grouping idea itself vs. from simply staying in the AR training regime.\n3. Method novelty is incremental over permutation-style / two-stream LMs. At a high level, A3 = permutation/pseudo-permutation LM (XLNet-like) + explicit grouping curriculum + a decoding heuristic. The paper frames it as a “generalization of AR to any order, any subset,” but several of the enabling ingredients — two-stream separation, masked/pseudo-permutation training, curriculum over masks — have been explored in earlier LM or masked-hard-attention work. The paper’s specific combination is tidy, but the conceptual leap is smaller than the title suggests.\n4. Group choice is exogenous and may be the hard part. The model is trained on random / curriculum-defined partitions, but the real deployments that need “any subset” (layout-controlled editing, multi-span infilling, multi-document patching) tend to have structured subsets (spans aligned to discourse / layout). A3 doesn’t learn the grouping policy, and the paper doesn’t show that the learned model is robust to strongly biased or adversarial group patterns. That is exactly where arbitrary-order models tend to break."}, "questions": {"value": "1. On distributional correctness: your dynamic re-sampling is essentially a confidence- or entropy-driven iterative refinement. How do you ensure that, after several such rounds, the final joint over all tokens is still close to the model’s intended factorization — and how does this compare empirically to ASSD-style “correct-by-construction” decoding for any-subset ARMs? (A small experiment against the scheme in Guo & Ermon 2025 would make the generative claim much harder to dispute.)\n2. On asymptotic scaling vs. plain AR: you attribute the quality gap to “only 2B tokens,” but you also inject extra supervision signals (more factorization patterns, more masked positions). Do you have evidence that, at equal data and wall-clock, A3 does not hurt the base AR’s next-token perplexity, especially on long contexts where order permutations are most disruptive? A curve like “data ↑ → (AR PPL, A3 PPL) → gap ↓” would make the core claim much more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uptXM429Gh", "forum": "vtDUomlazQ", "replyto": "vtDUomlazQ", "signatures": ["ICLR.cc/2026/Conference/Submission12424/Reviewer_yNMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12424/Reviewer_yNMv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045261852, "cdate": 1762045261852, "tmdate": 1762923313548, "mdate": 1762923313548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a challenge in sequence generation: reconciling the training stability/ generation quality of autoregressive (AR) models with the flexibility/ parallelism of diffusion-based models. This problem is theoretically meaningful, as existing paradigms struggle to balance efficiency, quality, and adaptability—key requirements for real-world applications like long-context reasoning, text infilling, and fast content generation.\n\nA3 resolves this by generalizing AR factorization to support \"groupwise token prediction in arbitrary orders,\" with three core designs:\n(1) two-stream attention architecture, (2) progressive training strategy, and (3) flexible inference, which supports groupwise AR sampling (fast, fixed group sizes) and dynamic resampling (high-quality, confidence/entropy-based subset selection).\n\nEmpirically, A3 (trained on 2B tokens) outperforms state-of-the-art diffusion models across QA (TriviaQA), commonsense reasoning (PIQA, HellaSwag), and infilling (ROCStories) tasks (e.g., A3-8B achieves 78.1% PIQA accuracy vs. 63.3% for DiffuLlama-7B). It also surpasses AR models in AR-disadvantaged tasks (e.g., ROCStories ROUGE-L: 18.6 vs. 10.5 for Llama-3.1-8B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focus a critical problem, which addresses a gap in sequence generation—reconciling AR’s stability/quality with diffusion’s flexibility/parallelism—aligned with real-world needs (long-context generation, infilling).\n\n2. Explicitly outperforms AR models in infilling (ROCStories) by utilizing bidirectional context, validating its flexibility."}, "weaknesses": {"value": "1. Fails to cite or discuss existing NAR research on Block-wise generation (e.g., Block-AR models that split sequences into fixed/masked blocks for parallel prediction) and progressive training (e.g., curriculum-based NAR training that increments block size or relaxes order constraints). This gap obscures A3’s incremental innovation—readers cannot distinguish whether A3’s groupwise/progressive designs are novel or iterative improvements on prior NAR work.\n\n2. No controlled experiment comparing A3 with an AR model trained on the same 2B tokens (e.g., fine-tuned Llama-3.1-8B). The gap with top AR models (e.g., 19.4% vs. 52.1% on TriviaQA) may stem from data scarcity, not A3’s design—undermining assessment of its architectural advantage.\n\n2. Only evaluates sequences of length 2048, with no tests on long contexts (16k/128k tokens). A3’s claimed advantage in solving AR’s long-context inefficiency remains unproven.\n\n3. Figure 3 claims 3–4x faster decoding than AR but lacks direct time data and performances for baseline AR models (same hardware/sequence length) and explicit absolute time values, making efficiency gains hard to quantify."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s9QMuaDpFW", "forum": "vtDUomlazQ", "replyto": "vtDUomlazQ", "signatures": ["ICLR.cc/2026/Conference/Submission12424/Reviewer_TFMj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12424/Reviewer_TFMj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094019083, "cdate": 1762094019083, "tmdate": 1762923313302, "mdate": 1762923313302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces A³ (Any-order Any-subset Autoregressive modeling), a generalization of standard autoregressive (AR) language modeling. Instead of predicting tokens strictly left-to-right, A³ allows generation in arbitrary orders and subsets while maintaining a valid probabilistic formulation.\n\nThe method builds on a two-stream attention mechanism (content/query streams) similar to XLNet and uses a curriculum training schedule that progressively transitions from left-to-right to multi-token and random-order prediction. At inference, A³ supports both groupwise parallel decoding and dynamic resampling, trading off speed and quality.\n\nExperiments across QA, reasoning, and infilling tasks show that A³ matches or surpasses diffusion-based models (e.g., DiffuLlama, Dream) using significantly less data, and maintains competitive performance to AR models while enabling faster, flexible generation.\n\nOverall it is a practical step toward bridging AR and diffusion paradigms, retaining AR stability while introducing parallel generation flexibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Insightful formulation. The paper presents a very interesting perspective on any-order, any-subset autoregressive modeling, effectively bridging the strengths of AR and masked diffusion models in a unified probabilistic framework.\n\n2. Clarity and organization. The paper is well written and easy to follow, with clear explanations, sound motivation, and well-structured methodology.\n\n3. Experiments across multiple reasoning and generation benchmarks validate the effectiveness of the proposed approach, showing competitive or superior performance with improved flexibility."}, "weaknesses": {"value": "1. What's new relative to prior AR generalizations? The author mentioned that the proposed method builds closely on existing ideas from XLNet (permutation-based AR) and masked diffusion modeling, with the main difference being a unified training/inference view. While conceptually elegant, the contribution may feel incremental rather than fundamentally new.\n\n2. Evaluation scope and ablations are limited. Experiments mainly compare against diffusion-style baselines; there is less analysis against modern AR variants (e.g., speculative decoding, parallelized transformers) or ablations on key design choices like grouping strategy and curriculum schedule.\n\n3. Practical speed–quality trade-offs unclear. Although the paper claims parallel generation benefits, detailed runtime comparisons and latency measurements are missing, making it hard to assess the real efficiency gains of “any-order” decoding in practice."}, "questions": {"value": "1. How does A³ fundamentally differ from prior permutation-based autoregressive approaches such as XLNet or Permutation LM beyond the introduction of multi-token subsets? Could you clarify what new modeling capacity A³ enables that these earlier frameworks cannot?\n\n2. The paper emphasizes that A³ enables parallel or groupwise decoding. Could you provide concrete runtime or latency benchmarks (e.g., decoding speedups vs. standard AR and diffusion models) to quantify the practical benefit of this flexibility?\n\n3. The curriculum that transitions from left-to-right to any-order prediction seems important. How sensitive is model performance to the curriculum schedule (e.g., ratio of L2R vs. random-order batches)? Have you explored automatically learned or adaptive curricula?]\n\n4. Since any-order generation can, in principle, apply to structured data (e.g., image or audio tokens), do you anticipate A³ extending naturally to multimodal diffusion–AR hybrids, or would significant architectural adjustments be required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RWHhQfBZsn", "forum": "vtDUomlazQ", "replyto": "vtDUomlazQ", "signatures": ["ICLR.cc/2026/Conference/Submission12424/Reviewer_6bME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12424/Reviewer_6bME"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115340581, "cdate": 1762115340581, "tmdate": 1762923313068, "mdate": 1762923313068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}