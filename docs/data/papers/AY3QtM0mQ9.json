{"id": "AY3QtM0mQ9", "number": 2884, "cdate": 1757297876430, "mdate": 1763541573295, "content": {"title": "Language Bias in LVLMs: From In-Depth Analysis to Simple and Effective Mitigation", "abstract": "Large Vision-Language Models (LVLMs) extend large language models with visual understanding, but remain vulnerable to hallucination, where outputs are fluent yet inconsistent with images. Recent studies link this issue to *language bias*—the tendency of LVLMs to over-rely on text while neglecting visual inputs. Yet most analyses remain empirical without uncovering its underlying cause. In this paper, we provide a systematic study of language bias and identify its root in modality misalignment during training. Our analysis shows that both Visual Instruction Tuning (VIT) and Direct Preference Optimization (DPO) often prioritize textual improvements, which may cause LVLMs to overly lean toward language modeling rather than balanced multimodal understanding. To address this, we propose two simple yet effective methods: **Language Bias Regularization (LBR)**, which mitigates language bias through regularization during instruction tuning, and **Language Bias Penalty (LBP)**, which penalizes language bias in the DPO training process. Extensive experiments across diverse models and benchmarks demonstrate the effectiveness of our approach. LBR consistently improves performance on over ten general benchmarks, while LBP significantly reduces hallucination and improves trustworthiness. Together, these methods not only mitigate language bias but also advance the overall alignment of LVLMs, all without introducing any additional data or auxiliary models.", "tldr": "We diagnose that language bias in LVLMs stems from training misalignment and introduce two simple methods, LBR and LBP, that correct this during training to improve general performance and reduce hallucinations without extra model and data.", "keywords": ["Large Vision-Language Models", "Hallucination", "Alignment", "Language Bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/759f84a3b4c5ccd5d9ae8068bf8696e584a8859f.pdf", "supplementary_material": "/attachment/ffe78e17440f2c591c42b89f0f06be842d8e3eb0.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates why Large Vision-Language Models (LVLMs) tend to generate fluent but visually inconsistent responses — a phenomenon often described as hallucination. The authors argue that this stems from language bias, meaning that the model over-relies on textual input while underutilizing visual information. They systematically analyze the training process and identify modality misalignment—particularly in Visual Instruction Tuning (VIT) and Direct Preference Optimization (DPO)—as the root cause of this bias. To counter it, they introduce two mitigation strategies: (1) Language Bias Regularization (LBR): A regularization term added during instruction tuning to encourage better vision-language balance; (2) Language Bias Penalty (LBP): A penalty term incorporated into DPO training to discourage excessive dependence on linguistic patterns. Extensive experiments across multiple benchmarks show that LBR enhances general LVLM performance, while LBP significantly reduces hallucinations, all without requiring extra data or auxiliary models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a principled, quantitative framework for understanding language bias in LVLMs — moving beyond the largely empirical discussions in prior literature. The decomposition of training dynamics into multimodal vs. text-only contributions (Eq. 5–6) is both elegant and informative.\n\n2. Both LBR and LBP are remarkably simple yet effective additions to existing training pipelines. Their plug-and-play nature, lack of dependency on external data, and strong generalization across model scales are notable practical advantages.\n\n3. The authors evaluate their methods across more than ten benchmarks spanning general capabilities, visual QA, image captioning, and hallucination detection. The consistent improvements lend strong credibility to the approach.\n\n4. The paper successfully integrates theoretical insights (modality misalignment) with pragmatic training solutions (LBR/LBP), showing a clear cause–and–effect linkage between analysis and mitigation."}, "weaknesses": {"value": "1. While conceptually motivated, both LBR and LBP are incremental in formulation—essentially regularization and penalty terms on text-only likelihoods. Their simplicity, though advantageous, may reduce the perceived methodological depth compared to prior multimodal alignment work (e.g., contrastive or causally grounded approaches).\n\n2. While the authors assert that LBP avoids degradation of general capabilities, the analysis could better quantify potential trade-offs in reasoning or linguistic fluency, especially under constrained visual contexts.\n\n3. Lack of recent baselines in 2025, e.g., LACING [1]\n\n[1] https://arxiv.org/abs/2411.14279"}, "questions": {"value": "NONE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zJSzOkxbAb", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_Eywr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_Eywr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557067936, "cdate": 1761557067936, "tmdate": 1762916429445, "mdate": 1762916429445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why Large Vision-Language Models (LVLMs) tend to generate fluent but visually inconsistent responses — a phenomenon often described as hallucination. The authors argue that this stems from language bias, meaning that the model over-relies on textual input while underutilizing visual information. They systematically analyze the training process and identify modality misalignment—particularly in Visual Instruction Tuning (VIT) and Direct Preference Optimization (DPO)—as the root cause of this bias. To counter it, they introduce two mitigation strategies: (1) Language Bias Regularization (LBR): A regularization term added during instruction tuning to encourage better vision-language balance; (2) Language Bias Penalty (LBP): A penalty term incorporated into DPO training to discourage excessive dependence on linguistic patterns. Extensive experiments across multiple benchmarks show that LBR enhances general LVLM performance, while LBP significantly reduces hallucinations, all without requiring extra data or auxiliary models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a principled, quantitative framework for understanding language bias in LVLMs — moving beyond the largely empirical discussions in prior literature. The decomposition of training dynamics into multimodal vs. text-only contributions (Eq. 5–6) is both elegant and informative.\n\n2. Both LBR and LBP are remarkably simple yet effective additions to existing training pipelines. Their plug-and-play nature, lack of dependency on external data, and strong generalization across model scales are notable practical advantages.\n\n3. The authors evaluate their methods across more than ten benchmarks spanning general capabilities, visual QA, image captioning, and hallucination detection. The consistent improvements lend strong credibility to the approach.\n\n4. The paper successfully integrates theoretical insights (modality misalignment) with pragmatic training solutions (LBR/LBP), showing a clear cause–and–effect linkage between analysis and mitigation."}, "weaknesses": {"value": "1. While conceptually motivated, both LBR and LBP are incremental in formulation—essentially regularization and penalty terms on text-only likelihoods. Their simplicity, though advantageous, may reduce the perceived methodological depth compared to prior multimodal alignment work (e.g., contrastive or causally grounded approaches).\n\n2. While the authors assert that LBP avoids degradation of general capabilities, the analysis could better quantify potential trade-offs in reasoning or linguistic fluency, especially under constrained visual contexts.\n\n3. Lack of recent baselines in 2025, e.g., LACING [1]\n\n[1] https://arxiv.org/abs/2411.14279"}, "questions": {"value": "NONE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zJSzOkxbAb", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_Eywr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_Eywr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557067936, "cdate": 1761557067936, "tmdate": 1763653103426, "mdate": 1763653103426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses language bias in Large Vision-Language Models (LVLMs) – a key cause of hallucinations (fluent but visually inconsistent outputs. It identifies the root as modality misalignment in two core training stages: VIT and DPO, where models prioritize textual improvements over visual alignment. The authors formalize language bias as a quantifiable term (ℬ) and propose two mitigation methods: Language Bias Regularization (LBR) for VIT (penalizes excessive text reliance) and Language Bias Penalty (LBP) for DPO (suppresses existing bias). Extensive experiments on LLaVA-1.5 (7B/13B) and LLaVA-NEXT (3B) show LBR improves performance across 10+ general benchmarks, while LBP reduces hallucinations on MMHalBench/AMBER. Human evaluation further validates bias mitigation, with no extra data/auxiliary models needed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper quantifies language bias (via ℬ) and links it to modality misalignment in VIT/DPO, providing a principled theoretical foundation rather than surface-level observations.\n* LBR and LBP are simple to integrate into existing training pipelines (no extra data/models), with minimal computational overhead (similar VRAM to baselines). \n* Experiments cover diverse models (3B–13B scales), benchmarks (general LVLMs, hallucination-focused, text/visual tasks), and human evaluation. Results are consistent, confirming generalization (e.g., LBR improves MME by 35 points; LBP cuts 7B model hallucination rate by 27% on MMHalBench)."}, "weaknesses": {"value": "* The authors set LBR’s α=1e-5 and LBP’s γ=1 but provide no rationale for these specific values. \n* All experiments use LLaVA variants. No tests on other LVLM architectures raise questions about whether LBR/LBP generalize to non-LLaVA frameworks.\n* LBP is claimed to be effective for long responses, but only qualitative examples (500+ tokens) are provided. No quantitative data on hallucination rate trends as response length increases (e.g., 100 vs. 1000 tokens) is included."}, "questions": {"value": "* Have you tested LBR/LBP on non-LLaVA LVLMs? If performance drops, what architectural differences (e.g., vision encoder type) might explain this, and how could your methods adapt?\n* Can you provide quantitative data (e.g., hallucination rate per 100 tokens) showing how LBP performs vs. DPO as response length scales (e.g., 200, 500, 1000 tokens)?\n* Did you analyze whether the vision encoder (e.g., CLIP) contributes to bias (e.g., poor visual feature extraction)? If so, how do LBR interact with encoder improvements?\n\nI look forward to an active discussion with the authors during the rebuttal phase and will revise my score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lzZ9URFcqN", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_Ms6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_Ms6F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925083687, "cdate": 1761925083687, "tmdate": 1762916429227, "mdate": 1762916429227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large Vision-Language Models (LVLMs) often rely too much on text and not enough on images, causing inconsistency or so-called \"hallucinations\". This paper studies why that happens and finds that during training, models improve almost as much using text alone as when using both text and images. To fix this, the authors add two simple training terms: Language Bias Regularization (LBR) for visual instruction tuning (VIT) and Language Bias Penalty (LBP) for preference optimization (DPO). Tested on LLaVA models, these methods bring steady improvements across benchmarks and reduce hallucinations. The study gives a useful diagnosis of modality imbalance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated by the known problem of language bias in LVLMs and clearly defines it as the over-reliance on textual priors. This issue is supported by an in-depth quantitative analysis that verifies how the bias emerges during training.\n2. The quantitative diagnosis of language bias by comparing text-only and multimodal likelihood improvements during training is clear. By decomposing the process into VIT and DPO, it offers a good view of how this bias emerges and persists across different fine-tuning stages.\n3. The proposed mitigation methods, LBR and LBP, are simple and easy to implement. They also deliver consistent improvements across multiple benchmarks while maintaining overall model performance. \n4. The inclusion of ablation studies and a human evaluation adds credibility to the findings, helping to illustrate the effects of each component and providing qualitative support for the claimed improvements."}, "weaknesses": {"value": "1. Tehnitically speaking, the overall contribution is limited. LBR and LBP are mathematically simple regularization terms added to existing VIT and DPO objectives, offering minimal theoretical novelty. The main value seems to lie in the diagnostic analysis rather than in algorithmic innovation. For example, the paper does not explore combining the two methods to test whether they address complementary sources of bias, leaving the contribution feeling incremental.\n2. The evaluation is limited in scope and strength. All experiments are conducted only on the LLaVA family, making it unclear whether the methods generalize to the others. The reported gains are small, typically under two points across benchmarks. The baseline comparisons are weak, as, for example, LBR is tested only against the vanilla VIT setup without including stronger existing alternatives. \n3. Regarding the analysis, Figure 3(a) shows that text-only likelihood improvement dominates during VIT, but the interpretation that this directly proves modality misalignment is stronger than what the data alone supports. The observed pattern could also result from dataset bias, where many predictions depend primarily on textual cues. Otherwise, it is difficult to explain why the two curves remain so close. Figure 3(b) likewise shows that text-only gains dominate during DPO. However, note that the RLHF-V preference data and the reference model might already be language-biased, which may partly account for this trend. Also, the analysis actually focuses only on the later stages of training, not the entire lifecycle of an LVLM.\n4. The paper uses \"hallucination\" as a broad, catch-all term, though the issue it studies is more precisely the inconsistency across modalities between textual and visual information. This conceptual ambiguity weakens the framing and contributes to a gap between the stated goal (reducing modality inconsistency) and the evaluation (hallucination benchmarks). The related-work section reflects this lack of grounding. It is overly compressed and omits several closely related studies on cross-modal consistency and modality imbalance [1, 2, 3]. In fact, the authors themselves acknowledge this misalignment between purpose and evaluation. \n\n[1] Cross-Modal Consistency in Multimodal Large Language Models (Zhang et al., 2024)\n\n[2] MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts (Lu et al., 2024)\n\n[3] Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs? (Park et al., 2025)"}, "questions": {"value": "Some statements need to be more precise. For example, in line 48 \"most studies\", supporting citations are required. \nMore details are needed for the human evaluation.\nThe main concerns stand on the limited scope of the evaluation and the lack of connection to existing multimodal studies and benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jjPdxVF45i", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_kcji"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_kcji"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941594990, "cdate": 1761941594990, "tmdate": 1762916428992, "mdate": 1762916428992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a systematic analysis of language bias in Large Vision-Language Models, identifying it as a fundamental cause of hallucination arising from modality misalignment during training. The authors decompose the problem across both the VIT and DPO training stages, showing via quantitative evidence that performance improvements often stem from an over-reliance on textual priors rather than genuine visual grounding. To mitigate this, they propose two lightweight yet effective strategies, Language Bias Regularization (LBR) and Language Bias Penalization (LBP), which respectively act during the vision-alignment and preference-optimization stages. Experimental results demonstrate consistent reductions in hallucination rates and enhanced visual fidelity across multiple benchmarks and human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes language bias as *text-conditioned likelihood gain* and provides a unified analytical framework covering both pretraining (VIT) and fine-tuning (DPO) stages, backed by clear quantitative metrics.\n2. The proposed methods (LBR and LBP) are simple, interpretable, and easy to implement — requiring no additional data or model architectures, thereby offering strong reproducibility and generality.\n3. Extensive experiments across multiple LVLM architectures and benchmarks (e.g., MMHalBench, AMBER, Object HalBench) verify the robustness and universality of the proposed mitigation strategies."}, "weaknesses": {"value": "## \n\n1. The causal chain “hallucination → language bias → modality misalignment” remains conceptually abrupt. Prior work [1] has provided stronger empirical evidence showing that training objectives (e.g., likelihood maximization in VIT/DPO) incentivize models to overfit textual priors while ignoring visual cues. Incorporating this body of evidence could strengthen the argument.\n2. The consistency between evaluation results and claims is limited: LBR yields modest improvements on automatic metrics but performs well in human evaluations. The authors should more explicitly distinguish between “preventive” bias (VIT stage) and “corrective” bias (DPO stage) and clarify their observable metrics.\n3. The strong dependency on the *reference model* introduces scalability concerns. If the reference already embeds linguistic priors, bias could propagate. Moreover, training a separate ref for each target model could limit practicality — a point not fully addressed.\n4. The positioning relative to related methods (e.g., V-DPO [2], MFPO [3], RLAIF-V [4]) is insufficient. The paper could benefit from discussing how its approach complements inference-time strategies such as VCD [5] or OPERA [6].\n\n[1] Chen et al. *Understanding Vision-Language Model Hallucination through Likelihood Objectives*. 2025.\n\n[2] Zhao et al. *V-DPO: Mitigating Language Bias in Vision-Language Models via Visual-Guided Direct Preference Optimization*. 2024.\n\n[3] Li et al. *MFPO: Multi-Modal Fair Preference Optimization for Vision-Language Models*. 2024.\n\n[4] Liu et al. *RLAIF-V: Open-Feedback Alignment for Vision-Language Models*. 2024.\n\n[5] Zhang et al. *VCD: Vision Contrastive Decoding for Reducing Hallucination in LVLMs*. 2024.\n\n[6] Wu et al. *OPERA: Over-Trust Penalized Reasoning for Faithful Multimodal Generation*. 2024."}, "questions": {"value": "1. Can the authors provide a reproducible diagnostic protocol to disentangle and quantify *stage-specific* bias, namely, text gain introduced during VIT versus exacerbated during DPO, along with their effects on output length and error type?\n2. Since LBR’s performance is sensitive to regularization strength, have the authors considered adaptive weighting strategies (e.g., token-level or step-wise) to balance underfitting and overconstraint across different text lengths?\n3. Given the potential propagation of bias from reference models, would introducing a *visual contrastive anchor* (e.g., perturbed-image consistency) or using alternative formulations like V-DPO/MFPO help mitigate this dependency?\n4. The paper questions CHAIR’s adequacy for long-form hallucination. Have the authors evaluated newer metrics (e.g., OPERA [6], VCD [5]) or human–AI hybrid evaluation frameworks to ensure consistency with AMBER and MMHalBench results?\n5. Regarding scalability, how does LBP behave under larger preference datasets or extended context windows? Does it synergize or conflict with RLAIF-V’s feedback-driven optimization [4]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ah6hOceecv", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987396231, "cdate": 1761987396231, "tmdate": 1762916428803, "mdate": 1762916428803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a systematic analysis of language bias in Large Vision-Language Models, identifying it as a fundamental cause of hallucination arising from modality misalignment during training. The authors decompose the problem across both the VIT and DPO training stages, showing via quantitative evidence that performance improvements often stem from an over-reliance on textual priors rather than genuine visual grounding. To mitigate this, they propose two lightweight yet effective strategies, Language Bias Regularization (LBR) and Language Bias Penalization (LBP), which respectively act during the vision-alignment and preference-optimization stages. Experimental results demonstrate consistent reductions in hallucination rates and enhanced visual fidelity across multiple benchmarks and human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes language bias as *text-conditioned likelihood gain* and provides a unified analytical framework covering both pretraining (VIT) and fine-tuning (DPO) stages, backed by clear quantitative metrics.\n2. The proposed methods (LBR and LBP) are simple, interpretable, and easy to implement — requiring no additional data or model architectures, thereby offering strong reproducibility and generality.\n3. Extensive experiments across multiple LVLM architectures and benchmarks (e.g., MMHalBench, AMBER, Object HalBench) verify the robustness and universality of the proposed mitigation strategies."}, "weaknesses": {"value": "## \n\n1. The causal chain “hallucination → language bias → modality misalignment” remains conceptually abrupt. Prior work [1] has provided stronger empirical evidence showing that training objectives (e.g., likelihood maximization in VIT/DPO) incentivize models to overfit textual priors while ignoring visual cues. Incorporating this body of evidence could strengthen the argument.\n2. The consistency between evaluation results and claims is limited: LBR yields modest improvements on automatic metrics but performs well in human evaluations. The authors should more explicitly distinguish between “preventive” bias (VIT stage) and “corrective” bias (DPO stage) and clarify their observable metrics.\n3. The strong dependency on the *reference model* introduces scalability concerns. If the reference already embeds linguistic priors, bias could propagate. Moreover, training a separate ref for each target model could limit practicality — a point not fully addressed.\n4. The positioning relative to related methods (e.g., V-DPO [2], MFPO [3], RLAIF-V [4]) is insufficient. The paper could benefit from discussing how its approach complements inference-time strategies such as VCD [5] or OPERA [6].\n\n*(The earlier references were descriptive summaries rather than the formal paper titles. I have updated them with the correct bibliographic information and included the original links for completeness.)*\n\n[1] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning (https://arxiv.org/abs/2505.15966) *: Understanding Vision-Language Model Hallucination through Likelihood Objectives*. 2025.\n\n[2] V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization ( https://arxiv.org/abs/2411.02712) *: Mitigating Language Bias in Vision-Language Models via Visual-Guided Direct Preference Optimization*. 2024.\n\n[3] Modality-Fair Preference Optimization for Trustworthy MLLM Alignment (https://arxiv.org/abs/2410.15334) *: Multi-Modal Fair Preference Optimization for Vision-Language Models*. 2024.\n\n[4] RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness (https://arxiv.org/abs/2405.17220) *: Open-Feedback Alignment for Vision-Language Models*. 2024.\n\n[5] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding (https://arxiv.org/abs/2311.16922) *: Vision Contrastive Decoding for Reducing Hallucination in LVLMs*. 2023.\n\n[6] OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation (https://arxiv.org/abs/2311.17911) *: Over-Trust Penalized Reasoning for Faithful Multimodal Generation*. 2024."}, "questions": {"value": "1. Can the authors provide a reproducible diagnostic protocol to disentangle and quantify *stage-specific* bias, namely, text gain introduced during VIT versus exacerbated during DPO, along with their effects on output length and error type?\n2. Since LBR’s performance is sensitive to regularization strength, have the authors considered adaptive weighting strategies (e.g., token-level or step-wise) to balance underfitting and overconstraint across different text lengths?\n3. Given the potential propagation of bias from reference models, would introducing a *visual contrastive anchor* (e.g., perturbed-image consistency) or using alternative formulations like V-DPO/MFPO help mitigate this dependency?\n4. The paper questions CHAIR’s adequacy for long-form hallucination. Have the authors evaluated newer metrics (e.g., OPERA [6], VCD [5]) or human–AI hybrid evaluation frameworks to ensure consistency with AMBER and MMHalBench results?\n5. Regarding scalability, how does LBP behave under larger preference datasets or extended context windows? Does it synergize or conflict with RLAIF-V’s feedback-driven optimization [4]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ah6hOceecv", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987396231, "cdate": 1761987396231, "tmdate": 1763545142521, "mdate": 1763545142521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a systematic analysis of language bias in Large Vision-Language Models, identifying it as a fundamental cause of hallucination arising from modality misalignment during training. The authors decompose the problem across both the VIT and DPO training stages, showing via quantitative evidence that performance improvements often stem from an over-reliance on textual priors rather than genuine visual grounding. To mitigate this, they propose two lightweight yet effective strategies, Language Bias Regularization (LBR) and Language Bias Penalization (LBP), which respectively act during the vision-alignment and preference-optimization stages. Experimental results demonstrate consistent reductions in hallucination rates and enhanced visual fidelity across multiple benchmarks and human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes language bias as *text-conditioned likelihood gain* and provides a unified analytical framework covering both pretraining (VIT) and fine-tuning (DPO) stages, backed by clear quantitative metrics.\n2. The proposed methods (LBR and LBP) are simple, interpretable, and easy to implement — requiring no additional data or model architectures, thereby offering strong reproducibility and generality.\n3. Extensive experiments across multiple LVLM architectures and benchmarks (e.g., MMHalBench, AMBER, Object HalBench) verify the robustness and universality of the proposed mitigation strategies."}, "weaknesses": {"value": "## \n\n1. The causal chain “hallucination → language bias → modality misalignment” remains conceptually abrupt. Prior work [1] has provided stronger empirical evidence showing that training objectives (e.g., likelihood maximization in VIT/DPO) incentivize models to overfit textual priors while ignoring visual cues. Incorporating this body of evidence could strengthen the argument.\n2. The consistency between evaluation results and claims is limited: LBR yields modest improvements on automatic metrics but performs well in human evaluations. The authors should more explicitly distinguish between “preventive” bias (VIT stage) and “corrective” bias (DPO stage) and clarify their observable metrics.\n3. The strong dependency on the *reference model* introduces scalability concerns. If the reference already embeds linguistic priors, bias could propagate. Moreover, training a separate ref for each target model could limit practicality — a point not fully addressed.\n4. The positioning relative to related methods (e.g., V-DPO [2], MFPO [3], RLAIF-V [4]) is insufficient. The paper could benefit from discussing how its approach complements inference-time strategies such as VCD [5] or OPERA [6].\n\n*(The earlier references were descriptive summaries rather than the formal paper titles. I have updated them with the correct bibliographic information and included the original links for completeness.)*\n\n[1] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning (https://arxiv.org/abs/2505.15966) *: Understanding Vision-Language Model Hallucination through Likelihood Objectives*. 2025.\n\n[2] V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization ( https://arxiv.org/abs/2411.02712) *: Mitigating Language Bias in Vision-Language Models via Visual-Guided Direct Preference Optimization*. 2024.\n\n[3] Modality-Fair Preference Optimization for Trustworthy MLLM Alignment (https://arxiv.org/abs/2410.15334) *: Multi-Modal Fair Preference Optimization for Vision-Language Models*. 2024.\n\n[4] RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness (https://arxiv.org/abs/2405.17220) *: Open-Feedback Alignment for Vision-Language Models*. 2024.\n\n[5] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding (https://arxiv.org/abs/2311.16922) *: Vision Contrastive Decoding for Reducing Hallucination in LVLMs*. 2023.\n\n[6] OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation (https://arxiv.org/abs/2311.17911) *: Over-Trust Penalized Reasoning for Faithful Multimodal Generation*. 2024."}, "questions": {"value": "1. Can the authors provide a reproducible diagnostic protocol to disentangle and quantify *stage-specific* bias, namely, text gain introduced during VIT versus exacerbated during DPO, along with their effects on output length and error type?\n2. Since LBR’s performance is sensitive to regularization strength, have the authors considered adaptive weighting strategies (e.g., token-level or step-wise) to balance underfitting and overconstraint across different text lengths?\n3. Given the potential propagation of bias from reference models, would introducing a *visual contrastive anchor* (e.g., perturbed-image consistency) or using alternative formulations like V-DPO/MFPO help mitigate this dependency?\n4. The paper questions CHAIR’s adequacy for long-form hallucination. Have the authors evaluated newer metrics (e.g., OPERA [6], VCD [5]) or human–AI hybrid evaluation frameworks to ensure consistency with AMBER and MMHalBench results?\n5. Regarding scalability, how does LBP behave under larger preference datasets or extended context windows? Does it synergize or conflict with RLAIF-V’s feedback-driven optimization [4]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ah6hOceecv", "forum": "AY3QtM0mQ9", "replyto": "AY3QtM0mQ9", "signatures": ["ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2884/Reviewer_5Dge"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987396231, "cdate": 1761987396231, "tmdate": 1763730664350, "mdate": 1763730664350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}