{"id": "aMs6FtNaY5", "number": 2119, "cdate": 1756992493610, "mdate": 1759898168268, "content": {"title": "TTT3R: 3D Reconstruction as Test-Time Training", "abstract": "Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear complexity in the sequence length. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, enabling a balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a $2\\times$ improvement in global pose estimation over baselines while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code will be made publicly available.", "tldr": "A simply state update rule to enhance length generalization in CUT3R.", "keywords": ["3D Reconstruction", "Structure from Motion", "Recurrent Neural Networks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0edd84e7cd203d251dd175979c1df53a30d8112.pdf", "supplementary_material": "/attachment/8ad06d0641a1041e0845f4974cde0d7c18f6740d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes TTT3R, a test-time training framework for online 3D reconstruction tasks, including depth estimation and camera pose estimation from long video sequences. While prior offline reconstruction models (e.g., VGGT, Point3R) often run out of memory when processing long sequences, TTT3R offers a training-free and memory-efficient solution. Built upon the recurrent architecture of CUT3R, it introduces a confidence-guided state update rule that adaptively balances memory retention and adaptation to new observations, leading to improved performance and generalization on long video sequences."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is simple, intuitive, and well-motivated, reinterpreting 3D reconstruction as a test-time training problem and providing a fresh, elegant, and training-free plug-and-play solution that requires no additional parameters or fine-tuning, making it easy to integrate into existing 3D reconstruction pipelines.\n\n2. The results are clearly better than previous online methods in long sequences, where most offline methods run out of memory."}, "weaknesses": {"value": "1. The contribution is somewhat incremental, as the proposed method mainly builds upon CUT3R with a modified state update rule. Since TTT3R is presented as a general training-free framework, it would be valuable to include or discuss additional baselines beyond CUT3R to better demonstrate generality.\n\n2. The performance is still lower than offline methods such as VGGT on short sequences, indicating that the proposed test-time mechanism, while efficient, does not fully close the performance gap with global-attention architectures.\n\n3. It would also strengthen the paper to compare or at least discuss more recent approaches, such as MegaSaM, VIPE, VGGT-SLAM, and VGGT-Long. Even though some of these may not yet be peer-reviewed, acknowledging them would help position this work within the rapidly evolving landscape of long-sequence 3D reconstruction."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SubM6VsUaq", "forum": "aMs6FtNaY5", "replyto": "aMs6FtNaY5", "signatures": ["ICLR.cc/2026/Conference/Submission2119/Reviewer_6nKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2119/Reviewer_6nKa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776071983, "cdate": 1761776071983, "tmdate": 1762916038632, "mdate": 1762916038632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TTT3R reframes 3D reconstruction from image sequences as a test-time training problem to address the limited length generalization of recurrent models like CUT3R. It introduces a confidence-guided state update rule, deriving a closed-form learning rate from cross-attention alignment scores. This training-free intervention balances historical retention with new observations, mitigating catastrophic forgetting. Evaluations show TTT3R improves performance on long sequences (e.g., 2× better pose estimation) while maintaining CUT3R's efficiency (20 FPS, 6 GB memory for thousands of frames)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper lie in:\n1) Good motivation, sequence-length generalization is a real failure mode for stateful online 3D models. Tackling it at test time without retraining is practical.\n2) It is very interesting to see the training-free, plug-and-play intervention design. Requires no fine-tuning or extra parameters; preserves CUT3R’s O(1) memory and O(1) per-step compute while adding only a cheap per-token gate; practical and implementation-friendly.\n3) We can see the obvious gains in length generalization (e.g., 2× ATE improvement over CUT3R on ScanNet/TUM-D at ≥200–1000 views; Fig. 6) without sacrificing throughput (20 FPS; Fig. 7) or memory (6 GB; Fig. 2).\n4) The presentation of this paper has clear technical reformulation casts CUT3R’s recurrent state update as test-time training with fast weights; derives a closed-form per-token learning-rate gate beta_t from alignment confidence, Eq. 7, yielding an explicit update rule (Eq. 8) that mitigates softmax-induced overwriting."}, "weaknesses": {"value": "May major concerns lie in:\nThis paper proposes a simple, empirical state-update rule to enhance sequence-length generalization for CUT3R. It seems to be a straightforward extension of CUT3R. Specifically,\n1) From the TTT3R HTML, the core proposal for a state update appears to be a gradient step on a per-timestep loss with respect to the state, i.e., Update(S_{t-1}, X_t) = S_{t-1} − beta ∇_S L(S_{t-1}, X_t). This is a clean, causal mechanism aligned with the TTT literature but adapted to “fast weights”/state instead of parameters.\n2) The update form given in TTT3R is easy to integrate with CUT3R,  that is to treat the persistent scene state as a latent variable and make a local corrective step using the current-frame loss. If the loss L uses only X_t (and not future frames), this is strictly causal and compatible with streaming.\n3) CUT3R maintains a persistent state via state tokens and a learned update/readout (ViT decoders). CUT3R trains with a fixed context length and acknowledges resets of the state on single-view datasets, which suggests the usual risk of degradation when inference sequences are longer than the training horizon. A test-time state-adaptation rule that damps drift and improves long-horizon stability is therefore a relevant add-on.\n\nSome other technical problems:\n1) What is the test-time loss L(S_{t-1}, X_t)?\nIf it uses only self-supervised signals from a single frame (e.g., priors, regularizers) it may be weak/noisy. If it uses multiview photometric/reprojection consistency, you must ensure it’s causal (only past frames). Please make explicit exactly which losses are used at test time and why they are valid surrogates for “state correctness.”\nIf supervised losses are used in any evaluation, that would not be a realistic test-time condition—please clarify.\n2) Stability analysis vs “framework to analyze dynamics”\nThe claim of “a TTT-based framework to analyze the dynamics” suggests more than proposing an update rule. Do you provide any formal analysis (e.g., linearization around a fixed point, Jacobian spectral radius of the state dynamics, contraction conditions w.r.t. beta, or Lyapunov-style arguments)?\nIf not, please temper the language or add at least a local linear analysis that shows how the beta step acts as a stabilizer/damping term and under what conditions it improves stability.\n3) Evidence that the update improves sequence-length generalization in CUT3R\nPlease report metrics as a function of time t on sequences substantially longer than the training horizon (e.g., 2–4×). Plot error growth with/without TTT state updates, and show that the slope is reduced. Include both static and dynamic scenes since CUT3R handles both.\nProvide ablations on: number of gradient steps per frame, beta schedule, gradient clipping, choice of loss, and whether backprop-through-time is disabled (it should be, for causality/cost).\nCompare to strong non-TTT baselines: periodic state reset, EMA/shrinkage to s0, learned gating/forget gates, and “burn-in” with keyframes."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XjxycBTsKS", "forum": "aMs6FtNaY5", "replyto": "aMs6FtNaY5", "signatures": ["ICLR.cc/2026/Conference/Submission2119/Reviewer_dBBg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2119/Reviewer_dBBg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881659109, "cdate": 1761881659109, "tmdate": 1762916038431, "mdate": 1762916038431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, TTT3R, introduces a training-free method to improve long-sequence generalization in recent recurrent 3R models (in particular, CUT3R). The task is feed-forward point-cloud reconstruction from a sequence of RGB frames. Prior work on this task includes DUSt3R, VGGT, CUT3R, et al. For long video, feed-forward reconstruction, recurrent transformers like CUT3R are generally considered competitive.\n\nThe contribution can be summarized in one sentence: rather than updating every state token as in CUT3R, TTT3R redesigns a per-token update weight (a “learning rate” in the paper’s test-time-training view) so that only those state tokens that are highly relevant to the current observations receive large updates.\n\nIn summary, the paper presents a training-free, neat idea with useful insights into how to weight state updates. The experiments show that this approach improves CUT3R’s generalization on long sequences. Nice idea, and nice results.\n\nThat said, I think the presentation would benefit from revision. Personally, I wouldn’t frame the method as “test-time training.” The same idea could be described more directly and precisely without introducing new terminology. See details below.\n\nOverall, this is a good paper with a somewhat complex presentation. I would consider accepting it if the authors rephrase certain terms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- A neat and elegant idea. Only a small amount of code modification is needed to alleviate the forgetting problem of CUT3R on long-sequence inputs.\n- The results are impressive. Figures 7, 8, and 9 clearly show that this method achieves significantly better reconstruction quality than CUT3R.\n- The visualizations are well done. Thank you for including Figure 10 and the supplementary webpage."}, "weaknesses": {"value": "The main weakness lies in the presentation.\n\nI would not frame the method as “test-time training.” The same idea could be described more directly and precisely without introducing new terminology. For example, I find Equation (5) unhelpful in conveying the main idea. It introduces gradient notation for a reconstruction error, which requires readers to spend time understanding the notation and the context of TTT. Once readers parse what TTT and Equation (5) mean, the paper essentially presents Equation (6), which is not really related to TTT. I would describe the approach instead as “per-token adaptive weighting of state updates,” which is much more intuitive.\n\nPerhaps I misunderstood something, but in Equation (6), is it truly necessary to introduce the gradient notation?"}, "questions": {"value": "- I am very interested to know whether the authors have tried applying this new updating rule to fine-tune the CUT3R model. Could the performance be further improved?\n- The authors present an insightful design for the weight updating rule. Have they explored alternative approaches to weight updating? I would be interested to learn about that as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "coLYaLoDcC", "forum": "aMs6FtNaY5", "replyto": "aMs6FtNaY5", "signatures": ["ICLR.cc/2026/Conference/Submission2119/Reviewer_jkPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2119/Reviewer_jkPo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973918515, "cdate": 1761973918515, "tmdate": 1762916037740, "mdate": 1762916037740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of catastrophic forgetting and poor length generalization in recurrent 3D reconstruction foundation models. It re-frames the recurrent state update mechanism as an online, test-time training (TTT) problem. The primary contribution is TTT3R, a novel, training-free framework that derives a closed-form adaptive learning rate based on the alignment confidence between the memory state and new observations. This dynamically balances memory retention and adaptation, enabling robust and accurate 3D reconstruction and pose estimation over long image sequences without incurring additional computational or memory costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The re-interpretation of the RNN state update (specifically in CUT3R) as a test-time training process is a novel and insightful conceptual contribution. The derivation of a closed-form, confidence-guided learning rate (Eq. 7) from this perspective is an elegant solution to the problem of balancing state retention and adaptation.\n\n2. The method is training-free and acts as a plug-and-play module for the CUT3R baseline. It achieves significant performance gains, including a 2x improvement in global pose ATE on long sequences. It consistently outperforms other online methods (CUT3R, Point3R) on multiple challenging benchmarks (ScanNet, TUM-D) while, crucially, maintaining the same low memory footprint (6GB) and real-time inference speed (20 FPS) as the baseline.\n\n3.   The paper tackles a highly significant and practical bottleneck in the field. Length generalization is a major limiting factor for applying efficient, streaming 3D reconstruction models to real-world, long-term video sequences for applications like SLAM or AR."}, "weaknesses": {"value": "1. Clarity of Final Update Rule: The presentation of the final, implemented update rule is confusing. Equation (8) merely restates the generic TTT formula. The actual update rule, which appears to be $S_t = S_{t-1} + \\beta_t \\cdot [softmax(Q_{S_{t-1}} K_{X_t}^\\top) V_{X_t}]$ (where $\\beta_t$ is from Eq. 7), is only implied by diagrams (Fig. 4, 5) and the analysis of CUT3R (Eq. 6). The paper would be much clearer if this final equation was explicitly stated.\n\n2. Relation to Gating Mechanisms: The proposed confidence-guided learning rate $\\beta_t$ is functionally very similar to a standard gating mechanism. The paper could be strengthened by a more thorough discussion of the conceptual and practical differences between its TTT-derived update rule and simply adding a more traditional, or even learnable, gating module to the CUT3R state update.\n\n3. Another relevant work, Test3R, also present test-time training based on dust3r series with self-consistency guided adaptation. A discussion and comparison with this method would be necessary to better contextualize the paper's contribution. \n\n[1] Test3R: Learning to Reconstruct 3D at Test Time, NeurIPS 2025"}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "omO4UPaUQD", "forum": "aMs6FtNaY5", "replyto": "aMs6FtNaY5", "signatures": ["ICLR.cc/2026/Conference/Submission2119/Reviewer_yvT7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2119/Reviewer_yvT7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994508863, "cdate": 1761994508863, "tmdate": 1762916037415, "mdate": 1762916037415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}