{"id": "kBo7M7UoCz", "number": 11543, "cdate": 1758201285067, "mdate": 1759897568731, "content": {"title": "Difference-Based Graph Attention Networks: A Dual Attention Mechanism for Similarity and Dissimilarity in Graph Learning", "abstract": "Most Graph Neural Networks (GNNs) rely solely on similarity-based attention\nmechanisms, limiting their ability to distinguish nodes that are structurally similar\nbut semantically distinct. We introduce Difference-Based Graph Attention Net-\nwork (DGAT), a novel architecture that integrates both similarity and dissimilarity\nattention within a unified geometric framework. DGAT models contrastive rela-\ntionships using orthogonal projections and wedge-product approximations, cap-\nturing richer feature interactions beyond alignment. Our formulation is grounded\nin a generalized Iwasawa–Cayley decomposition, where the combination of sim-\nilarity and dissimilarity attention correspond to orthogonal, scaling, and shifting\noperations. We also connect its behavior to discrete analogs of differential opera-\ntors and function orthogonality, establishing a principled geometric interpretation.\nExperiments across homophilic OGB graphs, specially in OGBl-PPA, and het-\nerophilic benchmarks show that DGAT consistently outperforms GAT, GATv2,\nand Graph Transformer architectures, especially in settings requiring fine-grained\nrepresentational contrast or role differentiation.", "tldr": "We employ a second, dissimilarity-based attention, combined with orthogonal projections and Iwasawa decompositions, to enhance dot-product attention predictions.", "keywords": ["Graph Neural Networks", "dissimilarity", "orthogonality"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54b4c1f7f832a615f60083f1aad4f0a99d86d32d.pdf", "supplementary_material": "/attachment/3db5f6e8f2d39a467ac2bac7357f4cf6cee758fb.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Difference-Based Graph Attention Network (DGAT), which is a novel GNN architecture that extends traditional similarity-based attention with an additional dissimilarity-aware attention pathway. DGAT models both similarity (via cosine or additive attention) and dissimilarity (via orthogonal projection and wedge-product approximations) within a unified geometric framework. The authors provide a theoretical grounding based on the Iwasawa-Cayley decomposition from Lie group theory, showing that similarity and dissimilarity components correspond to orthogonal, scaling, and shifting operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduce a dissimilarity-based attention mechanism into graph attention networks for the first time, addressing the long-standing limitation of relying solely on feature similarity. By modeling both similarity and dissimilarity between nodes, the proposed DGAT significantly enhances representation expressiveness. The paper provides a geometric interpretation by linking DGAT to the Iwasawa-Cayley decomposition, offering strong theoretical grounding and interpretability. Moreover, the DGAT framework is highly flexible and extensible, integrating seamlessly with existing architectures such as GAT, GATv2, and Graph Transformers."}, "weaknesses": {"value": "1. The paper lacks a clear and intuitive diagram or framework illustration that helps readers better understand how the proposed approach works.\n\n2. The paper should clarify how the proposed method differs from or relates to graph contrastive learning, which also uses node similarity and dissimilarity.\n\n3. The motivation is unclear, the paper only mentions that previous models ignored node dissimilarity but does not analyze why dissimilarity was not utilized. Moreover, since the experiments include both homophilic and heterophilic graphs, it is not clear which type the study primarily focuses on. Some baseline models are designed for homophilic graphs, so applying them directly to heterophilic settings may raise questions about the fairness and validity of the comparisons.\n\n4. The baseline methods are outdated and lack comparisons with more recent GNN models such as GARN[1]. In addition, the baselines mainly include attention-based architectures, so it is unclear why other classic methods such as GCN[2], GraphSAGE[3], and JKNet[4] were not included for comparison.\n\n[1] Wang Y, Wen J, Zhang C, et al. Graph aggregating-repelling network: Do not trust all neighbors in heterophilic graphs. Neural Networks, 2024, 178: 106484.\n\n[2] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, 2017.\n\n[3] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Proceedings of Advances in Neural Information Processing Systems, 2017.\n\n[4] Xu K, Li C, Tian Y, et al. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning, 2018.\n\n5.The paper lacks an analysis of time and space complexity."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zpvVbhgpb7", "forum": "kBo7M7UoCz", "replyto": "kBo7M7UoCz", "signatures": ["ICLR.cc/2026/Conference/Submission11543/Reviewer_Jp3V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11543/Reviewer_Jp3V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953951334, "cdate": 1760953951334, "tmdate": 1762922636579, "mdate": 1762922636579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a new graph neural network called DGAT. The authors argue that existing attention-based GNNs are built on similarity-based attention mechanism, which is inefficient in capture complex graph information. To this end, the authors develop a new attention module based on orthogonal projections and wedge-prodcut approximations to preserve contrastive relationships between nodes. Experimental results on various datasets show the effectiveness of DGAT on graph data mining tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper is well-organized and easy to follow.\n\n2.The authors provide the theoretical analysis of the proposed method.\n\n3.The proposed DGAT provides new insights for attention-based GRL methods."}, "weaknesses": {"value": "1.The research gap is overclaimed.\n\n2.Mainstream baselines are missing.\n\n3.Some important experiments are missing."}, "questions": {"value": "1.The authors claim the limitation in existing attention-based GRL methods which lack objectivity. There are also several works, such as FAGCN and ACMGNN, which are designed to capture both low-frequency information (similarity) and high-frequency information (dissimilarity) in graph representation learning.\n\n2.In the experiment part, I suggest the authors add more recent graph Transformers as baselines for performance comparison.\n\n3.Moreover, necessary experimental designs such as ablation study and parameter analysis are also required for strengthening the experiment part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "demxrTwnep", "forum": "kBo7M7UoCz", "replyto": "kBo7M7UoCz", "signatures": ["ICLR.cc/2026/Conference/Submission11543/Reviewer_21ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11543/Reviewer_21ta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447814551, "cdate": 1761447814551, "tmdate": 1762922636262, "mdate": 1762922636262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces Difference-Based Graph Attention Networks (DGAT), a novel GNN architecture that extends standard attention mechanisms to jointly capture similarity and dissimilarity between nodes. The method integrates a dual attention pathway, which contains a similarity-based component using cosine or additive attention and a dissimilarity-based component using orthogonal projections and wedge-product approximations. The final representation is obtained via a learned gating mechanism that balances both components. DGAT is grounded in the Iwasawa–Cayley decomposition, offering geometric interpretability connecting orthogonal, scaling, and shifting operations. Experiments are conducted on multiple homophilic (OGBg-MolHIV, OGBn-Proteins, OGBl-PPA, OGBl-DDI) and heterophilic (Minesweeper, Roman-Empire, Amazon-Ratings, Questions) benchmarks, showing consistent performance improvements over GAT, GATv2, and Graph Transformer baselines"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dual-path mechanism and gating function of DGAT address an underexplored limitation of GAT-like models that emphasize only similarity\n\n2. DGAT based on the theoretical grounding via the Iwasawa–Cayley decomposition. This linkage provides interpretability and situates DGAT in a geometric-algebraic context.\n\n3. The authors conduct comprehensive experiments, covering both homophilic and heterophilic benchmarks. The results show the improvements of DGAT compared with baselines."}, "weaknesses": {"value": "1.The authors claim the convergence guarantees of DGAT in the introduction. However, I do not find the specific proof or lemma to quantify this property. If boundedness can lead to convergence, the author needs to clearly point this out and provide a proof.\n\n2.Ablation studies are missing. For example, the selection of $\\lambda$ and similarity v.s. dissimilarity isolation are not empirically separated.\n\n3.Computational overhead and training stability. Claims about efficiency and non‑expansiveness lack runtime or convergence curves."}, "questions": {"value": "1.In Section 1, the authors claim that DGAT is “non-expansive” with “convergence guarantees.” However, no formal proof, theorem, or empirical demonstration is included.\n\nQuestion: Could you please provide the precise mathematical derivation or an outline of the argument that establishes non-expansiveness and convergence guarantees? Are these guarantees derived from the gating mechanism, the orthogonal projection, or both?\n\n2.Eq. (4) introduces different versions of the gating function (i.e., w‑orthogonal, torsion, and default). The manuscript names them but provides no training dynamics or comparison. \n\nQuestion: How do these gating variants quantitatively differ in learned behavior or performance? Have you tested their contributions through ablations, and if not, could you clarify why certain gates are only conceptually presented but not empirically analyzed?\n\n3.I observe that the authors claim the efficiency related to head compensation and GPU-based orthogonal projection (in section A.5), yet no timing results are presented.\n\nQuestion: What is the relative runtime cost of DGAT compared to GATv2 and Graph Transformer? Does the dissimilarity computation introduce additional latency or memory bottlenecks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eeJKG13kqH", "forum": "kBo7M7UoCz", "replyto": "kBo7M7UoCz", "signatures": ["ICLR.cc/2026/Conference/Submission11543/Reviewer_c2Dv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11543/Reviewer_c2Dv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465089992, "cdate": 1761465089992, "tmdate": 1762922635935, "mdate": 1762922635935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DGAT, a dual-path attention layer that couples a standard similarity path with a dissimilarity/orthogonal path computed via Gram-projection / wedge-product surrogates and combined with a learned gate (including an orthogonality-enforcing “w-orth” and an optional torsion/Lie-bracket-inspired variant). Core updates are given in Eqs. (1)–(3) and the gate in Eq. (4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear, principled idea. The orthogonal/difference channel complements similarity attention to encode contrast rather than only alignment; the equations and gate make the mechanism explicit and modular.\n2. Geometric grounding. The Iwasawa perspective provides an interpretable decomposition (K/A/N) mapping cleanly onto (similarity / difference / gating). This is rare in GNN attention papers and helps motivate design choices.\n3. Empirical signal. On OGB tasks and heterophilic benchmarks, DGAT variants are reported to outperform GAT/GATv2/Graph Transformers under matched settings (with a head-count compensation to control params)."}, "weaknesses": {"value": "1. While the geometric story is appealing, some claims (e.g., “non-expansive and convergent operator”) are mentioned in the intro but I did not see full proofs in the provided snippets; if they exist in the appendix, make them crisp with assumptions and operator norms (Lipschitz constants, spectral bounds). (Pointer to tighten: Sections A.3–A.4 already frame energy/orthogonality—turn these into formal theorems.)\n2. The paper introduces w-orth and torsion gates; more ablation would clarify when each helps, sensitivity to λ, and whether improvements persist if gates are simplified. (Some hyperparameter tables appear, but targeted gate ablations would strengthen claims.)\n3. It seems that results are described as best-of-run with std as “difference between best results,” which is non-standard. Prefer mean±std over many seeds."}, "questions": {"value": "1. Can the w-orth constraint hurt when neighborhoods are tiny/noisy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V2sLH4RdVB", "forum": "kBo7M7UoCz", "replyto": "kBo7M7UoCz", "signatures": ["ICLR.cc/2026/Conference/Submission11543/Reviewer_eYdE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11543/Reviewer_eYdE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088315458, "cdate": 1762088315458, "tmdate": 1762922635496, "mdate": 1762922635496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}