{"id": "f70HHKXnEC", "number": 24344, "cdate": 1758355956887, "mdate": 1763124073758, "content": {"title": "Quantifying Information Flow in Diffusion Models: Entropy-Guided Noise Scheduling and Mutual Information Evaluation", "abstract": "Denoising diffusion probabilistic models (DDPMs) and their variants have achieved strong performance across a wide range of tasks, from image restoration to text-to-image generation. Despite these successes, the interplay between timesteps and noise schedules in the diffusion process remains poorly understood. In particular, it is unclear how these factors shape information flow and influence the quality of the final output. This paper investigates diffusion models through the lens of information theory. We introduce an entropy-guided noise scheduling strategy and a mutual-information-based evaluation framework. First, leveraging the differential entropy of Gaussian distributions, we develop a method to compute entropy values of noisy images that are consistent with the diffusion process. Building on this, we design an entropy-guided scheduling strategy to explicitly link timesteps with noise levels during the forward process. Finally, we propose a mutual-information-based evaluation metric to assess the image restoration ability of DDPMs. Experiments on MNIST and Fashion-MNIST demonstrate the feasibility of quantifying and guiding information flow in diffusion models.", "tldr": "This paper introduces an entropy-guided noise scheduling strategy and a mutual-information-based evaluation framework for diffusion models.", "keywords": ["Diffusion Models", "Noise Scheme", "Entropy", "Mutual Information"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0861523fc609a3d13702b5bfeda42dd71abb8a09.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript attempts to analyze diffusion models from an information-theoretic perspective and presents two main contributions: first, it proposes an *entropy-guided noise scheduling* strategy, derived from the differential entropy of Gaussian distributions, as a unified measure of noise levels. This strategy allows for the alignment or mixing of different noise schedules during reverse sampling. Second, it introduces a *mutual information-based evaluation framework* aimed at using mutual information to assess the image restoration capacity of models. The authors validate the feasibility of these methods through experiments on the MNIST and Fashion-MNIST datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The advantage of this paper lies in the use of differential entropy $H(x_t)$ as a unified *scale* to quantify noise levels. As shown in the formula, entropy is a monotonic function of $\\bar{\\alpha}_t$, providing a clear theoretical basis for comparing and aligning different noise schedules (e.g., Linear vs. Cosine) or varying total timesteps (e.g., T=300 vs. T=1000).\n\n2. The paper demonstrates the possibility of *mixing* different noise schedules during a single reverse sampling process. For instance,  starting from a standard Gaussian noise, the image is first denoised using a Quadratic schedule, and then the sampling process switches to a Cosine schedule at an equivalent entropy point, still producing recognizable images. This experiment supports the idea that the state of the diffusion process (defined by entropy or $\\bar{\\alpha}_t$) is more important than the specific noising path taken."}, "weaknesses": {"value": "1. The paper merely uses this framework to align existing schedules, rather than applying information theory principles to design a new, more effective scheduling strategy. \n \n\n2. The experimental validation lacks convincing evidence, as the baseline model performance is relatively poor. Table 6 shows that the FID score of DDPM on MNIST ranges from 118.9 to 550.9, which appears unusually high.\n\n3. While the paper claims that the metrics capture 'image complexity,' no evidence is provided to support that these findings apply to more complex, higher-resolution, color datasets such as CIFAR-10, CelebA, or ImageNet."}, "questions": {"value": "This work provides a valuable investigation into using entropy and information theory to guide and evaluate diffusion models. However, the discussion appears to be missing a comparison to a highly relevant and recent body of work, [1], which is the conference version of the initial submission for the ICLR 2025 paper. It would be helpful if the authors could clarify how their work differs from the following methods:\n\n[1] Li, S., et al., EVODiff: Entropy-aware Variance Optimized Diffusion Inference, NeurIPS 2025. (An earlier version titled Improving Denoising Diffusion with Efficient Conditional Entropy Reduction was submitted to ICLR 2025 in 2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JLPIbfQ996", "forum": "f70HHKXnEC", "replyto": "f70HHKXnEC", "signatures": ["ICLR.cc/2026/Conference/Submission24344/Reviewer_wH5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24344/Reviewer_wH5s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760868802466, "cdate": 1760868802466, "tmdate": 1762943051249, "mdate": 1762943051249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Yq4GMVtSZp", "forum": "f70HHKXnEC", "replyto": "f70HHKXnEC", "signatures": ["ICLR.cc/2026/Conference/Submission24344/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24344/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763124072885, "cdate": 1763124072885, "tmdate": 1763124072885, "mdate": 1763124072885, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an information-theoretic framework for analyzing diffusion models, introducing entropy-guided noise scheduling and mutual-information-based evaluation. The authors derive analytical expressions for computing entropy values of noisy images during the diffusion process and use these to develop a scheduling strategy that links timesteps with noise levels across different schedules. They also propose mutual information as an evaluation metric for assessing image restoration capacity. Experiments on MNIST and Fashion-MNIST demonstrate the feasibility of quantifying information flow and combining different noise schedules based on entropy matching."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "[1] The application of entropy and mutual information to analyze diffusion processes provides a fresh viewpoint on noise scheduling and evaluation.\n\n[2] The entropy-matching approach for combining different noise schedules offers a principled method for hybrid schedule design (Sec 3.2.1-3.2.3).\n\n[3] The paper systematically compares multiple noise schedules (Linear, Cosine, Quadratic, Sigmoid) across different timesteps and provides extensive quantitative results (Tables 2-7).\n\n[4] The derivation of entropy expressions for noisy images (Theorem 3.1, Corollary 3.1) is mathematically sound and well-explained."}, "weaknesses": {"value": "[1] Experiments are confined to MNIST and Fashion-MNIST (28×28 grayscale), which are relatively simple compared to modern diffusion model applications. No results on larger or more complex datasets like CIFAR or ImageNet.\n\n[2] The proposed methods show only small improvements over standard approaches, with some hybrid schedules actually performing worse (Table 7 FID scores).\n\n[3] The entropy computation is straightforward for Gaussian distributions, and the mutual information implementation uses basic histogram methods without addressing the challenges of continuous-valued images.\n\n[4] The evaluation focuses on low-level metrics but lacks analysis of how the proposed methods affect final generation quality or sampling efficiency in practical scenarios."}, "questions": {"value": "[1] How would the entropy-guided scheduling approach scale to higher-resolution color images and more complex datasets?\n\n[2] Could you provide more insight into why some hybrid schedules (like Quadratic-Cosine) perform significantly worse than individual schedules?\n\n[3] Have you considered more sophisticated mutual information estimators (like kernel-based methods) that might work better for continuous image data?\n\n[4] What are the computational overheads of the proposed methods, and how do they affect training and sampling times compared to standard approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e07HrftFHN", "forum": "f70HHKXnEC", "replyto": "f70HHKXnEC", "signatures": ["ICLR.cc/2026/Conference/Submission24344/Reviewer_JSNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24344/Reviewer_JSNp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380050638, "cdate": 1761380050638, "tmdate": 1762943050961, "mdate": 1762943050961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies diffusion models from an information-theoretic perspective by introducing entropy-guided noise scheduling and mutual-information-based evaluation. The authors argue that entropy can quantify information flow during the diffusion process and propose using it to align or combine different noise schedules. Experiments on MNIST and Fashion-MNIST support the feasibility of this analysis."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is clearly written and follows a well-organized experimental procedure.\n* The attempt to interpret diffusion models via entropy and mutual information is conceptually interesting and could inspire further analytical work.\n* The idea of unifying different noise schedules under a common metric may have some heuristic value for schedule design."}, "weaknesses": {"value": "* The key observation—($H(x_t) = \\frac{n}{2}\\ln((2\\pi e)(1-\\bar{\\alpha}_t))$)—is trivial, as it merely reflects the log-scale of predefined Gaussian noise variance. It does not yield any new insight into diffusion dynamics.\n* The entropy differences across noise schedules are expected, since they directly follow from how the schedules are defined.\n* The proposed “entropy matching” and hybrid scheduling are empirically motivated rather than theoretically derived, and their benefit may not generalize beyond toy datasets.\n* Experiments are restricted to MNIST and Fashion-MNIST, limiting the relevance and transferability of the findings to realistic generative modeling setups.\n* Overall, the work reiterates well-known properties of diffusion processes without producing substantive analytical or methodological novelty."}, "questions": {"value": "* Could the authors formalize the theoretical connection (if any) between entropy change and optimal schedule design, beyond empirical matching?\n* Would learning noise schedules adaptively, e.g., by optimizing entropy progression or dataset-dependent information retention, yield more meaningful insights?\n* Have the authors tested whether their entropy-based analysis still holds for other prediction targets (e.g., (x_0), (\\epsilon), or velocity prediction) or higher-resolution datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IZGt21sbpB", "forum": "f70HHKXnEC", "replyto": "f70HHKXnEC", "signatures": ["ICLR.cc/2026/Conference/Submission24344/Reviewer_EXCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24344/Reviewer_EXCn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993717229, "cdate": 1761993717229, "tmdate": 1762943050612, "mdate": 1762943050612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This reads more like an experimental record than an article. It's simply a compilation of experimental data, lacking any explanation of the experiment's motivation, analysis of the results, or clear conclusions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "NA"}, "weaknesses": {"value": "NA"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vUGLJwWCoa", "forum": "f70HHKXnEC", "replyto": "f70HHKXnEC", "signatures": ["ICLR.cc/2026/Conference/Submission24344/Reviewer_yse5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24344/Reviewer_yse5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130641821, "cdate": 1762130641821, "tmdate": 1762943050373, "mdate": 1762943050373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}