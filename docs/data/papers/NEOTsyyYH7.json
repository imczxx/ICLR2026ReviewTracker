{"id": "NEOTsyyYH7", "number": 4308, "cdate": 1757659630860, "mdate": 1759898040508, "content": {"title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control", "abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate in simulation that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model.  This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.", "tldr": "", "keywords": ["reinforcement learning", "world model", "humanoid"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/743093ee1550661e89bc3587e19beeb45124998f.pdf", "supplementary_material": "/attachment/d8ad58511b7e2d4b0102dc612f30569390e4a2cf.pdf"}, "replies": [{"content": {"summary": {"value": "The authors address issues with reinforcement learning control of humanoid robotic systems. They point out that proximal policy optimization can be brittle when transferred to zero-shot settings and is sample-inefficient even in parallel environments. The proposed solution is to use the soft actor-critic algorithm with a learned physics-aware world model to first pre-train a policy on a large-scale, parallelizable simulator and then finetune online using synthetic samples generated by the world model for the target humanoid task. The authors demonstrate that their pretrained policy can be deployed zero-shot in the real world and performs better during finetuning on simulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "> The authors provide a fast, scalable implementation of SAC, which opens up new avenues for research on the limitations of online reinforcement learning\n> The authors conduct extensive studies to verify the efficacy of their algorithm. I appreciated their use of 8 seeds, which is much better than the typical practice in other deep RL research (as few as three seeds in most works I have reviewed). The inclusion of zero-shot transfer in real robotics was also a promising finding, although no experiments were conducted to finetune on real-robot data.\n> There are interesting findings to this work, such as identifying the limitations of prior research on plasticity, finding that the pretraining algorithm does affect finetuning performance."}, "weaknesses": {"value": "Our biggest concern is the quality of organization and writing in the current submission. A good example of this is the exclusion of results to answer Q3 in the experiment section. If the paper had reduced content in other sections, it would have been easier to include them in the main paper. For example, Comments on design choices in the method section can be moved to the appendix, such as those about the experimental evidence used to justify decisions, which should be explained in the Q3 results (lines 256 & 257). The introduction could be more coherent and more transparent about why physics-aware world models are being employed.\n\nIn terms of contribution, this type of work might have more impact at a robotics-focused conference. The authors' method seems promising as a pretrain-and-finetune solution for humanoids, but it offers limited novelty beyond addressing issues with SSRL, which their work is heavily influenced by. There are aspects of the work that challenge this comment, such as their finding that issues of plasticity (lines 223-227) do not arise when scaling the parallel environments, but this is not emphasized in the main experiments and is left as future work. \n\nThe experiment section demonstrates that the framework works and identifies which hyperparameters the model is sensitive to. I did not find the results shown for Q1 particularly insightful, because all that is shown is that the authors do \"just as well\" as other methods. If there are other benefits, this should be emphasized; otherwise, Q1 could be moved to the appendix or condensed to make space for more critical analysis of LIFT. Q2 & Q3 deserve more discussion or consideration, particularly as Q2 is the application scenario the authors are studying, and Q3 helps the reader understand which hyperparameters are essential for applying LIFT. \n\n\n\nWriting opinions \n\nLine 063: The last sentence in the opening paragraph doesn't logically follow from the limitations of PPO per se, particularly because the \"pretrain-finetune paradigm\" isn't explained as to WHY it solves PPO issues. Maybe something along the lines of \"off-policy algorithms are a means of addressing this\" would make more sense given the following paragraph.\nLine 065: The author uses \"However\" twice in this paragraph.\n\nLine 076: \"we conduct a preliminary study in which humanoid walking is trained from scratch while limiting data collection to deterministic execution.\"\n> This sentence should be deleted. When I read this in the introduction, my first question was whether the research was complete. That's probably not what the authors meant, but that is how it came across.\n\nLine 082: (ii) The use of physics-informed world models is not motivated well in the introduction, so from the reader's perspective, this comes out of nowhere as to why these are used over non-physics-aware models. \n\nLine 238: The information on the state vector could be moved to the appendix."}, "questions": {"value": "Q0: Could the author's expand on the following interpretation (Line 416): . PPO maintains reasonable performance initially but degrades over time, eventually collapsing, likely due to its KL-regularized updates that limit fast policy adaptation? Did the author's include the KL divergence between the current policy and old policy? What about the clip coefficient? \n\nQ1: For Q1, if your algorithm only does just as well as baselines, what inherent advantage does LIFT provide that prior methods do not have for the pretraining phase? I'm thinking about algorithmic benefits, not the transfer learning aspect (i.e. Q2).\n\nQ2: Line 211 - Are the authors actually using a Gaussian policy, or is there a tanh activation transformation to normalize actions? This transformed distribution was used in the original SAC. \n\nQ3: How much of the world model is learned as opposed to being defined by the BRAX? What simulator is used to pretrain the model? Is it re-using the same tools as the world model simulator? Is there any issue of mismatch between the pre-trained simulator and the target environment? \n\nQ4: Why is SSRL not included as a comparison in the reported results of the paper for Q1 or Q2? The author's algorithm appears heavily motivated by this prior work, yet no results appear in the main paper.\n\nQ5: What are the reasons for not comparing LIFT with a physics-informed model vs a non-physics-informed model?\n\nQ6: What are the potential limitations of LIFT for high-dimensional observation data? What changes would be needed to use images, for example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Wk7yUQE7o", "forum": "NEOTsyyYH7", "replyto": "NEOTsyyYH7", "signatures": ["ICLR.cc/2026/Conference/Submission4308/Reviewer_2rhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4308/Reviewer_2rhQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605503794, "cdate": 1761605503794, "tmdate": 1762917288275, "mdate": 1762917288275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LIFT, a three-stage framework that bridges large-scale off-policy pretraining and efficient safe finetuning for humanoid robot locomotion. The key pipeline involves: (i) large-scale pretraining of policies using Soft Actor-Critic (SAC) in massively parallel simulators, (ii) pretraining a physics-informed world model leveraging Lagrangian dynamics and residual neural predictions on stored transitions, and (iii) iteratively finetuning both the policy and world model in new environments using deterministic real-world execution and stochastic exploration confined to the model. Experiments on multiple humanoids and transfer scenarios indicate that LIFT achieves competitive pretraining performance and robust, efficient finetuning—outperforming prior model-free and model-based RL baselines under challenging adaptation conditions. The code is open-sourced."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed pipeline is systematically motivated, decomposing the challenge of bridging data-hungry RL pretraining with safety and efficiency in real-world adaptation. \n- The integration of Lagrangian dynamics with learned residuals for improved world model rollouts is novel in the humanoid domain and addresses both sample efficiency and stability concerns. \n- The work provides results for two complex humanoid platforms (Booster T1 and Unitree G1), encompassing flat and rough terrain, various velocity targets (in-distribution, long-tail, and out-of-distribution), and sim-to-sim as well as sim-to-real transfers. Notably, the policy achieves zero-shot deployment and robust adaptation."}, "weaknesses": {"value": "- **Task Diversity**: The evaluation focuses on forward locomotion tasks (varying speed targets on flat or rough terrain). This is a narrow slice of humanoid skills. The significance would be further bolstered by testing, say, different locomotion gaits or disturbances, or tasks like turning, obstacle avoidance, etc. It’s unclear how readily the approach extends to non-locomotion tasks or more complex objectives.\n- **Use of Privileged Information**: The method relies on a privileged state (including full physical state and other quantities like ground friction) for training the world model and critic. This is standard in sim-to-real pipelines, but it presumes access to those privileged observations. The paper doesn’t fully discuss how feasible this is outside simulation – e.g., obtaining exact friction or global state on a real robot might require estimation. If such info is unavailable or inaccurate, it could affect performance. The authors do not address this point in depth, and it could be a concern for deploying the finetuning part in the wild.\n- **Comparison to Other Model-Based RL**: The paper compares to MBPO as a baseline for learned dynamics, but other model-based approaches (e.g., Dreamer or recent model-based fine-tuning methods) are not explicitly compared in finetuning. Given that DreamerV3 and others have shown strong model-based learning on locomotion, an empirical comparison or at least a discussion would have been useful to position LIFT against the broader landscape of model-based RL in terms of performance and efficiency."}, "questions": {"value": "1. Can the authors clarify their design choices regarding the length and scaling of the autoregressive unroll horizon in world model training? How sensitive is LIFT to this hyperparameter in practice?\n2. In real-robot adaptation, what additional safety or recovery mechanisms (beyond physics-consistent reset) would be paramount, and how does LIFT plan to address distributional shift that may be more severe than sim-to-sim transfer?\n3. Is there a plan or preliminary evidence for multi-task or non-locomotion humanoid adaptation under LIFT, or does the framework generalize only to velocity-tracking locomotion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RZdHTCrNkz", "forum": "NEOTsyyYH7", "replyto": "NEOTsyyYH7", "signatures": ["ICLR.cc/2026/Conference/Submission4308/Reviewer_JXM1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4308/Reviewer_JXM1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932744682, "cdate": 1761932744682, "tmdate": 1762917287885, "mdate": 1762917287885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LIFT, a three-stage framework to bridge the gap between large scale pre-training and sample-efficient fine-tuning for humanoid locomotion control.\n\nThe framework first challenges the consensus that PPO is the default for large-scale training. It demonstrates that off-policy SAC, when implemented in JAX with large-batch updates and a high Update-To-Data (UTD) ratio, can achieve comparable or faster wall-clock convergence than PPO for pre-training, while also enabling zero-shot sim-to-real.\n\nThe authors then propose a model-based fine-tuning stage, where a physics-informed world model (combining Lagrangian dynamics with a learned residual network) is pre-trained on the SAC simulation data. In stage (iii), this world model is adapted to a new environment. The robot collects rollout data in the target environment using a deterministic policy for safety. This new data is used to fine-tune the world model. Stochastic exploration is then confined to rollouts within this updated world model, and the policy is updated using this safe, synthetic data.\n\nThe authors validate this approach in \"sim-to-sim\" experiments, showing that the LIFT framework can successfully fine-tune policies for new, out-of-distribution tasks (e.g., new velocity commands in a new simulator) where model-free fine-tuning of SAC, PPO, and FastTD3 all fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper provides a valuable finding by demonstrating that SAC, an off-policy algorithm, can be successfully scaled for massively parallel pre-training of humanoid controllers.  The open-sourcing of a JAX-based SAC implementation for this purpose is a welcome contribution.\n* The proposed hybrid physics-informed world model works and the target environment adaptation works well for humanoid simulation. The ablation study in Appendix A.3 (Fig. 6) shows that a standard \"black-box\" MBPO-style model completely fails to adapt, while LIFT's physics-informed model succeeds. This provides strong evidence that embedding physics priors is essential for world-model-based adaptation in this domain.\n* The LIFT framework's main contribution is its integration of pre-training with a model-based fine-tuning loop. The proposed world-model finetuning framework could be used in more humanoid finetuning cases."}, "weaknesses": {"value": "* Lack of Real-World Fine-Tuning (Sim-to-Sim): The paper's primary weakness is that its core claim—safe, efficient fine-tuning—is only validated in a \"sim-to-sim\" setting (MuJoCo to Brax). While the paper shows zero-shot *pre-training* on a real robot (Appendix A.6), it does not \"close the loop\" and test the *fine-tuning* procedure in the real world. The claim of \"safety\" is significantly weaker without this, as collecting even \"deterministic\" data in a new real-world environment carries risks (e.g., from initial state distribution mismatch or model error) that the paper does not address.\n\n* Missing Discussion on Safety for Real-World Adaptation: Following W1, the paper overlooks the practical hardware challenges of data collection for fine-tuning. Even a deterministic policy can fail. Methods like RTR [1] have shown that specially designed safety systems (e.g., a \"teacher\" robot arm) are crucial for enabling this kind of real-world adaptation on humanoids. A discussion of these practical requirements is missing.\n\n* Incomplete Related Work and Missing Citations: The related work section is sparse in several key areas, weakening the paper's positioning and claims of novelty. (a) Context for SAC Pre-training: The paper's first contribution is scaling SAC for pre-training. However, it fails to cite work that specifically discusses the *challenges* of scaling SAC in massively parallel simulators and its typical instability compared to PPO like [2]. (b) Model-Based Fine-Tuning Landscape: The paper's core contribution is a model-based fine-tuning method, but it omits citing some other model-based adaptation strategies, such as learning residual \"delta action\" models (e.g., ASAP [3]) or other concurrent humanoid world-model papers (e.g., Hu et al., 2025 [4]). I believe a whole paragraph of this line of related work is needed.\n\nReferences:\n\n[1] Hu, Kaizhe, et al. \"Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids.\" Conference on Robot Learning (CoRL). 2025. \n\n[2] Raffin, Antonin. \"Getting SAC to Work on a Massive Parallel Simulator...\" Blog Post. 2025. \n\n[3] He, Tairan, et al. \"ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills.\" Robotics: Science and Systems (RSS). 2025. \n\n[4] Xinyang Gu, et al. \"Learning Humanoid Locomotion with World Model Reconstruction.\" Robotics: Science and Systems (RSS). 2024."}, "questions": {"value": "The major questions are addressed in the weakness section. Here are some additional questions:\n\n*  What are the primary practical barriers that prevented the authors from attempting the stage (iii) *fine-tuning* loop on the real robot? How would you propose to manage the safety of the initial \"deterministic\" data collection in the real world?\n* In stage (iii), the world model and the policy are fine-tuned concurrently (Sec 4.3). How are these updates interleaved? Is there a risk of the policy overfitting to an inaccurate, partially-fine-tuned world model, and how is this instability managed (e.g., how is the world model rollout horizon scheduled)?\n* In Section 4.1, the paper states it relies on SAC's state-dependent variance for exploration. How critical was this choice during pre-training compared to a simpler state-independent variance? Did this choice have any downstream effects on the fine-tuning stage (e.g., better exploration *within* the world model)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xoe5ysWiVm", "forum": "NEOTsyyYH7", "replyto": "NEOTsyyYH7", "signatures": ["ICLR.cc/2026/Conference/Submission4308/Reviewer_2CYj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4308/Reviewer_2CYj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994780720, "cdate": 1761994780720, "tmdate": 1762917287672, "mdate": 1762917287672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}