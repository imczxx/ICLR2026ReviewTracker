{"id": "JPLRtQINNy", "number": 8980, "cdate": 1758105598538, "mdate": 1759897750278, "content": {"title": "Domain Bridging: Enabling Adaptation without Peeking at Target Data", "abstract": "Adapting models to target domains with proprietary data remains a challenging problem. One possible setup to enable adaptation is to allow target domain owners to privately evaluate candidate models on their own data. For example, model providers consider how to adjust models to better fit the unseen target data, relying solely on returned model performance. Existing methods adopt Zeroth-Order (ZO) optimization to refine model parameters or employ a two-stage learning process that first identifies the target-related samples in the source data and then retrains the model. However, we find that these methods struggle to generalize well for the target tasks during inference, primarily because of the failure to account for data-statistical shifts between source and target domains. To address this limitation, we introduce the concept of domain bridging in the context of model adaptation for proprietary target data. The core idea is to bridge the domain gap by learning target-aligned perturbations on source data, enabling the fine-tuned model to achieve better performance on target domains. A natural attempt is to extend ZO optimization to this setting. However, this approach fails to produce reliable perturbations on real datasets. To address this, we design a target-aligned, sample-wise perturbation learner, enabling reliable adaptation from performance-only feedback. We provide theoretical convergence guarantees and demonstrate through experiments on five datasets across image and text modalities that our domain bridging method achieves state-of-the-art performance, improving accuracy by approximately 4\\%.", "tldr": "Domain Bridging introduces an efficient framework that learns source data perturbations to bridge domain gaps, enabling effective model fine-tuning for target domains without requiring direct access to proprietary target data.", "keywords": ["domain bridging", "evaluation-based adaptation", "zeroth-order optimization", "proprietary target data"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91d535950cee1688cbd85ca498b8c8316925af32.pdf", "supplementary_material": "/attachment/416e0678fb813e63667363d45584a0cd79908874.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles evaluation-based domain adaptation when target data are inaccessible. Instead of updating model weights directly from performance-only feedback or using a 2-stage 'value then retrain' method, the authors propose Domain Bridging, learn sample-wise perturbations of the source data that make standard fine-tuning on the perturbed source behave as if training on the unseen target. They provide an efficient bi-level procedure with a tailored gradient estimator, prove convergence under mild assumptions, and show consistent gains across multiple vision/language benchmarks, with improved sample-efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Thank you for your inspiring work. The main ideas were sound, and the paper delivered them very well. Below are some of the strengths I wish to underline.\n\n- The paper has clear motivations (privacy-aware adaptation) and a compelling idea to address realistic issues in evaluation-based DA.\n- The proposed method is sound, and its effectiveness is supported by empirical results. \n- The paper is well-written and easy to understand \n- Clear Positioning: I appreciate the author's effort in adding Appendix A. Research Position, which concisely captures how the paper relates to other works."}, "weaknesses": {"value": "Below, I have listed some suggestions that would strengthen the paper.\n\n\n- Statistical Stability: In Sec 4.1. The authors claim that the results of the 'best performance from 10 ind. runs' were reported. I believe that for a fair comparison, the mean performance and its standard error/deviation across #N runs should be reported. Could you please report them?\n- The theoretical analysis is sound. However, it relies on strong assumptions (strong convexity and a bounded Hessian), which usually do not hold for most modern deep networks. Could you elaborate on this assumption? Alternatively, the authors could show experiments on small, linear models or provide surrogate approximations. \n- Baseline comparisons with DG: I also looked at Appendix E. (comparisons on DG and sDG). However, since DG and sDG do not have target domain access, the comparisons in Tab. 11 & 12 are of less significance (as DA has target domain access). For a fair comparison, the DG/sDG methods should be evaluated with a setting that has a matching feedback budget.\n- Hyperparameter Tuning: How were the hyperparameters chosen? In Line 336, you mentioned following the setting in Liu et al., 2018a, but the authors have not shown how changes in $ùúâ, Œ≥, œµ, Œ∑, Œ¥$ affect overall performance. Please see Questions."}, "questions": {"value": "Please refer to the weaknesses for additional questions.\n\n- Step Sizes Assumption: The proof assumes diminishing step sizes; however, and correct me if I'm wrong, but experiments appear to use fixed param lr ùúâ and perturbation lr. Could you explain if these experimental setting aligns with the theory (diminishing schedule).\n \n- Scability & Architectural Adjustments: Do performance gains and query efficiency persist on larger backbones (e.g., larger ResNet variants) or in different architectures (e.g., vision transformers) -- this is of low priority\n\n- Hyperparameters: We want to see how the changes in hyperparameters affect the performance (e.g., Target holdout acc., support-holdout gap, and query efficiency)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qfpgJjeXZp", "forum": "JPLRtQINNy", "replyto": "JPLRtQINNy", "signatures": ["ICLR.cc/2026/Conference/Submission8980/Reviewer_Eamd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8980/Reviewer_Eamd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841254549, "cdate": 1761841254549, "tmdate": 1762920711475, "mdate": 1762920711475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Domain Bridging (DB), a novel method for adapting pre-trained models to proprietary target domains without direct data access. The core idea is to learn sample-wise perturbations on the accessible source data, guided solely by performance feedback (e.g., accuracy) from the unobserved target domain. This process steers the model's feature representations to better align with the target domain. The authors propose an Efficient Domain Bridging (EDB) algorithm that addresses the limitations of direct Zeroth-Order optimization by using a more reliable gradient estimation within a bi-level optimization framework. Experiments on image and text classification tasks show that EDB achieves state-of-the-art performance, improving accuracy by approximately 4% over existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper presents a novel concept of domain bridging via source data perturbation. This approach effectively narrows the representation gap between source and target domains without violating data privacy, offering a fresh perspective in evaluation-based model adaptation.\n\n2. The method demonstrates consistent and significant improvements across multiple datasets (Office-31, Office-Home, PACS, VLCS, Amazon Review) and modalities (image, text), validating its robustness and generalizability. It also shows faster convergence and better query efficiency compared to baseline methods."}, "weaknesses": {"value": "1. The EDB method depends on a zeroth-order estimator for gradients, which can be noisy and less precise than true gradients. The performance gap between EDB and its variant with exact gradients (EDB) indicates that approximation errors limit the method's full potential.\n\n2. While EDB converges faster than baselines, the process of learning sample-wise perturbations for the entire source dataset within a bi-level optimization framework is inherently complex and could lead to higher computational costs per iteration, especially with large-scale source data.\n\n3. The adaptation process is highly reliant on the performance feedback from the target domain. The paper shows that performance degrades with noisy feedback, suggesting the method might be vulnerable to imperfect or adversarial feedback in real-world scenarios.\n\n4.  The robustness analysis against noisy feedback, while valuable, uses simulated Gaussian noise added to the performance metric. The paper would be more convincing if it tested the method against more realistic noise types, such as label noise in the target domain's support set or non-IID noise distributions that might occur in real-world data. \n\n5. This paper lacks the analysis of perturbation interpretability. The paper correctly notes that the learned perturbations do not visually resemble the target data, which is a privacy feature. However, a deeper analysis of what these perturbations represent or how they correlate with specific domain shift characteristics (e.g., style, texture) would provide valuable insights into the mechanistic interpretation of the \"bridging\" process, moving beyond quantitative distance metrics like MED.\n\n6.The experiments are conducted on standard academic benchmarks (e.g., Office-31, Office-Home). Although the paper mentions that perturbations can be computed in parallel, it does not fully demonstrate the method's scalability and computational efficiency on larger, more complex datasets (e.g., DomainNet, VisDA 2017) or with larger model architectures (ViT). A more thorough scalability analysis would strengthen the claim for practical deployment."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "76ED0xWyOo", "forum": "JPLRtQINNy", "replyto": "JPLRtQINNy", "signatures": ["ICLR.cc/2026/Conference/Submission8980/Reviewer_Ed7R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8980/Reviewer_Ed7R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962126972, "cdate": 1761962126972, "tmdate": 1762920711113, "mdate": 1762920711113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a domain bridging framework for adaptation across domains with distributional shifts, where target domain data are proprietary and cannot be directly accessed, using only performance feedback from the target side. This paper proposes an Efficient Domain Bridging (EDB) algorithm that solves a bi-level optimization problem, which (1) finds the best sample-wise perturbation on the source data by optimizing the target domain owner feedback, using an approximated gradient method; and (2) fine tune the model parameters by minimizing the loss on the perturbed source data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a domain bridging framework for model adaptation with proprietary target data.\n    \n2. The method is evaluated across diverse benchmarks (both image and text).\n    \n3. The presentation is generally clear."}, "weaknesses": {"value": "1. **Presentation and notation clarity.**  \n    Several parts of the paper, particularly Section 2.2, lack sufficient clarity in presentation. The description of Retraining with Source Data Valuation (RSDV) is ambiguous. For example, it is unclear what $\\phi_{\\pi^t[i]}$ represents, and whether $\\phi_{\\pi^t[i]}$ and $\\phi_{\\pi^{t-1}[i]}$ correspond to the same data point. Similarly, the definitions of $V(\\theta^t_{i})$ and $V(\\theta^t_{i-1})$ are not explicit? Furthermore, the paper should clarify whether the reweighted loss indeed uses $\\phi$ values as sample weights. In addition, the notation $\\hat{\\ell}$ introduced in line 250 does not appear in Eqs. (5)‚Äì(6), which disrupts the consistency of notation.\n    \n2. **Conceptual positioning and insufficient comparison.**  \n    The proposed framework appears conceptually closer to zeroth-order (ZO) fine-tuning and adversarial or robustness-oriented perturbation methods than to conventional domain adaptation. The idea of ‚Äúdomain bridging‚Äù largely combines these existing techniques and applies them to the scenario where target domain data are proprietary and inaccessible. Because of this hybrid nature, the current comparisons, which focused mainly on ZO and RSDV baselines, are not sufficient to convincingly demonstrate the uniqueness or superiority of the proposed method.\n    \n3. **Lack of hyperparameter sensitivity analysis.**  \n    The paper does not report how sensitive the results are to hyperparameters in Algorithm 1."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tSIq7SSmfc", "forum": "JPLRtQINNy", "replyto": "JPLRtQINNy", "signatures": ["ICLR.cc/2026/Conference/Submission8980/Reviewer_RAvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8980/Reviewer_RAvJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963769829, "cdate": 1761963769829, "tmdate": 1762920710741, "mdate": 1762920710741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}