{"id": "Acb2tZ5jR6", "number": 7801, "cdate": 1758036777520, "mdate": 1763698148533, "content": {"title": "Point Bridge: 3D Representations for Cross Domain Policy Learning", "abstract": "Robot foundation models are starting to realize some of the promise of developing\ngeneralist robotic agents, but progress remains bottlenecked by the availability of\nlarge-scale real-world robotic manipulation datasets. Simulation and synthetic data\ngeneration are a promising alternative to address the need for data, but the utility\nof synthetic data for training visuomotor policies still remains limited due to the\nvisual domain gap between the two domains. In this work, we introduce POINT\nBRIDGE, a framework that uses unified domain-agnostic point-based representa-\ntions to unlock the potential of synthetic simulation datasets and enable zero-shot\nsim-to-real policy transfer without explicit visual or object-level alignment across\ndomains. POINT BRIDGE combines automated point-based representation ex-\ntraction via Vision-Language Models (VLMs), transformer-based policy learning,\nand inference-time pipelines that balance accuracy and computational efficiency\nto establish a system that can train capable real-world manipulation agents with\npurely synthetic data. POINT BRIDGE can further benefit from co-training on small\nsets of real-world demonstrations, training high-quality manipulation agents that\nsubstantially outperform prior vision-based sim-and-real co-training approaches.\nPOINT BRIDGE yields improvements of up to 44% on zero-shot sim-to-real trans-\nfer and up to 66% when co-trained with a small amount of real data. POINT\nBRIDGE also facilitates multi-task learning. Videos of the robot are best viewed at:\nhttps://pointbridge-anon.github.io/", "tldr": "We introduce Point Bridge, a framework that uses unified domain-agnostic point-based representations to unlock the potential of synthetic simulation datasets and enable zero-shot sim-to-real policy transfer.", "keywords": ["Imitation Learning", "Sim-to-real", "Multitask Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/593ab31bd1a9d0f3502f39833dba684bf7efa4a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **Point Bridge**, a framework that bypasses the visual domain gap by learning policies on a unified, domain-agnostic 3D point-based representation. Instead of training on raw pixels, the policy operates on 3D point clouds representing the task-relevant objects and the robot's end-effector.\nThe method achieves zero-shot sim-to-real transfer and significantly improves the task success rate over the baseline using raw pixel inputs. They also show that the performance could be further boosted by co-training with a small amount of real-world data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear, Well-motivated Idea**: Using compact 3D keypoint representations to move the sim/real gap from image appearance to a geometric representation is intuitive and practically appealing. The idea connects naturally to data-generation systems like MimicGen that scale up simulated demonstrations. \n\n2. **Strong Empirical Results**: Extensive real-robot rollouts, zero-shot sim-to-real experiments, and sim+real co-training analyses with multiple ablations give evidence the approach improves success rates compared to an image-based baseline.\n\n3. **Clarity**: The paper is exceptionally well-written and easy to follow."}, "weaknesses": {"value": "1. **Dependence on Foundation Models**: The perception pipeline is composed of multiple foundation models (Gemini, Molmo, SAM2, and Foundation Stereo) without a verification mechanism. The performance may be affected if any model fails.\n\n2. **The Domain Gap is Shifted, Not Removed**: By discarding RGB and using only geometry, the method reduces appearance mismatch but increases reliance on accurate depth and occlusion handling. Physics and contact dynamics gaps between sim and real remain, so claims about “closing the sim-to-real gap” should be tempered.\n\n3. **Limited Task Scope**: The experiment tasks are mostly pick-and-place tasks without complex dynamic, geometry, and environment constraints. Although the paper includes \"Put bowl in oven\" as the articulation task, the robot does not seem to learn the articulation to close the oven door in Figure 3 and the video.\n\n4. **Weak Baseline Comparison**: The image-based baseline is trained on a dataset generated from MimicLab, whose visual rendering gap is notably bigger than the current state-of-the-art simulators (e.g., Issac Sim). The baseline model does not fully demonstrate the limit of image-based sim-to-real transfer. There is also a missing direct empirical comparison to other keypoint/point-based representations or the closest prior works that use learned keypoints or structured geometric inputs.\n\n5. **No Failure Analysis**: Results are reported as success counts without a structured failure taxonomy, which helps understand robustness and reproducibility."}, "questions": {"value": "1. The policy uses only 3D keypoint positions without RGB. Does this create ambiguity for more complex tasks? How does this design handle tasks that require appearance cues (e.g., sorting by color or distinguishing visually similar objects with the same geometry)? Prior work [1] indicates that combining visual features with geometry can help. Could you elaborate on this?\n\n2. What are the independent failure modes and success rates of the perception pipeline? How do compounded errors (e.g., Gemini failing to identify the correct object, SAM2 failing to segment, or FoundationStereo producing noisy depth) propagate to the policy? Is it possible to develop a verification mechanism to make it more robust?\n\n3. Does the current perception pipeline really capture \"task-relevant\" keypoints? The current method seems to be simply sampling all points on the task object without understanding the task. For instance, in the task \"closing the drawer\", the points on the handle and the corners of the drawer should be enough to solve the task and infer the articulation. Would a small set of semantically relevant keypoints perform as well or better? How does the method generalize to variations in object geometry (wider/taller/deeper drawers) without infinitely creating more assets in the simulation?\n\n4. Since the model uses geometry only, what specific benefits does a small amount of real data provide? Is the improvement due primarily to the exact same geometry of the object?\n\n**Typos**:\n1. In Table 2’s caption, should “single task” be “multitask” (line 373)?\n2. The order of Table 3 and Table 4 appears inverted.\n\n[1] Fang, Xiaolin, et al. \"KALM: Keypoint Abstraction Using Large Models for Object-Relative Imitation Learning.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VVWpidet7M", "forum": "Acb2tZ5jR6", "replyto": "Acb2tZ5jR6", "signatures": ["ICLR.cc/2026/Conference/Submission7801/Reviewer_h3iK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7801/Reviewer_h3iK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555684091, "cdate": 1761555684091, "tmdate": 1762919844267, "mdate": 1762919844267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Point Bridge, a novel framework designed to address the sim-to-real transfer bottleneck in robotic manipulation. The core innovation is the use of a unified, domain-agnostic point-based representation to bridge the visual domain gap. By leveraging VLMs for automated extraction of task-relevant 3D keypoints, Point Bridge distills observations into a compact point cloud, upon which transformer-based policies are trained. The paper demonstrates that this approach enables effective zero-shot sim-to-real transfer using purely synthetic data from tools like MimicGen. Furthermore, it shows that performance can be substantially enhanced through co-training with small amounts of real-world data and that the framework naturally facilitates multitask learning. Extensive real-world experiments on several manipulation tasks report significant improvements over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The central idea of a point-based representation is both powerful and elegant. It directly attacks the problem of visual domain gap by moving away from pixel-level inputs to a more geometric abstraction. This is a more scalable approach than striving for photorealistic simulation.\n\n2.The framework presents a complete and highly automated pipeline. It intelligently integrates synthetic data generation, VLM-guided scene filtering, and modern policy learning into a cohesive system. The automation of point extraction, removing the need for manual annotation, is a particularly crucial contribution for practical adoption.\n\n3.The experimental validation is thorough and compelling. The evaluation covers not only zero-shot transfer but also co-training and multitask scenarios, showcasing the flexibility of POINT BRIDGE. The large number of real-world evaluations lends significant credibility to the results. The ablation studies on depth estimation strategies and the importance of camera-aligned point sampling in simulation provide valuable insights into the system’s engineering nuances."}, "weaknesses": {"value": "1.POINT BRIDGE exhibits a strong dependence on external pre-trained vision models. The entire pipeline’s entry point relies on models like Gemini and SAM2. Consequently, the robustness of POINT BRIDGE is inherently tied to the performance of these components, and any failures in perception cannot be easily corrected within the framework itself.\n\n2.The framework relies on assumptions about a calibrated scene with known camera intrinsics and extrinsics. This requirement for a consistent reference frame might limit deployment in more dynamic setups where camera poses are not fixed or precisely known.\n\n3.The abstraction into point clouds, while beneficial for generalization, can lead to a loss of critical scene context. The paper itself notes that this can limit performance in cluttered environments, as fine-grained visual details or contextual cues necessary for disambiguation might be discarded."}, "questions": {"value": "1.Could you provide more detail on the failure cases and robustness of the VLM-guided pipeline? For instance, how often did the initial object identification or the subsequent segmentation with SAM2 fail or produce inaccurate results in your real-world trials? A discussion of common failure modes and whether the system has any inherent mechanisms to detect or mitigate them would be very helpful.\n\n2.The choice of 128 points per object is noted. Was this parameter systematically ablated? It would be interesting to know if there is a point of diminishing returns or if certain tasks benefit from a different number of points. Furthermore, was any sampling strategy beyond uniform sampling explored that might better capture object geometry for dexterous manipulation?\n\n3.The paper repeatedly emphasizes minimal visual and object-level alignment. To better understand the boundaries of this claim, could you clarify what minimal object alignment entails? Does it allow for transferring policies between objects of different categories with entirely different geometries, or does it assume functional and rough geometric similarity? Showcasing results on a task with extreme object shape variation between sim and real would powerfully reinforce this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ik8x9zPlGa", "forum": "Acb2tZ5jR6", "replyto": "Acb2tZ5jR6", "signatures": ["ICLR.cc/2026/Conference/Submission7801/Reviewer_7sFb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7801/Reviewer_7sFb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910192616, "cdate": 1761910192616, "tmdate": 1762919843860, "mdate": 1762919843860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript Point Bridge proposes to use a 3D point cloud representation as the basis of a robotic policy. By utilizing this approach the authors are able to bride training on simulation data with deployment on real robots. The advantages of simulation data are utilized by the use of MimicGen, increasing the size of the training dataset substantially. On real data, object- and robot-centric point clouds are generated using multi-view stereo and SAM2 for object segmentation. Performance is evaluated in simulation and on real data by comparing to an image-only baseline, showing substantial improvements on the same amount of training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The method shows good transfer from simulation training to real-world deployment.\n- The proposed pipeline is well-engineered, utilizing powerful open-vocabulary models likely capable of generalization to broader scenarios."}, "weaknesses": {"value": "- The work has very limited novelty. Point cloud and point track representations have been used in numerous previous works (as cited by the authors), for specialist policies as well as of generalist VLA models. While these works do not explicitly target the sim2real problem, they show capable policies on simulation data, human demonstrations and real robot demonstrations. In this context, especially human demonstration data also represents a domain transfer problem.\n- The work does not compare to any point cloud and point track-based method. Such a comparison could demonstrate the potential advantage of the proposed pipeline over the baseline methods on the sim2real problem.\n- The method requires a calibrated depth/stereo camera setup, with no change between training and inference. The authors do not evaluate the sensitivity of calibration change between training and deployment.\n- Minor: Tables 1/2 have incorrect captions, with both being labeled as single task."}, "questions": {"value": "- A list of differences w.r.t. the most important point tracking works would help the reader to position the work's contributions.\n- A benchmark against these methods in the domain transfer case would show the advantages of the proposed method.\n- It is unclear how strongly the method depends on a match of camera calibration between training and inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JU8CilKbFX", "forum": "Acb2tZ5jR6", "replyto": "Acb2tZ5jR6", "signatures": ["ICLR.cc/2026/Conference/Submission7801/Reviewer_g2VG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7801/Reviewer_g2VG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927114087, "cdate": 1761927114087, "tmdate": 1762919843462, "mdate": 1762919843462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes POINT BRIDGE, a cross-domain policy learning framework that centers on “task-relevant points” (keypoints / sparse point clouds) as a unified 3D representation for sim-to-real transfer. The system uses a VLM-guided pipeline (segmentation + depth/reconstruction) to extract task-relevant 3D points (via FoundationStereo / RGB-D / stereo triangulation / tracking), then feeds them to a PointNet + Transformer (BAKU) policy. The goal is zero-shot sim-to-real from large-scale synthetic training, with further gains from limited real co-training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Unified point representation: Mapping both sim and real to task-relevant points is pragmatic and deployment-friendly.\n\n2.Empirical gains: The approach improves over image-based baselines in both zero-shot and limited co-train regimes.\n\n3.Systematic ablations: The paper compares multiple depth/reconstruction sources and discusses viewpoint alignment, offering evidence for deployment trade-offs (success vs. frequency).\n\n4,Implementation clarity: The synthetic-to-3D-to-policy pipeline is clearly described and appears reproducible."}, "weaknesses": {"value": "1.Limited novelty: Most components (data generation, object filtering, depth estimation, policy learning) are existing modules strung together; the main contribution is a well-engineered integration and representation choice rather than a new learning principle.\n\n2.Baseline coverage (3D/depth): Comparisons are primarily against image-based policies. Missing are baselines that take dense point clouds/depth directly (e.g., point-cloud based, depth-only Diffusion/BC variants) under matched data—making it hard to claim that POINT BRIDGE universally outperform other input modalities.\n\n3.Task simplicity: Core evaluations focus on pick-and-place / stacking, leaving uncertainty about performance on high-contact, non-rigid, assembly, or constrained tasks. I would like to see more results on more complex tasks.\n\n4.Latency/robustness under-analyzed: FoundationStereo yields ~5 Hz, additionally the paper applys many foundation models, The latency and robustness of the pipeline should be more seriously analyzed."}, "questions": {"value": "Please see 2.3.4 in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4DSz0aj6Xx", "forum": "Acb2tZ5jR6", "replyto": "Acb2tZ5jR6", "signatures": ["ICLR.cc/2026/Conference/Submission7801/Reviewer_oEVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7801/Reviewer_oEVA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7801/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978170334, "cdate": 1761978170334, "tmdate": 1762919842932, "mdate": 1762919842932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We thank the reviewers for their constructive feedback. We are glad that the reviewers found our approach pragmatic (reviewer oEVA), well engineered (reviewers oEVA, g2VG), and scalable (reviewer 7sFb) with strong empirical results (reviewers oEVA, 7sFB, h3iK). The reviewers have requested additional experiments and important clarifications. **We have also revised the paper with additional experimental results in Appendix A.2.2.** We have provided detailed responses to each review, along with a summary of some shared concerns in this global response. \n\n**(A) New experimental results:** We have added several new experimental results in Appendix A.2.2 to provide clarifications for reviewer questions as well as further establish the robustness of Point Bridge. The new results include: **(a)** Comparison with point cloud and point track baselines (Reviewers oEVA, g2VG, h3iK). **(b)** A study of sensitivity to calibration changes between training and deployment (Reviewers g2VG, 7sFb). **(c)** Effect of background distractors on policy performance. **(d)** A study of generalization to a novel, held-out set of object instances (Reviewer h3iK). **(e)** An analysis of latency and robustness of the VLM pipeline (Reviewers oEVA, 7sFb, h3iK).\n\n**(B) Concerns about novelty (Reviewers oEVA, g2VG):** We acknowledge concerns about the novelty of each individual component in Point Bridge; however, Point Bridge as a whole constitutes a first-of-its-kind turn-key framework for sim-to-real manipulation that offers significant value to the community. We will also be releasing the code for reproducibility. Our work identifies key design choices that enable robust zero-shot sim-to-real transfer, with simple modifications delivering a 44% gain in zero-shot transfer and up to 66% when co-trained with limited real data. The simplicity and modularity of our framework is a core advantage, making it easy to adapt and extend. Unlike prior point cloud or point track methods that require extra manual annotations for scene filtering [1, 2] or rely on large unfiltered datasets [3], Point Bridge automates scene segmentation without annotation overhead. Addressing reviewer g2VG’s comment that “human demonstration data also represents a domain transfer problem”, existing approaches still require manual human annotation [1], graph construction [2], or co-training with robot-collected data [4, 5]. Our framework unifies automated scene filtering, robust point cloud generation in arbitrary scenes, and policy training, resulting in policies that are agnostic to background or distractor changes (see new experiments in Appendix A.2.2). \n\n**(C) Concerns about baseline coverage (Reviewers oEVA, g2VG, h3iK):** We address this concern by providing additional results in Appendix A.2.2. We add an unfiltered point cloud baseline with 512 points per scene for the single-task zero-shot sim-to-real setting and observe that without scene filtering, the policy performance drops significantly. It is worth noting that to make this baseline work, we had to filter the point cloud to remove the table and curtains, without which this would not work at all due to the differences between simulated and real scenes. Notably, this kind of filtering is performed automatically by the VLM-based point extraction pipeline in Point Bridge. We also add a point track baseline, Point Policy [1], which uses sparse human annotations with semantic correspondence for labelling key points on a new scene image. As shown in Appendix A.2.2, semantic matching between simulated and real-world images fails to yield accurate keypoint labels, resulting in poor performance for this baseline.\n\n**(D) Concerns about sensitivity to calibration (Reviewers g2VG, 7sFb):** The results in Table 1 and Table 2 assume that the camera viewpoints are identical between simulation and the real world. To relax this assumption, we provide new results in Appendix A.2.2 where we synthetically generate the segmented point clouds used by Point Bridge from eight distinct camera views positioned around the robot in simulation. For each view, we extract a 3D segmented point cloud and transform it to the robot’s base frame using the relevant camera extrinsics. This variation captures different occlusion patterns and increases the model’s robustness to camera viewpoint changes that may occur between training and deployment. The results in Table 7 evaluate single-task zero-shot sim-to-real transfer under such viewpoint variations between simulation and real. Notably, even without matched camera viewpoints between simulation and reality, Point Bridge attains an average success rate of approximately 47% across three tasks. While we observe a drop in performance when transitioning from matched to randomized viewpoints, this opens up an important opportunity for future work: developing methods that achieve increased robustness to viewpoint-dependent discrepancies in 3D point distributions for policy learning."}}, "id": "fN488kII99", "forum": "Acb2tZ5jR6", "replyto": "Acb2tZ5jR6", "signatures": ["ICLR.cc/2026/Conference/Submission7801/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7801/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7801/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697604374, "cdate": 1763697604374, "tmdate": 1763697604374, "mdate": 1763697604374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}