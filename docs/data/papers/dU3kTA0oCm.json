{"id": "dU3kTA0oCm", "number": 19585, "cdate": 1758297448292, "mdate": 1759897031637, "content": {"title": "Accelerate Autoregressive Normalizing Flows Sampling with GS-Jacobi Iteration", "abstract": "AutoRegressive Normalizing Flows (abbreviated as AR Flow) enjoy extensive applications in tasks such as density estimation and image generation. However, due to the causal affine coupling blocks requiring sequential computation, the sampling process is extremely slow. \nIn this paper, we demonstrate that through a series of optimization strategies, such AR Flows sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method.\nSpecifically, we find that blocks in AR Flows have varying importance: a small number of blocks play a major role in image generation, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM):\nCRM is used to identify whether a Flow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. \nThe TarFlow was chosen as the main experimental subject in our study owing to its SOTA performance on several benchmarks.\nExperiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53× in Img128cond, 5.32× in AFHQ, 2.96× in Img64uncond, and 2.51× in Img64cond without degrading FID scores or sample quality.", "tldr": "Accelerate TarFlow Sampling with Gauss-Seidel-Jacobi Iteration, speed up 5×", "keywords": ["Normalize Flow", "Generation model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4e60734da04a5afada9690fed1b991138fdfbcf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method to accelerate sampling in autoregressive normalizing flows by reformulating the inverse transformation as a nonlinear fixed-point problem solved through a hybrid Gauss Seidel and Jacobi iteration. Two diagnostic metrics are introduced to analyze convergence behavior and guide adaptive computation. The method enables efficient parallel updates within flow blocks while preserving numerical stability. Experiments show faster sampling without loss in visual."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a clear and well-motivated problem: the slow sampling speed of autoregressive normalizing flows, which has long hindered their practical use. To my knowledge, the introduction of diagnostic metrics for convergence and initialization offer novel insights in sampling from normalizing flow models."}, "weaknesses": {"value": "The major concern for me is that the method is closely tied to specific autoregressive normalizing flow architectures, mainly TarFlow. Also, the paper does not compare against alternative approaches that achieve faster sampling through model distillation [1], learned/high-order samplers [2-3], leaving unclear how the proposed iteration method performs relative to these stronger baselines. \n\nAdditionally, the convergence ranking metric and the initial guessing metric seem to rely on heuristic choices. While the authors presented empirical study on their robustness, both metrics appear sensitive to architecture, dataset, and initialization choices (which is also related to my first point), which raises questions about stability under different models or datasets. \n\nLastly, Proposition 1 does not discuss about the convergence of approximation error. How large should T be? Analysis on the convergence rate would strengthen the paper. Current manuscript does not indicate how accurate the approximated proposed method is. \n\n\n[1] Progressive Distillation for Fast Sampling of Diffusion Models\n\n[2] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps\n\n[3] Learning to Discretize Denoising Diffusion ODEs"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iDFqp4gloV", "forum": "dU3kTA0oCm", "replyto": "dU3kTA0oCm", "signatures": ["ICLR.cc/2026/Conference/Submission19585/Reviewer_gFyH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19585/Reviewer_gFyH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761264223410, "cdate": 1761264223410, "tmdate": 1762931456085, "mdate": 1762931456085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sampling method for autoregressive normalizing flows. Recently, TarFlow shows that normalizing flows with autoregressive flow layers can perform comparably to other deep generative models. However, autoregressive flows are slow in inference because they must compute $x_{i}$ iteratively. The paper proposes treating the inverse process as a nonlinear system, enabling it to be solved using the fixed-point iteration method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The idea is simple, but it can effectively improve TarFlow’s sampling speed."}, "weaknesses": {"value": "1. The significance and impact of the proposed method appear limited because this method is tailored for autoregressive normalizing flows, which represent only a small subset of deep generative models.\n\n2. The proposed method seems to bring in new problems. That is, when we use the fixed-point iteration method, we need to recompute $\\sigma$ and $\\mu$ at each iteration. That means we will need to run the VIT T times for each layer. When T is greater than the number of patches, the method will be slower than the baseline.\n3. Do we have an analysis of the relationship between T and the image size? How can we determine T for the proposed method?"}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJ6LtxMxO4", "forum": "dU3kTA0oCm", "replyto": "dU3kTA0oCm", "signatures": ["ICLR.cc/2026/Conference/Submission19585/Reviewer_jKzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19585/Reviewer_jKzN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542389884, "cdate": 1761542389884, "tmdate": 1762931455083, "mdate": 1762931455083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical sampling bottleneck in autoregressive normalizing flows (AR flows) by introducing a parallelizable Gauss-Seidel/Jacobi iteration strategy. The authors observe that in AR flow models (e.g. TarFlow), sampling is very slow because each affine coupling block operates as a causal RNN that must be executed sequentially. The paper’s key contribution is to reformulate the AR flow sampling as solving a diagonal nonlinear system and apply a hybrid Gauss-Seidel-Jacobi iteration to solve it in parallel, dramatically accelerating generation without loss of quality. Notably, they introduce two novel metrics – Convergence Ranking Metric (CRM) and Initial Guessing Metric (IGM) – to adapt the iteration procedure to the model’s characteristics, ensuring stability and efficiency. Empirical results on state-of-the-art TarFlow models show 4.5×–5.3× speed-ups with essentially no degradation in FID (image quality), which is a significant practical improvement.\n\nIn terms of novelty and significance, the idea of using fixed-point iterations to accelerate AR flows builds on some prior work (e.g. Newton-based solvers for autoregressive inversion). However, this paper goes further by hybridizing Jacobi and Gauss-Seidel updates and introducing adaptive metrics to handle non-uniform convergence across model components, which is a fresh and non-trivial innovation.\n\nThe writing is well-structured, with a logical flow from identifying the problem to proposing the method and validating it. However, there are numerous typos, so I cannot give a high rating for the presentation."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method yields dramatic improvements in sampling speed for autoregressive normalizing flows. Across multiple models, it achieves 4×–5× speedups without degrading image fidelity, as evidenced by nearly unchanged FID scores (within <1% of the baseline).\n\nInnovative Use of Iterative Solvers in AR Flows: The paper introduces a novel hybrid of Jacobi and Gauss-Seidel iteration to parallelize what was a sequential process. This is a creative cross-disciplinary idea, applying classic numerical methods to deep generative modeling. The approach is well-grounded in theory. The authors show that their fixed-point iteration will converge to the correct solution (under the model’s triangular Jacobian structure) and provide an error propagation formula.\n\nA major strength is the introduction of the CRM and IGM metrics to guide the sampling procedure. These metrics directly tackle the two main challenges identified: (1) different transformer blocks have non-uniform convergence behavior, and (2) naive initialization can cause instability. CRM provides a principled way to determine which coupling blocks are “tough” (slower to converge) so the algorithm can allocate more iterations or use sequential updates for those, while treating others with fast Jacobi updates. IGM allows the sampler to intelligently choose a safe starting point for the iteration, preventing the divergence (“numeric overflow”) that would otherwise occur in sensitive early blocks. The use of these metrics is empirically justified. By addressing these issues, the proposed method is robust. It converges where a naive parallel iteration would fail, and it does so efficiently by not over-investing computation in blocks that don’t need it."}, "weaknesses": {"value": "The proposed solution, while effective, adds considerable complexity to the sampling process. Implementing the GS-Jacobi sampler requires computing the CRM and IGM metrics using the model’s weights and a batch of training data. This offline analysis step is unusual for generative model sampling and might need to be repeated if the model or data distribution changes. Moreover, the sampling algorithm introduces new hyperparameters (e.g. how to segment a tough block, how many Jacobi vs. Gauss-Seidel iterations to use) that are not trivial to choose a priori. The tuning was done on a case-by-case basis for each dataset/model. Such manual optimization might be necessary for new models, which is a potential drawback in terms of ease of use. The method works impressively once tuned, but the paper does not provide a simple recipe for selecting these hyperparameters automatically.\n\nThe parallel iteration helps only to the extent that many parts of the model can converge quickly in a few Jacobi steps while isolating a few slow parts. Thus, the speed-up is not guaranteed for every AR flow architecture.\n\nThis paper contains numerous typos and grammatical issues. Here are the ones I found just by skimming through it:\n* L36: solution -> high-resolution\n* L84: images generation -> image generation\n* L84: attention mechanic -> attention mechanism\n* L87: trys -? tries\n* L134: denotes -> denote\n* L140: can be calculate -> can be calculated\n* L144: an non-linear -> a non-linear\n* L154: Converge and Error Propagation -> Convergence and Error Propagation\n* L196: take all $X(0) = Z$ cause -> taking all $X(0)=Z$ causes\n* L215: centers in 0 -> centers at 0\n* L262: matrixs -> matrices\n* L290: suffer -> suffers\n* L302: GUASS-SEIDEL-JACOBI -> GAUSS-SEIDEL-JACOBI\n* L304: unit a time -> unit at a time\n* L307: an non-decrease -> be a non-decreasing\n* L308: defination -> definition\n* L315: cumsum -> cumulative sum\n* L361: not -> no\n* L365: maximum value occur -> maximum value that occurs\n* L368: attention layers parameters -> attention layers' parameters\n* L369: simple -> simply\n* L374: simpe -> simple\n* L376: relative -> relatively\n* L421: learing -> learning"}, "questions": {"value": "Could you elaborate on why Gauss–Seidel is superior to other alternatives for accelerating convergence in autoregressive flows? What motivates using this GS–Jacobi scheduling over a more straightforward sequential sampling or existing parallelization techniques, and why is it expected to succeed where naive parallelization fails?\n\nDid you try Anderson acceleration, SOR, or block-Newton? How do they compare in stability and speed?\n\nHow generalizable is TarFlow to domains beyond images, e.g., audio or language, where the autoregressive structure and dependencies differ significantly?\n\nWhile the paper reports up to ~5× speed-ups on moderate-size image models (e.g. 128×128 resolution in Img128cond) without quality loss, how does the method scale with increasing sequence length or model size? Is the parallel iterative scheme still efficient for substantially larger images or longer sequences, and what are the memory or computation trade-offs as these grow? It would be useful to know if any limitations or diminishing returns appear when scaling up to more complex datasets or very high-dimensional generation tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yTs2GsDJNS", "forum": "dU3kTA0oCm", "replyto": "dU3kTA0oCm", "signatures": ["ICLR.cc/2026/Conference/Submission19585/Reviewer_iyWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19585/Reviewer_iyWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960205068, "cdate": 1761960205068, "tmdate": 1762931454583, "mdate": 1762931454583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}