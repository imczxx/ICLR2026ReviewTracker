{"id": "EZZOwOhF3W", "number": 24131, "cdate": 1758353137744, "mdate": 1759896780371, "content": {"title": "PuzzleJAX: A Benchmark for Reasoning and Learning", "abstract": "We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description language designed to support rapid benchmarking of tree search, reinforcement learning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning environments that provide hard-coded implementations of fixed sets of games, PuzzleJAX allows dynamic compilation of any game expressible in its domain-specific language (DSL). This DSL follows PuzzleScript, which is a popular and accessible online game engine for designing puzzle games. In this paper, we validate in PuzzleJAX several hundred of the thousands of games designed in PuzzleScript by both professional designers and casual creators since its release in 2013, thereby demonstrating PuzzleJAX’s coverage of an expansive, expressive, and human-relevant space of tasks. By analyzing the performance of search, learning, and language models on these games, we show that PuzzleJAX can naturally express tasks that are both simple and intuitive to understand, yet often deeply challenging to master, requiring a combination of control, planning, and high-level insight.", "tldr": "We introduce a GPU-accelerated version of the PuzzleScript game engine and benchmark search, RL, and LLMs on a dataset of human-authored games.", "keywords": ["benchmark", "puzzle games", "domain specific language", "jax", "tree search", "reinforcement learning", "large language models", "reasoning", "planningBenchmark", "Puzzle Games", "Domain Specific Language", "Reinforcement Learning", "Tree Search", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac4f3f4c4636d03fbb7cf75c581789119147bf3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents PuzzleJAX, a GPU-accelerated implementation of PuzzleScript, a domain-specific language for grid-based puzzles. It benchmarks three solution methods (tree search, reinforcement learning with a simple CNN, and large language models) across a set of existing PuzzleScript puzzles, and further profiles PuzzleJAX’s runtime efficiency on consumer hardware."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- PuzzleJAX enables to easily benchmark 2D-grid reasoning across a wide range of puzzles, as shown by the experiments.\n- The GPU-accelerated puzzle engine allows considerable speedups.\n- The authors provide clear implementation details and a well-documented codebase.\n- PuzzleJAX can be use to retrofit existing PuzzleScript puzzles."}, "weaknesses": {"value": "- As presented, since the experiments focus on trivial algorithms rather than SOTA methods, PuzzleJAX is more a codebase than a full research paper. While the need for targeted reasoning benchmark is indeed important, the authors fail to compare and position their proposed benchmark to existing ones. In fact, the work offers little novelty, as similar 2D puzzle benchmarks already exist [1].\n- The authors mainly evaluate solving rates, while the claimed contribution (speed) receives limited experimental focus and relies on overly simple algorithms.\n- The paper reports speed comparisons on a single hardware configuration, providing weak insights about the claimed GPU acceleration benefits.\n- The LLM experiments are opaque, with limited details (e.g., prompts), making the results difficult to interpret.\n- The paper is not fully self-contained, as many results require consulting figures in the appendix.\n- The content is largely descriptive, emphasizing PuzzleScript and puzzle types over analysis.\n- The conclusions are insufficiently supported by rigorous experimentation, for instance (l.452), the experiments are too narrow to draw the conclusion that tree search outperforms any RL or LLM-based algorithm:\n> Our empirical results—with uninformed search broadly outperforming RL and LLMs—highlight the difficulty of leveraging guided exploration and pre-trained knowledge in solving well-designed grid-based puzzles.\n\n[1] Estermann, B., Lanzendörfer, L. A., Niedermayr, Y., & Wattenhofer, R.. Puzzles: _A benchmark for neural algorithmic reasoning_. Advances in Neural Information Processing Systems (NeurIPS), 2024."}, "questions": {"value": "- Can the author benchmark PuzzleJAX across different hardwares?\n- Can the authors provide further details and examples about the LLMs experiments?\n- Can the authors justify their choices of RL algorithm and models?\n- Can the authors benchmark SOTA RL methods/models on 2D-grids?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KD5dX4fTSm", "forum": "EZZOwOhF3W", "replyto": "EZZOwOhF3W", "signatures": ["ICLR.cc/2026/Conference/Submission24131/Reviewer_gKG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24131/Reviewer_gKG6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961170598, "cdate": 1760961170598, "tmdate": 1762942949462, "mdate": 1762942949462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PuzzleJAX, a GPU-accelerated game engine and benchmark for tile-based puzzle games. The key contribution is a high-performance JAX-based reimplementation of the PuzzleScript engine, which allows for the dynamic compilation of any game expressible in its domain-specific language (DSL). This provides access to thousands of existing human-designed puzzle games, offering a vast and diverse testbed for reasoning and learning agents. The authors provide initial benchmarks for tree search, reinforcement learning (PPO), and LLM agents. Their results highlight that even simple puzzles pose significant challenges for modern AI techniques, with naive breadth-first search often outperforming more sophisticated RL and LLM agents that struggle with long-horizon planning, sparse rewards, and structured rule application."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The creation of a large-scale and computationally efficient benchmark is an appreciated contribution. The benchmark is a rich, human-relevant space of tasks, avoiding the pitfalls of toy problems. The ability to automatically compile thousands of diverse environments addresses a critical need for testing generalization and avoiding benchmark overfitting.\n- The reimplementation of the PuzzleScript engine in JAX is valuable and non trivial. The insight to model the rewrite rules as convolutional operations is elegant. The resulting speedups, especially for batched simulations essential for RL, are impressive.\n- The evaluations are sound and informative: the paper doesn't just present the environment; it provides an initial analysis across three major AI paradigms. The finding that BFS is often superior to PPO and various LLMs clearly surfaces the current limitations of these models.\n- The work opens exciting future research by having full interoperability with the existing PuzzleScript DSL, making it possible to scale to many more puzzles the benchmark. The framework opens up exciting avenues for future research, such as meta-learning across games, procedural content generation, and co-evolutionary systems of game players and designers, as mentioned in the discussion.\n- The paper is well-written and well-presented."}, "weaknesses": {"value": "- The RL baseline is PPO with a heuristic reward based on distance-to-win conditions. While a standard choice, it is well-known that such agents struggle with hard-exploration problems and sparse rewards, which many of these puzzles represent. The conclusion that \"RL struggles\" could be strengthened by including or at least discussing more sophisticated exploration methods (e.g., RND, ICM) or model-based RL approaches (e.g., MuZero-style planning) that integrate search. Without this, the paper primarily shows the limitations of a specific class of model-free RL algorithms, not necessarily RL as a whole.\n- The performance of LLMs is shown to be very poor, which is a key finding. However, the paper could benefit from a deeper analysis of the prompting strategy. The current approach provides the state, rules, and actions. Have the authors experimented with more advanced prompting techniques like Chain-of-Thought, providing few-shot examples of gameplay, or asking the model to form an explicit plan before acting? \n- The paper mentions using a \"distance-to-win heuristic\" for the RL reward. It's unclear how this heuristic is systematically and automatically derived for any game expressed in PuzzleScript, given the diversity of win conditions (for example\"All A on B\", \"No A\", \"Some A\"). A more detailed explanation of this reward function generation in the appendix would be valuable for reproducibility and understanding the limitations of the RL setup."}, "questions": {"value": "- Could you elaborate on the automatic derivation of the reward function for RL agents? How does your system handle complex or disjunctive win conditions (e.g., \"All A on B OR All C on D\")?\n- Could you test more RL agents and different prompting strategies for the LLMs? \n- In the LLM experiments, how were invalid actions handled? Did the model frequently produce actions outside the valid set, and if so, was it reprompted or was the turn simply wasted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R0D6as03nA", "forum": "EZZOwOhF3W", "replyto": "EZZOwOhF3W", "signatures": ["ICLR.cc/2026/Conference/Submission24131/Reviewer_qUwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24131/Reviewer_qUwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905588783, "cdate": 1761905588783, "tmdate": 1762942949291, "mdate": 1762942949291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PuzzleJAX, a JAX-based reimplementation of the PuzzleScript puzzle-game engine that compiles the DSL to GPU-accelerated environments and exposes a unified observation/action interface for learning and search. It claims broad interoperability with existing PuzzleScript games, validating hundreds of human-authored titles and reporting 2×–16× speedups over the JavaScript engine (notably at large batch sizes). Initial benchmarks show a sharp contrast across methods: uninformed BFS solves many early levels that stump PPO agents and LLM players, highlighting puzzles as a stringent testbed for long-horizon reasoning. The work positions PuzzleJAX as both a benchmark and a generator for semantically varied tasks within a consistent framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is clear writing and easy to follow.\n\n* Brings a large family of human-designed, tile-based puzzles into a single GPU-friendly framework, avoiding overfitting to one game while keeping action/obs spaces uniform.\n\n* Clear speedups vs. the baseline engine (2×–16×), especially at scale; Fig. 2 (p. 4) visualizes throughput gains."}, "weaknesses": {"value": "* Less than half of scraped games validate end-to-end; many levels fail with state/solution errors, which may limit the stability in agentic RLVR\n\n* No curated set of PuzzleScript games is released (engine only), which complicates comparability across papers unless the community converges on a shared subset/split."}, "questions": {"value": "* Should we evaluate the actual effectiveness and speed in an RLVR setting? Otherwise, I’m concerned that this paper may not be a good fit for ICLR"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LwW9fByWEA", "forum": "EZZOwOhF3W", "replyto": "EZZOwOhF3W", "signatures": ["ICLR.cc/2026/Conference/Submission24131/Reviewer_yAaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24131/Reviewer_yAaw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918078995, "cdate": 1761918078995, "tmdate": 1762942949019, "mdate": 1762942949019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PuzzleJAX, a GPU-accelerated puzzle game engine and description language for benchmarking tree search, reinforcement learning, and LLM reasoning. Unlike fixed game environments, PuzzleJAX allows dynamic compilation of any game defined in its PuzzleScript-inspired DSL, supporting a wide range of puzzles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reimplementing PuzzleScript in JAX provides hardware-accelerated simulation, yielding 2×–16× speedups over JavaScript implementations.\n2. The system supports automatic generation and compilation of new puzzle rulesets — allowing continuous benchmark expansion and procedural diversity."}, "weaknesses": {"value": "1. PuzzleJAX is a benchmark of over 500 diverse game environments. But the paper does not provide clear metrics or details on how to use PuzzleJAX as a benchmark. As as example if LLM developers were to use PuzzleJAX then how should they go about evaluating their models, what pipeline and parameters are to be used, what metrics should they collect and how should they compare with other LLM based solutions.\n2. Comparison is not shown against other gaming benchmarks."}, "questions": {"value": "1. Is it possible to do Tree Search on PuzzleJAX?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Tm6MfP25TD", "forum": "EZZOwOhF3W", "replyto": "EZZOwOhF3W", "signatures": ["ICLR.cc/2026/Conference/Submission24131/Reviewer_CwXu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24131/Reviewer_CwXu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135539915, "cdate": 1762135539915, "tmdate": 1762942948746, "mdate": 1762942948746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}