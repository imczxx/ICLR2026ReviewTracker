{"id": "FWVTbp54LQ", "number": 10117, "cdate": 1758161135592, "mdate": 1759897672738, "content": {"title": "Understanding Input Transformation-Based Attacks via Target Function Space Expansion", "abstract": "Research on transfer-based adversarial attacks provides critical insights into distinctions among Deep Neural Networks (DNNs), revealing their vulnerabilities when exposed to unseen noise. Among these transfer-based adversarial attacks, input transformation-based attacks are popular due to their simplicity and effectiveness. However, their mechanisms remain poorly understood, potentially hindering advancements in DNNs. This work explores the mechanism of the attacks, suggesting that 1) when trained with input transformations, models can improve transformation invariance by capturing diverse features from transformed inputs rather than transformation-invariant features. Therefore, given a surrogate model $f_s$ trained with input transformations $\\varphi$, adversarial attacks can leverage these transformations to expand the target function space $f_s \\circ \\varphi$, thereby effectively and rapidly improving adversarial transferability, as domain shifts are mitigated; 2) input transformation-based attacks enhance adversarial transferability by expanding the target function space. Such transformations effectively act as modifications to the target model, thereby improving attack robustness against diverse models; and 3) L2-normalization should be incorporated into the attack paradigm to mitigate gradient imbalance during adversarial example generation. This imbalance arises from domain shift variability induced by different transformations. Based on the findings, we design a simple transformation-based attack called SimAttack. It achieves a mean attack success rate of 95.4\\% on 12 models, and some of the generated examples are also effective against GPT 4.1.", "tldr": "Input transformation-based attack can improve model transformation invariance via in-domain data, thereby enhancing adversarial transferability.", "keywords": ["Mechanism Interpretability", "Adversarial Attack", "Adversarial Transferability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a5179bf06f0da19859a0211ab95857814e8fc31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the mechanism behind input transformation-based adversarial attacks, arguing that such methods enhance transferability by expanding the target function space. The authors also propose the use of L2 normalization to address gradient imbalance during attack generation. Based on these insights, they introduce SimAttack, a simple yet effective method that achieves state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1、The paper is well-organized, clearly written, and accessible. The logical flow from motivation to methodology and experiments is smooth and easy to follow.\n\n2、A key finding is that surrogate models trained with input transformations produce adversarial examples with significantly higher transferability than those trained without. This insight is valuable and empirically supported.\n\n3、The proposed SimAttack demonstrates strong performance across multiple surrogate and target models, outperforming existing methods in most settings."}, "weaknesses": {"value": "1、The authors conclude that input transformations improve transferability by increasing feature diversity rather than extracting transformation-invariant features. This claim seems overly absolute. An alternative interpretation is that both mechanisms—diversity and invariance—may work together. For instance, Figures 1 and 4 could support the idea that increasing transformations help the model focus on more robust and invariant features by filtering out noise or irrelevant variations.\n\n2、The concept of \"function space expansion\" is not fully grounded. A more formal or intuitive explanation of how transformations expand the function space and why this improves transferability would strengthen the theoretical contribution.\n\n3、The claim that models do not learn transformation-invariant features is strong but not thoroughly verified. More evidence across different model architectures and transformation types would make the argument more convincing."}, "questions": {"value": "1、Consider a more balanced interpretation of how input transformations work, acknowledging the potential role of both feature diversity and invariance.\n\n2、Provide a deeper theoretical or empirical analysis of how function space expansion improves transferability.\n\n3、Extend the validation of the \"no invariant features\" claim with additional experiments or references."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0o76JpqR3R", "forum": "FWVTbp54LQ", "replyto": "FWVTbp54LQ", "signatures": ["ICLR.cc/2026/Conference/Submission10117/Reviewer_j7Ax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10117/Reviewer_j7Ax"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464591072, "cdate": 1761464591072, "tmdate": 1762921493037, "mdate": 1762921493037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to understand the previous poorly-understood aspects input-transformation-based adversarial attacks. The authors proposed SimAttack that achieves an average attack access rate of 95.4% on both CNN- and ViT-based models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important yet under explored aspect in input transformation-based attacks.\n- Interesting idea for adversarial attacks."}, "weaknesses": {"value": "Thank you for the great efforts to the paper. However, the review has to give low ratings due to the following reasons:\n\n1. The paper suffers from poor writing, and it is hard to follow due to English wording choices. At least major re-writing is required before it is ready for publication. The review lists some of the issues below.\n    - In contribution statement (section 1). Point 2 is repetitive and should be merged with or revised to distinguish it from Point 1.\n    - \"Please correct the non-standard citation format in Section 2. The current in-text citations, such as 'SGM Wu et al. (2020)' and 'DIM Xie et al.', do not follow any established guidelines.\"\n    - What does “shuffle (3)” mean in Lines 179-180? The parameter (3) is left unexplained.\n    - In lines 265-266, what does “enhancing the bound of random noise” mean? Raising the magnitude?\n    - In lines 301-302, what does “compensating for model differences” mean?\n    - In lines 311-312, what does “bias” mean?\n    - The wording choice “Transformation number $I$” in lines 254-255 is weird. It should be “transformation count”.\n    - How  “noise reduction” is related to Figure 5?\n2. Method soundness.\n    - The paper proposes to understand transformation in a new function space. A new prospective is that “transformation modifies the surrogate model, not the input data” is very interesting. However, it is not clear that how this point is illustrated in further sections, e.g. Section 3.2. How is figure 4 related to “function space expansion”?\n    - The attack formulation in SimAttack (Alg. 1) is wrong. Step 4 describes $g_t$ as a scalar (since a norm is a scalar) instead of a vector. The dimension does not match. Hence the total algorithm is barely understandable.\n3. Evaluation \n    - As a tradition of adversarial attack literature, it would be better to include a combination of adversarial attacks for better comparison, say SimAttack + PIM + DIM.\n    - The evaluation on multi-modal models is limited. Only a case study (without elaboration on experimental setup) is discussed."}, "questions": {"value": "Please respond to the concerns in \"Weakness\" section. The reviewer will raise my rating if my concern gets solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jc6D7V3AXV", "forum": "FWVTbp54LQ", "replyto": "FWVTbp54LQ", "signatures": ["ICLR.cc/2026/Conference/Submission10117/Reviewer_ZZS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10117/Reviewer_ZZS9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777006886, "cdate": 1761777006886, "tmdate": 1762921492547, "mdate": 1762921492547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores why input-transformation-based transferable attacks (e.g., DIM, SIM, TIM) are effective. The authors argue that transformations expand the *target function space* rather than enforcing transformation invariance. They propose **SimAttack**, which applies multiple random transformations per iteration, normalizes each gradient by its L2 norm, and accumulates momentum. Experiments on ImageNet show improved transferability across 12 models compared with prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides an intuitive conceptual framing of transformation-based attacks as *function-space expansion*, potentially unifying several heuristic approaches.  \n- SimAttack is simple to implement and empirically stable across multiple benchmarks.  \n- Broad comparisons are conducted against several known baselines."}, "weaknesses": {"value": "1. **Unfair and uninformative experimental setup**  \n   - The paper uses **I = 2000 transformations per iteration**, whereas most comparable works (e.g., *Learning to Transform Dynamically for Better Adversarial Transferability*, *Improving the Transferability of Adversarial Samples by Path-Augmented  Method*) use **≤10 transformations** with equal-length augmentation chains for fairness.   \n   - Unlike prior works that optimize transformation sequences, SimAttack introduces **no selection strategy or adaptive mechanism**—it simply stacks thousands of random transforms.  \n\n2. **Questionable mechanistic evidence (Fig. 1 and Fig. 3)**  \n   - **Fig. 1**: Claims that models “produce invariant outputs but rely on different features,” yet gradient similarity is computed **without spatial realignment (\\(\\phi^{-1}\\))**, so low cosine similarity could simply reflect pixel misalignment.  \n   - **Fig. 3**: Uses **uniform noise as a surrogate transformation**, which is unrealistic and does not represent genuine geometric or photometric domain shifts.  \n   - Since these two figures underpin the claimed “function-space expansion” mechanism, their methodological flaws **undermine the core argument**.\n   - The “function-space expansion” idea is intuitive but lacks formal definition or measurable evidence (e.g., gradient diversity, representation distance).\n\n3. **Limited novelty**  \n   - Applying multiple augmentations per iteration is already standard practice in input-diversification attacks.  \n   - The proposed method adds no new learning component, optimization strategy, or theoretical formulation.  \n   - Without formal analysis of “function-space expansion,” the contribution remains descriptive and incremental.\n\n4. **Language and clarity issues**  \n   Overall writing quality requires substantial polishing.\n   - **L33–L35:** “*models can improve transformation invariance by capturing diverse features … rather than transformation-invariant features*” — need clarification.  \n   - **L274:** Incorrect capitalization (*In Addition*).  \n   - **L420 / L546:** Inconsistent notation (*L2-norm* vs *l2-norm*)."}, "questions": {"value": "1. In **Fig. 1**, were gradients spatially realigned (via \\(\\phi^{-1}\\)) before cosine similarity? Without alignment, low similarity may result from pixel shifts, not feature changes.  \n2. In **Fig. 3**, why use *uniform noise* as a transformation? Would geometric or photometric transforms (e.g., rotation, blur) better support the “expansion–domain-shift” claim?  \n3. SimAttack uses **I = 2000** transformations per iteration, while others use ≤10. Could you report results under equal computational budgets or chain lengths?  \n4. Is there any quantitative metric (e.g., gradient diversity or feature-space distance) to substantiate the “function-space expansion” hypothesis?  \n5. How sensitive is transferability to \\(I\\)? Does performance drop sharply when \\(I\\) is reduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aUZ6mSm2a9", "forum": "FWVTbp54LQ", "replyto": "FWVTbp54LQ", "signatures": ["ICLR.cc/2026/Conference/Submission10117/Reviewer_1NMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10117/Reviewer_1NMG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919494170, "cdate": 1761919494170, "tmdate": 1762921492187, "mdate": 1762921492187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the underlying mechanisms through which input transformation-based adversarial attacks improve attack transferability. Such attacks enhance the transferability of adversarial examples by incorporating input transformations during the generation process. Through experimental analysis, the authors demonstrate that the improved generalization of models trained with input transformations does not stem from the prevailing view that “input transformations help models learn transformation-invariant features.” Instead, they argue that models learn more diverse feature representations from transformed inputs, thereby effectively expanding the mapping between input and output spaces. Consequently, applying input transformation-based attacks to surrogate models can be understood as expanding the target function space of the attack, which in turn enhances the generalization of adversarial examples. Building on this insight, the paper proposes a simple transformation-based attack method called SimAttack. The method randomly samples a large number of composite transformations, aggregates the L2-normalized gradients computed on each transformed input, and incorporates momentum updates. The L2 normalization and momentum updates help mitigate domain shifts introduced by different transformations. Experimental results show that SimAttack achieves state-of-the-art (SOTA) attack performance compared to existing input transformation-based attacks and is capable of transferring successfully even to GPT-4.1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper reinterprets the principle of input transformation-based attacks from the perspective of function space expansion. It further identifies that domain shifts introduced by different transformations cause gradient imbalance, which negatively affects the transferability of adversarial examples. To address this issue, the paper proposes using L2-normalized gradients as a solution.\n2.\tIn the theoretical analysis section, the paper supports its arguments through well-designed experiments and insightful analysis. In the experimental section, it validates the effectiveness of the proposed method across various datasets and model architectures.\n3.\tThe work provides a novel interpretive perspective for future studies on input transformation-based adversarial attacks."}, "weaknesses": {"value": "1.\tAs noted in the Related Work, existing methods such as L2T, BSR, and OPS already apply input transformations during adversarial example generation to improve transferability. The proposed SimAttack shows no substantial difference from these methods — its only distinguishing feature appears to be the use of conventional L2-norm — which weakens its novelty. The authors should provide a detailed explanation of why SimAttack is effective and clearly articulate how it differs from prior work to strengthen the paper’s originality and persuasive power.\n2.\tThe authors rely only on gradients and cosine similarity to argue that models do not learn input-invariant features; the paper lacks an examination of internal and output feature representations. The authors should further demonstrate whether the models actually learn input invariance.\n3.\tFor the experiments in Figure 2, applying data augmentation during adversarial-example generation on a model that was not trained with such augmentation improves attack performance. This seems to contradict the statement on line 66: “If the surrogate model fails to extract meaningful features from these transformed data φ_i(x^adv_{t−1}), the resulting updates may become severely noisy.” In other words, augmented inputs that the surrogate model has never seen should hinder generation of meaningful perturbations, yet the results show improved performance. Please reconcile or clarify this apparent inconsistency.\n4.\tThe computational cost is high: the default number of transformations I = 2000 leads to a very large computational burden, which would limit applicability in low-latency or high-throughput scenarios. The paper lacks experiments comparing compute resources and wall-clock time against baselines to substantiate claims about the method’s efficiency.\n5.\tThe paper lacks ablation studies to demonstrate the effect of operations such as L2-normalization on improving adversarial transferability. Including such experiments would help clarify the contribution of each component to the overall performance.\n6.\tThe paper does not evaluate performance against more robust defenses, for example adversarially trained models. Tests on stronger defenses are needed to assess the method’s effectiveness under more realistic, robust settings.\n7.\tThe writing is not sufficiently clear in places, which impedes understanding. In particular, frequent switches between describing model training and adversarial example generation phases cause confusion; please improve clarity and explicitly distinguish these stages throughout the manuscript.\n8.\tThere are some typographical errors — for example, Section 4.4 refers to “our proposed AdaAES” instead of the method presented in this paper. Please proofread and correct such mistakes."}, "questions": {"value": "The specific suggestions are as outlined in the Weaknesses section above. Here, I list the issues I am particularly concerned about:\n\n1.\tI am curious why SimAttack is so effective, and what innovations it offers compared to existing methods.\n\n2.\tFrom the Gradient maps, although the model’s gradients change after adding data augmentation, it is still possible to observe that the model can extract core information, such as the outline of a sunflower. Does this imply that the model is still capturing invariant features?\n\n3.\tAs mentioned in Weaknesses 2, I feel somewhat confused about the boundary between noisy and meaningful signals.\n\n4.\tI would like to know the differences between this work and existing methods in terms of time and memory (GPU) requirements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XfEdVpdvy", "forum": "FWVTbp54LQ", "replyto": "FWVTbp54LQ", "signatures": ["ICLR.cc/2026/Conference/Submission10117/Reviewer_H8pD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10117/Reviewer_H8pD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982347591, "cdate": 1761982347591, "tmdate": 1762921491813, "mdate": 1762921491813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}