{"id": "GaBIQ32oCA", "number": 25179, "cdate": 1758365038357, "mdate": 1759896731364, "content": {"title": "Efficient Similarity-Based Fast Unlearning via Pearson Correlation Detection", "abstract": "Machine unlearning has emerged as a critical requirement for neural networks to selectively forget specific training data while preserving model performance on remaining data. However, existing approximate unlearning techniques are computationally expensive when applied repeatedly to remove multiple similar data points. This work introduces a fast, novel approach that leverages Pearson's correlation-based similarity detection to efficiently and rapidly unlearn data points that are similar to previously unlearned samples. Our fast unlearning method exploits the key observation that once a data point has been unlearned through approximate unlearning techniques, similar data points can be rapidly removed using a lightweight similarity-based approach without requiring the full computational overhead of the original unlearning procedure. \nWe establish certain theoretical properties and assurances of our similarity-based unlearning approach. We demonstrate that by measuring Pearson's correlation between target data points and previously unlearned samples, we can identify candidates for efficient removal and apply an unlearning process. This approach significantly reduces computational costs for removing multiple related data points while maintaining comparable forgetting effectiveness. Our evaluation across four datasets demonstrates that the proposed method effectively unlearns correlated data points while maintaining model utility, providing a highly scalable solution for privacy-preserving machine learning systems. Experimental results show that our proposed approach shows an improvement $10^{-2}$ in terms of accuracy compared to state-of-the-art baselines.", "tldr": "", "keywords": ["Machine unlearning", "Similarity detection", "Pearson correlation coefficient"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8ee8d2f80446675805c5b66ae822c3b34c0add0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a similarity-based machine unlearning method that speeds up sequential unlearning when multiple points are similar. After unlearning a first target z′ via an influence-function step using a damped Hessian, the method avoids a second Hessian-inverse–vector product (HVP) for a similar point. It assumes the gradients are approximately proportional and then reuses and rescales the first update, with stability coming from damping and a Sherman–Morrison rank-1 inverse for the updated Hessian. This yields a closed-form approximation for the second unlearning step that replaces a heavy HVP with simple vector operations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Reusing the first unlearning step and substituting the second HVP with a simple scaling factor is an interesting idea that meaningfully reduces the heaviest compute in sequential unlearning. Overall, the proof appears sound."}, "weaknesses": {"value": "- The method first measures similarity in input space via Pearson correlation between features x and 'z′ and then assumes gradient proportionality ∇f(x,w∗)≈α∇f(z′,w∗) to justify re-scaling the first update. However, this is problematic, non-linear models (cross-entropy, CNNs), feature correlation does not reliably imply gradient collinearity—especially across layers/representations. Can the performance of the proposed method be still guaranteed under non-linear cases? If not, the applicable scope of this method is quite limited. \n\n- The author needs to be very careful with the notations. It is quite hard to follow (Also, the table about the variable should move to appendix). Earlier, influence is stated with a λw∗\\lambda w^*λw∗ term (i.e., Δ=λw∗+∇f(z′,w∗)), but in Section 3 the “damped” single-point step uses only ∇f(z′,w∗). Then Algorithm 1 introduces λw and a learning rate η inside these closed-form updates (Steps 5 and 7), which don’t appear in Eq. (15) or the main derivation. Do those represent the same idea? What is the difference between those representations? These mismatches make it hard to see what is actually implemented and evaluated. \n\n- The performance is also not that convincing: the “43.2% win rate” of Pearson over cosine/projection is on a linear regression model on the Diabetes dataset only; there’s no variance/CI, and 43% isn’t overwhelming dominance; The main results table mixes architectures and leaves blanks for some datasets/baselines, which makes it hard to draw clear conclusions.\n\n- The authors report “accuracy” even for regression and introduce an “Acc. Unlearn” metric without defining it. The results table also leaves dashes for regression baselines and mixes CNN/MLP figures within the same cells. Collectively, these issues make the paper difficult to follow and may suggest the experiments were not fully integrated to the paper. Substantial improvements are needed. For unlearning, standard practice is to compare against retraining from scratch and to include membership-inference/privacy audits and utility–privacy trade-offs—none of which are provided."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tLryemcALy", "forum": "GaBIQ32oCA", "replyto": "GaBIQ32oCA", "signatures": ["ICLR.cc/2026/Conference/Submission25179/Reviewer_g1RW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25179/Reviewer_g1RW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699960133, "cdate": 1761699960133, "tmdate": 1762943353281, "mdate": 1762943353281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a machine unlearning approach leveraging the similarity of examples to be forgotten with other examples in the dataset with a view to making repeated unlearning operations more efficient. So, basicallly, the rationale is: once we have already unlearned some  training examples we can/should unlearn other similar examples (not explicitly defined by user requests). \nThis would save from waiting until these similar points were included in later requests for which we wo9uld have to run the unlearning algorithm again every time.\nSiilarity is based on Pearson correlation values between the examples and already-unlearned examples. Then the approach predicts the parameter update we would have gotten if the full (expensive) unlearning step again was executed. \n\nThe authors derive an approximation formula and a Hessian-regularized update. They prove an error bound that scales polynomially with feature dimensions. Empirically, they test on four small datasets (MNIST, a synthetic Gaussian mixture, and two tabularones) and report that their method's utility and forgetting quality compared to a baseline, which comes with greater computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is both practical and interesting.\n\n2. The formal development is nicely done and non-trivial.\n\n3. The proposed two-phase method (discovering simmilarity and updating the mdoel) makes perfect sense.\n\n4. Some empirical evidence is provided showing the superiority of using Pearson correlation compared to cosine similarity or raw projection for  predicting the effect of sequential unlearning updates."}, "weaknesses": {"value": "1. **Empirical evaluation is very weak**.\nExperiments on small/clean datasets (MNIST, synthetic mixtures, simple tabular) and on smaller models are no longer acceptable for proving unlearning effectiveness these days. We need to know that the method works well (and how well) in settings where unlearning is actually difficult (larger models, nontrivial distributions). \n\n2. **Baselines are not state of the art**.\nThe only real comparison is against MITR (which frankly I had never heard of before). There are no comparisons to SOTA methods like SalUn (,https://arxiv.org/abs/2310.12508) and even more recent and stronger frameworks like RUM / RUM-SalUn ( see https://arxiv.org/abs/2406.01257 from NeurIPS24) and its companion for efficiency (https://arxiv.org/abs/2410.16516).\nWithout strong baselines, claims of superior performance are unsupported. The RUM and its companion are especially important since they also look at sequential unlearning (as this paper does).\n\n3. **There is no discussion/analysis and/or empirical results regarding the possibility of unlearning something that should not be unlearned)**.\nEven if an example is sufficiently correlated with an unlearned one, assuming the correlation that it should also be unlearned is a big leap of faith. This assumption is never stress-tested. Do we start unlearning legitimately useful (or even legally necessary) data just because it’s similar in representation space to one example that was flagged? \n\nNo measure of downstream utility on “highly correlated but still valid” examples. No boundary is defined for when not to propagate the unlearning. Automatically expanding the given forget set based on a heuristic, should be accompanied by a \"safety\" analysis.\n\n4. **Efficiency is dealt with at a very abstract and thus inappropriate level**. To my mind, efficiency means (and should be shown) using:\n\n- Wall-clock unlearning time per additional forget request (seconds/minutes on given hardware).\n\n- Total compute (eg number of backward passes, Hessian-vector products, etc).\n\n- Memory / storage overhead (do we need to keep extra state in order to efficiently deal with later correlated ones? Are gradients stored? What about influence)?\n\n- Scalability wrt number of unlearning requests (linear growth wrt number of correlated examples, ...).\n\nThis is fundamental for this paper, as its contribution is to improve efficiency wrt successive unlearning requests..."}, "questions": {"value": "Please address all weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8eQgUMJrX", "forum": "GaBIQ32oCA", "replyto": "GaBIQ32oCA", "signatures": ["ICLR.cc/2026/Conference/Submission25179/Reviewer_gNmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25179/Reviewer_gNmy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934901514, "cdate": 1761934901514, "tmdate": 1762943352904, "mdate": 1762943352904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an efficient machine unlearning method designed to quickly remove multiple, similar data points without the high computational cost of repeated unlearning procedures. The core idea is to first unlearn an initial data point using a standard approximate technique, and then leverage Pearson's correlation to identify similar data points. The method removes these similar points using a lightweight, scaled update derived from the first unlearning step. The authors establish theoretical properties for this similarity-based approach and demonstrate across four datasets that it effectively unlearns correlated data, reduces computational overhead and maintains model performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a clear, practical bottleneck in existing unlearning procedures: the cost of repeated removals of related data. The proposed solution, which approximates subsequent unlearning steps by scaling the first, is an original and computationally efficient approach.\n\n2. The authors provide a dedicated empirical study comparing it against cosine similarity and projection-based methods, justifying the choice to use Pearson correlation."}, "weaknesses": {"value": "1. Empirical evidence is incomplete. The paper's premise is that it is more efficient than repeatedly applying the original approximate unlearning procedure. However, the experiments didn't directly compare against this baseline. The comparison is only against MITR and a similarity-measure ablation. Also, the key performance metrics that the paper uses, such as AU, are never explained how they are calculated.\n\n2. For eq. (15), the paper states the critical denominator can be zero only when the Hessian is a scaled identity. This assertion isn’t convincingly derived and looks incorrect as written. It's highly likely that $s_{z'}^{(\\lambda)}$ could equal 1 in many other scenarios that don't involve the Hessian being a simple scaled identity matrix.\n\n3. The paper's core assumption is that the gradients are proportionally related by the Pearson correlation of the features, but the theoretical justification for this link is only provided for linear/quadratic cases, yet results are reported on MLP/CNNs for classification. There's no experiment or analysis to show that this core assumption holds for these deep networks."}, "questions": {"value": "1. Please define the metrics (e.g. \"Acc. Unlearn\") more precisely, and add standard unlearning/privacy tests.\n\n2. Can you please clarify the discrepancy between the \"$10^{-2}$\" accuracy improvement claimed in the abstract and the different improvements shown in Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySp3kkg7ot", "forum": "GaBIQ32oCA", "replyto": "GaBIQ32oCA", "signatures": ["ICLR.cc/2026/Conference/Submission25179/Reviewer_DF2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25179/Reviewer_DF2p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959267351, "cdate": 1761959267351, "tmdate": 1762943352592, "mdate": 1762943352592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fast, similarity-based machine unlearning method that leverages Pearson correlation to efficiently remove data points similar to those already unlearned, thereby avoiding costly repeated Hessian computations. The approach provides formal theoretical guarantees with defined error bounds, demonstrating that the approximation scales polynomially with input dimensionality. Extensive experiments on four datasets further validate the method’s efficiency and effectiveness in achieving rapid and reliable unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper derives a closed-form approximation that reuses prior unlearning updates via a scaling factor combining Pearson correlation and damped self-influence, avoiding repeated Hessian inversions. Targeting this exact scenario is novel and useful in real deployments.\n2. The paper provides formal error bounds under standardized data assumptions, linking approximation error to input dimension, Hessian conditioning, and similarity metrics\n3. The paper is well organized and individual steps in the derivation are annotated, which makes it easier for a reader to verify the formula."}, "weaknesses": {"value": "1. The paper only compares against MITR, undermining claims of superiority.\n2. The metric “Acc. Unlearn” is used, but it is not aligned with standard machine unlearning evaluation protocols (e.g., retraining gap, membership inference after unlearning), making it difficult to accurately assess its privacy significance.\n3. The assumption is only approximately valid under high Pearson correlation and it is supported solely by similarity in the input space, not by similarity in the gradient or representation space, potentially weakening the error bounds in practice."}, "questions": {"value": "see the comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TKc1frSBLa", "forum": "GaBIQ32oCA", "replyto": "GaBIQ32oCA", "signatures": ["ICLR.cc/2026/Conference/Submission25179/Reviewer_BNii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25179/Reviewer_BNii"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006009218, "cdate": 1762006009218, "tmdate": 1762943352395, "mdate": 1762943352395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}