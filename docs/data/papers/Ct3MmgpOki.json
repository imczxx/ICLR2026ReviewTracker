{"id": "Ct3MmgpOki", "number": 23784, "cdate": 1758348408604, "mdate": 1759896797520, "content": {"title": "Tabular Anomaly Detection via Reconstruction with Attention-Based Bottleneck", "abstract": "Tabular anomaly detection (TAD) is an important task in machine learning, since many real-world datasets are represented in tabular form. However, it remains challenging due to the lack of labeled anomalies and the heterogeneous nature of features. Although many deep learning methods have been developed for TAD, most still rely on simple multilayer perceptrons (MLPs), overlooking architectural design, and in some cases even  underperform traditional machine learning methods such as KNN. Motivated by this, we propose LATTE, a simple yet effective reconstruction-based framework that introduces (i) an attention-based bottleneck to capture inter-column dependencies and (ii) a learnable memory bank, inspired by KNN, to retrieve prototypical normal patterns and amplify anomaly signals. By unifying these components, LATTE achieves consistently outperforms state-of-the-art methods on standard TAD benchmarks.", "tldr": "", "keywords": ["tabular learning", "anomaly detection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1576fb1c80023a187417e41103b968ac4fedf3bc.pdf", "supplementary_material": "/attachment/c6764c3a53775cce8a4b5631c7dd84dce60e596e.zip"}, "replies": [{"content": {"summary": {"value": "The present work proposes a novel attention-based and memory augmented anomaly detection method for tabular data. Their approach focuses on the one-class classification setting whereby the training set only contains *normal* samples, and the overall aim is to characterize this *normal* distribution to identify out-of-distribution samples in inference.\n\nThe proposed method, coined LATTE, implicitely combines inter-feature dependencies through using attention-based architectures, and inter-sample relations by training a memory bank used to augment the latent representation. LATTE is a reconstruction-based method that consists in training a model to reconstruct a sample, in an autoencoder-like approach, and uses the reconstruction error as the anomaly score in inference; the higher the error the less likely to belong to the normal distribution the tested sample is.\n\nThe authors test their method on 20 datasets built as a combination of the ADBench and ODD benchmarks. Their experimental results demonstrate the relevance of their method as they obtain competitive performance based on the AUPRC metric and average rank.\nA thorough ablation study demonstrate the relevance of each components in LATTE."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1**: The paper is well-written, easy to follow and properly structured.\n\n**S2**: Approach is novel, well motivated and offers strong performance. \n\n**S3**: Experiments are rigorous and compare to most relevant methods found in the literature.\n\n**S4**: Some ablations studies and figures are particularly relevant, e.g. fig 3 and 4.\n\n**S5:** Work is fully reproductible."}, "weaknesses": {"value": "**W1**: Some relevant references are missing. In particular, the authors mention on several occasions how very few attention based methods can be found in the literature. Moreover, they rely on a memory-bank to augment their reconstruction-based method. To our knowledge, [1] proposes a relatively similar approach in spirit. While demonstrating significantly poorer performance than the proposed method, [1] puts forward an attention-based method that involves a retrieval module akin to the memory bank in spirit. It might be worth mentioning in the related works, or including it in the benchmark.\n\n**W2**: Ablation studies, although interesting, do not mention any statistical testing between set-ups. Given the small variation between obtained metrics in some cases, e.g. Table 3 and 4 or Figure 2.\n\n**W3**: Some relevant experimental details are missing from the paper (see questions).\n\n**W4**: No discussion on complexity/training time can be found in the paper.\n\n[1] Retrieval Augmented Deep Anomaly Detection for Tabular Data. *Hugo Thimonier, Fabrice Popineau, Arpad Rimmel and Bich-Liên Doan*. CIKM 2024."}, "questions": {"value": "Some minor typos were found in the submitted manuscript:\n- line 181: \"(...) tokens Z as a effective (...)\" -> an.\n- line 365: \"We conduct how different (...)\". \n\n**Q1**: In light of recent work [3], while the authors mention how AUPRC has been considered as a relevant metric for evaluating AD methods, it might also be interesting to consider some alternative metrics, e.g., AUROC and threshold-dependent metrics like the F1-Score as previously used in the literature [2, 4, 5]. This should be quick, as the code provided in the supplementary material shows that they have been computed.\n\n**Q2**: As mentioned in **W3**, it might be interesting to:\n- (i) mention what criterion was used to stop training,\n- (ii) display training curves with and without the different additions mentioned in Table 3.\n\n**Q3**: As mentioned in **W2**, no statistical testing is mentioned in the ablation, while they are critical to motivate an approach. We encourage the authors to augment section 4 with statistical tests to further demonstrate the relevance of their approach.\n\n**Q4**: While the authors rely on T-SNE and provide a very informative visualization in Figure 4, [6] shows how T-SNE is very sensitive to hyperparameters, and any representations can be obtained with a well-chosen set of parameters. Could the authors provide a similar visualization using a different dimension reduction method, e.g., UMAP?\n\n**Q5**: A possible additional visualization could also be helpful to understand the underlying mechanisms driving LATTE: could the authors provide a representation (using T-SNE/UMAP) of the chosen vectors in the memory bank for normal samples vs anomaly samples, to see how close to the sample of interest they are? I expect the chosen vectors from the memory bank to be scattered in the representation space for anomalies, while being close for normal samples.\n\nGiven the strengths, weaknesses, and interrogations I have, I lean towards accepting the paper. I am open to increasing my score should the authors address my concerns/interrogations.\n\n[2] DRL: Decomposed Representation Learning for Tabular Anomaly Detection. *Hangting Ye, He Zhao, Wei Fan, Mingyuan Zhou, Dan dan Guo, Yi Chang*. ICLR 2024.\n\n[3] A Closer Look at AUROC and AUPRC under Class Imbalance. *Matthew McDermott, Haoran Zhang, Lasse Hansen, Giovanni Angelotti and Jack Gallifant*. NeurIPS 2024.\n\n[4] MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data. *Jiaxin Yin, Yuanyuan Qiao, Zitang Zhou, Xiangchao Wang, Jie Yang*. ICLR 2024.\n\n[5] Anomaly Detection for Tabular Data with Internal Contrastive Learning. *Tom Shenkar, Lior Wolf*. ICLR 2022\n\n[6] How to Use t-SNE Effectively\", Wattenberg, et al., \"Distill, 2016. http://doi.org/10.23915/distill.00002"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1b5W3nblzL", "forum": "Ct3MmgpOki", "replyto": "Ct3MmgpOki", "signatures": ["ICLR.cc/2026/Conference/Submission23784/Reviewer_LM4S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23784/Reviewer_LM4S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761047931606, "cdate": 1761047931606, "tmdate": 1762942805645, "mdate": 1762942805645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LATTE, a reconstruction-based framework for unsupervised tabular anomaly detection (TAD). LATTE introduces two key architectural components: (1) an attention-based bottleneck that replaces conventional MLP encoders/decoders to better model inter-column dependencies, and (2) a learnable memory bank inspired by k-nearest neighbors (KNN) to store prototypical normal patterns and amplify reconstruction errors for anomalies. The method is evaluated on 20 standard TAD benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe paper is well-structured and clearly explains the architecture and experimental setup. Figures and equations are effectively used to illustrate the model design.  \n•\tThe implementation details are thorough, and the ablation studies (e.g., on attention components, memory bank size, temperature, and query design) demonstrate careful empirical validation of design decisions."}, "weaknesses": {"value": "•\tWhile the paper claims that attention mechanisms better capture inter-column dependencies, it does not clearly articulate what specific limitations of MLPs in TAD this addresses, nor does it provide empirical or theoretical evidence (e.g., attention maps, feature interaction analysis) showing that MLPs fundamentally fail to model such dependencies. The use of attention feels more like a design choice than a solution to a well-defined problem. A stronger motivation would link the architectural design to known failure modes of existing methods (e.g., on dependency-type anomalies).  \n•\tThe paper asserts that LATTE “consistently outperforms” state-of-the-art methods based on average AUC-PR and rank. However, a closer inspection of Table 1 reveals that LATTE achieves the best result on only 3 out of 20 datasets, while being outperformed by baselines like KNN, Disent, or MCM on many others (e.g., breastw, cardio, pendigits, wine). The average metrics can mask substantial performance variance, and the claim of consistent superiority is therefore overstated.   \n•\tThe individual components—attention bottlenecks and memory banks—are well-established in other domains, and thus the novelty is somewhat limited. A more nuanced discussion of architectural novelty versus engineering integration would be warranted."}, "questions": {"value": "Attention mechanisms and memory addressing typically increase computational cost compared to MLPs or KNN. Could the authors report inference time relative to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YV2iMu07ip", "forum": "Ct3MmgpOki", "replyto": "Ct3MmgpOki", "signatures": ["ICLR.cc/2026/Conference/Submission23784/Reviewer_Q6HB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23784/Reviewer_Q6HB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729177232, "cdate": 1761729177232, "tmdate": 1762942805401, "mdate": 1762942805401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the one class tabular anomaly detection task. The proposed method follows the reconstruction-based method, and incorporate attention-based method to capture the normal distribution. An additional latent memory bank is designed to incorporate the global normal characteristics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The studied problem is important.\n\nS2: The paper structure is clear and easy to follow. \n\nS3: The compared baselines are extensive."}, "weaknesses": {"value": "W1: Towards the novelty of this paper. (1) The paper claimed one of their core motivations is inspired by KNN, but this is confused. The fundamental logic of KNN-based anomaly detection is to leverage the local relationships between samples (**input space**), where a normal sample is assumed to be closer to its neighbors in the training set than an anomaly would be. In contrast, the mechanism of the proposed latent memory bank operates differently. In this paper, latent memory bank assumes that normal sample representation Z, compared to anomalous representation,  is more close to the distribution of learned global vectors M in the bank, thus each normal sample represnetation can be better replaced as the combination of M (**latent space**), thus could be better decoded to reconstruct input sample. This approach appears less aligned with the sample-wise relational reasoning of KNN and more reminiscent of the concept in DRL, which assumes that normal representation could be better reprensentd by a linear combination of global random vectors (**latent space**). Could the authors clarify the specific inspiration drawn from KNN?\n\n(2) The second point is critical, as it substantially impacts the perceived novelty of the paper. The paper positions the combination of attention mechanisms and KNN-inspired concepts as a key contribution. However, this specific combination seems to have been recently and directly explored by the prior work NPT-AD [1], which incorporates attention across both features and samples to model their interactions for sample reconstruction. The absence of a comparison with this highly relevant and important baseline significantly weakens the novelty claims. Could the authors explain the unique advantages and methodological distinctions of their approach compared to NPT-AD? \n\n[1] Beyond Individual Input for Deep Anomaly Detection on Tabular Data. ICML 2024.\n\nW2: In Line 164, the authors mentioned that 'be interpreted as the positional embedding of the i-th column'. But in tabular domain, features are order-agnostic, thus positional embedding is not appropriate for this type of data.\n\nW3: The encoder-decoder architecture uses the low-rank based projection, which is similar to the SetTransformer [2]. Why not directly using SetTransformer? What is the performance benefits compared to the more complex design?\n\n[2] Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. ICML 2019.\n\nW4: Based on W2, why using such low-rank projection? Whai if the latent dimension of Z is the same as input X?\n\nW5: In Eq.6, why bi is from the encoder, rather than be the new learnable parameters? It seems that it can limit the representation learning ability in such auto encoder based architecture. And it is different from the autoencoder family. Is there any specific reason?\n\nBased on W1 to W5, the rationale behind the methodology requires thorough elaboration.\n\nW6: Is there any theoretical analysis?"}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eE8YWhdijE", "forum": "Ct3MmgpOki", "replyto": "Ct3MmgpOki", "signatures": ["ICLR.cc/2026/Conference/Submission23784/Reviewer_xpHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23784/Reviewer_xpHw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903259453, "cdate": 1761903259453, "tmdate": 1762942803998, "mdate": 1762942803998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LATTE, a reconstruction-based framework for tabular anomaly detection (TAD). The authors motivate their work by observing that many deep TAD methods rely on simple MLP architectures and are often outperformed by traditional methods like KNN . LATTE is an autoencoder-style model that replaces the standard MLP bottleneck with an attention-based architecture (using cross- and self-attention blocks) . Additionally, inspired by KNN, the authors introduce a learnable memory bank that intercepts the latent tokens and replaces them with prototypical normal patterns via an attention-based \"memory addressing\" step . The model is trained to minimize reconstruction error on normal data. The authors report SOTA performance on 20 TAD benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper correctly identifies that many prior reconstruction-based TAD methods use overly simplistic MLP autoencoders. By replacing this with a more powerful attention-based (i.e., Transformer-style) encoder and decoder, the model is inherently better equipped to capture the complex inter-column dependencies crucial for TAD.\n- The model achieves strong average AUC-PR on a wide range of real-world and synthetic datasets (Tables 1 & 2), demonstrating the empirical effectiveness of using a modern attention architecture for this task."}, "weaknesses": {"value": "1. The paper's framing is highly problematic. It heavily relies on the \"inspiration\" from KNN's strong performance. However, the proposed \"Memory Module\" is implemented as a learnable, attention-based lookup (Eq. 9), which is fundamentally different from the non-parametric, distance-based retrieval of k-nearest neighbors. This \"KNN-inspired\" narrative feels forced and does not accurately reflect the model's Transformer-based mechanism.\n2. The paper's primary architectural novelty is the \"Latent Memory Bank\". However, the ablation study in Table 3 shows that removing this memory module entirely results in a performance drop of only 0.0041 AUC-PR (0.7124 vs. 0.7083). This indicates the memory module contributes less than 1% of the model's total performance. The vast majority (99.4%) of the model's effectiveness comes from simply using a superior Attention-based Autoencoder backbone (Attn-Enc + Attn-Dec).\n3. When the negligible memory module is set aside, the paper's core architecture is an attention-based autoencoder (cross-attention encoder, self-attention bottleneck, cross-attention decoder) . This is a known architectural pattern (e.g., a variant of the Perceiver model). The paper's SOTA results are therefore unsurprising—it simply demonstrates that a powerful, modern Transformer-based backbone outperforms the older MLP-based backbones used by prior work. This is an expected engineering result, not a fundamental conceptual advance."}, "questions": {"value": "Re: Weakness #1 & #2: Given that the ablation in Table 3 shows the Memory Module contributes only 0.6% (0.7124 vs 0.7083) to the final AUC-PR, how can the authors justify this component as a core contribution? Furthermore, how is this learnable attention-based lookup (Eq. 9) functionally analogous to the non-parametric, distance-based retrieval of KNN, as claimed in the motivation?\n\nRe: Weakness #3: The paper's primary performance gain (Table 3) comes from replacing the MLP Encoder/Decoder with Attention-based ones. Is the paper's main takeaway simply that \"Transformer-based autoencoders are better than MLP-based autoencoders for tabular data\"? If so, how does this differentiate from existing work on deep learning for tabular data (e.g., TabTransformer, FT-Transformer) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XAmOjOdaCD", "forum": "Ct3MmgpOki", "replyto": "Ct3MmgpOki", "signatures": ["ICLR.cc/2026/Conference/Submission23784/Reviewer_MPiY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23784/Reviewer_MPiY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972791990, "cdate": 1761972791990, "tmdate": 1762942803620, "mdate": 1762942803620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}