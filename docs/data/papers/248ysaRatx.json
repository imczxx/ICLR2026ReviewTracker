{"id": "248ysaRatx", "number": 7135, "cdate": 1758009079244, "mdate": 1763575327439, "content": {"title": "Feedback-driven recurrent quantum neural network universality", "abstract": "Quantum reservoir computing uses the dynamics of quantum systems to process temporal data, making it particularly well-suited for machine learning with noisy intermediate-scale quantum devices. Recent developments have introduced feedback-based quantum reservoir systems, which process temporal information with comparatively fewer components and enable real-time computation while preserving the input history. Motivated by their promising empirical performance, in this work, we study the approximation capabilities of feedback-based quantum reservoir computing. More specifically, we are concerned with recurrent quantum neural networks, which are quantum analogues of classical recurrent neural networks. Our results show that regular state-space systems can be approximated using quantum recurrent neural networks without the curse of dimensionality and with the number of qubits only growing logarithmically in the reciprocal of the prescribed approximation accuracy. Notably, our analysis demonstrates that quantum recurrent neural networks are universal with linear readouts, making them both powerful and experimentally accessible. These results pave the way for practical and theoretically grounded quantum reservoir computing with real-time processing capabilities.", "tldr": "The paper proves quantitative universal approximation results for learning state-space systems using recurrent quantum neural networks.", "keywords": ["quantum machine learning", "quantum neural networks", "recurrent neural networks", "expressivity", "universal approximation", "state-space systems", "quantum reservoir computing"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3fdbfdd90c01cefd0fc6cf4d2d5f3e985ca1d60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This theoretical paper discusses the universal approximation bound for RQNNs. The authors extend approximation error bounds from previous QNN work and introduce a time factor as a state-space system to derive the approximation bound for RQNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and logically structured. \n2. The subject is important for the quantum reservoir computing community, providing theoretical foundations for a field dominated by empirical studies."}, "weaknesses": {"value": "The presentation could be more reader-friendly with graphical illustrations of the problem statement and derivation direction (nice to have but minor)."}, "questions": {"value": "1. In equation (12), where N = 2^{qubit count} relates to qubit size and n relates to parameter count, the role of λ (the gradient bound) in the denominator appears extremely important. How would λ be determined in practice? For example, does it relate to quantum noise? \n2. Can the measurement shot requirement for extracting quantum information from the QRNN play an important role in the approximation bound? For example, the empirical estimation of expectation value typically has a distance error of the form O(1/√N_{shot}) (ref. eq. (19) in [1] and also eq. (19) in [2]).\n\n[1] Qi, J., Yang, CH.H., Chen, PY. et al. Theoretical error performance analysis for variational quantum circuit based functional regression. npj Quantum Inf 9, 4 (2023)\n\n[2] Liu, CY., Kuo, EJ., Abraham Lin, CH. et al. Quantum-Train: rethinking hybrid quantum-classical machine learning in the model compression perspective. Quantum Mach. Intell. 7, 80 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeheFLrQWh", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Reviewer_WKBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Reviewer_WKBd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698473106, "cdate": 1761698473106, "tmdate": 1762919300682, "mdate": 1762919300682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This theoretical paper discusses the universal approximation bound for RQNNs. The authors extend approximation error bounds from previous QNN work and introduce a time factor as a state-space system to derive the approximation bound for RQNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and logically structured. \n2. The subject is important for the quantum reservoir computing community, providing theoretical foundations for a field dominated by empirical studies."}, "weaknesses": {"value": "The presentation could be more reader-friendly with graphical illustrations of the problem statement and derivation direction (nice to have but minor)."}, "questions": {"value": "1. In equation (12), where N = 2^{qubit count} relates to qubit size and n relates to parameter count, the role of λ (the gradient bound) in the denominator appears extremely important. How would λ be determined in practice? For example, does it relate to quantum noise? \n2. Can the measurement shot requirement for extracting quantum information from the QRNN play an important role in the approximation bound? For example, the empirical estimation of expectation value typically has a distance error of the form O(1/√N_{shot}) (ref. eq. (19) in [1] and also eq. (19) in [2]).\n\n[1] Qi, J., Yang, CH.H., Chen, PY. et al. Theoretical error performance analysis for variational quantum circuit based functional regression. npj Quantum Inf 9, 4 (2023)\n\n[2] Liu, CY., Kuo, EJ., Abraham Lin, CH. et al. Quantum-Train: rethinking hybrid quantum-classical machine learning in the model compression perspective. Quantum Mach. Intell. 7, 80 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeheFLrQWh", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Reviewer_WKBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Reviewer_WKBd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698473106, "cdate": 1761698473106, "tmdate": 1763614827194, "mdate": 1763614827194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper derives approximation bounds and universality statements for recurrent quantum neural networks. The proposed approach is based on a uniformly controlled quantum gate to apply multicontrolled rotations to a set of control and target qubits, and it has been recently shown that it can be efficiently implemented.\n\nThe authors first prove that RQNNs are able to uniformly approximate the filters induced by any contracting Barrontype state-space system.\nSecond, they extend this universality property to the much larger category of arbitrary fading memory, causal, and time-invariant filters. In this last result, neither Barron-type integrability nor contractivity conditions are needed for the target filter.\n\nThe paper is a strong theoretical contribution to the field. One of its major limitations for ICLR is that it is only theoretical."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Relevant theoretetical contribution\n- Excellent technical depth and rigor\n- Clear positioning in literature and in particular w.r.t recent literature"}, "weaknesses": {"value": "- Lack of empirical validation. Theoretical findings are strong. However, no numerical or experimental results are a limitation for this paper.\n- Some assumption (e.g., Barron-type integrability) may restrict practical applicability\n- A comparison between the proposed approach and classical RNNs or RC models for large n"}, "questions": {"value": "Could you discuss the fact that error rates do not suffer from the curse of dimensionality? I think you refer to d, but  I'm not sure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C2CesCi3fl", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Reviewer_mUbU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Reviewer_mUbU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901681703, "cdate": 1761901681703, "tmdate": 1762919300286, "mdate": 1762919300286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary\n\nThe manuscript rigorously derives approximation error bounds and a\nproof for universality of recurrent quantum neural networks (RQNNs).\nThe main novelty is the inclusion of recurrency/feedback, as similar\nresults have been recently presented for feedforward quantum neural\nnetworks (Gonon & Jacquier 2025). Universal approximation theorems are\nan important topic and well researched for classical neural networks,\nas they provide important insights on the power of different neural\nnetwork architectures. Given the importance of recurrent networks in\nprocessing time series, the results of this study are highly relevant.\n\n\nSoundness\n\nThe study is purely theoretical. The obtained results, however, look\nsound as detailed proofs are presented in the appendices. I admit that\nI did not dive deeply into all aspects of the proofs, but many aspects\nseem to rely on closely related previous studies on feedforward QNNs.\n\n\nPresentation\n\nThe general structure of the study is very good. The main text\nfocuses on the main results, and detailed proofs are being relegated\nto appendix sections. Also the appendix on previous techniques in\nquantum neural networks is very helpful.\n\nThe authors further attempt to provide simple introductions to the\nindividual sections to guide the reader through the logic of the\nconstruction of the quantum circuits and the proofs for their\napproximation capabilities. While this helped me to a certain extent,\nin my opinion, the manuscript is still hard to follow for non-experts\non quantum neural networks. Given that the ICLR community has very\nbroad backgrounds, publication at ICLR therefore requires some\nimprovements in terms of presentation. The main issue I have is that\nthe manuscript is not self-contained enough. See detailed points in\nweaknesses below.\n\n\nContribution\n\nThe authors present a systematic way to construct unitary operators\nfrom a suitable combination of rotation and Hadamard gates that upon\nmeasurement after application to initial states define a set of\nfunctions that can approximate target activation functions of neurons\nin recurrent networks to arbitrary accuracy. This is done by tuning\nthe parameters of the gates. Mathematically rigorous proofs are\npresented for these construction steps and the resulting approximation\npower. The analysis largely relies on a recently published study on\nsimilar error bounds and universality properties of quantum\nfeedforward neural networks. For non-expert readers, the novelty, i.e.\nthe specific differences in the derivations that feedback and recurrency\nintroduce, are hard to detect and not clearly presented enough."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The derivations in the study seem mathematically rigorous. The authors\nalso provide useful background on filters and functionals, and a\nhelpful review of existing literature in the related works and\nappendix sections."}, "weaknesses": {"value": "The study is not self-contained enough:\nreferences in many places to previous work Gonon and Jacquier (2025).\nThis makes it harder to follow. In particular, this issue appears when\nthe authors aim to introduce the unitary matrix V, which seems to be\nimportant for the recurrent quantum neural network architecture. But\nall details are relayed to the previous publication.\n\nThe study needs to better work out the novelty:\nAs often mentioned many times throughout the manuscript, the work\nlargely follows the techniques in Gonon & Jacquier 2025 and others on\nfeedforward QNNs. The differences between the feedforward and the\nrecurrent case of QNNs, however, needs to be highlighted more strongly\nso that the novelty of the results also becomes more apparent to\nreaders that are not experts in the field and familar with the\nprevious studies. In particular concerning the constructions in\nSection 3: how do they differ explicitly from the case of feedforward\nnetworks? This is not obvious, but crucial to judge the advances\ncompared to Gonon & Jacquier 2025.\n\nProposition 4.1:\nThis proposition seems to be central for the understanding of the\nprocedure. The proof of proposition 4.1 is only relayed to Gonon &\nJacquier 2025. For a more self-contained presentation that targets the\nbroader community of ICLR, it would be better to also present the\nproof of proposition 4.1 in the appendix of the current study.\n\nThe \"curse of dimensionality\" is emphasized in the abstract,\nintroduction and the summary of contributions, but never mentioned in\nthe results sections. It would be helpful to point to the results on\ndimensionality more specifically throughout the manuscript and\nemphasize the importance on the log scaling there."}, "questions": {"value": "Reservoir computing typically optimizes only the readout layer. The\nauthors mention in Section 1.2. that their results are for networks\nwhere all parameters are trainable, but they claim that they are also\ngeneralizable to random parameters in the recurrent layer. This\ngeneralization does not become clear from the current presentation.\nThis point is only mentioned again in the conclusion section, but it\nis not clear how the analyses of the current study support this claim.\nThis point needs to be elaborated much further.\n\n\nMinor points:\n- Clarify why it is necessary to extend previous results on functions to the first derivatives.\n- The abbreviation SAS is not defined.\n- Please spell out Barron-type conditions somewhere. \n- Please elaborate more on the function of control and target qubits for non-expert readers.\n- Please make clearer around line 266 that one needs N parallel circuits to have one for approximating each component of F in eq. (1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "euyUQpYTKX", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Reviewer_7EQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Reviewer_7EQ4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905177381, "cdate": 1761905177381, "tmdate": 1762919299594, "mdate": 1762919299594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the expressive power of recurrent quantum neural networks (RQNNs) in processing temporal data. The authors address a significant gap in quantum machine learning theory – namely, whether quantum recurrent models (a form of quantum reservoir computing with feedback) can universally approximate sequences and dynamical systems, and if so, under what resource requirements. They develop a rigorous theoretical framework combining quantum neural network function approximation results with classical reservoir computing theory. The main contributions include quantitative approximation error bounds and universality theorems for RQNNs with simple linear output layers."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. IMHO, this work is the first to establish quantitative universal approximation bounds for recurrent quantum neural networks. Prior to this, the literature lacked error guarantees for quantum recurrent models. The paper fills that gap by providing rigorous theorems (with proofs) that demonstrate RQNNs’ ability to approximate a broad class of time-dependent functions to arbitrary accuracy. \n2. Also, it shows that RQNNs can achieve universality without the need for high-degree polynomial readout functions. \n3. to reach a desired approximation error, the required number of qubits grows only logarithmically with $1/\\varepsilon$. In other words, exponentially increasing the accuracy only adds a linear number of additional qubits. This is a remarkable claim as it suggests no curse of dimensionality in qubit resources."}, "weaknesses": {"value": "1. Thm 4.6 relies on the requirement that the state transition function lies in the Barron function class and has bounded first derivatives (plus contractivity $\\lambda<1$), which means the results apply primarily to “well-behaved” systems (smooth, band-limited, and not too chaotic). Real-world temporal processes might violate these conditions (e.g., non-smooth or highly non-contractive dynamics). \n2. Minor weakness: the paper does not include any experimental or numerical simulation results to complement the theory. All results are analytical. This is a weakness in the context of a machine learning conference. Still, what I said is merely a comment, rather than a criticism.\n3. The paper assumes the existence of optimal parameters $\\theta$ for the RQNN (since it’s a universal approximation argument), but does not discuss how one might find these parameters in practice. Training a quantum model with many parameters is non-trivial, e.g., you might face barren plateaus, circuit noises, etc. You can argue that this is beyond the scope of this paper. Still, I think it is quite an important aspect to at least discuss them in the conclusion."}, "questions": {"value": "1. The paper asserts that RQNNs have approximation capabilities “as competitive as” classical reservoir families like echo state networks or state-affine systems. However, it doesn’t provide a direct comparison or quantification of any potential advantage. Hmm.. IMHO, most ppl in QML would ask about the potential for quantum advantage here, for instance, do there exist learning instances such that RQNNs and their classical counterparts exhibit a learning separation in terms of either time or sample complexity?\n2. Have the authors considered the practical side of how one would train or set the parameters $\\theta$ for an RQNN to approximate a given system? Would one use gradient-based variational quantum circuits training to fit observed data from the target system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0PCPiIU9Zv", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Reviewer_A7sM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Reviewer_A7sM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995201573, "cdate": 1761995201573, "tmdate": 1762919299095, "mdate": 1762919299095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General reply to reviewer comments"}, "comment": {"value": "Dear Reviewers\n\nWe would like to thank you for the time taken to carefully assess our paper and provide valuable feedback. We are very happy to hear your positive assessment of the paper. \nBased on your comments, we have uploaded a revised version of the paper in which we have incorporated all your comments and concerns.\nBelow, we provide answers to each of your comments and questions individually. To make it easier to follow the changes, we marked in colour all changes with respect to the original submission. We hope that these replies clarify all your questions. \nPlease do not hestiate to let us know in case any of our answers require further clarification."}}, "id": "QwjSTak5mk", "forum": "248ysaRatx", "replyto": "248ysaRatx", "signatures": ["ICLR.cc/2026/Conference/Submission7135/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7135/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission7135/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763576800372, "cdate": 1763576800372, "tmdate": 1763576800372, "mdate": 1763576800372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}