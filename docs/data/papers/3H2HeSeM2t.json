{"id": "3H2HeSeM2t", "number": 13783, "cdate": 1758222498044, "mdate": 1759897412921, "content": {"title": "On the Convergence of Adam-Type Algorithm for Bilevel Optimization under Unbounded Smoothness", "abstract": "Adam has become one of the most popular optimizers for training modern deep neural networks, such as transformers. However, its applicability is largely restricted to single-level optimization problems. In this paper, we aim to extend vanilla Adam to tackle bilevel optimization problems, which have important applications in machine learning, such as meta-learning. In particular, we study stochastic bilevel optimization problems where the lower-level function is strongly convex and the upper-level objective is nonconvex with potentially unbounded smoothness. This unbounded smooth objective function covers a broad class of neural networks, including transformers, which may exhibit non-Lipschitz gradients. In this work, we introduce AdamBO, a single-loop Adam-type method that achieves $\\widetilde{O}(\\epsilon^{-4})$ oracle complexity to find $\\epsilon$-stationary points, where the oracle calls involve stochastic gradient or Hessian/Jacobian-vector product evaluations. The key to our analysis is a novel randomness decoupling lemma that provides refined control over the lower-level variable. We conduct extensive experiments on various machine learning tasks involving bilevel formulations with recurrent neural networks (RNNs) and transformers, demonstrating the effectiveness of our proposed Adam-type algorithm.", "tldr": "", "keywords": ["Bilevel Optimization", "Adam", "Generalized Smoothness"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/102274df5eb23696678142dfec53049cadf4ecb6.pdf", "supplementary_material": "/attachment/3a446170b10644b0b337c713949d6b6a5b14e079.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a significant extension of the Adam optimizer to bilevel optimization problems under unbounded smoothness conditions, which is particularly relevant for modern deep learning architectures like transformers and RNNs that may exhibit non-Lipschitz gradients. The authors propose AdamBO, a novel single-loop algorithm that combines vanilla Adam updates for upper-level variables with SGD updates for lower-level variables, and establish its theoretical convergence with $\\mathcal{O}(\\epsilon^{-4})$ oracle complexity to find $\\epsilon$-stationary points. Their technical contributions include a novel randomness decoupling lemma for controlling lower-level error and refined analysis techniques for handling hypergradient bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors make a valuable contribution by extending the popular Adam optimizer to bilevel optimization, addressing a significant gap between theoretical optimization and practical deep learning needs. Their technical innovation—the randomness decoupling lemma—provides an elegant solution to the complex dependencies between upper-level and lower-level variables, enabling rigorous convergence analysis in this challenging setting. The theoretical analysis is sophisticated, establishing $\\mathcal{O}(\\epsilon^{-4})$ oracle complexity that matches the best known results for non-adaptive methods while maintaining the practical benefits of Adam's adaptive learning rates."}, "weaknesses": {"value": "While the paper has many strengths, there are several areas on theory and exp settings that could be strengthened to further enhance its impact and applicability. \n\n## Assumptions\nThe assumptions, while technically necessary, are quite strong and may limit the practical applicability of the theoretical results. Specifically:\n\nAssumption 3.1 \nThe (Lx,0, Lx,1, Ly,0, Ly,1)-smoothness condition requires knowledge of constants that are difficult to estimate or verify for complex neural networks. While the authors mention empirical verification for RNNs, they provide no guidance on how to estimate these constants for new architectures, making the theoretical guarantees difficult to apply in practice.\n\nAssumption 3.2(ii)\nThe uniform bound on ∥∇yf(x, y*(x))∥ across all x is a strong condition that may not hold for complex neural networks, especially as the number of parameters grows. The authors do not discuss how this assumption might be relaxed or verified in practice.\n\nAssumption 3.4 \nThis technical assumption requires certain properties to hold almost surely for each random realization, but the authors provide little discussion on its reasonableness or how it might be verified. This seems particularly restrictive and could limit the scope of the theoretical results.\n\n## Theory\n\nThe theoretical analysis contains several points that warrant further clarification:\n\nLemma 4.2\nThis key technical contribution—the randomness decoupling lemma—relies on condition (4), which seems circular. The authors state that it holds \"for any sequence {x̃t} and randomness {ξ̂t} such that\" the condition holds, but this doesn't explain why it holds for the Adam update rule specifically. More intuition about why Adam satisfies this condition would strengthen the argument.\n\nStopping time analysis\nThe adaptation of stopping time analysis from single-level to bilevel optimization is non-trivial, but the paper doesn't fully address how the additional complexity of the bilevel structure and potential correlation between variables affects the analysis. This gap leaves some uncertainty about the validity of the convergence proof.\n\nComplexity dependence on $\\lambda$\nThe O($\\lambda$^{-2}) dependence on the smoothing parameter $\\lambda$ could be substantial when $\\lambda$ is small. While the empirical analysis shows robustness, this remains a theoretical limitation that could be significant in practice, especially since $\\lambda$ is typically set to very small values (e.g., 10^{-8}).\n\n## Algorithm Design\nThe algorithm design has some limitations that affect practical implementation:\n\nChoice of β\nThe requirement that β = Θ̃(ϵ^2) creates a disconnect between the theoretical analysis and practical implementation, where β is typically set to 0.1. While the authors justify this by pointing to the stochastic optimization setting, this makes the theoretical results less applicable to real-world use of Adam.\n\nHyperparameter sensitivity\nThe algorithm has several hyperparameters (β, βsq, η, γ, λ) that need careful tuning. The paper provides theoretical guidelines but doesn't analyze sensitivity to these choices in practice, which is important for real-world applicability.\n\n## EXP \nThe experimental evaluation, while comprehensive, has a few limitations:\n\nThe paper compares with many baselines, but it's unclear if all are designed for the unbounded smoothness setting. Some methods might be at a disadvantage if they assume bounded smoothness, making the comparison less meaningful.\n\nComputational efficiency\nWhile the paper claims faster convergence, it doesn't analyze the computational overhead of maintaining Adam's momentum terms. This overhead could be significant in practice and should be quantified.\n\nPlease tell me if I was wrong."}, "questions": {"value": "Please refer to the weakness above.\n\nI would be grateful if the authors could provide convincing examples and extended exps on these issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPUNDsvidP", "forum": "3H2HeSeM2t", "replyto": "3H2HeSeM2t", "signatures": ["ICLR.cc/2026/Conference/Submission13783/Reviewer_YiCB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13783/Reviewer_YiCB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760705558238, "cdate": 1760705558238, "tmdate": 1762924315731, "mdate": 1762924315731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdamBO, a combination of SGD and Adam, for solving bilevel optimization problems. The authors provide the theoretical convergence analysis for AdamBO on an unbounded smoothness setting, showing the sample complexity of $O(\\varepsilon^{-4})$ to find $\\varepsilon$-stationary points. Some experimental results on meta-learning and deep AUC maximization are provided to verify the better performance of AdamBO over other baseline algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides the first Adam-type algorithm, to my best knowledge, for the bilevel optimization. The major contribution of this paper lies in the theoretical convergence analysis. In my view, the analysis is sound and technically solid, with some novel results such as Randomness decoupling (Lemma 4.2).\n\n- The experimental results look convincing on several classical minmax problems."}, "weaknesses": {"value": "- The convergence analysis, although it provides some novel results for bilevel optimization, looks very similar to [1]. In particular, both papers use the stopping time and the contradiction argument to derive the convergence. Thereby, the hyperparameter setups in both papers look very similar. \n\n- Given the very similar hyperparameter setups, there are some weaknesses borrowed from [1]. For example, the polynomial dependency on $\\lambda$, which usually takes $10^{-8}$ in practice; the sufficiently small value of $1-\\beta \\sim O(1/\\sqrt{T})$, which usually takes $0.001$ in practical. The latter weakness is also common in literature. Also, the setup for the learning rate is dependent on $\\lambda$, which could be a bit unnatural.\n\n- The experiment results look good. However, there are some commonly appearing min-max problems, such as GAN training and some other problems in adversarial training. It would be better to provide more experimental results on these problems."}, "questions": {"value": "- Is it possible to provide some improvement on the hyper-parameter setups in the convergence results?  \n\n- Since the major theoretical contribution lies in the analysis of Adam on bilevel optimization, it's suggested to provide more explanation on the additional essential challenges that do not emerge in single-level optimization.  \n\n- If the lower-level error is also updated by Adam, could the similar convergence guarantee still be derived? Or are there any potential differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xbdcKJ18Ij", "forum": "3H2HeSeM2t", "replyto": "3H2HeSeM2t", "signatures": ["ICLR.cc/2026/Conference/Submission13783/Reviewer_5Uki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13783/Reviewer_5Uki"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551774356, "cdate": 1761551774356, "tmdate": 1762924315234, "mdate": 1762924315234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies stochastic bilevel optimization problems where the lower-level function is strongly convex and the upper-level objective is non-convex with potentially unbounded smoothness. The paper introduces AdamBO, an Adam-type algorithm designed with the oracle calls involving stochastic gradient or Hessian/Jacobian-vector product evaluations, and shows that AdamBO achieves an oracle complexity of $\\widetilde{O}(\\epsilon^{-4})$ to find $\\epsilon$-stationary points, under certain assumptions. \n\nThe paper conducts meta-learning experiments on a larger language model, specifically an 8-layer BERT, with a classification dataset TREC, and experiments using deep AUC maximization on the imbalanced Sentiment140 dataset, to show the efficiency of the proposed algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed algorithm is the first Adam-type algorithm designed for solving stochastic bilevel optimization problems, and the derived convergence results are new. The given numerical experiments demonstrate promising performance of the studied algorithm.\n\n- The paper is clearly written."}, "weaknesses": {"value": "- While the derived convergence guarantees for AdamBO appear to be new and provide a theoretical assurance for finding a stationary point, some of the underlying assumptions may be a bit restrictive. Specifically, the bounded gradient requirement in Assumption 3.2(ii), along with the relaxed smoothness condition in Assumption 3.2(i), implies that the objective function exhibits standard smoothness in a neighborhood of each $(x, y^*(x))$. This effectively rules out the possibility of relaxed smoothness, which limits the generality of the theoretical framework.  \n\n- On Assumption 3.2(iii):  The strong convexity requirement for $g$ (I know this assumption has been made in some of the bilevel optimization literature) excludes certain practical scenarios, such as the case presented in Section 5.1 with the regularization term vanishing (i.e., $mu = 0$) for the validation set.   Also, regarding the experiments in Section 5.1, could you clarify the selection criterion for $\\mu$? In particular, does setting $\\mu > 0$ yield superior performance compared to the $\\mu = 0$ case?  \n\n- Assumption 3.4 is somewhat unnatural as it requires each random realization of the lower-level function satisfies the same property as in the population level although the authors claimed that this assumption has also been made in (Ghadimi & Wang, 2018), a paper already published for several years.\n\n- The studied algorithm appears to be an extension of Adam from single level optimization to bilevel optimization algorithm, an idea similar to some of the bilevel optimization literature.  The derived convergence result does not show any advantage comparing with other stochastic bilevel optimization algorithms and Adam on single-level optimization problems. This make the contribution a bit incremental. \n\n- As also clarified by the authors, the derived convergence bounds depend on $\\epsilon^{-2}$ and with a very small $\\beta = O(\\epsilon^2)$ setting, which contradict the parameters setting in Section E.5-E.6 where $\\beta = 0.1$ and $\\epsilon = 1.0\\times 10^{-8}$.\n\n- Some of the statement could be preciser. For example, the authors claim that Assumption 3.2 is standard in the bilevel optimization literature (Kwon et al., 2023; Ghadimi & Wang, 2018; Hao et al., 2024)., but (Kwon et al., 2023) does not consider the so called relaxed assumption with certain bounded gradient assumption in (ii). The authors claim that ``Moreover, existing convergence analyses of Adam that do not need such choice of $\\beta$ require other strong assumptions for the objective function\", but to my knowledge, recent related works have also studies Adam under weak assumption, e.g. bounded variance.\n\n- Conducting numerical results on three datasets may not be enough to illustrate the efficiency of the proposed algorithm."}, "questions": {"value": "see the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dpfz2wdbAk", "forum": "3H2HeSeM2t", "replyto": "3H2HeSeM2t", "signatures": ["ICLR.cc/2026/Conference/Submission13783/Reviewer_LUzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13783/Reviewer_LUzx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757617247, "cdate": 1761757617247, "tmdate": 1762924314861, "mdate": 1762924314861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdamBO, a novel single-loop Adam-type method for stochastic bilevel optimization. It addresses a challenging nonconvex-unbounded-smooth upper-level objective (e.g., Transformers) with a strongly convex lower-level problem. The algorithm achieves $\\(\\widetilde{O}(\\epsilon^{-4})\\)$ oracle complexity, supported by a key technical novelty: a \"randomness decoupling lemma.\""}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first Adam-type single-loop method for bilevel optimization, enhancing practicality.\n2. It establishes complexity for the important and challenging nonconvex-unbounded-smooth setting by introdcing a \"randomness decoupling lemma\" for tighter control of the lower-level variable."}, "weaknesses": {"value": "1. While the analysis is sound, the technical novelty appears incremental. The paper primarily combines well-established techniques from Adam and bilevel optimization without introducing a significantly new analytical framework.\n2. The model relies on strong convexity of the lower-level problem, excluding more general convex cases."}, "questions": {"value": "1. The proposed algorithm appears to be a direct extension of Adam to the bilevel setting, and its established convergence rate does not show a theoretical advantage over existing methods. This makes the overall contribution seem incremental. A key factor limiting its generality is the strong convexity assumption on the lower-level problem. Are there potential avenues to extend this work? For instance, could the strong convexity requirement be relaxed to a weaker condition, such as the Polyak-Łojasiewicz inequality or uniform convexity, to broaden the applicability of the algorithm?\n2. Cound you provide a concrete example of a non-pathological function that truly satisfies both Assumption 3.2(i) and (ii) while exhibiting genuinely \"unbounded\" smoothness properties not covered by standard analysis.\n3. While the theoretical results appear sound, the paper would benefit from a clearer articulation of the novel challenges in the proposed setting compared to the classical bilevel problem.  Specifically, explaining how each technical component of the analysis—such as the decoupling lemma—is necessitated by and addresses these specific challenges would help precisely delineate the work's novelty and significance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m1YVp7IyCs", "forum": "3H2HeSeM2t", "replyto": "3H2HeSeM2t", "signatures": ["ICLR.cc/2026/Conference/Submission13783/Reviewer_4nFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13783/Reviewer_4nFB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902010234, "cdate": 1761902010234, "tmdate": 1762924314343, "mdate": 1762924314343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}