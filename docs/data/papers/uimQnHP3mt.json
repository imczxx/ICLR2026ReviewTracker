{"id": "uimQnHP3mt", "number": 3501, "cdate": 1757450570216, "mdate": 1759898084343, "content": {"title": "SegDAC: Improving Visual Reinforcement Learning by Extracting Dynamic Objectc-Centric Representations from Pretrained Vision Models", "abstract": "Visual reinforcement learning (RL) is challenging due to the need to extract useful representations from high-dimensional inputs while learning effective control from sparse and noisy rewards. Although large perception models exist, integrating them effectively into RL for visual generalization and improved sample efficiency remains difficult. We propose **SegDAC**, a **Seg**mentation-**D**riven **A**ctor-**C**ritic method. SegDAC uses Segment Anything (SAM) for object-centric decomposition and YOLO-World to ground the image segmentation process via text inputs. It includes a novel transformer-based architecture that supports a dynamic number of segments at each time step and effectively learns which segments to focus on using online RL, without using human labels. By evaluating SegDAC over a challenging visual generalization benchmark using Maniskill3, which covers diverse manipulation tasks under strong visual perturbations, we demonstrate that SegDAC achieves significantly better visual generalization, doubling prior performance on the hardest setting and matching or surpassing prior methods in sample efficiency across all evaluated tasks.", "tldr": "Improving visual generalization and sample efficiency via text-grounded segmentation-driven scene decomposition and transformer-based actor critic networks", "keywords": ["reinforcement learning", "manipulation", "visual reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11380525597533a63b4251aa9bb429d6b295ddb4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper aims to enhance visual representations for actor-critic online reinforcement learning (SAC) by implementing a masking pipeline, termed SegDAC, via supervised segmentation (SAM) and text grounding (YOLO-World) models. The idea is to use a list of prompt words to generate segmentation masks for which pre-trained embeddings are extracted for the downstream policy training. The proposed method is benchmarked on Maniskill3, demonstrating improvements in performance on visual generalization and sample-efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Successful utilization of text-grounding in online RL.\n* The main advantage of using pre-trained supervised segmentation models is the robustness to perturbations such as distracting backgrounds. I find this the strongest trait of the proposed method.\n* Strong performance in visual generalization compared to baselines on the hard tasks.\n* I appreciate the extensive analysis and visualizations.\n* Very detailed appendix!"}, "weaknesses": {"value": "* Supervision: the paper claims (L47-48) that previous object-centric approaches “often relied on fixed slots, precomputed masks, or strong supervision, which limited their flexibility and general applicability.”, yet the proposed approach IS relied on strong supervision by using pre-trained supervised models, in contrast to L57-58: “SegDAC learns entirely in latent space, without human-labeled masks, auxiliary losses, or data augmentation”. \n* Reliance on pre-trained supervised models: these models are trained with human-labels over a certain type of pixel distributions, and without fine-tuning or any other adaption process, and ultimately they will fail to detect and generalize to unseen objects (as indicated in L200-202). This is in contrast to models that train representations as part of policy training or utilize self-supervised pre-trained representation that were trained in-domain (see below in the missing related work). In addition, even though “SegDAC works\nwithout careful prompt engineering” (L207), it still requires some sort of it as part of its pipeline.\n* Overall, it seems the pipeline is quite computationally complex with the utilization of pre-trained models (even though the authors use more efficient variants) and the loops that are required to extract embeddings per mask to my understanding (I would like a clarification for that).\n* Object-centric RL and learning in the latent space contribution: previous RL works used object-centric representations and trained over latent representations, some of them do not rely on supervision at all and are demonstrated on complex multi-object tasks. The idea of using masking in RL is not new and I don’t see this as a novel contribution (see list of papers below).\n* While the results on visually perturbed settings are interesting, the tasks themselves are simple in the sense that they typically involve a single-object and the manipulation itself is rather short-horizon.\n* Missing related work:\n\n[1] [Gmelin, Kevin, et al. \"Efficient RL via Disentangled Environment and Agent Representations.\" International Conference on Machine Learning. PMLR, 2023.](https://arxiv.org/abs/2309.02435)\n\n[2] [Shi, Junyao, et al. \"Composing Pre-Trained Object-Centric Representations for Robotics From\" What\" and\" Where\" Foundation Models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.](https://arxiv.org/abs/2404.13474)\n\n[3] [Zadaianchuk, Andrii, Georg Martius, and Fanny Yang. \"Self-supervised reinforcement learning with independently controllable subgoals.\" Conference on Robot Learning. PMLR, 2022.](https://arxiv.org/abs/2109.04150)\n\n[4] [Yoon, Jaesik, et al. \"An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning.\" International Conference on Machine Learning. PMLR, 2023.](https://arxiv.org/abs/2302.04419)\n\n[5] [Haramati, Dan, Tal Daniel, and Aviv Tamar. \"Entity-Centric Reinforcement Learning for Object Manipulation from Pixels.\" The Twelfth International Conference on Learning Representations.](https://arxiv.org/abs/2404.01220)\n\n[6] [Zhang, Weipu, et al. \"Objects matter: object-centric world models improve reinforcement learning in visually complex environments.\" arXiv preprint arXiv:2501.16443 (2025).](https://arxiv.org/abs/2501.16443)\n\n* Limitations: Appendix A.5. demonstrates important limitations of the model and baselines. I would appreciate a more explicit discussion of the limitations in the main text (even if short with reference to the relevant appendix).\n\n**Minor**\n\n* Typo in the title: “”Objectc-Centric” -> “Object-Centric”"}, "questions": {"value": "* How does your method compare to the list of methods I listed under the Weaknesses section? In particular, why have the authors not compared with an object-centric baseline?\n* How are “text tags” obtained for the grounding module? L192-193: “For simplicity, we defined task-specific lists in our experiments.” How does that align with the “no need for human labels” argument?\n* Clarification for the computational complexity concern I raised in the Weaknesses section.\n* The appendix mentions a project-page but I couldn’t find it. Have the authors meant to share additional results on this webpage?\n* Will the authors open-source their code for further research?\n\nI’m willing to increase my score given convincing answers to my questions and concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwYd4O37PT", "forum": "uimQnHP3mt", "replyto": "uimQnHP3mt", "signatures": ["ICLR.cc/2026/Conference/Submission3501/Reviewer_XakW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3501/Reviewer_XakW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760922418028, "cdate": 1760922418028, "tmdate": 1762916762317, "mdate": 1762916762317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses Yolo-World to segment the image and design an interesting architecture to support length-variable embeddings for policy learning. Experiments on Manipulation tasks and image segments variability evaluation show the method's edge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow\n2. The figures for illustration are clear. I really appreciate about that.\n3. The experiments about manipulation are through."}, "weaknesses": {"value": "1. This paper proposes the use of a segmentation model to assist RL. However, the computational cost this model introduces during both training and inference is undoubtedly unacceptable. To mitigate this, the authors propose employing YOLO-World. Nevertheless, the range of objects that YOLO-World can recognize is limited. I believe that in scenarios involving unrecognized objects, this method is unlikely to outperform the current SOTA. The authors should consider evaluating their approach on such scenarios—for instance, some simu-robots in MuJoCo or dealing with some easy-to-ignore objects—rather than relying solely on mainstream manipulation benchmarks. Additionally, the authors seem to overlook reporting the training and inference time required by YOLO-World.\n2. I do think the novelty here is limited. Compared to SAM-G, which generates all task-centric parts, this paper only uses language prompt to choose some specific task-centric parts. Additionally, the authors seem to overlook the comparison with SAM-G\n3. Week related literature: There are also some related visual-RL papers concerning binary mask [1]. Therefore, the comparison baseline should be updated.\n\nBased on the limited novelty and clear weakness, I tend to recommend rejection at this stage.\n\n[1] Focus On What Matters: Separated Models For Visual-Based RL Generalization"}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EISW2bB1fq", "forum": "uimQnHP3mt", "replyto": "uimQnHP3mt", "signatures": ["ICLR.cc/2026/Conference/Submission3501/Reviewer_Pb8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3501/Reviewer_Pb8s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537517519, "cdate": 1761537517519, "tmdate": 1762916761124, "mdate": 1762916761124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SegDAC, a framework that leverages pre-trained vision models to improve robustness and sample efficiency in visual reinforcement learning (RL).\nSpecifically, the method integrates YOLO-World for object detection and EfficientViT-SAM for instance segmentation to obtain object-level masks conditioned on textual prompts. The extracted object features are then fed into a transformer-based actor–critic architecture that can process a variable number of object tokens at each time step.\nThe approach is evaluated on the ManiSkill3 benchmark, which includes challenging object-manipulation tasks with visual domain shifts. SegDAC shows notable improvements in success rate, generalization, and sample efficiency compared to recent visual RL baselines such as SAC-AE, DrQ-v2, MaDi, and SADA.\nOverall, the paper aims to demonstrate that integrating pre-trained open-world visual understanding models can significantly enhance policy learning and robustness in visually complex environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tVisual robustness and generalization remain major challenges in RL, and this work takes a meaningful step by bridging pretrained vision models with policies.\n2.\tThe use of text-conditioned segmentation (YOLO-World + SAM) and a transformer-based policy/value network is technically sound and systematically executed.\n3. The paper reports detailed results, including in-distribution and out-of-distribution evaluations, and provides ablations on segmentation components and prompt usage.\n4.\tSegDAC achieves consistent gains across multiple manipulation tasks, suggesting that structured object-level representations help RL adapt to visual variability."}, "weaknesses": {"value": "1.\tWhile the system design is effective, the technical innovation mainly lies in the integration of existing pretrained vision models into RL. The conceptual contribution is non significant.\n2.\tSegmentation-based RL methods, such as SAM-G, FTD are not quantitatively compared, even though they are discussed in related work. Self-supervised methods like SGQN and SMG also worth mentioning in the related work.\n3.\tThe proposed method combines SAM, YOLO-World, and a transformer-based policy, which likely increases training and inference cost. However, there is no profiling or timing analysis to quantify this overhead.\n4.\tWhile there are ablations for segmentation and embedding components, the paper lacks analysis of transformer architecture choices and the impact of segmentation prompts.\n5.\tSince perception is entirely based on frozen pretrained models, the learned policy may rely on features that are not optimized for the downstream RL objective. The paper could discuss this trade-off more explicitly."}, "questions": {"value": "1.\tCould you provide quantitative results comparing SegDAC with segmentation-based RL baselines such as SAM-G or FTD?\n2.\tHow much additional computation (e.g., inference time per frame, GPU memory) does the online use of SAM and YOLO-World introduce compared to pixel-based methods?\n3.\tHow sensitive is performance to the choice of text prompts or detected object categories?\n4.\tHave you considered partial fine-tuning of SAM or YOLO-World to better adapt to specific manipulation scenes?\n5.\tCan the transformer actor-critic be replaced by simpler encoders (e.g., set transformer or MLP with pooling), and if so, what is the effect on performance and efficiency?\n6.\tCould the method generalize to scenarios with unseen object types or incomplete segmentation (e.g., occluded or novel shapes)? Some qualitative examples or failure cases would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YHGn9Xhp9i", "forum": "uimQnHP3mt", "replyto": "uimQnHP3mt", "signatures": ["ICLR.cc/2026/Conference/Submission3501/Reviewer_9rtw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3501/Reviewer_9rtw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753811556, "cdate": 1761753811556, "tmdate": 1762916760750, "mdate": 1762916760750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes leveraging pre-trained visual models to obtain per-frame segmentation embeddings, which are then used as task observation features for both the actor and critic networks. Specifically, the authors employ YOLO-World to detect regions of interest based on text prompts, followed by EfficientViT-SAM to segment objects within the detected bounding boxes. The resulting segmentation masks are post-processed and encoded into compact segmentation embeddings. The embeddings are then fed into a Transformer-based decoder architecture used by both the actor and critic. Experimental results on the ManiSkill3 benchmark demonstrate that this approach outperforms selected baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Demonstrates consistent performance improvements over baselines on ManiSkill3 over 8 tasks.\n\n- Provides clear experimental descriptions with detailed pipeline explanations for both baselines and the proposed method."}, "weaknesses": {"value": "- My main concern is the position of this work. The paper mainly integrates existing pre-trained visual tools (YOLO-World and EfficientViT-SAM) into a policy learning pipeline, but it is not clearly positioned in terms of its contribution to the research community, relative to the most relevant prior works or alternative design choices. Specifically, 1)missing comparison with the closest prior (FTD): The paper mentions but does not compare with FTD, which also enables agents to make decisions based solely on task-relevant objects by applying an attention mechanism to select relevant segments from a foundational segmentation model. Since this is conceptually the most related work, the lack of quantitative or qualitative comparison makes it difficult to judge the true improvement or novelty. 2) Unclear advantage over directly leveraging large vision models: The authors do not provide evidence or discussion on why explicit segmentation embeddings would outperform using general-purpose visual representations from VLMs such as DINOv2. Without such comparison or analysis, it remains unclear whether the explicit segmentation step is necessary or if similar benefits could be achieved through implicit visual understanding. 3) No exploration of fine-tuning or network variants: The paper does not discuss whether fine-tuning the embedding or segmentation modules within the same Transformer framework would improve results, nor does it analyze the impact of different architectural/training variants (e.g., alternative attention mechanisms, fusion strategies, or training from scratch). These ablations would help clarify how much the gains stem from model design versus pre-trained priors.\n\n- Insufficient evaluation under complex scenarios.  Although the paper evaluates eight tasks and performs data augmentation along dimensions like camera pose and background to increase task difficulty, it remains unclear how the method scales to more complex environments. for example, scenes involving multiple interacting or visually similar objects. Since the framework relies heavily on large pre-trained models, understanding their behavior and robustness in such challenging multi-object settings is critical.\n\n- Lack of real-world validation. The current experiments are conducted entirely in simulation, making it difficult to assess the approach’s real-world applicability. Either a real-robot demonstration or add evaluation in a more complex setting in the simulator (mentioned above) would greatly strengthen the empirical claims.\n\n- The writing needs improvement. The method sections contain unnecessary repetition, especially when repeatedly explaining the use of YOLO-World. This distracts from the main ideas and makes the narrative feel tool-driven rather than concept-driven."}, "questions": {"value": "- What are the exact proprioceptive inputs used by the policy? Do you include the relative position or orientation between the robot and the target object, or is such relational information inferred implicitly from the visual embeddings?\n\n- What is the computational overhead of using the Transformer-based actor–critic architecture? What's the inference time and training cost, and is the method feasible for real-time or on-robot deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TOFZ64OyTR", "forum": "uimQnHP3mt", "replyto": "uimQnHP3mt", "signatures": ["ICLR.cc/2026/Conference/Submission3501/Reviewer_baYC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3501/Reviewer_baYC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762432887401, "cdate": 1762432887401, "tmdate": 1762916760350, "mdate": 1762916760350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}