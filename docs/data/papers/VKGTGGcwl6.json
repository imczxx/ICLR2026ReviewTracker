{"id": "VKGTGGcwl6", "number": 13781, "cdate": 1758222428831, "mdate": 1759897413087, "content": {"title": "LLMs Get Lost In Multi-Turn Conversation", "abstract": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that when LLMs take a wrong turn in a conversation, they get lost and do not recover.", "tldr": "We discover that when LLMs take a wrong turn in a conversation, they get lost and do not recover.", "keywords": ["multi-turn", "underspecification", "llm simulation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da2e01bcdd775320d2b5a489a895a486dd884476.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work evaluates large language models under multi-turn and instruction-sharding scenarios (distinct from conventional single-turn evaluations with fully specified requirements). The authors propose a framework that simulates multi-turn instruction sharding by:  \n(1) Classifying simulation types (single-turn vs. multi-turn, fully-specified vs. sharded, concat, recap/snowball).  \n(2) Sharding process: extracting multi-round shards from single-round datasets.  \n(3) Simulation process: using LLMs as user simulators, policy classifiers, and answer extractors.  \n(4) Evaluation metrics: separating aptitude (upper-bound capability) and reliability (stability) across multiple attempts.  \n\nLarge-scale experiments across many models show:  \n(1) All tested open-/closed-source LLMs degrade under multi-turn sharding vs. single-turn complete instructions.  \n(2) The drop is driven mainly by increased unreliability (90th–10th percentile gap) rather than aptitude loss.  \n(3) Even coarse granularity causes decline.  \n(4) A likely cause is that models make early assumptions, generate premature solutions, and then over-rely on them."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Multi-turn instruction-sharding evaluation is important and under-explored.  \n(2) The simulation framework is well-defined; its categorization and evaluation design improve reliability and yield valuable conclusions.  \n(3) Experiments are broad and thorough, including multi-model, multi-simulation-method, and varying-slice-number analyses."}, "weaknesses": {"value": "(1) Answer-extraction and scoring mechanism is unclear.  \n- Figure 2 & L152 hint that every “answer attempt” in each turn is extracted and scored, yet Appendix Figure 10 scores only the final answer.  \n- If each attempt is scored against the full prompt, early correct partial answers may be unfairly penalized; if scored against the current shard, comparability with single-turn baselines is lost.  \n- Authors should clarify how final scores are computed (average, max, etc.) and what criteria are used for each attempt.  \n\n(2) Fairness of early-correct responses.  \n- If the model answers correctly before receiving all shards, it could be hallucination or redundant information in the original prompt; the asymmetry vs. the single-turn baseline needs justification.  \n\n(3) Realism of sharding.  \n- Atomizing instructions into many non-overlapping single-element shards is artificial; human interactions usually involve 2–3 rounds with decreasing element counts. The current method forces the model to fill context with speculative content, amplifying interference.\n\n(4) Use of reply-content classification.  \n- The paper mentions passing reply types to the user simulator to adjust the next shard distribution, but the appendix prompt for the user simulator does not include this step, leaving its utility unclear."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "er53KlGBlQ", "forum": "VKGTGGcwl6", "replyto": "VKGTGGcwl6", "signatures": ["ICLR.cc/2026/Conference/Submission13781/Reviewer_C44a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13781/Reviewer_C44a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569073323, "cdate": 1761569073323, "tmdate": 1762924313206, "mdate": 1762924313206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors present a simulation framework for multi-turn, underspecified conversations called sharded simulation. It converts high-quality single-turn instructions into a sequence of smaller “sharded” instructions that express alltogether the original task. Concretely, a complete query   is rewritten into a multi-turn exchange that begins with an underspecified first turn. Figure 5 illustrates the process. This is a clean, practical way to generate realistic multi-turn queries from single-turn benchmarks. \nUsing this setup, authors show a consistent “lost in conversation” effect: when models take a wrong turn, they rarely recover. Across tasks and models (from Llama-3.1-8B-Instruct to Gemini 2.5 Pro), average performance drops from ~90% in single-turn to ~65% in multi-turn settings, with declines observed evident in two-turn dialogs. The paper analyzes this failure mode and offers practical recommendations, calling on LLM builders to prioritize multi-turn reliability alongside aptitude."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "-A very clean experimental protocol: the sharded simulation is carefully constructed and well validated, and the suite of simulation modes (full, sharded, concat, recap, and snowball) is well designed to isolate where and why LLMs get lost in multi-turn conversations. Remarkably also, the authors repeat simulations for each instruction and quantify the resulting variability.\n\n-Large-scale evaluation across six tasks (code, databases, actions, math) with broad model coverage—from open-source to frontier (15 models in total, see tab 1).\n\n-The “lost in conversation” effect is demonstrated clearly.\n\n-Paper very well written and illustrations of tables and figures are extremely good\n\n-The paper culminates with Section 6, which presents concrete recommendations for multiple stakeholders involved in multi-turn conversational agents — including users, LLM developers, and system/chatbot designers."}, "weaknesses": {"value": "-It's a pity the root causes of conversational model failures are buried in the appendix.\n\n-I’m not fully convinced by the loss-of-middle-turns phenomenon (described in Appendix §F.3). This would probably require a deeper and per-model analysis."}, "questions": {"value": "-The example in Fig. 10 is contestable: the expected final answer is 85k calories, yet the assistant never provides a single final total: it reports separate values for the two pastry types. Reiterating the original request (“calculate the total calorie count of Andrew’s pastries”) at the end would likely have produced the correct aggregate. Is this a real failure ? I’m wondering if there would be other types of ‘lost in conversation’ issues.\n\n-Did you verify that, for models with smaller context windows, the full sharded conversation does not exceed their limit? Context capacity varies by model (some are maybe ~64k tokens, some much more), so overlength dialogs could confound the results.\n\n-In the gradual sharding experiments, we observe a performance degradation for N ≥ 2 shards. However, how can we explain the fact that increasing the number of shards (up to 8) does not necessarily lead to higher loss? In particular, the performance for 2 and 8 shards appears comparable in Figure 5c."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FzoUKtbhUA", "forum": "VKGTGGcwl6", "replyto": "VKGTGGcwl6", "signatures": ["ICLR.cc/2026/Conference/Submission13781/Reviewer_ig5S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13781/Reviewer_ig5S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733682804, "cdate": 1761733682804, "tmdate": 1762924312805, "mdate": 1762924312805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical performance gap between single-turn benchmarks and realistic, multi-turn, underspecified conversations. Using a novel \"Sharded Simulation\" method, the authors show that LLM performance drops by 39% (\"Lost in Conversation\"). This drop is attributed not to a loss of core \"Aptitude\" but to a massive spike in \"Unreliability\" (performance variance). The analysis identifies root causes like \"Answer Bloat\" and premature answering. This is a high-impact paper exposing a major flaw in current evaluations and offering a new framework for studying multi-turn reliability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper's significance lies in highlighting the critical gap between LLM benchmarks (single-turn) and real-world use (multi-turn).\n\nIts novel methodology, \"Sharded Simulation,\" provides a scalable and clever method to adapt existing benchmarks for multi-turn context evaluation.\n\nThe robust experimentation, consisting of large-scale tests (15 LLMs, 6 tasks), provides strong evidence for the findings.\n\nThe insightful analysis into \"Aptitude\" vs. \"Unreliability\" decomposition is a key insight, pinpointing consistency, not capability, as the main issue, and its root cause analysis (e.g., \"Answer Bloat\") is actionable."}, "weaknesses": {"value": "The user simulation is a simplification of real, messy human interaction. The reliance on the simulation might weaken the research scope.\n\nFindings are based on analytical tasks; generalizability to creative or open-ended tasks is unclear, and should be considered in future works."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ilBMdf4Q2U", "forum": "VKGTGGcwl6", "replyto": "VKGTGGcwl6", "signatures": ["ICLR.cc/2026/Conference/Submission13781/Reviewer_yCDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13781/Reviewer_yCDA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799996586, "cdate": 1761799996586, "tmdate": 1762924312261, "mdate": 1762924312261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "“LLMs Get Lost in Multi-Turn Conversation” examines how large language models handle underspecified dialogue by introducing a sharded simulation method that splits single-turn benchmark instructions into smaller “shards” revealed over multiple turns. Evaluating 15 models on six generation tasks across 200,000 simulated conversations, the authors find an average 39% performance drop from single-turn baselines, driven mainly by a large rise in unreliability (+112%) rather than a loss of aptitude (–15%). Control conditions show that rephrasing alone does not explain the effect, and that recap, repetition, or lower temperature settings offer only limited recovery. Qualitative analysis identifies four causes—premature answer attempts, compounding assumptions, over-weighting of first and last turns, and “answer bloat”—together forming the “Lost in Conversation” pattern, where models rarely recover after early mistakes. The authors note that their simulated, turn-by-turn setup idealizes real conversation but likely underestimates the reliability issues seen in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Strength #1: Novel Decomposition of Performance into Aptitude and Reliability\n\nThe paper makes an important conceptual contribution by decomposing overall performance degradation into two distinct components: aptitude (best-case capability, measured as 90th percentile performance, A90) and unreliability (variance across runs, measured as the 90-10 interpercentile range, U90/10). This framework reveals that the primary issue in multi-turn settings is not loss of capability but rather dramatic increases in inconsistency.  \n\nThe empirical findings are striking: while aptitude drops moderately from single-turn to multi-turn settings (average -16%), unreliability more than doubles (+112%). Critically, this pattern holds across all 15 models tested, from small open-weight models to state-of-the-art systems like GPT-4.1 and Gemini 2.5 Pro. This means that even the most capable models become highly unpredictable in multi-turn conversations---they retain the ability to solve tasks (high aptitude) but fail to do so consistently (low reliability).  \n\nThis decomposition is methodologically enabled by running 10 independent simulations per condition rather than reporting single averages, allowing the authors to measure variance meaningfully. The resulting metrics (P, A90, U90/10) provide a potentially generalizable framework that extends beyond this specific study. Other researchers can apply this lens to different settings (e.g., retrieval-augmented generation, agentic workflows, code generation) to separately diagnose capability versus consistency issues.  \n\nThe practical implications are significant: the paper argues convincingly that LLM builders should jointly optimize for both aptitude and reliability, rather than treating higher benchmark scores as the sole objective. For end users, the finding explains a common frustration---tasks that should work sometimes fail unpredictably, which may be more problematic than consistent failure. Overall, this decomposition transforms what could have been a purely negative result (performance drops in multi-turn) into a nuanced diagnostic framework with clear actionable insights.\n\n---\n\n### Strength #2: Systematic Behavioral Analysis Identifying Concrete Failure Modes\n\nBeyond demonstrating that performance degrades, the paper provides a thorough behavioral analysis identifying four specific failure modes, each supported by quantitative evidence from the conversation logs. This analysis strengthens the contribution by moving from \"models fail\" to \"models fail because they exhibit these specific behaviors.\"  \n\n**Premature answer attempts** (Section F.1, Table 3): The authors show that timing of first answer attempts strongly predicts success. Across all models, conversations where the first attempt occurs in the earliest 20% of turns achieve only 30.9% average performance, compared to 64.4% when attempts occur in the final 20%. This suggests models jump to solutions before gathering sufficient information, then struggle to revise.  \n\n**Answer bloat** (Section F.2, Figure 8): Through measurement of answer lengths across conversation turns, the paper demonstrates that multi-turn answers become progressively longer---20-300% longer than single-turn equivalents by the final attempt. Even correct solutions in multi-turn settings are 14-27% longer than correct single-turn solutions, indicating unnecessary complexity rather than mere verbosity accompanying errors.  \n\n**Loss-of-middle-turns** (Section F.3, Figure 9): Using citation patterns in the summarization task, the authors show that models disproportionately reference information from first and last turns (20% citation rate for turn-8 documents) while under-weighting middle turns (8% for turns 2-3). This extends the known \"lost in the middle\" phenomenon from single-turn long-context to multi-turn conversation.  \n\n**Over-verbosity** (Section F.4, Table 4): By binning conversations by response length, the paper establishes that longer responses correlate with worse performance on 5 of 6 tasks. Shortest-response conversations achieve 40.7% average performance versus 35.6% for longest-response conversations, suggesting verbose responses introduce problematic assumptions.  \n\nEach failure mode is empirically grounded with specific measurements rather than anecdotal observations, and together they form a coherent narrative: models make premature assumptions when information is incomplete, these assumptions compound into increasingly complex and incorrect solutions, and middle-context information is forgotten while early errors persist. This level of behavioral detail is rare in empirical LLM papers and provides concrete targets for future improvement---for instance, training models to defer solution attempts until sufficient information is gathered, or developing architectures less prone to middle-context loss in conversations.  \n\nThe analysis is reproducible (the authors plan to release conversation logs) and the failure modes are interpretable, making this a substantive contribution beyond the immediate experimental findings. Future work can test whether interventions targeting these specific behaviors improve multi-turn reliability."}, "weaknesses": {"value": "### Weakness #1: Limited Real-World Validation Restricts Generalizability\n\nThe paper’s central claim---that LLMs “get lost in multi-turn conversation”---is supported entirely through synthetic simulations in which single-turn benchmark instructions are artificially fragmented into minimal “shards” (typically 6–8 small facts revealed one per turn). While this setup is useful for controlled stress-testing, the authors do not demonstrate that real human–LLM conversations exhibit similar fragmentation or that comparable degradation occurs in practice. The only empirical link to real data is a citation to Herlihy et al. (2024), which notes that underspecification is common but does not quantify how information is distributed across turns. As a result, it remains unclear whether the reported 39% performance drop reflects natural conversational behavior or an artifact of the experimental design.\n\nThe paper’s gradual-sharding experiment (Section 5.3) partly addresses this by showing degradation even in two-turn conversations, but this analysis uses only 31 instructions and 2 models---too narrow to establish generality. Moreover, the authors do not show that their two-turn setup resembles real user interactions any more than their eight-turn cases. Because the title, abstract, and framing imply broad conclusions about multi-turn conversation, readers may overinterpret what is essentially a controlled fragility test. The work would be strengthened by either (1) analyzing real LLM chat logs to quantify natural fragmentation patterns, (2) validating degradation effects through human studies, or (3) reframing claims to emphasize that the results concern performance under idealized underspecification, not natural dialogue.\n\n---\n\n### Weakness #2: Confounded Design Obscures Causal Mechanisms\n\nThe experimental setup combines several distinct challenges, making it unclear which factors actually drive the observed degradation. Three design confounds are particularly important:\n\n1. No error correction.  \n   The user simulator never provides feedback when the model makes an incorrect assumption; it simply continues revealing shards. This means the task tests not only underspecified input handling but also self-correction without external feedback. In real conversations, users---especially experts---often clarify or correct midstream. Without comparing conditions with and without such feedback, it is impossible to disentangle how much of the degradation stems from underspecification itself versus the lack of correction.\n\n2. Minimal conversational context.  \n   Shards are intentionally atomic (“one fact per turn”), stripping away the pragmatic cues---motivation, examples, and framing---that humans normally provide when clarifying requests. It is therefore unclear whether performance loss arises from the distribution of information across turns or from the absence of contextual scaffolding that would typically accompany clarification.\n\n3. Potentially unnatural communication patterns.  \n   The one-fact-per-turn design may fall outside the conversational distributions LLMs were trained on. Although the authors argue that their setup is “benign” and likely underestimates real-world difficulty, it might also be adversarial, inducing premature assumptions precisely because it violates ordinary information-packaging conventions.\n\nThese confounds complicate causal interpretation. The qualitative analysis identifies four recurring failure modes (premature attempts, compounding assumptions, loss-of-middle-turns, and answer bloat), but the mechanisms remain unclear. For instance, do models guess prematurely because information arrives slowly, or because the conversation lacks corrective and contextual cues? Additional ablations---such as allowing error correction or adding more naturalistic shard phrasing---would clarify which factors truly cause models to “get lost.” As written, the study convincingly shows that LLMs are brittle under a specific, highly controlled multi-turn stress test, but does not yet isolate the underlying causes."}, "questions": {"value": "Q1. How did you decide on shard granularity, and do you think the level of fragmentation reflects natural human–LLM dialogue?\n\nQ2. Did you test whether providing corrective feedback from the user simulator changes the magnitude of performance degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5aNr62pcAo", "forum": "VKGTGGcwl6", "replyto": "VKGTGGcwl6", "signatures": ["ICLR.cc/2026/Conference/Submission13781/Reviewer_RY95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13781/Reviewer_RY95"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13781/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990054506, "cdate": 1761990054506, "tmdate": 1762924311825, "mdate": 1762924311825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}