{"id": "ZC2N9q8USm", "number": 16789, "cdate": 1758268719358, "mdate": 1759897219302, "content": {"title": "ForestPersons: A Large-Scale Dataset for Under-Canopy Missing Person Detection", "abstract": "Detecting missing persons in forest environments remains a challenge, as dense canopy cover often conceals individuals from detection in top-down or oblique aerial imagery typically captured by Unmanned Aerial Vehicles (UAVs). While UAVs are effective for covering large, inaccessible areas, their aerial perspectives often miss critical visual cues beneath the forest canopy. This limitation underscores the need for under-canopy perspectives better suited for detecting missing persons in such environments. To address this gap, we introduce ForestPersons, a novel large-scale dataset specifically designed for under-canopy person detection. ForestPersons contains 96,482 images and 204,078 annotations collected under diverse environmental and temporal conditions. Each annotation includes a bounding box, pose, and visibility label for occlusion-aware analysis. ForestPersons provides ground-level and low-altitude perspectives that closely reflect the visual conditions encountered by Micro Aerial Vehicles (MAVs) during forest Search and Rescue (SAR) missions. Our baseline evaluations reveal that standard object detection models, trained on prior large-scale object detection datasets or SAR-oriented datasets, show limited performance on ForestPersons. This indicates that prior benchmarks are not well aligned with the challenges of missing person detection under the forest canopy. We offer this benchmark to support advanced person detection capabilities in real-world SAR scenarios. The dataset is publicly available at https://huggingface.co/datasets/anonreviewer2026/under-canopy-benchmark-anon.", "tldr": "We propose a new dataset for detecting missing persons in forest environments, with detailed pose and visibility annotations.", "keywords": ["Missing Person Detection", "UAV-based Search and Rescue", "Forest Environment Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11aa30121bed1aefee3808f33873606009dec0a4.pdf", "supplementary_material": "/attachment/c732ce0b0ce03b89f813222ffb84e2169ee3c472.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces 'ForestPersons', a new large-scale image dataset intended to support Search and Rescue (SAR) missions for missing persons in forested areas. The authors identify the limitations of existing high-altitude drone datasets, where the forest canopy obstructs the view of individuals below. As a solution, they have constructed a dataset of 96,482 images captured by simulating the perspective of Micro Aerial Vehicles (MAVs) at a low altitude of 1.5m–2.0m. This dataset includes diverse seasons (summer, fall, winter), lighting conditions, and poses of the missing person (standing, sitting, lying). A key feature is the quantification and labeling of occlusion levels caused by vegetation as a 'visibility level'."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear Problem Definition: The paper successfully highlights the explicit limitations of existing high-altitude SAR datasets (visual obstruction by the canopy) and justifies the need for a specialized dataset focused on the 'under-canopy' environment.\n\n- Data Diversity: The systematic effort to collect data across various seasons (notably including snow-covered winter scenes) and different poses (standing, sitting, lying) to address diverse scenarios in a forest environment is commendable.\n\n- Quantification of Occlusion: A core strength of this dataset is that it does not avoid the key challenge of occlusion in forest environments. Instead, it defines and labels it as a 4-level 'visibility' attribute, providing a benchmark to evaluate model robustness against occlusion.\n\n- Experimental Validation: The authors experimentally demonstrate that models trained on existing datasets (e.g., SARD, COCO) perform poorly on 'ForestPersons' (Table 2), thereby reinforcing the necessity and originality of the proposed dataset."}, "weaknesses": {"value": "1. Unrealistic Simulation of MAV Flight (Domain Gap):\nThis dataset was not captured by an actual flying MAV, but rather \"simulated\" by a person holding a camera at 1.5–2.0m. Consequently, unique visual artifacts inherent to actual MAV flight are missing. Specifically, 'Motion Blur' caused by the MAV's rapid movement and vibration, and the 'Rotor Wash' phenomenon, where propeller downwash disturbs surrounding leaves and branches, are not reflected in the data. Therefore, models trained on this dataset may suffer a significant performance drop when deployed on an actual MAV due to this 'Domain Gap'.\n\n2. Limitation of Data Modality (RGB-Only):\nThis paper relies exclusively on RGB (color) sensor data. However, many state-of-the-art SAR studies, including prior work mentioned by the authors (e.g., WiSARD), adopt a multimodal approach fusing RGB and Thermal imagery as a standard. In real-world forest environments, a missing person may be heavily occluded by bushes or camouflaged, making them impossible to identify with RGB alone. A thermal sensor plays a decisive role in such cases by detecting body heat. The reliance on RGB not only makes night-time detection impossible but also addresses a sub-optimal, simplified version of the real-world problem, as even daytime detection is severely limited.\n\n3. Lack of Realism in Staged Scenarios:\nThis dataset was built by filming 'actors' performing 'staged missing person scenarios'. Such simulated situations may not adequately reflect the severity and atypical nature of actual distress situations. A real victim may be in a highly irregular pose due to injury or hypothermia, or may be partially buried under dirt, leaves, or debris. The poses in the sample images appear relatively distinct, suggesting a lack of realism in representing data from extreme, real-world scenarios.\n\n4. Subjectivity of Core Annotations:\nThe reliability of the 'visibility level' label, one of the paper's key contributions, is questionable. The authors' own inter-annotator agreement analysis in Appendix F (Table 8) shows a Cohen's Kappa of approximately 0.45 for the 'visibility' attribute, which statistically represents only \"moderate agreement\". This implies that the core Ground-Truth label for visibility is highly subjective and noisy. Performance analyses based on this unreliable label (e.g., Figure 6) should be interpreted with caution."}, "questions": {"value": "1. Regarding Unrealistic MAV Flight Simulation (Domain Gap)\n\nQ1.1 (Domain Gap): This dataset was constructed using a handheld camera simulation rather than actual MAV flight. Consequently, dynamic environmental changes unique to MAV flight, such as 'motion blur' or 'rotor wash', are not included in the data. How do you assess the potential impact of this 'Domain Gap' on the performance of a model deployed on an actual MAV?\n\nQ1.2 (Future Plans): To address this 'Domain Gap', do you have plans to collect data using an actual autonomous MAV in the future, or at least to augment the simulation data with realistic noise such as motion blur?\n\n2. Regarding the Limitation of Data Modality (RGB-Only)\n\nQ2.1 (Exclusion of Thermal): In forest SAR environments, thermal sensors play a decisive role in detecting victims occluded by vegetation, as demonstrated in prior studies (e.g., WiSARD) cited in your paper. Could you explain if thermal data was intentionally excluded from your dataset design, or if it was omitted due to collection difficulties?\n\nQ2.2 (Scope Limitation): An RGB-only sensor makes night-time detection impossible and severely limits even daytime detection performance. Despite this limitation, what specific scenarios do you believe this RGB-only dataset can contribute to in real-world SAR operations?\n\n3. Regarding the Lack of Realism in Staged Scenarios\n\nQ3.1 (Realism of Poses): The dataset is built upon 'staged scenarios' featuring 'actors'. However, an actual victim might be in a much more irregular pose due to injury or hypothermia, or partially buried under dirt/leaves, than what is depicted in the sample images (e.g., Figure 3). What are your thoughts on the risk this discrepancy between 'staged' poses and 'actual' victim conditions poses to the model's generalization performance?\n\n4. Regarding the Subjectivity of Core Annotations\n\nQ4.1 (Annotation Reliability): The inter-annotator agreement for the 'visibility level', a key contribution of this paper, was only \"moderate\" (Cohen's Kappa $\\approx$ 0.45, Appendix F). This suggests the label is highly subjective and noisy. Given this low reliability, do you believe the performance analysis based on this label (e.g., Figure 6) is valid and meaningful? We would like to hear your opinion on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8mj81ajkIx", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Reviewer_KaHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Reviewer_KaHw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533189090, "cdate": 1761533189090, "tmdate": 1762926827541, "mdate": 1762926827541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a dataset to benchmark object detection models on the task of detecting humans in a forest from a ground-level perspective. This is intended to improve the performance of below-the-canopy drones in search and rescue (SAR) operations. Most existing benchmarks employ above-the-canopy imagery, which would limit their performance, particularly during leaf-on season.\nA suite of object detection models are benchmarked on the proposed dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Paper well written and structured, with correct experimental setup.\n- Motivation for the task is made clear."}, "weaknesses": {"value": "1. I appreciate the train-val-test split protocol used in this work, which is done at the sequence level. However, there are not enough details about the potential correlation between sequences. Is it possible that two sequences are captured the same day, on similar locations and with the same subjects? Overall, it would be useful to have some more information about the location of the sequences and the diversity of subjects (to make sure there’s no overfitting to a specific outfit or location type).\n2. In the same line, there is no discussion about the potential generalization to other forest types. From the few photos in the paper, it would seem to be some type of temperate broadleaf forest. Although the varying conditions the videos where taken in do suggest the dataset covers a large diversity of environments, I can only imagine that this would hardly work in denser tropical forests. It would be helpful to get an indication of which biomes are covered by the dataset, in order to assess potential for geographical generalization.\n3. This paper presents a dataset where the main edge is that it is capture in a different setting that other comparable datasets. As such, there is little novelty to speak of.  Novelty is typically a requirement according to the ICLR reviewer guidelines. I’m not 100% sure what this entails when it comes to datasets, but I would imagine enabling the benchmarking of so far un-benchmarkable tasks. The proposed dataset does not allow to evaluate methods on anything that is fundamentally different, although its different viewpoint and diversity will likely be helpful to train models that will be useful to practitioners. As such, it maybe worth questioning the adequacy of ICLR as a venue to publish this paper, although I do commend the authors for the quality of their work."}, "questions": {"value": "I would like to read the response of the authors to the questions formulated in weaknesses 1 and 2:\n- Can they provide statistics about commonalities between video sequences in terms of the location, time and subjects?\n- Have the authors considered to which geographical locations they would expect the dataset to generalize to?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I'm not sure how this could be addressed, but there are clear military applications for this task that I, personally, find a bit concerning."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LfB18By2u6", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Reviewer_FMJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Reviewer_FMJT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581095539, "cdate": 1761581095539, "tmdate": 1762926827185, "mdate": 1762926827185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback. We are pleased that all reviewers recognized the importance of addressing the under-canopy search and rescue (SAR) problem, and acknowledged the comparative analysis demonstrating limitations of existing benchmarks and detectors on ForestPersons. We were also encouraged by multiple reviewers who found the paper well structured and appreciated the clarity of its problem formulation and the strength of its experimental design.\n\nAt the same time, we understand the concerns raised regarding the use of simulated under-canopy data—specifically, **1) handheld/tripod capture, 2) staged actors, and 3) the suitability of the dataset for ICLR**. We address these common concerns below.\n\n---\n**1. Handheld/tripod capture (Domain Gap)**:  The reviewers are commonly concerned about the domain gap between the images captured by drone and those captured by handheld/tripod camera. To address these concerns, we conducted additional experiments focusing on motion-related artifacts caused by real MAV flights. Specifically, **we collected separate video clips by flying a MAV** equipped with a camera, capturing missing-person scenarios during flight, for evaluating models trained on ForestPersons. We also created motion-blurred variants of the training sets and trained detectors accordingly, then compared detector performance on models trained with and without motion-blur augmentation using a test dataset captured by real MAV.\n\nThe results (presented in the response to Reviewer KaHw) show that the detector trained without motion blur consistently performs better. This is likely because severe motion blur occurs primarily during aggressive MAV maneuvers, whereas in typical under-canopy scanning operations, such strong motion artifacts arise infrequently. Moreover, models trained with Forestperson show better performance compared to those trained with existing SAR datasets, even though they were collected using MAVs and UAVs. For detailed experimental settings, please refer to the rebuttal of W1 of Reviewer KaHw.\n\n---\n**2. Staged actors (Realism)**: Several reviewers raised a valid concern regarding the potential lack of realism in our staged scenarios, noting that real victims may present in more irregular poses (e.g., due to injury or hypothermia) than those captured by actors. We agree that the diversity of poses in actual distress situations is crucial for future improvement, but we respectfully emphasize that collecting data from real missing persons or placing volunteers in situations of actual physical danger (e.g., simulating extreme conditions) is **ethically prohibitive**.\n\nTo proactively address the demand for more extreme poses in a scalable and ethical manner, we are exploring the use of generative models finetuned with ForestPersons. We believe this represents a practical and scalable approach to bridge the gap between staged environments and real-world unpredictability. We will include preliminary examples of the synthetic images in the Appendix of our revised manuscript to demonstrate the feasibility of this essential future work.\n\n---\n**3. ICLR venue suitability**: We emphasize that the primary objective of this work aligns with the Datasets and Benchmarks track. Our goal is not to propose a new model architecture, but to provide a high-quality dataset and to formalize an under-canopy SAR problem grounded in real operational constraints. Prior ICLR-accepted works[1, 2, 3, 4] that provided the data necessary for enabling learning-based researches to address important yet underexplored problems were **consistently well-received at the venue**.\n\nIn the same spirit, ForestPersons introduces an unaddressed SAR under-canopy challenge and delivers the first dataset that alleviates the field’s major bottleneck—data scarcity—thereby enabling reproducible and quantitative follow-up research. Beyond domain-specific data collection, ForestPersons reveals that widely used object detection benchmarks overlook critical factors necessary for detection in natural environments, pointing to a gap between current benchmark assumptions and real under-canopy conditions. By publicly releasing the dataset, we provide a foundational resource for a socially consequential yet understudied problem in AI community.\n\n---\nWith the reviewer’s valuable reviews, in the revised manuscripts, we will have carefully incorporated the additional experimental results, synthetic data examples, and necessary clarifications into the manuscript to address these points comprehensively.\n\n---\n    [1] TAU-106K (Dataset for understanding traffic accident) https://openreview.net/forum?id=Fb0q2uI4Ha \n    [2] AIMS.au (Dataset for analysis of modern slavery countermeasures) https://openreview.net/forum?id=ybfmpJiKXX \n    [3] SWEb (Web dataset for Scandinavian languages) https://openreview.net/forum?id=vhPE3PtTgC \n    [4] CircuitNet 2.0 (Dataset for chip design) https://openreview.net/forum?id=nMFSUjxMIl"}}, "id": "mWxQ91lqJj", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763629326414, "cdate": 1763629326414, "tmdate": 1763635422395, "mdate": 1763635422395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback. We are pleased that all reviewers recognized the importance of addressing the under-canopy search and rescue (SAR) problem, and acknowledged the comparative analysis demonstrating limitations of existing benchmarks and detectors on ForestPersons. We were also encouraged by multiple reviewers who found the paper well structured and appreciated the clarity of its problem formulation and the strength of its experimental design.\n\nAt the same time, we understand the concerns raised regarding the use of simulated under-canopy data—specifically, **1) handheld/tripod capture, 2) staged actors, and 3) the suitability of the dataset for ICLR**. We address these common concerns below.\n\n---\n**1. Handheld/tripod capture (Domain Gap)**:  The reviewers are commonly concerned about the domain gap between the images captured by drone and those captured by handheld/tripod camera. To address these concerns, we conducted additional experiments focusing on motion-related artifacts caused by real MAV flights. Specifically, **we collected separate video clips by flying a MAV** equipped with a camera, capturing missing-person scenarios during flight, for evaluating models trained on ForestPersons. We also created motion-blurred variants of the training sets and trained detectors accordingly, then compared detector performance on models trained with and without motion-blur augmentation using a test dataset captured by real MAV.\n\nThe results (presented in the response to Reviewer KaHw) show that the detector trained without motion blur consistently performs better. This is likely because severe motion blur occurs primarily during aggressive MAV maneuvers, whereas in typical under-canopy scanning operations, such strong motion artifacts arise infrequently. Moreover, models trained with Forestperson show better performance compared to those trained with existing SAR datasets, even though they were collected using MAVs and UAVs. For detailed experimental settings, please refer to the rebuttal of W1 of Reviewer KaHw.\n\n---\n**2. Staged actors (Realism)**: Several reviewers raised a valid concern regarding the potential lack of realism in our staged scenarios, noting that real victims may present in more irregular poses (e.g., due to injury or hypothermia) than those captured by actors. We agree that the diversity of poses in actual distress situations is crucial for future improvement, but we respectfully emphasize that collecting data from real missing persons or placing volunteers in situations of actual physical danger (e.g., simulating extreme conditions) is **ethically prohibitive**.\n\nTo proactively address the demand for more extreme poses in a scalable and ethical manner, we are exploring the use of generative models finetuned with ForestPersons. We believe this represents a practical and scalable approach to bridge the gap between staged environments and real-world unpredictability. We will include preliminary examples of the synthetic images in the Appendix of our revised manuscript to demonstrate the feasibility of this essential future work.\n\n---\n**3. ICLR venue suitability**: We emphasize that our work’s objective aligns with the primary area of the ICLR Datasets & Benchmarks. Our goal is not to propose a new model architecture, but to provide a high-quality dataset and to formalize an under-canopy SAR problem grounded in real operational constraints. Prior ICLR-accepted works[1, 2, 3, 4] that provided the data necessary for enabling learning-based researches to address important yet underexplored problems were **consistently well-received at the venue**.\n\nIn the same spirit, ForestPersons introduces an unaddressed SAR under-canopy challenge and delivers the first dataset that alleviates the field’s major bottleneck—data scarcity—thereby enabling reproducible and quantitative follow-up research. Beyond domain-specific data collection, ForestPersons reveals that widely used object detection benchmarks overlook critical factors necessary for detection in natural environments, pointing to a gap between current benchmark assumptions and real under-canopy conditions. By publicly releasing the dataset, we provide a foundational resource for a socially consequential yet understudied problem in AI community.\n\n---\nWith the reviewer’s valuable reviews, in the revised manuscripts, we will have carefully incorporated the additional experimental results, synthetic data examples, and necessary clarifications into the manuscript to address these points comprehensively.\n\n---\n    [1] TAU-106K (Dataset for understanding traffic accident) https://openreview.net/forum?id=Fb0q2uI4Ha \n    [2] AIMS.au (Dataset for analysis of modern slavery countermeasures) https://openreview.net/forum?id=ybfmpJiKXX \n    [3] SWEb (Web dataset for Scandinavian languages) https://openreview.net/forum?id=vhPE3PtTgC \n    [4] CircuitNet 2.0 (Dataset for chip design) https://openreview.net/forum?id=nMFSUjxMIl"}}, "id": "mWxQ91lqJj", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763629326414, "cdate": 1763629326414, "tmdate": 1763683272022, "mdate": 1763683272022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a dataset on under-tree canopy people detection, which is different from the over-canopy person detection datasets. Therefore, it contributes a significant new novel step forward in its domain, no dataset like this exists already. The data is big with a lot of varying background conditions. They provide a detailed benchmark with various backbones and also give an analysis on the possible weaknesses of this dataset as well  (inter-annotator dis-/ agreements, failure cases, recall etc). Also, the dataset is available on huggingface and I have downloaded and probed around in it to verify that it has the contents and labels that it claims."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Well written and thorough benchmarks with different levels of difficulty and settings."}, "weaknesses": {"value": "The length of the clips is highly variable, between 50-450 frames. An analysis of how the number of frames available vs the person detection accuracy is needed. Does collecting more data on a scene from various angles help get better performance? What is the ideal number of frames, after which the gains are minimal?\n\nOne thing I don't feel comfortable is the pose classification. On an initial read, it feels  like they are providing actual human pose instead of what they have provided: lying down, sitting, and standing classifications."}, "questions": {"value": "Is it possible to get an above canopy and a below canopy view, i.e. fly two different drones at the same time ? This could open generative applications, i.e. generating under canopy views from over-canopy views? Something like what was done in the AG-Reid.v2 dataset https://arxiv.org/pdf/2401.02634?\n\nThere has been a lot of interest in having fast FPS processing, especially on edge devices. It is not necessarily related to the quality of your dataset, but since you have already done experiments with multiple methods and backbones on different hardware, it will be interesting to see a column/section on the FPS of these various methods? Especially a FPS (or model size or compute time/memory requirement) vs accuracy and uncertainty?\n\nAny experiments on if data attribute (pose, weather, location type,  weather etc) prediction or making the model aware of the attributes helps with improving person detection performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8vrtlbH3H", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Reviewer_stPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Reviewer_stPN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970320832, "cdate": 1761970320832, "tmdate": 1762926826666, "mdate": 1762926826666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ForestPersons, a large-scale dataset for under-canopy missing person detection in Search and Rescue (SAR) missions, containing 96,482 images and 204,078 annotations collected from ground-level perspectives to simulate Micro Aerial Vehicle viewpoints. The dataset captures individuals in diverse poses (lying, sitting, standing) who are naturally partially occluded by vegetation, branches, and terrain across different seasons, weather conditions, and lighting. Individuals were positioned in different postures and naturally partially occluded by vegetation, branches, or uneven terrain, with annotations including bounding boxes, pose labels, and visibility levels. Experiments demonstrate large performance drops when applying models trained on existing SAR datasets to ForestPersons, and significant degradation from ground-level person datasets, establishing the need for domain-specific training data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental validation demonstrates a substantial practical problem, with existing SAR models showing catastrophic performance drops on under-canopy scenarios, providing strong empirical evidence for the dataset's necessity and filling a genuine gap in \"Search and Rescue\" applications that could have real-world impact for missing person detection."}, "weaknesses": {"value": "-- limited technical and scientific novelty: This is mainly a domain-specific dataset contribution without methodological innovations in computer vision or machine learning. The work involves training standard object detection models on forest imagery and demonstrates expected domain transfer limitations, offering no new architectures, techniques, or fundamental insights beyond data collection for a specific application scenario.\n\n-- narrow scope and generalizability: The dataset addresses a very specific task (under-canopy person detection for SAR) with limited broader applicability to computer vision research. While the data collection required significant effort and funding, the contribution is primarily valuable to SAR practitioners rather than advancing general object detection, occlusion handling, or robustness techniques that could benefit the wider research community.\n\n-- simulated rather than authentic data: The dataset uses staged photography with handheld/tripod cameras positioned at 1.5-2m height to simulate MAV perspectives, rather than actual drone footage from real SAR missions, raising questions about ecological validity and whether the simulated conditions truly represent operational SAR scenarios."}, "questions": {"value": "Have you evaluated modern Large Vision-Language Models like GPT-5/4o, Gemini, or Claude on your benchmark? (also open source models like MOLMO?) These models often demonstrate strong zero-shot object detection and description capabilities across diverse domains. Given that your evaluation focuses on traditional object detection models (YOLO, Faster R-CNN, etc.) mostly from 2015-2021, it's unclear whether the identified performance gaps persist with state-of-the-art VLMs that might already handle under-canopy person detection effectively without domain-specific training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bOxiLAYnyk", "forum": "ZC2N9q8USm", "replyto": "ZC2N9q8USm", "signatures": ["ICLR.cc/2026/Conference/Submission16789/Reviewer_DXNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16789/Reviewer_DXNw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762085888262, "cdate": 1762085888262, "tmdate": 1762926826223, "mdate": 1762926826223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}