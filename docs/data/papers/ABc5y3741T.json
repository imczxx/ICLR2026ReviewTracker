{"id": "ABc5y3741T", "number": 5771, "cdate": 1757933925005, "mdate": 1759897954988, "content": {"title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought", "abstract": "Recent frontier models employ long-chain-of-thought reasoning to explore solution spaces in context and achieve stronger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduce **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artifacts.  As a Korean case study, we curate **Yi-Sang-HQ**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train nine models (4B–35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score ($64.0_{\\pm2.5}$), ranking first on 5/9 benchmarks and second on the remainder. Smaller and mid-sized models also benefit substantially, with an average improvement of $+18.6$ points across the evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, indicating that reasoning patterns can be engineered to improve non-English performance. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning.", "tldr": "", "keywords": ["Multilingual", "Math", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45d819ea619135243ae1c9df9048acaadec51904.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To improve mid-resource language reasoning, this paper introduces a new paradigm, language-mixed CoT, which switches between English and the target language. In this way, English acts as an anchor to excel in reasoning while minimising translation artefacts. To do so, the authors perform a study using Korean as an example, collecting a large number of questions from various scopes and the corresponding reasoning solutions generated from Qwen3-32B. Experimental results show that models fine-tuned on this dataset achieve substantial performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed paradigm, dataset collection recipe, and collected dataset may be of interest to sub-communities\n- The experimental setup is broad, covering a variety of model sizes (4B-35B) and different evaluation benchmarks"}, "weaknesses": {"value": "- In line 61, '...,leaving open how to achieve language-specific reasoning', which is a very vague statement. In fact, many works have proposed approaches to improve reasoning for low-resource languages, such as language alignment[1],  multilingual fine-tuning [2, 3], and cross-lingual fine-tuning [4]. Instead of focusing on a single language, these works study reasoning in a broader scenario, i.e., multiple low-resource languages. Also, the general pipeline of this work is to collect data and extract responses from a teacher model, followed by supervised fine-tuning. Overall, I think the novelty of this paper is very limited\n- In lines 222-223,  'To pursue truly multilingual reasoning,...', this is very vague. What is truly multilingual reasoning? This is not clear to me. If the authors consider language-mixed reasoning to be true multilingual reasoning, a rigorous definition or reference is needed.\n- The contribution is overclaimed. For example, the authors state that their models outperform closed systems, which are defined by training on closed data. If this definition is adopted, almost all current state-of-the-art models, including many labelled as open-source, would fall under the \"closed\" category.\n- Distilling data from larger models and using it to fine-tune smaller models makes sense, but I do not understand why data is used to fine-tune the comparable model (Qwen3-32B vs KO-REAson-35B) in this work. This shows that the teacher model (Qwen3-32B) is good enough.\n- Taking language-Mixed CoT as a more effective supervision signal is an interesting direction, while this paper lacks an in-depth related analysis and discussion. For instance, what specific aspects of language mixing contribute to better performance? Does it help bridge cross-lingual semantic gaps, improve cross-lingual reasoning consistency, or simply leverage the stronger reasoning ability in English?\n\n\n[1] Zhu et al. Question Translation Training for Better Multilingual Reasoning. ACL 2024 Findings  \n[2] Lai et al. mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models. ACL 2024  \n[3] Chen et al. Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. EMNLP 2024 Findings  \n[4] Chai et al. xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning. AAAI 2025  \n\n\nTypos/suggestions:\n- Line 179: no space between text and footnote number\n- Line 194: table caption is too close to the above text\n- The caption of Figure 4 is unreadable, with too much text in bold; this figure is first referenced in lines 317-323, but some information (e.g. style and option) in the figure is introduced in the later part."}, "questions": {"value": "See above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NePe3Jn6LN", "forum": "ABc5y3741T", "replyto": "ABc5y3741T", "signatures": ["ICLR.cc/2026/Conference/Submission5771/Reviewer_7kJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5771/Reviewer_7kJM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533582846, "cdate": 1760533582846, "tmdate": 1762918251672, "mdate": 1762918251672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to reviewers"}, "comment": {"value": "We would like first to thank all reviewers for their efforts and time spent reading and reviewing our paper.\nIn this comment, we summarize three general issues that have been repeatedly questioned across different reviewers.\n\n--- \n\n## Main scientific inquiry of this paper.\nRecent releases of “open‑weight but closed recipe models” (e.g., GPT‑OSS, QwQ, EXAONE) report strong results. Still, one can easily notice that they rarely have any details into which training decisions drive gains, especially when the goal is to build high‑quality non‑English/non‑Chinese reasoning models. Our scientific aim is to make those decisions explicit and test them systematically. We begin by identifying practical obstacles in this setting: the scarcity of target-language datasets, the lack of high-quality supervision, and the failure modes of naively translated reasoning traces.\n\n**Methodology and findings.** In this work, we present a replicable recipe: (i) address data scarcity via targeted crawling plus careful translation; (ii) curate a high‑quality subset with explicit filtering, decontamination, and ablations; and (iii) compare English‑only and language‑mixed chain‑of‑thought (CoT) supervision to identify when mixing helps. Our experiments demonstrate that this recipe yields a robust target-language reasoner that generalizes to held-out tasks, transfers to vision–language settings, and maintains solid performance in English.\n\n**Positioning to prior work.** We thank the reviewers for pointing out missing citations, which we will add during the review period. However, to summarize our differences with the referenced works, relative to multilingual CoT/alignment work [1–4], which primarily evaluates on grade-school benchmarks (e.g., MGSM), **our study tests whether similar ideas scale to more challenging reasoning tasks.** Qi et al. (2025) [5] examine “prompting”(or prefix control) of the thinking language at inference time and reports a readability–accuracy trade‑off; in contrast, we investigate “training‑time supervision” with language‑mixed traces, across multiple model sizes and families, and **find that such control is not only an effective prompting mechanism but also a proper supervision signal for building multilingual reasoners.**\n\nWe would like to highlight that, like all scientific works, we build on existing methods. However, through extensive efforts in data curation and training, we demonstrate empirical evidence to prove and guide future work, showing that such multilingual supervision can be extended to train much stronger reasoners.\n\nReferences:  \n[1] Zhu et al., Question Translation Training for Better Multilingual Reasoning, ACL‑Findings 2024.  \n[2] Lai et al., mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models, ACL 2024.  \n[3] Chen et al., Breaking Language Barriers in Multilingual Mathematical Reasoning, EMNLP‑Findings 2024.  \n[4] Chai et al., xCoT: Cross‑lingual Instruction Tuning for Cross‑lingual CoT Reasoning, AAAI 2025.  \n[5] Qi et al., When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy, 2025  \n\n---\n\n## Limited teacher choice\n\nWe would like to note that our paper already includes extensive experiments, with over 100 ablations spanning teacher models, augmentation schemes, and seed sources, to highlight the decisions that matter. Within the teacher dimension, we compare long-CoT teachers (Qwen3‑32B, Qwen3‑4B) and short-CoT/solution-only variants (Gemini‑2.5‑Pro; Qwen3‑32B with reasoning disabled). See Figure 4 for the aggregate teacher comparison and Table 9 for per‑benchmark details. \n\nHowever, as requested by the reviewers, to widen teacher choice, **we have just started a new distillation with DeepSeek‑R1‑32B**, using the same language-mixed prompting and filtering pipeline. As in §4/§5.3, we generate a large superset and then down-select via degeneration filters, Korean-ratio bounds (5–20%), and n-gram decontamination before training students on the filtered set. On our hardware (8×H200), producing a ~50k high‑quality subset requires ≈30 wall‑clock hours; we are now training and evaluating student models and will add the results during the review period."}}, "id": "IAfL73gleo", "forum": "ABc5y3741T", "replyto": "ABc5y3741T", "signatures": ["ICLR.cc/2026/Conference/Submission5771/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5771/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5771/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763362605181, "cdate": 1763362605181, "tmdate": 1763362605181, "mdate": 1763362605181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive case study on building strong reasoning models for Korean, a mid-resource language. Its primary contributions are: (1) the introduction of \"Language-Mixed Chain-of-Thought,\" a supervision method that allows models to reason primarily in English while preserving key Korean terms; (2) the curation and release of Yi-Sang, a large-scale (5.79M prompts, 3.7M reasoning traces) Korean instruction-tuning dataset sourced from native web content; and (3) the distillation of this into Yi-Sang-HQ, a high-yield 260k subset, used to train the KO-REAson model series (4B-35B), which demonstrates state-of-the-art or competitive performance on a suite of nine Korean benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The proposed \"Language-Mixed CoT\" is a thoughtful and empirically grounded solution to the known problems of translation artifacts (from English-only CoT) and degraded reasoning (from Korean-only CoT). The ablation studies in Table 2 robustly support its effectiveness.\n\n(2) The scale and nature of the Yi-Sang dataset fill a critical gap in the ecosystem. Moving beyond translated corpora to native, web-crawled prompts is a crucial step for building robust, real-world multilingual models. The detailed documentation of the data collection and refinement pipeline is highly valuable for the community.\n\n(3) The authors conduct over 100 ablations to justify key decisions (teacher model, data categories, augmentation strategies) and demonstrate the efficacy of their final dataset across nine different models from six families. This greatly strengthens the claims and provides a reproducible recipe."}, "weaknesses": {"value": "(1) The comparison in Table 1 is excellent, but it would be beneficial to include a baseline that represents a more standard approach—for instance, a model trained on a directly translated version of a high-quality English reasoning dataset (like OpenThought)—to more directly isolate the benefit of native prompts and Language-Mixed CoT versus simply having more Korean data.\n\n(2) The process of down-selecting to the 260k Yi-Sang-HQ subset is well-documented, but the specific ratios chosen for the final mixture (62k OpenThought, 86k Code, etc.) feel somewhat arbitrary. A more principled approach (e.g., based on performance-density or optimal mixing ratios) would have strengthened this part of the methodology, though the strong end results justify the outcome."}, "questions": {"value": "(1) Could you provide more detail on the n-gram decontamination process? Specifically, what was the pass/fail criterion, and what fraction of the data was removed? \n\n(2) You show impressive gains from the 260k Yi-Sang-HQ subset. Did you observe any indications of performance saturation? Are there plans to scale training to the full 3.7M instance Yi-Sang dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lOjLT8yDfi", "forum": "ABc5y3741T", "replyto": "ABc5y3741T", "signatures": ["ICLR.cc/2026/Conference/Submission5771/Reviewer_Rtnv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5771/Reviewer_Rtnv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823735432, "cdate": 1761823735432, "tmdate": 1762918251249, "mdate": 1762918251249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hybrid language thought chain (CoT) supervision scheme that alternates between English \"anchoring\" and the target language (Korean) for thought training to maintain reasoning ability while reducing translation noise. The paper also constructs a large Korean post-training corpus, YI-SANG (5.79 million cue words --> 3.7 million long CoTs), and refines it into a high-yield subset (YI-SANG-HQ) containing 260,000 words through targeted ablation and filtering (e.g., removing traces longer than 16,000 lemmas; n-gram decontamination). Using this data, the paper trains nine instruction-tuned models across six series (4B–35B), achieving state-of-the-art results on nine Korean benchmarks using KO-REAson-35B. Furthermore, despite being trained only on text, the model is transferable to English reasoning and Korean VLM tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is simple but effective. Language-Mixed CoT consistently beats monolingual CoTs across models/sizes (Table 1). The description and motivation are clear. Also, it introduces a large and native prompt corpus of 5.79M Korean user-authored prompts, 3.7M CoTs, downselected to a 260k high-yield mix with concrete rules. Thorough ablations are conducted. \nThe results clearly demonstrate that KO-REAson-35B ranks 1st on 5/9 and 2nd on the rest, and improvements persist across 4B–35B and multiple families. In addition, the paper also reveals that text-only training still helps English reasoning (AIME-25/GPQA) and Korean VLM reasoning tasks."}, "weaknesses": {"value": "**Missing citations to relevant works & Positioning vs closely related work.** Qi et al. (2025) [1] show that forcing models to think in the user's language improves readability but can hurt accuracy, revealing a trade-off. They also try to force the model to reason in the target language via prefix-hacking. The work is closely related to this paper. Although there are differences, it would still be important to include and clarify the distinctions with that work.\n\n**Limited Choice of Teacher model.** Most long-CoT supervision relies on Qwen3-32B. It is promising to explore other teacher choices or alternative anchors (e.g., non-English).\n\n**Contamination/Overfitting Risk.** Although n-gram decontamination (n=13) is applied, the paper acknowledges iterative use of held-in benchmarks as proxies and shows larger gains on held-in vs held-out. More contamination checks (exact-match, semantic) would increase the solidity of the findings.\n\n---\n\n***Reference***:\n\n*[1] When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy. Qi et al., 2025*"}, "questions": {"value": "Based on the Weaknesses, several questions and suggestions are listed below:\n\n**Comparison to existing work** Include and discuss the distinctions with the existing work [1].\n\n**Anchor choice** It would be better to write more about the pros and cons of selecting English as the anchor. In practice, sometimes we may only have access to sufficient corpora of non-English languages, like a Switzerland company may only have training data in German, French, and some Italian, with less English data. Have you tried other languages as the anchor, and how do they affect accuracy and readability?\n\n**Teacher sensitivity** It would be promising to also attempt different teacher models (e.g., Gemini-2.5-Pro reasoning enabled, or open R1 variants) and see if the results or trends remain similar.\n\n**Beyond n-gram check.** In addition to n-gram filtering (n=13), semantic matching or exact match checks on each benchmark dataset will make the results more robust.\n\n---\n\n***Reference***:\n\n*[1] When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy. Qi et al., 2025*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qxjwOucHrs", "forum": "ABc5y3741T", "replyto": "ABc5y3741T", "signatures": ["ICLR.cc/2026/Conference/Submission5771/Reviewer_oqNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5771/Reviewer_oqNR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877218543, "cdate": 1761877218543, "tmdate": 1762918250362, "mdate": 1762918250362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Language-Mixed Chain-of-Thought (CoT) as a multilingual reasoning strategy and introduces Yi-Sang-HQ, a 260k-example Korean reasoning dataset generated under this schema. The work aims to enhance the robustness of reasoning for non-English languages and to demonstrate that open-source pipelines can rival those of closed-source systems. The authors conduct small-scale ablations on language-mixing (Table 1), category-wise analyses for dataset composition (Table 2), and large-scale fine-tuning across nine model families and parameter ranges (Table 4)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong practical motivation and relevance. Addresses the real problem of reasoning degradation in translated multilingual data and proposes a scalable, open alternative.\n2. High-value resource contribution. The proposed Korean reasoning corpora, Yi-Sang-HQ, is well-engineered, with transparent filtering and selection procedures.\n3. Conceptually interesting reasoning schema. Language-Mixed CoT is novel and intuitive, supported by small-scale ablation results."}, "weaknesses": {"value": "1. Unclear Statement of Enquiry. It’s difficult to judge the scientific merit of this work without a clearly defined statement of enquiry. The closest thing to it appears in the first sentence of Section 4: “When constructing multilingual reasoning data in a target language (other than English), a central question is how to represent the reasoning process: should it be written in the target language or left in English?” However, no question guiding the inquiry is stated in Section 5. In other words, the authors do not propose how this dataset can be used to perform any knowledge-generating investigations. Yi-Sang-HQ is presented as an engineering resource to improve model performance, not as a scientific instrument for discovering new insights about multilingual reasoning.\n2. Small-Scale Experiment to Verify Language-Mixed CoT. Given that Language-Mixed CoT is presented as one of the contributions, readers might expect an extensive empirical investigation of whether reasoning traces should be written in English, the native language, or a mix of both. In practice, Table 1 is intentionally limited in scope; it involves only two small models and a few held-in benchmarks, operating within the confines of an ablation study whose role is to guide dataset construction. This design choice is reasonable, but it also means the experiment serves as design verification rather than as a standalone empirical contribution."}, "questions": {"value": "1. According to Weakness 1: Could the authors clarify the scientific enquiry that this dataset enables? It would strengthen the paper if the authors could explicitly articulate how the dataset can be used to investigate knowledge-generating questions about multilingual reasoning—such as what reasoning patterns transfer across languages, or how mixed-language supervision affects reasoning generalization.\n2. According to Weakness 2: as of now, I think only the first and third contributions are valid. The presentation will be strengthened by focusing on the dataset contribution, while ablation studies serve as design support rather than a standalone contribution. Would the authors agree with this statement?\n3. On the usage of the term \"language-mixed\" vs \"code-switched.\" When I first read the title (and even the abstract), I thought the paper would be about LLM reasoning in one language while answering in another language (Fig.1 helps clarify this though.) I'm not sure if code-switching would be a better word choice for this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uogC0zYjzU", "forum": "ABc5y3741T", "replyto": "ABc5y3741T", "signatures": ["ICLR.cc/2026/Conference/Submission5771/Reviewer_BzTR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5771/Reviewer_BzTR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928442775, "cdate": 1761928442775, "tmdate": 1762918249885, "mdate": 1762918249885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}