{"id": "80JylHgQn1", "number": 2569, "cdate": 1757148120356, "mdate": 1759898140303, "content": {"title": "Instilling an Active Mind in Avatars via Cognitive Simulation", "abstract": "Current video avatar models can generate fluid animations but struggle to capture a character's authentic essence, primarily synchronizing motion with low-level audio cues instead of understanding higher-level semantics like emotion or intent. To bridge this gap, we propose a novel framework for generating character animations that are not only physically plausible but also semantically rich and expressive. Our model is built on two technical innovations. First, we employ Multimodal Large Language Models to generate a structured textual representation from input conditions, providing high-level semantic guidance for creating contextually and emotionally resonant actions. Second, to ensure robust fusion of multimodal signals, we introduce a specialized Multimodal Diffusion Transformer architecture featuring a novel Pseudo Last Frame design. This allows our model to accurately interpret the joint semantics of audio, images and text, generating motions that are deeply coherent with the overall context. Comprehensive experiments validate the superiority of our method, which achieves compelling results in lip-sync accuracy, video quality, motion naturalness, and semantic consistency. The approach also shows strong generalization to challenging scenarios, including multi-person and non-human subjects. Our video results are linked in https://anonymous.4open.science/w/InstillinganActiveMindinAvatars_Anonymous/ .", "tldr": "This paper introduces a novel framework that uses a Large Language Model (LLM) for semantic guidance and a Multimodal Diffusion Transformer (DiT) for fusion to generate expressive, context-aware video avatars, demonstrating competitive performance", "keywords": ["Video Generatio", "Human Animation", "Avatar", "Multimedia"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c757ee86de7073716e106c068add1dc6f67fe5b.pdf", "supplementary_material": "/attachment/90413298ed6401185218d90b9f159f4ce3b242c1.zip"}, "replies": [{"content": {"summary": {"value": "This study proposes an avatar video generation framework consisting of two synergistic systems: System 1 is responsible for audio-to-video generation, and System 2 undertakes tasks of context awareness and logical inference. Through the collaboration of the two systems, the framework can better understand contextual semantic information to generate avatar videos; verified by extensive experiments, this method can significantly improve the quality and vividness of the generated content.​"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed dual-system avatar generation framework makes full use of the capabilities of MLLMs and optimizes the performance of video generation models. It provides an accurate analysis of the current dilemmas in avatar generation tasks, demonstrating strong innovation and inspiration in this field. Moreover, the dual-system model is expected to be applied to more general video generation frameworks, reflecting its high potential.​\n- It puts forward optimization details such as \"pseudolast-frame\". Aiming at the problem of error accumulation in autoregressive generation in the field of video generation, this method effectively alleviates this issue and may significantly improve the performance in real-time streaming generation scenarios.​\n- The experimental design is comprehensive and extensive, covering a large number of comparative experiments, ablation experiments, and professional user studies, which provides strong support for the research conclusions.​"}, "weaknesses": {"value": "In the current research, the combination of MLLM and MMDiT lacks tightness, and MLLM tends to play the role of a \"prompt refiner\"."}, "questions": {"value": "The following points need to be further clarified:​\n- What are the core differences between the application of MLLM in this research and existing prompt refiner methods?​\n- What specific technical efforts has the authors made to enhance the synergy between MLLM and MMDiT?​"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MJi99eFXeV", "forum": "80JylHgQn1", "replyto": "80JylHgQn1", "signatures": ["ICLR.cc/2026/Conference/Submission2569/Reviewer_fLxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2569/Reviewer_fLxy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882645694, "cdate": 1760882645694, "tmdate": 1762916285901, "mdate": 1762916285901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage framework for avatar-centric video generation conditioned on audio, text, and reference images. In the first stage, a video plan is generated, while in the second stage, the actual video is synthesized based on this plan and the multimodal conditions. Notably, when incorporating the reference image, the paper introduces the Pseudo Last Frame (PLF) technique, which places the reference image at a distant, unreachable position in RoPE. This approach introduces reference information while avoiding forcing the model to regress to that exact image.\n\nAs far as I know, two-stage frameworks have been explored in various contexts, and the paper's emphasis on \"cognitive simulation\" appears to be more of a conceptual framing. Therefore, there are certain limitations in terms of novelty. However, considering the effort involved in adapting to this specific domain, the innovation of PLF, and the promising qualitative and quantitative results, I am tentatively inclined to accept this paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and easy to follow. The figures and tables are well-designed and visually appealing.\n\n2. The proposed PLF is quite interesting and demonstrates effectiveness.\n\n3. The quantitative metrics show notable improvements. The visualization and supplementary video results demonstrate significant quality enhancements."}, "weaknesses": {"value": "1. The proposed two-system framework—using an LLM as a planner followed by another system/model as an executor—has been explored in various contexts including video generation, text-to-image generation [1], robotics [2], and audio generation [3]. While the application to avatar generation shows merit, the overall structural approach is relatively well-established in the literature.\n\n2. The concept of \"cognitive simulation\" proposed in the paper could benefit from more rigorous justification. From my understanding, what is referred to as cognitive simulation appears to largely correspond to the planner-executor framework mentioned above. Given the prevalence of this framework across different domains, it would be helpful to clarify the specific advantages or unique aspects that the cognitive simulation framing brings to this particular application."}, "questions": {"value": "1. Regarding PLF, during inference, is the reference image placed only at the \"pseudo last frame\" position, or is it placed at both the first frame and the \"pseudo last frame\" position simultaneously?\n\n2. Is there a comparison between PLF and the prior method that \"conditions the model on a reference image sampled from the training video\"?\n\n3. How does PLF perform on image-to-video benchmarks? In other words, within a broader scope, is PLF a superior method for introducing reference images in image-to-video generation?\n\n##### References\n\n[1] Qu L, Wu S, Fei H, et al. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 643-654.\n\n[2] Kannan S S, Venkatesh V L N, Min B C. Smart-llm: Smart multi-agent robot task planning using large language models[C]//2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024: 12140-12147.\n\n[3] Liang J, Zhang H, Liu H, et al. Wavcraft: Audio editing and generation with natural language prompts[C]. ICLR 2024 Workshop on LLM Agents, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MKpsa8cfu0", "forum": "80JylHgQn1", "replyto": "80JylHgQn1", "signatures": ["ICLR.cc/2026/Conference/Submission2569/Reviewer_Rgov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2569/Reviewer_Rgov"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813398546, "cdate": 1761813398546, "tmdate": 1762916285651, "mdate": 1762916285651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles controllable avatar generation that produces semantically meaningful, context-aware motion beyond basic lip-sync. The key challenge is aligning high-level cognitive intent with low-level reactive control, as naive multimodal fusion often causes semantic interference. To address this, it proposes a cognitive-simulation framework combining high-level reasoning (System 2) with low-level motion generation (System 1). Experiments demonstrate consistent improvements across multiple metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces a clear and creative dual-system framework (System 1 and System 2) for avatar generation, combining cognitive reasoning with generative modeling in a fresh way. The architecture includes well-designed and tested components such as the MMDiT backbone, semantic soft guidance, and the Pseudo Last Frame, which together improve the link between reasoning and motion generation. The experiments are thorough and the results show that the proposed method produces more expressive and context-aware avatars than strong baselines like VASA-1 and SadTalker, with ablation studies further supporting each design choice."}, "weaknesses": {"value": "While MLLMs are powerful for reasoning, they come with high computational costs. For tasks such as analyzing a character's speech content, emotion, or planning shot composition, using a large MLLM may be unnecessary. A smaller, task-specific model jointly trained with the generative system could offer a more unified and efficient solution. Although incorporating an MLLM adds conceptual richness, the paper does not provide any quantitative analysis of the additional computational or latency overhead. Nonetheless, the work presents a creative and promising direction by integrating cognitive reasoning into avatar generation and demonstrates clear value. I would recommend a score above the acceptance bar."}, "questions": {"value": "It would be helpful to better understand the robustness and efficiency of the proposed System 2 reasoning module. Specifically, are the results consistent when the reasoning outputs contain noise, particularly common hallucinations, or ambiguity in the input image or audio? Additionally, can the System 2 module be made more lightweight, and is there any analysis of its additional inference time or computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qZFE1Thm9s", "forum": "80JylHgQn1", "replyto": "80JylHgQn1", "signatures": ["ICLR.cc/2026/Conference/Submission2569/Reviewer_1USz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2569/Reviewer_1USz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992355272, "cdate": 1761992355272, "tmdate": 1762916285484, "mdate": 1762916285484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for generating dynamic and expressive video avatars by \"instilling an active mind,\" drawing an analogy from the dual-process theory of human cognition (System 1 and System 2). The authors argue that existing avatar models are largely \"System 1\" agents, capable of reactive tasks like lip-sync but failing to exhibit higher-level semantics like emotion, intent, or contextual awareness. To bridge this gap, they propose a dual-system framework. A \"System 2\" module, termed Agentic Reasoning and powered by a Multimodal Large Language Model (MLLM), acts as a deliberative planner. It processes all input modalities (audio, reference image, text prompt) to generate a high-level, structured plan (in JSON format) that outlines the avatar's expressions, actions, and emotional journey. This plan then guides a \"System 1\" module, a reactive rendering engine built upon a novel Multimodal Diffusion Transformer (MMDiT). This engine synthesizes the final video, conditioned on both the high-level plan and the immediate audio signal. Key technical innovations include a \"Pseudo Last Frame\" (PLF) conditioning mechanism to maintain identity without sacrificing motion dynamics, and a specialized training strategy (MM-Branch Warm-up) to effectively train the complex multi-modal architecture. The authors demonstrate state-of-the-art performance through extensive experiments, including quantitative metrics and user studies, against a wide range of strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The System 1/System 2 analogy is a standout feature, providing a clear and compelling motivation for the entire architecture. It reframes the problem in a way that is likely to inspire future work. Meanwhile, such method will improve the performance of avatars in the practice application.\n2. The introduction of the Pseudo Last Frame (PLF) to decouple identity from static posture is a significant practical contribution that directly addresses a common failure mode in existing models.\n3. The model achieves state-of-the-art performance across a comprehensive suite of metrics and, most tellingly, wins decisively in head-to-head user preference studies against top-tier competitors. And the paper provides textbook-quality ablation studies that convincingly demonstrate the necessity and effectiveness of its proposed components, from the high-level reasoning module down to specific architectural choices. Meanwhile, results shows that the model's success in diverse scenarios, including multi-person turn-taking and non-human avatars, highlights the robustness and power of the proposed framework."}, "weaknesses": {"value": "1. About the Latentcy: The framework, involving a large MLLM for planning followed by a large diffusion transformer for rendering, is undoubtedly computationally intensive. The paper mentions training on a very large dataset (11,000 hours) and inference at 480p. A discussion of the computational requirements (e.g., VRAM, training time) and, more importantly, the ``inference latency'' would be valuable for understanding the practical deployability of the system.\n2. The experiments primarily showcase clips of up to 720 frames (around 30 seconds). While the Agentic Reasoning module is designed for long-range planning, the paper does not explicitly test the limits of this coherence over much longer durations (e.g., several minutes). It would be interesting to know if any failure modes, such as narrative drift or loss of consistency, emerge in longer-form content generation.\n3. A more complex settings with difficult reasoning will be beneficial to claim the improvement of this paper, and the setups of such benchmark will be useful for fairly comparison."}, "questions": {"value": "See the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q0W0NUQvMQ", "forum": "80JylHgQn1", "replyto": "80JylHgQn1", "signatures": ["ICLR.cc/2026/Conference/Submission2569/Reviewer_MK7E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2569/Reviewer_MK7E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762833889441, "cdate": 1762833889441, "tmdate": 1762916285312, "mdate": 1762916285312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}