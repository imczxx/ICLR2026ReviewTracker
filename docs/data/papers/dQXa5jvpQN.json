{"id": "dQXa5jvpQN", "number": 11241, "cdate": 1758194179192, "mdate": 1759897599089, "content": {"title": "ToolTweak: An Attack on Tool Selection in LLM-based Agents", "abstract": "As LLMs increasingly power agents that interact with external tools, tool use has become an essential mechanism for extending their capabilities. These agents typically select tools from growing databases or marketplaces to solve user tasks, creating implicit competition among tool providers and developers for visibility and usage. In this paper, we show that this selection process harbors a critical vulnerability: by iteratively manipulating tool names and descriptions, adversaries can systematically bias agents toward selecting specific tools, gaining unfair advantage over equally capable alternatives.\nWe present **ToolTweak**, a lightweight automatic attack that increases selection rates from a baseline of around 20\\% to as high as 81\\%, with strong transferability between open-source and closed-source  models.\nBeyond individual tools, we show that such attacks cause distributional shifts in tool usage, revealing risks to fairness, competition, and security in emerging tool ecosystems.\nTo mitigate these risks, we evaluate two defenses: paraphrasing and perplexity filtering, which reduce bias and lead agents to select functionally similar tools more equally.\nAll code will be open-sourced upon acceptance.", "tldr": "We investigate attacks biasing an API to be selected over comparable alternatives and how to defend against them.", "keywords": ["Tool Use", "LLM Agents", "Fairness", "Bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49a353a6d0e5a8ca7ab6c47713a44b2af728eb2d.pdf", "supplementary_material": "/attachment/cb5d1d2b5293c1fed87a483c3f52c038087c1788.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces ToolTweak, a gradient-free adversarial attack that iteratively rewrites a tool’s name and description using an attacker LLM to increase its selection rate by LLM-based agents. The attack assumes the adversarial tool provider can modify their own tool’s metadata and observe usage statistics. The authors demonstrate that ToolTweak significantly increases selection rates across multiple models and also evaluate simple paraphrasing defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to read.\n\nThe authors clearly present the setup, problem formulation, and threat model."}, "weaknesses": {"value": "1. Editing tool descriptions to increase selection rates has already been explored in prior work, including [1] and [2]. This makes the novelty and contribution of this paper incremental. Additionally, the authors fail to cite and discuss [2], which presents a very similar black-box attack. \n\n\n2. The assumptions made in ToolTweak are strong: the attack requires access to usage statistics and full visibility into the entire tool database. This level of access is unrealistic for many practical deployment settings. Even under this setup, as shown in Table 1, the attack underperforms compared to simple manual suffix addition.\n\n3. The defense strategies evaluated are relatively simple and resemble standard jailbreak mitigation techniques. A deeper exploration of defenses specifically tailored to agentic LLM tool selection would be more meaningful and impactful.\n\n4. In the main results (Section 4.3), the attacker and victim models are the same. This setup risks overfitting to model-specific biases and undermines generalizability.\n\n[1] Faghih, Kazem, et al. \"Gaming Tool Preferences in Agentic LLMs.\" \n\n[2] Chen, Liuji, et al. \"Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection.\""}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wUVC3UKalN", "forum": "dQXa5jvpQN", "replyto": "dQXa5jvpQN", "signatures": ["ICLR.cc/2026/Conference/Submission11241/Reviewer_jvww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11241/Reviewer_jvww"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760900129392, "cdate": 1760900129392, "tmdate": 1762922401887, "mdate": 1762922401887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates vulnerabilities in tool selection within large language model agents. It introduces ToolTweak, a black box optimization procedure that edits tool names and descriptions to increase the chance that a target tool is chosen. Experiments show strong effects across several models, with selection rates rising from about twenty percent to more than eighty percent. The authors also evaluate simple mitigation methods such as paraphrasing and perplexity filtering."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper highlights a realistic security and fairness issue in tool based ecosystems that has not been thoroughly explored before.\nIt provides clear empirical results with consistent quantitative gains across different models.\nThe work connects technical findings to broader implications for fairness and competition, which adds practical value."}, "weaknesses": {"value": "The technical novelty of the attack is limited. The idea of iteratively optimizing textual metadata to maximize the probability of a desired token output has already been widely used in earlier adversarial text generation research. The contribution mainly lies in applying such optimization to the tool selection setting.\n\nThe optimization process is relatively simple and lacks deeper analysis of efficiency or convergence. It is closer to an empirical heuristic than a new algorithmic framework.\n\nThe baseline comparisons are narrow. The experiments mainly contrast ToolTweak with simple suffix modifications but do not include stronger adversarial optimization baselines such as gradient based or reinforcement learning based methods.\n\nThe evaluation of defenses is rather basic. The two proposed methods, paraphrasing and perplexity filtering, are intuitive but limited. It would be more convincing to include comparisons with other feasible defenses such as metadata normalization, usage based anomaly detection, or classifier based filtering that detects subjective or promotional language in tool descriptions."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n3vAXxzsVn", "forum": "dQXa5jvpQN", "replyto": "dQXa5jvpQN", "signatures": ["ICLR.cc/2026/Conference/Submission11241/Reviewer_QcA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11241/Reviewer_QcA4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524817526, "cdate": 1761524817526, "tmdate": 1762922401459, "mdate": 1762922401459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an attack method targeting the LLM-based proxy tool selection process, ToolTweak. By iteratively optimizing the tool name and description, it significantly increases the probability of the target tool being selected. Experiments show that this method can increase the tool selection rate from approximately 20% to a maximum of 81% on multiple models, and demonstrates strong cross-model migration. The paper also evaluated two defense methods (interpretation and PERPLEXITY filtering), and analyzed the potential impact of attacks on the fairness and security of the tool ecosystem."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The issue is of practical significance: With the widespread use of tool invocation in LLM proxies, bias and attacks in the tool selection process are indeed a security and fairness issue worthy of attention.\n+ The experimental design is systematic and comprehensive: multiple open-source and closed-source models were evaluated, and the migration of attacks, defense effects, and influencing factors (such as tool sequence, parameter complexity, etc.) were analyzed.\n+ Defense methods are evaluated: two defense methods are explored, and their effectiveness was analyzed, providing a reference for subsequent research."}, "weaknesses": {"value": "- The threat model is overly assumptions: attackers are assumed to have access to all the tool information in the entire tool library, which may not be realistic in actual attack scenarios and limits the practicality and universality of the method.\n- The innovation of the method is limited: Tool selection attacks have been involved in existing studies (such as《From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection》). Essentially, the method proposed in this paper is still metadata manipulation based on iterative optimization, and the technical method is not novel enough.\n- The implementation details are not described clearly: Although there are appendices to supplement, the specific mechanism by which LLMS generate \"bias\" names and descriptions during the attack process, as well as the convergence conditions for iterative optimization and other key details, are still not transparent enough, affecting the feasibility of reproduction."}, "questions": {"value": "- In the actual tool platform, is it possible for attackers to obtain all the tool information? Is there an attack scheme under a weaker assumption?\n\n- Compared with the existing tool selection attack methods, what is the innovation of ToolTweak? Are there any significant advantages in terms of attack efficiency, concealment, and mobility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lq7rCVzSGB", "forum": "dQXa5jvpQN", "replyto": "dQXa5jvpQN", "signatures": ["ICLR.cc/2026/Conference/Submission11241/Reviewer_sLJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11241/Reviewer_sLJw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789609818, "cdate": 1761789609818, "tmdate": 1762922401028, "mdate": 1762922401028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ToolTweak, a gradient-free adversarial attack that manipulates the names and descriptions of tools to bias tool selection in LLM-based agents. The attack is automatic and transferable, significantly increasing a targeted tool’s selection rate (e.g., from ~20% to over 80%) across various agents and tasks. The authors also analyze its broader impact on fairness and security in tool ecosystems and propose a paraphrasing defense, showing that the vulnerability remains a persistent challenge."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is identifying a newly occured research problem with function calling.\n\n2. The draft is super clear and concise to read."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. Thought this paper is proposing a new research question, I am not convinced on this point. Because tool selection a ranking task, any tool providers can optimize their tools through black box method (acturally they should do). Will the problem still exist if all tools are optimized using the same TOOLTWEAK method?\n\n2. I think a more in-depth discussion on \"how the distributional shifts in tool usage will impacting the product\" is necessary. I currently did not see how large impact it will cause as we should have tool providers' competition to ensure tool improvement.\n\n3. Do authors think a uniform distribution is the optimal one and should not be tweaked?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vznb8C4eZF", "forum": "dQXa5jvpQN", "replyto": "dQXa5jvpQN", "signatures": ["ICLR.cc/2026/Conference/Submission11241/Reviewer_aZub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11241/Reviewer_aZub"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956146401, "cdate": 1761956146401, "tmdate": 1762922400370, "mdate": 1762922400370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}