{"id": "6IiZXiqP3Q", "number": 2457, "cdate": 1757095130345, "mdate": 1759898146831, "content": {"title": "Position: Want Better ML Reviews? Stop Asking Nicely and Start Incentivizing with a Credit System", "abstract": "With soaring submission counts, stricter reciprocity review policies, widespread adoption of platforms like OpenReview, and without the offsetting pressure of publication fees, the machine learning (ML) community arguably has more peer-review firepower than almost any other scientific field. And yet, almost *everyone* has *many* unpleasant things to share about their review experience. Worse, there is little public space to seriously discuss — let alone debate — what makes a review system effective, or how it might be improved.\n\nIn this position paper, we expand our discussion from the two core problems: *How to reasonably limit the number of submissions?* and *How to incentivize good and discourage bad review practices?* We first assess the strengths and shortcomings of existing attempts in addressing such problems. Specifically, we present five takes on some popular conference mechanisms and propose two alternative designs for improvement.\n\nOur general position is that meaningful improvement to peer review in ML won’t come from polite best practice suggestions tucked into Calls for Papers or Reviewer Guidelines — it requires **enforceable yet fine-grained procedural safeguards** paired with **a currency-like credit system (what we called *OpenReview Points*)** that ML practitioners can “spend” across one or multiple major conferences and throughout the publication lifecycle.", "tldr": "We argue a credit system would promote better ML reviews.", "keywords": ["position paper", "ML review", "conference mechanism"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f45403e61b5636d5c31bcc1ff033591f828372e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This position paper addresses how to improve peer review at major AI conferences, focusing on two key issues: limiting submission volume and incentivizing higher-quality reviews. The authors use recent evidence (e.g., CVPR's desk rejection of irresponsible reviewers, NeurIPS's submission explosion) to highlight problems in current peer review practices.\n\nThe paper critiques existing solutions. First, desk-rejecting papers from irresponsible reviewers does not scale and penalizes only a small fraction of problematic cases. Second, mandatory reviewing degrades review quality because not all reviewers are qualified. Third, reviewer discussions rarely occur due to lack of incentives.\n\nTo address these issues, the authors propose two mechanisms. The first is an OpenReview Points system where reviewers earn points through reviewing activities such as completing reviews or joining emergency review pools. These points can be used to exempt future reviewing duties or waive conference registration fees. The second is a unanimous voting mechanism for reviewer penalties, where authors can report low-quality reviews and the reported reviewer is penalized if all other reviewers and the Area Chair unanimously agree."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The unanimous voting system proposed by the authors is interesting, and could be part of a credit system. It supplements the drawbacks of reviewers having to review and spend a lot of time reviewing low quality works.\n- I strongly agree that an incentive-based system is needed to make peer review sustainable at scale. The proposed point system, while underdeveloped in its current form, has promising extensions. For example, if properly implemented, such a system could motivate reviewers to submit timely reviews, addressing another persistent problem in conference workflows.\n\n- The alternative views are fair and address some of the important points (gaming, favoring researchers with more institutional support)."}, "weaknesses": {"value": "- Looking at the Abstract, the paper has modified the .sty file of latex. This is a violation of the formatting instruction. \n- The paper's writing style is unprofessional and unsuitable for academic publication. Phrases such as \"Emergencies happen. Burnout is real. But the system doesn't care — once you're in the pool, and you're staying there\" and \"we have the largest scholarly firepower reserve of any scientific field\" are informal and colloquial. They read more like a blog post or Twitter rant than a scholarly position paper.  The manuscript requires substantial revision to meet academic writing standards. \n- The proposed OpenReview Points system is superficial and lacks critical implementation details. The authors do not address who would maintain the system or provide precise criteria for awarding points. The current proposal relies on naive, empirical heuristics. While the authors acknowledge that \"points need more sophisticated balancing,\" they fail to provide this framework—precisely what a position paper should offer in detail.\n- Furthermore, the financial implications are unexplored. Who would cover the costs of waived registration fees? Major conferences like ICLR depend heavily on registration revenue to fund their operations. If points exempt reviewers from paying fees, the paper must explain how conference budgets would remain sustainable. Without addressing these practical concerns, the proposal remains incomplete.\n- The paper identifies submission explosion as a central problem in the peer review crisis but does not explain how OpenReview Points would address this issue. The proposal focuses on incentivizing reviewers but provides no mechanism for limiting submission volume, leaving a gap between the stated problem and the proposed solution."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9VmPOF49Ps", "forum": "6IiZXiqP3Q", "replyto": "6IiZXiqP3Q", "signatures": ["ICLR.cc/2026/Conference/Submission2457/Reviewer_4eUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2457/Reviewer_4eUb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760928898992, "cdate": 1760928898992, "tmdate": 1762916244885, "mdate": 1762916244885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses the growing challenges of the machine learning peer-review system amid rising submission volumes and stricter reciprocity policies. It identifies two core issues, limiting submissions and incentivizing good reviewing practices, and critiques existing solutions. The authors propose that genuine improvement requires enforceable procedural safeguards combined with a credit-based system (OpenReview Points), allowing researchers to “spend” review credits across conferences. This framework aims to promote accountability, fairness, and sustainability throughout the publication process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles one of the most pressing meta-scientific challenges in the ML community: the scalability and fairness of the peer-review process under exploding submission volumes. \n\n- The motivation is clear and resonates strongly with the current frustration many researchers face in large-scale conferences like NeurIPS, ICML, and ICLR."}, "weaknesses": {"value": "- While the authors acknowledge that position papers need not present numerical results, no attempt is made to explore even minimal feasibility or simulation evidence (e.g., hypothetical modeling of point distributions, reviewer load dynamics, or incentive equilibria). Even small-scale experiments or thought experiments could have strengthened the argument’s credibility.\n\n- Compared with prior accepted position papers at ICLR or ICML (e.g., Ngo et al., 2024; Yang, 2025), this work lacks integration of historical experience and data-informed reflection from the ML community. The proposed framework would be more convincing if supported by retrospective evidence or case studies drawn from real conference practices.\n\n- The proposed “OpenReview Points” system remains high-level. The paper does not provide enough concrete guidance on: (1) how points would be standardized across conferences with differing review policies, (2) how abuse or collusion could be prevented, and (3) how the infrastructure and governance of such a credit market would operate in practice.\nAs a result, while conceptually appealing, it lacks operational realism.\n\n- As the central stance and innovation of the paper, the proposed credit system does not sufficiently consider potential negative side effects. For example, using credits as a prerequisite for paper submission could disadvantage early-career researchers who have not yet accumulated review credits, and allowing credit redemption to skip review duties could further accelerate the loss of high-quality reviewers.\n\n\n> [1] Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning perspective. In ICLR 2024.\n> [2] Jing Yang. Position: The artificial intelligence and machine learning community should adopt a more transparent and regulated peer review process. In ICML 2025 Position Paper Track."}, "questions": {"value": "- Could the authors provide conceptual or simulated evidence showing that a credit-based system would stabilize reviewer workload or improve review quality metrics?\n\n- How might OpenReview Points be coordinated across independently managed conferences (e.g., ICLR, NeurIPS, ICML) with distinct review policies and timelines?\n\n- How would gaming prevention be ensured? For instance, to avoid mutual positive flagging or collusive exchanges of review credits?\n\n- From a design perspective, how can a credit-based review system both incentivize high-quality reviewers to participate and avoid disadvantaging newcomers to the field?\n\n- The paper does not appear to fully utilize the available space in the ICLR format. Does this reflect a lack of additional experimental or design details, or could the authors expand with more concrete elaboration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mTQ26fnQhA", "forum": "6IiZXiqP3Q", "replyto": "6IiZXiqP3Q", "signatures": ["ICLR.cc/2026/Conference/Submission2457/Reviewer_NVPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2457/Reviewer_NVPF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541612226, "cdate": 1761541612226, "tmdate": 1762916244740, "mdate": 1762916244740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This position paper addresses persistent problems in the peer review process at large-scale ML conferences, arguing that polite guidance and voluntary best practices no longer suffice. The authors propose two concrete reforms: (1) enforceable procedural guardrails, and (2) a credit-based incentive system called OpenReview Points, which would allow participants to earn and spend credits for reviewing, opting out, or gaining privileges such as registration waivers.\n\nI strongly agree with the central position: as venues scale, interest-based reviewer matching and informal guidelines break down. Treating large ML conferences more like economic systems — rather than purely academic gatherings — is a necessary shift. As seen in large volunteer-driven events like the Olympics, voluntarism often fails at scale unless paired with incentives or compensatory frameworks. This paper takes a bold and timely step in that direction by proposing a flexible, modular credit system that can be adapted per conference. While implementation details need to be further developed, this work opens an important conversation and presents a promising path forward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Timely and necessary: Tackles a widely recognized crisis in ML peer review, especially at scale.\n\n* Concrete and actionable: The credit system provides clear examples of earning/spending points, offering a tangible mechanism for reform.\n\n* Candid critique of existing systems: Identifies the limits of desk rejections, reciprocal reviewing, and submission caps with clarity."}, "weaknesses": {"value": "* Implementation is underdeveloped: Lacks specifics on governance, cross-conference coordination, fraud prevention, and fair point allocation.\n\n* No empirical or simulated analysis: A hypothetical case study or retrospective analysis using real conference data would improve feasibility.\n\n* Assumes duty-based reviewing: Many still see reviewing as voluntary labor, not a formal obligation, which complicates enforcement.\n\n* Operational complexity unaddressed: Running such a credit system would require major infrastructure and consensus."}, "questions": {"value": "* How would OpenReview Points be initialized for new or first-time contributors?\n* Would OpenReview itself manage the credit system, or would a new governance body be needed?\n* Can credits be pooled, transferred, or inherited across teams or co-authors?\n* How do you prevent low-effort “review farming” aimed at harvesting points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsPqf08YQ4", "forum": "6IiZXiqP3Q", "replyto": "6IiZXiqP3Q", "signatures": ["ICLR.cc/2026/Conference/Submission2457/Reviewer_ZGAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2457/Reviewer_ZGAX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858275560, "cdate": 1761858275560, "tmdate": 1762916244619, "mdate": 1762916244619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This position paper argues that ML peer review quality cannot be improved through polite guidelines alone, proposing instead a currency-like \"OpenReview Points\" system combined with fine-grained procedural safeguards. The authors identify two core problems: excessive submissions and lack of accountability for reviewers. They critique existing mechanisms (submission caps, mandatory reciprocal reviewing, desk rejections) and propose a credit economy where researchers earn/spend points for various reviewing activities and privileges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper discusses a timely topic : Everyone in ML has complained about bad reviews at some point. The paper tackles an issue that actually matters to the community.\n\n2. The paper does a good job explaining the two main issues: too many submissions and no consequences for lazy reviewers. The example of SACs handling 80 papers each at NeurIPS really drives the point home.\n\n3. The authors don't just complain - they actually look at what conferences are already trying (submission limits, mandatory reviewing, desk rejections) and explain why these don't really fix the problem."}, "weaknesses": {"value": "1. The actual benefit of the proposed credit system is unclear. Research shows that incentive systems often don't improve peer review quality. For example, Gasparyan [1] studies multiple incentive types (monetary, certificates, CME credits, open recognition) and concludes no single incentive model consistently improves peer-review quality.\n\n2. Turning reviewing into a “currency economy” risks distorting intrinsic motivations. Once credit accumulation becomes an explicit metric, reviewers may optimize for the fastest or most visible ways to earn points instead of offering thoughtful, time-consuming feedback. Despite claiming to avoid gamification, the system could still reproduce inequality or favor individuals with more time or institutional resources.\n\n\n\n[1] Gasparyan AY, Gerasimov AN, Voronov AA, Kitas GD. Rewarding peer reviewers: maintaining the integrity of science communication. J Korean Med Sci. 2015;30(4):360-364. doi:10.3346/jkms.2015.30.4.360"}, "questions": {"value": "See  Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lqqQN6Acut", "forum": "6IiZXiqP3Q", "replyto": "6IiZXiqP3Q", "signatures": ["ICLR.cc/2026/Conference/Submission2457/Reviewer_GX2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2457/Reviewer_GX2E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762762312862, "cdate": 1762762312862, "tmdate": 1762916244513, "mdate": 1762916244513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This position paper explores how to limit the number of paper submissions to AI conferences and incentivize high-quality reviews while discouraging low-quality ones. It critiques retaliatory processes such as desk-rejecting papers from irresponsible reviewers as too coarse for handling fine-grained bad-review behavior beyond abandoning review tasks, and proposes proportional penalties. It criticizes mandatory reciprocal reviewing for degrading review quality. It suggests a system of review points to be used for \"privileges,\" such as avoiding reviews or free conference registration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem is important, should be discussed, and its causes and symptoms should be understood.\nFree registration is a strong incentive and is practical for unlimited virtual registration."}, "weaknesses": {"value": "1. Using points to avoid reviewing may send the wrong message about the value of reviews,\nI disagree with the statement that avoiding reviewing and academic service is a privilege; it's the other way round, academic service is a privilege.\n\n2. This paper takes a narrow view of handling the increasing number of papers, and is missing a key element: automated scientific discovery and AI's increasing role in performing research and writing papers.\nIt focuses on the symptom, the increase in submissions, rather than addressing the cause, which includes AI and automated research and writing.\n\n3. This paper would be good as a blog, not an ICLR research paper."}, "questions": {"value": "Have the authors considered posting the text as a blog or presenting their positions in a panel? to hear diverse opinions and perspectives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1lE7gfe0jF", "forum": "6IiZXiqP3Q", "replyto": "6IiZXiqP3Q", "signatures": ["ICLR.cc/2026/Conference/Submission2457/Reviewer_uVme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2457/Reviewer_uVme"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762834600072, "cdate": 1762834600072, "tmdate": 1762916244185, "mdate": 1762916244185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}