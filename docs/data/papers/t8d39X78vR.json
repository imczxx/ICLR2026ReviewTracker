{"id": "t8d39X78vR", "number": 20167, "cdate": 1758303230646, "mdate": 1759896997146, "content": {"title": "Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network", "abstract": "Novel research ideas play a critical role in advancing scientific inquiries. Recent advancements in Large Language Models (LLMs) have demonstrated their potential to generate novel research ideas by leveraging large-scale scientific literature. However, previous work in research ideation has primarily relied on simplistic methods, such as keyword co-occurrence or semantic similarity. These approaches focus on identifying statistical associations in the literature but overlook the complex, contextual relationships between scientific concepts, which are essential to effectively leverage knowledge embedded in human literature. For instance, papers that simultaneously mention \"keyword A\" and \"keyword B\" often present research ideas that integrate both concepts. Additionally, some LLM-driven methods propose and iteratively enhance research ideas using the model's vast internal knowledge, but they fail to effectively leverage the valuable scientific concept network, limiting the grounding of these ideas in established research. To address these challenges, we propose the \\textbf{Deep Ideation} framework, which integrates a scientific network that not only captures keyword co-occurrence but also incorporates contextual relationships between keywords, providing a richer scientific foundation for LLM-driven ideation. Our framework introduces an explore-expand-evolve workflow for Deep-Ideation which integrates several key components to iteratively refine research ideas. Throughout this workflow, we maintain an Idea Stack to track research progress across iterations. To guide this search and evolution process, we integrate a critic engine trained on real-world reviewer feedback, providing continuous signals on the novelty and feasibility of generated ideas. Experimental results across multiple AI domains show that our approach significantly improves the overall quality of generated ideas by \\textbf{10.67\\%} compared to other methods, with the generated ideas exceeding the acceptance level of top conferences. Human evaluation highlights the practical value of the generated ideas in supporting scientific research while ablation studies further confirm the effectiveness of each component of the workflow.", "tldr": "", "keywords": ["Large Language Model", "LLM Agent", "Automated Scientific Discovery", "AI Scientist"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f942ab57a56f69a4ea4e4893947cf4a1f203716f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Deep Ideation, a framework that integrates LLMs with a scientific concept network to generate novel and feasible research ideas. The system constructs a concept-relation graph via keyword co-occurrence and contextual relationships, and conducts an explore-expand-evolve workflow. An Idea Stack maintains iterative research progress, while a Critic Model, fine-tuned on real-world review data, provides feedback on novelty and feasibility. Experiments across four domains show about 10 % gains over baselines such as SciMON, ResearchAgent, and MOOSE-Chem by both LLM-judge and human evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The work moves beyond static keyword or embedding-based ideation by dynamically retrieving and composing concept relations from a curated scientific network.\n\n(2) Fine-tuning on real review text gives a realistic feedback signal for novelty and feasibility assessment.\n\n(3) The released concept network and dataset could support further AI-for-Science research."}, "weaknesses": {"value": "(1) The paper does not analyze whether the training corpus, review data, and accepted-paper references pre- or post-date the LLMs’ training cut-off—important for judging fairness and novelty.\n\n(2) Section 4.2.2 lists 54 researchers but omits selection criteria, expertise, or calibration details.\n\n(3) Novelty/feasibility scores are subjective; the paper reports no inter-rater agreement between human evaluators, nor consistency among different LLM judges.\n\n(4) Only Evolve and Critic modules are tested; other components remain unanalyzed.\n\n(5) Occasional redundancy and verbosity, e.g. Appendix A.1.3 and A.1.7 contain essentially identical prompts for keyword replacement, suggesting redundancy or editing oversight."}, "questions": {"value": "- How is the edge feature F_{ij} concretely computed from multiple papers?\n\n- Please clarify whether any papers or reviews used in training or evaluation post-date the LLMs’ own data cut-off.\n\n- How were the 54 human evaluators selected, and how consistent were their scores with LLM-based evaluations?\n\n- Did you measure agreement among different LLM judges (e.g., GPT-4o vs Gemini 2.5 Flash)?\n\n- What is the computational cost and scalability of the system?\n\n- How is the stopping criterion in the Evolve phase determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4LgCFaazUN", "forum": "t8d39X78vR", "replyto": "t8d39X78vR", "signatures": ["ICLR.cc/2026/Conference/Submission20167/Reviewer_hrL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20167/Reviewer_hrL1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726000681, "cdate": 1761726000681, "tmdate": 1762933683377, "mdate": 1762933683377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Deep Ideation framework, which uses a concept network to iteratively retrieve and incorporate scientific concept relationships. The paper also collects approximately 100k papers from 10 major AI conferences to create such co-occurrence keywords. Finally, the paper released a review dataset based on real-world reviewer feedback."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper collects approximately 100k papers from 10 major AI conferences to create such co-occurrence keywords. Finally, the paper released a review dataset based on real-world reviewer feedback.\n2. The paper includes an ablation study and a case study. The paper also includes several SOTA baselines. The paper conducts both human and automatic evaluation. The paper provides both quantitative and qualitative analysis. \n3. The paper includes additional implementation details in the appendix."}, "weaknesses": {"value": "1. Why use a co-occurrence concept graph instead of using scientific IE systems to extract keywords and their relationships? The framework seems to be purely based on prompting. The idea of using neighboring keywords has been proposed in ResearchAgent and SciMon. The paper might need to include a domain-specific LLM such as OLMO2.\n2. Some details of the paper are also not clear. For the method section, the evolve part is really confusing. What is the exact algorithm? How to determine whether the keywords need to be removed? This part is also similar to the iterative idea refinement of ResearchAgent and SciMon. For the experiment section, what is the evaluation dataset? Are all of the 100k papers used for evaluation? What are the evaluation metrics used in Section 4.2? What is the background of human annotators? What is their inter-annotator agreement? \n3. The paper fails to include a use of LLMs section. The paper also fails to provide code. The paper needs to include an ethics consideration."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g6Hh4iuDhp", "forum": "t8d39X78vR", "replyto": "t8d39X78vR", "signatures": ["ICLR.cc/2026/Conference/Submission20167/Reviewer_Dots"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20167/Reviewer_Dots"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981209580, "cdate": 1761981209580, "tmdate": 1762933682948, "mdate": 1762933682948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Deep Ideation, a novel LLM agent framework designed to generate high-quality research ideas by dynamically interacting with a vast scientific concept network. The framework constructs this network from approximately 100,000 papers, capturing contextual keyword relationships beyond simple co-occurrence. It employs an explore-expand-evolve iterative workflow, guided by an Idea Stack and a Critic Model trained on real-world reviewer feedback. Experimental results show a significant improvement in idea quality (10.67% over baselines) across multiple AI domains, with generated proposals exceeding the acceptance bar for top conferences."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The core innovation lies in integrating a comprehensive scientific concept network that incorporates contextual relationships between keywords, providing a richer, more grounded foundation for LLM ideation than previous statistical methods.\n2. The explore-expand-evolve workflow, combined with an Idea Stack and keyword management modules, allows for a structured and continuous optimization of research ideas, mirroring the cognitive process of human researchers.\n3. The introduction of a Critic Model trained on real-world reviewer feedback is a powerful mechanism to guide the search for novelty and feasibility, ensuring that the iterative process is not \"directionless and blind\"."}, "weaknesses": {"value": "1. The formal definition of the edge feature $F_{ij}$ (contextual relationship) is high-level ($g(\\cdot)$ aggregating relation)9999999. The paper lacks detail on how this complex contextual information is practically represented, quantified, and distilled into a format that the LLM agent can robustly query and leverage for \"Relation Analysis\" beyond just raw text snippets.\n2. The Critic Model is fine-tuned on a relatively small dataset (4278 examples). Its ability to consistently and accurately evaluate novelty and feasibility across diverse and potentially unrelated AI domains (the four domains tested) remains a key concern regarding generalization."}, "questions": {"value": "1. The $F_{ij}$ feature captures the contextual relationship between co-occurring keywords12. Could the authors provide a case study or ablation study that specifically compares the performance of the ideation agent when using this complex $F_{ij}$ feature (contextual summary) versus a simpler edge feature (e.g., just the co-occurrence frequency or a semantic embedding of the connecting text)?\n2. Given the iterative nature and multiple LLM calls per iteration (Relation Analysis, Keyword Selection, Idea Formulation, Critic Model), what is the amortized cost (e.g., total tokens/API cost) and time required to generate a single idea that achieves the \"acceptance level\" benchmark, compared to the one-shot or less complex iterative baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GcEtMbdRNg", "forum": "t8d39X78vR", "replyto": "t8d39X78vR", "signatures": ["ICLR.cc/2026/Conference/Submission20167/Reviewer_DTQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20167/Reviewer_DTQ4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762258124129, "cdate": 1762258124129, "tmdate": 1762933682467, "mdate": 1762933682467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Deep Ideation, a framework integrating LLM agents with a scientific concept network for automated research idea generation. It constructs a network from 100k papers for keyword co-occurrences and contextual relations. The ideation loop follows an explore–expand–evolve process, with a Critic Model trained on peer reviews on Novelty & Feasibility (1-5 scale) to guide refinement.\nExperiments across 4 AI domains against baselines SciMON, ResearchAgent, MOOSE-Chem, etc. Reported 8-10% improvements in LLM/human scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper address an intereateing and important area of the LLM-driven research-idea generation\n- Large corpus (100K papers) for the concept network, with modular design of explore / expand / evolve with critic feedback loop\n- Clear visual pipeline for fig 1-2, easy to follow with good presence.\n- Consistent experiment reporting across 4 domains with intent and structured scoring rubrics. The evaluation model reported is comprehensive.\n- Ablations without critic show the contribution of the critic model"}, "weaknesses": {"value": "- Similar concept networks and iterative LLM ideation already exist. The method-wise novelty is somehow limited to incremental algorithmic refinements rather than conceptual advances.\n- The critic is trained on LLM-generated paper–review pairs rather than real human reviews. Although an ablation (with vs. without critic) is included, the small (3–5%) improvement is measured only by LLM judges without correlation or reliability analysis. Also how to prevent the data contamination and model checkpoint date is not clear in the paper.\n- The dataset statistics not reported.\n- The paper scores ideas on novelty and feasibility, both defined through self-constructed rubrics without references to prior peer-review standards. These definitions are generic and may conflate novelty with simple graph distance. An Effectiveness —how meaningful, insightful, or actionable the generated ideas are— should also be inclused accroding to Si et al. 2024 (Can LLM generate novel research ideas?).\n- Reported improvements are numerically minor and may fall within variance. The human evaluation details is very unclear (only mentions \"meet the expectations of domain expert\" is not enough, lacks details on blinding, recuiting, expertise, and statistical significance etc. ), making the evidence of real improvement very unconvincing. The paper does not report which dataset was used for the human study or how it aligns with the system evaluation set.\n- The core graph is built from 100K papers, but the value of this corpus for actual idea generation is unclear. The model relies only on keyword co-occurrences and short LLM-generated contextual relations—mechanistically simple and lacking deeper conceptual or causal understanding. The aggregation function g(⋅) is undefined (I might miss it), and no analysis show the graph contributes meaningfully to novelty or feasibility.\n- Comparisons are partly unfair (e.g., MOOSE-Chem is in a different domain). Other relevant baselines are missing—such as multi-agent simulation methods (“Many Heads Are Better Than One”), graph simulation systems (ResearchTown), and method-focused ideation such as IdeaSynth/WebWeaver/LDC."}, "questions": {"value": "- What's the distribution of the dataset? this is very important to whether the graph construction make sense but not reported in the paper.\n- How is relation(vi, vj, p) computed? Manual LLM parsing or embedding similarity?\n- Is the critic model fine-tuned with actual numerical review scores or textual rationales only?\n- How do you ensure generated ideas are not just rephrasings of seen papers but after the model checkpoints"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ccbvN4w8sE", "forum": "t8d39X78vR", "replyto": "t8d39X78vR", "signatures": ["ICLR.cc/2026/Conference/Submission20167/Reviewer_gb37"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20167/Reviewer_gb37"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762300230562, "cdate": 1762300230562, "tmdate": 1762933681968, "mdate": 1762933681968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}