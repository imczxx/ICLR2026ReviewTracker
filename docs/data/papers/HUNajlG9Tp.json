{"id": "HUNajlG9Tp", "number": 22143, "cdate": 1758326707854, "mdate": 1759896883856, "content": {"title": "AutoRule: Reasoning Chain-of-Thought Extracted Rule-based Rewards Improve Preference Learning", "abstract": "Existing rule-based rewards in preference-based reinforcement learning rely on manual engineering, limiting scalability. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chains of these interpretations, and synthesizes them into a unified rule set. Using the finalized rule set, we employ language-model verifiers to judge rule satisfaction, using this metric as an auxiliary reward alongside the learned reward model during policy optimization.  Empirically, AutoRule yields gains for both Llama-3-8B and Olmo-2-7B in both in-distribution and out-of-distribution benchmarks. On Llama-3-8B, it achieves a 25.6\\% relative improvement in length-controlled win rate against GPT4 on AlpacaEval2.0, and a 6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to baseline models. Further analysis shows that the extracted rules exhibit strong agreement with dataset preferences and are behaviorally consistent across multiple runs, extraction scales, and aggregated scores. Notably, these rules also contribute to mitigating reward hacking in reward models, likely because they serve as constraints that prevent the policy from exploiting spurious features. Extracted rules are provided; code and model checkpoints will be open-sourced.", "tldr": "We introduce AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards.", "keywords": ["RLHF", "Rule-based Rewards", "Reasoning Chain-of-Thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/796a6faf55aa6b2efb86a47e8f5831eb9bca9342.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a pipeline for extracting rule-based rewards for preference alignment of LLMs. The pipeline employs a reasoning LLM (e.g. DeepSeek-R1) to generate reasoning chains for a subset of training samples, extract rules from the reasoning chains, and then merge the extracted rules into a concrete list, which is used by a verifier LLM to produce 0/1 rewards during training. The paper examines the effectiveness of this pipeline on UltraFeedback, MT-Bench, and AlpacaEval2.0 and presents a series of in-depth analysis for the quality of rules, consistency of rules, and pipeline ablation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of extracting reward rules from reasoning models is an interesting idea, since rules are compact and likely to be generalizable.\n\n2. The proposed pipeline is straight-forward and concise."}, "weaknesses": {"value": "1. The presentation needs improvement. For example, even though the author mention RaR and RLCF, two concurrent word dedicated on automatic rule construction, from line 145-146 it is unclear how the proposed method differs from them. In addition, the inclusion of conciseness as a reference is not mentioned in section 3, but it is experimentally analyzed in line 375. There are also some minor mistakes. The symbol $y$ is defined to be output sequences, so I think in line 166 and 176 the correct notation is $(o,r)\\sim\\pi_\\phi(\\cdot|x)$. The 1 and 2 shown in line 174 and line 175 are also confusing.\n\n2. Although this method is claimed to be resolving reward hacking, the only evidence provided are the learning curves presented in Fig 3a & 3b, where the model reward of alternative methods begin to decline after training proceed but the model reward of the proposed method keep improving. Although I agree that such results are positive, their connection with reward hacking needs further illustration.\n\n3. The proposed pipeline has some space to improve. See questions below."}, "questions": {"value": "1. The results shown in Table 2 suggest that the proposed method is label efficient, if we regard token from the teacher model as labels. But how do you determine the amount subset of data to be used for rule extraction? Is the size of such subset correlated with the overall performance? \n\n2. Rather than random selection, is there a better way to select samples in this subset?\n\n3. Section 5.3 presents quantitative evaluation for the extracted rules. What do you mean by \"from different rule lists\" in line 431?\n\n4. I think generalizability is a more intuitive name for the equation in line 424. Could you provide results similar to fig 2a, but in the setting of cross-domain generalization? In other words, evaluate rules extracted from UltraFeedback on other datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4KmJvZrf9O", "forum": "HUNajlG9Tp", "replyto": "HUNajlG9Tp", "signatures": ["ICLR.cc/2026/Conference/Submission22143/Reviewer_SJ6k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22143/Reviewer_SJ6k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540956941, "cdate": 1761540956941, "tmdate": 1762942087567, "mdate": 1762942087567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an automated framework for extracting explicit natural language rules from human preferential data and using them as additional rewards in RLHF. By doing so, they provide some interpretable or constraint like rewards and hence mitigating reward hacking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a fully automated pipeline to extract the rules from the data. The amount of manual engineering is pretty nominal\n* The extracted rules are human interpretable and seem to be aligned with known good practices for llm responses."}, "weaknesses": {"value": "* The results are quite marginal and limited. Llama 3 8B is quite old at this point. And the AE LC Win rate is quite low, compared to Llama 3 8B Instruct. \n* THe results seem to raise a question whether the advantage of rules is mainly effective in out of distribution or extreme scenarios, rather than in distribution (seems contrary as the rules are derived from this distribution). \n* The conciseness constraint added to the verifier is an implicit design choice. It may bias the model toward shorter responses and is not extensively evaluated\n* The cited parallel works (RaR and RLCF) show larger improvements on their respective benchmarks, comparing those methods with the presented evaluation would be needed to contextualize the contributions of this method."}, "questions": {"value": "1. How sensitive is the rule set to the choice of teacher model?  Would using a smaller or different reasoning model significantly change the rules or performance?\n2. Can one rule set extracted from UltraFeedback generalize to other domains?\n3. After merging, are the final rules entirely complementary, or did you observe any cases of overlap/conflict between rules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9kywaoU5oE", "forum": "HUNajlG9Tp", "replyto": "HUNajlG9Tp", "signatures": ["ICLR.cc/2026/Conference/Submission22143/Reviewer_V4rY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22143/Reviewer_V4rY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802038337, "cdate": 1761802038337, "tmdate": 1762942087103, "mdate": 1762942087103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a methodology for automatically generating a set of grading rules based on a preference dataset. To do so, the preferred and dispreferred responses are presented to a reasoning model which is instructed to justify the preference. From the reasoning chain elicited, concrete rules are extracted and merged with other existing rules. These rules are then used during reward model inference. The authors find that the rules generated by AutoRule lead to higher quality reward signals than other rule extraction / reward shaping methodologies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Extracting preference rules over a whole preference dataset is both novel and timely given recent research on rubric based optimization.\n* AutoRule leads to impressive performance gains in terms of best performance and robustness to overoptimization\n* The authors conduct thorough evaluations and ablations which robustly demonstrate their claims."}, "weaknesses": {"value": "* One potential confounder is that it's unclear how much of the performance gain comes from the utility of the autogenerated rules versus the amount of inference compute spent. Namely, autorule requires doing a forward pass per rubric item. It would be useful to have an inference cost fixed evaluation, potentially by varying the thinking length of a normal llm judge."}, "questions": {"value": "* Did the authors experiment with how RM performance is related to the number of generated rules? I would be interested in understanding the scaling behavior there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L0K0PtGRRk", "forum": "HUNajlG9Tp", "replyto": "HUNajlG9Tp", "signatures": ["ICLR.cc/2026/Conference/Submission22143/Reviewer_EXre"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22143/Reviewer_EXre"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956094279, "cdate": 1761956094279, "tmdate": 1762942086723, "mdate": 1762942086723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automated framework that extracts rule-based rewards from preference feedback, removing the need for manual rule design. It uses a three-stage pipeline that contains reasoning-based interpretation, rule extraction, and synthesis. The pipeline is followed by LM verification to generate auxiliary rewards. The method boosts alignment performance (+25.6% vs GPT-4 on AlpacaEval 2.0, +6.1% on MT-Bench), reduces reward hacking, and improves interpretability, with all code and models open-sourced."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method has significant gains on AlpacaEval 2.0 and MT-Bench for Llama-3-8B and Olmo-2-7B.\n\nThe proposed method has substantial implications for the community: it provides explicit, human-readable constraints that explain policy behavior.\n\nOpen-sourced rules, code, and checkpoints."}, "weaknesses": {"value": "The authors argue it is the first fully automated rule-extraction system for RLHF/post-training. However, such pipelines are pretty industrial; therefore, the protocol and engineering efforts might not be innovative for the community.\n\nThe multi-stage design and instruction-following nature did provide logical transparency of the pipeline, but the paper did not clearly illustrate the merits of such designs. It is conceptually appealing, but I was unable to digest the evidence presented in the main body and the appendix."}, "questions": {"value": "It is out of scope for this paper, but it would be very helpful if the dependency on the reward function were clarified. \n\nIn many real-world datasets, binary reward models, e.g., the Bradley-Terry (BT) model, are known to be subject to 'intransitivity' because they rely on scalar variables, which assume all preferences are transitive. \n- The literature below studied representative preference datasets in the real world, where the 'transitive' relationship between preference annotations may not always hold. \n- https://arxiv.org/abs/2409.19325 (Duan et al, 2017)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eN6Q5gz8gX", "forum": "HUNajlG9Tp", "replyto": "HUNajlG9Tp", "signatures": ["ICLR.cc/2026/Conference/Submission22143/Reviewer_zUE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22143/Reviewer_zUE9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982392556, "cdate": 1761982392556, "tmdate": 1762942086482, "mdate": 1762942086482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}