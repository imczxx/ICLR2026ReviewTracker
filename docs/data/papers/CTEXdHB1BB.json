{"id": "CTEXdHB1BB", "number": 5424, "cdate": 1757908348338, "mdate": 1759897976195, "content": {"title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs’ reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyper-parameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce ***C****onditional adv****AN****tage estimati****ON*** (***CANON***), amplifying the impact of the target metric without presuming its direction. Specifically, *CANON* regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, *CANON* based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, *CANON* further improves token efficiency, yielding a more favorable Pareto frontier in the performance–cost trade-off.", "tldr": "", "keywords": ["language models", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93f6893a3750fad2266f0681aea13907d24109e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CANON, an advantage estimation method for RLVR. Instead of relying on hand-crafted human priors such as the assumption that lower entropy is better or that higher entropy is better, CANON dynamically identifies which metric trend leads to better performance. It works by conditionally partitioning sampled responses into two equal-sized groups based on a target metric, then computing two types of advantages: CANON-Inter (comparing against the other group) and CANON-Intra (comparing within its own group). The overall advantage is computed as $\\mu$ × CANON-Inter + (1 - $\\mu$) × CANON-Intra. The authors find that CANON-Inter works better for math tasks while CANON-Intra works better for complex logic tasks. CANON-Dynamic schedules $\\mu$ during training based on training accuracy or training steps, achieving a balance between the two approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- CANON selects its own direction by empirically determining which metric trend performs better, rather than rolling with a fixed assumption. The motivation to remove hand-crafted directional priors and let the data determine which metric trend is beneficial appears novel and well-justified.\n- The method provides a systematic way to control the emphasis on exploration (favored by CANON-Intra) and exploitation (favored by CANON-Inter). This offers a new approach to balance a metric, which is especially valuable for entropy that benefits from careful, dynamic balancing."}, "weaknesses": {"value": "- While adaptive scheduling makes sense for entropy (balancing exploration and exploitation), its value is less clear for response length, where shorter responses are almost always preferable for efficiency.\n- The choice between CANON-Dynamic and cosine-based scheduling appears to depend on the specific model being trained, with no universally optimal approach, thus requiring trial and error to determine the best strategy.\n- This re-introduces hyperparameter tuning complexity, as the values of mu and alpha (group proportion parameter), along with their scheduling and adjustments, constitute additional hyperparameters that require careful calibration, which contradicts the paper's stated motivation.\n- The method only considers one metric at a time, and it remains unclear what other metrics beyond entropy and length could be meaningfully employed with this approach.\n- The evaluation metric for the AIME benchmarks, Avg@10, seems insufficient given their small sample size. To my knowledge, Avg@64 is more commonly used in practice to achieve stable and reliable results for such tasks."}, "questions": {"value": "- Is there a principled way to incorporate multiple metrics simultaneously within a single unified advantage calculation?\n- In the upper right corner of Figure 1, wouldn't it be clearer if the inter- and intra-group advantages were each visually subdivided to show their two constituent subgroups? This would help readers more readily understand that each advantage metric is calculated using two distinct subgroups."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AxJ6q3Lrfl", "forum": "CTEXdHB1BB", "replyto": "CTEXdHB1BB", "signatures": ["ICLR.cc/2026/Conference/Submission5424/Reviewer_pJ7Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5424/Reviewer_pJ7Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722789051, "cdate": 1761722789051, "tmdate": 1762918053893, "mdate": 1762918053893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conditional advANtage estimatiON (CANON), sorting the sampled responses according to a metric and splitting them into two groups, instead of a hand-crafted prior with intra- and inter-group advantage. Experimental results demonstrate that CANON, based on entropy, consistently outperforms previous methods across three LLM backbones on both mathematical and complex logical reasoning benchmarks. Moreover, when applied to response length, CANON improves token efficiency and achieves a better performance–cost Pareto frontier. The authors conducted experiments around mathematics and logical reasoning on several models. However, I don't think this is enough to demonstrate the effectiveness of the method, and the biased report prevents me from giving a score of acceptance, see Weakness and Question. If the author can provide a convincing explanation, I am willing to increase my score."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The author provides experiments and ablation/analysis experiments to validate the proposed method.\n2. The motivation is clear.\n3. The paper is well written and easy to understand except for section 5.3.\n4. The author provides theoretical analysis and has the potential to incorporate some previous work into a more unified view."}, "weaknesses": {"value": "I have no doubts about the method itself, but there are several issues with the experimental part that directly affect whether the paper should be accepted.\n1. The author mainly reports the results on Qwen-2.5. Although only AIME 2025 performed worse than other baselines, it is the only test set that is later than the training time of Qwen-2.5. Due to the risk of data leakage in Qwen-2.5, I suspect that the effectiveness improvement brought by the method is related to data leakage. Logic reasoning tasks can, to some extent, demonstrate that the method is still effective. Without this part of the experiment, I would give it a score of 4, but I still look forward to more model experiments that prove the effectiveness of the method. At least supplement all the results of the models involved in Figure 3.\n2. The results and their explanation in Figure 5.3 are very misleading. I suggest the author make revisions and truthfully report the results of various strategies, rather than picking choices that are beneficial for reporting. In my understanding, CANON-Dynamic is a combination of several selected strategies, rather than a fixed strategy. The author's report and explanation can easily be misleading, so I give 2 points in the Presentation. Due to the lack of representation of other results of the model in Figure 3, and the fact that the baseline (DR.GRPO) in the figure has its own advantages and disadvantages compared to non-combined results (CANON-Inter/-Intra), the experiments in this paper cannot demonstrate that the method performs well on other models."}, "questions": {"value": "1. Why are alpha=0.96 and 0.88 selected in the analysis of Table 2? Do these parameters need to be carefully picked?\n2. Other questions, please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4wfNqnUSNR", "forum": "CTEXdHB1BB", "replyto": "CTEXdHB1BB", "signatures": ["ICLR.cc/2026/Conference/Submission5424/Reviewer_Aak7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5424/Reviewer_Aak7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880950724, "cdate": 1761880950724, "tmdate": 1762918053373, "mdate": 1762918053373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conditional advANtage estimatiON (CANON). CANON regroups responses based on specific metrics, leading to two subgroups of responses. By leveraging inter-group and intra-group statistics, CANON introduces the inter-group advantage and intra-group advantage, which provide a stronger advantage signal. The authors further introduce several strategies to enhance this approach, and experiments on reasoning tasks validate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper is well-written.\n2.\tThe ablation studies in Section. 6 are informative. The difference caused by the ratio of inter-group and intra-group advantage seems an interesting topic to be discovered further."}, "weaknesses": {"value": "1.\tAs shown in Theorem 1, CANON yields a stronger advantage signal. However, it is essentially a numerical amplification (adding the difference between the means of the two groups) of the advantage, and consequently, of the loss. What is the difference between CANON and this variant? If they are the equivalent, what is the purpose of regrouping?\n2.\tCANON splits responses into two subgroups. However, since such regrouping may help training, what would happen if we performed a finer-grained regrouping? For example, we could split responses into four, six, or eight groups and calculate the corresponding advantages.\n3.\tDR.GRPO is outdated in the context of LLM reasoning. It is suggested that the authors compare CANON with more recent baselines, especially those focusing on entropy. \n4.\tThe ablation studies focusing on different roles of inter-group and intra-group advantage (in Section 5.1 and Section 6) are valuable. However, despite the experiment observation, it is suggested that the authors provide a more detailed theoretical analysis on it, especially the reason behind Fig. 5 and Fig. 2.\n5.\tCANON-Dynamic appears effective. However, there exist several dynamic scheduling methods, It is suggested that the authors compare theirs with other alternative scheduling strategies, both theoretically and experimentally."}, "questions": {"value": "Please see **Weaknesses** part. I am looking forward to the authors’ response, and may reconsider this paper based on that."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXtWjADNyT", "forum": "CTEXdHB1BB", "replyto": "CTEXdHB1BB", "signatures": ["ICLR.cc/2026/Conference/Submission5424/Reviewer_Xngh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5424/Reviewer_Xngh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909086185, "cdate": 1761909086185, "tmdate": 1762918053123, "mdate": 1762918053123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Conditional Advantage Estimation (CANON), a novel method for reinforcement learning in large reasoning models. The authors identify a key limitation in prior work: current methods often rely on hand-crafted penalties or preferences for training metrics (like entropy or response length), which can be overly biased and require careful tuning. CANON avoids this by not presuming a direction for the metric's impact. Instead, it regroups sampled responses based on the metric (e.g., high-entropy vs. low-entropy) and computes two novel advantage estimates. The \"inter-group\" advantage identifies which metric trend leads to higher accuracy, while the \"intra-group\" advantage identifies the best responses within a given trend. \n\nExtensive experiments show that CANON outperforms strong baselines like GRPO on math and complex logic reasoning tasks. Furthermore, when applied to response length, the method establishes a new, superior Pareto frontier for the performance-efficiency trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates a novel and well-motivated advantage estimation method. splitting responses into conditional groups and calculating inter- and intra-group advantages directly addresses the problem of biased, hand-crafted priors. The theoretical analysis showing DR.GRPO as a special case of CANON further solidifies the contribution.\n2. The authors provide convincing results across multiple models and challenging benchmarks. \n3. The proposed method has a practical impact on reasoning efficiency with some token reduction at the same performance level or higher performance in low-token-budget scenarios."}, "weaknesses": {"value": "1. Curious why don't the authors put CANON-Dynamic's results in Table 1? Wouldn't that be more intuitive?\n2. The paper's experiments are focused exclusively on response length and entropy.\n3. The paper argues against hand-crafted tuning, but the CANON-Dynamic method, which achieves the best-balanced performance, relies on a handcrafted hyperparameter and may need to tune it for different tasks seperately."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uuL29rEXRE", "forum": "CTEXdHB1BB", "replyto": "CTEXdHB1BB", "signatures": ["ICLR.cc/2026/Conference/Submission5424/Reviewer_dLuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5424/Reviewer_dLuf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983879444, "cdate": 1761983879444, "tmdate": 1762918052855, "mdate": 1762918052855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}