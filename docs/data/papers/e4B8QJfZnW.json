{"id": "e4B8QJfZnW", "number": 18529, "cdate": 1758288826799, "mdate": 1759897097909, "content": {"title": "Clustering Improves Differentially Private Inference", "abstract": "Differentially private (DP) language model inference is an approach for generating private synthetic text. A sensitive input example is used to prompt an off-the-shelf large language model (LLM) to produce a similar example. Multiple examples can be aggregated together to formally satisfy the DP guarantee.\n\nPrior work creates inference batches by sampling sensitive inputs uniformly at random. We show that uniform sampling degrades the quality of privately generated text, especially when the sensitive examples concern heterogeneous topics.\n\nWe remedy this problem by clustering the input data before selecting inference batches. Next, we observe that clustering also leads to more similar next-token predictions across inferences. We use this insight to introduce a new algorithm that aggregates next token statistics by privately computing medians instead of averages. This approach leverages the fact that the median has decreased local sensitivity when next token predictions are similar, allowing us to state a data-dependent and ex-post DP guarantee about the privacy properties of this algorithm. Finally, we demonstrate improvements in terms of representativeness metrics (e.g., MAUVE) as well as downstream task performance. We show that our method produces high-quality synthetic data, at significantly lower privacy cost, than a previous state-of-the-art method.", "tldr": "We introduce a new and improved algorithm for producing representative synthetic data with DP inference.", "keywords": ["Large-language models", "private inference", "synthetic data"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b737898c8a8ca654d075d11252ab69aefb6ccf3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studied the problem of private synthetic data generation and proposed an optimized version of private prediction based solution. Specifically, the authors identify that uniformly sampling ICL examples can lead to degraded utility due to heterogeneous distribution within the sample batch. To encounter this problem, the author proposed a DP clustering method by first calculating k cluster centers using public data, and then DP select the top-k' centers upon private data to avoid skewed distribution. The authors also proposed using median aggregation rather than mean aggregation to lower the sensitivity for clustering-based batching strategy. The experiment results shows improved utility and privacy cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- Applying DP for private synthetic data generation is an important direction.\n- Extensive experiments on clustering and empirical private estimates have been conducted."}, "weaknesses": {"value": "- The contribution of this paper is somewhat limited. The main utility improvement comes from the clustering-based batching for DP sampling. In [1], the authors have already discussed the choice of non-uniform batching, and assign prompts with the same label to the same batch. This work instead proposed to cluster based on the embedding similarity. I wonder if the authors could better clarify the difference.\n- According to the ablation studies, the utility improvement mainly comes from using PT models and add more ICL samples. The clustering contributes marginal utility improvement according to Table 3 and Table 4.\n- The selection of the cluster number $k$ is not clearly stated. Is it data dependent? Besides, does the $\\epsilon$ in the DP top-$k'$ selection has significant impacts? That is,  is the top-$k'$ clusters consistently being selected? According to Figure 2, the top-100 public centers almost includes all the samples.\n\n[1]: Private prediction for large-scale synthetic text generation. https://arxiv.org/pdf/2407.12108"}, "questions": {"value": "- The authors discussed the comparison against PE methods. I wonder if there will be similar improvements if PE methods refer to PT models rather than IT models.\n- Regarding the explanation of why median aggregation yields a smaller sensitivity, the authors wrote ' if Z contains at least 3 identical vectors, then the local sensitivity of median(Z) is zero.'. I have two questions : 1) in this case, add/remove a sample causes no impact to the algorithm, which should somehow degrade the utility; 2) this example does not make sense to me. Since the batch is sampled from a cluster, which implies a similar embedding distribution, there should be few outliers in the batch. In this case, the sensitivity for mean aggregation (theoretically 2C/B) should be similar to that of median aggregation (2C).\n- The refined understanding of privacy risk is promising. Could the authors elaborate on the criteria for private/public tokens and how the data-dependent accounting is designed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ToENDab18T", "forum": "e4B8QJfZnW", "replyto": "e4B8QJfZnW", "signatures": ["ICLR.cc/2026/Conference/Submission18529/Reviewer_dwQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18529/Reviewer_dwQB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296980117, "cdate": 1761296980117, "tmdate": 1762928224037, "mdate": 1762928224037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies private synthetic data generation via LLM inference. Prior work follows a subsample-and-aggregate approach: split the private data uniformly at random into batches, query the LLM to get next-token logits for each record (non-privately), and then privately aggregate those logits to sample tokens. While this method can yield good downstream task accuracy, the synthetic text may be a poor representative of the original distribution because uniform batching mixes heterogeneous examples.\n\nThe authors propose batching similar examples together to improve representativeness and reduce privacy cost. They first compute public cluster centers using a non-private dataset, then privately rebalance by adding DP noise to cluster counts and keeping the top-$k$ centers to ensure that the clusters are not too small while spending a small privacy budget. Then, for each batch they compute logits for all seeds in the batch, apply clipping, and aggregate by the median rather than the mean. They analyze this with a data-dependent DP guarantee: when a batch’s logits are concentrated (small “median gaps”), the local sensitivity is smaller, which yields a lower $\\epsilon$. Clustering makes batches more homogeneous, so the realized privacy loss can be lower than worst-case bounds.\n\nEmpirically, the method improves both representativeness metrics (e.g., MAUVE) and downstream accuracy compared to prior DP inference baselines. However, the overall privacy guarantee depends on the data and outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow. \n\n- Their proposed methods is simple but effective. It does a small change to prior DP-inference pipelines (replacing uniform batching and mean aggregation with cluster-informed batching and median aggregation) that is easy to adopt. The proposed method produces more representative synthetic data (higher MAUVE) and strong downstream accuracy in comparison with previous methods."}, "weaknesses": {"value": "- The privacy guarantee is ex-post and data-dependent, which is weaker. For example, releasing the realized $\\epsilon$ may leak information about the private data’s “median gaps.”\n\n- MAUVE is lower on Yelp, which is farther from DBPedia, suggesting that public–private distributional closeness matters; performance may degrade on niche private datasets."}, "questions": {"value": "Would applying PTR/smooth sensitivity keeps the advantage of the method, while also give a data-independent privacy guarantee?\nHow does the number of clusters affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LuiZZG4kGv", "forum": "e4B8QJfZnW", "replyto": "e4B8QJfZnW", "signatures": ["ICLR.cc/2026/Conference/Submission18529/Reviewer_3wGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18529/Reviewer_3wGe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755347452, "cdate": 1761755347452, "tmdate": 1762928223568, "mdate": 1762928223568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary: This paper addresses the problem that random sampling leads to heterogeneous batches and degrades data quality when generating synthetic text via differentially private (DP) language model inference. It proposes an improved method combining clustering and median aggregation. Core contributions include: 1) Constructing homogeneous batches by pre-clustering sensitive data to solve the heterogeneity issue of random sampling; 2) Designing a median-based logit aggregation algorithm that leverages the similarity of prediction results under homogeneous data to reduce local sensitivity, achieving a data-dependent ex-post DP guarantee; 3) Validating on datasets such as AGNews and Yelp that the method significantly outperforms existing baselines in MAUVE (distributional similarity) and downstream task accuracy with lower privacy cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Novel use of clustering as a preprocessing step to improve DP inference quality; novel median-based aggregation with a tailored data-dependent ex-post privacy analysis.\n2.The paper pairs formal analysis (Algorithm 1, Theorem 2 and proof sketch) with practical experiments and empirical privacy checks. Results are presented across multiple datasets and ε settings.\n3.The paper has a well-structured hierarchy, with coherent logic from problem formulation, method design to experimental verification. Explanations of core concepts (such as local sensitivity, data-dependent ex-post DP) are concise and clear.\n4.It solves the key problem of \"imbalance between quantity and representativeness\" in DP synthetic data, especially suitable for sensitive heterogeneous data scenarios such as medical records. It promotes the practicalization of DP inference technology."}, "weaknesses": {"value": "1.It relies on public datasets (DBPedia) to construct cluster centers. If the distribution of public data differs significantly from that of sensitive data, cluster imbalance may still occur; DP clustering methods perform poorly on high-dimensional text embeddings.\n2.There is a lack of further attack experiments (such as membership inference/extraction for a small number of anomalous samples).\n3. It fails to conduct a fair comparison with the latest DP synthetic data methods (such as API-based private evolution technology) under the same prompt and model settings.\n4. The sensitivity analysis of hyperparameters such as the number of clusters and embedding models is insufficient, and no general criteria for optimal parameter selection are clarified.\n5. Although the data-dependent ex-post DP guarantee is more accurate, the privacy cost needs to be calculated after generation, making it impossible to determine the privacy budget in advance. This increases the complexity of practical deployment."}, "questions": {"value": "1.How was the initial public k (e.g., 1000 → rebalanced to 60/80) chosen? Any sensitivity/selection rule? \n2. How should multiple per-output data-dependent ε values be combined into a final released ε? Which composition theorem is applied in practice?\n3. This method is not compared with some of the latest baselines: Aug-PE[1], DP-fusion[2], DP-SynRAG[3], INVISIBLEINK[4].\n4. The privacy cost of median aggregation depends on generation results. In practical deployment, how to balance the needs of \"ex-post calculation\" and \"real-time generation\"?\n\n[1] Differentially private synthetic data via foundation model APIs 2: Text\n[2] DP-fusion: Token-level differentially private inference for large language models\n[3] Differentially private synthetic text generation for retrieval-augmented generation (RAG)\n[4] InvisibleInk: High-utility and low-cost text generation with differential privacy"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F3DXVzmmnR", "forum": "e4B8QJfZnW", "replyto": "e4B8QJfZnW", "signatures": ["ICLR.cc/2026/Conference/Submission18529/Reviewer_Wa8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18529/Reviewer_Wa8q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928691452, "cdate": 1761928691452, "tmdate": 1762928222793, "mdate": 1762928222793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the quality of synthetic data generated via differentially private (DP) inference methods, where multiple LLM outputs seeded by different users’ data are aggregated to ensure privacy. The authors argue that heterogeneity among seed examples in each batch can harm the representativeness of the synthetic data. To address this, they propose a privacy-preserving pre-clustering approach that groups similar examples before aggregation and introduce a median-based aggregation (instead of averaging). Their experiments show improved MAUVE and accuracy scores compared to considered baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a practical limitation of existing DP inference methods by considering the effect of data heterogeneity.\n\n- Proposes a simple and intuitive modification (clustering and median-based aggregation) that can be easily integrated into existing frameworks."}, "weaknesses": {"value": "- I find the argument in line 52 to be flawed. I think it incorrectly conflates privacy guarantees with data representativeness or quality. Differential privacy doesn’t require the aggregated response to be “representative” of the data; it only ensures that the presence or absence of any individual record (seed example) doesn’t significantly affect the output distribution.\n\n- Table 3 is difficult to interpret because the reported ε values are not directly comparable. As far as I understand, the methods are based on different privacy formulations i.e data-dependent ex-post differential privacy versus approximate differential privacy, which provide distinct guarantees and should not be contrasted using the same ε metric."}, "questions": {"value": "- The process for selecting top-k cluster centers and reassigning seeds is unclear [line 244]\n\n- Would using median aggregation alone (without clustering) yield similar improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hUrxfp9QJ9", "forum": "e4B8QJfZnW", "replyto": "e4B8QJfZnW", "signatures": ["ICLR.cc/2026/Conference/Submission18529/Reviewer_X6y8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18529/Reviewer_X6y8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933860792, "cdate": 1761933860792, "tmdate": 1762928221777, "mdate": 1762928221777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}