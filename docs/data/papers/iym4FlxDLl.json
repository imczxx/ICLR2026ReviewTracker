{"id": "iym4FlxDLl", "number": 1018, "cdate": 1756828096543, "mdate": 1759898232061, "content": {"title": "Exploring Markov-chain-style Evidence Using LLMs for Complex Claims", "abstract": "The remarkable success of rationale generation provokes the precise evidence discovery, which aims to identify a small subset (evidence) from the context to infer a target claim. However, existing general methods often fall short in accurately modeling evidence strength and collaborative support. This paper reformulates evidence discovery as a multi-step prompt construction process and introduces a heuristic search framework, named McsE, to explore \\textbf{M}arkov-\\textbf{c}hain-\\textbf{s}tyle \\textbf{E}vidence. Specifically, we propose a novel strength modeling perspective: Large Language Models (LLMs) can effectively serve as reward functions to estimate evidence strength when appropriately prompted. Then, we incorporate independent and collaborative reward mechanisms to systematically explore diverse potential reasoning paths, ultimately establishing the most effective prompt path as evidence. Experiments conducted on three widely-used datasets show that the proposed framework outperforms seven baselines, with distinct advantages in extracting complex evidence.", "tldr": "", "keywords": ["Evidence Discovery", "Large Language Model", "Heuristic Search", "Complex Claim"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5b6386a32d6ac4b5ddb8113b43227f16892c2c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of evidence discovery for claim verification—a vital task for explainable and trustworthy AI. The authors critique existing methods, which largely focus on simple relevance detection and often ignore the interdependence or collaborative nature of evidence. They propose McsE, a heuristic search framework that models evidence discovery as a Markov Decision Process, leveraging LLMs as reward functions to score the “strength” of evidence. Key innovations include the use of context-grounded prompts and the explicit modeling of both independent and collaborative evidence support. The framework is evaluated on multiple datasets (HoVer, PubMedQA, CovidET), demonstrating superior performance over both statistical, embedding-based, and LLM-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is clearly written.\n2. Evidence discovery is formulated as a Markov Decision Process, allowing a systematic exploration of reasoning paths rather than independent selection."}, "weaknesses": {"value": "**My main concern is that the build retrieval MDP may be equivalent, even only a subset of the active retrieval or active RAG methods**\n\nThere is a wide collection of active retrieval and generation methods, including [1], the famous self-RAG, [2], the famous IterGen. These methods build the new retrieval query by synthesizing the unsolved part of the claim or the original query.\n\nYour MDP designs an active retrieval process, which can be considered as a special variant of active retrieval methods where we do not use the LLM to analyze between retrieval steps. So based on my idea, I cannot recognize this paper's contribution now.\n\n\n[1] Asai A, Wu Z, Wang Y, et al. Self-rag: Learning to retrieve, generate, and critique through self-reflection[J]. 2024.\n\n[2] Ugare S, Gumaste R, Suresh T, et al. IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking[J]. arXiv preprint arXiv:2410.07295, 2024."}, "questions": {"value": "1. I would like to see comparison results to active RAG methods.\n\n\n2. Please change figures to Vector graphics"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WamdX6dHdd", "forum": "iym4FlxDLl", "replyto": "iym4FlxDLl", "signatures": ["ICLR.cc/2026/Conference/Submission1018/Reviewer_xZfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1018/Reviewer_xZfh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882861651, "cdate": 1761882861651, "tmdate": 1762915656897, "mdate": 1762915656897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces McsE, a heuristic search framework for evidence discovery that formulates the task as a Markov Decision Process and uses large language models (LLMs) as reward functions to estimate the strength of evidence supporting complex claims. The framework explicitly models both independent and collaborative (multi-hop) support among context sentences, enabling the discovery of reasoning chains rather than isolated evidence. Experiments on several datasets (HoVer, PubMedQA, CovidET, MultiRC) show that McsE outperforms strong statistical, embedding-based, and LLM-based baselines, particularly for multi-hop evidence extraction. Ablation studies and analyses are provided to support the value of the main components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses an important and challenging problem: discovering collaborative, multi-hop evidence for claims in a general, task-agnostic way. \n- The authors provide a Novel reformulation of evidence discovery as a Markov Decision Process with LLM-based reward functions, explicitly modeling both independent and collaborative support.\n- Evaluation across multiple datasets and domains, with consistent improvements over strong baselines, with ablation studies and analysis of prompt design and collaborative reward modeling."}, "weaknesses": {"value": "- High computational cost limits scalability and real-world applicability, especially for large candidate sets or longer reasoning chains.\n- Relies on LLMs for reward estimation, risking propagation of LLM hallucinations or biases as 'evidence'.\n- Evaluation on some datasets may be biased by LLM-generated ground truth and limited manual annotation."}, "questions": {"value": "- Can the authors provide more details on the manual annotation/correction process for datasets with GPT-4-generated evidence? How is annotation quality and inter-annotator agreement ensured?\n- How robust is the LLM-based reward signal to model drift, prompt variations, or out-of-domain contexts? Have you tested the framework with other LLMs or under distribution shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NoMuHOYiII", "forum": "iym4FlxDLl", "replyto": "iym4FlxDLl", "signatures": ["ICLR.cc/2026/Conference/Submission1018/Reviewer_D9zv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1018/Reviewer_D9zv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936912221, "cdate": 1761936912221, "tmdate": 1762915656767, "mdate": 1762915656767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Markov-chain style Evidence (McsE), a framework for evidence discovery that retrieves supporting evidence for input claims using markov chain search. The authors argue that current methods fail to differentiate relevance and support, and these approaches struggle to identify collaborative / inter-sentence interactions, yet oftentimes multiple sentences must be combined to validate a complex claim. Therefore, McsE formulates evidence discovery as a multi-step search process with MDP, in which the search process leverages LLM as a scoring function to estimate evidence strength. McsE combines independent and collabirative supports, and also considers cumulative rewards in the value funtion to guide the iterative selection of evidence. To validate the effectiveness of the proposed method, the authors perform extensive experiments to show that McsE can outperform existing retriever / ranker baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authros identify critical challenges in evidence discovery / retrieval and motiviated by these challenges, they proposed McsE to leverage LLMs to guide the evidence discovery process with MDP search.\n\n2. The authors introduce a novel method for evaluating evidence strength by using LLMs to score evidence for input claims, and thus constructing a robust value function to guide to search of the evidence chain.\n\n3. McsE outperforms serveral baselines across three diverse benchmarks, demonstrating advantages in retriving and evaluating evidence for complex, multi-hop claims."}, "weaknesses": {"value": "1. There's limited novelty within the proposed evidence discover method, as there are already a few papers with MCTS- or MDP-drive process for search / retrieval. Similar works like MCTS-RAG (https://arxiv.org/pdf/2503.20757), ETS (https://aclanthology.org/2025.acl-long.1175.pdf) and other chain of evidence / search should also be discussed and potentially included as baselines.\n\n2. Lack of direct downstream performance (e.g., RAG /fact verification accuracy etc.). In addition, the entire framework's effectiveness relies on the LLM's ability to score with \"according to\" prompt; the authors should provide more analysis on prompt / scoring (log-p, verbalized probs etc.) selection and their effectiveness of the overall perofrmance.\n\n3. The McsE framework is significantly more expensive than traditional retriever / ranker models, as it involves a complex heuristic search and multiple calls to an LLM to estimate value. Such computation costs are not disscussed in details."}, "questions": {"value": "See weakness.\n\nAlso, there are some mismatches on figure / table captions, for example line 244 - 245 mentions figure 2, but should actually be figure 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ssmvI92L9K", "forum": "iym4FlxDLl", "replyto": "iym4FlxDLl", "signatures": ["ICLR.cc/2026/Conference/Submission1018/Reviewer_x8v5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1018/Reviewer_x8v5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037650944, "cdate": 1762037650944, "tmdate": 1762915656616, "mdate": 1762915656616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}