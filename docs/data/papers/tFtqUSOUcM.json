{"id": "tFtqUSOUcM", "number": 21396, "cdate": 1758317128534, "mdate": 1763643253076, "content": {"title": "In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior", "abstract": "In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. In bandit settings, we prove this principled exploration reaches regret-optimal behaviour even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.", "tldr": "SPICE uses an ensemble of value heads and bayesian context fusion to perform in-context reinforcement learning on suboptimal data.", "keywords": ["in‑context reinforcement learning", "decision transformer", "sequence modeling", "value ensembles", "randomised prior functions", "Bayesian context fusion", "posterior UCB", "offline reinforcement learning", "contextual bandits", "exploration–exploitation", "epistemic uncertainty", "importance weighting", "gradient‑free adaptation", "meta learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed5b487eb5cb4a8f81173180cdd0deed1b9d1c45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a Bayesian ICRL method named SPICE that enables efficient adaptation from suboptimal offline data. This method introduces a value-function ensemble trained with Bayesian shrinkage to learn a calibrated prior over action values, and performs Bayesian fusion at inference time to combine this prior with task-specific contextual evidence. SPICE further employs a weighted policy-head objective to improve representation learning under behavior-policy bias. The authors provide a theoretical analysis in the stochastic bandit setting, establishing a logarithmic regret bound for the proposed posterior-UCB controller, and demonstrate through experiments on both bandit and MDP environments (e.g., Dark Room) that SPICE achieves significantly lower cumulative regret and higher returns compared to existing in-context RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors integrate a policy head and value ensemble head to the Transformer backbone\n    1. In policy head, the authors include three different complementary weights to mitigate behavior policy bias, emphasize reward-relevant behaviors, and focus learning on uncertain or under-explored regions, respectively. These mechanisms enhance representation learning from suboptimal offline data and improve the backbone model's ability to support reliable value estimation.\n    1. In value ensemble head, the authors employ Bayesian shrinkage during training to obtain a calibrated value prior, and perform Bayesian fusion at inference to combine this prior with task-specific contextual evidence. These design choices enable principled uncertainty modeling, task generalization, and coherent exploration under limited or noisy data. \n1. In addition to the architecture innovations, this paper also gives a rigorous theoretical analysis of the proposed Bayesian controller in the stochastic bandit setting, providing formula bounds of regret demonstrating how prior miscalibration only introduces a constant warm-start penalty. This analysis offers a sound theoretical foundation for Bayesian ICRL.\n1. The empirical experiments well match their theory, showing both the correctness of the theory contribution and the effectiveness in the real tasks."}, "weaknesses": {"value": "1. While ICRL methods are often tested in simplified environments such as Dark Room, it would strengthen the paper to evaluate SPICE on high-dimensional continuous control benchmarks (e.g., MuJoCo or Meta-World). These are widely adopted in reinforcement learning research and would better demonstrate the method’s scalability and generalization beyond discrete or low-dimensional tasks.\n1. The choice of PPO as a baseline may be suboptimal, as PPO is an on-policy algorithm that is typically sample-inefficient and struggles in sparse-reward settings. Using off-policy algorithms (e.g., DDPG, SAC) or PPO combined with Hindsight Experience Replay (HER) [1] would provide a fairer and more competitive comparison, particularly for the sparse-reward nature of the Dark Room environment.\n1. The paper does not discuss the choice of kernel function used for measuring context-state similarity during Bayesian fusion, even though this choice can affect the final performance. A discussion or ablation on kernel type would clarify how robust the method is to this critical design decision.\n\n[1] Crowder, Douglas C., et al. \"Hindsight experience replay accelerates proximal policy optimization.\" arXiv preprint arXiv:2410.22524 (2024)."}, "questions": {"value": "1. Could the authors extend regret or sample-complexity analysis to MDPs? This would significantly strengthen the theoretical contribution and demonstrate the broader applicability of SPICE beyond the bandit setting.\n1. While the three weighting factors in the policy head intuitively improve representation learning from suboptimal offline data, the policy head itself is not used during inference. Could the authors quantify how much these weighting schemes affect the overall performance and how sensitive is the algorithm to the policy-head training objective and the associated hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dENDTnPcOx", "forum": "tFtqUSOUcM", "replyto": "tFtqUSOUcM", "signatures": ["ICLR.cc/2026/Conference/Submission21396/Reviewer_SFAi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21396/Reviewer_SFAi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804018145, "cdate": 1761804018145, "tmdate": 1762941745197, "mdate": 1762941745197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SPICE, a Bayesian ICRL method that learns an ensemble-based prior over Q-values from suboptimal data and, at test time, fuses this prior with kernel-weighted context statistics to form per-action posteriors. The agent acts greedily offline or with a posterior-UCB rule online. Experiments on bandits and a darkroom MDP show strong adaptation and regret reduction versus DPT/AD under weak supervision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Learning an explicit, calibrated value prior for ICRL  is a clear and novel idea.\n2. Theoretical contribution: proves regret-optimality (O(log K)) in stochastic bandits without requiring optimal pretraining.\n3. Empirical validation aligns with theory: SPICE tracks UCB’s logarithmic regret and adapts quickly from suboptimal logs, whereas sequence-only ICRL baselines remain tied to behavior policy"}, "weaknesses": {"value": "1. The assumption of SPICE is too strong to limit its application.  The Bayesian fusion uses kernel-weighted evidence, implicitly assuming  a form of local smoothness in the action-value landscape—i.e., that states close under the chosen kernel share similar action values, and consequently that evidence from nearby states is informative for the query. However, this can break in domains with discontinuous or highly multimodal action-value structure where small perturbations in state can correspond to very different optimal actions.  Thus it is hard to apply this method to more complcated real-world tasks, unless the authors prove that using experiments results. \n2. The method assumes the ensemble yields reasonably calibrated priors. Overconfident or biased ensembles can harm short-horizon performance despite the bandit warm-start guarantee. No stress tests on deliberate miscalibration are provided\n3. Validation is confined to bandits and a stylized darkroom MDP; scalability to complex, long-horizon, or partially observable tasks remains untested.\n4. Comparisons omit recent in-context RL methods that do not require optimal labels (e.g., ICEE, DIT). Including them is important to discover the true value of this method\n5. In Eq. (4), the policy uses three weighting factors (importance, advantage, epistemic). While advantage weighting is motivated for learning from suboptimal data, the contribution of the other two is less clear. A targeted ablation isolating each term would clarify their necessity and impact."}, "questions": {"value": "1. Darkroom evaluation: Are the test tasks interpolation within the training task distribution or extrapolation  to out-of-distribution goals/dynamics? Please clarify how held-out tasks differ from training  and quantify the distributional shift.\n2. Kernel sensitivity: How sensitive is performance to the choice of kernel and bandwidth in Bayesian fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4GLlI00Oba", "forum": "tFtqUSOUcM", "replyto": "tFtqUSOUcM", "signatures": ["ICLR.cc/2026/Conference/Submission21396/Reviewer_Yryg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21396/Reviewer_Yryg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822635611, "cdate": 1761822635611, "tmdate": 1762941743350, "mdate": 1762941743350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SPICE, an In-Context Reinforcement Learning (ICRL) algorithm that learns a value prior from suboptimal data using a transformer-based Q-ensemble. At test time, it performs a gradient-free Bayesian fusion of this prior with context evidence to guide exploration via a UCB policy. The method demonstrates superior performance over existing approaches, particularly in adapting from non-optimal trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- By using value-based learning instead of imitation, SPICE can train on mixed-quality historical data, making it far more practical than methods that require optimal demonstrations.\n- The algorithm is backed by an optimal O(log K) regret bound for bandits. The theory confirms that any prior miscalibration only adds a constant cost, providing a strong justification for its test-time efficiency.\n- It uses a novel combination of advantage and uncertainty weighting to guide the transformer in learning representations specifically tailored for Q-value estimation, thereby improving the quality of the learned prior."}, "weaknesses": {"value": "- The paper does not analyze the algorithm's breaking point with poor data. If training data is too heterogeneous (e.g., mixed with random policies), the Q-ensemble may fail to learn a useful prior. An analysis of performance degradation versus data quality is needed to define the method's practical limits.\n- Performance critically depends on the state-similarity kernel, but little guidance is offered on its selection. A mismatch between the kernel's similarity metric and the true Q-function's structure could corrupt the Bayesian update.\n- The formal theory is confined to the bandit setting and does not address complexities of full MDPs, such as long-horizon credit assignment or compounding errors from bootstrapping.\n- Sec. 3.1 is too long and could be organized in a better way."}, "questions": {"value": "- The scale of Fig. 3 makes it very hard to compare SPICE with baselines other than DPT.\n  \n- How does SPICE perform in more complicated environments other than the bandit and grid-world navigation setups investigated in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tcdjfVzsPM", "forum": "tFtqUSOUcM", "replyto": "tFtqUSOUcM", "signatures": ["ICLR.cc/2026/Conference/Submission21396/Reviewer_3pzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21396/Reviewer_3pzy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997561662, "cdate": 1761997561662, "tmdate": 1762941742959, "mdate": 1762941742959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SPICE, an algorithm that solves the in-context reinforcement learning (ICRL) problem by learning a prior over Q-functions when learning from the offline data, and utilizes a UCB-like online inference mechanism to solve the RL problem in context. The paper provides theoretical justification for the algorithm and runs experiments on bandit/control tasks to demonstrate the effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides both theoretical and justification for the algorithm proposed."}, "weaknesses": {"value": "I think the writing quality of this paper does not meet the standard of ICLR. I feel lost while reading the paper. First, the paper should start with the formulation of the ICRL problem rather than the architecture design. Second, the architecture part contains too many details without explanation, and I do not know what I am supposed to pay attention to to understand the idea. Third, though I am relatively familiar with bandit algorithms, I do not understand where formulas 14 and 15 come from. Besides, Figure 1 is poorly made and is hard to read."}, "questions": {"value": "1. What is the (theoretical) formulation of ICLR that this paper is trying to use?\n2. In the architecture part, which design choices are the most crucial ones?\n3. What is the high-level idea of precision additivity, and what do formulas 14, 15 mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fl1xuceY5v", "forum": "tFtqUSOUcM", "replyto": "tFtqUSOUcM", "signatures": ["ICLR.cc/2026/Conference/Submission21396/Reviewer_6roR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21396/Reviewer_6roR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213590865, "cdate": 1762213590865, "tmdate": 1762941742547, "mdate": 1762941742547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}