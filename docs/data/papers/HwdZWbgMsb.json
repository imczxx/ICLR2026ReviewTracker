{"id": "HwdZWbgMsb", "number": 10802, "cdate": 1758182206257, "mdate": 1763727193680, "content": {"title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation", "abstract": "3D instance segmentation is crucial for understanding complex 3D environments, yet fully supervised methods require dense point-level annotations, resulting in substantial annotation costs and labor overhead.\nTo mitigate this, box-level annotations have been explored as a weaker but more scalable form of supervision.\nHowever, box annotations inherently introduce ambiguity in overlapping regions, making accurate point-to-instance assignment challenging.\nRecent methods address this ambiguity by generating pseudo-masks through training a dedicated pseudo-labeler in an additional training stage. \nHowever, such two-stage pipelines often increase overall training time and complexity, hinder end-to-end optimization.\nTo overcome these challenges, we propose BEEP3D—Box-supervised End-to-End Pseudo-mask generation for 3D instance segmentation.\nBEEP3D adopts a student-teacher framework, where the teacher model serves as a pseudo-labeler and is updated by the student model via an Exponential Moving Average.\nTo better guide the teacher model to generate precise pseudo-masks, we introduce an instance center-based query refinement that enhances position query localization and leverages features near instance centers.\nAdditionally, we design two novel losses—query consistency loss and masked feature consistency loss—to align semantic and geometric signals between predictions and pseudo-masks.\nExtensive experiments on ScanNetV2 and S3DIS datasets demonstrate that BEEP3D achieves competitive or superior performance compared to state-of-the-art weakly supervised methods while remaining computationally efficient.", "tldr": "", "keywords": ["3D instance segmentation", "weakly-supervised learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7fa40de708dfc48999127654d3846447ce5788d.pdf", "supplementary_material": "/attachment/4ea79b13a4a96f532c3057cba5caeee4e8040f7a.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies 3D instance segmentation using only 3D bounding boxes. The authors propose BEEP3D: a single-stage training framework that generates pseudo-masks online under a student-teacher scheme where the teacher is updated via EMA from the student; instance-center–guided positional queries for refinement; and two consistency losses, query consistency and mask-feature consistency, to align representations. The method achieves near fully supervised AP on ScanNetV2 and S3DIS and removes the need for a separately pre-trained pseudo-labeler."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **End-to-end pseudo-mask generation**\n\n  * Integrates the pseudo-labeler into the training loop in a single stage, avoiding extra pretraining and simplifying the pipeline and T′ cost.\n  * Provides a clear comparison with two-stage methods in terms of pipeline and parameter freezing.\n\n* **Closeness to fully supervised upper bound**\n\n  * On ScanNetV2, the “% full” relative to the corresponding fully supervised method reaches about 98%. Importance: strong performance at the cost of weak supervision.\n  * Maintains competitiveness across AP, AP50, and AP25 thresholds. Importance: robustness under different IoU requirements.\n  * Achieves leading AP on S3DIS Area 5 as well. Importance: cross-dataset effectiveness.\n  * Works with multiple backbones (MAFT, SPFormer). Importance: compatibility."}, "weaknesses": {"value": "* **Insufficient statistical significance and reproducibility information**\n\n  * Does not report variance across multiple runs, confidence intervals, or significance tests.\n  * Random seeds and data split/resampling strategies are not specified.\n  * No links to code and model weights or a licensing plan are provided.\n\n* **Confirmation bias and error propagation in the pseudo-label loop**\n\n  * The teacher is obtained via EMA of the student; early student errors may be locked into the target ( $\\hat{m}_u \\cup m_l$ ).\n  * Lacks comparisons to “frozen teacher” or “no teacher” alternatives to bound the loop’s benefit.\n\n* **Limited evidence for generalization and noise robustness**\n\n  * Evaluation is limited to ScanNetV2 and S3DIS, which are similar domains.\n  * No systematic degradation curves under box noise, offsets, or scale perturbations (related to the Sketchy-3DIS setting).\n  * No stratified analysis of cases with heavy occlusion or dense overlap.\n  * The ISBNet variant shows “negligible” gains without analysis of causes.\n  * Pseudo-mask quality (mACC) is computed only on the training set; no validation-set measure is reported."}, "questions": {"value": "* **Statistical significance and reproducibility**\n\n  * Will you provide links to the code and model weights, including environment files and training scripts?\n\n* **Confirmation bias and error propagation in the pseudo-label loop**\n\n  * What is the effect of varying the EMA decay {0.90, 0.95, 0.99} and the update frequency on AP and training stability?\n  * Can you add and explain ablations with a frozen teacher and with no teacher to quantify the loop’s upper bound and its necessity?\n\n* **Generalization and noise robustness**\n\n  * Can you evaluate on additional domains (e.g., different building styles) and report at least cross-domain validation results?\n  * Will you sweep box center offsets, scale perturbations, and size noise, plot degradation curves, and compare to a Sketchy-style perturbation protocol?\n  * Can you provide a hard-case analysis with AP and visualizations on subsets that exhibit heavy overlap and strong occlusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j62d2Tq8aN", "forum": "HwdZWbgMsb", "replyto": "HwdZWbgMsb", "signatures": ["ICLR.cc/2026/Conference/Submission10802/Reviewer_WQrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10802/Reviewer_WQrj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704581657, "cdate": 1761704581657, "tmdate": 1762922016859, "mdate": 1762922016859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "Dear reviewers, \n\nWe sincerely appreciate thoughtful comments and detailed feedback. We have carefully addressed all the raised concerns in our rebuttal and revised manuscript.\n\nFirst, we deeply appreciate your recognition of the following strengths in our work:\n- SOTA performances on ScanNetV2 and S3DIS, closing the gap to fully supervised methods.\n- Training efficiency derived from the end-to-end single-stage training paradigm.\n\nAdditionally, we are grateful for the opportunity to clarify and address several key concerns raised in the reviews.\n\n**Q1.** Negligible performance improvements on ISBNet and limitations on transformer-based architecture. (Reviewer UCvp, bt6s, WQrj)\n\n**A1.** Transformer decoders with object/instance queries have become a mainstream design paradigm in recent 3DIS, as demonstrated by works such as Mask3D [1] and MAFT [2], and by the fact that many of the top-performing methods on the ScanNetV2 test server [3–6] adopt transformer-based architectures. Our modules—Instance Center-based Query Refinement and Consistency Losses—are therefore intentionally developed within this widely adopted query–decoder framework.\nRegarding the limited gains observed with ISBNet, we agree that our components are tightly coupled with transformer-based architectures and are not yet fully optimized for non-transformer backbones. As discussed in the paper, this reflects an architectural mismatch rather than a conceptual limitation of BEEP3D. We view this not as a drawback but as a natural consequence of designing components tailored to the dominant architecture family in modern 3DIS. To further demonstrate BEEP3D’s generality within transformer-based models, we have begun experiments integrating BEEP3D with OneFormer3D[6], and we will include the results during the rebuttal period.\n\n**Q2.** Robustness on noisy box annotation. (Reviewer UCvp, WQrj)\n\n**A2.** We acknowledge the concern about sensitivity to box noise. We note that most existing box-supervised 3DIS methods—except for Sketchy-3DIS—are also highly vulnerable to noisy annotations. As shown in Table 1 of Sketchy-3DIS [7], conventional box-supervised baselines experience substantial degradation under noisy boxes (e.g., on the ScanNetV2 validation set, AP50 drops from 59.7→52.4 for Box2Mask and 70.4→53.5 for GaPro+SPFormer). This confirms that noise robustness is a general challenge in box-supervised 3DIS rather than a limitation unique to our method.\nWhile Sketchy-3DIS explicitly focuses on noise robustness, this comes with a clear trade-off: under clean box annotations, BEEP3D outperforms Sketchy-3DIS in both AP50 and AP25 on the ScanNetV2 validation set. As the reviewers noted, noisy boxes are common in real-world weak supervision scenarios, and we agree that incorporating dedicated noise-robust mechanisms is a promising direction for future work.\n\n**Q3.** Failure case analysis (Reviewer UCvp, v8Bp, WQrj)\n\n**A3.** We appreciate the reviewers’ suggestions to provide a more detailed analysis of failure cases. As shown in Appendix A3 of the revised manuscript, BEEP3D tends to fail in highly cluttered scenes where the bounding box of one instance fully overlaps with that of another. In such cases, the coarse box-level supervision provides insufficient spatial cues to distinguish the two instances, and BEEP3D inherits this inherent limitation. We believe this analysis clarifies the model’s behavior under challenging annotation conditions and highlights meaningful directions for future work.\n\n\nWe sincerely hope these revisions effectively address the reviewers' concerns and contribute to improving the overall quality of our paper. Thank you once again for your thoughtful review and valuable feedback.\n\nBest regards,  \n\nThe Authors\n\n[1] Schult, Jonas, et al. \"Mask3D: Mask Transformer for 3D Semantic Instance Segmentation.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[2] Lai, Xin, et al. \"Mask-attention-free transformer for 3d instance segmentation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[3] Lu, Jiahao, and Jiacheng Deng. \"Relation3D: enhancing relation modeling for point cloud instance segmentation.\" 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2025.\n\n[4] Wang, Duanchu, Haoran Gong, and Di Wang. \"Enhancing 3D Instance Segmentation With Dense Connection Decoder and Layer-Aware Fusion.\" IEEE Robotics and Automation Letters (2025).\n\n[5] Lu, Jiahao, et al. \"Query refinement transformer for 3d instance segmentation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[6] Kolodiazhnyi, Maxim, et al. \"Oneformer3d: One transformer for unified point cloud segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[7] Deng, Qian, et al. \"Sketchy Bounding-box Supervision for 3D Instance Segmentation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}}, "id": "3ylxHxsg9k", "forum": "HwdZWbgMsb", "replyto": "HwdZWbgMsb", "signatures": ["ICLR.cc/2026/Conference/Submission10802/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10802/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10802/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725644822, "cdate": 1763725644822, "tmdate": 1763727287135, "mdate": 1763727287135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of 3D instance segmentation under weak supervision, specifically from 3D bounding boxes instead of dense point-level annotations. The authors propose BEEP3D, a student-teacher framework where the teacher generates pseudo-masks that guide the student network in an end-to-end manner. Unlike prior methods that either require pretraining a separate pseudo-labeler or rely on heavy geometric priors, BEEP3D integrates pseudo-mask generation directly into training. It uses instance center–based query refinement instead of employing learnable parameters. Two consistency losses are introduced to align the student and teacher representations. The framework is implemented on MAFT, ISBNet, and  SPFormer models, and tested on the ScanNetV2 and S3DIS benchmarks, where it showed competitive results compared to fully supervised methods. The method also cuts training complexity compared to previous weakly supervised methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method achieves near-supervised accuracy on two standard datasets.\n- The writing is mostly clear, the figures are illustrative, and the ablation studies are well-structured. Visual results show improved segmentation in overlapping regions.\n- Unlike BSNet or GaPro, BEEP3D eliminates an extra pseudo-labeling stage, reducing training time without sacrificing accuracy."}, "weaknesses": {"value": "- The key ideas proposed in this paper (EMA teacher-student updates, query refinement, and consistency-based losses) are well-established concepts adapted from semi-supervised and weakly supervised learning to 3DIS. \n- The authors note that when integrated with ISBNet (a non-transformer-based network), BEEP3D underperforms compared to BSNet on S3DIS. This suggests that the framework’s advantages rely heavily on transformer-based query mechanisms, limiting applicability to other architectures.\n- While the paper explicitly positions BEEP3D as transformer-based, it tests only on MAFT and SPFormer. Since there are several other transformer architectures for 3DIS (e.g., Mask3D, OneFormer3D, QueryFormer), broader evaluation across more transformer variants would strengthen claims of generality even within its intended paradigm."}, "questions": {"value": "- Can the authors analyze pseudo-mask quality over training epochs to substantiate claims about refinement?\n- Can the authors evaluate their framework across a wider set of transformer-based 3DIS methods (1 or 2 extra) to strengthen their evaluation on the two chosen datasets?\n- There seems to be a minor issue with the citation format throughout the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aQiClpAgWw", "forum": "HwdZWbgMsb", "replyto": "HwdZWbgMsb", "signatures": ["ICLR.cc/2026/Conference/Submission10802/Reviewer_bt6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10802/Reviewer_bt6s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835054237, "cdate": 1761835054237, "tmdate": 1762922016491, "mdate": 1762922016491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BEEP3D, a novel framework for 3D instance segmentation using only 3D bounding box supervision. The core challenge is the ambiguity from overlapping boxes. BEEP3D cleverly solves this with an end-to-end student-teacher framework, where the teacher model acts as a pseudo-labeler and is updated via an EMA of the student. This unifies pseudo-mask generation and segmentor training into a single stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The end-to-end, single-stage paradigm is a significant advancement over prior multi-stage methods (e.g., GaPro, BSNet). It's elegant, efficient, and avoids reliance on simulated data or complex priors.\n\n2. The instance center-based query refinement is a smart strategy to leverage the most reliable signal from the weak supervision. The dual consistency losses provide robust supervision for the student model.\n\n3. The method achieves state-of-the-art (or highly competitive) performance on ScanNetV2 and S3DIS. Impressively, it closes the gap to fully-supervised methods, achieving 98.1% of the full-supervision AP on the ScanNetV2 validation set.\n\n4. As shown in Table 5, the method eliminates the separate pre-training time ($T'$) for pseudo-label generation, making it efficient and practical.\n\nClarity: The paper is well-written, and the method is presented clearly."}, "weaknesses": {"value": "1. Evaluation on ScanNet++: It would be valuable to include results on ScanNet++, which offers more challenging indoor scenes and richer geometric details. This would further demonstrate the generalization ability of the proposed method.\n\n\n2. Failure Case Discussion: Adding a brief discussion of failure cases (e.g., in extremely cluttered scenes) would provide a more complete picture of the method's limitations and guide future work."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O5jfzksoBk", "forum": "HwdZWbgMsb", "replyto": "HwdZWbgMsb", "signatures": ["ICLR.cc/2026/Conference/Submission10802/Reviewer_v8Bp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10802/Reviewer_v8Bp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914402808, "cdate": 1761914402808, "tmdate": 1762922016146, "mdate": 1762922016146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose BEEP3D, an end-to-end framework for weakly supervised 3D instance segmentation using only 3D bounding box annotations. The method employs a student-teacher architecture where the teacher model is updated via EMA of the student model and serves as a pseudo-labeler. To improve pseudo-mask quality in ambiguous overlapping regions, the framework introduces two key techniques: (1) instance center-based query refinement that leverages center coordinates from bounding boxes as strong priors to guide the teacher model, and (2) two consistency losses (query consistency and masked-feature consistency) to ensure alignment between student and teacher models at both query and feature representation levels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tBEEP3D integrates pseudo-label generation and segmentation model training within a unified training loop. This eliminates the need for separate pre-training stages, significantly simplifying the training pipeline and improving overall efficiency.\n2.\tThe method exploits strong geometric priors implicit in box annotations, specifically instance center coordinates. By enforcing the teacher model's position queries to consistently aggregate these center points, it provides robust spatial guidance for the model. \n3.\tTo ensure effective knowledge transfer from teacher to student, the paper designs two novel consistency losses."}, "weaknesses": {"value": "1.\tExperimentally, on the validation set, the method achieves relatively limited AP improvements. More critically, it underperforms BSNet+MAFT across both AP₅₀ and AP₂₅ metrics. On the test set, the absence of corresponding performance data from competing methods prevents fair comparison. Additionally, comparisons with the latest state-of-the-art methods are missing. \n2.\tThe method's core innovations, particularly Instance Center-based Query Refinement and Query Consistency Loss, heavily rely on Transformer-specific query mechanisms, making them relatively incremental. It is not a general weakly supervised approach, when applied to non-Transformer architectures, performance even degrades, as shown in Table 2 where Ours + ISBNet underperforms BSNet + ISBNet. \n3.\tThe Instance Center-based Query Refinement critically depends on accurate instance center points extracted from bounding boxes. The design does not address method robustness when box annotations contain noise (common in weak supervision). Meanwhile, the hard arg max assignment in pseudo-mask generation may lead to rapid performance degradation when teacher model predictions are inaccurate (error accumulation)."}, "questions": {"value": "1.\tCould the authors provide error analysis to better understand failure cases and model limitations?\n2.\tIn Table 1 validation set, although BEEP3D's AP (57.3) is slightly higher than BSNet (56.2), it shows comprehensive deterioration in both AP₅₀ and AP₂₅ metrics. Does this indicate that BEEP3D sacrifices instance detection recall to optimize high-IoU segmentation precision? Can the authors explain the underlying causes of this critical metric regression?\n3.\tGiven that student and teacher models share identical network architecture, a more straightforward self-training baseline seems viable: using only a student model where predictions at step N generate pseudo-masks to supervise training at step N+1. Can the authors justify the necessity of adopting the student-teacher framework compared to this more direct self-training loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nVNrotThal", "forum": "HwdZWbgMsb", "replyto": "HwdZWbgMsb", "signatures": ["ICLR.cc/2026/Conference/Submission10802/Reviewer_UCvp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10802/Reviewer_UCvp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970615456, "cdate": 1761970615456, "tmdate": 1762922015676, "mdate": 1762922015676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}