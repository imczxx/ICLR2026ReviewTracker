{"id": "Y47Ab11TjP", "number": 21052, "cdate": 1758313230389, "mdate": 1759896945018, "content": {"title": "DiZCo: Planning Zero-Shot Coordination in World Models", "abstract": "Developing intelligent agents capable of seamlessly cooperating and coordinating with other agents in shared environments, including humans, has become a critical research challenge in the field of AI. This requires agents to understand environment dynamics and anticipate other agents responses' to each action. Current research approaches Human-AI coordination through model-free policies for optimality and population-based training for robustness. However, these approaches are brittle and can fail when collaborating with people due to the diverse and unpredictable nature of human behavior, which cannot be comprehensively captured by the training distribution. Striving for a solution that balances robustness and optimality, we introduce **DiZCo**, the first framework that leverages generative models to enable real-time, search-based planning in a complex human-AI cooperative task. We first train a generative model to predict future world trajectories conditioned on current state, ego actions, and partner strategy based on their identity, serving as our world model. Then we train a generative action proposer that proposes plausible ego action candidates based on the world state. At test time, we identify the optimal future trajectory by searching through outcomes of all proposed action candidates passed into our world model. Offline evaluations indicate that the DiZCo framework outperforms state-of-the-art model-free policies in joint reward. To validate that this method can be feasible for real-time human interaction, we engineer a system that enables model-based planning and search to operate at speeds fast enough to cooperative live with humans. A preliminary user study resulted in positive feedback, collectively underscoring its practical effectiveness for real-time human-AI collaboration.", "tldr": "We introduce a framework that addresses the human-AI collaboration by by combining environmental and partner modeling with test-time search.", "keywords": ["human-AI cooperation", "generative modeling", "multi-agent planning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afae61cc0f0b7f36780658eca7817abd3c3ec1cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces DIZCO, a framework that utilizes generative models to enable real-time, search-based planning in complex human-AI cooperative tasks. DIZCO is a model-based method that employs a generative model (the world model) to predict future world trajectories conditioned on the current state, the ego agent's actions, and the partner's identity. Offline evaluations indicate that the DIZCO framework outperforms state-of-the-art model-free policies in terms of joint reward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The work operates in an offline setting, leveraging rollouts sampled from a previous population-based method. This is a significant strength as it eliminates the need for human labeling and substantially reduces the reliance on costly RL sampling.\n\nThe experimental results clearly demonstrate superior performance when compared to previous population- based training methods."}, "weaknesses": {"value": "1. The claim in line 20, “the first framework that leverages generative models to enable real-time…,” appears to be an overclaim. Existing works, such as Proagent [1] and E3T [2], also focus on using generative models or related concepts (like partner prediction modules) to address human-AI cooperative tasks. The authors should specify the unique differentiating characteristic of DIZCO's use of generative models compared to prior art.\n[1]Zhang, Ceyao, et al. \"Proagent: building proactive cooperative agents with large language models.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 16. 2024.\n\n[2] Yan, Xue, et al. \"An efficient end-to-end training approach for zero-shot human-AI coordination.\" Advances in neural information processing systems 36 (2023): 2636-2658.\n\n2. The motivation for using a diffusion model for planning and generalizing to $N$ agents is unclear. It appears to be primarily used for sequential state prediction.\n\n3. The paper mainly describes the methodology for the two-player setting. To justify the highlight on the $N>2$ agents setting, the authors may need to re-examine and potentially clarify the formulation (e.g., in line 229) to better accommodate and demonstrate generalizability to $N$ agents.\n\n4. It would be better to demonstrate the necessity of the diffusion model by comparing its performance against other powerful neural network architectures (such as CNNs or RNNs/LSTMs) or computing the world model prediction precision.\n\n5. The \"Partner Conditioned World Model\" relies only on the predicted ego action sequences and the partner's identity. There is a concern regarding prediction precision due to the cumulative error from both sequential action prediction (via the Action Proposal Model) and trajectory prediction.\n\nIn general, the overall workflow of the method is too complex and could benefit from a clearer and straightforward motivation for each component."}, "questions": {"value": "Typos: There appears to be a missing closing parenthesis ')' in line 185.\n\nIn line 229, the text mentions \"a fixed partner policy $\\pi_{\\text{partner}}$.\" Please clarify which specific action policy plays the role of $\\pi_{\\text{partner}}$ in the overall framework?\n\nThe paper mentions computing a deterministic reward function $R(s_t, s_{t+1})$. Given that the states ($s_t, s_{t+1}$) are generated by a diffusion model (which is stochastic), please clarify if the reward function $R$ is itself a separate trainable model?\n\nThe workflow involving the Inverse Dynamics Planner is confusing. The model first generates full \\tau^* trajectories based on action proposals, and then the Inverse Dynamics Planner is used to \"convert the full \\tau^* trajectories into sequences of ego actions and execute it into our environment.\" If the trajectories are already generated conditioned on action proposals, why is a separate Inverse Dynamics Planner necessary to re-convert the trajectory back into actions? This step makes the overall workflow seem overly complex and lacks clear motivation. A clearer explanation of its role is needed.\n\nThe word size in the figures is too small, making them difficult to read. This should be corrected for readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v9H6FXeBoc", "forum": "Y47Ab11TjP", "replyto": "Y47Ab11TjP", "signatures": ["ICLR.cc/2026/Conference/Submission21052/Reviewer_nVJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21052/Reviewer_nVJz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921789059, "cdate": 1761921789059, "tmdate": 1762961529718, "mdate": 1762961529718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DIZCO combines a partner‑conditioned diffusion world model with a diffusion‑based action proposer. At test time it generates candidate actions, simulates their futures in the learned world model, and searches over the rollouts to select a plan, enabling adaptation to novel partners and outperforming SOTA baselines in Overcooked. It further demonstrates 2→N compositional generalization via composable CFG, and presents an asynchronous architecture for online interaction. The method and system are mutually reinforcing, and the experiments cover end‑to‑end performance plus key ablations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear factorization. The approach cleanly decouples partner/environment modeling from planning/search, aligning with the test‑time compute paradigm for reasoning.\n\n2.End‑to‑end gains. On challenging layouts with a Human Proxy, DIZCO significantly outperforms baselines; the “action‑proposer only” variant lags, underscoring the central role of world‑model + search.\n\n3.Compositional generalization. Using CFG to add conditions enables zero‑shot planning for N>2 agents—preliminary but directionally novel."}, "weaknesses": {"value": "1.Benchmark scope. Overcooked is relatively constrained for human‑AI collaboration. Stronger evidence on richer variants (e.g., Overcooked‑V2) or additional coordination suites is needed.\n\n2.Result prioritization. The placement is suboptimal: Sec. 6.1 is underpowered and fails to make the method’s effectiveness unmistakable, while several important results are pushed to the appendix.\n\n3.Exposition depth. Given the breadth (world models, diffusion policies, compositional guidance), both the main text and appendix are too terse, making it hard to reproduce or scrutinize design choices.\n\n4.Live‑collaboration claim under‑supported. The core claim is real‑time human–AI collaboration, yet quantitative evaluation relies on a Human Proxy; the live user study is qualitative only (no sample size, controls, or statistics), so the claim risks overstatement."}, "questions": {"value": "1.Human Proxy fidelity. What are the data composition, scale, and style diversity for the proxy? How closely does it match real human behavior, and what gaps are measured?\n\n2.Embodied collaboration. Can the method transfer to embodied robotics settings? If so, what changes are required for reward specification, action mapping, latency control, and safety[1]?\n\n3.Diversity of teammates. How do you quantify the diversity of generated partner behaviors or joint plans? Without sufficient diversity, how is compositional generalization justified[2]?\n\n4.Why it works (toy example). Provide a minimal, visual toy example that shows how test‑time search over world‑model rollouts corrects a suboptimal initial proposal.\n\n5.Limits of composable CFG. As N increases or interactions strengthen, does additive composition introduce conflicts or inconsistencies? Show failure cases or a breakdown curve vs. N.\n\n6.Why diffusion for the world model? What concrete advantages over autoregressive/latent‑trajectory alternatives under compute‑matched search budgets?\n\nRef:\n\n[1] Multi-agent embodied ai: Advances and future directions\n\n[2] Learning to Coordinate with Anyone"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ou1Qnnbc4O", "forum": "Y47Ab11TjP", "replyto": "Y47Ab11TjP", "signatures": ["ICLR.cc/2026/Conference/Submission21052/Reviewer_3vb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21052/Reviewer_3vb8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925453468, "cdate": 1761925453468, "tmdate": 1762940626663, "mdate": 1762940626663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiZCo, a diffusion-based framework for zero-shot coordination in human-AI cooperative tasks. The core idea is to leverage generative models for real-time, search-based planning. The authors propose a two-model architecture: an action proposer that generates candidate action sequences and a partner-conditioned world model that simulates future trajectories. By performing test-time search over these simulated rollouts, the agent can adapt to novel partners without relying on pre-trained policies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using diffusion-based world models for zero-shot coordination is novel. The compositional generalization of the world model to an arbitrary number of agents is a creative and technically sound contribution. And the real-time human interaction framework could be useful for the community."}, "weaknesses": {"value": "- **Technical Errors and Inconsistencies**:\n    - Several typos and formatting issues exist (e.g., line 64: missing Figure 1 reference; line 260: repeated \"on\"; line 283: repeated \"the\"; line 358: grammatical error).\n    - Equations (4) and (5) are incorrect with conditions missing and poorly explained, especially the sudden introduction of \"vehicle\" without context.\n    - The use of an inverse dynamics planner is not well-justified. If the action diffusion model outputs action sequences, why is an additional inverse planner needed?\n    - Missing references for tables and figures (e.g., line 431: missing table reference; line 367: missing figure reference).\n- **Experimental Evaluation**: The experiments are insufficiently comprehensive:\n    - Autonomous Driving only tests world model generalization, not full decision-making performance.\n    - Overcooked is only tested on two layouts, limiting the scope of evaluation.\n    - Only one human proxy model is used for evaluation, while previous works [1,2] use multiple models, raising concerns about generalizability.\n    - No code is provided, which hinders reproducibility and verification of results.\n- **Presentation**:\n    - Figures 4 and 5 are too small and illegible, making it difficult to interpret the ablation and sensitivity results.\n\nReferences\n\n[1] Chao Yu, et al. Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased. ICLR 2023.\n\n[2] Lihe Li, et al. LLM-Assisted Semantically Diverse Teammate Generation for Efficient Multi-agent Coordination. ICML 2025."}, "questions": {"value": "1. How is partner identity modeled and integrated into the world model? Is it learned or hand-engineered?\n2. How is diversity in action candidates ensured? Is there any explicit diversity-promoting mechanism, or does it rely solely on the stochasticity of the diffusion model?\n3. Why is an inverse dynamics planner necessary? If the action proposer outputs action sequences, why not use them directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vIXIjXbG0p", "forum": "Y47Ab11TjP", "replyto": "Y47Ab11TjP", "signatures": ["ICLR.cc/2026/Conference/Submission21052/Reviewer_rUkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21052/Reviewer_rUkP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965299244, "cdate": 1761965299244, "tmdate": 1762940626456, "mdate": 1762940626456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of human-AI coordination (e.g. overcooked) and adapting to unseen partners or more agents than were seen during training. The method consists of two diffusion models, one a world model and the other an action proposer, which together define partner-conditioned action plans to maximise joint reward. An asynchronous plan creation vs plan execution mechanism allows the computationally intensive plan creation to span multiple steps while actions are executed according to the latest available plan for low latency.\n\nThe paper presents an interesting use of generative models for human-AI coordination in decision-making tasks, as well as a nice asynchronous execution mechanism. Unfortunately, the framing and testing of the algorithm is fundamentally unfair, along with some other issues. Therefore, the paper does not convince me of the methodology and is not competitive as is."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is reasonably clear and the diagrams and graphs look good\n* The asynchronous plan creation/execution mechanism is interesting and could be applied to other RL methods, somewhat in line with how humans may take many steps to formulate a plan, while executing according to the latest known plan.\n* Compared to the considered baselines, the results are indeed favourable.\n* The authors present a decently large set of experiments, including two environments and several ablations."}, "weaknesses": {"value": "* The main issue is that the method unfairly uses privileged information, i.e. expert trajectories to train both the world model and action proposer. By contrast, all considered baselines use RL for the policy. GAMMA indeed uses expert trajectories, but only to model partner identities, which can be considered part of the environment. Its policy is trained without expert demonstrations. So it seems Dizco could just be reproducing high-performing expert trajectories rather than truly generalising. To be convinced of efficacy, I would need to see it outperform state-of-the-art imitation learning baselines and the framing of the method would need to be as an imitation learning algorithm.\n* The method is tested on simple 2D environments and already needs the asynchronous mechanism to account for slow test-time computation. I’m concerned that, were it to be extended to more complex domains (e.g. COMBO tested on ThreeDWorld), the computation would be prohibitively slow and therefore the action plan would be very stale.\n* Some typos, GAMMA not cited, some table references missing."}, "questions": {"value": "* I didn’t fully understand why the inverse dynamics model was needed? Don’t you already have access to the action sequence that generated the candidate state sequence (since the latter was generated based on the former by the world model)?\n* Isn’t this setting an example of a Dec-POMDP rather than a Markov game since the reward is shared?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KdCzOuK4rf", "forum": "Y47Ab11TjP", "replyto": "Y47Ab11TjP", "signatures": ["ICLR.cc/2026/Conference/Submission21052/Reviewer_5zGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21052/Reviewer_5zGh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762532823729, "cdate": 1762532823729, "tmdate": 1762940626147, "mdate": 1762940626147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}