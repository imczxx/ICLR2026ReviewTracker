{"id": "vjdteya8oa", "number": 12073, "cdate": 1758205515721, "mdate": 1759897535828, "content": {"title": "CARE: Contextual Affinity Exploration with Twin Concordance for Graph Out-Of-Distribution Detection", "abstract": "This paper studies the problem of graph out-of-distribution (OOD) detection, which aims to identify anomaly graphs out of a graph dataset. Prior efforts usually focus on the utilization of topological structures with unsupervised graph learning to foster typical pattern recognition, which overlooks the semantic structure preserved in contextually affine neighborhoods. Towards this end, we propose a novel approach named Contextual Affinity Exploration with Twin Concordance (CARE) for graph OOD detection. The core of CARE is to explore and exploit the contextual affinity of the graph data samples for discriminative graph representations. \nIn particular, our CARE first builds a contextual affinity graph to depict the semantic structure in the hidden space. More importantly, we introduce high-order affinity to enhance geometric understanding of the structure by utilizing a meta-graph neural network. To enhance representation discriminability with high robustness, we introduce twin concordance learning, which not only minimizes the difference of affinity distributions across different views, but also encourages the consistency between contextually affinitive neighbors. Finally, we introduce a compression strategy to expand the decision boundary for enhanced separation between in-distribution and out-of-distribution graphs. Extensive experimental results demonstrate the superiority of our CARE across ten real datasets in comparison to various baselines.", "tldr": "", "keywords": ["Graph Neural Networks", "Out-of-distribution Detection", "Concordance Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8b29147309dc1eac4a6056982f12378a4a01a8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focus on graph-level OOD detection and propose to leverage contextual affinity to better separate ID graphs from OOD ones. To achieve this, the paper extract high-order affinitive relationships between graphs and propose twin concordance learning to learn separable representations. Extensive experiments demonstrate the effectivenss of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivational example in the introduction is clear and makes sense.\n2. The paper well structured and the figures and tables are well ploted.\n3. The prosposed method is compared with comprehensive datasets and baselines."}, "weaknesses": {"value": "1. The construction of contextual affinity graph is problematic, it depends on GNN-generated graph representations, but I don't think GNN (random initialized)  can capture graph (semantic) affinity.\n2. I believe enough graphs should be included in the batch to capture global affinity, the paper did not evaluate the influnce of different batch size. What's more, the datasets are small, I doubt its performance on large datasets as a batch can only include a small fraction of graphs.\n3. The role of graph affinity in OOD dectection is not clear, making the core contribution of the paper limited."}, "questions": {"value": "1. which readout function did you use?\n2. I still don't get the effect of Perturbation-aware Concordance $L_{ins}$. As augmentations like node dropping will destroy semantics [1], this work is still leveraging topology for OOD detection.\n\n[1] SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection, CIKM'24."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rXhI8gWMI4", "forum": "vjdteya8oa", "replyto": "vjdteya8oa", "signatures": ["ICLR.cc/2026/Conference/Submission12073/Reviewer_7ZC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12073/Reviewer_7ZC5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620023184, "cdate": 1761620023184, "tmdate": 1762923045194, "mdate": 1762923045194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CARE, an unsupervised graph OOD detection framework that jointly models contextual affinity among graphs through reciprocal nearest-neighbor relations, enhances high-order structural semantics using a meta-GNN, enforces consistency via a twin concordance scheme, and applies decision-boundary compression to better separate ID and OOD data. Extensive experiments over 10 graph OOD benchmarks and 14 anomaly detection datasets demonstrate clear improvements over recent strong baselines. \n\nHowever, there still remain some concern about novelty and efficiency and causing my overall rating to 4."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper is easy to follow and well-structured.\n\n2. Strong empirical results which shows CARE outperforms most baselines on a wide range of datasets with both OOD and anomaly detection settings."}, "weaknesses": {"value": "1. Although CARE combines components in a new way, many ideas resemble prior lines. For example, affinity graph has been widely used in existing Anomaly [1]  /OOD detection [2] methods.\n\n[1] A Data-centric Framework to Endow Graph Neural Networks with Out-Of-Distribution Detection Ability\n\n[2] Good-d: On unsupervised graph out-of-distribution detection. \n\n2. CARE relies on per-batch affinity graph construction and meta-graph propagation, and I have concern about these will introduce non-trivial computational overhead scaling with batch size.\n\n3. The author can include case studies showing semantic affinity improves detection beyond structural similarity."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v49tKVCmE9", "forum": "vjdteya8oa", "replyto": "vjdteya8oa", "signatures": ["ICLR.cc/2026/Conference/Submission12073/Reviewer_FYCP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12073/Reviewer_FYCP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726135346, "cdate": 1761726135346, "tmdate": 1762923044247, "mdate": 1762923044247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CARE, a framework for graph OOD detection that introduces contextual affinity to model relationships among graphs in a batch. It builds a contextual affinity graph, performs twin concordance learning (instance- and context-level consistency), and adds a decision-boundary compression loss. Experiments show modest improvements (1–2 % AUC) over prior methods such as HGOE and GOOD-D."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Experiments cover several OOD benchmarks and include ablation studies.\n\n* The framework is easy to follow.\n\n* The method seems stable across datasets."}, "weaknesses": {"value": "* The paper argues that prior OOD detection methods neglect contextual relations among graphs. This claim is poorly justified, since many subgraph-based or message-passing GNN approaches inherently model structural and semantic context. The motivation appears to overstate the novelty by framing a non-existent research gap.\n\n* The paper claims that the contextual-aware concordance module uses a neural network–parameterized soft clustering function that is optimized jointly with the model to enforce semantic consistency among contextually related graphs. However, the released code shows that clustering is implemented via offline FAISS K-means on frozen embeddings, producing hard assignments (im2cluster) and static centroids. This is a standard pseudo-labeling step (as in DeepCluster-style pipelines), not a learnable differentiable module. Therefore, the key claim of end-to-end concordance learning is not substantiated by the implementation.\n\n* Once the offline clustering step is recognized, the proposed approach reduces to a combination of well-known components: standard contrastive consistency between augmented views and an auxiliary cluster-based regularization. The “contextual affinity” concept largely renames batch-level kNN similarity used in existing contrastive GNN methods. The overall framework is incremental and lacks a clear theoretical or algorithmic advancement.\n\n* Reported AUC gains (~1–2%) over prior work are within the expected variance of such benchmarks. No statistical tests or sensitivity analyses compared with other baselines are presented to confirm robustness or significance. Given the limited conceptual contribution, the empirical section does not convincingly demonstrate the necessity of the proposed components.\n\n* The method is presented as a black-box aggregation of multiple losses, without explaining why contextual affinity or boundary compression improves OOD discrimination from a distributional or geometric standpoint. This makes it difficult to assess the mechanism or generalizability of the approach."}, "questions": {"value": "* The released code relies on a module named HCL as the base model, yet this component is not mentioned, described, or cited anywhere in the paper. Moreover, the corresponding model.py file is missing, preventing reproducibility. Could the authors clarify the origin of this HCL model, its role in CARE, and why it was omitted from the manuscript?\n\n* The implementation uses an offline FAISS K-means step for clustering, which contradicts the paper’s description of a “neural-network-parameterized soft clustering” optimized jointly with the main model. Why is the code different from the stated method, and which version (paper or code) reflects the actual results reported in Table 1?\n\n* In the anomaly detection and ablation sections, the paper presents results without comparing against existing baselines (e.g., GOOD-D, HGOE) beyond the main OOD detection table. Could the authors explain why such comparisons were omitted and whether the proposed modules offer consistent advantages across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ggr7MJPBwl", "forum": "vjdteya8oa", "replyto": "vjdteya8oa", "signatures": ["ICLR.cc/2026/Conference/Submission12073/Reviewer_swdP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12073/Reviewer_swdP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728162762, "cdate": 1761728162762, "tmdate": 1762923043886, "mdate": 1762923043886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for graph OOD detection, termed CARE. The core idea is to leverage the contextual affinity of the samples for discriminative graph representations. Specifically, CARE first constructs a contextual affinity graph based on sample relationships within the hidden space. It then employs a meta-graph neural network to capture high-order affinity. Subsequently, a twin concordance learning is used to foster a robust ID boundary. Finally, a novel decision boundary compression strategy is proposed for better ID/OOD separation. Extensive experiments on ten real-world dataset pairs demonstrate the superior performance over SOTA baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel idea: the proposed method opens a fresh contextual perspective for OOD detection. The core idea can be applied to various related tasks.\n2. Technically sound: the motivation behind CARE is well supported by the theoretical analysis.\n3. Comprehensive experimental evaluation: the proposed method is validated by extensive experiments, including comparison experiments on OOD and anomaly detection, an ablation study, and a sensitivity analysis.\n4. Clear writing: the paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. The evaluation mainly emphasizes accuracy improvements. Additional metrics, such as runtime efficiency or memory consumption, are not extensively analyzed, which may impede a complete understanding of practical deployment constraints.\n2. An explanation of the intuition behind the assumption in Theorem C.1 should be provided for a more solid discussion.\n3. The paper states that one of four augmentation strategies is randomly employed. It's unclear if all augmentations are equally effective or if the choice of augmentation strategy significantly impacts the results.\n4. Typo: The result of formula 8 is a similarity score, which is a scalar. However, it is defined as a distribution in the context."}, "questions": {"value": "1. What is the computational cost of the proposed method? Comparison experiments in terms of running time against the most recent approaches should be provided to clearly show the trade-off.\n2. The model's performance is likely sensitive to the choice and degree of data perturbation. The authors should include a sensitivity analysis on the perturbation rate to demonstrate robustness.\n3. Could the authors provide an explanation of the intuition behind the assumption in Theorem C.1?\n4. The CARE framework is composed of three modules: contextual affinity graph construction, twin concordance learning, and decision boundary compression. Which of these modules is the most critical for the model's overall performance gains?\n5. Did the authors experiment with the impact of different augmentation strategies (e.g., using only Node Dropping vs. only Subgraph)? Is the performance robust to this choice, or is the random combination critical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5z8AZx6qSh", "forum": "vjdteya8oa", "replyto": "vjdteya8oa", "signatures": ["ICLR.cc/2026/Conference/Submission12073/Reviewer_DQCU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12073/Reviewer_DQCU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843783935, "cdate": 1761843783935, "tmdate": 1762923043401, "mdate": 1762923043401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}