{"id": "LnYzMi3s0s", "number": 20325, "cdate": 1758304756041, "mdate": 1759896983738, "content": {"title": "Understanding Sampler Stochasticity in Training Diffusion Models for RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) improves pretrained generative models, and its sampling design is important for training reliable, high-quality models. In practice, stochastic SDE samplers promote exploration during training, while deterministic ODE samplers enable fast, stable inference; this creates a discrepancy in sampling stochasticity that induces a preference-reward gap. In this paper, we establish a non-vacuous bound on this gap for general diffusion models and a sharper bound for Variance Exploding (VE) and Variance Preserving (VP) models with (mixture) Gaussian data. Methodologically, we leverage the stochastic gDDIM scheme to attain arbitrarily high stochasticity while preserving data marginals, and we evaluate, under multiple preference rewards, the performance of RL algorithms (e.g., log-likelihood and group-relative policy variants). Our numerical experiments validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.", "tldr": "High stochasticity in Diffusion RLHF is also beneficial to ODE sampling", "keywords": ["Generative Diffusion Models", "Fine-tuning", "Reinforcement Learning from Human Feedback", "Deterministic Sampling", "Human Preference Alignment"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/081b48c993a8e0cc2a75d4d307da11b7de663f1a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the mismatch between stochastic and deterministic samplers during inference when finetuning diffusion models with RLHF. To that end, the authors define the improvement of finetuning as the expected reward difference between finetuned determinisitic ODE sampler and the pretrained model. Moreover, they define the reward gap as the difference between the expected reward obtained by finetuned ODE and SDE samplers.\nThe authors conduct theoretical analyses for Variance Exploding (VE) and Variance Preserving (VP) processes in a tractable setting for Gaussian and GMM target densities.\nThe empirical study focuses on two T2I settings: First, finetuning of Stable Diffusion 1.5 with DDPO, and second, FLUX1 with mixGRPO, where the authors present several observations.\n\nAs a disclaimer, I cannot make any statement about the significance of the theoretical results presented in this paper."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper sheds light on a timely and important question, i.e., how deterministic inference and stochastic finetuning are connected. The authors conduct both a theoretical analysis in a tractable setting as well as an empirical study."}, "weaknesses": {"value": "The paper and results are hard to follow for someone who is not very familiar with RLHF for T2I (like myself). I think the authors should explain the experimental setup better (see Questions for some things that may need clarification).\n\nFor the remaining weaknesses, please see Questions."}, "questions": {"value": "- Are the results for the SDE samplers in Table 1 produced with the same $\\eta$ used for training? \n- What happens if we train with $\\eta < 1$? \n- I'm a bit confused about Table 2: If I understand it correctly, $T$ is the time horizon of the dynamical system. However, in Table 2 it is a discrete quantity called 'steps'. I assume the authors refer to the number of diffusion steps as $T$ (?). Does it mean you use a fixed number of diffusion steps during training? \n- Regarding the observation: 'High Stochasticity Benefits Moderate Time Steps' -> Does this also hold for smaller or larger time steps?\n- What is meant by $T=0$?\n- Regarding Figure 3: How can the authors conclude 'indicating that image quality improves for both samplers'? From my understanding the reward gap does not give any insights on image quality but only on the relative performance between SDE and ODE sampling. Could the authors please clarify. \n- 'This is also consistent with our empirical observation that the performance of T2I generation deteriorates when fine-tuning with very large $\\eta$' -> Would be good if the authors could reference the results that support this claim\n- Regarding Theorem 3.2: Why did the authors not consider general shifted reward functions, i.e. $r(x) = -(x - \\mu)^2$? This would help to understand how the shift of the reward function affects the theoretical result.\n- Both Theorem 3.2 and Corollary 3.1 are given for very specific reward function. How transferable are these results?\n\nSome minor comments:\n- The quality of the figures 3 and 4 could be improved (minor).\n- Line 282: The authors refer to Appendix xxx\n- Line 375: contray"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7ws4ZShpJa", "forum": "LnYzMi3s0s", "replyto": "LnYzMi3s0s", "signatures": ["ICLR.cc/2026/Conference/Submission20325/Reviewer_8xJ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20325/Reviewer_8xJ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500840771, "cdate": 1761500840771, "tmdate": 1762933785597, "mdate": 1762933785597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the reinforcement learning from human feedback (RLHF) for text-to-image models. More specifically, examining the impact of feedback being given on stochastic draws from diffusion SDEs, whereas inference uses the corresponding deterministic ODEs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem formulation is somewhat original, and certainly of commercial relevance given the massive interest in text-to-image models.\n- The writing and mathematical presentation is mostly clear.\n- The introduction, background and related work sections are comprehensive and easy to follow."}, "weaknesses": {"value": "**Tentatively incremental contributions**\n\nI'll willingly admit I'm not an expert on RLHF and the literature which this paper builds upon. However, my impression is that the main results are direct consequences of other works - putting the novelty into question. However, I'm open to be convinced otherwise.\n\n**Relevance of the theoretical results unclear**\n\nFirst, I find it difficult to interpret the generality of the results in sections 3.1-3.3. Are they primarily pedagogical examples since they happen to be rare examples of where things can be computed in closed form, or are they of practical interest in themselves? \n\nSecond, the general result in section 4 relies on dissipativity of f (Assumption 4.1.1). Why would that hold? (I believe conditions 2 can be fulfilled, especially if you use e.g. ReLU activations.)\n\n**Minor things:**\n- l.90-96 essentially repeat the preceding paragraph. I suggest to merge them.\n- l.107-119 could be omitted, in my opinion.\n- Most figures are too small, in particular the text within them. \n- l.195 \"th prompt\"\n- l.200 \"isotopic Gaussian\"\n- l.200 sure you can use Monte Carlo sampling, but won't the variance be huge?\n- l.207 \"std\" and l.211 \"clip\", please use proper mathematical notation.\n- l.226 in eq. (8), how do you evaluate KL - I assume you don't have access to the densities?\n- l.232 and elsewhere, please use \\text{} for descriptive subscripts like REF, SDE, ODE and italics for variables like $t$.\n- l.313 I suppose $W_2$ means Wasserstein-2 distance?\n- Table 1: you're most likely using too many significant digits. (Based on your plots and how noisy RL typically is)\n- Figure 3: given the amount of noise, I think only the left-most figure would allow you to draw any conclusions."}, "questions": {"value": "- As stated above, I see the practical relevance of the problem, but the *scientific* relevance is more questionable. Isn't the question of stochastic vs. deterministic more broadly applicable to, say, distributional reinforcement learning? I believe such a formulation would have a greater and more lasting impact.  \n- Please motivate the relevance and validity of the theoretical results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Bf2HT3oORn", "forum": "LnYzMi3s0s", "replyto": "LnYzMi3s0s", "signatures": ["ICLR.cc/2026/Conference/Submission20325/Reviewer_uqq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20325/Reviewer_uqq6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836195103, "cdate": 1761836195103, "tmdate": 1762933785043, "mdate": 1762933785043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the gap between SDE samplers and ODE samplers in RLHF fine-tuning of diffusion models. Theoretical bounds are derived and numerical experiments are conducted."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. A theoretical bound is derived for SDE vs ODE samplers in fine-tuning of diffusion models.\n2. Comprehensive numerical experiments are conducted to support the claims."}, "weaknesses": {"value": "I have several major concerns:\n1. I think the gap between SDE and ODE samplers is confusing to me. As they are equivalent, why is there a gap after fine-tuning. One should expect the same distribution of $ Y_T $ regardless which sampler is used. The authors should elaborate this point very carefully.\n2. Theorem 4.1 is not that interesting as it looks. First, the proof seems to be incorrect. One inequality sign seems to be flipped when applying Young's inequality. Second, Assumption 4.1 (1) does not hold for VP SDE as shown in [1] unless you modify the OU process. Hence, I strongly suspect the bound in Theorem 4.1 is not informative.\n3. The definition of $ Y_t^\\theta $ is not clear. In particular, the authors should explain what is parameterized. Usually, fine-tuning starts with a given reference model, e.g., a pre-trained score network. In this case, choosing $ \\theta $ as the optimal parameter associated with the optimal distribution is not correct; see [2]. This is because one has to find the optimal score network following the sampling dynamics instead of solving an unconstrained entropy-regularized optimization problem. Therefore, I do not think the results in Sections 3.1 and 3.2 are sound. \n\n[1] Tang, Wenpin, and Hanyang Zhao. \"Contractive diffusion probabilistic models.\" arXiv preprint arXiv:2401.13115 (2024). \\\n[2] Han, Yinbin, Meisam Razaviyayn, and Renyuan Xu. \"Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "1. I suggest shortening the review in page 3 - 4\n2. Line 282, \"Appendix xxx\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ALyxmZ7z0Z", "forum": "LnYzMi3s0s", "replyto": "LnYzMi3s0s", "signatures": ["ICLR.cc/2026/Conference/Submission20325/Reviewer_Edf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20325/Reviewer_Edf2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241693638, "cdate": 1762241693638, "tmdate": 1762933784615, "mdate": 1762933784615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the finetuning of diffusion models using RLHF. It addresses the \"reward gap\" between stochastic SDE samplers used in training and deterministic ODE samplers used at inference. The paper aims to characterize this gap theoretically and validate it empirically."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper's strengths are its theoretical contributions:\n1. Formalizing the theory of the SDE-ODE reward gap in the continuous-time limit.\n2. Developing sharp bounds for this gap for VE and VP models with Gaussian and Gaussian Mixture targets and providing a general bound for arbitrary distributions."}, "weaknesses": {"value": "Despite its theoretical novelty, the paper suffers from fundamental weaknesses:\n1. The paper's central flaw is that its theory is practically limited. The bounds apply only to continuous-time processes, ignoring the discretization error from using $N$-step samplers, which is the dominant error in practice. The authors even cite (Liang et al., 2025), which provides the tools for this analysis, but they fail to apply this to their own work (e.g., their Section 3.3), making the theoretical contribution less significant.\n2. The experiments are insufficient. Using Stable Diffusion 1.5  as a primary testbed is not enough. While the paper uses FLUX.1, it compares it against SD 1.5 instead of the relevant SOTA benchmark, SDXL. This omission makes the empirical conclusions scientifically unsound.\n3. The paper quality is far below ICLR standards. It contains uncorrected placeholders (e.g., \"Appendix xxx\" on line 288 ), not introduced bullet points in Section 1, and uninterpretable figures (e.g., Figures 3 and 4) that are low-resolution screenshots from wandb without even the legends."}, "questions": {"value": "In addition to weaknesses, I have the following questions:\n1. Can the authors provide a bound on the discretization error for their gDDIM sampler? How can we be sure this un-analyzed error does not dominate the stochasticity gap you have bounded?\n2. You cite Liang et al. (2025). Why did you not apply this framework to your Gaussian Mixture analysis to provide a practical, end-to-end bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jiOx85APTI", "forum": "LnYzMi3s0s", "replyto": "LnYzMi3s0s", "signatures": ["ICLR.cc/2026/Conference/Submission20325/Reviewer_JNkV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20325/Reviewer_JNkV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762290452658, "cdate": 1762290452658, "tmdate": 1762933784028, "mdate": 1762933784028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}