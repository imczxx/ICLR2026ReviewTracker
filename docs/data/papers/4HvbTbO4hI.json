{"id": "4HvbTbO4hI", "number": 2688, "cdate": 1757206021198, "mdate": 1759898133299, "content": {"title": "A Unifying View of Vector, Product and Scalar Quantization: An Information-Theoretic Perspective", "abstract": "Discrete visual tokenization, predominantly driven by vector, scalar, and product quantization, lacks a unifying conceptual framework that elucidates the impact and tradeoffs of different quantization optimization objectives. In this paper, we propose a unified information-theoretic framework to shed light on these considerations. To do so, we view quantization as information compression and define the information loss (quantization error), compression ratio, and input/output as information-theoretic quantities. Using this framework, we resolve three central open questions: First, we theoretically prove and empirically demonstrate that minimizing quantization error, rather than maximizing codebook utilization, is the paramount optimization objective for ensuring training stability and reconstruction fidelity. Second, we establish two critical fairness conditions for intrinsic algorithm comparison: controlling the latent feature distribution variance and ensuring identical compression ratios. Third, we demonstrate, both theoretically and empirically, that under these conditions, modern vector quantization outperforms scalar and product quantization at minimizing quantization error. Our work provides a foundational reframing of quantization algorithms, resolving conceptual ambiguities and providing the first artifact-free comparison that establishes quantization error minimization as the core optimization criterion.", "tldr": "We present a unified view of VQ, PQ, and SQ by taking an information theoretic perspective.", "keywords": ["Unified View", "Vector Quantization", "Product Quantization", "Scalar Quantization", "Information-Theory"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afc98dc0a30d467272922f15d7999537bbaa577a.pdf", "supplementary_material": "/attachment/b80d7aea1360fa99b8fadc02c263d5d846c450d0.zip"}, "replies": [{"content": {"summary": {"value": "For discrete autoencoders, this paper demonstrates  that  that minimizing quantization error (not maximizing codebook utilization) is key for stability and fidelity, establishing fairness conditions for algorithm comparison, and demonstrating vector quantization's superiority in minimizing error under these conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, with clear and accessible content. I think its primary contribution lies in Proposition 1, which points out that minimizing quantization error can maximize codebook utilization (a mainstream approach currently proposed based on intuition and experiments), while the latter cannot guarantee the former.  The paper should develop its analysis and experiments on this aspect. Unfortunately, the authors shifts its focus to theoretical analyses of three quantization methods, which clearly lack innovation as detailed later."}, "weaknesses": {"value": "1）The title of the paper is overly grandiose and inconsistent with its research content. The paper primarily investigates the impact of three quantization methods for discrete autoencoders. However, the title gives the impression that the paper presents a brand new theoretical analysis method for quantization.\n\n2）The quantization analysis  based on minimizing error (presented in Section 4.4) is a classic information-theoretic approach, and the paper's method and results lack  innovation. Drawn from these results, the conclusion that VQ outperforms the other two methods,  lacks rigor and is very likely to be incorrect for two reasons: first, quantization error is closely related to the actual distribution of the data, which the paper does not study; second, the theoretical bounds provided in the paper are not tight, making performance comparisons based on them unreasonable."}, "questions": {"value": "My  major concerns are given above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GycjR7f8HA", "forum": "4HvbTbO4hI", "replyto": "4HvbTbO4hI", "signatures": ["ICLR.cc/2026/Conference/Submission2688/Reviewer_yku1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2688/Reviewer_yku1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624700332, "cdate": 1761624700332, "tmdate": 1762916331682, "mdate": 1762916331682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified information-theoretic framework for analyzing and comparing Vector Quantization (VQ), Product Quantization (PQ), and Scalar Quantization (SQ). By viewing quantization as an information compression process, it defines key quantities such as information loss (quantization error) and compression ratio, and derives theoretical and empirical conclusions regarding their relationships. The authors demonstrate that minimizing quantization error should be the primary optimization objective. They further establish fairness conditions for comparing different quantization schemes and validate their framework through controlled experiments using the VQ-Transplant model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The unified benchmark and consistent architectural setup for comparing VQ, SQ, and PQ is a practical contribution that clarifies prior inconsistencies.\n2. The conclusion that minimizing quantization error (rather than maximizing codebook utilization) is more critical for reconstruction and generation quality is insightful."}, "weaknesses": {"value": "Some definitions and experimental details are insufficiently explained (see Questions)."}, "questions": {"value": "1. Definition 3 quantifies compression ratio Qr as the ratio between the input and output information quantities, derived from spatial and codebook dimensions. However, from an information-theoretic perspective—as the authors themselves claim—the compression ratio should ideally be defined in terms of entropy rather than quantity counts. Two signals with the same spatial dimensions can differ substantially in entropy. Could the authors justify whether their definition of Qr is reasonable, or discuss an entropy-based measure?\n2. Figure 2 shows a linear relationship between quantization error and latent distribution variance. Which quantization scheme (VQ, PQ, or SQ) is used for this analysis? Does this linearity hold consistently across different quantization methods?\n3. The experiments employ a pre-trained VQ-oriented model, with subsequent substitution of its quantization module for PQ and SQ. Could this adaptation lead to suboptimal performance for the latter methods due to the encoder-decoder’s alignment with the VQ latent space? Have the authors considered retraining the encoder to avoid this potential bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKAFocArTu", "forum": "4HvbTbO4hI", "replyto": "4HvbTbO4hI", "signatures": ["ICLR.cc/2026/Conference/Submission2688/Reviewer_3HhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2688/Reviewer_3HhS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795969423, "cdate": 1761795969423, "tmdate": 1762916331513, "mdate": 1762916331513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unifying information-theoretic framework to analyze and compare various quantization methods (VQ, PQ, SQ) used in discrete visual tokenization. The core contributions are threefold: 1) It theoretically and empirically argues that minimizing quantization error (information loss) is a more critical optimization objective than maximizing codebook utilization. 2) It establishes two essential \"fairness conditions\" for comparing quantization algorithms: identical latent distributions and constant compression ratios. 3) Under these fair conditions, it demonstrates the intrinsic superiority of modern VQ methods over PQ and SQ. The claims are supported by well-designed experiments using a \"VQ-Transplant\" framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed information-theoretic framework provides a principled and clear perspective to understand the fundamental trade-offs in quantization, demystifying the relationship between quantization error, codebook utilization, and reconstruction quality.\n2.The introduction of two \"fairness conditions\" is a significant contribution. It addresses a critical gap in prior work where comparisons were often confounded by architectural or training differences. This sets a higher standard for future research in this area.\n3.The \"VQ-Transplant\" experimental design is clever, effectively isolating the performance of the quantization module itself. The strong correlation found between quantization error and reconstruction fidelity (r-FID) provides convincing evidence for the paper's main claim."}, "weaknesses": {"value": "The paper frames its analysis in information theory, defining quantities based on bit counts and using squared Euclidean distance as \"information loss.\" While Proposition 1 insightfully connects codebook utilization to conditional entropy, the main empirical metric remains MSE. The paper could better articulate the connection between minimizing the theoretical information loss (e.g., H(X|Z)) and the practical objective of minimizing MSE. Is the framework a fundamental new theoretical lens, or primarily a useful reframing of established concepts with information-theoretic terminology?"}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xMCntxaCpB", "forum": "4HvbTbO4hI", "replyto": "4HvbTbO4hI", "signatures": ["ICLR.cc/2026/Conference/Submission2688/Reviewer_SKn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2688/Reviewer_SKn5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915742040, "cdate": 1761915742040, "tmdate": 1762916331288, "mdate": 1762916331288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified information-theoretic framework for analyzing vector, product, and scalar quantization (VQ, PQ, SQ). By viewing quantization as an information compression process, the authors formally define quantities such as information loss, compression ratio, and information capacity. The paper proves that minimizing quantization error (information loss) is a more fundamental optimization objective than maximizing codebook utilization, and introduces two fairness conditions for comparing quantization algorithms: (1) identical latent feature distributions and (2) identical compression ratios. Under these conditions, both theoretical and empirical analyses demonstrate that VQ outperforms PQ and SQ in minimizing information loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed information-theoretic formulation provides a rigorous, unifying perspective on quantization algorithms.\n\nThe authors formally prove that minimizing quantization error implies full codebook utilization (but not vice versa), and derive scaling laws for optimal VQ/PQ/SQ errors.\n\nThe two fairness conditions (controlled latent distributions and compression ratios) are well-motivated and improve reproducibility and validity of empirical evaluations."}, "weaknesses": {"value": "While the framework is elegant, it mostly systematizes existing methods rather than introducing fundamentally new algorithms.\n\nSome notations (e.g., Q_i,Q_o,Q_r) and assumptions could be clarified for broader readability.\n\nLimited discussion on downstream effects or interpretability benefits for generative models\n\nThe paper is not well-organized and difficult to follow, even for readers familiar with quantization and visual representation learning. The presentation contains an excessive number of equations and derivations, while the key insights and conclusions are often buried or insufficiently highlighted."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tHjo51veIy", "forum": "4HvbTbO4hI", "replyto": "4HvbTbO4hI", "signatures": ["ICLR.cc/2026/Conference/Submission2688/Reviewer_ExUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2688/Reviewer_ExUQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058077925, "cdate": 1762058077925, "tmdate": 1762916331141, "mdate": 1762916331141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}