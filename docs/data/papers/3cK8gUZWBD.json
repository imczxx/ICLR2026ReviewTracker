{"id": "3cK8gUZWBD", "number": 20568, "cdate": 1758307569599, "mdate": 1763751125272, "content": {"title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization", "abstract": "The increasing adoption of LLM agents with access to numerous tools and sensitive data significantly widens the attack surface for indirect prompt injections. Due to the context-dependent nature of attacks, however, current defenses are often ill-calibrated as they cannot reliably differentiate malicious and benign instructions, leading to high false positive rates that prevent their real-world adoption. To address this, we present a novel approach inspired by the fundamental principle of computer security: data should not contain executable instructions. Instead of sample-level classification, we propose a token-level sanitization process, which surgically removes any instructions directed at AI systems from tool outputs, capturing malicious instructions as a byproduct. In contrast to existing safety classifiers, this approach is non-blocking, does not require calibration, and is agnostic to the context of tool outputs. Further, we can train such token-level predictors with readily available instruction-tuning data only, and don’t have to rely on unrealistic prompt injection examples from challenges or of other synthetic origin. In our experiments, we find that this approach generalizes well across a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB and SEP, achieving a 7–10× reduction of attack success rate (ASR) (34% to 3% on Agent Dojo), without impairing agent utility in both benign and malicious settings.", "tldr": "novel and practical defense for indirect prompt injection using precise tool output sanitization", "keywords": ["Indirect Prompt Injection Defense", "AI Agent Security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab0550719efeb7d2532f1068489e0328abca5b7d.pdf", "supplementary_material": "/attachment/8259d6c623345c71d8b14e8a11fd0fdbb0f993ca.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a token-level sanitization process that surgically removes instructions directed at AI systems from tool outputs to defend against indirect prompt injection attacks. The authors evaluate their sanitization method across various datasets to validate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The evaluation covers multiple datasets, providing some breadth to the experimental validation."}, "weaknesses": {"value": "1. Unrealistic assumption about instruction-free data: The core assumption that tool outputs should not contain any instructions is flawed and impractical. Consider a realistic scenario where a user asks, \"Check my to-do list and help me solve the tasks on it.\" In this case, the retrieved data will inevitably contain instructions (i.e., the to-do tasks themselves), which the agent must process to complete the user's request. The proposed sanitization would incorrectly remove legitimate task information, breaking normal functionality. The authors need to address how their method distinguishes between malicious injected instructions and legitimate instructions within tool outputs.\n\n2. Limited novelty: The idea of using LLM-based sanitization to filter prompts has already been explored in prior work. For instance, Shi et al. [1] proposed PromptArmor, which achieves similar goals through prompt-based defenses without requiring any additional training. They showed that directly applying SOTA LLMs can achieve 0 ASR with high utility. \n\n3. Insufficient comparison with state-of-the-art defenses: The paper evaluates against too few baseline defense methods. More critically, recent defenses have already achieved nearly 0% ASR on Agentdojo The authors should include comprehensive comparisons with state-of-the-art defenses, including PromptArmor [1] and other recent methods, and try to explain why their approach is needed if existing methods already achieve near-perfect defense.\n\n\n[1] Shi, Tianneng, et al. \"Promptarmor: Simple yet effective prompt injection defenses.\" arXiv preprint arXiv:2507.15219 (2025)."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RWvwjqd2b8", "forum": "3cK8gUZWBD", "replyto": "3cK8gUZWBD", "signatures": ["ICLR.cc/2026/Conference/Submission20568/Reviewer_sJGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20568/Reviewer_sJGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717731391, "cdate": 1761717731391, "tmdate": 1762933981429, "mdate": 1762933981429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "$\\newcommand{Ro}{\\textcolor{green}{ETDd}}$ $\\newcommand{Rt}{\\textcolor{blue}{F5hk}}$ $\\newcommand{Rtr}{\\textcolor{purple}{AvCC}}$ $\\newcommand{Rf}{\\textcolor{orange}{sJGo}}$\n\nWe thank all Reviewers for their replies, and are delighted that they find our work novel ($\\Ro, \\Rt$), important ($\\Rt$, $\\Rtr$), well-written ($\\Rt, \\Rf$), and thoroughly evaluated ($\\Ro, \\Rt, \\Rf$). In this reply we attempt to respond to points raised by multiple reviewers and also reply to them individually below.\n\nWe briefly want to reiterate the basic objective of CommandSans here. Instead of flagging “malicious instructions” in tool outputs – a hard, low resource and ill-defined classification objective relied on by prior works like PromptArmor (mentioned by $\\Rf$) – CommandSans takes a different angle and reframes the sanitization task: It focuses on detecting instructions directed at AI agents in tool outputs, without differentiating malicious-ness (cf. original revision, L230–236 and abstract). While this over-approximation may appear restrictive, our experiments show that it in fact does not significantly harm agent utility across benchmarks. The reason for this, is that AI instructions very rarely legitimately occur in tool output, and even if they do, CommandSans’s non-blocking nature will often not impair agent utility, since LLMs are more than capable of recovery, even in the presence of partially masked input.\n\nApart from empirical validation, this design also aligns with fundamental computer security principles of separating instructions from data: Tool outputs are external and untrusted data sources, making them a potential vector for prompt injection (instructions). As such, tool outputs should never provide executable instruction to the AI. \n\n**Changelog**  \nWe have updated the manuscript to reflect the changes (temporarily marked in blue) and new experiments suggested by the reviewers. Concretely, we \n* included A.2 to showcase second-order prompt injections,\n* extended A.4 to include further training and data augmentation details ,\n* added Figure 5 (in A.5) that shows the token-level Precision/Recall graphs on AgentDojo,\n* added A.11, where we evaluate CommandSans adapted to code settings,\n* added A.12 which showcases 7 additional baselines and confidence intervals for all results reported in the body of the paper,\n* A.13 showing an example for false positives in an AgentDojo trace,\n* includee several minor fixes to the representation."}}, "id": "tuEAwMMlE9", "forum": "3cK8gUZWBD", "replyto": "3cK8gUZWBD", "signatures": ["ICLR.cc/2026/Conference/Submission20568/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20568/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20568/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763751220896, "cdate": 1763751220896, "tmdate": 1763751220896, "mdate": 1763751220896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to defend against prompt injection attacks in LLM agents as a byproduct of detecting 'malicious instruction tokens'. They trained a RoBERTa-based model for masking 'malicious instruction tokens' as a binary classification problem. Then they remove the detected masked tokens since they are treated as an injection so that the agents can continue their original normal behavior. Based on this result, they claim their defense is stronger than two previous baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to understand \n\n2. The task is important, and the authors are trying to tackle a critical task."}, "weaknesses": {"value": "1. The idea is not new and not solid. Training a binary classifier for tokens based on RoBERTa does not sound like a promising idea to defend prompt injections at the era of 2025. The boundary between 'normal' and 'malicious' tokens is hard to classify on the token-level and the authors did not provide enough evidence to support this.\n\n2. The baselines are weak and not enough. Only two baselines are used to prove the superiority of the authors' method."}, "questions": {"value": "1. Why train a RoBERTa for token classification? How about simple prompting? what's the robustness of this classifier? eg, false negative"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aLIc7LkYKy", "forum": "3cK8gUZWBD", "replyto": "3cK8gUZWBD", "signatures": ["ICLR.cc/2026/Conference/Submission20568/Reviewer_AvCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20568/Reviewer_AvCC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959250170, "cdate": 1761959250170, "tmdate": 1762933980933, "mdate": 1762933980933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CommandSans introduces a novel token-level sanitization approach for defending against indirect prompt injection attacks on AI agents. Rather than binary sample-level detection (which causes high false positives and blocks agents entirely), CommandSans surgically removes AI-directed instructions from tool outputs at the token level. The method is inspired by the security principle that data should not contain executable instructions. Using a BERT-based classifier trained on instruction-tuning data with LLM-based labeling, CommandSans achieves 7-19× reduction in attack success rate across benchmarks (AgentDojo, BIPIA, ASB, InjecAgent, SEP) while maintaining agent utility. The non-blocking nature and lack of calibration requirements make it practical for deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel framing of the problem as token-level instruction sanitization rather than sample-level detection, which is theoretically well-motivated by security principles and practically effective.\n- Comprehensive evaluation with consistent strong performance, including human red-teaming study showing robustness against expert attackers.\n- Acknolodges limitations and even performs a human red team search against CommandSans\n- Strong performance for utility and safety"}, "weaknesses": {"value": "- The conclusion mentions \"bridging gap between research and deployment\" but more discussion of actual deployment considerations (latency, costs, failure modes) would help.\n- Figures are a bit messy/text heavy and hard to follow, captions could be more descriptive.\n- Given the experiments it is hard to tell how generalizable this method can be and if it can scale well to more data/larger models.\n- Could benefit from more discussion on the cost to use CommandSans and to train CommandSans"}, "questions": {"value": "### Important\n\n1. [Section 5.2] The semantic reframing attack succeeded in 1% of attempts. Can you provide more analysis of this attack class and potential defenses? Is this a fundamental limitation of instruction-detection approaches?\n2. [Section A.4] Can the authors provide more details on the data augmentation strategy—what characters, at what frequencies, and how gradually does augmentation strength increase from 0 to 20%?\n3. Can the authors comment on the failure points of commandsans more? When the utility is lower, what about commandsans is causing failure? \n4. Can the authors comment on how they think CommandSans can be adapted to semantic reframing?\n5. [252] This claim does not seem well substantied to me, because the detector is essentially acting as a classifier, I do not see how instruction-following is relevant. I would be more convinced if you could either show experiments that show instruction following actually affects second-order attacks in this context or explain more why.\n\n\n### Minor\n\n1. [83]]missing 'on'\n2. [103] an -> a\n3. [52] extra 'a'\n7. [Section 6] The conclusion mentions \"bridging gap between research and deployment\" but more discussion of actual deployment considerations (latency, costs, failure modes) would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0vy1Jfy3qO", "forum": "3cK8gUZWBD", "replyto": "3cK8gUZWBD", "signatures": ["ICLR.cc/2026/Conference/Submission20568/Reviewer_F5hk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20568/Reviewer_F5hk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997250378, "cdate": 1761997250378, "tmdate": 1762933980425, "mdate": 1762933980425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CommandSans, a token-level prompt sanitization framework for defending LLM agents against indirect prompt injection attacks. Instead of traditional sample-level detection (which blocks entire inputs when an attack is suspected), CommandSans performs a more fine-grained token tagging to identify and remove only the instructional components directed at AI systems, leaving benign content intact.\n\nDisclaimer: I am not from the LLM safety community but instead the MARL community, so my judgment may be inaccurate"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Novel framing of the defense problem\nTreats prompt-injection defense as a token-level sanitization task, rather than binary classification.\n\n(2) Thorough evaluation methodology\nCombines five public benchmarks with a human red-teaming study, giving both quantitative and qualitative validation."}, "weaknesses": {"value": "(1) Limited generalization to structured or code-like domains\n(1.1) Performance drops notably in Code QA and Table QA tasks (Table 2), suggesting a distributional mismatch between training and deployment data. \n(1.2) I only see a limited example of instructions. Would a simple codebook or ML masking for similar prompts perform as well? If not, why?\n(1.3) What if \"book a meeting on Google Calendar\", which seems benign, is used as a method to attack?\n\n(2) No statistical analysis and confidence interval are reported in all tables.\n\n(3) Citations should be carefully examined (e.g., L265, L266 author names are weird)"}, "questions": {"value": "(1)(2) see weakness\n\n(3) How does CommandSans handle semantic or implicit prompt injections that do not contain explicit instruction tokens (e.g., malicious intent framed as compliance rules or reasoning guidance), and can token-level sanitization fundamentally defend against such reframed attacks?\n\n(4) Is there any methodology to optimize the defense strategies for the Pareto optimal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9PkPSoqw9w", "forum": "3cK8gUZWBD", "replyto": "3cK8gUZWBD", "signatures": ["ICLR.cc/2026/Conference/Submission20568/Reviewer_ETDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20568/Reviewer_ETDd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065852315, "cdate": 1762065852315, "tmdate": 1762933979819, "mdate": 1762933979819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}