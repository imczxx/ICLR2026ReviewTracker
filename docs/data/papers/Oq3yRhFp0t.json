{"id": "Oq3yRhFp0t", "number": 25423, "cdate": 1758367926684, "mdate": 1759896721570, "content": {"title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "abstract": "Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) and using established datasets (e.g., COCO, ImageNet and its variants, etc).\n\nThe main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.\n\nWe observe that 1) the models are not close to the state-of-the-art specialist models at any tasks, and 2) they perform semantic tasks notably better than geometric ones. However, 3) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks and 6) reasoning models, e.g. o3, show improvements in geometric tasks.", "tldr": "", "keywords": ["vision benchmark", "multimodal foundation models", "vision language models", "standard computer vision tasks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e62eedf4fc606a238123b0c26aeb9f413944fcad.pdf", "supplementary_material": "/attachment/d87ced81699641e0183dde7f95a0332ea626ea78.zip"}, "replies": [{"content": {"summary": {"value": "This paper benchmarks VLMs on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets. It developed a novel method to translate vision tasks into text-promptable, API-compatible formats via prompt chaining. It draws a few interesting findings: VLMs are not close to the state-of-the-art specialist\nmodels at any tasks; GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors develop a very clever way to prompt the VLMs to perform dense prediction tasks by using superpixels. They also find a better way to prompt object detection through iterative cropping and splitting the image into grids. This new technique creates the new possibility of systematically benchmarking the spatial and semantical understanding capability of VLMs, and the results are interesting.\n2. The paper is well-written. The experiments are extensive, covering 6 fundamental vision tasks and the state-of-the-art VLMs, both closed and open-sourced ones."}, "weaknesses": {"value": "1. Some of the specialist models are outdated. This could make the readers think that the gaps between VLMs and SOTA models are smaller than they actually are. Examples: the authors should compare with SAM2 in semantic segmentation (Tab 4); compare with Lotus [1] and Moge2 [2] for depth and normal map estimation.\n2. For depth and normal map estimation, it is not clear how many random pairs are needed for a good convergence of the optimization algorithm. The results of depth and normal in Fig 3 also look very coarse, probably due to the granularity of the superpixels. It would be interesting to see if higher quality depth/normal map will emerge if we use more superpixels and/or more pairs.\n\n[1] Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction, ICLR 2025\n\n[2] MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details, Arxiv 2025"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NdZHhBZdD2", "forum": "Oq3yRhFp0t", "replyto": "Oq3yRhFp0t", "signatures": ["ICLR.cc/2026/Conference/Submission25423/Reviewer_GMJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25423/Reviewer_GMJw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941148192, "cdate": 1761941148192, "tmdate": 1762943428200, "mdate": 1762943428200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive benchmark evaluating the visual understanding capabilities of state-of-the-art multimodal foundation models (MFMs), including GPT-4o, Gemini 1.5/2.0, Claude 3.5 Sonnet, Qwen2-VL, and Llama 3.2, on a suite of standard computer vision tasks: classification, object detection, semantic segmentation, grouping, depth estimation, and surface normal prediction. Recognizing the challenge that most MFMs are text-based and do not natively output dense or structured visual predictions, the authors introduce a systematic “prompt chaining” framework that decomposes vision tasks into API-compatible sub-tasks. The results demonstrate that while MFMs are decent generalists—especially on semantic tasks like classification and segmentation—they lag substantially behind state-of-the-art specialist vision models, and struggle most with geometric (3D) tasks. The paper includes both quantitative and qualitative analyses, explores prompt sensitivity, and openly acknowledges the limitations of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1) Thorough and Timely Benchmarking:** The paper provides one of the most exhaustive and systematic evaluations to date of leading MFMs' visual understanding, moving well beyond the customary VQA or captioning settings to address a broader spectrum of fundamental computer vision tasks.\n\n**2) Task Translation via Prompt Chaining:** The “prompt chaining” framework is thoughtfully designed, enabling apples-to-apples comparisons between text-based MFMs and vision specialists through structured sub-task decomposition. Figure 2 effectively visualizes the design for converting complex tasks like depth and segmentation into textual queries, making the framework transparent and reproducible.\n\n**3) Comprehensive Model and Task Coverage:** The analysis involves a diverse set of MFMs (both closed- and open-weight) across six core vision tasks and multiple datasets, situating the current state of MFMs within established benchmarks (COCO, ImageNet, Hypersim, etc.), including corruption and robustness variants. Table 1 and Table 3–6 substantiate this comprehensiveness.\n\n**4) Control Baselines and Calibration:** The empirical results are carefully calibrated with various baselines—(a) top specialist models, (b) specialists subjected to the same chaining and superpixel constraints, (c) oracle variants, and (d) blind guess. This tightens the attribution of observed deficits to either model limitations or task translation artifacts."}, "weaknesses": {"value": "**1) Prompt Chaining Overhead and Realism:** The proposed evaluation relies on decomposing tasks into a large number of textual API calls, which is computationally expensive (as noted in App. I). While the paper claims this is for benchmarking only, the translation introduces additional sources of potential error (task granularity, superpixel boundaries, etc.), and the extent to which these reflect real “model” limitations is not exhaustively disentangled. For instance, in Figure 2 and related descriptions, there is acknowledgment that chaining is not optimal—yet the effect of granularity and design choices is mostly probed by coarse ablations, potentially underestimating the ceiling performance of some MFMs.\n\n**2) Limited Exploration of Advanced Prompting or Visual Tools:** There is a missed opportunity to rigorously evaluate more advanced or recently proposed visual prompt engineering techniques (e.g., interactive alignment, visual rulers, or interactive markers for geometric tasks, as briefly mentioned in App. E.1, Figure 15) across all models as a systematic solution for object localization or dense prediction. The decision to use superpixel-based batch querying is pragmatic but may limit the apparent ability of models on fine-structured tasks.\n\n**3) Insufficient Error Analysis on Geometric Tasks:** While the paper establishes that MFMs fare significantly worse on geometric tasks (Tables 5 and 6; Figure 3), there is minimal deep dive into why—for example, what kinds of 3D/normal ambiguities (left-right, scale, out-of-plane rotation) are most problematic, or whether cues are absent due to model design, pretraining data, or prompt interface. Figure 3 and Table 6 show low/negative correlations for some directions, but the causal factors or failure modes are not dissected in detail."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xYrHX0GXof", "forum": "Oq3yRhFp0t", "replyto": "Oq3yRhFp0t", "signatures": ["ICLR.cc/2026/Conference/Submission25423/Reviewer_9qaY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25423/Reviewer_9qaY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946783986, "cdate": 1761946783986, "tmdate": 1762943427883, "mdate": 1762943427883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper benchmarks multimodal foundation models for standard CV tasks. They proposed an interesting way to avoid using API calls for promptable computer vision tasks. The whole evaluation tasks covers 2 categories, semantic and geometrical understanding, and there are 6 sub-tasks in total. GPT-4o achieves an overall best performance, but is still lagging far behind CV models. For geometric tasks, under the promtable format, all the MFM failed to work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The promptable task design itself is interesting and is effective for semantic understanding tasks. \n- This paper is well written and easy to follow\n- Experimental evaluations are comprehensive."}, "weaknesses": {"value": "The biggest problem is, the pipeline of the geometric understanding tasks (depth and surface normal) does not work at all for MFMs:\n- In Figure 3, neither depth nor surface normal works as expected, making it not surprising that all the models lagged far behind CV models. \n- The root cause of the mismatch is the ranking algorithm. It is impossible to get the numeric results for depth and surface normal purely from ranks. \n- Without these two geometric tasks, all 4 tasks are basically semantic understanding tasks, with the performance of all the compared models highly correlated. \n\nOther minor weaknesses: \n- The observations did not provide key findings, unless the ranking. Some interesting points may be: why the reasoning models work worse on semantic understanding? \n- The metrics for surface normal and depth are not the most popular ones. \n- Results with GPT-4o image generation is confusing and did not fit the whole story."}, "questions": {"value": "- How will random superpixelizations/grid seeds affect the performance?\n- Could other CV tasks also be evaluated in the same manner, such as pose estimation, edge detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oBknGdjwDc", "forum": "Oq3yRhFp0t", "replyto": "Oq3yRhFp0t", "signatures": ["ICLR.cc/2026/Conference/Submission25423/Reviewer_hd9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25423/Reviewer_hd9G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950314960, "cdate": 1761950314960, "tmdate": 1762943427662, "mdate": 1762943427662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive benchmark evaluating multimodal foundation models (MFMs) on standard computer vision tasks including classification, object detection, semantic segmentation, grouping, depth estimation, and surface normal prediction. The key contribution is a prompt chaining framework that decomposes complex vision tasks into text-solvable sub-tasks, enabling API-level evaluation of closed-source models. The study evaluates GPT-4o, o4-mini, Gemini 2.0 Flash, Gemini 1.5 Pro, Claude 3.5 Sonnet, Qwen2-VL, and Llama 3.2 on established datasets. Main findings show that: (1) MFMs lag significantly behind specialist models on all tasks, (2) they perform better on semantic tasks than geometric ones, (3) GPT-4o performs best among non-reasoning models, and (4) reasoning models (o1, o3, o4-mini) show promising improvements on geometric tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive Model Coverage: The evaluation includes a diverse set of models (7 main MFMs plus reasoning models), providing a thorough landscape of current multimodal model capabilities across both open and closed-source systems.\n\n- Multiple Task Evaluation: The breadth of tasks evaluated (6 core vision tasks) spanning semantic to geometric understanding provides valuable insights into where MFMs excel and struggle."}, "weaknesses": {"value": "- Potential Data Contamination: While the authors conduct in-the-wild evaluations to address this, the use of standard benchmarks (ImageNet, COCO) raises concerns about training data leakage for closed-source models, which could inflate performance estimates.\n\n- Limited Analysis of Failure Modes: While the paper shows that MFMs struggle with geometric tasks, there is limited investigation into why they fail (e.g., lack of 3D training data, architectural limitations, reasoning deficits). The \"blurry vision\" hypothesis is mentioned but not systematically explored.\n\n- Task Selection Bias: The choice of tasks favors dense prediction problems that can be decomposed into classification. Other important vision capabilities (e.g., visual reasoning, fine-grained recognition, video understanding) are not evaluated."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1fkS3CuGpF", "forum": "Oq3yRhFp0t", "replyto": "Oq3yRhFp0t", "signatures": ["ICLR.cc/2026/Conference/Submission25423/Reviewer_J3EM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25423/Reviewer_J3EM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996748129, "cdate": 1761996748129, "tmdate": 1762943427471, "mdate": 1762943427471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}