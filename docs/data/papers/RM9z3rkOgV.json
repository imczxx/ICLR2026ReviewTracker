{"id": "RM9z3rkOgV", "number": 22768, "cdate": 1758335225730, "mdate": 1759896847786, "content": {"title": "CURV: Enhancing Chart Understanding through Visual Grounded Reasoning", "abstract": "Chart question answering (CQA) requires multimodal large language models (MLLMs) to integrate visual comprehension with logical reasoning, yet current models struggle with accurate visual grounding and coherent reasoning chains. \nWhile external chain-of-thought prompting and visual cues significantly improve performance, current MLLMs lack intrinsic visually grounded reasoning capabilities, leading to inaccurate perception and reasoning disconnected from visual evidence.\nTo address these limitations, we propose CURV, a curriculum learning framework that develops intrinsic visual grounded reasoning capabilities by reformulating CQA as multi-turn visual reasoning, where each step coordinates logical reasoning with dynamic visual grounding through spatial attention concentration.\nTo assist model learning, we further introduce CCQA, a three-level curriculum dataset with scalable synthetic generation across diverse chart types and reasoning patterns. Our curriculum systematically progresses from basic single-operation reasoning to complex multi-chart compositional tasks.\nExperiments demonstrate that CURV achieves up to 10.79% accuracy improvements over baselines and strong generalization to real-world benchmarks and out-of-domain multimodal reasoning tasks, validating the effectiveness of internalizing visual reasoning with dynamic grounding for enhanced chart understanding capabilities.", "tldr": "We propose a curriculum learning framework with tailored multi-level datasets that guide MLLMs to reason with dynamic visual grounding, thereby enhancing their chart understanding and multimodal reasoning capabilities.", "keywords": ["chart understanding", "multimodal reasoning", "curriculum learning", "scalable chart understanding benchmark", "large multimodal language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a1a20abd7cd6206131cdb87ff9961972b6e83ec.pdf", "supplementary_material": "/attachment/db8760db1b9f45a01cdc6e1ecdc6da55270185bb.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the poor performance of Multimodal Large Language Models (MLLMs) on Chart Question Answering (CQA). The authors posit that MLLMs lack \"intrinsic\" visually grounded reasoning capabilities. To address this, they propose two main contributions: \n1) CURV, a curriculum learning framework that trains MLLMs to internalize reasoning;\n2) CCQA, a new, scalable, synthetic dataset built with a three-level curriculum of increasing difficulty.\n\nThe CURV method uses Supervised Fine-Tuning (SFT) to train models on (Question, Reasoning, Bounding-Box, Answer) quadruplets. This process is designed to force the model to explicitly coordinate its logical reasoning steps ($R_t$) with dynamic visual grounding via bounding boxes ($B_t$)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear Problem Identification: The paper does a good job of identifying and demonstrating a clear failure mode in modern MLLMs: the disconnect between their reasoning chains and the visual evidence in an image. \n\n2. Good Presentation: The paper is easy to follow."}, "weaknesses": {"value": "1. Reliance on SFT is a Methodological Step Backward: The paper's entire framework is built on SFT. For a complex reasoning task, this is a major limitation. SFT is \"imitation learning\"; the model is only trained to mimic the single \"golden path\" reasoning trace provided by the dataset's templates. This approach is notoriously brittle, suffers from exposure bias (it never learns to recover from its own errors), and does not teach the model a generalizable policy for reasoning. The state-of-the-art in complex reasoning has moved decisively towards RL and preference-based methods, which are more robust. This paper's reliance on SFT feels outdated.\n2. Trivial \"Tools\": The paper presents \"applied,\" \"boxed,\" and \"cropped\" as three distinct grounding strategies. \"Applied\" (a yellow highlight) and \"Boxed\" (a red border)  are functionally identical. \"Cropped\" (a zoom-in) is a standard, non-novel technique in computer vision. Presenting these three minor variations as a \"comparative analysis\" (Table 6)  further pads the paper without adding real substance. They both take bounding box coordinates and overlay a visual cue. The core mechanism is simply \"training with bounding boxes.\"\n3. Quality of Dataset: The dataset is presented as a core contribution, but its quality is highly questionable for training robust models. The dataset is synthetic and template-based. The semantic richness of the domain is likely ignored, as the task only requires applying a logical template to the data. Through SFT, the model might only memorize the template of the constructed dataset. It is a large, clean, but ultimately sterile and over-simplified dataset. Its template-based nature creates a \"toy world\" where reasoning is unrealistically clean and follows predefined paths. If a real-world problem requires a slightly different logical step, the model has never been exposed to it and will likely fail. This is not robust reasoning; it is high-fidelity mimicry.\n4. Core Method is a Combination of Existing Ideas: The central framework, CURV, is not a new method. It is a straightforward combination of two very well-established concepts: (1) Curriculum Learning  and (2) Interleaved Reasoning. The idea of training models to produce step-by-step reasoning grounded in the image has been explored before. This paper simply applies a standard curriculum to this data format. This represents a solid engineering effort but lacks the fundamental research novelty expected at ICLR."}, "questions": {"value": "1. Can you justify the decision to use SFT, an imitation-learning approach, over more modern RL-based methods for training a robust reasoning policy? How can you be sure your model is learning to \"reason\" rather than just \"memorizing\" the synthetic reasoning templates?\n2. Can you defend the \"meta-learning\"  claim? How is your data generation process (systematically combining 30 domains, 7 chart types, and 12 operators)  fundamentally different from a standard, large-scale, and well-structured curriculum design?\n3. To isolate the contribution of your method from your data, did you run a crucial baseline where a model is fine-tuned on the CCQA dataset (Levels 1+2) using only the final (Question, Answer) pairs, without the intermediate ($R_t, B_t$) supervision? This would quantify the true benefit of the grounded reasoning curriculum."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PCG5zN6mG1", "forum": "RM9z3rkOgV", "replyto": "RM9z3rkOgV", "signatures": ["ICLR.cc/2026/Conference/Submission22768/Reviewer_sseA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22768/Reviewer_sseA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760681980167, "cdate": 1760681980167, "tmdate": 1762942378531, "mdate": 1762942378531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Chart Question Answering (CQA), where multimodal LLMs often fail to reason with visual grounding. The authors propose CURV, a curriculum learning framework that reformulates CQA as multi-turn reasoning with dynamic visual grounding, pairing each reasoning step with a bounding box in the chart. CURV is trained with a multi-objective loss combining language modeling and mask-based visual grounding.\n\nTo support training, they introduce CCQA, a synthetic dataset covering seven chart types and 30 domains, designed with progressive difficulty, interleaved visual grounding, and template-based accuracy. Experiments on CCQA, real-world, and out-of-domain benchmarks show CURV improves answer accuracy, reasoning, and visual grounding, with applied masking performing best."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- **Novel and well-motivated framework:** CURV addresses a clear limitation in MLLMs by internalizing visually grounded reasoning through multi-turn structured reasoning.\n- **Effective curriculum learning:** CCQA is well-designed to support progressive learning, with strong alignment between reasoning and visual grounding.\n- **Comprehensive evaluation:** Experiments cover multiple models, benchmarks, and metrics, including reasoning, visual grounding, and generalization.\n- **Transparent analysis:** The authors honestly discuss trade-offs in curriculum levels, providing insight into robustness vs. generalization."}, "weaknesses": {"value": "- **Limited CCQA dataset details:** The paper does not provide information on the total number of samples, nor the distribution of examples across the curriculum levels, which is important for assessing dataset scale and difficulty.\n- **Relevant prior work missing:** The paper does not cite **[arXiv:2506.11991](https://arxiv.org/pdf/2506.11991)**, which also explores multimodal chain-of-thought reasoning and evaluation on charts; including this reference would strengthen the positioning.\n- **Reproducibility:** There is no public code or repository for CURV or CCQA, which limits reproducibility of results.\n- **Position of Related Work:** The Related Work section appears after Results; integrating it earlier could improve narrative flow and clarity.\n- **Minor writing issue:** The sentence “These reveal MLLMs’ lack of logical decomposition and visual reasoning capabilities” has a typo."}, "questions": {"value": "**Actionable Feedback**\n\n1. Include CCQA statistics (number of samples, distribution across levels) to improve clarity.\n2. Cite relevant prior work ([arXiv:2506.11991]) to strengthen contextual positioning.\n3. Consider releasing code and CCQA dataset to support reproducibility.\n4. Reorganize Related Work section to appear earlier for better narrative flow.\n5. Fix minor writing issues, e.g., _“These reveal…”_ → _“These results highlight…”_."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8DAvqWzgm5", "forum": "RM9z3rkOgV", "replyto": "RM9z3rkOgV", "signatures": ["ICLR.cc/2026/Conference/Submission22768/Reviewer_oBSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22768/Reviewer_oBSm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664240943, "cdate": 1761664240943, "tmdate": 1762942378095, "mdate": 1762942378095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CURV, a curriculum learning framework designed to improve multimodal large language models’ ability to perform visually grounded reasoning for chart question answering (CQA). Instead of relying on external prompts or cues, CURV trains models to conduct multi-step reasoning while dynamically focusing on relevant chart regions. To support this, the authors construct CCQA, a synthetic curriculum dataset covering seven chart types and 30 domains, organized into three levels that range from simple single-operation tasks to multi-chart reasoning. Experiments on CCQA and multiple public benchmarks demonstrate that models fine-tuned with CURV achieve higher accuracy and better generalization than baseline models, suggesting that integrating structured visual grounding with stepwise reasoning enhances chart understanding performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is well defined. Sections 2.1 and 2.2 clearly explain why this task is important and highlight where current models are struggling.\n\nThe CCQA dataset hierarchy is well structured and effectively illustrated. The authors provide strong examples demonstrating where model reasoning fails for each hierarchy type. This hierarchical split is useful for evaluating how and where models struggle across different reasoning levels.\n\nThe three proposed strategies are clearly defined and well supported with examples. However, these explanations would be more impactful if they were discussed in greater detail in the main content rather than being confined to the appendix."}, "weaknesses": {"value": "The CCQA dataset is not explained properly. While the paper mentions 7 chart types and 30 domain categories, implying 210 base images with “hundreds” of derived questions per image, the total number of questions is never specified. This makes it very difficult to evaluate this dataset. Additionally, the process for ensuring the correctness of the “query–reasoning–grounding–answer” quadruplets for each question is unclear and requires further explanation.\n\nThere is a lack of clarity regarding the baseline setup. The paper should elaborate on how prompting was done for the baseline, what visual or contextual information was provided alongside the images, and how this aligns with the proposed strategies. It’s not fully clear if baseline methods were given equally strong training or prompts, so comparisons may favor the new method.\n\nTable 2 does not include boxed and cropped results for InternVL—these values are only shown for Qwen2.5VL. The reported performance improvements are minimal (1–4% overall increase in Table 3), which is expected given the fine-tuning setup. Even the 10.49% gain for Qwen2.5VL is limited to Level 1 and remains about 5% lower than GPT-4.1-mini. Without disclosure of the total question count, it is difficult to assess the statistical significance of these improvements. Furthermore, Table 3 omits closed-source models such as GPT-4o and GPT-4.1-mini.\n\nThe training starts from a small set of base charts that are reused a lot. This could introduce synthetic bias risk. Performance drops sharply when multiple subplots must be reasoned about together showing that the approach doesn’t work well for multi-chart settings. Few ablations on of how sensitive results are to number of steps, image resolution, etc.\n\nThe related work section is insufficient. It does not clearly articulate how this method differs from prior chart visual grounding approaches, such as RefChartQA, or from visual grounding methods applied to non-chart data. How is your method different from these existing grounding strategies? Additionally, it fails to properly distinguish CCQA from existing datasets mentioned in Table 3, which weakens the justification for the novelty of the proposed dataset.\n\nMany results are judged by another AI, there’s limited human evaluation and no error bars or confidence intervals.\n\nWhile the paper contains substantial and valuable content, the overall organization and ordering could be improved. Many important details are buried in the appendix, making it harder to follow the main narrative. Moving key explanations and examples into the main text would greatly improve readability and coherence."}, "questions": {"value": "Section 2.1 mentions human-annotated visual information when discussing bottlenecks in MLLMs. How were these annotations performed, and were there any specific guidelines or quality-control procedures followed during the annotation process?\n\nSince human-curated reasoning during inference is unfeasible, how different would the performance in Table 1 be if the VA and RVA setups used MLLM-generated visual information instead of human-annotated information?\n\nSection 6.4 mentions that adding Level 3 training improves complex reasoning but weakens performance on simpler levels. How would changing hyperparameters such as w1,w2,w3​ (Formula 29), λ (Formula 25), and μ(i) (Formulas 27 and 28) affect this trade-off? What specific values of w(i) were used, and what was the rationale for those choices?\n\nGiven that the paper shows multi-chart understanding remains challenging, how does the proposed agent design or reasoning mechanism help address this issue?\n\nHow were the baselines setup? Can we check robustness of the approach when there are messy charts with lots of clutter and information overload?\n\nHow are regions chosen across multiple charts? How to prevent model from focusing on wrong regions or getting confused between multiple charts?\n\nCan we show quality of grounding as a table to measure how often the model points to the right place and how that ties to answer accuracy?\n\nAny small human review to check the reasoning and highlighted regions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WFQKkJEoVk", "forum": "RM9z3rkOgV", "replyto": "RM9z3rkOgV", "signatures": ["ICLR.cc/2026/Conference/Submission22768/Reviewer_eU2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22768/Reviewer_eU2M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942600866, "cdate": 1761942600866, "tmdate": 1762942377895, "mdate": 1762942377895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CURV, a curriculum learning framework designed to build intrinsic visual grounded reasoning capabilities in models. This is achieved by progressively training models to coordinate visual attention with logical reasoning, starting with simple single-operation tasks and moving toward complex nested reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty and Technical Soundness: The idea of using grounded visual reasoning is novel and technically sound, providing a strong approach to improve performance at the intersection of LVLMs and structured data understanding.\n- Clarity: The paper is well-written and easy to follow, with a clear structure.\n- Motivation: The motivation behind the work is interesting and well-presented.\n- Novel Finding: The finding in the analysis that MLLMs can internalize these capabilities through grounded visual reasoning is novel."}, "weaknesses": {"value": "## Dataset Quality and Assessment (Major Concern)\n\nThe quality and statistics of the proposed dataset are not well-assessed. The majority of the dataset's details are discussed in the supplementary material, and even after reading through it, the reviewer cannot find an in-depth analysis of the proposed dataset. Since this paper heavily relies on the proposed dataset, a thorough analysis and detailed statistics are mandatory to warrant the effectiveness of the methods.\n- Required Data Statistics: The authors should provide essential data statistics, such as the number of training and testing data samples, and other distributions like question types.\n- Bias and Diversity: Based on the supplementary material, the dataset is generated by an LLM, which can introduce bias (e.g., a monotonic trend thatmostly upward or downward in generated samples) and may lack the diversity required to cover real-world distributions. Does the data pipeline face similar issues? How is this issue mitigated or solved?\n- Quality Control: Since the LLM can make mistakes, is there any post-filtering or human-in-the-loop filtering involved in the dataset generation?\n- Quality Assurance: To ensure the quality of the dataset, can the authors perform a human evaluation on both the training and test sets to demonstrate the correctness and diversity of the synthetic data?\n- Context: How many data samples are in the training set and testing set? Please include a comparison with previous datasets or benchmarks.\n\n## Effectiveness Assessment and Experimental Comparison (Major Concern)\n\nThe effectiveness of the dataset and method is not well-assessed in the experimental comparisons (Tables 2 and 3). The current comparison involves models fine-tuned on chart-specific data against their baseline models on a chart benchmark. It is obvious and not surprising that fine-tuning on a specific downstream domain will improve the domain capability of general models like QwenVL and InternVL. A more head-to-head comparison should be considered: For example, \n- To Assess Dataset Effectiveness: It is suggested to have a baseline model trained exhaustively on existing chart domain training data (e.g., the 1.4M training data organized by TinyChart) and compare it with the same baseline model trained on both classical chart domain data plus the proposed data. Assuming the classical data has reached its limit to boost performance, if the proposed dataset can bring significant further improvement, one would be convinced of its effectiveness.\n- To Assess Visual Grounding: To assess the benefit of visual grounding reasoning training, compare a model trained with and without the grounding loss on the exact same training dataset. This way, with the training data fixed, the actual benefit from the grounding loss training can be clearly observed.\n\n*The reviewer acknowledges this is a long paper with many details in the supplementary material and may have missed specific points mentioned in the appendix. They are willing to reevaluate the paper if this concern can be addressed.*"}, "questions": {"value": "- In line 81, the authors mention \"coordinate visual attention\" and a \"mask-based grounding loss.\" How exactly are the masks collected? And how is the mask loss computed.is it based on the bounding boxes proposed in the reasoning process or the output-to-vision tokens attention map?\n- How is the human-annotated visual information provided to the model? Is it provided exactly as a set of bounding box coordinates?\n- The trends observed in Table 1 and Table 2 appear to be opposite. Table 1 suggests that human-annotated visual information brings significantly more value than CoT reasoning for chart domain understanding. However, this observation does not hold in Table 2, where the opposite trend is seen: reasoning (blue bar) brings more improvement than visual information (yellow bar). Can the authors provide more discussion on this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qRjttpOYQ5", "forum": "RM9z3rkOgV", "replyto": "RM9z3rkOgV", "signatures": ["ICLR.cc/2026/Conference/Submission22768/Reviewer_URig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22768/Reviewer_URig"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111314243, "cdate": 1762111314243, "tmdate": 1762942377626, "mdate": 1762942377626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CURV, a curriculum learning framework designed to build intrinsic visual grounded reasoning capabilities in models. This is achieved by progressively training models to coordinate visual attention with logical reasoning, starting with simple single-operation tasks and moving toward complex nested reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty and Technical Soundness: The idea of using grounded visual reasoning is novel and technically sound, providing a strong approach to improve performance at the intersection of LVLMs and structured data understanding.\n- Clarity: The paper is well-written and easy to follow, with a clear structure.\n- Motivation: The motivation behind the work is interesting and well-presented.\n- Novel Finding: The finding in the analysis that MLLMs can internalize these capabilities through grounded visual reasoning is novel."}, "weaknesses": {"value": "## Dataset Quality and Assessment (Major Concern)\n\nThe quality and statistics of the proposed dataset are not well-assessed. The majority of the dataset's details are discussed in the supplementary material, and even after reading through it, the reviewer cannot find an in-depth analysis of the proposed dataset. Since this paper heavily relies on the proposed dataset, a thorough analysis and detailed statistics are mandatory to warrant the effectiveness of the methods.\n- Required Data Statistics: The authors should provide essential data statistics, such as the number of training and testing data samples, and other distributions like question types.\n- Bias and Diversity: Based on the supplementary material, the dataset is generated by an LLM, which can introduce bias (e.g., a monotonic trend thatmostly upward or downward in generated samples) and may lack the diversity required to cover real-world distributions. Does the data pipeline face similar issues? How is this issue mitigated or solved?\n- Quality Control: Since the LLM can make mistakes, is there any post-filtering or human-in-the-loop filtering involved in the dataset generation?\n- Quality Assurance: To ensure the quality of the dataset, can the authors perform a human evaluation on both the training and test sets to demonstrate the correctness and diversity of the synthetic data?\n- Context: How many data samples are in the training set and testing set? Please include a comparison with previous datasets or benchmarks.\n\n## Effectiveness Assessment and Experimental Comparison (Major Concern)\n\nThe effectiveness of the dataset and method is not well-assessed in the experimental comparisons (Tables 2 and 3). The current comparison involves models fine-tuned on chart-specific data against their baseline models on a chart benchmark. It is obvious and not surprising that fine-tuning on a specific downstream domain will improve the domain capability of general models like QwenVL and InternVL. A more head-to-head comparison should be considered: For example, \n- To Assess Dataset Effectiveness: It is suggested to have a baseline model trained exhaustively on existing chart domain training data (e.g., the 1.4M training data organized by TinyChart) and compare it with the same baseline model trained on both classical chart domain data plus the proposed data. Assuming the classical data has reached its limit to boost performance, if the proposed dataset can bring significant further improvement, one would be convinced of its effectiveness.\n- To Assess Visual Grounding: To assess the benefit of visual grounding reasoning training, compare a model trained with and without the grounding loss on the exact same training dataset. This way, with the training data fixed, the actual benefit from the grounding loss training can be clearly observed.\n\n*The reviewer acknowledges this is a long paper with many details in the supplementary material and may have missed specific points mentioned in the appendix. I'm willing to reevaluate the paper if this concern can be addressed.*"}, "questions": {"value": "- In line 81, the authors mention \"coordinate visual attention\" and a \"mask-based grounding loss.\" How exactly are the masks collected? And how is the mask loss computed.is it based on the bounding boxes proposed in the reasoning process or the output-to-vision tokens attention map?\n- How is the human-annotated visual information provided to the model? Is it provided exactly as a set of bounding box coordinates?\n- The trends observed in Table 1 and Table 2 appear to be opposite. Table 1 suggests that human-annotated visual information brings significantly more value than CoT reasoning for chart domain understanding. However, this observation does not hold in Table 2, where the opposite trend is seen: reasoning (blue bar) brings more improvement than visual information (yellow bar). Can the authors provide more discussion on this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qRjttpOYQ5", "forum": "RM9z3rkOgV", "replyto": "RM9z3rkOgV", "signatures": ["ICLR.cc/2026/Conference/Submission22768/Reviewer_URig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22768/Reviewer_URig"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111314243, "cdate": 1762111314243, "tmdate": 1763141525625, "mdate": 1763141525625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}