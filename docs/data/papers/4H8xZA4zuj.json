{"id": "4H8xZA4zuj", "number": 24942, "cdate": 1758362256769, "mdate": 1763681707033, "content": {"title": "On the Interaction of Batch Noise, Adaptivity, and Compression, under $(L_0,L_1)$-Smoothness: An SDE Approach", "abstract": "Understanding the dynamics of distributed stochastic optimization requires accounting for several major factors that affect convergence, such as gradient noise, communication compression, and the use of adaptive update rules. While each factor has been studied in isolation, their joint effect under realistic assumptions remains poorly understood. In this work, we develop a unified theoretical framework for Distributed Compressed SGD (DCSGD) and its sign variant Distributed SignSGD (DSignSGD) under the recently introduced $(L_0, L_1)$-smoothness condition. Our analysis leverages stochastic differential equations (SDEs), and we show that while standard first-order SDEs might lead to misleading conclusions, including higher-order terms helps capture the fine-grained interaction between learning rates, gradient noise, compression, and the geometry of the loss landscape. These tools allow us to inspect the dynamics under general gradient noise assumptions, including heavy-tailed and affine-variance regimes, which extend beyond the classical bounded-variance setting. Our results show that normalizing the updates of DCSGD emerges as a natural condition for stability, with the degree of normalization precisely determined by the gradient noise structure, the landscape’s regularity, and the compression rate. In contrast, our model predicts that DSignSGD converges even under heavy-tailed noise with standard learning rate schedules, a finding which we empirically verify. Together, these findings offer both new theoretical insights and practical guidance for designing stable and robust distributed learning algorithms.", "tldr": "We develop an SDE-based framework for DCSGD and DSignSGD, showing DCSGD needs noise- and compression-dependent normalization for stability, while DSignSGD remains robust and convergent even under heavy-tailed noise.", "keywords": ["Stochastic Differential Equations", "$(L_0", "L_1)$-Smoothness", "Distributed Learning", "Adaptivity"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f860517cdd80b8da711acfe7c42fc68000e97031.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed new SDE models that enable correct analysis of SGD with proper learning rate restrictions and conditions, under the $(L_0, L_1)$-smoothness condition. The approach was used to proving convergence bound for the models of distributed compressed GD and signed GD, resulting in a more general condition. Other relaxations of assumptions and conclusions are also drawn from this approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed new SDE approach fixes the previous issue on learning rate restrictions and convergence guarantees under $(L_0, L_1)$-smoothnesses are provided, along with rigorous proofs.\n\n(2) The proposed framework was used to strengthen the convergence guarantee of DCSGD and DSignSGD, under a more general assumption than the commonly used smoothness condition. This also leads to relaxations of certain assumptions originally needed.\n\n(3) Some other insightful conclusions on the interplay of different factors affecting convergence and the convergence of adaptive method under heavy-tailed noise are drawn."}, "weaknesses": {"value": "(1) The main contribution of the paper seems to be the development of a novel and correct SDE framework for analyzing distributed stochastic optimization. It clarifies some subtle inconsistencies in SDE-based approaches, extending them to include compression and noise structure, but it does not introduce new algorithms or fundamentally new convergence phenomena, which makes the contribution rather incremental.\n\n(2) I do agree that the unification of compression, noise and adaptivity are unified for the first time, but each of those components are analyzed before. They are just not presented in a single theorem. I get the sense that combining them is still possible even without the SDE approach, one just needs to solve the complicated algebra correct. Some of the insights seem to be known already. \n\n(3) The empirical evidence is limited even to support the insights."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NptNwIeBYZ", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Reviewer_ahK4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Reviewer_ahK4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760624669490, "cdate": 1760624669490, "tmdate": 1762943255932, "mdate": 1762943255932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a unified theoretical framework for distributed compressed SGD (DCSGD) and its sign variant distributed SignSGD (DSignSGD) under the $(L_{0},L_{1})$-smoothness condition, to understand the joint effect of gradient noise, communication compression and the use of adaptive update rules. The analysis is based on stochastic differential equations (SDEs), by including higher-order terms, \nthat allows relatively general gradient noise, including heavy-tailed and affine-variance regimes. Theoretical insights are obtained for DCSGD and DSignSGD that lead to some practical guidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Even though the SDE approach has been applied before to study optimization algorithms,  no prior work has worked with the $(L_{0},L_{1})$-smoothness condition. Under this condition, convergence bounds are obtained for DCSGD and DSignSGD.\nNew insights are obtained.\n\n(2) The paper employs higher-order approximation, and claims that both the classic first-order and second-order SDEs from the literature can lead to conclusions that contradict the dynamics of discrete-time SGD."}, "weaknesses": {"value": "(1) The assumption in Theorem 4.1., i.e. equation (4) is hard to check because the right hand side depends on $\\mathbb{E}\\Vert\\nabla f(X_{t})\\Vert_{2}$, which is not explicit. If you can provide some tight bounds on $\\mathbb{E}\\Vert\\nabla f(X_{t})\\Vert_{2}$, it will be easier to see if (4) can be satisfied.\n\n(2)  The term higher-order SDE in the paper is a bit misleading, because it seems to be a second-order approximation as well, with the only difference being flipping the sign in the drift term in equation (2) to get equation (3). I am not convinced that the so-called higher-order SDE approximation in equation (3) is better than the second-order SDE, i.e. equation (2) from the literature. Let me explain more in detail. The author(s) of the paper first consider ODE approximation for quadratic function and obtain equation (7), which shows faster convergence as $\\eta$ increases and claims this is problematic. Actually, I am not sure about the claim. The reason is that for first or second order approximation to hold, $\\eta$ has to be small. When $\\eta$ is very small, indeed, a larger stepsize will lead to faster convergence. For you to obtain divergence, $\\eta$ needs to be large, which is not in the regime of SDE approximation. Then, the author(s) further analyze the ODE dynamics from equation (8) to equation (14) to try to convince why SDE approximation in equation (3) is better than the second-order SDE. However, this is problematic from two perspectives. First of all, SDE is different than ODE.  You are claiming an SDE approximation is better by ignoring the diffusion term. This is problematic, especially because the diffusion term also depends on $\\eta$. Second, the first-order and second-order approximation in the literature has theoretical guarantees, that is, the approximation has been proved rigorously, whereas in the current paper, the author(s) simply claim their equation (3) is better, but there is no rigorous approximation result of a particular order. My guess is that the reason the author(s) of the paper reaches a perhaps wrong approximation model to study is because SDE is fundamentally different than ODE. You simply cannot ignore the diffusion term, to argue which ODE approximates better, and then claim which SDE approximates better when both drift and diffusion terms depend on $\\eta$. The flip sign from equation (2) to equation (3) might just be because the ignorance of the Brownian noise term and hence the correction term from Ito's calculus. Because of this, the results and insights obtained for DCSGD and DSignSGD lack a rigorous theoretical foundation, and might be problematic."}, "questions": {"value": "(1) In Theorem 4.1., when you define the random time $\\hat{t}$, you should mention whether it is independent of the SDE process or not.\n\n(2) In the proof of Theorem B.1., $\\mathcal{O}(\\text{Noise})$ should be spelled out explicitly as an Ito integral term, and there should be more explanations how you obtain equation (68). Same can be said about the proof of Theorem B.2. and how you obtain (85).\n\n(3) On the right hand side of (67) and (84), you do not have expectations, and hence what on the right hand sides are stochastic. You either missed expectations or there is something wrong here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dSivrvFiqX", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Reviewer_cisk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Reviewer_cisk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740480476, "cdate": 1761740480476, "tmdate": 1762943255622, "mdate": 1762943255622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a detailed theoretical study on distributed stochastic optimization by modeling the dynamics of compressed and adaptive gradient methods through stochastic differential equations (SDEs) under the recently introduced (L_0,L_1)-smoothness framework. The authors analyze two representative distributed methods, Distributed Compressed SGD (DCSGD) and Distributed SignSGD (DSignSGD), and highlight the limitations of conventional first- and second-order SDE approximations. They propose refined SDE models that correctly capture learning rate constraints, stability thresholds, and the interaction between gradient noise, compression, and adaptivity. The theoretical results reveal that DSignSGD remains stable even under heavy-tailed noise, while DCSGD requires a normalization factor determined jointly by the compression rate, noise variance, and smoothness parameters. The analysis is complemented by a few illustrative synthetic experiments confirming the theoretical predictions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a rigorous and well-developed theoretical exploration of distributed stochastic learning dynamics through SDE analysis. Its contribution lies in unifying the understanding of batch noise, adaptivity, and compression within a realistic smoothness regime that extends beyond classical Lipschitz assumptions.\nIt provides an interesting integration of SDE-based modeling with distributed optimization theory, offering insights into how continuous-time approximations can reveal stability and convergence properties of discrete stochastic methods. The connection between theoretical results and empirical observations (although limited in scope) is clearly articulated and helps bridge formal analysis with intuition. The manuscript is technically strong, with a clear structure, consistent notation, and effective conceptual explanations that accompany the mathematical derivations"}, "weaknesses": {"value": "Despite its depth and rigor, the paper lacks substantial empirical support for the theoretical framework. The presented numerical demonstrations are illustrative but minimal, and the claims regarding practical guidance (e.g., normalization requirements, robustness to heavy-tailed noise) would be more convincing if validated on standard distributed-learning benchmarks.\nFurthermore, the practical implications of the analysis (for example, how the derived stability conditions can inform hyperparameter tuning or algorithmic design choices) are only briefly mentioned. A clearer discussion on how practitioners can use the theoretical findings to guide the configuration of distributed systems would strengthen the impact.\nAs a minor point, the title could more explicitly reflect that the focus is on distributed learning"}, "questions": {"value": "Can the proposed analysis be shown to recover or specialize existing known results in simpler settings, such as the single-user (non-distributed) case, or  the uncompressed setting?\nClarifying this would help readers understand how the framework generalizes and extends classical convergence analyses.\n\nAre there examples or empirical indications that the normalization prescriptions derived from the analysis lead to measurable improvements in practical distributed systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dxV4nE12j1", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Reviewer_zQGA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Reviewer_zQGA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830441904, "cdate": 1761830441904, "tmdate": 1762943254010, "mdate": 1762943254010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on non-convex finite sum under the more general $(L_0,L_1)$-smoothness assumption, which relaxes the standard $L$-smoothness condition.  It studies continuous-time analysis via SDE modeling (instead of the standard discrete iteration analysis). Under this setup, the paper analyzes two types of gradient compressors: one is a class of unbiased compressors, and the second is sign-SGD, under assumptions of affine variance/heavy-tailed noise. It argues that standard first and second-order SDE analyses do not accurately reflect the constraints on the learning rate as appear in the standard discrete analysis, and therefore proposes a new second-order SDE model to address this. The paper is primarily theoretical and includes a toy example."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper presents a new SGD model that better reflects the results of discrete analysis under a more general $(L_0,L_1)$-smoothness condition and for affine variance/ heavy-tailed noise.\n* It provides results for two types of compressors, including Sign-SGD, which is a very practical and efficient method."}, "weaknesses": {"value": "* The motivation for moving to a continuous model (instead of a discrete one) could be strengthened, as the authors argue that the classical continuous model is less accurate than the discrete one; therefore, it should be clarified why we prefer to use SDE over the standard discrete analysis in this case.\n* In Theorem 4.3, the term $S_0$ depends on the stochastic variance, which is usually supposed to be independent of it, as in Theorem 4.2. Why does it hold in this case?\n* $M_\\nu$ and $\\mathcal{l}_\\nu$ should be briefly defined in the main paper for readability and not only in the appendix, since they are still a part of the main results of the paper. Could the authors provide a brief explanation of each of them?\n* The proposed model is limited to second-order SDEs, and it’s unclear how to address the inaccuracy of the standard SDE in first-order modeling, which can be better suited for practical non-convex functions. \n* The analysis is limited to finite-sum problems (i.e., guaranteeing minimization of the empirical risk and not generalization), and to a class of unbiased compressors (whereas biased compressors are more common in practice, excluding sign-SGD)."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3oj6qXV6pi", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Reviewer_9dpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Reviewer_9dpY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905755993, "cdate": 1761905755993, "tmdate": 1762943253297, "mdate": 1762943253297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a \"corrected\" continuous-time model for DCSGD and DSignSGD by flipping the sign of the $\\\\frac{\\\\eta}{2}\\\\nabla^2f\\\\nabla f$ term in the classic second-order SDE approximation so that the continuous model reproduces the stability threshold of the step size in the GD. It then derives error bounds for a few settings ($(L_0-L_1)$ smoothness, unbiased compression, and affine variance) for both DCSGD and DSignSGD, and shows toy experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper states assumptions (e.g., $L_0-L_1$-smoothness, unbiased compression, affined variance) well with sufficient amount of related works. \n2. Proofs are straightforward and mostly consistent with stated assumptions (although the explanation is via toy examples).\n3. Overall structure is readable."}, "weaknesses": {"value": "1. Theoretical performance is limited to the error bound of the proposed new SDE approximation only.\n2. The justification for relating the SDE’s approximation to the step-size condition in its error bound is weak. I do not understand why a better approximation should also contain the step size in its performance analysis.\n3. Normalization claim in Fig. 1 is under-specified and unproven. The figure asserts “normalization stabilizes,” but the algorithm is not specified, and no theorem explains the performance of this normalization mechanism.\n4. Some technical proofs are not self-contained.\n5. Toy quartics are insufficient to justify claims about stability and training dynamics."}, "questions": {"value": "1. In Line 918, the authors state that\n> The following theorem formalizes that our new SDE model from Eq. 27 is formally a first-order weak\napproximation for SGD: Its proof is a trivial combination of the arguments in the previous theorem,\nand Lemma 1 and Lemma 2 in (Li et al., 2017).\n\nIn Eq. (70) of Appendix B.1.2, the authors describe the proposed SDE approximation as a second-order model, which contradicts with their above characterization. A careful, step-by-step justification is needed to show that the approximation satisfies Definition 3.4 (Lines 225–228) with $\\\\alpha = 1$ or $2$.\n\nContinue the question, if the authors can only show that the proposed SDE is a first-order model, can the authors explain why a sign flip downgrades the model from second order to first order?\n\n2. What is the technical novelty in authors' analysis compared to previous works? It is not specified in the manuscript.\n\n3. The authors heavily cited the paper (Li et al. 2017). The cited paper explained several SGD variants such as momentum SGD and Nesterov accelerated gradient. Could the authors use their proposed SDE for these SGD variants as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5lRYEvXcGb", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Reviewer_rkAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Reviewer_rkAo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954368569, "cdate": 1761954368569, "tmdate": 1762943252939, "mdate": 1762943252939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Answer"}, "comment": {"value": "Dear Reviewers and AC,\n\nWe sincerely appreciate your time, thorough reviews, insightful comments, and interesting questions regarding our paper. Your feedback has greatly contributed to the finalization of our work.\n\nWe are pleased that **all** Reviewers understood the main **contributions** of this **theoretical** paper:\n\n1. **Fundamental**: We identify **limitations** of existing first- and second- SDE models from the literature: They **fail to capture learning rate restrictions** under $(L_0,L_1)$-smoothness, **even** when the learning rate is infinitesimal;\n2. **Technical**: We derive **novel first**-order SDEs that **remediate** the limitations of classical models;\n3. **Insights**: Under a previously unexplored combination of general regularity and noise assumptions, we derive convergence bounds for the SDE models of DCSGD and DSignSGD. These reveal that DSignSGD **remains stable** even under heavy-tailed noise, while DCSGD requires a normalization factor determined **jointly** by the compression rate, noise variance, and smoothness parameters.\n\n## **What the reviewers appreciated:**\n\n\n### **Presentation and Soundness**\nzQGA scored both as **\"excellent\"**, stating that \"[t]he manuscript is **technically strong**, with a **clear structure**, consistent notation, and **effective conceptual explanations**\": The Reviewer found that we offer a \"**rigorous** and **well-developed** theoretical exploration of distributed stochastic learning\". In particular, \"[t]he connection between theoretical results and empirical observations [...] is **clearly articulated** and helps bridge formal analysis with intuition\". On the same note, rkAo confirms that the \"structure is readable\", that we state our assumptions **well**, and that our \"[p]roofs are straightforward\", echoing ahk4 who judged them \"**rigorous**\".\n\n\n### **Contributions and Novelty**\nWe provide a \"**detailed** theoretical study\" which \"highlights the **limitations** of conventional first- and second-order SDE approximations\" (zQGA). To remediate this, we \"propose *corrected* continuous-time model[s] for DCSGD and DSignSGD\" (rkao) \"that **correctly** capture learning rate constraints [and] stability thresholds\" (zQGA). These provide \"**insightful** conclusions on the interplay of different factors\" (ahk4) such as \"gradient noise, compression, and adaptivity\" (zQGA). Ours is a \"**unified theoretical** framework\" (cisk) that \"derives error bounds for a few settings\" (Rkao) that \"**no prior** [SDE-based] work has worked\" with (cisk). To summarize, \"the paper presents new [models] that better reflect the results of discrete analysis under a more general $(L_0,L_1)$-smoothness condition and for affine variance/heavy-tailed noise\" (9dpY).\n\n\n### **Experimental and practical perspective** \nSeveral reviewers highlighted potential **\"practical guidance\"** (zQGA and cisk), especially on a \"very practical and efficient method\" (9dpY) such as DSignSGD. zQGA requested that we **articulate** this discussion more and complement it with more realistic experiments: We provide this in our revised version. Importantly, we trained an **MLP** (See Figure C.1 and Appendix C.3 for implementation details). These new results exhibit the phenomena prescribed by our theory and already confirmed in our original Figure 1.\n\n## **Main revisions in response to the Reviewers**\n\nWe take all criticisms very **seriously** and have carefully revised the manuscript in response. In brief, we have:\n\n1. **Clarified the SDE perspective and positioning**, elaborating more on why we use continuous-time models and why capturing learning-rate restrictions is important; we now also provide a careful, step-by-step justification of the approximation order and validity of our SDEs, clarify their relation to the framework of Li et al. (2017), and added technical details in the proofs.\n\n2. **Specified the scope more explicitly**, including our focus on empirical risk minimization rather than generalization;\n\n3. **Strengthened the practical discussion**, making the implications for DCSGD and DSignSGD more explicit and adding an **additional MLP experiment** (Figure C.1, Appendix C.3) that confirms, in a more realistic setup, the phenomena predicted by our theory and already visible in Figure 1;\n\n4. **Added technical details**, including additional intermediate proof steps, small notation, and readability refinements.\n\nThese changes primarily concern **clarification, emphasis, and positioning**, rather than modifications to the core framework or the main conclusions. All changes with respect to the original submission are highlighted in *light brown* in the revised manuscript.\n\nOnce again, we are thankful to the reviewers for their constructive feedback. We look forward to the upcoming author–reviewer discussion period.\n\nThank you for your attention.\n\nBest regards,\n\nThe Authors"}}, "id": "xJZxOZ2P8h", "forum": "4H8xZA4zuj", "replyto": "4H8xZA4zuj", "signatures": ["ICLR.cc/2026/Conference/Submission24942/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24942/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission24942/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763681584840, "cdate": 1763681584840, "tmdate": 1763681584840, "mdate": 1763681584840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}