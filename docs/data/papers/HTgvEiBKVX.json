{"id": "HTgvEiBKVX", "number": 16432, "cdate": 1758264431846, "mdate": 1759897241189, "content": {"title": "Leveraging Holistic Explanations to Mitigate Popularity Bias for Recommender Systems", "abstract": "Recommender systems often suffer from popularity bias, where items with high historical engagement ensure a dominant presence in the recommendation lists while equally relevant but less popular items (called niche items) remain under exposed towards majority of the users, thus impacting their reach within mainstream platforms. This bias arises partly due to the learning strategy of existing recommender models which display heavy reliance on interaction frequency and shallow contextual features that characterize any item, which fail to capture the true preferences of any users. To address this, we propose Expl-Debias, a novel framework that leverages holistic explanations to enrich user–item preference modeling and mitigate popularity bias. Expl-Debias operates in two stages: (Stage-1) a base training phase that learns general user–item utility, and (Stage-2) a contrastive explanation-aware training phase that incorporates LLM-generated positive and negative explanations to explicitly guide relevance learning toward personally aligned items and away from popular yet irrelevant ones. Extensive experiments on three real-world datasets demonstrate that our approach significantly improves recommendation accuracy while substantially reducing popularity bias, outperforming state-of-the-art LLM recommendation and debiasing baselines. These results demonstrate that integrating contrastive explanations offers an effective new direction for mitigating popularity bias in recommendation by balancing the tradeoff occurring between the recommendation performance and the negative effect of popularity bias. We provide our code at https://anonymous.4open.science/r/Expl-Pop-Bias-089A/.", "tldr": "", "keywords": ["Large Language Models", "Recommender Systems", "Explainable Recommendation", "Debiased Recommender Systems"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28c04e2292c1dad350631e1e7b4c9681f279faf5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel framework called Expl-Debias to address the common problem of popularity bias in recommender systems (RS), where popular items are disproportionately recommended over equally relevant but less popular \"niche\" items.\n\nThe core idea is to enrich the model's understanding of user preferences by using explanations generated by a LLM. The framework operates in two stages:\n*  A standard recommender model is trained to learn general user-item relevance from historical interaction data\n* The model is then fine-tuned using a contrastive approach. For a given user-item pair, an LLM generates both a positive explanation and a negative explanation. The model is trained to increase the recommendation score when given the positive explanation and decrease it when given the negative one, effectively learning a more nuanced and personalized view of relevance\n\nThe main contribution is a new method for mitigating popularity bias by integrating these contrastive explanations, which helps the system better understand a user's true preferences beyond simple interaction frequency. The authors report that experiments on three real-world datasets show that they significantly improve recommendation accuracy while reducing popularity bias, outperforming other methods"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "originality: using contrastive, LLM-generated explanations to directly combat popularity bias.  While explainability in recommenders is not new, the core novelty lies in the framework's second stage, where it generates both \"pros\" and \"cons\" explanations for a user-item pair, which could presents a creative and previously unexplored mechanism for debiasing\n\nquality: the proposal is evaluated against three datasets. And there is a public link to the code which strenghen's the quality by ensuring reproducibility.\n\nclarity: The paper concisely defines the problem of popularity bias and lays out its proposed solution, Expl-Debias, in a straightforward, two-stage framework that is easy to understand.\n\nSignificance: The work addresses popularity bias, a critical and long-standing challenge in RS that negatively impacts user experience, fairness, and market diversity. The paper's contribution is significant because it proposes a solution that not only mitigates bias but does so while improving recommendation accuracy."}, "weaknesses": {"value": "*The proposed approach outsources the difficult task of \"root cause analysis\" to the LLM. The generated reason is an inference based on the user's history, not a ground truth. If the LLM's inference is wrong, it could introduce noise into the training process. The case study in Table 6 shows an example where the generated negative explanation aligns with the user's review, but this is an anecdote, not a systematic validation of the explanation quality.\n\n* Limited Historical Context: The Expl-Debias approach generates explanations using the user's \"purchasing history\" but doesn't specify how this history is sampled or truncated. The leave-one-out evaluation method it employs typically favors models that are good at predicting the next action based on recent history, which might not be representative of long-term, stable interests. LLM's computation expensive would be a blocker to process a longer horizon of history either.\n\n* Echo Chamber/Filter Bubble: If the LLM only generates explanations that reinforce a user's existing, narrow interests, the system might struggle to recommend novel or diverse items. The paper's main defense against this is its focus on mitigating popularity bias, which is one form of an echo chamber. By promoting niche items that align with a user's \"true preferences,\" it aims to broaden the recommendations. However, it does not explicitly discuss mechanisms for ensuring the diversity of the generated explanations themselves.\n\n*  if we have online experiment a/b testing on an industrial RS platform, the result would be more convincing, because we would be able to tradeoff some more interesting metrics regarding user satisfaction metrics versus popularity bias. RS operates in a live feedback loop where the model's output influences the user's next action, which in turn becomes training data for the next iteration of the model. Offline evaluations cannot capture this dynamic. An online A/B test is the only way to see if reducing popularity bias leads to a virtuous cycle of better discovery and engagement over time."}, "questions": {"value": "* explanations are produced based on per user's positive and negative feedback. From each user, in many recommendation scenarios, positive and negative feedback are not balanced; usually you have more data from one-side only.  Would this create a difficulty for users that do not have much feedback, in other words, would the proposed model only benefit a subset of users that have more feedback history?\n\n* What measures were taken to ensure the quality and factual accuracy of the LLM-generated explanations? Did you perform any human evaluation or use automated metrics to assess them? How does the framework handle potentially noisy or low-quality explanations? Is there a mechanism to weigh or filter them? How do you ensure the generated \"negative explanations\" are genuinely reflective of a user's potential dislike, rather than just being generic negative statements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hsxuuDXZ9r", "forum": "HTgvEiBKVX", "replyto": "HTgvEiBKVX", "signatures": ["ICLR.cc/2026/Conference/Submission16432/Reviewer_yERR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16432/Reviewer_yERR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633104386, "cdate": 1761633104386, "tmdate": 1762926549084, "mdate": 1762926549084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims at alleviating popularity bias in recommender systems by adding holistic explanations (LLM-generated pros/cons) into training. The method is two-stage: (Stage-1) standard BPR utility training, and (Stage-2) contrastive explanation learning that encourages higher scores with positive explanations and discourages with negative explanations. The authors further propose a post-hoc popularity-aware re-ranking algorithm(Appx. B.2) that linearly combines a “positive-minus-negative” score with a “no-explanation” score and enforces a per-user popularity constraint. Experiments on Yelp, Amazon-Beauty, Amazon-Sports report gains in NDCG/HR and reductions in PopRate, KLD, UPC, and Appx. D.4 shows the re-ranking step can further improve both accuracy and fairness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear two-stage formulation and intuitive use of positive/negative explanations\n- Consistent improvements from Stage-2 across three datasets and multiple K \n- Transparent ablations across LLM generators and encoders\n- Metrics for popularity bias are standard and explicitly defined"}, "weaknesses": {"value": "- While the reported results suggest that Stage 2 improves both NDCG and HR, this outcome is somewhat surprising. Even if Stage 2 leads to more “relevant” recommendations of niche items that better match user preferences, the test set itself is likely biased toward popular items, that is, popular items are more frequently observed as held-out (test) items than niche ones. This raises concerns about the simultaneous and significant gains reported in both ranking metrics (NDCG/HR) and debiasing metrics, as these goals can potentially conflict due to the test set distribution. Additionally, it would be valuable to include multiple qualitative examples (beyond Table 6) to give readers a clearer sense of the types of improvements achieved.\n\n- It is somewhat unfair to exclude the related work section from the main manuscript, effectively expecting reviewers to consult the appendix for this essential content. Since related work is an integral part of any scientific paper, omitting it from the main body allows the authors to use that space for additional material that would otherwise need to be excluded, effectively exceeding the 9-page limit. This creates an unfair advantage compared to submissions that included the related work section in the main manuscript, as required. In addition to the point above, it appears that the authors have manipulated the formatting to aggressively condense the text (e.g., lines 438-454 and Tabs. 2-4). Moreover, several important details such as the reranking algorithm are omitted from the main text and deferred to the appendix. This again allows the authors to include more content than would normally fit within the 9-page limit, which undermines the fairness of the submission process.\n\n- A fundamental aspect of explanation quality is fidelity. Specifically, how well the explanation reflects the actual reasoning of the underlying model (i.e., model faithfulness). However, the proposed explanation method appears disconnected from the internal logic of the recommender system. It simply provides the user profile and the recommended item to a LLM, which has no visibility into the actual recommendation model’s internals, only its input and one of its outputs. As a result, it is unclear whether the LLM-generated explanations genuinely justify the recommendation model’s decision or merely generate plausible-sounding rationales. This calls into question the faithfulness of the explanations. To meaningfully assess the fidelity of the LLM-based explanations, the authors should evaluate them against established explanation methods using counterfactual metrics (e.g., see Barkan et al. WWW 2024), or other measures that quantify alignment with the model’s true decision-making rationale. \n\n- Popularity split fixed at top-10% with no robustness check. Fairness metrics directly depend on this boundary. Lack of sensitivity analysis weakens the generality of debiasing claims.\n\n- Can you explain why a simple method that adds a learnable item popularity bias to the BPR formulation cannot effectively alleviate popularity bias? Once trained, recommendations could be generated based solely on the dot product between the user and item embeddings, while discarding or attenuating the learned bias term using a scalar factor. Have you experimented with such an approach, and if so, how does it compare to your proposed method?\n\n- Popularity split fixed at top-10% with no robustness check. Fairness metrics directly depend on this boundary. Lack of sensitivity analysis weakens the generality of debiasing claims. An investigation of the results under varing |V_P| (e.g., 5%, 15%, 20%) would enable the assessment of the sensitivity of themethod to |V_P| using the PopRate/KLD/UPC metrics\n\n- Training-inference mismatch on explanations. If only e⁺ is used at inference, improvements may partly reflect prompted alignment toward positives, while the intended demotion via e⁻ is not exercised, reducing confidence in the claimed holistic mitigation at run time.\n\n- Datasets are Yelp/Beauty/Sports, text-heavy. No cold-start or multimodal evidence. Popularity bias often manifests differently in multimodal or short-history regimes. The absence of such settings limits external validity. It would be helpful to add at least one non-text or cold-start setting to probe robustness. \n\n- The dataset splits in Sec. 4.1 are not detailed to the sufficient level. Does Lns. 271-272  mean that the last item from the history of each user is used for as test? And the second-last (one before the last item) is used for validation? Also, is the same train-val-test splits are the same for both stage 1 and stage 2 (otherwise the improved results obtained by stage 2 can be due to information leak.\n\n- To properly assess the generalizability of the Stage 2 method, it is necessary to evaluate it across multiple recommendation models, not just BPR. Testing on additional architectures such as ALS and NCF would strengthen the empirical evidence and demonstrate the robustness of the approach beyond a single baseline.\n\n\nMinor:\n\n- The appendix should be submitted as a separate supplementary materials file, rather than being included in the main manuscript. In any case, the references section should appear before the appendix.\n\n- The font size in Tables 24 is too small and should be increased for readability."}, "questions": {"value": "- The dataset splits described in Section 4.1 are not detailed sufficiently. Do lines 271–272 mean that the last item in each user's interaction history is used as the test item, and the second-to-last item is used for validation? Additionally, are the same train/validation/test splits used for both Stage 1 and Stage 2 of the training pipeline? If not, the improved results reported in Stage 2 could be partially due to information leakage, which would undermine the fairness of the comparison.\n\n- It is not very clear how ranking is performed after stage 2 training is over? Is it simply based on the same difference as appear in the training objective (Eq. 7)?\n\n- Can you evaluate with both e⁺ and e⁻ active at inference (or justify why e⁺ only is sufficient for fairness)?\n\n-  The method relies heavily on the correctness of the LLM-generated output. What measures are taken to mitigate or detect hallucinations by the LLM? \n\n- The reviewer is a bit confused. Algorithm 1 computes NDCG@K for multiple α values and picks α*_u maximizing NDCG subject to τ. However, in production, NDCG labels are unavailable per user at ranking time. Thus, reported re-ranking gains in Tab. 8 may not transfer to real-world settings, limiting the practical impact of the strongest results. If the reviewer did not understand this point, please clarify why is it a valid approach. Also, can you report results with global α (single value for all users) or an α predicted by a learned model without ground truth at ranking time? This would show deployability. \n\n- Finally, see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xYVdnjVWuC", "forum": "HTgvEiBKVX", "replyto": "HTgvEiBKVX", "signatures": ["ICLR.cc/2026/Conference/Submission16432/Reviewer_9HBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16432/Reviewer_9HBS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844149132, "cdate": 1761844149132, "tmdate": 1762926548500, "mdate": 1762926548500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses one of the most persistent issues in recommender systems — popularity bias, where popular items dominate recommendations while niche but relevant items remain underexposed. The authors propose Expl-Debias, a two-stage debiasing framework that leverages holistic explanations to enhance user–item preference modeling. In Stage 1, a base recommender learns general user–item utilities through traditional Bayesian Personalized Ranking (BPR). In Stage 2, the model incorporates contrastive learning based on LLM-generated positive and negative explanations, guiding the model to promote personally relevant items and suppress irrelevant popular ones. Experiments conducted on multiple benchmark datasets show that the proposed method improves both recommendation accuracy and fairness compared to several state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new perspective on debiasing by explicitly incorporating positive and negative textual explanations into the model training process, rather than using explanations only for post-hoc interpretation. This “contrastive explanation learning” approach is innovative and effectively bridges the gap between explainability and fairness.\n2. The two-stage design is intuitive and clearly structured and the post-hoc popularity-aware re-ranking algorithm provides an elegant balance between maintaining recommendation performance and controlling exposure bias.\n3. Extensive experiments on three real-world datasets with multiple LLM-based baselines show consistent improvements across both accuracy and fairness metrics. The paper includes detailed ablation studies on explanation generators and encoders, demonstrating robustness."}, "weaknesses": {"value": "1.The paper relies on LLM to produce positive and negative explanations but lacks sufficient evaluation of their accuracy and consistency. Without verifying whether these explanations are factually correct or stable across runs, the model’s reliability and fairness in real-world scenarios remain questionable.\n\n2.The paper does not clearly explain how contrastive learning between positive and negative explanations optimizes user–item representations. It remains unclear how embeddings change after Stage-2 training or whether they truly capture explanation-aware semantics. \n\n3.The goal of reducing popular-item over-promotion (Equation 3) is not formally modeled during Stage-2 training. The framework achieves debiasing only implicitly through explanation contrast, without a quantitative derivation of how explanation signals influence item weighting. This lack of mathematical clarity weakens the interpretability of the proposed mechanism.\n\n4.The experimental design and baselines raise concerns. The Stage-1 BPR model performs unexpectedly well, sometimes surpassing LLM-based baselines, suggesting that the chosen baselines may be outdated or insufficiently optimized. Moreover, evaluations use relatively small top-K values (K={2,...,5}), which may exaggerate improvements. \n\n5.In Section 5.2, the paper validates the promotion effect of positive explanations using 1% randomly selected niche items, yet the definition of “niche” is unclear. There is no quantitative criterion for selection, nor an analysis across different niche types. This omission makes it difficult to judge whether the model truly identifies valuable underexposed items."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YEo6Piyml5", "forum": "HTgvEiBKVX", "replyto": "HTgvEiBKVX", "signatures": ["ICLR.cc/2026/Conference/Submission16432/Reviewer_cLcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16432/Reviewer_cLcf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846124356, "cdate": 1761846124356, "tmdate": 1762926547736, "mdate": 1762926547736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Expl-Debias, a two-stage framework designed to mitigate popularity bias in recommender systems. The first stage establishes a baseline recommendation utility using a standard BPR loss function. The second, more crucial stage, enhances this by employing a contrastive learning approach that leverages LLM-generated positive and negative explanations for each user-item pair, thereby tuning the model to distinguish between personally relevant and merely popular items."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core contribution lies in its novel application of both positive and negative explanations to directly model user preferences in Stage 2. This holistic, explanation-aware approach provides a richer and more nuanced signal for why an item should or should not be recommended, moving beyond simple interaction data and tackling the root cause of popularity bias—a shallow understanding of user intent.\n\n- The proposed Expl-Debias framework is logically structured and practical. Stage 1 grounds the model with fundamental ranking capabilities, while Stage 2 elegantly refines these preferences using fine-grained explanations.\n\n- Extensive experiments across three real-world datasets is conducted to evaluate performance of Expl-Debias with multiple ranking and fairness metrics. The inclusion of a case study and detailed ablation studies on different components like LLM generators and encoders (Appendices D.2, D.3) adds significant depth and credibility to their findings, demonstrating the framework's robustness."}, "weaknesses": {"value": "- Stage 2 operates on a new data source (positive vs. negative explanations), but the core mechanism of pairwise ranking is not fundamentally new. This makes the technical contribution feel more like an incremental improvement over a well-established method rather than a completely new paradigm.  \n- The paper primarily compares against LLM-based recommenders and simple debiasing wrappers (FairIPS, FairPrompt). However, the field of bias mitigation in recommender systems is vast and includes many powerful, non-LLM techniques. The comparison would be significantly strengthened by including state-of-the-art baselines from other relevant domains, such as graph-based methods that model user-item connectivity or other contrastive learning frameworks designed specifically for debiasing, which could offer more competitive performance.\n- The experiments report results for very short cut-off values (K=2, 3, 5). In real-world applications, users often browse longer lists of recommendations. Failing to report performance on longer lists (e.g., K=10, 20) is a significant omission. A method's effectiveness and fairness might change at different list depths, and robust evaluation requires assessing performance across a wider range of cut-offs.\n- The proposed Expl-Debias method is given access to rich explanations, which serve as a powerful additional input. However, the baseline models are not provided with this information. This creates an \"apples-to-oranges\" comparison, where it is difficult to distinguish whether the performance improvement comes from the novel contrastive framework itself or simply from the advantage of having access to superior input data. For a fairer comparison, baselines should have been adapted to also leverage these explanations, perhaps in a more straightforward manner, to isolate the true contribution of the proposed learning strategy.\n- As popularity bias is often most severe in large-scale, extremely sparse environments where the vast majority of items have very few interactions. The current evaluation (# of reviews only 0.3M) does not sufficiently stress-test the method's ability to promote niche items under these more realistic and challenging conditions. Furthermore, the computational cost of generating and encoding LLM-based explanations for every user-item interaction could become a significant bottleneck on datasets with millions of users and items, an aspect of scalability that the current experiments do not address."}, "questions": {"value": "1. More specific details on the implementation of the baseline models (TALLRec, CoLLM, LLaRA) and debiasing strategies (FairIPS, FairPrompt) are expected? For reproducibility and to confirm the fairness of the comparison, it would be helpful to know the exact hyperparameter settings used for each baseline after tuning, and a confirmation that they were optimized as rigorously as the proposed model.\n    \n2. The process of generating and encoding explanations for every user-item pair seems computationally expensive. Please provide a discussion of the computational complexity, both theoretically (e.g., in Big-O notation) and practically? It would be highly valuable to see a report on the actual wall-clock time and hardware resources required for the explanation generation/encoding steps and for the end-to-end training of Expl-Debias, benchmarked against the baseline models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DxHp5hlAin", "forum": "HTgvEiBKVX", "replyto": "HTgvEiBKVX", "signatures": ["ICLR.cc/2026/Conference/Submission16432/Reviewer_rzYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16432/Reviewer_rzYg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986937411, "cdate": 1761986937411, "tmdate": 1762926546709, "mdate": 1762926546709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}