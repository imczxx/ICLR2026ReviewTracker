{"id": "mq43BAAos0", "number": 3940, "cdate": 1757570505285, "mdate": 1759898061949, "content": {"title": "DiffuPhyGS: Text-to-Video Generation with 3D Gaussians and Learnable Physical Properties via Diffusion Priors", "abstract": "Generating realistic 3D object videos is crucial for virtual reality and digital content creation. However, existing methods often struggle to achieve high-quality appearance and physics-aware motion, relying on manual inputs and pre-existing models. To address these challenges, we propose DiffuPhyGS, a novel framework that generates high-quality 3D objects with realistic and learnable physical motion directly from text prompts. Our approach features an LLM-Chain-of-Thought-based Iterative Prompt Refinement (LLM-CoT-IPR) method, which obtains prompt-aligned 2D and multi-view 3D diffusion priors to guide Gaussian Splatting (GS) to generate 3D objects. We further enhance 3D generation quality with a Densification-by-Adaptive-Splitting (DAS) mechanism. Next, we employ a material property decoder that utilizes a Mixture-of-Experts Material Constitutive Models (MoEMCMs) to predict the mixed material properties of the 3D object. We then apply the Material Point Method (MPM) to deform 3D Gaussian kernels, ensuring physics-grounded motion guided by implicit and explicit physical priors from the video diffusion model and a velocity loss function. Extensive experiments show DiffuPhyGS outperforms other methods in generating realistic physics-grounded motion across diverse materials.", "tldr": "", "keywords": ["Text-to-Video", "Gaussian Splatting", "Diffusion Model", "Dynamic 3D Generation", "LLM"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/777118cebfcc7bd9c153b5d79ff0f90db22ec799.pdf", "supplementary_material": "/attachment/0e4be4e1219c05d25cc845b5b239c822f564fca8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DiffuPhyGS, a framework that can generate dynamic 3D objects with physical information from text prompts. The improvements mainly include: a prior part, which proposes LLM-CoT-Iterative Prompt Refinement to make the prior more aligned and detailed; a visual modeling part, which proposes Densification-by-Adaptive Splitting to improve the shape and appearance quality of GS modeling; and a motion modeling part, which proposes a Mixture-of-Experts Material Constitutive Model to encompass different physical properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem this paper aims to solve is critical and important, considering that most current video & 3D & 4D generation lacks physical explanation and rationality.\n2. The paper improves upon existing methods from several aspects (i.e., prompt refinement, visual modeling, motion modeling). This consideration is comprehensive, but there may also be incremental."}, "weaknesses": {"value": "1. I think the core issue is the visual quality. I watched all the qualitative comparisons and videos provided in the authors' supplementary material. These results have many flaws, and I feel the physical properties do not meet expectations. For example:\nThe various parts of the hamburger all have a similar jelly-like texture, lacking the specific texture of each part.\nThe physical effects of the jar are very strange, feeling more like sand.\n2. I didn't see a significant visual improvement of the method compared to other baselines. The quantitative results have the same problem.\n3. My personal feeling is that the various modules proposed in the paper are not a holistic and critical improvement, but only some small, incremental improvements, and the effect of each component is difficult to be convincing given the current number of cases and visual quality.\n4. I think that a more important task for binding physical properties to GS is editing, for example, after generating an object, whether reasonable physical effects can be produced by initializing different heights or rotating a certain angle, or applying different forces (such as stretching). This paper (or similar papers) lacks these experimental results."}, "questions": {"value": "1. Why can the FVD metric be computed among the qualitative metrics, given that there are no ground-truth videos?\n\n2. Why are the first three metrics exactly the same in the ablation studies of w/o Velocity Loss and w/o Video Diffusion Prior? This seems illogical, since both the noise sampling and optimization process of SDS introduce randomness and uncertainty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "If3ulB66FH", "forum": "mq43BAAos0", "replyto": "mq43BAAos0", "signatures": ["ICLR.cc/2026/Conference/Submission3940/Reviewer_u1ud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3940/Reviewer_u1ud"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462400888, "cdate": 1761462400888, "tmdate": 1762917103643, "mdate": 1762917103643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffuPhyGS, a framework that generates high-quality 3D objects with realistic and learnable physical motion. The authors proposed using LLM-CoT-IPR and SDS to generate high-quality 3DGS objects and employ MPM to optimize the final 4D dynamic results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper presents a complete pipeline from text prompts to 3DGS generation, and finally to 4D motion generation."}, "weaknesses": {"value": "1. Lack of novelty: The techniques used in the paper, including LLM-CoT-IPR, 2D-SDS, MV3D-SDS, and MPM-based 4D-SDS, are all from existing methods, making it difficult to identify any technical contributions in the proposed approach.\n2. Insufficient experiments: The authors only used 5 cases for comparison, and the types of motions generated are too simple, with most of them involving objects falling from a high place."}, "questions": {"value": "1. Velocity Loss: How is the expected velocity change calculated? If the MPM simulation is already based on physical equations, why does the result not align with the expected velocity?\n2. Quantitative Evaluation: How are the ground-truth results obtained for the evaluated cases? For example, in Figure 3, the rubber burger is unrealistic in real-world settings. How can the authors determine which generated result is more reasonable across different methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1euJdUoTrP", "forum": "mq43BAAos0", "replyto": "mq43BAAos0", "signatures": ["ICLR.cc/2026/Conference/Submission3940/Reviewer_LSPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3940/Reviewer_LSPU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643624006, "cdate": 1761643624006, "tmdate": 1762917103413, "mdate": 1762917103413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DiffuPhyGS presents an end-to-end text-to-video pipeline that generates 3D Gaussian objects with physics-driven motion. The system refines prompts using an LLM loop, stabilizes geometry with multi-view diffusion guidance and Densification-by-Adaptive-Splitting, and learns per-Gaussian material mixtures. Motion is driven through an MPM simulator using both implicit diffusion cues and an explicit velocity loss. On a small set of handcrafted prompts, the method reports higher metrics scores and is preferred in a small user study over PhysDreamer, OmniPhysGS, and PhysGaussian."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles joint generation of text-aligned appearance and dynamics in a single 3D Gaussian framework, combining prompt processing, diffusion-based 3D synthesis, and differentiable physics.\n- Leveraging video diffusion priors with Score Distillation Sampling (SDS) to model motion and material is an interesting idea."}, "weaknesses": {"value": "- The evaluation is very limited, four template prompts plus qualitative figures, without standardized datasets or complex multi-object interactions. It’s hard to assess generalization beyond curated cases.\n- Baselines depend on geometry produced by DiffuPhyGS, and the main metrics blend appearance and motion. There are no physics-grounded metrics, so claims of physical fidelity aren’t well supported.\n- The 3D generation component lags behind recent text-to-3D/image-to-3D methods. SDS-based approaches tend to produce lower quality and run slowly for both geometry and appearance, which makes it hard to judge the pipeline’s full potential since results are limited by the underlying generator.\n- There isn’t a clear novelty claim on the 3D generation side. Using 2D and multi-view diffusion with SDS for 3D Gaussians has been explored before, and the LLM-CoT-IPR module mainly uses an existing LLM to refine prompts without introducing new techniques.\n- I would suggest the authors apply the motion and material learning techniques on more recent and higher-quality 3D generation methods such as Trellis[1].\n\n[1] Structured 3D Latents for Scalable and Versatile 3D Generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ja9CjYNkQ3", "forum": "mq43BAAos0", "replyto": "mq43BAAos0", "signatures": ["ICLR.cc/2026/Conference/Submission3940/Reviewer_hkzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3940/Reviewer_hkzr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996162986, "cdate": 1761996162986, "tmdate": 1762917103218, "mdate": 1762917103218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DiffuPhyGS, a framework for generating high-quality 3D objects and realistic physical motion from text prompts. It addresses some limitations in current text-to-video generation methods concerning visual appearance and physical behavior. \nOne of the central innovations of DiffuPhyGS is the LLM-CoT-IPR method, which employs a stepwise prompting refinement approach using large language models (LLMs). This technique enhances the alignment between textual inputs and the generated content, ensuring that the outputs closely adhere to the original prompts. Additionally, the framework incorporates a Hybrid Expert Material Constitutive Model (MoEMCM) that accurately predicts the properties of heterogeneous materials. This integration allows for improved fidelity in physical simulations, enhancing the realism of the generated objects."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Originality\n- The integration of LLM-CoT-IPR into the generation process effectively leverages textual information.\n- The combination of multi-view diffusion priors, video diffusion priors, and predictive models for physical properties establishes an efficient pipeline for generating videos that adhere to physical laws from text inputs.\n- Unlike previous methods that manually incorporate physical properties, the introduction of Mixture-of-Experts Material Constitutive Models (MoEMCMs) allows for adaptive estimation of physical properties for local Gaussian primitives.。\n### Quality\n- The experimental metrics are set up broadly, considering various factors.\n- The ablation study is well-structured and effectively demonstrates the impact of components such as LLM-CoT-IPR, velocity loss, and the Mixture-of-Experts Material Constitutive Models (MoEMCMs) on the generation results. The qualitative comparisons provided in the appendix are particularly noteworthy.\n### Clarity\n- There are no significant grammatical or spelling errors; the writing is relatively clear.\n### Significance\nThe research direction of this paper holds practical significance for video synthesis and augmented reality applications."}, "weaknesses": {"value": "The qualitative results do not always demonstrate an advantage over the quantitative metrics, which significantly diminishes the confidence and impact of the paper.\nThere is a lack of comparison regarding efficiency and memory usage.\nThere are uncertainties regarding the specific implementation of LLM-COT-IPR; does it participate in the optimization of video generation?"}, "questions": {"value": "1. Why is there no direct comparison with existing video models to determine whether they can accurately represent the corresponding physical laws?  \n2. In Table 1, there are multiple comparisons with baseline metrics; however, outside of the average metrics, there is no significant advantage in the individual metrics. Furthermore, how does OmniPhysGS achieve a score of 0.2 for the FVD metric on Jelly, which significantly surpasses all other methods?  \n3. In Figure 3, the \"Pancakes melting\" example displays a noticeable scale deformation. What causes this? Is it related to the method employed?  \n4. The user study results are considerably better than the baseline; could these be included in the main text? This might enhance the persuasiveness of the paper.  \n5. Could the authors summarize the fundamental differences between the method proposed in this paper and previous methods? Particularly concerning OmniPhysGS, this would help me gain a clearer understanding of the contributions of this paper.  \n6. How do the various methods compare in terms of efficiency and memory usage? It would be beneficial to present this comparison in a table.  \n7. I have some questions regarding the specific implementation of LLM-COT-IPR. According to Algorithm 1, it appears to involve scoring images. I am curious whether LLM-COT-IPR participates in the iterative process of video generation or is limited to optimizing image prompts.  \n\nIf the authors can effectively address my concerns, I would consider improving my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qNCpRfuDsV", "forum": "mq43BAAos0", "replyto": "mq43BAAos0", "signatures": ["ICLR.cc/2026/Conference/Submission3940/Reviewer_KzvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3940/Reviewer_KzvT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762489107775, "cdate": 1762489107775, "tmdate": 1762917103015, "mdate": 1762917103015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}