{"id": "ZmhRSFFiun", "number": 3493, "cdate": 1757447298852, "mdate": 1759898086791, "content": {"title": "Targeted perturbations reveal brain-like local coding axes in robustified, but not standard, ANN-based brain models", "abstract": "Artificial neural networks (ANNs) have become the de facto standard for modeling the human visual system, primarily due to their success in predicting neural responses. However, with many models now achieving similar predictive accuracy, we need a stronger criterion. Here, we use small-scale adversarial probes to characterize the local representational geometry of many highly predictive ANN-based brain models. We report four key findings. First, we show that most contemporary ANN-based brain models are unexpectedly fragile. Despite high prediction scores, their response predictions are highly sensitive to small, imperceptible perturbations, revealing unreliable local coding directions. Second, we demonstrate that a model's sensitivity to adversarial probes can better discriminate between candidate neural encoding models than prediction accuracy alone. Third, we find that standard models rely on distinct local coding directions that do not transfer across model architectures. Finally, we show that adversarial probes from robustified models produce generalizable and semantically meaningful changes, suggesting that they capture the local coding dimensions of the visual system. Together, our work shows that local representational geometry provides a stronger criterion for brain model evaluation. We also provide empirical grounds for favoring robust models, whose more stable coding axes not only align better with neural selectivity but also generate concrete, testable predictions for future experiments.", "tldr": "Brain models are susceptible to small-scale noise patterns", "keywords": ["brain encoding models", "robustness", "adversarial perturbations", "NeuroAI"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fd3d66479102b75d09e4fab2a54bc34179eb897.pdf", "supplementary_material": "/attachment/7b1eb143a74598d249488544d89cab7cdcde2dfc.zip"}, "replies": [{"content": {"summary": {"value": "This study uses adversarial perturbation methods to investigate how different brain models local representational geometry. The paper presents four main findings. First, even when different models achieve similar voxel-response prediction accuracy, their local geometries differ substantially and are highly sensitive to adversarial perturbations. This suggests that adversarial sensitivity may be a more informative indicator than prediction accuracy. Second, different models represent local geometry in distinct subspaces. Finally, the authors show that robustified models can better explain neural representations in the brain."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This study creatively applies adversarial perturbation to examine the local geometry of neural representations, which, to my knowledge, has not been systematically explored before.\n2. The overall methodology and analyses are solid with no obvious technical flaws.\n3. The writing is clear, and the logical flow is easy to follow."}, "weaknesses": {"value": "1.\tThe figures are poorly prepared—the text is barely legible, and the resolution is low. This seriously affects readability.\n2.\tThe core logic of the argument could be reconsidered to strengthen the conceptual contribution.\n3.\tIn Section 4, the rationale for how new stimuli can be used to test neuroscience theories is not clearly articulated."}, "questions": {"value": "1. The study is very interesting and provides a valuable example of how brain representations of images can be analyzed through adversarial perturbations. However, the results are not entirely surprising. The voxel encoding model linearly maps ANN features to voxel responses. Therefore, the adversarial sensitivity of predicted brain responses should largely depend on the adversarial sensitivity of the ANN features themselves. Since we already know that standard ANNs (without adversarial training) are fragile, it follows naturally that the voxel responses derived from them would also be fragile.\n\n2. In Section 2, the authors claim that adversarial sensitivity distinguishes between models better than prediction accuracy. Indeed, Figure 2A seems to support this. However, it is unclear why higher sparseness indicates that adversarial sensitivity is a better metric—the theoretical justification for this link should be elaborated.\n\n3. From Figure 2A, BLIP2 appears to perform better in terms of adversarial sensitivity. Why is that the case? Can we therefore conclude that BLIP2 is a better brain model overall? If so, what is the underlying reason for this advantage?\n\n4. In Section 4, the authors suggest using adversarial images as new stimuli to probe which features different brain regions respond to. However, the examples provided remain quite intuitive (e.g., that FFA responds more to face-like features) and do not yet seem to offer novel neuroscientific insights. Further discussion on how this approach could generate non-trivial discoveries would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L31TigKsbU", "forum": "ZmhRSFFiun", "replyto": "ZmhRSFFiun", "signatures": ["ICLR.cc/2026/Conference/Submission3493/Reviewer_Qijk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3493/Reviewer_Qijk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915720941, "cdate": 1761915720941, "tmdate": 1762916756096, "mdate": 1762916756096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose to use adversarial examples as a method to probe the neural predictivity of ANN-based models, along with the widely used linear regression neural prediction score. The paper reports following claims (1) ANN-based brain models can have similar neural prediction score but different sensitivity to adversarial probes, (2) Adversarial sensitivity have stronger correlation to internal model measurement such as sparseness and normalized variance, (3) Adversarially trained model share common perturbation subspace, while standard models do not, (4) adversarially trained models produces more semantically meaningful adversarial probes to the inputs. The analysis use fMRI dataset on Natural Scene Dataset, and a collection of CNN-based image models (VGG16, DenseNet, ResNet-50 Robust)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-organized, with clear section organization and good figures for the readers to follow. The motivation is clear and well-defined, as is, how to address the problem that multiple ANN-based models have similar neural prediction score. The paper proposes to use adversarial probes as the approach to offer further discrimination between different models, and show that while models can have similar neural prediction score, they can have different sensitivity level to local perturbation produce by adversarial probes (Fig.2).\n\nThe experiments are clear and easy to understand. And the experimental results support the main claim that local perturbation geometry, as measured by the adversarial probes, can reveal additional information about the models further than the neural prediction score (sparseness and normalized variance in Fig 2C, transferability and alignment of the local geometry in Fig 3, and adversarial probes of robust models embeds more robust features.\n\nHowever, despite these strengths, 3/4 main claims of the paper lacks novelty, and has been shown in previous works, which the paper did not explicitly mention. I will provide more details information about this limitation of the paper in the \"Weakness\" section"}, "weaknesses": {"value": "While the paper attempts to address a well-defined concern, that is, multiple ANN-based models have similar neural prediction score, the main contributions (3/4) have been seen in various previous works in both brain-based ANNs models and adversarial robustness literature, which the paper did not explicitly mention these previous contribution. Previous relevant works for each main claims are listed below:\n1. Claim 1: Most contemporary ANN-based brain models are unexpectedly fragile, response predictions are highly sensitive to small, imperceptible perturbation, revealing unreliable local coding directions\n\t1. Adversarial examples are discovered in the seminal work of Szegedy et al, Intriguing properties of neural networks, which show that imperceptible perturbation to the inputs can lead large change in the model outputs, which is cited by 20000 times and developed into a well-defined research topic. Therefore, it's difficult to consider the paper findings that \"ANN-based brain models are unexpectedly fragile, response predictions are highly sensitive to small, imperceptible perturbation\" a surprising, novel or significant claims\n\t2. Related works:\n\t\t1. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_.\n2. Claim 2: Model's sensitivity to adversarial probes can better discriminate between candidate neural encoding models than prediction accuracy alone\n\t1. There's a significant body of works in the adversarial robust and brain-inspired model literature has shown that adversarially trained models have higher alignment with neural response. And in turn that incorporating brain-like features also improve adversarial robust of the model (Guo et al, Dapello et al, Dapello et al). In the machine learning literature, there are many papers reports that models with the same clean accuracy can have different robust accuracy (Goodfellow et al). Therefore, it's difficult to consider claim 2 as novel or significant.\n\t2. Related works:\n\t\t1. Guo, C., Lee, M., Leclerc, G., Dapello, J., Rao, Y., Madry, A. and Dicarlo, J., 2022, June. Adversarially trained neural representations are already as robust as biological neural representations. In _International Conference on Machine Learning_ (pp. 8072-8081). PMLR.\n\t\t2. Dapello, J., Kar, K., Schrimpf, M., Geary, R., Ferguson, M., Cox, D.D. and DiCarlo, J.J., 2022. Aligning model and macaque inferior temporal cortex representations improves model-to-human behavioral alignment and adversarial robustness. _BioRxiv_, pp.2022-07.\n\t\t3. Dapello, J., Marques, T., Schrimpf, M., Geiger, F., Cox, D. and DiCarlo, J.J., 2020. Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations. _Advances in Neural Information Processing Systems_, _33_, pp.13073-13087.\n\t\t4. Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_.\n3. Claim 3: Standard models rely on distinct local coding directions that do not transfer across model architectures\n\t1. The authors show that adversarial examples generated by adversarially trained models have higher transferability and alignment than the ones generated by standard models, which I found quite surprising given that there are many previous works in the ML community study the transferability of adversarial examples across different models, that is, adversarial examples created by a model (for example, ResNet-50), can be used to attack another model (for example, VGG-16) (Gu et al 2023). I would be great if the authors can comment on these discrepancy between their findings and previous literature.\n\t2. Related works:\n\t\t1. Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z. and Yuille, A.L., 2019. Improving transferability of adversarial examples with input diversity. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (pp. 2730-2739).\n\t\t2. Tramèr, F., Papernot, N., Goodfellow, I., Boneh, D. and McDaniel, P., 2017. The space of transferable adversarial examples. _arXiv preprint arXiv:1704.03453_.\n\t\t3. Gu, J., Jia, X., de Jorge, P., Yu, W., Liu, X., Ma, A., Xun, Y., Hu, A., Khakzar, A., Li, Z. and Cao, X., 2023. A survey on transferability of adversarial examples across deep neural networks. _arXiv preprint arXiv:2310.17626_.\n4. Claim 4: Adversarial probes from robustified models produce generalizable and semantically meaningful changes, suggesting that they capture local coding dimensions of visual systems\n\t1. In Figure 4, the authors show that adversarial probes created by robust model elicit more meaningful change to the inputs than the probes generated by standard models, and therefore can be used to design stimulus to modulate neural responses. About using artificial probes to modulate neural responses, Bashivan et al 2019 has shown that ANNs can be used to generate synthetic stimulus that trigger responses of specific neurons. About robust models encode more robust features, Ilyas et al 2019 has shown that both theoretically and empiricially, adversarially trained models learn more robust and interpretable features.\n\t2. Related works:\n\t\t1. Bashivan, P., Kar, K. and DiCarlo, J.J., 2019. Neural population control via deep image synthesis. _Science_, _364_(6439), p.eaav9436.\n\t\t2. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B. and Madry, A., 2019. Adversarial examples are not bugs, they are features. _Advances in neural information processing systems_, _32_."}, "questions": {"value": "1. Fig 2C. What is the definition of sparseness (is this the ratio of activated units) and normalized variance? What is the measurement reported here (0.18 and 0.06) (is it correlation)?\n2. Fig 3B. What is the unit of the color bar? Is it L2 distance of the total change in neural response across M units?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bIeoE7xT9e", "forum": "ZmhRSFFiun", "replyto": "ZmhRSFFiun", "signatures": ["ICLR.cc/2026/Conference/Submission3493/Reviewer_auvF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3493/Reviewer_auvF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936894093, "cdate": 1761936894093, "tmdate": 1762916755879, "mdate": 1762916755879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an image model’s robustness to noise as a discriminating factor among equally predictive neural encoding models in fMRI. They perform several analyses to better understand how robustness relates to encoding models, such as whether similarly predictive models share local adversarial directions (coding axes). This work adds to previous findings that show a tight coupling between neural encoding model robustness and robust activations in the brain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Interestingly, the paper finds a number of neural models that show different interpretations of coding axes and provides an insightful data point on how underdetermined current state of the art neural encoding models are. The perturbation analysis is neat, it provides a clean way of measuring and comparing neural population sensitivity using a sensitivity subspace. In total, the paper explores an interesting question and demonstrates a novel analysis that should interest the computational neuroscience community."}, "weaknesses": {"value": "The main weaknesses are that it analyzes only fit encoding networks with minimal links back to the brain, and it uses identical architectures for its “robust” models, which dilutes the impact of the results."}, "questions": {"value": "One question I had while reading is to what extent the authors believe model sensitivity discrimination is distinct from neural predictivity. If the NSD stimulus set contained adversarial examples, such as small noise perturbations on top of images, then the current equally predictive models mentioned in the paper would likely separate in predictiveness. It could be that the most predictive encoding model is underdetermined by this dataset size rather than sensitivity being a distinct separation axis from predictivity.\n\nThe authors claim that robustness provides better discriminability than predictiveness alone. It is not clear that this is meaningful. These networks could also be cleanly separated by parameter count, training distribution, or any other meaningful difference in the model. The relation back to brain activity could be valuable, for example showing that robust models predict neural activity better in out of distribution cases, however the paper does not explore this. I find their claim 2 to be fairly weak because of this. Could the authors provide more evidence that sensitivity reflects a meaningful difference in neural activity rather than just discriminating between trained models.\nWhile interesting, it is not clear that the analysis here provides meaningful interpretations for the brain. For example, the finding in Figure 3E that the coding axes of the robust networks are more similar to each other than those of the nonrobust networks is not necessarily an indication of neural encoding, and could instead indicate network similarity. These robust ResNet 50 models may simply be more similar to each other in training protocol, data, and architecture than the other models being compared.\n\nThe authors could strengthen their claim by addressing the following.\n\n1. Can the authors relate their results to a testable prediction about neural activity. If the robust models more accurately capture true neural coding axes, they should have better generalization. One suggestion is to remove a distinct group of objects, for example images with faces, from training and evaluate on them during testing with robust and nonrobust models. Another is to examine scaling laws of encoding performance.\n2. Could the authors provide results for a robustified image model that does not share the same architecture as the ones used here? There is a confound in the claim that robust models share more generalizable coding axes, since to my understanding they share the exact same architecture and differ only in adversarial objective strength."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fhcqiZU34h", "forum": "ZmhRSFFiun", "replyto": "ZmhRSFFiun", "signatures": ["ICLR.cc/2026/Conference/Submission3493/Reviewer_ukS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3493/Reviewer_ukS1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956978442, "cdate": 1761956978442, "tmdate": 1762916755659, "mdate": 1762916755659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this submission, the authors present a quantitative analysis of the adversarial sensitivity of various encoding models of human vision, including adversarially trained encoding models. The authors highlight that robustness as a metric shows improved distinction across encoding models, making them a more suitable metric to rank the human-likeness of encoders. In addition, the authors also show that robustified ANNs produce relatively stronger transferrable perturbations highlighting that they pay attention to interpretable, object-centric image features. Their results suggest that adversarial sensitivity is correlated with similarity of encoding models to biological vision, and support the inclusion of adversarial robustness measurements to enhance our understanding of biological vision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- The current submission tackles a very critical problem of developing novel metrics to better distinguish among encoding models of human vision. There are plenty of options to choose an encoding model, all of which show little inter-model variability in prediction accuracy. There is a clear need for complimentary methods to vet models in their ability to encode human-like visual representations.\n- From a brief review of other works in this area, I agree with the authors that there is a scarcity of methods to better distinguish between encoding models of the brain apart from neural predictivity. Hence, I think the proposed direction addresses a pressing issue in the NeuroAI community.\n- The paper is written really well; the figures are very well done to clearly illustrate a few ambiguous concepts like the distinctness of perturbation spaces through simple schematics (e.g. Fig. 3).\n- I found Fig. 4 to be quite interesting in that there is a marked qualitative difference between adversarial perturbations generated for robustified models."}, "weaknesses": {"value": "Weaknesses:\n- Confound between adversarial training and adversarial sensitivity.\nIn their first contribution (Sec. 3-1), the authors compare the adversarial sensitivity of robustified and non-robustified ANNs to show that the former is less sensitive to adversarial perturbations. They use this result to highlight that non-robustified ANNs are brittle compared to robustified ones. This is not a novel finding; given the motivation of adversarial training is to reduce sensitivity to such perturbations.\n- On the enhanced variance of noise sensitivity over neural predictivity.\nA good metric is characterized by its ability to show individual differences between models. That said, I believe adding adversarial perturbations injects OOD noise into the images, hence increasing response instability across models. Hence, while I agree that adversarial sensitivity better separates models as a metric, I don't think this finding is also particularly novel / informative.\n- Transferability of adversarial attacks.\nThe authors here claim that adversarial probes do not transfer across models, i.e., they all have distinct perturbation spaces. I believe this claim is not well supported given there have been many techniques in the adversarial machine learning community that have been designed to improve adversarial attack transferability (see [1]). Hence, I think the lack of transferability here is not ONLY a trait of the encoding models, but it is also caused by the specific adversarial optimization algorithm in question.\n\nReferences:\n1. Gu, J., Jia, X., de Jorge, P., Yu, W., Liu, X., Ma, A., ... & Torr, P. (2023). A survey on transferability of adversarial examples across deep neural networks. arXiv preprint arXiv:2310.17626."}, "questions": {"value": "Additional notes:\nOverall, I feel the presented results seem to all point to the improved adversarial robustness of robustified ANN models, and the improved distinguishability of encoding models based on adversarial sensitivity. I think these results are interesting, but I would love to see a quantitative way in which ranking models via adversarial sensitivity actually correlates with another metric of brain-likeness that the neuroscience community currently uses. At the current stage, I feel that the paper is highlighting various properties of adversarially robustified ANNs, but does not make a connection back to whether the proposed adversarial sensitivity metric, apart from better separating the models being compared, truly produces more human-like representations. If the authors could please comment on how a reader can be confident that ranking based on adversarial sensitivity implies more human-likeness, that would significantly strengthen the current submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f0gM9Sfz0G", "forum": "ZmhRSFFiun", "replyto": "ZmhRSFFiun", "signatures": ["ICLR.cc/2026/Conference/Submission3493/Reviewer_SPWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3493/Reviewer_SPWp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131375870, "cdate": 1762131375870, "tmdate": 1762916755452, "mdate": 1762916755452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}