{"id": "papImkPLf5", "number": 19627, "cdate": 1758297779869, "mdate": 1759897029584, "content": {"title": "Sound Probabilistic Safety Bounds for Large Language Models", "abstract": "We introduce a novel framework for computing rigorous bounds on the probability that a given prompt to a large language model (LLM) generates harmful outputs. We study the applications of classical Clopper–Pearson confidence intervals to derive probably approximately correct (PAC) bounds for this problem and discuss their limitations. As our main contribution, we propose an algorithm that analyses features in the latent space to prioritize the exploration of branches in the autoregressive generation procedure that are more likely to produce harmful outputs. This approach enables the efficient computation of formal guarantees even in scenarios where the true probability of harmfulness is extremely small. Our experimental results demonstrate the effectiveness of the method by computing non-trivial lower bounds for state-of-the-art LLMs.", "tldr": "", "keywords": ["Large Language Models", "Rare Event Estimation", "LLM Safety"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8c092bdc4e00635367afde27dd205465f8c43c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce a new framework for computing rigorous bounds on the probability of harmful outputs from a prompt to a large language model (LLM). They study classical Clopper-Pearson confidence intervals and propose an algorithm to prioritize harmful branches in the autoregressive generation procedure."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ A pioneer work on studying probabilistic bounds for LLMs.\n+ Good presentation even for readers outside the domain."}, "weaknesses": {"value": "- Some parts are not clearly illustrated.\n- The experiment setting is too simple, which may hinder the practical usage.\n- The theoretical guarantee and proof are not given with the computing upper and lower bounds."}, "questions": {"value": "1. Page 6, Line 306: Given that the generation tree of LLM is very large, the partial tree construction should not be that useful.\n\n2. Page 7, Line 332: Do the \"activated features\" refer to all activation values inside the LLM? This may introduce high computational complexity.\n\n3. Page 8: The experiment is only conducted in 1B-level models, the extendability is unclear. Moreover, the safety oracle is a finite set of words with a limited number of words.\n\n4. A few related works should be discussed:\n- Song, Da, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma. \"Luna: A model-based universal analysis framework for large language models.\" IEEE Transactions on Software Engineering 50, no. 7 (2024): 1921-1948.\n- Zhang, M., Goh, K. K., Zhang, P., Sun, J., Xin, R. L., & Zhang, H. (2024). LLMScan: Causal Scan for LLM Misbehavior Detection. arXiv preprint arXiv:2410.16638."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yCBdYMp4Ew", "forum": "papImkPLf5", "replyto": "papImkPLf5", "signatures": ["ICLR.cc/2026/Conference/Submission19627/Reviewer_vuwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19627/Reviewer_vuwU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393643080, "cdate": 1761393643080, "tmdate": 1762931478566, "mdate": 1762931478566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a framework to compute bounds on the probability that a given prompt to LLM generates harmful outputs. The authors begin by Clopper-Pearson confidence intervals and propose an algorithm that analyses features in latent space to prioritize the autoregressive generation tree."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. **Good trial of math modelization of safety bounds**: The authors explore Clopper-Pearson exact confidence intervals and autoregressive generation tree to compute the bounds on the probability that a given prompt to LLM generates harmful outputs. This is a good trial to explore the theoretical basis behind LLMs safety."}, "weaknesses": {"value": "1. The computed bound is not **rigorous** as claimed by the authors (line 11, 65), because they use approximate linearity features and computational budget to approximate the bound.\n2. The experiment is very poor. It is more like a case study because only 2 cases are evaluated. Besides, the computed lower bound is still very low ($10^{-8} - 10^{-4}$). Considering the upper bound is at around $10^{-2}$, such a wide range cannot be used to interpret or improve LLMs safety.\n3. What does $X_i$ of line 209 correspond to? Does it correspond to each generated token? If yes, why can it be assumed as i.i.d. Bernoulli random variable since each token depends on the preceding tokens?\n4. In line 298, the authors assume that once a prefix of an output is harmful, every continuation extending this prefix is also harmful. But this is not always true because LLMs are observed to correct their wrong output prefix during inference [1]. Taking the same example in Figure 1, the output \"You need to install, wait wait wait, this is illegal, I cannot assist you.\" has a harmful prefix but is harmless as a whole.\n5. The presentation and organization of paper is poor: algorithm 1 is too long, experiment I is cut to different pages, and there is no conclusion section.\n\n[1] Course-Correction: Safety Alignment Using Synthetic Preferences"}, "questions": {"value": "1. What does $X_i$ of line 209 correspond to? Does it correspond to each generated token? If yes, why can it be assumed as i.i.d. Bernoulli random variable since each token depends on the preceding tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2OGV6nfutm", "forum": "papImkPLf5", "replyto": "papImkPLf5", "signatures": ["ICLR.cc/2026/Conference/Submission19627/Reviewer_ksh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19627/Reviewer_ksh1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657938329, "cdate": 1761657938329, "tmdate": 1762931478140, "mdate": 1762931478140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for computing rigorous bounds on the probability that a Large Language Model (LLM) generates harmful outputs when given a specific prompt. It first discusses the limitations of existing sampling-based methods, like Clopper–Pearson confidence intervals, which often yield trivial (zero) lower bounds because harmful events are rare in aligned LLMs. The core contribution is an algorithm that constructs a partial autoregressive generation tree, guided by a \"harmfulness feature\" vector computed in the latent space to prioritize the exploration of branches most likely to lead to harmful content. This approach efficiently computes non-trivial, mathematically sound lower bounds on the harmfulness probability, which is crucial for safety certification in high-stakes LLM applications. Experimental results demonstrate that this method consistently provides a superior lower bound compared to standard Monte Carlo and Clopper–Pearson techniques."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors present a strong theoretical grounding of the work presented, while maintaining good flow and ease-of-read throughout the paper.\n- The paper addresses relevant problem that has recently gained attention in the research community."}, "weaknesses": {"value": "- There are some articles the authors might like to consider that introduce some related notions to the ones discussed in the paper. For example, in a more general sense, there is literature on domain certification (not necessarily safety certification, as the authors discuss, but could this be seen as a particular case of domain certification?):\n\t* Emde, C., Paren, A., Arvind, P., Kayser, M., Rainforth, T., Lukasiewicz, T., ... & Bibi, A. (2025). Shh, don't say that! Domain Certification in LLMs. arXiv preprint arXiv:2502.19320.\n- I suggest taking a look at the related literature in the paper above, and clarifying how this work sets apart from other works such as:\n\t* Krause, B., Gotmare, A. D., McCann, B., Keskar, N. S., Joty, S., Socher, R., & Rajani, N. F. (2020). Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367.\n\t* Yang, K., & Klein, D. (2021). FUDGE: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218.\n\t* Fonseca, J., Bell, A., & Stoyanovich, J. (2025). Safeguarding large language models in real-time with tunable safety-performance trade-offs. arXiv preprint arXiv:2501.02018.\n- The authors claim they propose a method for computing \"rigorous bounds\" on the safety of an generated answer, given a certain prompt. However, their method relies on an oracle that is either manually defined with specific keywords, or a trained neural network classifier. While the former is not a scalable/generalizable approach, the latter is prone to its own limitations; jailbreak attempts could manipulate the scores produced by this neural network.\n- On the same note, the experimental evaluations conducted seem insufficient. The authors merely present two examples using predefined keywords, while the neural network approach the authors mention previously is never actually reported. Furthermore, in the keywords list displayed, the existence of the word \"scrapy\" (experiment I), or the word \"search\" (experiment II), are not dangerous on their own, yet their existence in an output is sufficient for it to be considered dangerous according to the proposed method. \n- In the two examples provided, it is impossible to accurately compare the varying top-k values since the temperature values are not the same.\n- In addition, the provided repository appears to be empty (I see two files listed, a readme and a jupyter notebook, both yielding the error \"The requested file is not found.\"). I cannot validate the reproducibility of this work, or validate any details I might be missing based on the actual implementation of the proposed method."}, "questions": {"value": "- I'm not particularly convinced on the significance of the proposed method in a practical setting. Generally, deployed models use very low temperature (much lower than the reported values) and top-k settings for next-token prediction, which raises the question on whether the instances found would show up at all, at a non-near-zero probability. Could the authors provide an analysis or additional experimental results using significantly lower temperature settings (e.g., T << 0.4) than those reported in the experiments, or even T=0 (greedy decoding)? This would help demonstrate the utility of the method in configurations commonly used for stable deployment, where stochasticity is minimized.\n- Given that the computed p_L​ is a mathematically rigorous lower bound on the true probability p, how does the framework's guarantee of rigor hold if the underlying safety oracle H itself is known to be imperfect or susceptible to manipulation, such as prompt injection or jailbreak attempts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHhjey9ARm", "forum": "papImkPLf5", "replyto": "papImkPLf5", "signatures": ["ICLR.cc/2026/Conference/Submission19627/Reviewer_k5WE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19627/Reviewer_k5WE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763793883, "cdate": 1761763793883, "tmdate": 1762931477689, "mdate": 1762931477689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces framework to deduce non trivial bounds for LLMs to samples harmful (as defined an oracle) responses to set of prompts. The proposed framework poses autoregessive sampling from a language model as a tree structure with each node being a token and subsequent edge leading to next token thereby making each unique path to a leaf being a unique sequence. Each sequence probability is the product of prob along one path of the tree (albeit very large tree due to vocab size). This tree depicts the joint distribution of sequences from given prompt. The framework depends on 3 \"observations\" (I would call them maybe axioms): 1) if a equence becomes labelled as harmful at one point in the sequence it can never go back to being \"unharmful\", 2) \"the sum of probabilities of harmful leaves in this subtree provides a lower bound on the true harmfulness probability p\" and 3)"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- I really liked the reading this paper, problem formulation for clear and concise, I also appreciate the covering of PAC bounds.\n- I think the proposed framework is intuitive in the sense it beam searches the most harmful responses and increases the lower bound of unsafe responses based on that.\n- This method translates really well to empirical experiments and thus is more relevant."}, "weaknesses": {"value": "See questions"}, "questions": {"value": "- Is the assumption that there is an oracle that labels harmful from non-harmful completion exists too strong?\n- How can this lower bound be leveraged? Like in my first point, one needs an oracle to for this, and if you have that then you just piecewise update the sampler to never sample generations which are harmful (i.e. H(prompt)= 1). And then your upperbound drops to 0 i.e. you never sample harmful response for given oracle function H. I'm not sure what is the way to leverage this framework at frontier scale? Is this just a mental model to think about bounds? I like this but I'm not sure what exact value this work provides.\n- I would like the second limitations addressed within this version of the paper for it to be accepted. For the reader, current experiments computes bounds on per prompt basis, but we need to see it from a distribution of prompts and perhaps see a histogram of different prompt distribution to make better sense. Currently, the experiments are too empty.\n- The paper reaches word limit without discussing much results. I think paper writing needs surgery to better convery + experimentally convince the reader."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ybu24bXyiB", "forum": "papImkPLf5", "replyto": "papImkPLf5", "signatures": ["ICLR.cc/2026/Conference/Submission19627/Reviewer_jNJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19627/Reviewer_jNJK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026689429, "cdate": 1762026689429, "tmdate": 1762931477315, "mdate": 1762931477315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}