{"id": "4eYSSSDle6", "number": 4095, "cdate": 1757599811523, "mdate": 1759898053162, "content": {"title": "PRL: Prompts from Reinforcement Learning", "abstract": "Effective prompt engineering remains a central challenge in fully harnessing the\ncapabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation.\nUnlike previous methods, PRL can produce novel few-shot examples that were not\nseen during training. Our approach achieves state-of-the-art performance across a\nrange of benchmarks, including text classification, simplification, summarization,\nand reasoning. On the classification task, it surpasses prior methods by 2.58%\nover APE and 1.00% over EvoPrompt. Additionally, it improves the average\nROUGE scores on the summarization task by 4.32 over APE and by 2.12 over\nEvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01\nover EvoPrompt. On the GSM8K mathematical reasoning benchmark, PRL further\nimproves accuracy by 2.72% over APE and by 4.53% over EvoPrompt. We will\nmake our implementation publicly available upon acceptance.", "tldr": "We propose to learn prompts through reinforcement learning, obtaining prompt including newly synthesized few-shot examples and outperforming other automatic prompting methods on text classification, summarization, simplification and GSM8K", "keywords": ["Large Language Models", "Prompt Engineering", "Reasoning", "Automatic Prompt Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e47373a207e3b664bcd14f843fd53e179d8de08b.pdf", "supplementary_material": "/attachment/a77fa9c9373805131f76e8eca80603e7a57fcb53.zip"}, "replies": [{"content": {"title": {"value": "3. Cross-model generalization"}, "comment": {"value": "There are two aspects to cross-model generalization: \n\n(i) Can we train with different combinations of prompt generator and prompt evaluator models and \n\n(ii) after training, can we switch to a different prompt evaluator model without retraining. \n\nPRL is robust at both of these cross-model generalizations. Regarding (i), using different generator and evaluator models can be beneficial. In Table 6 in the paper, we have introduced a PRL-LLaMA variant that uses Llama3.1-8B-Instruct as the generator. The results are reproduced below (the first line corresponds to PRL-Llama in the paper, the second line to PRL):\n\n\n| Prompt Generator       | Prompt Evaluator (Training) | Prompt Evaluator (Test)      | Rouge-1      | Rouge-2      | Rouge-L      |\n|------------------------|-----------------------|-----------------------|--------------|--------------|--------------|\n| LLama3.1-8B-Instruct   | Qwen2.5-7B-Instruct         | Qwen2.5-7B-Instruct  | 43.70±0.02   | 16.66±0.25   | 37.46±0.5    |\n| Qwen2.5-7B-Instruct    | Qwen2.5-7B-Instruct   | Qwen2.5-7B-Instruct | 42.47±0.83   | 16.17±0.24   | 37.73±0.36   |\n| Qwen2.5-7B-Instruct    | Qwen2.5-7B-Instruct | LLama3.1-8B-Instruct | 39.38±2.82 | 15.77±1.56 | 30.57±2.80 |\n\n\nThese results show that PRL is flexible: different generator models can be used without degrading performance, and cross-model training can even yield improvements.\n\nMoreover, for this rebutall we crafted a new experiment in which we train with Qwen2.5-7B-Instruct as prompt generator and Llama3.1-8B-Instruct as evaluator on the summarization task. We present the results in the following table:\n\n| Prompt Generator: Qwen2.5-7B-Instruct | Prompt Evaluator: Llama3.1-8B-Instruct (Train/Test) |\n| ------------------------------------------- | --------------------------------------------------------- |\n\n| Method  | ROUGE-1           | ROUGE-2           | ROUGE-L           |\n| ------- | ----------------- | ----------------- | ----------------- |\n| MI      | 28.92             | 9.29              | 25.09             |\n| APE     | 38.47±0.66 | 12.96±0.14        | 33.17±0.22        |\n| GA      | 37.48±2.65        | 13.92±1.14 | 33.48±2.54|\n| **PRL** | **40.06±0.02**    | **13.98±0.12**    | **34.06±0.01**    |\n |\n\n\nRegarding (ii), we also see at the table above that prompts produced by PRL using Qwen2.5-7B-Instruct (as both generator and evaluator during training) can be directly applied to Llama3.1-8B-Instruct for evaluation during test time. These transferred prompts achieve state-of-the-art results on the summarization task, demonstrating that PRL produces general prompts that remain effective across evaluation models."}}, "id": "g3TD44yyAQ", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564112079, "cdate": 1763564112079, "tmdate": 1763568324285, "mdate": 1763568324285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PRL (Prompts from Reinforcement Learning), a method for automatic prompt generation using reinforcement learning. The approach formulates prompt construction as a learning problem, enabling the model to generate few-shot examples that were not seen during training. The authors evaluate PRL on several tasks, including text classification, text simplification, summarization, and the GSM8K reasoning benchmark. Experimental results show consistent improvements over prior prompt optimization methods such as APE and EvoPrompt, with reported gains in accuracy, ROUGE, SARI, and reasoning performance. The paper claims that PRL offers a general and effective framework for enhancing LLM performance through learned prompts"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe method is clear and intuitive. The paper is well-written and easy to follow.\n\n2.\tOverall, the experimental results effectively support the authors’ claims."}, "weaknesses": {"value": "1.\tCompared to other approaches discussed in the paper, this method appears to be more computationally intensive.\n\n2.\tThe core of the method lies in the prompt generator, but the authors only use Qwen2-7B as the base model. It would be more convincing to evaluate PRL on models of different sizes and from different model families.\n\n3.\tThe generalization ability of PRL is not thoroughly studied. As different models exhibit varying levels of sensitivity and preference toward prompts, PRL may need to train separate prompt generators for different evaluation models. More tests about this could be further included.\n\n4.\tAdditional ablation studies could strengthen the paper. For instance, the influence of the “thinking process” in the prompt generator is not clearly analyzed. How much would the results change if this component were removed? If the performance remains similar, the additional computation might be unnecessary."}, "questions": {"value": "1.\tThe format of the citation at line 053 seems incorrect.\n\n2.\tThe prompt generator in PRL requires a base prompt. How sensitive is the method to the quality of this base prompt? Would iterative refinement improve performance? Additionally, what would happen if prompts were generated entirely from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0goXLAloyz", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Reviewer_rKJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Reviewer_rKJG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550022129, "cdate": 1761550022129, "tmdate": 1762917175704, "mdate": 1762917175704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "2. Comparison with GRACE & hard reasoning problems"}, "comment": {"value": "We want to thank the reviewers for pointing out the need of comparing PRL with more recent baselines and on more challenging benchmarks. Therefore we estimated GRACE [1] (published at NeurIPS2025) with Qwen2.5-7B-Instruct as prompt generator and evaluator. \nFollowing the reviewers' suggestions, we additionally evaluated PRL on three more challenging and recent benchmarks. We chose:\n\n- MATH500 [3] and DeepMath [2] both large-scale, high-difficulty mathematical reasoning benchmarks. \n- MedQA [4] is a large-scale multiple-choice medical QA benchmark based on professional board-exam-style questions. \n\nAs shown in the tables, PRL consistently outperforms all baseline prompt-optimization methods, including GRACE, on all three benchmarks, indicating that its advantages extend to substantially harder reasoning and domain-specific settings. We would be very happy to evaluate PRL on more additional datasets you would recommend and include those results in the final version. We present the results averaged over 3 runs along with standard deviations. \n\nWe performed evaluation on (best scores are in **bold**):\n\n1. Mathematical Reasoning tasks (DeepMath, Math500, GSM8K):\n\n| MATH500| |\n| -| - |\n\n|Method| Accuracy|\n| -| - |\n| APE    | 31.53±1.04 |\n| DE     | 34.20±1.39 |\n| GA     | 40.13±1.39 |\n| GRACE  | 33.20±1.60 |\n| PRL    | **44.40±1.40**|\n\n| DeepMath| |\n|-|-|\n\n|Method| Accuracy|\n| -| - |\n|APE| 15.47 ± 0.45 |\n|GA| 18.63 ± 2.37 |\n|DE| 16.10 ± 0.00 |\n| GRACE| 15.05 ± 0.16 |\n| PRL | **21.58 ± 0.22** |\n\n| GSM8K | |\n| -| - |\n\n|Method| Accuracy|\n|-|-|\n| MI| 78.20|\n| APE | 83.43±1.98|\n| GA| 81.62±1.38|\n| DE| 79.52±0.45|\n| GRACE| 82.37±1.82|\n| PRL| **86.15±0.55**|\n\n2. MedQA (domain base task):\n\n|Method |Accuracy|\n|-|-|\n|APE|45.66 ± 0.97|\n|GA|51.95 ± 1.61|\n|DE|51.76 ± 0.16|\n|GRACE| 52.26 ± 0.16|\n|PRL| **53.34 ± 0.11** |\n\n\n3. Classification tasks:\n\n| Method | CR                  | MR               | SST-5              | AG's News          | SST-2            | TREC               | Subj               | Avg         |\n| ------ | - | -| - | -| - | -| --| ----------- |\n| MI     | 87.25               | 87.40            | 52.31              | 82.29              | 92.70            | 69.20              | 57.95              | 75.59       |\n| NI     | 91.50               | 90.85     | 51.90              | 83.43              | 95.77        | 66.60              | 68.10              | 78.31       |\n| APO    | **93.48 ± 0.24**    | 89.97 ± 1.37     | 53.94 ± 0.29       | 83.73 ± 0.31 | 93.71 ± 0.25     | 71.30 ± 1.90       | 69.80 ± 5.96       | 79.42       |\n| APE    | 92.87 ± 0.02        | 89.90 ± 0.94     | 49.37 ± 5.66       | 82.58 ± 1.20       | 91.23 ± 0.66     | 77.07 ± 1.61 | 73.92 ± 1.39       | 79.56       |\n| GA     | 92.75 ± 0.40        | 90.45 ± 0.72     | 53.76 ± 1.13       | 82.24 ± 1.00       | 94.65 ± 1.04     | **79.20 ± 2.83**   | 74.93 ± 3.12 | 81.14 |\n| DE     | 93.38 ± 0.19 | 89.98 ± 0.24     | 55.25 ± 0.37 | 82.18 ± 1.04       | 93.29 ± 0.34     | 76.47 ± 0.38       | 73.08 ± 4.95       | 80.52       |\n| GRACE  | 90.92 ± 1.15        | 89.60 ± 1.51     | 53.96 ± 0.93       | 82.34 ± 0.39       | 93.61 ± 0.53     | 72.53 ± 8.62       | 73.92 ± 3.05       | 79.55       |\n| PRL    | 92.83 ± 0.24        | **91.27 ± 0.05** | **56.21 ± 0.15**   | **84.36 ± 0.08**   | **96.32 ± 0.04** | 77.07 ± 2.36 | **76.90 ± 0.95**   | **82.14**   |\n\n4. Summarization task:\n\n|Method|ROUGE-1|ROUGE-2|ROUGE-L|\n|-|-|-|-|\n|MI| 32.76| 10.39| 28.97|\n|APE| 37.12±2.02        | 12.97±0.74        | 33.32±1.68       |\n|GA| 39.69±1.76| 14.47±1.00        | 35.84±1.63       |\n|DE| 33.91±4.04| 12.53±1.47        | 31.05±3.79       |\n|GRACE|40.61±0.54| 14.65±0.53        | 35.86±0.54       |\n|PRL| **42.47±0.83**| **16.17±0.24**    | **37.73±0.36**   |\n\n5. Simplification task:\n\n|Method|SARI|\n|-|-|\n|MI| 43.77|\n|APE| 45.33±0.83|\n|GA| 46.25±0.47|\n|DE| 45.79±0.35|\n|GRACE| 50.21±0.18|\n|PRL| **52.26±3.51**|\n\nWe want to note that GRACE was developed with DeepSeek-R1 as prompt generator (called optimizer in their work) and DeepSeek-V3-0324 as evaluator. These are substantially larger and higher performing LLMs than the Qwen-2.5-7B-Instruct models we used. For the results above we have rerun GRACE on Qwen-2.5-7B-Instruct for a fair comparison. We have used the default hyperparameters given by the GRACE authors.\n\n\nReferences:\n\n[1] W. Shi, Y. Chen, S. Bian, X. Zhang, K. Tang, P. Hu, Z. Zhao, W. Lu, X. Du. No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization. NeurIPS, 2025.\n\n[2] He, Zhiwei, et al. \"Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning.\" arXiv preprint arXiv:2504.11456 (2025).\n\n[3] Lightman, Hunter, et al. \"Let's verify step by step.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[4] Yang, Hang, et al. \"Llm-medqa: Enhancing medical question answering through case studies in large language models.\" arXiv preprint arXiv:2501.05464 (2024)."}}, "id": "01GPeSEO3R", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763566100603, "cdate": 1763566100603, "tmdate": 1763570215863, "mdate": 1763570215863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Effective prompt engineering remains a **central challenge** in fully harnessing the capabilities of Large Language Models (LLMs). While precise input prompts can guide LLMs to perform complex tasks, the most impactful prompts often rely on *subtle semantic cues* that may *elude human perception*.\n\nTo address this, the paper introduces **PRL (Prompts from Reinforcement Learning)**, a novel RL-based approach for automatic prompt generation.\n\n**PRL's key innovations include:**\n\n1.  **Novel Few-shot Example Synthesis:** Unlike previous methods like Automatic Prompt Optimization (APO), which are restricted to selecting few-shot examples from training data, PRL is capable of generating and selecting **novel few-shot examples** that were *not seen during training*.\n2.  **Explicit Reasoning Integration:** PRL incorporates a *reasoning phase* prior to prompt generation, where the prompt generator first produces a **rationale** (enclosed within `<think>` tags) to guide the final output.\n3.  **RL Optimization Cycle:** PRL trains a **Prompt Generator ($\\pi_{\\text{generator}}$)** (a trainable language model) to refine base prompts and generate a corresponding reasoning trace. This generated prompt is evaluated by a **frozen Evaluation Model ($\\pi_{\\text{eval}}$)** (an LLM used only for inference), which calculates rewards based on formatting and task performance. The generator is optimized using the *Group Relative Policy Optimization (GRPO)* update rule.\n4.  **Prompt Selection Strategy:** The method uses a prompt selection strategy to mitigate training instability and noisy feedback, regularly testing generated prompts on the validation set and keeping the best overall one.\n\n**Main Contributions and Results:**\n\nPRL achieves **state-of-the-art performance** across a range of benchmarks: text classification, summarization, simplification, and GSM8K mathematical reasoning.\n\n*   On the **classification task**, PRL surpasses APE by **2.58%** and EvoPrompt by **1.00%** in mean accuracy.\n*   On **summarization**, it improves average ROUGE scores by **4.32** over APE and **2.12** over EvoPrompt.\n*   On **simplification**, it improves the SARI score by **6.93** over APE and **6.01** over EvoPrompt.\n*   On the **GSM8K mathematical reasoning** benchmark, PRL improves accuracy by **2.72%** over APE and **4.53%** over EvoPrompt.\n*   The research further suggests that **RL-based optimization naturally leads to the emergence of few-shot prompting behavior**."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:**\n\n*   PRL is highlighted as the **first RL-based prompt optimization method** capable of *generating and selecting novel, task-specific few-shot examples*. This is a crucial distinction, as it moves beyond the constraint of only using few-shot examples already present in the training data, a limitation faced by methods like APO.\n*   The observation that few-shot examples emerge **spontaneously** during the RL training process, without explicit encouragement, is a unique and insightful finding regarding how RL shapes LLM prompting behavior.\n\n**Quality & Significance:**\n\n*   The method demonstrates **superior performance** consistently across all four evaluated task types (classification, summarization, simplification, and reasoning), validating its robust generalizability and effectiveness.\n*   **Ablation studies confirm the value of core components:** The explicit reasoning phase proved critical, leading to a *substantial drop in accuracy* (from 75.05 to 60.12) on the SUBJ dataset when omitted. Furthermore, the Prompt Selection strategy was shown to *improve final performance and enhance training efficiency*, helping to manage the high variance inherent in RL training.\n*   PRL is shown to be effective even when applied to **larger, more powerful LLMs** (e.g., Qwen2-32B-instruct), demonstrating that even these models remain *vulnerable to prompt variation* and can benefit significantly from PRL's tailored prompts.\n\n**Clarity:**\n\n*   The architecture, including the roles of the Prompt Generator and the frozen Evaluation Model, and the overall RL training scheme (Figure 2) are clearly described.\n*   The reward function is systematically broken down into components: **formatting rewards** ($r_{token}$, $r_{structure}$) for the Generator, and **task performance rewards** ($r_{format}$, $r_{alignment}$) for the Evaluation Model, providing clarity on how the model’s behavior is guided."}, "weaknesses": {"value": "1.  **Significant Computational Overhead:** The paper explicitly lists as a limitation that the improved performance is obtained at the cost of a **significantly greater computational expense** than related, comparatively simpler work. The experimental setup involved training over 48 hours using two NVIDIA A100 GPUs (40 GB each). The paper lacks concrete discussion or suggested methods for mitigating or quantifying this increased computational burden, which limits its practical accessibility.\n2.  **Task-Specific Retraining Requirement:** Currently, the prompt generator must be **retrained for each new task**. The authors acknowledge that developing a *universal prompt generator* is a \"desideratum\" (an ideal goal), indicating that the current method’s efficiency is limited when facing a wide variety of tasks or zero-shot scenarios.\n3.  **Insufficient Detail on Few-shot Synthesis:** PRL's ability to **autonomously synthesize relevant few-shot examples** not present in the training set is a major contribution. However, the paper does not delve into *how* the prompt generator, guided by RL, manages to *create* these task-aligned, non-redundant examples—specifically, the internal reasoning or constraints utilized by $\\pi_{\\text{generator}}$ in its thought process ($\\langle \\text{think} \\rangle$ tag) to achieve this synthesis.\n4.  **Sensitivity of Baselines to Evaluation Model Choice:** The authors note that when reproducing EvoPrompt results, the relative effectiveness of its DE and GA variants was **sensitive to the choice of the underlying language model** (Qwen2.5-7B-Instruct). Although the authors ensured a fair comparison by using the same Evaluation Model across all baselines, this inherent sensitivity suggests that the observed superiority of PRL might be conditional on the chosen model, necessitating more explicit acknowledgement of this limitation in interpreting the main results."}, "questions": {"value": "1.  **Computational Efficiency and Trade-offs:** Given that PRL requires a *significantly greater computational expense*, could the authors provide a more detailed analysis of the performance gains versus the resource cost? Are there specific tuning levers (e.g., Prompt Selection frequency $t$, or the number of sampled prompts $n$) that could be adjusted to reduce training time substantially while maintaining competitive performance against baselines?\n2.  **Mechanics of Task-Dependent Few-Shot Emergence:** Few-shot examples were critical for classification tasks (improving accuracy significantly, e.g., SUBJ from 66.75 to 77.95), but PRL **consistently opted *not* to include few-shot examples** for the summarization task. What drives this striking, task-dependent behavior? Which specific components of the comprehensive reward function $R$ (e.g., $r_{alignment}$ or $r_{structure}$) are responsible for prompting $\\pi_{\\text{generator}}$ to spontaneously generate few-shot examples for classification but omit them for generation tasks?\n3.  **Path to a Universal Prompt Generator:** The paper identifies the need to *retrain the generator for each new task* as a limitation, noting that a *universal prompt generator* is a \"desideratum\". What are the initial conceptual steps or future research directions the authors are considering to enable the Prompt Generator to transfer or generalize prompting knowledge across different tasks without requiring full retraining?\n4.  **Baseline Robustness Across Evaluation Models:** All SOTA claims are established by evaluating baselines on the Qwen2.5-7B-Instruct model. Given the noted sensitivity of EvoPrompt variants to the base model choice, how would a *complete* set of benchmark results (including APE, EvoPrompt, and APO where applicable) compare if a distinct architecture, such as LLaMA 3.1-8B-Instruct, was used as the Evaluation Model for *all* methods, similar to the setup used for the portability study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IggJfC1GrK", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Reviewer_vKbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Reviewer_vKbp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566384808, "cdate": 1761566384808, "tmdate": 1762917175264, "mdate": 1762917175264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "1. Computational cost & task generalizability"}, "comment": {"value": "Below are our answers to issues raised by multiple reviewers:\n\n**Computational Cost:**\n\nWe agree that PRL is more compute-intensive than some prior methods. This is a consequence of its deliberately larger search space: whereas APO and EvoPrompt mainly operate by rephrasing a base prompt via critique loops or evolutionary edits (thus constraining exploration), PRL does not impose such restrictions. This broader search is precisely what enables PRL to synthesize novel few-shot examples and incorporate explicit reasoning structures, capabilities that, in turn, drive the observed performance gains.\n\nCrucially, PRL’s compute cost is front-loaded and amortized. Although we report a 48-hour budget, the built-in Prompt Selection mechanism surfaces strong prompts well before the end of training; these intermediate prompts can be deployed immediately. We illustrate this with the evolution of the final prompt’s performance over time:\n\n|CR|\n|-|\n\n| Time (hours) | Accuracy |\n|--------------|----------|\n| 2            | 0.924    |\n| 8            | 0.9245   |\n| 18           | 0.9255   |\n| 24           | 0.930     |\n| 48           | 0.930   |\n\n|SST5|\n|-|\n\n| Time (hours) | Accuracy |\n|--------------|----------|\n| 0.9          | 0.516 |\n| 1.7          | 0.536 |\n| 2.6          | 0.538 |\n| 3.4          | 0.544 |\n| 6.9          | 0.547 |\n| 12.0         | 0.549 |\n| 18.0         | 0.554 |\n| 23.1         | 0.560 |\n| 48.0         | 0.560 |\n\n\nEven when optimization continues beyond these early checkpoints, the prompts discovered at earlier stages are already competitive and practically useful. Moreover, PRL’s outputs are plain-text prompts, so there is no inference-time overhead relative to manually engineered prompts.\n\nThe general trend on other datasets is similar.\n\nWe want to note that GRACE, another concurrent SOTA prompter, for which we also provide additional experiments below, is also training for up to 48 GPU hours for some of the tasks that require longer output, e.g. summarization, simplification and DeepMath.\n\n**Task generalization:**\n\nIn line with the accepted evaluation protocol and similar to existing work our prompts are task-specific and do not generalize across tasks. Since PRL also synthesizes few-shot examples, which drives improvement in task performance, this task-specificity cannot be easily avoided.\n\nIn summary, PRL does use more compute because it searches a richer space that enables novel few-shot synthesis and explicit reasoning. However, Prompt Selection provides early, deployable prompts; inference remains as cheap as any manual prompt; and the learned prompts transfer across evaluators (Table 6). We believe this yields a favorable trade-off between computational cost and practical usability."}}, "id": "KcrzDtqUtE", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763566329125, "cdate": 1763566329125, "tmdate": 1763566329125, "mdate": 1763566329125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRL, a RL framework that trains a prompt-generation model to automatically produce effective task prompts. It can be viewed as another R1-style (GRPO RL training) approach applied to the domain of automatic prompt generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing prompt optimization as a reinforcement learning problem with a frozen evaluator is clear and straightforward.\n2. The method demonstrates consistent empirical improvements across multiple benchmark tasks."}, "weaknesses": {"value": "1. The paper motivates prompt optimization as an underexplored and crucial problem, but recent work (e.g., instruction tuning, preference alignment, RLHF) has substantially reduced the marginal importance of prompt optimization for strong LLMs. The paper would benefit from a deeper discussion or quantitative evidence that prompt optimization still provides meaningful gains for modern aligned models. \n2. Most baseline methods cited for automatic prompt generation date back to 2022–2023. It would strengthen the paper to incorporate or discuss more recent developments in this area.\n3. The proposed method requires RL training for each dataset, which introduces significant computational overhead compared to evolutionary or heuristic prompt optimizers. The paper lacks a systematic analysis of the resulting latency and compute cost versus accuracy trade-off, making it difficult to assess practical efficiency.\n4. The motivation for training a separate prompt generator model is somewhat unconvincing. The paper does not compare against stronger or larger generator models (e.g., Qwen2.5-72B, GPT) or expert-crafted prompts, which could potentially achieve comparable results without RL training.\n5. Since the generator must be retrained for every dataset, the method’s scalability and generality are limited. The paper does not explore whether a single generator can generalize across multiple datasets or related domains.\n6. The chosen benchmarks (e.g., GSM8K, SAMSum, SST, AGNews) are somewhat outdated and relatively simple. Evaluating on more challenging or recent datasets would better demonstrate PRL’s robustness and contemporary relevance.\n7. The ablation on model size lacks a clear causal interpretation. Performance improvements when scaling from 7B to 32B evaluators could largely stem from the inherent capability gain of the larger models rather than from PRL’s prompt optimization."}, "questions": {"value": "Could you include detailed statistics for the training, validation, and test datasets used in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jPX9FI034N", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Reviewer_tMpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Reviewer_tMpA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588537698, "cdate": 1761588537698, "tmdate": 1762917174871, "mdate": 1762917174871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRL (Prompts from Reinforcement Learning), a reinforcement learning-based\napproach to automatically generating and optimizing prompts for large language models (LLMs). PRL uniquely\nenables the synthesis of novel few-shot examples not seen during training and integrates explicit reasoning\nsteps before prompt output. This method achieves excellent empirical performance in tasks such as text\nclassification. The research scheme includes carefully designed reward shaping, prompt selection, and\ndetailed ablation experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PRL devises a clear RL-based prompt optimization loop. Compared to other methods, PRL can create few-\nshot prompt examples not limited to the original training data\n\n2. PRL is evaluated across varied tasks, and multiple ablation studies dissect the contribution of prompt\nselection, few-shot examples, and explicit reasoning. Additionally, its effectiveness is verified on models of\ndifferent architectures and sizes."}, "weaknesses": {"value": "1. Compared to other methods, PRL requires more computational resources for training and has insufficient\ngeneralization ability, which means that PRL needs to be retrained for different tasks, greatly limiting its\nusability.\n\n2. There is insufficient discussion on the instability, scalability, and generalization of reinforcement learning:\n- Can training on a single task generalize to other tasks? Furthermore, how does the performance and\ngeneralization of simultaneous multi-task learning compare to that of single-task learning?\n- Is model training sensitive to the introduced reward function, does reward manipulation exist, and\nwhat is the interaction between format specification rewards and task correctness rewards?\n- Is it effective to use a different generator model for training than the one used to evaluation model?\nFurthermore, can the same generator model be directly used for different evaluation models after\ntraining?\n\n3. One of the paper's cores is that few-shot prompting behavior \"spontaneously emerges\" from the RL setup,\nyet there is little to no formal analysis or justification. A more rigorous explanation (e.g., does the reward\nlandscape incentivize synthesis, or is it an artifact of the prompt generator's architecture?) is sorely missing.\n\n4. The compared methods are limited to those before 2023, lacking comparison results with new methods\nfrom 2024-2025."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qh7iphTJje", "forum": "4eYSSSDle6", "replyto": "4eYSSSDle6", "signatures": ["ICLR.cc/2026/Conference/Submission4095/Reviewer_tiGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4095/Reviewer_tiGz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966551636, "cdate": 1761966551636, "tmdate": 1762917174610, "mdate": 1762917174610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}