{"id": "CUx9KieuW9", "number": 5379, "cdate": 1757905965202, "mdate": 1759897978798, "content": {"title": "Robobench: A Comprehensive Evaluation Benchmark For Multimodal Large Language Models as Embodied Brain", "abstract": "Building robots that can perceive, reason, and act in dynamic, unstructured environments remains a core challenge. Recent embodied systems often adopt a dual-system paradigm, where System 2 handles high-level reasoning while System 1 executes low-level control. \nSystematic evaluation of System 2 is thus crucial for advancing embodied intelligence.\nYet existing benchmarks emphasize execution success, or when focusing System 2, suffer from incomplete evaluation dimensions and limited task realism, offering only partial assessment of embodied cognition abilities.\nTo bridge this gap, we introduce RoboBench, the first benchmark that systematically evaluates multimodal large language models (MLLMs) as embodied brains. RoboBench defines five critical dimensions—instruction comprehension, perception reasoning, generalized planning, affordance reasoning, and failure analysis—spanning 15 abilities, 26 tasks, and over 7,000 QA pairs. To ensure realism, we design task settings across diverse embodiments (single-arm, dual-arm, mobile manipulation), objects with rich physical and semantic attributes, multi-view scenes with occlusion and closed-loop feedback, sourced from large-scale real robotic datasets and curated in-house. For planning, RoboBench proposes a DAG-based evaluation framework capturing action–object dependencies and execution-order variations, enabling more faithful assessment of long-horizon reasoning than prior multiple-choice, BLEU, or generic LLM-based metrics.\nExperiments on 17 state-of-the-art MLLMs reveal fundamental limitations: difficulties with implicit instruction grounding, spatiotemporal reasoning, long-horizon and cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis.\nRoboBench provides a comprehensive scaffold to quantify embodied cognition, clarify System 2 performance, and guide the development of next-generation MLLMs toward more robust embodied intelligence.", "tldr": "", "keywords": ["Embodied AI", "Multimodal Large Language Model", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b9585d514d3544293e7b9e5521b97d214902101.pdf", "supplementary_material": "/attachment/43b292a27c41ced1d4f691ae448c42ecc1b762ba.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces RoboBench, a comprehensive benchmark designed to evaluate MLLMs as the cognitive \"embodied brain\" for robotic manipulation. RoboBench systematically evaluates MLLMs across five key dimensions: instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis, spanning 25 tasks and over 6000 QA pairs. A core contribution, as stated by the authors, is its \"MLLM-as-world-simulator\" framework, which assesses long-horizon planning by simulating the embodied feasibility of predicted steps rather than just symbolic accuracy. Experiments on 14 state-of-the-art MLLMs revealed fundamental limitations, including difficulties with implicit instruction comprehension, spatiotemporal reasoning, and execution failure diagnosis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The benchmark is deliberately scoped to the cognition layer of the manipulation pipeline, focusing on five interdependent capabilities: instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis.\n+ The benchmark ensures realism by curating datasets across diverse embodiments (single-arm, dual-arm, and mobile manipulation), objects with rich attributes, multi-view scenes with occlusion, and memory-driven navigation.\n+ The paper provides a clear presentation and articulates a strong motivation for the benchmark, addressing the limitations of existing evaluations that either focus too narrowly on execution success or lack task realism.\n+ The benchmark is comprehensive in its scale, covering 15 capabilities, 25 tasks, and a total of 6077 question-answer pairs."}, "weaknesses": {"value": "+ The paper introduces an \"MLLM-as-world-simulator\" framework for planning evaluation, which functions as an \"MLLM-as-judge\". This approach is susceptible to well-documented biases [1, 2, 3] inherent in using LLMs for evaluation, such as positional bias or favoring models with similar architectures or response styles. The paper does not analyze or mitigate these potential biases, which may affect the objectivity of the planning scores.\n\n+ The benchmark's evaluation of long-horizon planning relies on manually annotated DAGs to represent ground-truth dependencies. However, the paper does not report inter-annotator agreement (IAA), making it difficult to gauge the reliability of these crucial annotations.\n\n+ The scope of the benchmark is limited to table-top manipulation tasks, which may represent relatively short and constrained interactions from the perspective of an MLLM's capabilities.\n\n+ While a key novelty is the use of real-world data, the evaluated tasks are not fundamentally distinct in nature from those found in previous simulation-based benchmarks [4, 5]. As a result, the insights derived from the evaluation, while confirming existing knowledge, may not offer substantially new perspectives on the cognitive gaps of MLLMs in embodied AI.\n\n+ The failure analysis dataset is constructed by synthetically perturbing correct plans to create planning errors (e.g., wrong object, missing step) due to the lack of real-world failure data. This synthetic generation may not capture the true distribution and complexity of failures encountered in real robotic execution, potentially limiting the practical value of the conclusions drawn from this benchmark dimension.\n\n+ A minor presentation issue exists in Figure 1, where the top-right performance comparison radar chart is rendered at a low resolution, causing the text and plot details to be blurry.\n\n[1] Panickssery, Arjun, Samuel Bowman, and Shi Feng. \"Llm evaluators recognize and favor their own generations.\" Advances in Neural Information Processing Systems 37 (2024): 68772-68802.\n\n[2] Chen, Guiming Hardy, et al. \"Humans or llms as the judge? a study on judgement biases.\" arXiv preprint arXiv:2402.10669 (2024).\n\n[3] Dorner, Florian E., Vivian Yvonne Nastl, and Moritz Hardt. \"Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data.\" The Thirteenth International Conference on Learning Representations.\n\n[4] Li, Manling, et al. \"Embodied agent interface: Benchmarking llms for embodied decision making.\" Advances in Neural Information Processing Systems 37 (2024): 100428-100534.\n\n[5] Yang, Rui, et al. \"Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents.\" arXiv preprint arXiv:2502.09560 (2025)."}, "questions": {"value": "1. Will you provide the analysis or justify how you mitigate the potential biases when using MLLM-as-judge style metrics?\n\n2. What are the inter-annotator agreement statistics and QA procedures for the DAG annotations?\n\n3. What insights uniquely arise from your real-world setup vs simulator-based setup (e.g., any ablations showing differences)?\n\n4. I know this question might be hard, but if possible, could you please validate the synthetic failure set against real robot logs (or in a simulation environment) in a small subset? It is okay to skip this question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UkGvjXZIM7", "forum": "CUx9KieuW9", "replyto": "CUx9KieuW9", "signatures": ["ICLR.cc/2026/Conference/Submission5379/Reviewer_ci1f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5379/Reviewer_ci1f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760842874298, "cdate": 1760842874298, "tmdate": 1762918032049, "mdate": 1762918032049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an embodiment benchmark that systematically evaluates embodied agents across multiple dimensions, including instruction comprehension, perceptual reasoning, generalized planning, affordance prediction, and failure analysis, using a dataset of 6,077 QA pairs. It introduces a task planning evaluation framework based on a work-simulation rollout process, which measures task completion rates according to a directed acyclic graph (DAG) of subtasks. Finally, the benchmark is used to evaluate both open-source and closed-source MLLMs, providing valuable insights derived from the comparative results."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The writing is simple and clear, resulting in good overall readability.\n\nThe benchmark effectively assesses long-horizon planning ability by simulating whether the generated plans achieve key object-state milestones.\n\nIt comprehensively covers diverse hardware configurations including bimanual, single-arm, and mobile robot setups as well as multiple task viewpoints, enhancing its generality and applicability."}, "weaknesses": {"value": "The overall work feels engineering-oriented, focusing mainly on data curation rather than proposing new methods to improve MLLM performance, which limits the novelty and conceptual contribution of the paper.\n\nThe five key capabilities defined in this work appear to have some overlap. For example, embodied instruction comprehension could arguably be considered part of embodied generalized planning, as both involve understanding structured task sequences.\n\nThe motivation and practical value of the failure case analysis seem somewhat unconvincing. Moreover, as shown in Figure 2, identifying the actual cause of failure appears difficult without making additional assumptions.\n\nBased on Figure 4, the so-called systematic evaluation seems somewhat biased — with relatively few instances of affordance reasoning and error analysis, while planning tasks dominate the benchmark."}, "questions": {"value": "Would it be possible to perform a systematic evaluation of embodied agents across multiple dimensions by combining existing benchmarks, rather than creating an entirely new one?\n\nCould you clarify the practical value of the error analysis QA? There are many different types of failure cases, and it seems difficult to anticipate or comprehensively cover them in advance.\n\nAre all the QA pairs based on single images? If so, it may be challenging to accurately capture actions such as whether the robot is opening or closing a drawer.\n\nIt would be helpful to briefly discuss the cost and feasibility of conducting benchmarking experiments on RoboBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TSttU2mHzE", "forum": "CUx9KieuW9", "replyto": "CUx9KieuW9", "signatures": ["ICLR.cc/2026/Conference/Submission5379/Reviewer_pDCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5379/Reviewer_pDCv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877764382, "cdate": 1761877764382, "tmdate": 1762918031858, "mdate": 1762918031858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark for evaluating the cognitive capabilities of MLLMs as brains for robotic manipulation tasks, covering five dimensions—Instruction Comprehension, Perception Reasoning, Generalized Planning, Affordance Prediction, and Failure Analysis. The benchmark is constructed from a mix of large-scale real robotic datasets and curated in-house data, with an emphasis on realistic, diverse embodiments, object properties, and scene configurations. Notably, RoboBench introduces a planning evaluation pipeline using an MLLM-as-world-simulator framework to measure plan feasibility and long-horizon reasoning. Evaluation results on 14 MLLMs reveal limitations of current models in implicit goal inference, perception, planning robustness, affordance reasoning, and diagnosis of execution failures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The inclusion of the multi-view planning and error analysis task are good, which complements some existing evaluations for MLLM in embodied tasks."}, "weaknesses": {"value": "1. The paper mostly uses existing data from other benchmarks to construct this evaluation, without any notable principled data curation method. The goal seems to be mostly just extending coverage of existing benchmarks and meshing together aspects from previous evals. The engineering effort is useful, yet research contributions are limited.\n\n2. Comparison with relevant baseline benchmarks in Table 1 seems to be subjective and somewhat dubious. For example, it is unclear what is being referred to as Robustness Evaluation and why RoboBench has this while others do not."}, "questions": {"value": "Additional citations and comparisons: ECBench, ManipBench, BEAR"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PzhVvUOiCH", "forum": "CUx9KieuW9", "replyto": "CUx9KieuW9", "signatures": ["ICLR.cc/2026/Conference/Submission5379/Reviewer_VnZb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5379/Reviewer_VnZb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945200516, "cdate": 1761945200516, "tmdate": 1762918031534, "mdate": 1762918031534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive benchmark that evaluates MLLMs as the embodied brain for robotic manipulation. It covers five core cognitive dimensions: Instruction Comprehension, Perception Reasoning, Generalized Planning, Affordance Prediction, and Failure Analysis. It includes 15 capabilities, 25 tasks, and more than 6,000 Q&A pairs. The dataset is curated with attention to realism, encompassing diverse robot types, object properties, and environments. The paper also proposes a planning evaluation framework that uses world-simulation rollouts with DAGs to assess embodied feasibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive Scope and Structure: RoboBench uniquely combines five major cognitive dimensions, directly tracing the manipulation pipeline from instruction to error recovery, surpassing existing embodied benchmarks in breadth and integration.\n2. Novel Planning Evaluation Framework: The MLLM-as-world-simulator approach moves beyond text matching by assessing step-by-step plan execution feasibility using ground-truth action lists and annotated DAGs.\n3. The paper is easy to follow and well-strutured."}, "weaknesses": {"value": "1. While the empirical evaluation is rigorous, the paper lacks a deeper theoretical analysis of cognitive failure patterns, limitations in MLLMs’ reasoning processes, or broader learning-theoretic implications. Theoretical insights or analyses, such as attention-based analysis, probing, or architectural examination, are limited.\n2. Limited Analysis of Model Differences: The results show performance gaps but do not explore how different model designs, training data, or prompt styles affect them. For instance, is the weakness caused by weak visual-language connection or missing training examples? Some focused tests or discussion of these factors would make the analysis more insightful than just reporting performance numbers.\n3. It would be helpful if the author could summarize the key insights gained from this benchmark and its evaluation results, particularly regarding how these findings could guide future improvements in MLLMs."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7ORyVl3Ep", "forum": "CUx9KieuW9", "replyto": "CUx9KieuW9", "signatures": ["ICLR.cc/2026/Conference/Submission5379/Reviewer_JASz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5379/Reviewer_JASz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973513944, "cdate": 1761973513944, "tmdate": 1762918031253, "mdate": 1762918031253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}