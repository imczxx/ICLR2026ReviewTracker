{"id": "lsEnQ47VXh", "number": 6657, "cdate": 1757991484604, "mdate": 1759897902571, "content": {"title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning", "abstract": "Recent advances in diffusion-based image generation models (IGMs), such as Stable Diffusion (SD), have substantially improved the quality and diversity of AI-generated content. However, these models also pose ethical, legal, and societal risks, including the generation of harmful, misleading, or copyright-infringing material. Machine unlearning (MU) has emerged as a promising mitigation by selectively removing undesirable concepts from pretrained models, yet the robustness of existing methods, particularly under multi-modal adversarial inputs, remains insufficiently explored. To address this gap, we propose RECALL, a multi-modal adversarial framework for systematically evaluating and compromising the robustness of unlearned IGMs. Unlike prior approaches that primarily optimize adversarial text prompts, RECALL exploits the native multi-modal conditioning of diffusion models by efficiently optimizing adversarial image prompts guided by a single semantically relevant reference image. Extensive experiments across ten state-of-the-art unlearning methods and diverse representative tasks show that RECALL consistently surpasses existing baselines in adversarial effectiveness, computational efficiency, and semantic fidelity to the original prompt. These results reveal critical vulnerabilities in current unlearning pipelines and underscore the need for more robust, verifiable unlearning mechanisms. More than just an attack, RECALL also serves as an auditing tool for model owners and unlearning practitioners, enabling systematic robustness evaluation. Code and data are available at https://anonymous.4open.science/r/RECALL.", "tldr": "RECALL optimizes adversarial image prompts under text conditioning to recover erased concepts, serving both as an attack and as a robustness auditing tool for unlearned diffusion models.", "keywords": ["Adversarial attacks", "Machine unlearning", "Image generation model unlearning", "AI safety", "Stable Diffusion model", "AIGC"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/500853c7e0d1335ac722bbaa27eaf41d4cf77e48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RECALL, a reference-guided latent optimization framework designed to expose vulnerabilities in diffusion models after machine unlearning. The method leverages a multi-modal conditioning setup—combining text and image prompts—to adjust a latent representation until the unlearned model regenerates erased concepts. The paper presents the overall motivation, attack pipeline, and evaluation on multiple unlearning strategies (ESD, FMN, UCE, etc.), reporting higher attack success rates than prior works."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and generally easy to follow.\n\n2. The method’s structure and optimization process are well illustrated and explained.\n\n3. The conceptual idea of leveraging latent-space multi-modal guidance for unlearning attacks is mostly novel."}, "weaknesses": {"value": "Mostly concern about the evaluation of whether the method truly works. The current experimental setup fails to convincingly separate genuine recovery of “forgotten” concepts from trivial replay of reference content:\n\n1. The attack initialization already includes 25% of the reference image ($\\lambda$=0.25), meaning the optimization starts from a latent that partially encodes the harmful concept itself. This makes the reported ASR potentially inflated and methodologically invalid. \n\n2. The evaluation does not remove trivial copies — if the optimized latent simply reconstructs or memorizes the reference image, ASR no longer reflects a true unlearning breach. \n\n3. The paper does not compare the diversity or distributional coverage of generated samples across methods, leaving it unclear whether RECALL actually recovers a broader concept manifold or merely reproduces a few memorized instances compared to other approaches.\n\n4. The lack of ablations for $\\lambda$ approaching 0 (pure noise initialization)  makes it hard to assess whether the method generalizes beyond specific harmful exemplars.\n\n\nIn addition, the paper’s treatment of baselines raises serious concerns. Specifically, the most comparable baseline, UnlearnDiffAtk, has been publicly available since around October 2023, and it is unclear whether any later compatible unlearning attacks were tested. The authors should either include more recent baselines if available, or explicitly clarify in the rebuttal that no newer compatible works exist to justify the current comparison. \n\nMoreover, the authors appear to have **underreported** UnlearnDiffAtk’s performance: its own paper reports 76% ASR (ESD) and 98% (FMN) on Nudity tasks (Table 2), while this paper's Table 1 shows only 51% and 92% respectively. This inconsistency suggests that the baseline reimplementation may be incorrect or incomplete, casting doubt on the claimed relative advantage of RECALL."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eGw5zy75hS", "forum": "lsEnQ47VXh", "replyto": "lsEnQ47VXh", "signatures": ["ICLR.cc/2026/Conference/Submission6657/Reviewer_yC3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6657/Reviewer_yC3h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760676022091, "cdate": 1760676022091, "tmdate": 1762918969195, "mdate": 1762918969195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presented a multi-modal adversarial framework Recall with SOTA results in diffusion model white-box attack settings. The method optimizes the adversarial image in the latent space of the unlearned model itself, requiring no external classifiers. Extensive experiments across ten state-of-the-art unlearning methods and four tasks demonstrate that Recall consistently outperforms existing attacks in success rate, speed, and semantic alignment. The paper shows that current unlearning pipelines are fundamentally fragile against multi-modal adversarial inputs, urging the development of more robust safety measures."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Recall introduced multi-modal (image+text) attack with the text prompt unmodified, which generates the unlearned image while still keeping semantic fidelity to the original unmodified prompt. The experiment results show SOTA accuracy.\n2. Recall is computationally and practically efficient. It doesn't require external models or classifiers. Performing the adversarial optimization directly in the model's latent space is computationally more efficient, which is supported by experiment results.\n3. Recall is shows good generalization across models and tasks. It does not overfit to a specific reference image to guide the attack while still producing diverse outputs.\n4. Extensive generalization study and ablation study.\n5. The paper is well-written, with a clear and compelling narrative from motivation to result."}, "weaknesses": {"value": "1. The paper is more on empirical side. While the results are good, it lacks a theoretical analysis explaining why the multi-modal pathway is so vulnerable or providing formal guarantees about the attack's convergence.\n2. The adv_img even though is effective, it will be easily rejected by real image gen system by simple safe guarding before it reaches to the model.\n3. adversarial prompt attack was proven to be a good method. what about adversarial prompt + adversarial image, will it get higher ASR? There is no such ablation study in experiment."}, "questions": {"value": "1. 50-step DDIM scheduler is inefficient in general. How will the algorithm work with other faster scheduler?\n2. The method focuses on single concept (Van Gogh, or nudity etc.). How about multi-concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jvHzeSAGd8", "forum": "lsEnQ47VXh", "replyto": "lsEnQ47VXh", "signatures": ["ICLR.cc/2026/Conference/Submission6657/Reviewer_iacT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6657/Reviewer_iacT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444046624, "cdate": 1761444046624, "tmdate": 1762918968342, "mdate": 1762918968342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RECALL, an unlearning model attack method designed to operate within the image latent space of diffusion models. The core of the method involves using a reference image to guide the iterative generation of an adversarial latent representation ($z_{\\text{adv}}$), which successfully recovers a supposedly erased target concept (e.g., specific style or object). The authors conduct extensive experiments to evaluate the method's effectiveness, computational efficiency, and robustness, convincingly revealing significant vulnerabilities in current machine unlearning techniques when subjected to image latent space attacks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. RECALL successfully identifies adversarial examples in the latent image space, providing compelling evidence that existing unlearning methods (e.g., fine-tuning, knowledge distillation) fail to fully eradicate sensitive or proprietary concepts.\n2. The paper includes a comprehensive experimental evaluation and extensive ablation studies that thoroughly assess the potential of image-level attacks on unlearning methods across various metrics and unlearning targets.\n3. Efficiency and Practicality: The outstanding experimental results, coupled with a significantly shorter computational time compared to baselines, enhance the practical relevance and real-world applicability of the proposed attack."}, "weaknesses": {"value": "1. Despite the claim of \"reference independence\" in Section 5.5, the method fundamentally relies on a reference image during the adversarial optimization in Stages I and II. The authors must clarify the specific requirements for this reference image. For instance, what characteristics might a reference image possess that could cause the attack to fail or significantly degrade its performance? Furthermore, given the results in Table 4, which suggest that a simple Image-Only attack can already restore the target concept, the current method appears more like an effective way to refine this recovery by finding a latent state that is minimally destructive to the surrounding concept space, rather than a fundamentally new recovery vector.\n2. Insufficient Test Data Coverage: The evaluation is limited by the amount of test data used. To comprehensively assess the robustness of unlearning methods against RECALL, the authors should employ a larger and more diverse dataset.\n3. The paper primarily focuses on finding an adversarial latent in the image latent space ($z_{\\text{adv}}$) to recover the forgotten concept, with no explicit optimization or guidance related to the textual modality beyond standard conditional inputs. Given this, the claim of presenting a \"multi-modal attack\" requires further justification or clarification.\n4. The claim made in Lines 236-249 seems questionable. RECALL appears to be fundamentally an outcome of a trade-off between prompt following and sampling diversity, which aligns more closely with the diverse sampling results shown in Table 6.\n5. The paper would greatly benefit from a brief introductory section or paragraph in the main paper to clarify the common terminology used in this attack space: specifically, the concepts of text-only, image-only, and hybrid/multi-modal attacks/models."}, "questions": {"value": "1. ALGORITHM 1, Line 23 & 24: the final $z_{\\text{adv}}$ obtained at Line 23 appears to correspond to the latent state at time $t=0$ (the clean, final image latent). If this is the case, why is $z_{\\text{adv}}$ then directly fed back into the diffusion model at Line 24? This procedure deviates from the standard DDPM/DDIM sampling process, which typically starts diffusion from a noisy latent state at $t=T$. Conversely, if $z_{\\text{adv}}$ actually corresponds to $t=T$ (a noisy latent), how does the DDIM sampling process manage to produce the diverse recovery results shown in Figure 7?\n2. Table 2 CLIP Score Discrepancy: Table 2 shows that the CLIP Score for the images recovered by RECALL is higher than the score for the original Stable Diffusion (SD) model. Please provide an explanation for this phenomenon.\n3. The Periodic Integration ablation experiment is incomplete. Specifically, ablation studies are missing for the impact of key hyper-parameters such as the periodic interval and the regularization coefficient.\n4. Are LPIPS and IS truly appropriate metrics for evaluating the diversity of the generated images in this attack context? Considering the goal of concept recovery, would a metric based on the variance of DINO scores be a more suitable or complementary choice for measuring recovery diversity?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ogrMaMy1nn", "forum": "lsEnQ47VXh", "replyto": "lsEnQ47VXh", "signatures": ["ICLR.cc/2026/Conference/Submission6657/Reviewer_fUc5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6657/Reviewer_fUc5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743105628, "cdate": 1761743105628, "tmdate": 1762918968011, "mdate": 1762918968011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a multi-modal guided attack framework for unlearned diffusion model, where during its attack process only a single reference image is utilized. Also, authors implemented comprehensive experiments on different kinds of adversarial attack and different victim unlearned diffusion model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. novel multi-modal attack pipeline: latent encoding with reference image blending, iterative latent optimization and the final multi-modal attack using optimized adversarial image with the original text prompt.\n2. Strong empirical validation across diverse settings. The evaluation experiments are impressively comprehensive (10 unlearning methods and 3 attack baselines.) The proposed method, RECALL, consistently achieve the best attack performance and also superior semantic alignment."}, "weaknesses": {"value": "1. Authors have overclaimed the independency of their proposed attack method. During attack process, only a single reference image is needed, however, the reference images are still generated by original diffusion models. So, there is an assumption that the original diffusion models are accessible, which cannot be achieved in some cases. \n2. Although Appx. F claims the robustness across references, the main text underplays the sensitivity of results to poorly aligned or compositionally distinct reference images. A quantitative failure analysis would clarify generality limits.\n3. Some steps resemble prior latent alignment or DreamBooth inversion procedures."}, "questions": {"value": "1. Does RECALL rely on the specific cross-attention fusion mechanism in SD (text-image co-attention), or could it generalize to models like DALLE 3 or Flux that use distinct conditioning pipelines?\n2. Could model owners detect such attacks through latent distribution monitoring? If yes, how does RECALL evade simple detection heuristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "obgz6Kd8TN", "forum": "lsEnQ47VXh", "replyto": "lsEnQ47VXh", "signatures": ["ICLR.cc/2026/Conference/Submission6657/Reviewer_PMHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6657/Reviewer_PMHp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779976049, "cdate": 1761779976049, "tmdate": 1762918967033, "mdate": 1762918967033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}