{"id": "KUfKvlX3VY", "number": 18874, "cdate": 1758291638314, "mdate": 1759897076239, "content": {"title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "abstract": "Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models~(e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose \\texttt{FreeCache}, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. \nSecond, we introduce \\texttt{Guided Diffusion}, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality.\nWe conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver an average of \\textbf{12.14}$\\times$ end-to-end speedup across various tasks with negligible accuracy degradation. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains. Our code and implementation are anonymously available at https://anonymous.4open.science/r/anon-flash-dlm-A42B/.", "tldr": "", "keywords": ["Diffusion Model", "Diffusion Language Model", "Inference Speed", "Language Models", "Efficient Inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5b91299f084100267a3170e9622fbbef1c5ebf1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors identify two main problems in current diffusion language models (DLMs). First, the lack of KV-caching, and second, the reduced quality when generating multiple tokens in parallel. The authors introduce (1) FreeCache, a KV-caching approximation, and (2) GuidedDiffusion, to improve the sampling speed and performance respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The claim on line 207, that the KV projections for the clean tokens exhibit high temporal stability is well-supported, as the authors compute the similarity.\n2. On reasoning tasks, the authors obtain faster generation with a small reduction in performance. \n3. The paper is generally clear and well-written. Aside from the *key* edge case noted in the weaknesses, the implementation details should allow reproducibility."}, "weaknesses": {"value": "### Major Weaknesses\n1. **Overlap with previous work**. The two contributions of the authors overlap with prior work on discrete diffusion. Namely,\n    - **FreeCache**: KV caching for diffusion language models has already been explored in prior work, which unfortunately is not acknowledged or compared against in this submission. See, for example, [1-2].\n    - **GuidedDiffusion**: There is strong overlap with [3]. My understanding is that the main difference is that GuidedDiffusion here uses a large dLLM, whereas [3] uses a smaller one as the drafter and drafts only a few tokens. Since the AR and diffusion greedy predictions are supposed to align in this work, it seems important to compare GuidedDiffusion with [3], where Dream 7B would be the drafter and the AR model would be Qwen.\n\n2. **Key edge case not discussed**: When the AR model and the diffusion model disagree on the next token (i.e., their argmax outputs differ for a given prompt), how is this conflict resolved? Does the AR model take precedence? This scenario is likely to be common, yet I did not see it discussed.\n\n3. **Potentially unrealistic assumption**: The method assumes that the smaller AR model can reliably verify tokens, but if the AR model is weak, this verification should not work (see previous point).\n\n4. **Poor argument of the shortcomings of parallel generation**: Section 3.1.2 misses the most important point: the main challenge in parallel generation is that DLLMs predict a *factorized* distribution, because joint modeling over all masked tokens is computationally infeasible and grows exponentially with vocabulary size (see [1-2]).\n\n### Other weaknesses\n1. Missing citation to relevant background work: D3PM, SEDD, MD4.\n\n2. It is not stated how the similarity between KV projections in adjacent steps is measured (Figure 2). Is it the cosine similarity?\n\n3. **Focus on semi-AR generation not clear form the background**: The background (lines 153-155) mentions position selection based on confidence, but it is not clearly stated that the authors use semi-autoregressive (semi-AR) decoding. Since their approach relies on an AR model to validate tokens, this should be explicitly clarified to improve reader understanding.\n\n4. Lines 101-102: *\"these methods are computationally expensive for computation\"*. This should be clarified.\n\n5. Lines 342-345: *\"Unlike speculative decoding, Guided Diffusion does not require token-wise correction from the auto-regressive model. In other words, the proposed method produces output sequence without introducing the repetitive correction process, which guarantees the latency benefits obtained from the fast diffusion process.\"*. This paragraph sounds generic, and it is not clear what the authors mean. Guided Diffusion still uses an autoregressive model for generation, and, in my understanding, rejects tokens that are not the most likely under the AR model.\n\n6. Lines 348-349: *\"To that end, the reasoning power of the diffusion model is fully preserved, while the AR guider model simply ensures the causality of the output.\"*. This claim seems unjustified. What if the AR model is too weak? Then I would expect it will not be able to align its argmax with diffusion model.\n\n[1] Ma et al., dKV-Cache: The Cache for Diffusion Language Models.\n\n[2] Wu et al., Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding.\n\n[3] Christopher et al., Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion.\n\n[4] Xu et al., Energy-Based Diffusion Language Models for Text Generation.\n\n[5] Lui et al., Discrete Copula Diffusion."}, "questions": {"value": "1. Did the authors consider periodically refreshing the KV cache during sampling, rather than keeping it fixed throughout generation? If so, how does occasional refreshing impact performance?\n\n2. **Focus on greedy decoding**: Lines 154-155, Equation 3: The decoding scheme assumes greedy token selection, whereas most prior work (e.g., MDLM, MD4) uses sampling for token selection and random position choice. Restricting to greedy decoding may be problematic, e.g. for coding or math. Why did the author decide to focus on greedy decoding only? Speculative decoding can handle stochastic sampling for example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LHIapbWF3q", "forum": "KUfKvlX3VY", "replyto": "KUfKvlX3VY", "signatures": ["ICLR.cc/2026/Conference/Submission18874/Reviewer_fREk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18874/Reviewer_fREk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773226588, "cdate": 1761773226588, "tmdate": 1762930841137, "mdate": 1762930841137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel, training-free method to accelerate Diffusion Language Model (DLM) inference. The approach consists of two key techniques: 1) **FreeCache**, an efficient KV cache compression strategy that reduces computational overhead, and 2) **Guided Diffusion**, a collaborative workflow that uses a lightweight autoregressive (AR) model to guide the DLM's token unmasking process. The authors validate their method through comprehensive evaluations on various downstream tasks, demonstrating significant speedups with minimal accuracy degradation. The reported performance improvements are substantial and promising."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written, with a concise and logical explanation of the motivations, insights, and methodological details.\n- The experimental validation is  thorough, including both accuracy benchmarks and detailed inference latency measurements. The demonstrated inference speedup is impressive and a strong contribution."}, "weaknesses": {"value": "- While the overall inference improvement is significant, a detailed breakdown of the contribution from each component (FreeCache vs. Guided Diffusion) would be beneficial. An ablation study showing the speedup and accuracy impact of each technique individually would help readers understand their relative importance.\n- The AR-guided strategy is interesting. However, the analysis could be strengthened by including a discussion or experiment quantifying the inference overhead introduced by the AR model itself. A theoretical or empirical analysis of the computational cost of the guidance step would provide a more complete picture of the net performance gain."}, "questions": {"value": "The paper mentions partitioning the sequence into \"fixed-size blocks.\" How was this block size determined? what is the sensitivity of the results to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aBwzzVHwyY", "forum": "KUfKvlX3VY", "replyto": "KUfKvlX3VY", "signatures": ["ICLR.cc/2026/Conference/Submission18874/Reviewer_rtU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18874/Reviewer_rtU8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834028627, "cdate": 1761834028627, "tmdate": 1762930839790, "mdate": 1762930839790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FlashDLM, an inference-time framework to speed up diffusion language models without extra training. It has two parts. FreeCache observes that the Key/Value projections of finalized tokens change very little across denoising steps and reuses those projections, so later steps only recompute attention for the unfinished suffix instead of the full sequence. Guided Diffusion uses a small autoregressive model to check the DLM’s parallel token proposals and confirm the longest common prefix, which reduces the number of denoising steps and improves coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The method is training-free.\n2) The experiments connect the stability observation to a concrete algorithm and show clear improvements in latency with small or no drops in accuracy.\n3) The paper motivates FreeCache with a figure that shows stability across steps and positions. The description of block partitioning, active window recomputation, and progressive freezing is easy to follow.\n4) The work targets a key bottleneck for DLMs in long-context reasoning: repeated full-sequence passes and parallel decoding incoherence."}, "weaknesses": {"value": "1) The study centers on Dream-7B-Instruct and LLaDA-8B-Instruct with reasoning benchmarks. It is unclear how FreeCache scales to much larger DLMs or to domains such as dialogue safety, coding, or open-ended writing, where coherence needs may differ. Adding more domains or larger models would strengthen external validity.\n2) Guided Diffusion depends on a longest-prefix acceptance rule with top-k AR logits and a confidence threshold τ. A deeper ablation on k, τ, and different AR guiders would clarify robustness and failure modes."}, "questions": {"value": "1) Which block sizes and shrink schedules work best across tasks? Could a dynamic policy that adapts block size based on local uncertainty or AR-agreement rate further reduce recomputation?\n2) Beyond the current longest-prefix rule with top-k and threshold τ, have you tried alternatives such as per-token margin checks, KL limits, or short rollback windows, and how do they affect step count, latency, and accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HfmlZfTEef", "forum": "KUfKvlX3VY", "replyto": "KUfKvlX3VY", "signatures": ["ICLR.cc/2026/Conference/Submission18874/Reviewer_vA4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18874/Reviewer_vA4B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938283204, "cdate": 1761938283204, "tmdate": 1762930838926, "mdate": 1762930838926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### **Paper Summary**  \nDiffusion Language Models (DLMs) offer advantages like **parallel token generation** and **inherent bidirectionality** over autoregressive (AR) LLMs, but suffer from slow inference—their iterative denoising requires multiple full-sequence forward passes, leading to high latency. Additionally, parallel generation causes token incoherence, and reducing denoising steps degrades output quality.  To address these issues, the paper proposes **FreeCache** and **Guided Diffusion** techniques under the FlashDLM framework. **FreeCache** is  a training-free KV caching technique for DLMs: It exploits the temporal stability of KV projections in unmasked tokens to reduce redundant computations, enabling efficient long-context inference without fine-tuning. **Guided Diffusion** is a training-free cross-model guidance method: By using a lightweight AR model to validate DLM draft tokens, it resolves token incoherence and drastically reduces denoising iterations while preserving the DLM’s reasoning power.  Extensive experiments on benchmarks show the combined methods deliver an **average 12.14× end-to-end speedup** with negligible accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths of the Proposed Method**  \n1. **Training-Free Design for Off-the-Shelf Deployment**  \nThe core techniques (FreeCache and Guided Diffusion) require no additional training, fine-tuning, or dedicated calibration runs—they directly accelerate pre-trained DLMs \"off the shelf\". This avoids the overhead of retraining large models and significantly lowers the barrier to practical adoption.  \n\n2. **FreeCache: Targeted KV Caching to Cut Redundant Computation**  \nFreeCache leverages a key insight—KV projections of \"clean\" (unmasked) tokens exhibit high temporal stability across denoising steps—to dynamically shrink the active computation window. By freezing KV projections of completed token blocks and only recomputing for pending blocks, it eliminates redundant full-sequence calculations inherent to vanilla DLMs.  \n\n3. **Guided Diffusion Resolves Parallel Generation Incoherence**  \nGuided Diffusion addresses DLM’s critical token incoherence issue by using a lightweight AR model as a \"coherence supervisor\". It only unmask tokens where the DLM’s draft predictions match the AR model’s outputs, ensuring semantic consistency without sacrificing parallel generation advantages. Unlike heuristic sampling methods that suffer severe accuracy loss with fewer steps, Guided Diffusion recovers accuracy even with aggressive denoising reductions.  \n\n4. **Breakthrough Latency Parity with Autoregressive (AR) Models**  \nFor the first time, the method enables DLMs to match or outperform same-sized AR models in latency while preserving accuracy. This bridges the latency gap that previously restricted DLM adoption in high-throughput scenarios.  \n\n5. **Low Memory Overhead for Practical Deployment**  \nGuided Diffusion introduces minimal additional memory usage when combining DLMs and AR guiders. This efficiency makes the framework suitable for edge or resource-constrained environments."}, "weaknesses": {"value": "### **Weaknesses and Restrictions**  \n1. **Limited Validation Across DLM Architectures and Scales**  \nThe method’s experiments are restricted to only two DLMs: Dream-7B-Instruct and LLaDA-8B-Instruct. It lacks testing on larger DLM variants (e.g., 13B/34B parameter DLMs) or domain-specific DLMs, making it unclear whether FreeCache’s KV stability assumption or Guided Diffusion’s AR supervision generalizes to more complex or specialized DLM structures. \n\n2. **Sensitivity to the Quality and Domain Alignment of AR Guiders**  \nGuided Diffusion relies entirely on a lightweight AR model to ensure token coherence, but the paper provides limited analysis of how AR guider quality or domain mismatch impacts performance.\n\n3. **Fixed Block Size in FreeCache Lacks Adaptive Optimization**  \nFreeCache uses a fixed block size for all tasks and models, with no analysis of how block size impacts latency or accuracy. For short-sequence tasks, a 256-token block may lead to underutilization of the active window. For ultra-long sequences, a fixed block size could prevent optimal window shrinking. The lack of adaptive block size tuning limits FreeCache’s efficiency across sequence lengths and task types.  \n\n5. **Insufficient Comparison with State-of-the-Art DLM Acceleration Methods**  \nWhile the method compares against vanilla DLMs and AR models, it lacks head-to-head evaluation with other state-of-the-art DLM acceleration techniques."}, "questions": {"value": "### **Questions and Suggestions for Authors**  \n\n1. **Can FLASHDLM Maintain Performance on Ultra-Long Contexts?**  \nThe paper claims support for long-context generation (>1024 tokens) but only tests up to 1024 tokens. For ultra-long contexts, FreeCache’s frozen KV blocks may accumulate to cause memory bottlenecks, and Guided Diffusion’s matching ratio could drop due to increased semantic drift. Does FLASHDLM’s latency-accuracy tradeoff hold for these extended sequences, and are there optimizations needed to support them?  \n \n2. **Implement Adaptive Block Size for FreeCache to Improve Task Adaptability**  \nGiven the limitations of a fixed 256-token block size, the authors should design an adaptive block size mechanism. For instance, use smaller blocks for short sequences to reduce redundant computation, and larger blocks for long contexts to minimize window-shrinking overhead. \n\n3. **Evaluate Guided Diffusion with Low-Resource and Domain-Specific AR Guiders**  \nTo enhance practicality, the authors should test Guided Diffusion with low-resource AR models and domain-mismatched guiders."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WiIO55E7mS", "forum": "KUfKvlX3VY", "replyto": "KUfKvlX3VY", "signatures": ["ICLR.cc/2026/Conference/Submission18874/Reviewer_GXxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18874/Reviewer_GXxp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985869955, "cdate": 1761985869955, "tmdate": 1762930838222, "mdate": 1762930838222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}