{"id": "1B7tAhrzT1", "number": 12796, "cdate": 1758210375108, "mdate": 1759897485836, "content": {"title": "Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete Flow Matching", "abstract": "Discrete flow matching, a recent framework for modeling categorical data, has shown competitive performance with autoregressive models. However, unlike continuous flow matching, the rectification strategy cannot be applied due to the stochasticity of discrete paths, necessitating alternative methods to minimize state transitions. We propose a dynamic-optimal-transport-like minimization objective and derive its Kantorovich formulation for discrete flows with convex interpolants, where transport cost depends solely on inter-state similarity and can be optimized via minibatch strategies. In the case of bag-of-words (BoW) sourced flows, we show that such methods can reduce the number of transitions up to 8 times (1024 to 128) to reach the same generative perplexity without compromising diversity. Additionally, path nondeterminism in discrete flows precludes an instantaneous change-of-variables analogue, preventing precise probability estimation available to continuous flows. We therefore propose two upper bounds on perplexity, enabling principled training, evaluation and model comparison. Finally, we introduce Multimask Flow which outperforms masked flows in generative perplexity without sacrificing diversity, particularly when utilizing minibatch Optimal Transport.", "tldr": "", "keywords": ["Discrete Flow Matching", "CTMCs", "Cross Entropy", "Language Models", "Optimal Transport"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7964499e7bc9da505d36a3986cfb75aae2137b6d.pdf", "supplementary_material": "/attachment/cf6aeeef5b80e624f8b0d66012bcf962a7b1db97.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a minibatch OT for discrete flow matching (DFM), which corresponds to OT coupling strategy in continuous FM. They adapt the Kantorovich transport with quadratic cost in continuous setting into DFM and define the categorical dynamic objective. They further derive two practical upper bounds on perplexity for DFM, enabling principled raining, evaluation and model comparison. Finally, they implement the discrete minibatch OT to multitask flow. By experiments on several text datasets, they show that the OT coupling outperforms regular masked DFM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n2. The OT in discrete setting is well motivated and the proposed OT objective is clear and intuitive.\n3. The derived perplexity bounds are useful for model comparisons."}, "weaknesses": {"value": "1. Transition from continuous to discrete case may raise some novelty issue (don't take points off on this) \n2. Minibatch-OT depends on token embeddings (for L2) that co-evolve during training; stability/selection (e.g., EMA embeddings) is only briefly discussed.\n3. Bounds’ tightness is shown in narrower settings; calibration vs exact NLL remains partially open at scale."}, "questions": {"value": "In continuous setting, although OT-FM is promising in theory, it is not very widely used in practice. This may come from the bias of minibatch effect (approximating continuous OT via samples). In the discrete setting, I guess this issue will even be more severe. How sensitive of OT in DFM according to batch size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GAyTfTB0e9", "forum": "1B7tAhrzT1", "replyto": "1B7tAhrzT1", "signatures": ["ICLR.cc/2026/Conference/Submission12796/Reviewer_Yqsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12796/Reviewer_Yqsk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495626138, "cdate": 1761495626138, "tmdate": 1762923605369, "mdate": 1762923605369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores shortest distance transitions in Discrete Flow Matching (DFM) for categorical data and proposes probability estimation by perplexity upper bounds. It proposes a weighted path-length dynamic OT objective whose Kantorovich form depends only on token-level similarity. Under convex interpolants this yields a categorical Benamou–Brenier result. Practically, this enables minibatch OT training with either Hamming or embedding-L2 costs. The paper further derives two computable upper bounds on perplexity for DFM, usable as training losses and evaluation metrics. Empirically, minibatch-OT reduces steps by up to 8× at matched generative perplexity, and the authors introduce Multimask Flow which outperforms masked flows, especially with OT, while adding ~3.4% training overhead in their setup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The categorical Benamou–Brenier-style equivalence (Theorem 3.1) is interesting. The dynamic jump-minimization equals a Kantorovich problem with cost $c(x_0,x_1)=\\sum_i s(x_0^i,x_1^i)$, recovering Hamming distance and L2-embedding costs for different choices of $s$.\n- On OpenWebText with GPT-2–sized models, minibatch-OT cuts steps by ~8× to match the non-OT model’s generative perplexity. Training overhead is reported at ~3.4% for L=128, with favorable scaling to longer sequences."}, "weaknesses": {"value": "- The paper uses generative perplexity judged with external LMs. This could cause judge bias with the choice of external LMs.\n- Reported headline gains are emphasized on the judged metric rather than the bounds themselves.\n- Most results are GPT-2-scale on BoW settings. It’s unclear how benefits translate to larger LMs or real web texts. The method likely generalizes, but evidence is limited."}, "questions": {"value": "1. Could you report correlation between bound values and the generative perplexity across settings, and clarify which bound you recommend for model selection?\n2. Can authors detail hyperparameters choice or provide ablations?\n3. Can authors explore results at >1B parameters? The overhead scaling looks promising. Can you document wall-clock times at larger scales? \n4. How do Hamming vs L2-embedding vs learned metrics compare for step reduction and final quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "udIApB6rFe", "forum": "1B7tAhrzT1", "replyto": "1B7tAhrzT1", "signatures": ["ICLR.cc/2026/Conference/Submission12796/Reviewer_i4qU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12796/Reviewer_i4qU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648312382, "cdate": 1761648312382, "tmdate": 1762923604092, "mdate": 1762923604092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is arguably built on two parts. The first one is centred around a discussion of optimal transport for discrete data, where the authors show that the dynamic formulation of the transport problem is the same as the Kantorovich formulation. In the second part, the authors show multiple upper bounds on the perplexity of a discrete flow matching model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The provided proofs seem mostly fine.\n- The connections between the Hamming distance and the metric are interesting.\n- The connection between the discrete dynamic problem and the Kantorovich formulation seems novel.\n- The authors introduce Multimask Flow, where the vocabulary now also contains multiple mask tokens."}, "weaknesses": {"value": "- Poor typesetting overall. Examples include: line 127 (should not be numbered, nor in align/gather environments), line 159 – 161 (unindented), all tables could benefit from using booktabs, proofs in appendix have lines ending with equality sign – which should really be at the beginning.\n- It seems that the contribution in section 3 is poor. I must admit that I am not even certain of what is exactly proved. It does seem that it introduces the dynamic formulation for the discrete optimal transport in this exact form, but similar formulations exist (with flows on graphs, for instance), and, overall, it is a pretty well-studied area [1, 2, 3]. In general, discrete optimal transport is well-known, and standard libraries such as POT [4] includes the Hamming distance in the library. Other papers also use discrete OT [5]. Overall, I am not certain at all about the novelty in this part whatsoever, but it could be that the presentation, which I found lacking, hinders what the contributions really are.\n- Discrete OT has only been motivated by the fact that it works well in continuous settings. I do not see why we should expect a similar trend in the discrete setting. Moreover, because of the poor scaling, OT is not used in the continuous settings.\n- The alleged improvement enabled by discrete OT is measured terribly in the first experiment: only the number of “jumps” on an obscure dataset which I could not find (citation needed!) is referred to, without controlling for any quality metric. The constant flow of zero has zero jumps, so is arguably better by this sole metric.\n- What are the relative jumps? I believe the metric is not introduced anywhere.\n- Perplexity makes only sense in the autoregressive, so I shall assume that the authors meant the NLL.\n- As for section 4, upper bounds on the NLL exist in MDLM [6], for instance. Discrete flow matching, MDLM and other methods alike are essentially equivalent, and are all trained with the cross-entropy loss; it is quite easy to compute a bound on the test NLL, as the loss itself typically constitutes an (N)ELBO. I am not certain about the novelty; if the tightness is improved upon, it should have been somewhat motivated theoretically.\n- I am not certain how the tightness is proved in the experiments whatsoever.\n- In experiment 5.2, it is claimed that SEDD has an NLL of about 80, whereas in the original paper the authors get it down to about 33.\n- Multimask Flows are never properly introduced, only very briefly in section 5.3. The empirical evidence is weak for “outperforming masked flows”, and I believe that the generative perplexities for these models are much lower in the other works."}, "questions": {"value": "- What is the novelty in your discrete OT section?\n- Why is OT for discrete settings important? As you have seen in your own experiments, it scales poorly for vocabularies larger than… 3. How do you expect to scale it to 50k tokens?\n- You claim that similarity in the embeddings should be used to weigh the cost differently. What embeddings are you referring to? that of your untrained model? an auxiliary, pre-trained model?\n- Do we agree that you meant NLL and not perplexity?\n- For 5.1, do you have quality metrics interacting with the number of jumps?\n- Why Multimask Flows allegedly work better? Could it just be that they have more parameters? Do they use (that is to say, generate) the special tokens introduced? If so, how? Or did I misunderstand the proposed method altogether?\n- What is novel in your new NLL bound?\n- When do you use the bound in the experiments? At all times? for all methods? If you did for SEDD as well, for instance, then your bound is definitely not tighter.\n- Could you please expand on the claims about the tightness of your bounds?\n- How do you estimate the minibatch OT? POT?\n\nOverall, feels like 3 different ideas put together but without the depth that they might all necessitate. The presentations certainly hinders the understanding of the results.\n\n### References\n\n[1] Gabriel Peyré, Marco Cuturi. “Computational OT”.\n\n[2] Ravindra K. Ahuja, Thomas L. Magnanti, James B. Orlin. “Network Flows”.\n\n[3] Robert M. Gray. “Transport distance, Shannon information and source coding”. https://ee.stanford.edu/~gray/gretsi.pdf\n\n[4] Flamary R., Vincent-Cuaz C., Courty N., Gramfort A., Kachaiev O., Quang Tran H., David L., Bonet C., Cassereau N., Gnassounou T., Tanguy E., Delon J., Collas A., Mazelet S., Chapel L., Kerdoncuff T., Yu X., Feickert M., Krzakala P., Liu T., Fernandes Montesuma E. POT Python Optimal Transport (version 0.9.5). URL: https://github.com/PythonOT/POT\n\n[5] Xiaoyang Hou & Tian Zhu, Milong Ren, Dongbo Bu, Chunming Zhang, Xin Gao, Shiwei Sun. “GGFlow: A Graph Flow Matching Method with Efficient Optimal Transport”.\n\n[6] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov. “Simple and Effective Masked Diffusion Language Models”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O53uXKbXyW", "forum": "1B7tAhrzT1", "replyto": "1B7tAhrzT1", "signatures": ["ICLR.cc/2026/Conference/Submission12796/Reviewer_heAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12796/Reviewer_heAo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907259590, "cdate": 1761907259590, "tmdate": 1762923603787, "mdate": 1762923603787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper advances the discrete flow matching framework for training and sampling of CTMC processes over discrete state spaces. The main contribution of the paper are two fold: \n\ni) formulating dynamical optimal transport (OT) for discrete spaces along with empirical validation with mini-batch OT.  \n\nii) proving two upper bounds on the perplexity.\n\nAdditionally, the authors propose a new multimask source distribution. This distribution augments the vocabulary with approximately 50k additional mask tokens (i.e., tokens that have zero probability under the target distribution), and a uniform probability is assigned to each token. Combing the mini-batch OT with multimask source results in improved generative perplexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well written, mathematical theorem clear and well presented. \n\n2. The paper formulate dynamical OT for discrete state spaces and prove the relation to  Kantorovich formulation. This allows training with  minibatch-OT.\n\n3. Table 3 shows new and interesting results for minibatch-OT on generative perplexity."}, "weaknesses": {"value": "1. in subsection 5.2 If the only difference between Table 2 and the table presented in [1] is the model trained with (1-t) time weighting then it is of low novelty compared to previous works.\n\n2. As reported in the appendix, while multimask source gives an improvement in results of generative perplexity with minibatch-OT, it does hurt the perplexity of both with and without minibatch-OT (table 8).\n\n3. The authors states that computing minibatch OT on a batch of 1000 samples adds approximately 3.4% to training cost. However, for larger batches the authors states it can increase training cost by 10-15% which might not be tolerable for large scale models.\n \n4. The paper presents two bounds on perplexity in Equations (13) and (16). As acknowledged by the authors, these results overlap with prior work ([1–4]). While the paper provides a clear and unified exposition, the claimed novelty in enabling comparisons with autoregressive and discrete diffusion models seems somewhat limited given these existing results.\n\n[1] Shi, Jiaxin, et al. \"Simplified and generalized masked diffusion for discrete data.\" Advances in neural information processing systems 37 (2024): 103131-103167.\n\n[2] Zheng, Kaiwen, et al. \"Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling.\" arXiv preprint arXiv:2409.02908 (2024).\n\n[3] Shaul, Neta, et al. \"Flow matching with general discrete paths: A kinetic-optimal perspective.\" arXiv preprint arXiv:2412.03487 (2024).\n\n[4] Haxholli, Etrit, et al. \"Efficient perplexity bound and ratio matching in discrete diffusion language models.\" arXiv preprint arXiv:2507.04341 (2025)."}, "questions": {"value": "1. In section 5.3, the minibatch-OT is evaluated. However the similarity metric $s(x_0^i, x_1^i)$ is not specified. Could the authors please provide this information?\n\n2. The computational complexity of OT solvers typically scales quadratically or cubically with respect to the number of samples. In the paper, the authors mention that the OT solver increases the training cost by 10–15% for larger batch sizes. Could the authors elaborate on this observation? In particular, an estimation of the expected computational overhead for batch sizes comparable to those used in large-scale LLM training would be greatly appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3GYrmbuDsT", "forum": "1B7tAhrzT1", "replyto": "1B7tAhrzT1", "signatures": ["ICLR.cc/2026/Conference/Submission12796/Reviewer_Nso4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12796/Reviewer_Nso4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929620072, "cdate": 1761929620072, "tmdate": 1762923603359, "mdate": 1762923603359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}