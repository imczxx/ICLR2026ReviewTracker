{"id": "2g945Ngc7l", "number": 3054, "cdate": 1757323646916, "mdate": 1759898111345, "content": {"title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping", "abstract": "Recent advances in multimodal large reasoning models (MLRMs) have substantially\nimproved their ability to solve complex textual and visual tasks. However, these\nmodels tend to *overthink* on\nsimple problems, producing unnecessarily lengthy reasoning traces, while\n*under-exploring* on challenging ones, leading to missed solutions. To \naddress this imbalance, we propose **ARES**, a unified open-source framework\nfor *adaptive reasoning* that dynamically allocates exploration effort based\non task difficulty. Our approach is motivated by two key empirical findings:\n(i) while single-token entropy is noisy, *high window-entropy (HWE)\ntokens* (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage\nbenefits easy problems, while increasing it is essential for solving hard ones.\nBuilding on these insights, ARES introduces a two-stage training pipeline. In the\n*Adaptive Cold-Start* stage, we curate multimodal and textual data paired\nwith reasoning traces of length proportional to problem difficulty, equipping the\nmodel with initial difficulty awareness. In the second stage, we develop\n*Adaptive Entropy Policy Optimization (AEPO)*, which uses HWE tokens as\nexploration triggers to decide *when to explore*, and a hierarchical entropy\nreward with dynamic KL control to decide *how much to explore*. Extensive\nexperiments demonstrate that ARES achieves state-of-the-art performance and\nreasoning efficiency across diverse mathematical, logical, and multimodal\nbenchmarks, while closing the gap to leading commercial systems under\nsignificantly lower inference costs. The anonymous code repository is available at https://anonymous.4open.science/r/ARES-60728M.", "tldr": "ARES is a multimodal adaptive reasoning framework that curbs overthinking on easy tasks and promotes deeper exploration on hard ones, achieving state-of-the-art performance efficiently.", "keywords": ["Entropy based Multimodal Adaptive Reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95b33ff26ea096a509c734c993fec0628aea7616.pdf", "supplementary_material": "/attachment/455bebd0e0d42356ca565e41ff4414feee4aba5d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an adaptive-length reasoning approach based on the relative magnitude of high-window entropy (HWE). A dataset comprising samples of alternating difficulty levels is constructed to facilitate cold start. During the reinforcement learning (RL) phase, the model leverages the relative magnitude of HWE along with a hierarchical entropy-based reward signal to determine whether to explore further and to what depth. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is logically clear, accurately expressed, and easy to read; the experiments are substantial in part, demonstrating the effectiveness of certain proposed methods."}, "weaknesses": {"value": "1. Experiments\n\nThe paper premises that token-level entropy is noisy and argues that window-based entropy is therefore more advantageous. While the paper presents partial experiments on detecting reasoning-critical tokens, it lacks a quantitative analysis and fails to provide direct validation of the method's effectiveness on the main benchmark. Please provide comparative results on major standard benchmarks to substantiate the overall efficacy of the proposed approach.\n\nAdditionally, certain experimental results on Vision-G1 appear inconsistent with the originally reported values in the prior work (e.g., MM-Start: 66.0 → 63.1 on WeMath, etc.), which raises concerns about result reproducibility or evaluation consistency.\n\nThe paper lacks comparison with works on adaptive chain-of-thought reasoning; what are its advantages over them?\n\n[1] Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLMReasoning\n\n2. Methodology\n\nThe core idea of the paper hinges on using the relative magnitude of entropy to assess uncertainty. Although the method shifts from single-token entropy to high-window entropy (HWE), the fundamental concept remains closely related to a substantial body of prior work on entropy-based uncertainty estimation. This proximity to existing approaches diminishes the perceived novelty and originality of the proposed method.\n\n[1] Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models, 2024.11\n\n[2] Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity, 2025.05"}, "questions": {"value": "The experiments show significant improvements on other metrics; however, performance decreases on MathVista. What is the underlying reason for this degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "26AZ77gYbL", "forum": "2g945Ngc7l", "replyto": "2g945Ngc7l", "signatures": ["ICLR.cc/2026/Conference/Submission3054/Reviewer_gA48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3054/Reviewer_gA48"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761314074689, "cdate": 1761314074689, "tmdate": 1762916529916, "mdate": 1762916529916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARES, an adaptive inference framework for MLRMs, designed to address the issues of models “overthinking” on easy problems and “under-exploring” on hard ones. ARES employs a two-stage training pipeline:\n\n1. **Adaptive Cold-Start (AdaCS)**: Constructs a dataset where reasoning length correlates positively with problem difficulty, enabling the model to initially acquire difficulty-awareness.\n2. **Adaptive Entropy Policy Optimization (AEPO)**: Uses High Window Entropy (HWE) as an exploration trigger and introduces AEPO to adaptively govern both when to explore and how much to explore."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Precise Problem Identification**: Clearly pinpoints the non-adaptive reasoning-length allocation in existing MLRMs and empirically reveals (Figure 1) the interaction pattern between entropy and problem difficulty.\n2. **Novel Method Design**:\n   - Proposes **Window Entropy** as a more robust exploration trigger compared to single-token entropy;\n   - Designs a **hierarchical, difficulty-stratified reward function**, applying distinct entropy regulation strategies for easy/medium/hard problems;\n   - Introduces a **dynamic KL weighting mechanism**, relaxing constraints within high-entropy windows to enable token-level “thinking budget” allocation."}, "weaknesses": {"value": "1. **Limited Scale of Empirical Analysis**: The conclusion in Figure 1 regarding the “entropy–difficulty interaction” remains unclear whether the sample size and diversity are sufficient, casting doubt on the generalizability of the findings.\n2. **Weak Theoretical Support**: The proof in Appendix L—claiming a linear relationship between response length and the number of high-entropy tokens—relies on strong assumptions, and the resulting bound is overly loose, offering limited practical guidance. Moreover, the phenomenon itself is fairly intuitive, diminishing the theoretical contribution.\n3. **Clarity and Readability Issues**:\n   - The main text heavily depends on the appendix, disrupting reading flow;\n   - Mathematical symbols are introduced without clear definitions upon first appearance, raising the barrier to understanding."}, "questions": {"value": "1. **Implementation Details of the Exploration Mechanism**: Upon detecting high window entropy, the model “branches additional trajectories” (Section 3.2.1). However, the paper does not specify how many new trajectories are generated—is this number fixed or dynamically determined?\n\n2. **Choice of Window Size**: A window size of w = 4–8 is deemed optimal. But does this hold across different tasks (e.g., pure text vs. multimodal) or model scales? Is there an adaptive mechanism for selecting w?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DautQdpIMw", "forum": "2g945Ngc7l", "replyto": "2g945Ngc7l", "signatures": ["ICLR.cc/2026/Conference/Submission3054/Reviewer_A67V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3054/Reviewer_A67V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467142335, "cdate": 1761467142335, "tmdate": 1762916529489, "mdate": 1762916529489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **ARES**, a two-stage, adaptive reasoning framework for multimodal large reasoning models (MLRMs). The key idea is to use **high window entropy (HWE)** (i.e., the average token entropy in a sliding window) as a reliable signal for *reasoning-critical moments*, and then adapt exploration accordingly. \n\nIn the stage of **Adaptive Cold-Start (AdaCS)**, the model is fine-tuned on curated textual/multimodal data where *reasoning length is coupled to problem difficulty*. With regard to **Adaptive Entropy Policy Optimization (AEPO)** in RL stage, ARES (a) uses an HWE-based trigger to decide **when to explore** (using a batch-level 95th-percentile threshold and (b) employs a **hierarchical entropy reward** plus **dynamic KL control** to decide **how much to explore**. The intrinsic shaping term penalizes over-thinking on easy items and under-exploration on hard items via bucket-dependent deviations from the batch mean number of HWE tokens.\n\nExperiments across many benchmarks (e.g., MathVista, MathVision, MMMU-Pro) show improved accuracy-vs-length trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well-written and easy to follow, with a clear description of the method.\n2. The paper provides intuitive visual demonstrations to help better understand the paper."}, "weaknesses": {"value": "1. The paper contains several design choices that feel biased and currently under-validated by ablations: some are custom (e.g., the rate-to-length coupling that sets the target length for pass-rate p via linear interpolation from all-correct/all-wrong samples, as well as the online difficulty buckets), and others follow prior work (e.g., the 95th-percentile threshold). It remains unclear whether the gains on downstream reasoning benchmarks are sensitive to these hyperparameters or to sequence length; please add robustness ablations to substantiate these specific settings.\n\n2. **Training cost comparison lacks**: the method uses a two-stage pipeline (cold-start + RL) and introduces an additional hierarchical reward in the RL stage. Under identical training data and hardware, please report the training overhead relative to alternative algorithms (time/memory/GPU-hours/tokens processed) to clarify whether the efficiency trade-off is favorable.\n\n3. **Fair comparison needed**: as noted in `Line 315–322`, the AEPO surrogate objective is essentially based on DAPO; therefore, a **DAPO-only** baseline trained under the same protocol should be included for a fair comparison, rather than only contrasting against a base model or showing `Fig.3`-style downstream results without the DAPO's designs (e.g., higher clipping).\n\n4. In `Tab.2`, the ablation shows that **ARES-CS-Vanilla** outperforms **ARES-CS-7B** on **OlympiadBench**, **AIME25**, **MATH500**, and **GSM8K**, with shorter reasoning on 3/4 benchmarks, which appears to contradict the intended effectiveness of the proposed rate-to-length sampling strategy; please consider running an RL stage starting from **ARES-CS-Vanilla** and compare it directly to **ARES-RL-7B** to verify whether the strategy is actually effective.\n\n5. The ablation in `Tab.3` does not report how **Dynamic KL Loss** and **Entropy Reward** affect response length (and its variability), making it impossible to judge whether these components truly achieve their stated motivation; please add response-length metrics and curves to the table/analysis.\n\n6. Finally, the central motivation is to enable **adaptive reasoning** that mitigates overthinking on simple problems and short thinking on hard ones, but a single response-length comparison in `Tab.2` is insufficient to validate this claim; please include targeted analyses (e.g., per-difficulty accuracy/length and trigger usage) to directly demonstrate adaptation rather than relying only on aggregate length differences."}, "questions": {"value": "See the `Weaknesses` part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z99qPpDLAs", "forum": "2g945Ngc7l", "replyto": "2g945Ngc7l", "signatures": ["ICLR.cc/2026/Conference/Submission3054/Reviewer_eWR6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3054/Reviewer_eWR6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549382812, "cdate": 1761549382812, "tmdate": 1762916529304, "mdate": 1762916529304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for training multimodal large reasoning models (MLRMs) that dynamically adjust their reasoning depth according to task difficulty. Existing MLRMs tend to “overthink” simple problems and “under-explore” hard ones, leading to inefficiency and reduced accuracy. To overcome this, the authors introduce ARES, which combines two training stages: (1) Adaptive Cold-Start (AdaCS), aligning reasoning length with task complexity to instill difficulty awareness, and (2) Adaptive Entropy Policy Optimization (AEPO), which uses high window entropy (HWE) tokens as triggers for exploration and a hierarchical entropy-based reward to regulate reasoning depth. ARES introduces token-level entropy shaping and dynamic KL regularization to control when and how much to explore. Experiments show that ARES outperforms strong baselines on multimodal and textual reasoning benchmarks, improving both accuracy and inference efficiency, demonstrating that entropy-guided adaptive reasoning can balance performance and computation cost"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novel Conceptual Contribution: Introduces the use of window-level token entropy as an interpretable and quantitative signal to trigger reasoning exploration, bridging uncertainty modeling with adaptive inference.\n\n* Principled Methodology: The dual-stage pipeline (AdaCS + AEPO) offers a theoretically grounded and empirically validated mechanism for aligning reasoning length with difficulty.\n\n* Empirical Rigor: Extensive experiments across 10+ multimodal and textual reasoning benchmarks demonstrate consistent improvements over state-of-the-art open-source models at both 3B and 7B scales.\n\n* Efficiency–Accuracy Balance: The adaptive exploration mechanism leads to shorter reasoning chains for easy tasks and deeper exploration for difficult ones, improving token efficiency without performance degradation."}, "weaknesses": {"value": "* Hyperparameter Sensitivity: Although claimed to be “hyperparameter-free,” several thresholds (e.g., 95th percentile entropy cutoff, window size 4–8) could affect stability and may require empirical tuning.\n* Although ARES demonstrates clear effectiveness at moderate-to-large scales (3B–7B parameters), all experiments are limited to the 7B model, with no quantitative evidence for models beyond this range (≥13B–70B). Consequently, the scalability of the entropy-shaping mechanism to very large foundation models remains uncertain."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nEiDq4Mm60", "forum": "2g945Ngc7l", "replyto": "2g945Ngc7l", "signatures": ["ICLR.cc/2026/Conference/Submission3054/Reviewer_oeN6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3054/Reviewer_oeN6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654016508, "cdate": 1761654016508, "tmdate": 1762916529141, "mdate": 1762916529141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}