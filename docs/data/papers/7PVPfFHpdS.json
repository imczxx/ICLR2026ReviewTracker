{"id": "7PVPfFHpdS", "number": 20123, "cdate": 1758302759488, "mdate": 1759897000546, "content": {"title": "TraMEL: An Exemplar Replay-Based Continual Learning Framework for Malware Traffic Analysis", "abstract": "Most prior work on continual malware detection has centered on static code analysis. In contrast, this paper explores continual learning (CL) for malware traffic analysis, which leverages encrypted flow features to capture behavioral signals resilient to obfuscation and encryption. Unlike intrusion detection systems that focus on coarse anomaly detection, malware traffic analysis requires fine-grained family-level classification under evolving and imbalanced distributions, making it a distinct and challenging setting for CL.\n\nWe introduce TraMEL (Traffic-based Malware Exemplar Learning), a replay-based CL framework designed for malware traffic. TraMEL integrates (i) adaptive exemplar selection to balance long-tailed family distributions and (ii) an exemplar refinement phase to counter task recency bias while operating under strict memory budgets. We evaluate both standard class-incremental and temporally shifted scenarios, showing that TraMEL consistently outperforms strong CL baselines such as iCaRL, ER, and TAMiL by 10â€“30 percentage points on CICAndMal2017 and IoT23. Remarkably, it approaches the performance of joint training, a theoretical upper bound with full access to past data. These results demonstrate that CL on malware traffic is both feasible and practical, providing a memory-efficient approach for real-world detection. Code is available at \\url{ https://anonymous.4open.science/r/ICLR2026-code-D575/}.", "tldr": "", "keywords": ["Malware Detection", "Continual Learning", "Machine Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b861205d3dd2011fc02b3b8eb483c9aa194b5038.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a continual learning framework for malware traffic analysis, namely TraMEL.\nThe methodology relies on large replay buffer, and the training loss which penalizes regression of accuracy on previously-correctly-classified samples. The classifier is then chosen among different strategies, selecting transformers for this cause.\nWhen compared with the state of the art of continual learning (not continual learning dedicated to traffic), results are mixed, with ER competing with TraMEL (even if ER is not strictly for malware detection)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ this paper ships together interesting techniques to provide a continual learning framework for traffic malware detection\n+ interesting choice of including a regression-free loss function into the objective function\n+ it is also interesting that transformers are acting well on tabular data"}, "weaknesses": {"value": "The paper is interesting, but the scope is very narrow for being considered by a venue like ICLR.\nThis contribution is more suited to a Security conference (A or B) like EuroS&P, ECML, SAC, or workshops like AISec, DLS, WoRMA, etc.\nThe reasons behind these comments are the following:\n\n**1) Incremental contribution that does not provide insights on how to use domain knowledge.** The main modification is using the regression-free loss, while all the other components seem to be already investigated and proposed by prior work, outside the malware domain. Also, while the paper clearly states at the beginning that domain knowledge is rarely used in these contexts, it falls into the same problem. If the issue was the length of the replay buffer (which the experiments show it was) then also other methods outside security can be used by tweaking that parameter. Also, the strict resource requirements are mentioned, but the paper does not provide a quantitative way to judge such claim. Thus, the contribution feels more as an application of a new loss rather then providing systemic insights in the domain.\n\n**2) Not discussed why there is a boost in performance.** Is it because the transformer is useful? Or is the sampling? Only the buffer replay size is discussed, but no complete ablation study is provided.\n\n**3) Tabular data = ensembles of trees.** Why the paper deploys deep networks, when tabular data is empirically proven to be the best data format for ensembles of trees? See attached references. However, this would not be a contribution, but the correct usage of prior works published on top-tier venues.\n\n**4) No limitations are discussed.** The paper has limitations, related to the choice of architecture, of the choice of the dataset, and other many that the paper does not take into account, being a huge miss for this paper.\n\n**5) Missing baselines.** While the paper states that there are few work on continual learning for traffic data, some are presented. It is not clear why they were not evaluated. The papers state they are just not practical, without being convincing on why. \n\n**Minor comments.**\n1. there is an Appendix ?? reference broken at page 4\n2. space is used strangely, with paragraphs with different gaps\n3. RMA attacks exist? Is there a reference to those? Or is it a novel concept here?\n4. Figure 1 is not really informative for the paper.\n**References.**\n\nGrinsztajn, L., Oyallon, E., & Varoquaux, G. (2022). Why do tree-based models still outperform deep learning on typical tabular data?. Advances in neural information processing systems, 35, 507-520.\n\nShwartz-Ziv, R., & Armon, A. (2022). Tabular data: Deep learning is not all you need. Information Fusion, 81, 84-90.\n\nGorishniy, Y., Rubachev, I., Khrulkov, V., & Babenko, A. (2021). Revisiting deep learning models for tabular data. Advances in neural information processing systems, 34, 18932-18943.\n\nFernÃ¡ndez-Delgado, M., Cernadas, E., Barro, S., & Amorim, D. (2014). Do we need hundreds of classifiers to solve real-world classification problems?. The journal of machine learning research, 15(1), 3133-3181."}, "questions": {"value": "1) Which is the reason that bring a boost i performance? The architecture of the classifier? The replay buffer? The regression-free loss? Or is it the combination?\n2) Why the paper uses complex networks, where ensembles of trees are empirically proved the best on tabular data (given a good feature extractor)?\n3) What is exactly the contribution w.r.t. standard continual learning methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KFrAGg2f5T", "forum": "7PVPfFHpdS", "replyto": "7PVPfFHpdS", "signatures": ["ICLR.cc/2026/Conference/Submission20123/Reviewer_jRDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20123/Reviewer_jRDc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403847939, "cdate": 1761403847939, "tmdate": 1762933022818, "mdate": 1762933022818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the TraMEL framework for fine-grained malware family classification within non-stationary network traffic. The framework addresses the challenges of catastrophic forgetting and class imbalance through a memory-efficient strategy that combines diversity-aware exemplar selection and an exclusive exemplar refinement phase. \n\nTo achieve this, it introduces a dual-loss objective during refinement to explicitly counteract task recency bias, thereby maintaining a robust balance between stability and plasticity. Empirically evaluated on two datasets under both class-incremental and temporal-shift scenarios, it demonstrates performance gains over strong replay-based baselines. This closely approaches the theoretical upper bound of joint training accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper formalizes malware traffic analysis as a unique and challenging Class-IL problem.\n\n+ The introduction of an exclusive refinement phase, combined with tailored distillation losses ($\\mathcal{L}_{past}$ and $\\mathcal{L}_{current}$), is a well-motivated and empirically demonstrated method for countering the prevalent task recency bias.\n\n+ The finding that transformer architectures are superior for malware traffic's tabular data structure is important.\n\n+ The framework achieves significant and consistent performance increases (10-30 percent) over strong, established CL baselines across multiple datasets."}, "weaknesses": {"value": "- Unclear use of some terms.\n\n- The selection of datasets and baselines should be argued better.\n\n- The refinement mechanism and fixed buffer size inherently limit scalability, as the efficacy of rehearsal degrades dramatically if the memory budget cannot be proportionally maintained as the number of tasks grows indefinitely.\n\n- The cluster size ($N_{k}$) for exemplar selection and the distillation weights ($\\alpha, \\beta$) require extensive empirical tuning, which may make it complex to deploy or generalize to new, unseen datasets without a similar tuning effort."}, "questions": {"value": "The following needs clarification, better arguments to make the paper stronger. \n\n(1) The exemplar refinement phase is great, but I felt that its computational burden and dependency on specific loss weight tuning deserve a deeper discussion regarding future scalability. Specifically, while the refinement effectively counters recency bias, it introduces a separate, mandatory optimization step after every task, increasing the wall-clock time and computational load relative to standard replay methods. The paper can consider quantifying the exact overhead (e.g., training time in minutes/hours) added by the refinement phase compared to the joint training phase alone, especially in the tightest budget scenarios. \n\n(2) The K-means clustering-based selection (TraMEL-K) is shown to be crucial for tight memory budgets, but its reliance on a pre-determined optimal cluster number ($N_{k}=600$ for IoT23) is unclear to me. Here, I understand that the dependence of performance on the specific, empirically found value of $N_{k}$ (Section A.2, Table 6) suggests that this hyperparameter may not generalize well. If $N_{k}$ must be tuned per dataset, it negates some of the framework's practical use cases.\n\n\n(3) The paper defines the temporal-shift scenario as disjoint families grouped by the year of first appearance, which is acknowledged as a stricter than real deployments lower bound. Although the retrograde malware attack threat model is excellent, the current temporal-shift benchmark does not fully capture the recurring nature of the RMA's third phase (reintroducing legacy families or slightly altered variants). This is because the paper uses only disjoint families, and it assesses drift, not genuine recurrence.\n\n(4) The paper's foundational arguments on buffer constraints, related work, and core assumptions need stronger substantiation to clearly frame its novelty.\n\n- The paper repeatedly asserts that detectors must operate under strict memory budgets due to storage and scalability limits. This motivation is currently asserted without concrete, domain-specific justification. \n\n-  The term closed-world assumption is central to differentiating this work from traditional IDS, but it is never formally defined. \n\n-  The Related Work section is dismissive toward several recent, relevant CL efforts in network security, primarily by stating their limited relevance.  I believe,  instead of dismissing related CL works (\\eg SPIDER, SPCIL) or other malware CL methods (\\eg MalCL), the paper can briefly and precisely articulate the technical difference in feature space."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EtJqUYvJKk", "forum": "7PVPfFHpdS", "replyto": "7PVPfFHpdS", "signatures": ["ICLR.cc/2026/Conference/Submission20123/Reviewer_XFd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20123/Reviewer_XFd3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761670510, "cdate": 1761761670510, "tmdate": 1762933021862, "mdate": 1762933021862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TraMEL, an exemplar replay-based continual learning (CL) framework for fine-grained malware family classification from encrypted network traffic. It addresses catastrophic forgetting in class-incremental (Class-IL) and temporal shift scenarios, evaluating multiple exemplar selection strategiesâ€”random sampling, class-mean (following iCaRL), and clustering-based (K-means) to handle long-tailed distributions, along with fixed-memory buffer management and a refinement phase using distillation losses weighted by hyperparameters Î± and Î² to mitigate recency bias. Evaluated on CICAndMal2017 (Android, 42 families) and IoT23 (IoT, 9 families) datasets using disjoint class and temporal splits, TraMEL outperforms baselines like ER, iCaRL, and TAMiL by 10-30% in accuracy, approaching joint training under tight buffers (0.5% of data, 3000 samples). Additional contributions include the Retrograde Malware Attack (RMA) threat model and exploration of backbones (MLP, CNN, ViT)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel problem framing: TraMEL extends continual learning to malware-traffic analysis, addressing encrypted flow data rather than static code features an underexplored yet operationally realistic domain. \n\nRefinement for bias mitigation: The two-phase training scheme, joint training followed by exemplar-only refinement, substantially reduces recency bias. \n\nInterpretable stabilityâ€“plasticity control: The analysis of the refinement loss coefficients (Î±, Î²) offers clear insight into how past-knowledge preservation and current-task retention interact, enabling tunable control over forgetting. \n\nThe evaluation in two complementary settings: standard class-incremental learning (Class-IL) with disjoint families as a strict worst-case benchmark and a temporal split grouping families by time of first appearance to simulate natural drifts, offers a thorough assessment of the model's robustness, capturing both controlled and realistic evolutionary dynamics in malware traffic. \n\nMemory-efficient: Fixed-capacity buffers compared to baselines with proportional per-class quotas scale from 200â€“60 k exemplars, maintaining near-joint accuracy while operating within realistic storage limits"}, "weaknesses": {"value": "Lack of imbalance-aware evaluation: The accuracy results give a solid picture of overall\nperformance, showing the thorough testing done. For even better understanding of how the\nmodel deals with rare malware types where common ones might overshadow them, adding\nbalanced metrics that weigh all types equally (like average F1-score, recall across classes, or\nprecision-recall curves per class) would be helpful.\n\nLack of open-world/label-scarce robustness. Testing with fully labeled and known malware\ntypes sets a strong foundation. To make it more applicable to real-world situations where new\nthreats show up without labels, adding checks for handling unknown attacks or learning from\npartially labeled data would be a nice step. An ablation study on completely unknown malware\ntypes can be helpful to determine the real-world scenario\n\nCross-dataset tuning fairness is unclear. The tuning of key settings (Î±, Î²) on one dataset\n(CICAndMal2017) is clearly described and thoughtfully done. To build more trust in results\nacross datasets, noting if the other dataset (IoT23) used the same settings or was tuned on its\nown would ensure everything is fair and make it easier for others to repeat or expand the work.\n\nBackbone ablation is underreported. Using a Vision Transformer as the main model looks\npromising, with the initial results suggesting it beats simpler options like multi-layer perceptrons\nor convolutional networks. To back this up fully and give a clearer view, adding more detailed\nnumbers for those alternatives in the comparison section would make the advantages stand out\nbetter."}, "questions": {"value": "The paper excludes two minority families (Torri, Trojan), reducing IoT23 from 11â†’9\nclasses. Could you clarify the rationale? This would provide valuable context for readers\nreplicating or extending your experiments.\n\nWhen ð¾ is full and per-class quotas shrink after task ð‘–, which exemplars are removed\nfrom earlier classes, and do you re-encode/re-cluster old classes under the updated\nmodel?\n\nWere MLP, CNN, and ViT evaluated under identical settings, like the same buffer ð¾,\nrefinement budget ð‘˜, and distillation weights (ð›¼, ð›½) when the claim about ViTâ€™s superior\nperformance was made?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JiLmacclQX", "forum": "7PVPfFHpdS", "replyto": "7PVPfFHpdS", "signatures": ["ICLR.cc/2026/Conference/Submission20123/Reviewer_2rZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20123/Reviewer_2rZ4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794956749, "cdate": 1761794956749, "tmdate": 1762933021026, "mdate": 1762933021026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TraMEL (Traffic-based Malware Exemplar Learning), a replay-based continual learning (CL) framework tailored to malware traffic analysis (MTA) with encrypted flows. \n\nTraMEL combines imbalance-aware exemplar selection (random, class-mean, and diversity-oriented K-means) and a post-task exemplar refinement phase that reduces task-recency bias via a composite loss balancing cross-entropy with two distillation terms that anchor both past and current behavior. The authors define a refinement objective that explicitly trades off plasticity vs. stability.\n\nEvaluated in Class-IL and temporal split settings on CICAndMal2017 and IoT23, TraMEL consistently outperforms strong replay baselines (ER, iCaRL, TAMiL), narrowing the gap to joint training while operating under strict memory budgets; the study also analyzes buffer size, refinement epochs, and loss weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors focused the study on encrypted malware traffic at family level, a more realistic and fine-grained setting than conventional binary IDS settings.\n\n- Two-stage design (diversity-aware exemplar selection + refinement) targeted to long-tailed, evolving families under tight buffers; the dual-distillation refinement is thoughtfully motivated.\n\n- Comprehensive evaluation on CICAndMal2017 and IoT23 across Class-IL and temporal splits; repeated runs; reports task-wise/mean accuracy and forgetting.\n\n- Demonstrates that memory-efficient replay with careful selection and refinement can deliver robust, fine-grained malware family classification under driftâ€”relevant to practical SOC/defense pipelines with retention constraints."}, "weaknesses": {"value": "- The paper argues that real deployments exhibit family recurrence, yet both Class-IL and temporal settings keep disjoint families across tasks (a conservative lower bound). This complicates claims about performance under true recurrence, open-set families, or re-emergence. A small synthetic recurrence experiment (e.g., re-introducing early families later) would strengthen the validity.\n\n- Baselines are standard replay methods; however, related class-imbalance-aware CL or compressed/coreset replay variants are not included empirically. Even a small-scale comparison (or discussion) would contextualize TraMELâ€™s diversity-aware selection.\n\n- Since the study emphasizes practical constraints and evolving distributions, it would be useful to discuss continual semi-supervised one-class approaches for malware detection on trafficâ€”an adjacent but distinct framing that reduces label burden (e.g., Continual Semi-Supervised Malware Detection, MAKE 2024), and clarify how TraMELâ€™s supervised replay compares or could be hybridized."}, "questions": {"value": "- Could you report a recurrence experiment (e.g., re-introduce a subset of Task-1/2 families at Task-5/6) to quantify retention when families returnâ€”potentially with smaller buffers? This would align the setup with the paperâ€™s motivation (re-emergent variants). \n\n- You note that larger k (e.g., >100) improves coverage. How sensitive are results to k vs. per-class sample counts, especially for extreme long tails? Any heuristics you recommend? \n\n- Have you thought about adding automatic tuning (or an early-stopping criteria during refinement) to bound extra compute?\n\n- Given labeling constraints in MTA, how does TraMEL interact with semi-supervised or one-class continual regimes? A discussion (or small pilot) contrasting TraMELâ€™s supervised replay with continual semi-supervised detection on traffic data would be valuable; for instance, see Continual Semi-Supervised Malware Detection (MAKE 2024).\n\n- I suggest adding a compact pseudocode box for the three-phase loop. Please, also, include the seeds and data splits you used for temporal grouping for easier replication."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WXl3DlyNHn", "forum": "7PVPfFHpdS", "replyto": "7PVPfFHpdS", "signatures": ["ICLR.cc/2026/Conference/Submission20123/Reviewer_BCzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20123/Reviewer_BCzy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974564246, "cdate": 1761974564246, "tmdate": 1762933020565, "mdate": 1762933020565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}