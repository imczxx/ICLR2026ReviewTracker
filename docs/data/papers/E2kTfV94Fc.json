{"id": "E2kTfV94Fc", "number": 21330, "cdate": 1758316295469, "mdate": 1759896928047, "content": {"title": "Incoherent Beliefs & Inconsistent Actions In Language Models", "abstract": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30\\% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.", "tldr": "We show that LLMs are inconsistent in their beliefs, and also take actions contrary to those beliefs.", "keywords": ["llms", "calibration", "consistency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b030797dc78c35592b908e716910993c3ef7df8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the consistency of “beliefs” in LLMs through a number of different experiments. First, model classification probabilities are elicited in response to limited evidence (a medical diagnosis task), then elicited again with additional evidence, in order to test how models update on new evidence. Do they update according to Bayes Rule? Apparently not. Second, it is examined whether models would place bets that relate properly to their elicited beliefs (event probabilities). Do they bet rationally? It seems not. Lastly, an experiment is conducted where a simulated user pushes back on a model’s answer to a question, and it is measured whether the model flips its answer — this flipping behavior is correlated against the model’s initial credence in its answer. There seems to be a mildly positive correlation between strength of initial belief and the model’s propensity to stick with its original answer in the face of a user challenge. Finally, the paper seeks to understand whether the three notions of belief consistency above can be understood in terms of the model’s calibration or accuracy on the underlying task. Results here are mixed."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Very important: The paper tackles an important and timely question: do models have consistent internal beliefs that guide behavior, including self-reports and actions subject to constraints of rationality, like betting? Overall, the paper suggests that they generally do not.\n- Important: The paper approaches its central question through multiple experiments that all provide some evidence for the full picture. This is necessary for a complex question like the question of whether models coherently update their beliefs and coherently act on them."}, "weaknesses": {"value": "- Very important: I think the primary weakness of the paper is that it spreads itself too thin across too many experimental directions. By my count, the paper conducts at least five quite distinct experiments, (Secs 4, 5, 6, 6.2, and 7). This ambition immediately becomes a stumbling block for the analysis. Beginning with Section 4, I had many concerns. How could we make a general claim about Bayesian updating in models based on only one dataset? This dataset concerns a specific task, and the results could be confounded by at least three variables: (1) the task involves significant prior knowledge, (2) the task involves numerical reasoning, and (3) the task is often refused by the model when probing involves free-text generations. Without exploring the space of tasks more thoroughly, I find it extremely difficult to be confident in the claim that models do not do Bayesian updates on new evidence. Limited space also makes it hard to present all the critical details to each experiment. On Sec 3, I do not understand how p(E|.) is computed. Should we take p2 or p2* as the model’s “chief” way of computing its posterior? What should we make of them being inconsistent? Is the only conclusion that the model doesn’t do Bayesian inference, or could we conclude that it can do Bayesian inference through the computations in p2, but for some reason the computations involved in p2* are not successful and therefore don’t represent the model’s true posterior? I would go on with similar issues for each of the following sections. While each direction is very interesting, it is hard to walk away confident in the conclusions presented (and the results are often quite noisy / unstable, as in Sec. 7).\n- Of some importance: Just as a technical point, there is an ambiguity in the claim at lns 317-319. It is true that a model should be more willing to defend answers that it has higher confidence in. That is not necessarily the same thing as a model’s p(answer|question). The reason for this is that the important quantity here is “stickiness” of p(answer|question), not the absolute magnitude of p(answer|question). Stickiness is a function of the strength of evidence for the answer. I find it hard to put this succinctly, but will try here. Consider being asked to bet on an event X occurring. This should depend on your p(X). If you do a little bit of research, your p(X) might be 0.7, but you might still be quite open to adjusting that number in the face of new evidence. If you’ve done tons of research, you might be quite confident that 0.7 is the right probability for the event, and you would be less willing to change your credence in the face of new evidence. The deference experiment the paper conducts should really rely on the belief stickiness, not the credence itself. The issue of course is that we don’t know how to measure belief stickiness, because we don’t know how models update their probabilities to begin with (the subject of Sec 4)!"}, "questions": {"value": "- Feel free to respond to any of the suggested weaknesses.\n- Besides this, note that there is actually a decent amount of work now on Bayesian updating in LLMs, or at least how models respond to new evidence. See:\n\nTalmor et al, Leap-Of-Thought: Teaching Pre-Trained Models to\nSystematically Reason Over Implicit Knowledge, https://proceedings.neurips.cc/paper_files/paper/2020/file/e992111e4ab9985366e806733383bd8c-Paper.pdf\n\nWilie et al, Belief Revision: The Adaptability of Large Language Models Reasoning,  https://arxiv.org/pdf/2406.19764\n\nHase et al, Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?, https://arxiv.org/pdf/2406.19354\n\nQiu et al, Can Language Models Perform Implicit Bayesian Inference Over User Preference States?, https://openreview.net/pdf?id=arYXgfHAIh"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vUTLuHmTxB", "forum": "E2kTfV94Fc", "replyto": "E2kTfV94Fc", "signatures": ["ICLR.cc/2026/Conference/Submission21330/Reviewer_G83G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21330/Reviewer_G83G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839324257, "cdate": 1761839324257, "tmdate": 1762941700489, "mdate": 1762941700489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the ability of LLMs to coherently update their beliefs and examines whether the actions they take are consistent with those beliefs.\nThe authors find three main results:\n1. LLMs are largely inconsistent in how they update their beliefs.\n2. LLMs frequently take actions that contradict their stated beliefs.\n3. Action–belief discrepancy is not strongly correlated with task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an interesting and important question about LLM reasoning consistency and belief dynamics.\n\nThe experimental coverage is broad with a range of settings and analysis."}, "weaknesses": {"value": "The experimental setup is not sufficiently robust without further justification.\n- It is inherently more difficult for the model to estimate P(E | D = 1,X) (line 204) , since this inference task differs from the standard training objective of predicting the answer directly. Instead, it requires conditioning on the answer to infer which factors caused it. This discrepancy raises doubts about the accuracy of p_2^* and makes the comparison setup in Figure 3 less convincing.\n- Section 5 focuses only on the betting task as an example of belief–action alignment. However, this might not be a representative action type, since models could be biased or unfamiliar with betting scenarios. Testing with different actions could strengthen the argument. Or the authors should provide a stronger justification for why betting in a market setting is an appropriate and convincing test of belief–action consistency.\n- In Section 6, the authors only use the feedback message “Your answer to the initial question is incorrect” (line 323). More varied or informative feedback types could provide stronger evidence.\n\nThe description of the experimental setting lacks clarity. For example, in line 161, it is unclear how the second-round prompt is presented. Do you explicitly mention that additional information is being provided, or do you simply show X, E without any contextual hint?"}, "questions": {"value": "1. Would it be possible to test some of the setups on a larger open-source model (e.g., around 70B parameters) or a large reasoning model to examine whether the observed trends remain consistent? I understand that this would involve substantial additional effort, so it is reasonable if the authors do not have the capacity to perform such experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ukj4zRGyL8", "forum": "E2kTfV94Fc", "replyto": "E2kTfV94Fc", "signatures": ["ICLR.cc/2026/Conference/Submission21330/Reviewer_FvrH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21330/Reviewer_FvrH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878301636, "cdate": 1761878301636, "tmdate": 1762941700118, "mdate": 1762941700118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive empirical investigation into the behavioral consistency of Large Language Models (LLMs) in sequential and interactive scenarios. The authors probe two core aspects: 1) the coherence of belief updates upon receiving new evidence, measured against Bayesian norms, and 2) the alignment between an LLM's stated confidence and its subsequent actions (betting in a prediction market and defending answers under challenge). Using datasets like Pima Indians Diabetes and Metaculus, the study finds significant apparent inconsistencies. Models deviate substantially from Bayesian updating, and their posterior beliefs often degrade predictive accuracy compared to their priors. Crucially, models frequently place bets that are directionally opposite to their elicited confidences and exhibit only moderate \"deference-consistency\" (i.e., defending high-confidence answers more robustly). The paper also explores mitigation via prompting and activation steering. A central, unresolved question underpinning the entire work is the validity of the elicited confidences as a true proxy for the model's \"beliefs.\""}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Research Topic: Understanding LLM behavior beyond static benchmarks is crucial for deployment.\n\n2. Novel Experimental Paradigms: The betting market and deference consistency experiments are creative approaches to studying LLM behavior.\n\n3. Interesting observation: The finding of a negative correlation between static calibration (ECE) and consistent agentic behavior (betting) is the paper's most significant contribution."}, "weaknesses": {"value": "1. Related Work - Confidence slicitation and calibration: expecting few more works such as https://arxiv.org/abs/2508.15260 and https://arxiv.org/abs/2503.22353. The second one also focuses on the LLM consistency problem.\n\n2. The entire analytical framework is built on the premise that elicited confidences are a valid measure of an LLM's \"belief.\" The paper does not sufficiently validate this premise or engage deeply with the philosophical and practical challenges of defining and measuring belief in a stateless, auto-regressive model. The observed phenomena might be better described as \"inconsistencies between different output behaviors\" rather than between \"belief and action.\"\n\n3. Regarding the Bayesian exp, it's kinda circular reasoning to me. Again, elicited confidences are valid is the assumption. If inputs are unreliable, p2 is meaningless. You're comparing unreliable generations to each other, not testing coherence.\n\n4. The studied actions are limited to betting and textual deference. The generalizability of these findings to other action spaces (e.g., tool use, physical reasoning) remains an open question."}, "questions": {"value": "1. To what degree do the confidence estimates from your three elicitation methods (logits, sampling, verbal) correlate with each other for the same question/model? Low convergence would suggest we are not reliably measuring a stable underlying construct, which would critically challenge the interpretation of your results.\n\n2. Any reference or evidence supports the statement that the elicitation methods are reliable? It could be cited or proved. But I did not find it. A simple alternative story could be: Elicitation methods are unreliable; behavioral measures (betting) may be more accurate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jkt1Aw23sx", "forum": "E2kTfV94Fc", "replyto": "E2kTfV94Fc", "signatures": ["ICLR.cc/2026/Conference/Submission21330/Reviewer_WZDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21330/Reviewer_WZDe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955620875, "cdate": 1761955620875, "tmdate": 1762941699573, "mdate": 1762941699573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a quantitative analysis of LLM coherence, focusing on three key research questions: whether LLMs adhere to Bayesian consistency when updating beliefs, the extent of the action-belief gap, and their deference consistency in user interactions. The study analyzes how existing confidence estimation methods (logit, verbal, and sampling) correlate with model performance across these tasks, using multiple distinct datasets appropriate for each question. The authors ultimately conclude that LLMs are largely inconsistent with updating beliefs, inconsistent in action-belief behavior, and moderate self-inconsistent in how they respond to challenges. In addition, the paper finds that high task performance and good calibration are strong guides to rational belief updating but are weak predictors of whether model actions will match those beliefs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The most significant strength of the paper is the finding of a strong correlation between adherence to Bayes' Rule, good calibration, and high task performance. This key finding is crucial for future work, as it suggests that the potential for coherent, rational thought fundamentally exists within LLMs when properly measured.\n- The study addresses a highly timely and critical topic at the intersection of AI alignment and reliability. The goal of studying belief consistency and the action-belief gap is important for deploying safe LLMs.\n- The methodology has good breath and rigor in certain areas. It examines three distinct research questions, uses multiple model sizes, and compares three different methods of confidence extraction.\n- The paper effectively shows the benefits of targeted interventions (referencing concepts like activation steering and demonstrating prompt ablation) to improve deference-consistency behavior."}, "weaknesses": {"value": "- W1. The experimental design confounds the technical issue of LLM calibration with the paper’s theoretical claim of rational inconsistency, resulting in measuring artifacts from uncalibrated models and un-tuned prompts.\n- W2. The methodological detail and rigor is uneven across the different experiments which puts the reliability of the main conclusions into question. \n- W3. The analysis is undermined by measurement challenges, as confidence metrics yield contradictory results, and the theoretical test for consistency relies on a simplistic definition of rational behavior.\n- W4. The paper's ultimate conclusion is diminished because the final solution for improving consistency (the use of more precise prompts) reiterates established best practices."}, "questions": {"value": "- Did the authors test a baseline where the models were finetuned on the target domain to achieve basic calibration before consistency testing?\n- Why were prompt ablation studies omitted for the belief extraction (RQ1) when they were deemed necessary for RQ3? \n- Could the authors analyze why logit confidence yields results that contradict those from verbal/sampling confidence regarding belief-action consistency?\n- Have you considered testing rational metacognition by requiring the LLM to inject its estimated confidence into its context before acting?\n- Could the authors discuss the definition of consistency and potential limitations in its use as a standard for rational revision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iimc55tdN6", "forum": "E2kTfV94Fc", "replyto": "E2kTfV94Fc", "signatures": ["ICLR.cc/2026/Conference/Submission21330/Reviewer_Lk29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21330/Reviewer_Lk29"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052611427, "cdate": 1762052611427, "tmdate": 1762941698992, "mdate": 1762941698992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}