{"id": "rSLxqMONIk", "number": 11694, "cdate": 1758203121625, "mdate": 1759897560546, "content": {"title": "Adaboost-Based Local-Forest Adversarial Learning for Imbalanced Domain Adaptation", "abstract": "Class imbalance poses a significant challenge in unsupervised domain adaptation (UDA). We propose Adaboost-based Local-Forest Adversarial Learning (ALFAL), a framework that leverages sample-wise label matching rates to guide both adaptive sampling and interpolation-based generation. ALFADA first samples informative instances and constructs a local discriminative forest (LDF) via clustering to enable fine-grained regional alignment on the basis of the global discriminator in the framework of domain adversarial learning. To further enhance adaptation for minority classes, a Boosted Pairwise Interpolation Generator (BPIG) synthesizes interpolation samples between high-weight source and confident target instances. These auxiliary samples are optimized through an adversarial learning-based exploration mechanics to explore challenging regions. Experiments demonstrate that ALFADA consistently outperforms existing state-of-the-art methods on imbalanced domain adaptation benchmarks.", "tldr": "We propose a framework that integrates matching-driven adaptive sampling, local discriminator ensembles, and interpolation-based augmentation to address imbalanced domain adaptation.", "keywords": ["Imbalanced domain adaptation", "adaboost", "adversarial learning", "sample interpolation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e649bd56de3d6e7cb34dee0e653b811548ce68e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the unsupervised domain adaptation (UDA) problem, where labeled source-domain data and unlabeled target-domain data are available. The authors propose a new framework, named ALFADA, designed to handle class imbalance in UDA. The approach combines three main components:  \n(1) a weighted sampling scheme inspired by AdaBoost,  \n(2) a set of local discriminators constructed via clustering in a latent feature space, and  \n(3) a data synthesis module that interpolates between high-weighted source instances and confident target samples of the same predicted class, where the interpolation coefficients are optimized adversarially."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles a meaningful and practically important challenge — class imbalance in domain adaptation. The proposed framework attempts to integrate ideas from boosting, adversarial training, and data augmentation in an interesting way."}, "weaknesses": {"value": "1. **Poor writing and organization.**  The paper is difficult to follow due to unclear exposition and inconsistent terminology. For instance, the proposed method is first referred to as _ALFAL_ in the abstract but later as _ALFADA_, suggesting a lack of proofreading and editorial care. Figure 1 occupies significant space yet fails to illustrate the proposed framework effectively, nor is it adequately explained in the text. Repetitions (e.g., Lines 186 and 197) and abrupt term introductions further hinder readability.\n2. **Unclear motivation and intuition.**  The rationale behind the design of key components—e.g., Equations (1)–(3) and (14)—is not well justified. The paper presents a complex combination of techniques (weighted sampling, clustering, adversarial interpolation) but does not clearly explain why each component is necessary or how they interact to address class imbalance.\n3. **Inconsistent and undefined terminology.**  Many ad hoc terms appear without proper definition or explanation.\n\t- “Matching rate” (Line 173) is introduced before being defined, and later replaced by “matching error rate,” creating confusion.\n\t- “Domain discriminator” (Line 234), “local domain mismatch,” and “enhance feature continuity” (Line 285) are mentioned without prior explanation or formal definition.   \n\t- The meaning of “global distribution shift” versus “local feature region alignment” (Line 279) remains vague.  \n\tThese inconsistencies make it difficult to interpret the proposed approach.\n\t\n\t\n4. **Notation issues and unclear formulation.**  In Equation (14), the argmax operator appears to be applied over a random variable or distribution, which is mathematically unclear. The purpose and implications of this formulation should be clarified.\n\nThe paper lacks coherence and polish. The logical flow of sections (method, objective, and analysis) is weak. As a result, it is challenging to extract a precise understanding of the proposed contributions."}, "questions": {"value": "- Could you elaborate on the intuitions behind the design of Equations (1)–(3)?\n- What is meant by _“global distribution level shift”_? Why is it necessary to decompose domain alignment into local and global levels, and how does this help address class imbalance?\n- For Objective (14), what is the rationale for perturbing the interpolation coefficients rather than the instances themselves, as done in conventional adversarial training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EDBQ2gKYsh", "forum": "rSLxqMONIk", "replyto": "rSLxqMONIk", "signatures": ["ICLR.cc/2026/Conference/Submission11694/Reviewer_4rAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11694/Reviewer_4rAT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678905079, "cdate": 1761678905079, "tmdate": 1762922744443, "mdate": 1762922744443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called Adaboost-based Local-Forest Adversarial Domain Adaptation (ALFADA) for addressing the Class-Imbalanced Domain Adaptation (CDA) problem. To tackle issues such as class imbalance, pseudo-label noise, and fine-grained domain misalignment, the authors introduce three modules: adaptive matching error rate sampling, Local Discriminative Forest (LDF), and Boosted Pairwise Interpolation Generator (BPIG). Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly explains the rationale and purpose of each step in the method section, which facilitates the reviewer’s understanding and makes the overall article clear and easy to follow."}, "weaknesses": {"value": "1. The overall novelty of the paper is limited. In my view, the authors' main innovation lies in designing sample or category weights that correspond to the samples and loss function. However, other aspects, such as adversarial learning and sample interpolation, are well-established techniques.\n2. The abstract is poorly written. It does not clearly state the core problem or how the authors address it.\n3. All issues mentioned in the \"questions\" section."}, "questions": {"value": "1. After reading the entire paper, I noticed that pseudo-label supervised learning for target domain samples is not used. Compared to adversarial training, this is often a more effective learning method. Why didn’t the authors consider using it? The target domain samples above the threshold are considered confident, and these samples could be used for learning.\n2. Target domain samples with low confidence seem to be excluded from training entirely, which discards a significant amount of useful information. Proper utilization could bring improvements. I believe this is a design flaw.\n3. Regarding Section 3.2, Equation 1 seems redundant. Pseudo-labels are outputs from the previous epoch, and the samples are all high-confidence, meaning that the predicted labels will always match the pseudo-labels, resulting in an error rate of zero. Even if the error rate isn’t zero, Equation 2 could still yield a positive or negative value, depending on whether the error rate exceeds 0.5. Therefore, Equation 3 might actually reduce the weight. Moreover, as the error rate increases, αc becomes smaller, which decreases the weight wi for misclassified samples. This contradicts the authors’ explanation.\n4. The clustering section is puzzling. If the source and target domains are not aligned, how can they be accurately clustered into K categories? This would introduce a lot of noise, leading to failure in subsequent adversarial alignment.\n5. How is the validation set selected in Section 3.3.2?\n6. Where does wk come from in Equation 11?\n7. In Equation 12, λ is supposed to be a sample, but in Equation 14, it appears to be obtained through adversarial learning. How does this transition happen?\n8. In Equation 17, hat(xp) is written. It should correspond to Equation 12."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AolWr4d7mH", "forum": "rSLxqMONIk", "replyto": "rSLxqMONIk", "signatures": ["ICLR.cc/2026/Conference/Submission11694/Reviewer_gYgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11694/Reviewer_gYgh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698188324, "cdate": 1761698188324, "tmdate": 1762922743827, "mdate": 1762922743827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a critical issue in Unsupervised Domain Adaptation (UDA) known as Class-Imbalanced Domain Adaptation (CDA). The class imbalance exacerbates the domain distribution shift, causing adaptation models to become biased towards majority classes and perform poorly on minority ones. The authors propose a unified framework named ALFADA. The core idea is to leverage the property of the AdaBoost algorithm by implementing an adaptive mechanism that identifies and concentrates on hard-to-align feature regions and under-represented minority-class samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A key highlight is the integration of the AdaBoost principle (dynamic reweighting to focus on difficult instances) across the entire framework, from sample selection and local alignment to data generation.\n2. The integration of Local Discriminative Forest (LDF) is a good design. By partitioning the feature space via weighted clustering and assigning specialized local discriminators, it captures fine-grained discrepancies that a global discriminator would miss.\n3. Pairwise Interpolation Generator introduced linear combination of data generation and the adversarial search for the optimal interpolation coefficient $\\lambda$ enhances classifier robustness."}, "weaknesses": {"value": "1. The proposed ALFADA framework is considerably complex and introduces a large number of hyperparameters, e.g., the confidence threshold, number of clusters, clustering frequency, Beta distribution parameters and so on. This work lacks a detailed discussion on the selection criteria or a sensitivity analysis for these parameters, which could pose challenges for reproducibility. \n2. Combined with the above weakness, the complex framework with large amount of hyperparameters and the min-max bi-level optimization would lead to the substantially higher computational costs. The absence of an analysis on training efficiency makes it difficult to assess the method's practicality on large-scale datasets.\n3. Multiple stages of the framework depend on the quality of pseudo-labels generated for the target domain. Although a confidence threshold is used for filtering, performance may suffer from error accumulation in the early stages of training or in tasks with very large domain gaps. The ablation study on the quality of pseudo-labels should be discussed.\n4. The framework contains the inner-loop adversarial optimization required to find optimal $\\lambda^*$ for each sample generated by BPIG. Is this process guaranteed to converge, or improves the oscillation during training?\n5. Regardless of the method design, the reviewer would like to question the reproducibility of this work, since there is no discussion about the experimental settings and details. The authors are expected to discuss the details during the rebuttal time."}, "questions": {"value": "Please see above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iYVSSrPQmw", "forum": "rSLxqMONIk", "replyto": "rSLxqMONIk", "signatures": ["ICLR.cc/2026/Conference/Submission11694/Reviewer_HRSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11694/Reviewer_HRSN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968999374, "cdate": 1761968999374, "tmdate": 1762922743375, "mdate": 1762922743375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an AdaBoost-based Local-Forest Adversarial Learning framework to address the class imbalance problem in unsupervised domain adaptation, with a particular focus on hard-to-align samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Exploring the imbalance domain adaptation problem with a focus on hard-to-align samples is an interesting direction.\n2. The method is well illustrated."}, "weaknesses": {"value": "1. The paper does not provide a clear illustration or definition of what constitutes a hard-to-align sample\n.\n2. The novelty of the proposed method appears limited. The proposed local/global discriminator and boosted pairwise interpolation generator are very similar to prior works, and the authors fail to adequately justify the differences or contributions compared to existing methods.\n\n3. The performance improvement of the proposed method is marginal compared to baselines.\n\n4. No ablation study is conducted."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vHZNn18B8M", "forum": "rSLxqMONIk", "replyto": "rSLxqMONIk", "signatures": ["ICLR.cc/2026/Conference/Submission11694/Reviewer_uhpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11694/Reviewer_uhpw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141679032, "cdate": 1762141679032, "tmdate": 1762922742954, "mdate": 1762922742954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}