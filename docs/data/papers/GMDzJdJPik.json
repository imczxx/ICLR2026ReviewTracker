{"id": "GMDzJdJPik", "number": 9139, "cdate": 1758112616067, "mdate": 1758874573088, "content": {"title": "SearchAttack: Red Teaming Search-augmented LLMs via Injecting the Multi-hop Information-seeking Tasks", "abstract": "Search-augmented Large Language Models (LLMs), which integrate web search with generative reasoning, are highly attractive attack targets, as they can be weaponized to exploit real-time information for malicious purposes. However, existing studies remain limited in assessing their vulnerabilities to the malicious use of their knowledge search and application capabilities. This study proposes \\textbf{\\textit{SearchAttack}}, a method that uses multi-hop information-seeking queries and harmfulness rubrics to exploit LLMs' web search capability for malicious goal achieving. The core attack strategy is: 1) Embedding sensitive cues into multiple challenging information-seeking tasks, thereby triggering LLMs to launch the search process for solving harmful tasks; 2) Using a reverse-engineered rubric to guide LLMs in organizing searched knowledge into a valuable malicious report. We further build a harmful behavior dataset that reflects ongoing Chinese black and gray market activities in 2025 to evaluate search-augmented LLMs' attack value. Experiments have shown that SearchAttack achieves state-of-the-art attack success rate and generates more practically harmful outputs.", "tldr": "We introduce a knowledge-augmented LLM red teaming method and real-world criminal datasets to reveal the weaponization potential of search-augmented LLMs for emerging and domain-specific malicious tasks", "keywords": ["LLM safety", "Red Teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dabee112a1ecb28c639b1dbc9f27be1504234599.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}