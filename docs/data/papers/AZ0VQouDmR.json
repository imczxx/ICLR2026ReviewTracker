{"id": "AZ0VQouDmR", "number": 5742, "cdate": 1757931007196, "mdate": 1763650583549, "content": {"title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "abstract": "While recent text-to-image (T2I) models show impressive capabilities in synthesizing images from brief descriptions, their performance significantly degrades when confronted with long, detail-intensive prompts required in professional applications. We present DetailMaster, the first comprehensive benchmark specifically designed to evaluate T2I models' systematic abilities to handle extended textual inputs that contain complex compositional requirements. Our benchmark introduces four critical evaluation dimensions: Character Attributes, Structured Character Locations, Multi-Dimensional Scene Attributes, and Spatial/Interactive Relationships. The benchmark comprises long and detail-rich prompts averaging 284.89 tokens, with high quality validated by expert annotators. Evaluation on 7 general-purpose and 5 long-prompt-optimized T2I models reveals critical performance limitations: state-of-the-art models achieve merely ~50% accuracy in key dimensions like attribute binding and spatial reasoning, while all models showing progressive performance degradation as prompt length increases. Our analysis reveals fundamental limitations in compositional reasoning, demonstrating that current encoders flatten complex grammatical structures and that diffusion models suffer from attribute leakage under detail-intensive conditions. We open-source our dataset, data curation code, and evaluation tools to advance detail-rich T2I generation and  enable applications previously hindered by the lack of a dedicated benchmark.", "tldr": "We introduce the first comprehensive benchmark for evaluating T2I models on long prompts, showing that current encoders flatten complex grammatical structures and that diffusion models suffer from attribute leakage under detail-rich conditions.", "keywords": ["generative model", "long prompt", "detail-rich prompt", "compositional text-to-image generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2462840e98444626973d3bf72ad3ff5873e8b624.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DETAILMASTER, the first benchmark for evaluating T2I models on long detail-rich prompts, focusing on four dimensions: Character Attributes, Structured Character Locations, Multi-Dimensional Scene Attributes, and Spatial/Interactive Relationships.\n\nIt identifies T2I models’ flaws (compositional reasoning issues, attribute leakage) in long prompts, sets a standard for evaluation, and enables progress in professional applications like industrial prototyping hindered by poor detail fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It fills the gap with long-prompt T2I evaluation by proposing DETAILMASTER— they claim as the first benchmark (actually concurrent with [1] TIT-Score) with avg 284.89-token prompts (vs. existing <100-token ones) and 4 targeted dimensions (e.g., Structured Character Locations).\n\n2. It reveals critical T2I flaws (prompt-length accuracy degradation, backbone limits) and guides research (prioritize long-prompt training over context windows). \n\n[1] TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency (Arxiv Oct 3rd.)"}, "weaknesses": {"value": "**1. Incomplete Failure Mechanism Validation:** It attributes poor performance to \"encoder grammar flattening\" and \"diffusion attribute leakage\" but lacks causal tests. For instance, no ablation comparing T5 (grammar-aware) vs. CLIP (flat) encoders on prompt structure preservation, or attribute leakage tracking via attention visualization. Adding such experiments would confirm root causes and guide model improvements. Instead of proposing a benchmark, which is more like an effort job, readers may want to know more insights from the huge benchmark experiments. \n\nFor example: \n\n(1) what is the reason to make the T2I models behave diversely? The training data, training scheme or text encoder differences? \n\n(2) Any potential to solve the problem with your proposed benchmark? \n\n(3) How are the unified models (BAGEL, Blip3-o, Janus, Janus-pro, Janus-flow, etc.) perform on your benchmark? \n\n(4) Would better LLM/VLM as the encoder benefit the generation?\n\n**2. High Evaluation Computational Barrier:** The MLLM-based evaluation requires 20GB–39GB GPU VRAM and 10+ hours per run, excluding small teams. It could adopt lightweight MLLMs (e.g., Qwen2.5-VL-2B) fine-tuned on its annotation data, or distill the evaluator into a smaller model. This would reduce resource needs while preserving accuracy."}, "questions": {"value": "refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OBAgLl9b3t", "forum": "AZ0VQouDmR", "replyto": "AZ0VQouDmR", "signatures": ["ICLR.cc/2026/Conference/Submission5742/Reviewer_YsLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5742/Reviewer_YsLh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760856427344, "cdate": 1760856427344, "tmdate": 1762918233734, "mdate": 1762918233734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the comprehensive benchmark to address ex\u0002tended textual inputs that contain complex compositional requirements. The benchmark introduces four critical evaluation dimensions: Character Attributes, Structured Character Locations, Multi-Dimensional Scene Attributes, and Spatial/Interactive Relationships."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes the comprehensive compositional dataset on long, complex prompts.\n2. The paper is well-written and easy-to-follow.\n3. The experiments are extensive."}, "weaknesses": {"value": "1. The paper lacks discussion of ConceptMix, which targets at compositional T2I generation.\n2. The attribute pipeline relies on MLLM (e.g.,  use MLLM to identify its background composition, lighting conditions, and stylistic elements), which may introduce hallucinations or mistakes. And use MLLM as evaluators may still introduce problems though authors tried to mitigate. For example, the evaluation results are not easy to reproduce.\n\n[A] Wu X, Yu D, Huang Y, et al. Conceptmix: A compositional image generation benchmark with controllable difficulty[J]. Advances in Neural Information Processing Systems, 2024, 37: 86004-86047."}, "questions": {"value": "1. What are the ways to prevent hallucinations of MLLM in pipeline construction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Aq2RplSOlR", "forum": "AZ0VQouDmR", "replyto": "AZ0VQouDmR", "signatures": ["ICLR.cc/2026/Conference/Submission5742/Reviewer_PbGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5742/Reviewer_PbGa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671081656, "cdate": 1761671081656, "tmdate": 1762918233234, "mdate": 1762918233234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DETAILMASTER for evaluating the consistency between images and text generated by T2I models under complex long-text conditions. DETAILMASTER starts from open-source image-caption datasets with dense annotations, further expanding on Character Attributes, Structured Character Locations, Spatial/Interactive Relationships, and Multi-Dimensional Scene Attributes in the prompt through MLLM & LLM, synthesizing the final version of DETAILMASTER. The paper evaluates several general T2I models as well as T2I models specifically optimized for long text, demonstrating that even the most advanced T2I models still need further improvement in aligning images and text under complex long-text prompts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The consistency of text and images in complex long prompts is crucial for evaluating the capabilities of T2I models, and existing benchmarks are indeed lacking in this aspect;\n2. The benchmark synthesis process in the paper comprehensively considers various aspects under long text prompts, such as Character Attributes, Structured Character Locations, and Multi-Dimensional Scene Attributes;\n3. The paper conducts extensive experiments on existing open-source and closed-source models, indicating that the current state-of-the-art models still need further improvement in handling complex long texts."}, "weaknesses": {"value": "1. Recent diffusion models that use MLLM as a text encoder, such as Hunayuan Image 3.0 and Qwen-Image, possess stronger text understanding capabilities. How do these models perform on DetailMaster?\n2. During evaluation, DetailMaster needs to detect the bounding box for each character based on the Character List. How does it handle cases when the prompt contains multiple repeated characters and there are interactions between these repeated characters?\n3. Due to the inherent hallucinations of LLMs/MLLMs, there might be inconsistencies between the final prompt and the image content in DetailMaster's benchmark creation process. The paper does not develop a secondary verification process to ensure higher accuracy.\n4. In section 3.2.3, the paper admits that using a single MLLM family (i.e., QwenVL) for both data curation and evaluation could raise concerns about potential self-enhancement bias. Although evaluation results on previous t2i models using InternVL and QwenVL indicate consistent relative rankings, I am still curious whether the evaluation results of diffusion models like Qwen-Image, which use QwenVL as a text encoder, are consistent between QwenVL and InternVL evaluations in DetailMaster."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "N/A"}}, "id": "u8MYZzdZlj", "forum": "AZ0VQouDmR", "replyto": "AZ0VQouDmR", "signatures": ["ICLR.cc/2026/Conference/Submission5742/Reviewer_nKZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5742/Reviewer_nKZ7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726166603, "cdate": 1761726166603, "tmdate": 1762918232484, "mdate": 1762918232484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for assessing T2I models on their ability to faithfully handle long prompts. To craft the benchmark, the authors follow a data curation pipeline for the evaluation dataset. The paper also includes analyses and insights into the limitations of current diffusion models w.r.t handling long prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Data curation pipeline \n* Analysis of limitations of current benchmarks pertaining to long prompts\n* Robustness and validity of the benchmark through human evaluation"}, "weaknesses": {"value": "* Lack of controlled experiments to drive the insights and analyses in Section 4. It would have been much better to take a single model architecture and ablate it under different setups to drive the insights. More on this in \"Questions\".\n* Lack of results with several models that are known for handling long and complex prompts (such as SANA [1], Lumina-Next [2], and QwenImage [3]).\n* It's said in the paper multiple times (L39, for example) that T2I models are trained on short-length prompts. However, many recent models actually leverage denser prompts during their training phase (SANA, for example).\n\n**References**\n\n[1] SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers; Xie et al.; 2024.\n\n[2] Lumina-Image 2.0: A Unified and Efficient Image Generative Framework; Qin et al.; 2025.\n\n[3] Qwen-Image Technical Report; Wu et al.; 2025."}, "questions": {"value": "> Our analysis reveals fundamental limitations in compositional reasoning, demonstrating that current encoders flatten complex grammatical structures and that diffusion models suffer from attribute leakage under detail-intensive conditions.\n\nCould the authors reference the sections where this was analyzed? Section 4 didn't seem to address these points.\n\n> To achieve higher precision and greater detail, we subsequently reprocess the corresponding 4,565 samples using Qwen2.5-VL-72B-Instruct as both the LLM and the MLLM. This new process generates an improved dataset containing 4,116 prompts with an average token length of 284.89.\n\nHow is the number of prompts changing from 4,565 to 4,116? How were they validated? It would also be beneficial to the community if the authors could include all the system prompts that were used throughout this work.\n\n> Section 4 analyses\n\nIt would have been better and more helpful to fine-tune a specific architecture on image and long prompt pairs, and then study the effects. Models like QwenImage already leverage an LLM backbone for encoding the text prompts and were also trained with a longer sequence length. So, I believe this is feasible. The models mentioned in \"Long prompt training matters more than increasing token capacity.\" section all have varying amounts of confounding factors and hence it makes it difficult to drive educated observations.\n\n> Datasets\n\nIt's unclear what proportions of the samples from each dataset were used to construct the benchmark. It could also be beneficial to include some commentary about how the original captions aren't long enough and miss the important, desirable details (possible through some quantifiable numbers). For example, what character attributes are missing in the original captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fPPJSIZhXx", "forum": "AZ0VQouDmR", "replyto": "AZ0VQouDmR", "signatures": ["ICLR.cc/2026/Conference/Submission5742/Reviewer_5wZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5742/Reviewer_5wZt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838900801, "cdate": 1761838900801, "tmdate": 1762918232114, "mdate": 1762918232114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for assessing T2I models on their ability to faithfully handle long prompts. To craft the benchmark, the authors follow a data curation pipeline for the evaluation dataset. The paper also includes analyses and insights into the limitations of current diffusion models w.r.t handling long prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Data curation pipeline \n* Analysis of limitations of current benchmarks pertaining to long prompts\n* Robustness and validity of the benchmark through human evaluation"}, "weaknesses": {"value": "* Lack of controlled experiments to drive the insights and analyses in Section 4. It would have been much better to take a single model architecture and ablate it under different setups to drive the insights. More on this in \"Questions\".\n* Lack of results with several models that are known for handling long and complex prompts (such as SANA [1], Lumina-Next [2], and QwenImage [3]).\n* It's said in the paper multiple times (L39, for example) that T2I models are trained on short-length prompts. However, many recent models actually leverage denser prompts during their training phase (SANA, for example).\n\n**References**\n\n[1] SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers; Xie et al.; 2024.\n\n[2] Lumina-Image 2.0: A Unified and Efficient Image Generative Framework; Qin et al.; 2025.\n\n[3] Qwen-Image Technical Report; Wu et al.; 2025."}, "questions": {"value": "> Our analysis reveals fundamental limitations in compositional reasoning, demonstrating that current encoders flatten complex grammatical structures and that diffusion models suffer from attribute leakage under detail-intensive conditions.\n\nCould the authors reference the sections where this was analyzed? Section 4 didn't seem to address these points.\n\n> To achieve higher precision and greater detail, we subsequently reprocess the corresponding 4,565 samples using Qwen2.5-VL-72B-Instruct as both the LLM and the MLLM. This new process generates an improved dataset containing 4,116 prompts with an average token length of 284.89.\n\nHow is the number of prompts changing from 4,565 to 4,116? How were they validated? It would also be beneficial to the community if the authors could include all the system prompts that were used throughout this work.\n\n> Section 4 analyses\n\nIt would have been better and more helpful to fine-tune a specific architecture on image and long prompt pairs, and then study the effects. Models like QwenImage already leverage an LLM backbone for encoding the text prompts and were also trained with a longer sequence length. So, I believe this is feasible. The models mentioned in \"Long prompt training matters more than increasing token capacity.\" section all have varying amounts of confounding factors and hence it makes it difficult to drive educated observations.\n\n> Datasets\n\nIt's unclear what proportions of the samples from each dataset were used to construct the benchmark. It could also be beneficial to include some commentary about how the original captions aren't long enough and miss the important, desirable details (possible through some quantifiable numbers). For example, what character attributes are missing in the original captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fPPJSIZhXx", "forum": "AZ0VQouDmR", "replyto": "AZ0VQouDmR", "signatures": ["ICLR.cc/2026/Conference/Submission5742/Reviewer_5wZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5742/Reviewer_5wZt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838900801, "cdate": 1761838900801, "tmdate": 1763693838684, "mdate": 1763693838684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}