{"id": "kn7MOLXP25", "number": 18683, "cdate": 1758290053003, "mdate": 1759897087796, "content": {"title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting", "abstract": "As the role of Large Language Models (LLM)-based coding assistants in software development becomes more critical, so does the role of the bugs they generate in the overall cybersecurity landscape. \nWhile a number of LLM code security benchmarks have been proposed alongside approaches to improve the security of generated code, it remains unclear to what extent they have impacted widely used coding LLMs. \nHere, we show that even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios in a realistic use setting, suggesting that the safety-functionality trade-off has until now prevented effective patching of vulnerabilities. \nTo help address this issue, we introduce a new severity metric that reflects the risk posed by an LLM-generated vulnerability, accounting for vulnerability severity, generation chance, and the formulation of the prompt that induces vulnerable code generation - Prompt Exposure (PE). \nTo encourage the mitigation of the most serious and prevalent vulnerabilities, we use PE to define the Model Exposure (ME) score, which indicates the severity and prevalence of vulnerabilities a model generates.", "tldr": "Even recent Coding LLMs fail on established vulnerability benchmarks due to the utility–safety tradeoff. We propose methods to prioritize vulnerabilities and introduce new measures to compare models.", "keywords": ["LLM; Cybersecurity; Code Generation; Coding LLMs;"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ce1eb187cd399a30ac0d02783c374bb0ca4fecd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper discusses the security of code generated by LLMs, noting that open-weight models still produce vulnerable code in scenarios that have been documented for years. To improve the evaluation and mitigation of these risks, the paper introduces two new metrics: Prompt Exposure (PE) and Model Exposure (ME). The PE score assesses the severity of a vulnerability generated in response to a specific prompt by accounting for the vulnerability's inherent severity. The ME score then provides an overall summary of a model's security based on an aggregation of its PE scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper demonstrates that even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios in a realistic use setting. Even the best models generate between 10% and 40% of code snippets that are vulnerable in well-documented and widely known scenarios\n- The authors create an automated benchmark for security evaluation by extending the AATK dataset"}, "weaknesses": {"value": "- While the paper states they are the first to propose a systematic approach for analyzing LLM-generated code vulnerabilities, the core empirical foundation of the evaluation relies on existing work (Asleep at the Keyboard Paper)\n- The methodology relies entirely on CodeQL to detect vulnerabilities in the generated code. If a prompt reformulation is not precise enough, the vulnerability patterns expected by CodeQL may not be present in the generated code.\n- The definitions of Prompt Exposure (PE) and Model Exposure (ME) rely on specific assumptions about the Common Vulnerability Scoring System (CVSS) and is limited to this system"}, "questions": {"value": "Did you also check for false positives of the static analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BTTJ4xCmOH", "forum": "kn7MOLXP25", "replyto": "kn7MOLXP25", "signatures": ["ICLR.cc/2026/Conference/Submission18683/Reviewer_Lv5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18683/Reviewer_Lv5j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829728323, "cdate": 1761829728323, "tmdate": 1762928385433, "mdate": 1762928385433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two new metrics, namely Prompt Exposure (PE) and Model Exposure (ME), for quantifying the severity of vulnerabilities generated by LLMs in response to a prompt and grouped over multiple prompts for a model respectively. To do this, the Asleep At The Keyboard (AATK) benchmark is filtered for Python snippets with vulnerabilities that can automatically be detected by CodeQL -- resulting in a benchmark with 17 snippets. The Prompt Exposure (PE) for a prompt prone to a CWE (vulnerability type) is computed as follows: first the severity score for the CWE is approximated using the CVSS-B scores for associated CVEs. Further, the probability of generating vulnerable completions with the prompt is computed by sampling completions and evaluating them with CodeQL, and the likelihood of the prompt being used is also computed. The PE is then computed using these three values. The Model Exposure (ME) is computed by aggregating the PE scores for various prompts for a model. \nOn the AATK benchmark, the paper finds that the ME scores are more useful in ordering LLMs by the severity of vulnerabilities and this ordering differs from that induced by only looking at percentage of vulnerable completions by the model. Other findings include the sensitivity of certain prompts to reformulations and certain models being more prone to vulnerable code completion with prompt changes."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of severity scores for LLM generated code is novel and timely. Moreover, the consideration of differences in prompts and vulnerability types makes the metrics more fine-grained and compelling to use. \n\n- The evaluation of newer LLMs on the not-so-new AATK benchmark is also very useful in pointing out that secure code completion should still be an active area of research. Further, the proposed ME scores induce an ordering over LLMs taking the severity of completions into account, which should be useful in practice.\n\n- The paper was written well and the key ideas, specifically about how the metrics are computed, were easy to read.\n\nOverall, I think this is an important problem to work on and the proposed metrics considering prompt and CWE variance is novel and seems useful."}, "weaknesses": {"value": "- The title mentions \"actionable\" reporting. Which the ME scores can be useful in choosing between LLMs, I find it difficult to understand how these scores guide actions / future research for specific models. The prompt-level granularity offered by PE scores might be too specific to take any meaningful actions for updating these models. A CWE-level metric, not explored in this work, would be more \"actionable\" in the sense that it could identify specific vulnerabilities that any model is prone to generate.\n- The value of the proposed metrics is tied to a benchmark that can support them. The benchmark here is arguably quite small (17 Python snippets) to generalize the findings to more samples from other languages, vulnerability types.\n- Since one of the key ideas for defining the Prompt Exposure metric as such relies on including the probability of vulnerable generations and likelihood of prompts, I would have expected a discussion / experiment on how ablating these components affects the score. The components would be deemed more useful only if the ordering induced by them differs significantly from that just induced by raw CVSS scores.\n- While the paper notes this limitation, I would have expected a more detailed justification for why it is okay to approximate the score for a CWE using CVEs tagged with it (since a given CVE might be tagged with multiple CWEs). The severity of CWEs is also expected to be language specific (i.e., some languages are more prone to certain vulnerabilities over others). At the very least, I would expect the sensitivity of the score to the CVEs used in the computation to be discussed (i.e., if I changed the cut-off date or randomly sampled some CVEs corresponding to a CWE, how much does the score change?).\n\nOverall, I think that the paper could benefit from additional discussion and / or experiments justifying the various components of the metric and how it can be extended beyond the 17 Python snippets considered here. Further, a discussion on how this can benefit future work would be very helpful."}, "questions": {"value": "I will summarize my questions from the weaknesses section above (please refer to that section for more details):\n\n1. How do the proposed metrics promote action / guide future work?\n2. How sensitive is the PE score to the choice of the set of CVEs used (either by cut-off date or random selection)?\n3. How does the ordering change if various components of the score are ablated?\n4. How well do you think these results would generalize to benchmarks from other languages and other vulnerability types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4RCVNul67Y", "forum": "kn7MOLXP25", "replyto": "kn7MOLXP25", "signatures": ["ICLR.cc/2026/Conference/Submission18683/Reviewer_TwVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18683/Reviewer_TwVN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844595136, "cdate": 1761844595136, "tmdate": 1762928384761, "mdate": 1762928384761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper deals with vulnerabilities in LLM-generated code.\nIt takes inspiration from the \"Asleep at the Keyboard\" (AATK) paper from 2021, and builds on its dataset. The original AATK dataset contains 54 code completion scenarios that could generate a vulnerability in a specific CWE category. The new AATK dataset proposed is smaller with just 17 scenarios (only in Python). As for AATK, CodeQL is used to assess the security of the completed code.\nThe paper then proposes a derivitative of the FIRST Common Vulnerability Scoring System SIG (CVSS) metric to deal with snippets.\nThe paper concludes that coding LLMs still cannot be trusted to write secure code, and that small prompt variations can strongly affect security."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Understanding the security of LLM-generated code is an important topic"}, "weaknesses": {"value": "- The novelty (of dataset and scoring method) is very limited\n- The resulting dataset is smaller than the original, raising the question about the contribution.\n- The paper writing is verbose and chaotic -- it is very hard to pinpoint the precise contributions and arguments. Some text, like section 3.1, contains no relevant information. Also section 4.1 for some reason discusses LLM performance on HumanEval, which seems unrelated.\n- The findings are inconclusive and unsurprising. The conclusion that \"coding LLMs still cannot be trusted to write secure code\" appears to suggest that at some point they will, which unfortunately will never be the case."}, "questions": {"value": "I have no further questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1oUJ1EKBnE", "forum": "kn7MOLXP25", "replyto": "kn7MOLXP25", "signatures": ["ICLR.cc/2026/Conference/Submission18683/Reviewer_PToF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18683/Reviewer_PToF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990154213, "cdate": 1761990154213, "tmdate": 1762928383297, "mdate": 1762928383297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new evaluation framework for measuring how frequently LLMs generate insecure code and how severe the vulnerabilities are. The main contribution is the definition of Prompt Exposure (PE), which is a risk score that combines the severity of the vulnerability (via a representative CVSS score for each CWE class), the probability of generating vulnerable code, and how likely the prompt is to appear in real-world developer usage. This is aggregated across prompts to compute a Model Exposure (ME) score, enabling ranking of LLMs by security risk.\nThe authors test open-source models (comprising CodeLlama, Qwen, DeepSeekCoder, StarCoder, & Qwen variants) using 17 prompts adapted from Asleep at the Keyboard, with 10 paraphrases per prompt (generated by and then manually selected from GPT 3.5) and 25 model generations per paraphrase. They also evaluate human-level code correctness capability via HumanEval, Multilingual HumanEval, and HumanEval-Instruct. They show that modern models still frequently generate vulnerable code for many CWE categories, even when prompted in slightly different ways, and that any safety-tuning of more recent models has not eliminated these behaviours."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The novel risk-based metrics PE and ME, allowing models to be ranked by their security exposure. \nFigure 2 is one of the strongest components of the paper. It clearly visualises the variance in vulnerability generation across CWE categories, models, and paraphrased prompts, demonstrating that some models consistently produce insecure code across minor prompt perturbations. However, this analysis is limited by the extremely small dataset of 17 prompts, all inherited from Asleep at the Keyboard."}, "weaknesses": {"value": "* the work does not adequately position itself within the growing body of work on evaluating LLM-generated code security. Several benchmarks now exist that specifically assess vulnerability generation and, in many cases, also evaluate functional correctness, thus addressing exactly the functionality–security trade-off the paper discusses. These include Sec-Code-Bench, SecRepoBench, SafeGenBench, and benchmarks that jointly test correctness and security such as CWEval, SecCodePLT, BaxBench, and CodeGuard+. The absence of comparison or discussion of these efforts makes the contribution appear less novel and less grounded in current literature.\n* the evaluation is limited to Python, which restricts the generality of the proposed PE/ME framework.\n* the evaluation is limited only to open-source models. which is not customary for security papers.\n* the evaluation is based on only 17 Python-based vulnerability-inducing prompts from Asleep at the Keyboard - and even with 10 paraphrases each, this is a very limited test set for drawing generalisable conclusions or ranking models by security exposure. This is particularly problematic given the existence of the much larger benchmarks which contain hundreds to thousands of vulnerable examples across diverse CWE categories.\n* Although a “Valid” metric is reported for the AATK prompts, this only measures whether the generated code is syntactically valid (passes py_compile), not whether it produces correct output, or performs the intended task."}, "questions": {"value": "•  Why do you only use 17 vulnerability prompts? Do you believe this is sufficient to rank models? \n•  Why are existing security evaluation frameworks not used as baselines?\n•  Do you check whether vulnerable code is functionally correct beyond Valid%? For example, does the code run and produce correct outputs?\n•  Would your metric detect malicious prompt-induced vulnerabilities (jailbreaks), not just accidental ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2K6d8Dw2Fu", "forum": "kn7MOLXP25", "replyto": "kn7MOLXP25", "signatures": ["ICLR.cc/2026/Conference/Submission18683/Reviewer_hkbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18683/Reviewer_hkbs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053810432, "cdate": 1762053810432, "tmdate": 1762928382377, "mdate": 1762928382377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}