{"id": "asEBSfChXc", "number": 13228, "cdate": 1758215355709, "mdate": 1759897454186, "content": {"title": "Reasoning Supervision for Vision Transformers in Human Activity Recognition", "abstract": "Recent reasoning methods have been explored to improve model transparency and trust, particularly in video understanding, where actions are defined by temporal order, object interactions, and state transitions. However, most approaches remain post-hoc, offering limited opportunity to influence a model’s internal reasoning process or improve its accuracy. In this work, we move beyond post-hoc explanation and introduce a Reasoning Supervision training pipeline that directly enhances model performance. This setting presents unique challenges: how to generate training-time reasoning guidance, what form this guidance should take, and how to inject it effectively into the model. Our framework addresses these challenges by leveraging large language models (LLMs) as proxy annotators to generate high-quality spatial supervision. We introduce two complementary loss functions to inject this guidance into the model: a spatial alignment loss that aligns attention with LLM-derived spatial reasoning guidance and a temporal reasoning loss that encourages coherent, human-like temporal dependencies across frames. Applied to Vision Transformer architectures, Reasoning Supervision consistently improves performance, establishing a simple yet effective paradigm for advancing ViT-based video understanding models.", "tldr": "", "keywords": ["ViT", "Reasoning", "XAI", "LLMs", "HAR", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0760e50ae544e6a72256b8bdcceb74250352d306.pdf", "supplementary_material": "/attachment/8504c2319da0ad2da44a83111c5b231426b93ee3.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a training framework named Reasoning Supervision (RS) to enhance the performance and interpretability of Vision Transformers (ViTs) in video action recognition tasks. The authors contend that most current \"reasoning\" methods rely on post-hoc interpretation. In contrast, this work aims to directly introduce \"reasoning supervision signals\" during the training phase to guide the model's internal attention distribution and temporal consistency.\n\nExperiments conducted on several video datasets, including SSv2, UCF50, UCF101, and Desktop Assembly, demonstrate that RS significantly improves model performance (particularly on smaller datasets) while also enhancing its robustness and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed framework is model-agnostic and can be adapted to any Vision Transformer (ViT) architecture. It demonstrates effectiveness, notably in few-shot learning tasks on datasets such as UCF50 and Desktop Assembly, where it achieves substantial performance gains (up to +32% in Top-1 accuracy)."}, "weaknesses": {"value": "I find that the novelty of this paper is somewhat limited. Furthermore, I have several concerns regarding its methodological design and experimental setup:\n\n- **Use of LLM for Guidance?** The paper mentions employing GPT-4.1-mini as an annotator to provide reasoning guidance. In my view, it would be more accurate to describe this as using a Visual Language Model (VLM) rather than a pure Large Language Model (LLM), as VLMs possess visual perception and reasoning capabilities. This implies that the reasoning supervision signals are distilled from a visual-language model, not a pure text-based one. Given that VLMs already exhibit visual reasoning ability and strong text alignment, why not directly use an open-source VLM as the starting point for training? VLMs have become a common paradigm in video tasks. Is there a specific advantage to the current approach, compared to using a VLM alone to generate textual guidance?\n\n- **Reliability of VLM-Generated Signals:** While the authors acknowledge the presence of noise and bias in the LLM-generated outputs, there is no quantitative evaluation (such as a correlation analysis between signal quality and model performance) to assess the impact of these issues.\n\n- **Lack of Experiments on Larger-Scale Datasets:** The experiments are conducted only on relatively small datasets. It remains unclear whether the proposed method generalizes well to larger and more representative benchmarks such as Kinetics-400, 600, or 700, all of which provide standard training and test splits. Evidence on such datasets is necessary to demonstrate the universality and scalability of the approach.\n\n- **Insufficient Comparison with Other Reasoning-Based Methods:** The experimental comparisons focus mainly on traditional models, with no direct ablation or comparison against other reasoning-oriented or attention-alignment methods (e.g., those based on CLIP or other VLMs). This limits the claim of superiority in reasoning-aware modeling.\n\n- **Lack of Visual Validation:** The claim of improved interpretability would be better supported if more visual examples were provided—for instance, by comparing attention maps before and after applying the proposed reasoning supervision."}, "questions": {"value": "See Weakness.\n\nIn addition, the meaning of the variable *z* in line 274 is unclear. It should be clarified whether *z* represents the [CLS] token or what?. I could not find its definition in the main text.\n\nThe rationale behind the specific ratios assigned to the components ({im}→20%, {ctx}→60%, {nar}→100%) lacks sufficient justification. Is these values base on intuition? A comprehensive ablation study may necessary to empirically validate this configuration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OoE5koEmD6", "forum": "asEBSfChXc", "replyto": "asEBSfChXc", "signatures": ["ICLR.cc/2026/Conference/Submission13228/Reviewer_neTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13228/Reviewer_neTy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760800672445, "cdate": 1760800672445, "tmdate": 1762923917718, "mdate": 1762923917718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a brand new pipeline, the Reasoning Supervision framework, which moves beyond post-hoc explanation and is directly injected into the training stage. Starting with a comparison between conventional reasoning methods and the proposed approach, authors perform three significant challenges that their methodology stands for. Applied to ViT architectures, Reasoning Supervision can improve performance, establishing a simple yet effective paradigm for advancing ViT-based video understanding models. It introduces spatial alignment and three-level temporal consistency losses that regularize attention to be semantically meaningful and temporally coherent, turning reasoning into a first-class training signal rather than a byproduct."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Two-Stage Training Pipeline. TSViT is designed as a two-stage framework that first performs spatial learning at the frame level and then performs temporal learning at the clip level.\nUnify among Different Temporal Scales. The TSViT is encouraged to produce temporally aligned representations among the hierarchical structure.\nLeverage the Spatial Information. Detecting from a single frame, the attention map should be a guidance that reveals the most relevant area.\nDetailed formulas. Authors provide sufficient evidence of the proposed framework and complementary loss."}, "weaknesses": {"value": "Confused about \"LLM-Driven Knowledge Generation\": Strictly speaking, ChatGPT should not be just considered as LLMs, especially GPT-4o.  So the expression may lead to confusion. \nNo association information: And in subsection 2.2, \"Knowledge Selection,\" paragraph, I think there is no relation between DINO and the proposed data generation way. \nIncorrect cause and effect. In 2.3 Reasoning Guidance Injection, the LLM-derived guidance is only relevant to the spatial information, according to the formal paragraph. So the reason for two complementary losses should be reconsidered.\nAblation Study. Given the performance of the proposed framework among datasets, there should be ablation studies to demonstrate the importance of each component in the pipeline."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ksB5Oe5VQ", "forum": "asEBSfChXc", "replyto": "asEBSfChXc", "signatures": ["ICLR.cc/2026/Conference/Submission13228/Reviewer_tZ4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13228/Reviewer_tZ4f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761279093153, "cdate": 1761279093153, "tmdate": 1762923917072, "mdate": 1762923917072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Reasoning Supervision, a training paradigm for vision transformers in human activity recognition that injects spatial and temporal reasoning signals during training. It leverages large language models to generate spatial guidance and enforces temporal consistency through complementary loss functions. Evaluation is performed on SSv2, UCF50 and UCF101 datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method tackles the important problem of integrating spatial and temporal cues for video classification.\n* The proposed method shows improved performance in limited settings: e.g., on the UCF50 and UCF101 datasets with limited training data.\n* The paper explains the proposed method in detail."}, "weaknesses": {"value": "* The method does not seem to be applicable to state of the art methods such as Video-MAE, which outperforms the proposed method significantly. Video-MAE achieves 75.4% Top-1 accuracy on SSv2 versus only 59.77% Top-1 accuracy on SSv2 of the proposed method.\n\n* The proposed method does not lead to significantly improved performance when the base models are already pre-trained K400. On both SSv2 and UCF101, the performance improvement on Top-1 accuracy is <= 1%. So training on a larger dataset negates the need of the proposed approach.\n\n* The method uses a temporal consistency loss, with three levels of temporal granularity over immediate cues, derived from the first 20% of frames and contextual cues, obtained from the middle 60% of frames. These numbers 20% and 60% seem to be chosen arbitrarily and should be ablated.\n\n* The paper should include additional datasets for evaluation such as \"Ava: A video dataset of spatio-temporally localized atomic visual actions. CVPR, 2018.\".  Following prior work such as Video-MAE.\n\n* The paper should discuss prior work on grounding to fine-grained visual information in videos such as: \"Look, Remember and Reason: Grounded reasoning in videos with language models, ICLR 2024\"; \"Fine-grained Spatiotemporal Grounding on Egocentric Videos, ICCV 2025\".\n\n* Typos: \"LLM-riven\" in Figure 2."}, "questions": {"value": "* Does the proposed approach extend to state of the art models such as Video-MAE?\n* The rationale behind selecting the 20& and 20% thresholds should be discussed in more detail.\n* The paper should explain the choice of evaluation datasets in more detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yQFKWp8v8D", "forum": "asEBSfChXc", "replyto": "asEBSfChXc", "signatures": ["ICLR.cc/2026/Conference/Submission13228/Reviewer_o1Jc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13228/Reviewer_o1Jc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009605295, "cdate": 1762009605295, "tmdate": 1762923916812, "mdate": 1762923916812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Reasoning Supervision framework that integrates spatial and temporal reasoning signals into Vision Transformers for human activity recognition. Unlike post-hoc explanation methods, it trains models to reason during learning using LLM-generated spatial guidance to highlight task-relevant regions without human labels. A custom two-stage ViT architecture (TSViT) handles per-frame spatial tokenization and temporal modeling. Two loss functions guide training: a spatial alignment loss aligning attention maps with LLM-identified regions, and a temporal consistency loss enforcing prediction coherence across time scales. This approach helps the model focus on semantically meaningful elements and reason more consistently over time. The contributions include LLM-driven supervision, multi-scale temporal alignment, and improved performance and interpretability across benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Significance - This work addresses an important challenge in video understanding – how to make models both more accurate and interpretable by injecting domain knowledge.\n- Reasoning Guidance Injection - The concept of multi-scale temporal reasoning (immediate, contextual, narrative levels) is interesting in video action recognition, providing a hierarchical way to enforce prediction consistency.\n- Undertrained Setting - It can be gleaned from Table 1 results that the method is more effective in undertrained settings, where reasoning supervision offers strong inductive bias and boosts performance."}, "weaknesses": {"value": "- Scalability to Large Datasets - While the method seems to perform well on small and mid-scale datasets, its impact on the large and diverse Something-Something v2 dataset was minimal. This is likely due to sparse LLM-generated spatial cues, suggesting the approach may struggle in complex, high-variation settings without denser or more targeted supervision. \n- LLM Dependency - The method depends heavily on LLM-generated supervision, raising concerns about guidance quality and computational cost. Inaccurate or biased LLM outputs could mislead the model, but the paper doesn't assess how often this occurs. Additionally, generating supervision for all frames is costly, forcing the authors to subsample frames, which may limit effectiveness.\n- Ablation and Analysis - The paper lacks clear ablation to isolate the contributions of spatial and temporal losses. While qualitative insights are offered, quantitative comparisons (e.g., using only one loss at a time) are missing. Design choices like the 20%/60%/100% temporal split and loss weightings (λ_s, λ_t) are also not justified or tested for sensitivity, leaving questions about robustness and optimality.\n- TSViT Impact - The benefits of TSViT are unclear. Without reasoning supervision, it performs similarly or slightly worse than standard baselines, and with supervision, other models still outperform it in Table 1. Strong results on Desktop Assembly are noted, but lack of comparison limits interpretation.\n- Interpretability - Although not a major weakness, the paper emphasizes quantitative gains and alignment loss as a proxy for interpretability but lacks qualitative analysis. Visualizations comparing attention maps with and without reasoning supervision or user studies could better support authors' assertion of method leading to \"semantically meaningful\" representation. Without such examples or failure case analysis, the interpretability and robustness remain somewhat speculative."}, "questions": {"value": "- Scalability to SSv2 -  Could denser or more targeted LLM guidance improve performance on large, diverse datasets like SSv2? Have you tested this beyond the current 10% annotation rate?\n- LLM Guidance Quality - Did you assess how often LLM-generated masks are noisy or misleading? How robust is the model to imperfect guidance?\n- Ablation Clarity - Can you provide quantitative results isolating the impact of spatial vs. temporal losses (and its three levels)? How do performance and robustness change when each is used independently?\n- Design Sensitivity - Were the 20%/60%/100% temporal splits and loss weights (λ_s, λ_t) chosen empirically or tuned? Have you evaluated sensitivity to these choices?\n- TSViT Justification - What unique benefits does TSViT offer compared to standard ViT backbones when used with reasoning supervision? Could you share results from Desktop Assembly using other architectures for comparison?\n- Interpretability Evidence - Can you provide qualitative examples (e.g., attention maps or visualizations) showing that reasoning supervision improves semantic focus or prediction faithfulness?\n- Failure Cases - Have you analyzed scenarios where reasoning supervision fails or misguides the model? Such insights could clarify the method’s limitations and robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1znW45KWgI", "forum": "asEBSfChXc", "replyto": "asEBSfChXc", "signatures": ["ICLR.cc/2026/Conference/Submission13228/Reviewer_Hnqe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13228/Reviewer_Hnqe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762330996928, "cdate": 1762330996928, "tmdate": 1762923916530, "mdate": 1762923916530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}