{"id": "eqG8h5Ztq0", "number": 13791, "cdate": 1758222693173, "mdate": 1759897412216, "content": {"title": "MedQuanBench: Quantization-Aware Analysis for Efficient Medical Imaging Models", "abstract": "Quantization is a crucial technology for facilitating the deployment of medical AI models, especially on 3D radiological data. However, existing studies often lack comprehensive evaluations across diverse architectures, modalities, and quantization techniques, which limits our understanding of the real-world trade-offs among applicability, efficiency, and performance. In this work, we introduce MedQuanBench, a large-scale and diverse benchmark designed to rigorously evaluate quantization techniques for 3D medical imaging models. Our benchmark spans a wide range of modern architectures (e.g., CNNs and Transformers). We systematically evaluate representative post-training quantization strategies across model scales and dataset sizes. Additionally, we perform detailed sensitivity analyses to identify which model components are most vulnerable to quantization, including layer-wise degradation and activation distribution shifts. Our results show that 8-bit quantization consistently preserves segmentation accuracy across diverse architectures, making it a reliable choice for deployment. Furthermore, with appropriate configuration, such as selecting proper quantization granularity based on the model structure, 4-bit precision can also achieve near-lossless performance. These results show MedQuanBench as a fundamental benchmark for optimizing quantization strategies and guiding the development of deployment-ready, low-bit medical imaging models.", "tldr": "", "keywords": ["Quantization", "medical imaging", "3D", "benchmark", "efficiency", "sensitivity"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73eeeea8f767346db99da6e1e71e571bc5f3998f.pdf", "supplementary_material": "/attachment/8efc1a8a74d3a841611f57aad4a8e2114f72a734.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MedQuanBench, a large-scale benchmarking platform designed to systematically evaluate the effectiveness of quantization techniques on 3D medical imaging models. The authors systematically evaluate representative post-training quantization (PTQ) strategies across a variety of modern model architectures, including CNNs and Transformers, across various model scales and dataset sizes. Furthermore, the paper conducts a detailed sensitivity analysis to identify the model components most vulnerable to quantization, exploring issues such as layer-level performance degradation and shifts in activation distributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark is designed to be comprehensive. \n\nIt covers:\n* Diverse architectures: From classic CNNs (nnU-Net) to hybrid architectures (MedFormer, SwinUNETR, UNETR) and pure CNN variants (STU-Net).\n* Diverse modalities and tasks: Including CT and MRI, as well as various organ, tumor, and brain segmentation tasks.\n* Diverse scales: Models ranging from 10M to 2B parameters, and datasets ranging from hundreds to tens of thousands of samples, are tested.\n\n**Hardware-Aware Perspective:** This paper offers a significant strength. The analysis in Section *Quantized Operation on Real Hardware* is excellent. The authors not only provide quantization formulas but also explain in detail how different quantization granularities map to hardware primitives on modern GPUs (using NVIDIA Blackwell as an example)."}, "weaknesses": {"value": "**1. Quantization Methodology Incompleteness**\n\nSymmetric vs. Asymmetric Quantization: The paper exclusively uses symmetric quantization (Eq. 1), ignoring asymmetric schemes critical for non-negative activations (e.g., ReLU outputs). Asymmetric quantization ($X_q = round(X/S) + Z$) often outperforms symmetric methods in low-bit regimes (e.g., INT4) by better fitting skewed distributions. This omission weakens the benchmark’s applicability to real-world medical models.\n\nCalibration Methods: Reliance on naive min-max scaling (Eq. 1) is suboptimal. Advanced calibration techniques, such as KL divergence and percentile-based scaling, mitigate outlier sensitivity and improve INT4 robustness. The authors must justify why these were excluded or add experiments comparing calibration strategies.\n\n---\n\n**2. Underdeveloped \"Advanced PTQ\" Evaluation**\n\nHyperparameter Sensitivity: Advanced methods (smoothing, SVD, rotation) in Table 4 show marginal gains, but their hyperparameters (e.g., smoothing factor α, SVD rank) lack optimization studies. For instance, α=0.5 (Appendix D) may be arbitrary; a sensitivity analysis of α ∈ [0.1, 0.9] is needed to validate \"limited effectiveness.\"\nScope of Application: Applying these methods only to the \"most sensitive layer\" (Sec 4.3) overlooks their intended global/block-wise use. Testing them holistically (e.g., activation smoothing across all layers) would provide stronger evidence for their (in)effectiveness.\n\n---\n\n**3. Hardware Evaluation Gaps**\n\nINT4 Acceleration Omission: While INT8 hardware results (Table 5) are thorough, INT4 lacks real-device profiling. Claims about Blackwell’s 4-bit support remain theoretical. Without latency/memory metrics for INT4, the \"near-lossless\" performance claim (Abstract) is unsupported for clinical deployment.\n\n---\n\n**4. Presentation & Technical Issues**\n\na) Inefficient Data Visualization.\nOne problem is table design. Vertical tables (Tables 1–3, 5) waste space and hinder cross-architecture comparison. For example, Table 1’s left half is empty; DSC/NSD drops require vertical scanning. It is extremely hard to compare between experiments.\n\nb) Underutilized Content.\nSome important content is buried in appendices. SegFormer3D’s layer-wise sensitivity (App F, Table 11) can be moved to the main text (Sec 4.3) to strengthen architectural insights. Other important experiments in appendix can also be moved if the author save the main text space by reorganizing the table. Another acceptable way is to replace bulky tables with small multiples of line/bar charts to compare quantization granularity across models."}, "questions": {"value": "Thanks to your appendix experiment, otherwise the article would be far from enough just by relying on the experiments shown in the main text. Try to reorganize your presentation especially those tables.\n\nOthers see weaknesses.\n\nConsidering relevance, please cite this work if you did not write it. \n\n*Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines*\nhttps://arxiv.org/pdf/2501.17343v1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "not applicable"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8ks4UaCWdW", "forum": "eqG8h5Ztq0", "replyto": "eqG8h5Ztq0", "signatures": ["ICLR.cc/2026/Conference/Submission13791/Reviewer_aXnd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13791/Reviewer_aXnd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761367913739, "cdate": 1761367913739, "tmdate": 1762924327515, "mdate": 1762924327515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MedQuanBench, a large-scale benchmark for post-training quantization (PTQ) of 3D medical imaging models across CNN and hybrid (Transformer-based) architectures, bit-widths (INT8/INT4), and quantization granularities (per-tensor, per-channel/token, and an adaptive per-voxel strategy for 1×1×1 convs). It evaluates four datasets (BTCV, TotalSegmentator V2, AbdomenAtlas 1.1, WholeBrain) and reports segmentation metrics (DSC/NSD) under varied scales. Key findings: INT8 is near-lossless across backbones, while INT4 degrades sharply for hybrid architectures under coarse granularity. A layer-wise sensitivity study identifies 1×3×3 convolutions as the main INT4 bottleneck; replacing a sensitive 1×3×3 convolution with 1×1×1 recovers much of the drop. Hardware profiling on NVIDIA Ada/TensorRT shows ~3.2–3.8× model size reduction and ~2.1–2.7× speedups for INT8 quantization with negligible accuracy loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear empirical conclusion: INT8 preserves FP32 accuracy broadly; INT4 requires careful granularity, especially for hybrid architectures\n- Hardware-aware analysis with real INT8 deployment on TensorRT, reporting latency and memory gains consistent across models\n- Scale studies across model and dataset sizes, highlighting increasing INT4 sensitivity with larger, fine-grained tasks\n- Activation analyses showing spatially localized outliers in medical models vs. channel-localized outliers in LLMs, motivating granularity choices"}, "weaknesses": {"value": "- Lack of motivation: The paper assumes that this is an issue in the medical domain that requires careful investigation, particularly in the segmentation setting but without setting the stage for why it is indeed an important problem to solve.\n- Unclear contributions: The paper describes the benchmark dataset as a contribution, but it is not clear to me what different insights subsets of the total dataset provide and why one needs to evaluate quantization on the whole benchmark vs. a subset.\n- Method coverage: Focuses on PTQ; limited exploration of QAT or mixed-precision baselines that may further close INT4 gaps in critical layers. \n- Limited clinical validation: Strong segmentation metrics, but few task-level clinical end-points (e.g., time-to-diagnosis, error costs) to contextualize acceptable accuracy loss. \n- Robustness of the approach: Sensitivity analyses are primarily layer-wise; fewer robustness tests for distribution shift beyond datasets listed (OOD clinical sites, scanners, protocols). \nInterpretability of failures: While sensitive layers are identified, the failure modes under INT4 (e.g., boundary errors for small structures) could use more granular error breakdowns/visuals.\n- Generality of the approach: I find this is too specific to a particular context of medical imaging (i.e. segmentation) to be useful for the broader ICLR community."}, "questions": {"value": "- Why does the study focus exclusively on post-training quantization (PTQ)? Could including quantization-aware training (QAT) or mixed-precision methods provide a more complete landscape of quantization robustness in medical imaging models?\n\n- How were the datasets selected? Is the intention to represent medical imaging diversity (e.g., modality, anatomy, resolution) and in what way?\n\n- The analysis identifies 1×3×3 convolutions as particularly quantization-sensitive. Can you provide intuition on why these layers—versus larger kernels or attention modules—dominate degradation?\n\n- The paper demonstrates negligible accuracy loss for INT8 quantization, but what degree of degradation (e.g., 1–2% DSC drop) is clinically acceptable for deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BaBoABA8mP", "forum": "eqG8h5Ztq0", "replyto": "eqG8h5Ztq0", "signatures": ["ICLR.cc/2026/Conference/Submission13791/Reviewer_1ZXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13791/Reviewer_1ZXZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937388342, "cdate": 1761937388342, "tmdate": 1762924326934, "mdate": 1762924326934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MedQuanBench, a study of post‑training quantization (PTQ) for 3D medical image segmentation that combines four public datasets (BTCV, Total Segmentator V2, AbdomenAtlas 1.1 and Whole Brain), to evaluate multiple CNN and hybrid CNN-Transformer architectures at both 8‑bit and 4‑bit precision. The paper compares per‑tensor, per‑channel/token and per‑voxel (adaptive stratification) scaling and find that INT8 quantization preserves full‑precision performance while significantly reducing model size and latency. Per‑tensor INT4 quantization severely degrades the performance on transformer models, whereas CNNs fare better and can recover much of their performance using per‑voxel scaling (e.g., STU‑Net‑B improves from 0.647 to 0.829 Dice in per-channel to adaptive stratification). Larger model and dataset scales increase INT4 sensitivity, and layer‑wise analysis reveals that the 1 × 3 × 3 convolution is particularly critical and replacing it with a 1 × 1 × 1 convolution and using per‑voxel quantization improves INT4 robustness. Further, the paper notes that the activation smoothing, SVD and rotation offer limited or no gains, and supplements the experiments with real‑hardware profiling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The following are the strengths of the paper.\n\n1. By comparing per‑tensor, per‑channel/token and per‑voxel (adaptive stratification) scaling, the paper highlights the importance of granularity. It clearly shows that coarse per-tensor quantization is not suitable for 4‑bit precision, whereas finer granularity may recover accuracy. These insights/findings are helpful.\n\n2. The authors perform incremental dequantization and identify the 1 × 3 × 3 convolution as the most sensitive layer. Replacing it with a 1 × 1 × 1 convolution closes the gap to full‑precision performance. Such targeted analysis helps guide architecture design for low‑bit inference.\n\n3. Table 5 presents model size and latency measurements of real INT8 quantization using NVIDIA TensorRT, confirming the practical benefits of quantization."}, "weaknesses": {"value": "The following are the weaknesses of the paper.\n\n1. The paper claims that quantization is under‑studied in medical imaging, yet several prior works address this issue. MedQ introduced lossless ultra‑low‑bit quantization for U‑Net segmentation in 2021 **[1]**. U‑Net Fixed‑Point Quantization (2019) **[2]** also demonstrated 4‑bit weight quantization for medical segmentation and reported memory reduction with minimal accuracy loss. Recent EfficientQ (2024) provides a PTQ method tailored for medical segmentation and is publicly available **[3]**. None of these works are discussed, instead the paper mainly references general PTQ methods and LLM quantization. This weakens this study and leaves readers unaware of existing solutions.\n\n2. The hardware profiling numbers in Table 5 match those reported in a separate study that introduced a TensorRT‑based PTQ framework for 3D medical segmentation **[4]**. The **[4]** pre‑quantized U‑Net, SwinUNETR, UNesT and others and published the exact model size and latency reductions (e.g., U‑Net from 23.11 MB/2.62 ms to 6.61 MB/1.05 ms). MedQuanBench reuses these numbers but presents them as part of its own study, without acknowledging the source. Reusing results without citation is problematic and may mislead readers into believing these measurements were performed in this study.\n\n3. MedQuanBench is presented as a benchmark yet consists solely of separate evaluations on four datasets without any aggregate metric or consolidated score. Results are reported independently for BTCV and AbdomenAtlas 1.1, so it is unclear what benchmark performance means or how different methods would be ranked overall. The lack of unified evaluation criteria weakens the value of calling it a benchmark, which is the main message of the paper.\n\n4. (Minor) The text around lines 351–358 asserts that larger datasets increase sensitivity but appears under Table 2 without referencing Table 3, making the narrative confusing and suggesting the paper’s structure needs refinement.\n\n---\n\n**[1]** MedQ: Lossless ultra-low-bit neural network quantization for medical image segmentation (Medical Image Analysis, 2021)\n\n**[2]** U-Net Fixed-Point Quantization for Medical Image Segmentation (MICCAI 2019)\n\n**[3]** EfficientQ: An efficient and accurate post-training neural network quantization method for medical image segmentation (Medical Image Analysis, 2024)\n\n**[4]** Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines (arXiv: 2501.17343)"}, "questions": {"value": "MedQuanBench addresses a practical problem, how to deploy memory and compute intensive 3D segmentation models on limited hardware by using low-bit quantization. However, the contributions seem minimal and are not well presented. The paper provides per-dataset analyses yet calls the work MedQuanBench. It also fails to properly discuss related work and reuses reported results without citing the original sources. Clarification on these points would be appreciated."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The paper appears to reuse hardware profiling results (Table 5) from a previously published study (Table 1 in **[4]**) that introduced a TensorRT-based PTQ framework for 3D medical segmentation. However, the current submission presents these results as if they were obtained in this study, without citing or acknowledging the original source.\n\n---\n\n**[4]** Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines (https://arxiv.org/pdf/2501.17343v1)"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xrQOQYqQ2d", "forum": "eqG8h5Ztq0", "replyto": "eqG8h5Ztq0", "signatures": ["ICLR.cc/2026/Conference/Submission13791/Reviewer_CQLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13791/Reviewer_CQLJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255160441, "cdate": 1762255160441, "tmdate": 1762924326575, "mdate": 1762924326575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies post-training quantization (PTQ) for 3D medical segmentation. It explores 8-bit and 4-bit settings and compares quantization granularities. Results show 8-bit is essentially lossless, while 4-bit is fragile unless granularity is fine and certain layers are handled carefully. A small architectural tweak (replacing a 1×3×3 conv with 1×1×1) improves 4-bit stability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. INT8 works effectively out of the box, delivering almost no accuracy loss while providing noticeable improvements in inference speed and model size.\n2. The paper has some good insights on quantization, showing evidence that coarse per-tensor INT4 quantization fails on many models, especially hybrids that include transformer blocks.\n3. The paper identifies the sensitivity of 1×3×3 convs, and the simple 1×1×1 replacement is a good takeaway that could help the community.\n4. Evaluations are conducted on multiple models (CNN + hybrid) and datasets."}, "weaknesses": {"value": "My main concern is that the paper positions itself as a benchmark but does not clearly define a unified evaluation metric, which makes it hard to compare performance across methods. Furthermore, the exploration of INT4 improvements is quite narrow, while granularity and the 1×1×1 replacement are studied, other methods, such as mixed precision or selective dequantization, could have been tested to provide a more detailed picture."}, "questions": {"value": "It would help if the benchmark framing were made more consistent, for instance, having a single unified metric per model and dataset would make it easier for future work to compare against this benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CCHmdRjMbY", "forum": "eqG8h5Ztq0", "replyto": "eqG8h5Ztq0", "signatures": ["ICLR.cc/2026/Conference/Submission13791/Reviewer_zJSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13791/Reviewer_zJSB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294406036, "cdate": 1762294406036, "tmdate": 1762924326123, "mdate": 1762924326123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}