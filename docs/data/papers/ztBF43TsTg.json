{"id": "ztBF43TsTg", "number": 9151, "cdate": 1758113251395, "mdate": 1763109956335, "content": {"title": "From Language to Action Streams: Bridging LLM Autoregression for Long-Horizon Robot Action Prediction", "abstract": "Vision-Language-Action(VLA) models is a transformative paradigm for robotic control, leveraging pre-trained vision-language models(VLMs) to directly translate natural language instructions and visual observations into low-level actions. \nThe prominent idea of ``Action-as-Language\" discretizes action spaces into tokens for large language models(LLMs), reframing action prediction as a standard sequential language generation task. \nHowever, current implementations underutilize the LLM's full generation potential, confining action prediction to fixed-length, single-step token sequences and limiting the policy's generation horizon.\nTo overcome this limitation, we propose the \\textbf{Action Stream} paradigm, which customizes LLM training and inference recipes to VLAs, enabling the generation of extended chains of action tokens and facilitating implicit long-horizon generation with task performance improvements.\nFor training action streams, we propose a two-phase approach: Long-horizon Behavior Cloning(L-BC) and Step-wise Action Alignment(S-AA). \nL-BC enables VLA models to generate coherent multi-step action sequences, while S-AA mitigates exposure bias during sequential inference, creating a framework that enables long-horizon generation while reducing error accumulation.\nDuring deployment, decoding strategies from language generation can be successfully transferred to action streams, enabling efficient solution search and task performance improvements.\nThrough extensive evaluations on the simulation benchmark and real-world robotic setups, we demonstrate that the Action Stream paradigm achieves improved task performance when extending the generation horizon, representing a significant step toward unified vision-language-action modeling.", "tldr": "We propose an autoregressive generation method for long-horizon action generation in robot control tasks.", "keywords": ["vision language action model", "large language model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/24b4ee2d501d5dddf8ebd03883ac0ed7b44f0edb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a simple extension to the Vision-Language Action model paradigm where the VLA is tasked with producing additional steps into the future when predicting an action at the current timestep. The training of this model, built on Llama 2, introduces two training methods: long horizon behaviour cloning and step-wise action alignment. The first step of training enables multi-step action prediction, while the second step of training attempts to limit bias during inference. Results on simulated and real-world robots seem promising."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A simple but quality method that increases performance without too much additional overhead. \n2. The paper is well-written and results across simulation and real-world settings are a bonus."}, "weaknesses": {"value": "1. I think there are missing baselines (RT-1-X, RT-2-X, Octo, for example) that could make the work even stronger. Showing that the two step approach for generating multiple steps of actions improves performance across a broad range of models would make this work much stronger. I think this would fit within the paper very well, because the authors don't claim SOTA for their specific algorithm, but showing that their proposed two step training method improves performance across additional VLA models would show that this could be a fruitful paradigm to use as the standard for training any future VLA models. I think validating the additional action steps are a benefit across more models should be tested in simulation. \n2. I think some discussion & comparison of world models would be incredibly valuable to the reader for full context of different methods in this space. In my view, these VLA models boil down to world models (from the RL literature i.e. Dreamer V3 or TD-MPC-2) where the VLA models have a similar notion of how their actions will impact the environment/task that they are interacting with. \n3. For each of the tables, some more information is required to ensure that full context of what the table contains can be gathered. For example, in Table 1, what does the bolding of results mean? What is H? What are the labels of Spatial, Object, etc? I think making the descriptions of these tables contain this information would be a large help to the reader."}, "questions": {"value": "1. How does the VLA paradigm relate to the world model paradigm in RL?\n2. The proposed method doesn't leverage any auxiliary objectives for training (i.e. predicting the next state, predicting the next action). Do the authors think the integration of these objectives could enable predictions even further into the future? Would the bias mitigation strategy be needed if these objectives were added?\n3. Results in simulation seem to show means and standard deviations, how many seeds were used to generate these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9AxD9uwKsl", "forum": "ztBF43TsTg", "replyto": "ztBF43TsTg", "signatures": ["ICLR.cc/2026/Conference/Submission9151/Reviewer_kePK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9151/Reviewer_kePK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576202545, "cdate": 1761576202545, "tmdate": 1762920834633, "mdate": 1762920834633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for all the valuable reviews. We will continue to refine our work."}}, "id": "bslDqDNPSk", "forum": "ztBF43TsTg", "replyto": "ztBF43TsTg", "signatures": ["ICLR.cc/2026/Conference/Submission9151/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9151/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763109955231, "cdate": 1763109955231, "tmdate": 1763109955231, "mdate": 1763109955231, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Action Stream paradigm for Vision-Language-Action (VLA) models, customizing OpenVLA training and inference to enable long-horizon action sequence generation in robotics. The approach involves a two-phase training method: Long-horizon Behavior Cloning (L-BC) for imitating expert streams and Step-wise Action Alignment (S-AA) to mitigate exposure bias. Extensive evaluations on simulation benchmarks and real-world setups demonstrate improved task performance with extended generation horizons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Innovative Framework for Long-Horizon Generation​​. The Action Stream leverages the inherent coherence of autoregressive models, allowing policies to generate smooth, multi-step actions from a single observation. The paradigm shift from step-wise to stream-wise prediction reduces temporal inconsistencies, as visualized in trajectory analyses.\n\n- ​​Robust Training Methodology with Error Mitigation​​. L-BC provides a foundation for imitating expert demonstrations, while S-AA introduces online exploration and alignment via Direct Preference Optimization (DPO), pinpointing and correcting the first deviation in action sequences. This reduces compounding errors and enhances policy reliability, as evidenced by improved success rates across horizons."}, "weaknesses": {"value": "- The article's novelty is limited. The authors merely added multi-step action prediction on top of OpenVLA, which does not significantly differ from the original VLA architecture.  \n\n- The paper fails to address key VLA limitations, particularly limited generalization, especially in text generalization.  \n\n- The article only compares with one baseline OpenVLA, and the results are clearly not state-of-the-art on LIBERO benchmarks.  \n\n- As horizon H increases, Action Stream performance declines, indicating severe cumulative prediction error. This may stem from limited data scale, suggesting VLA has not truly learned underlying action space patterns."}, "questions": {"value": "The same as weekness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bEzMEA4Jmk", "forum": "ztBF43TsTg", "replyto": "ztBF43TsTg", "signatures": ["ICLR.cc/2026/Conference/Submission9151/Reviewer_ep9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9151/Reviewer_ep9u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890724465, "cdate": 1761890724465, "tmdate": 1762920834204, "mdate": 1762920834204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present Action Streams, a VLA training paradigm with two components: a long-horizon behavior cloning phase (L-BC), which trains a VLA model to extend its action generation horizon, and a step-wise action alignment step phase (S-AA), which accounts for error accumulation in the long-horizon action output. Action Streams is built on top of OpenVLA. L-BC is implemented by creating long-horizon action streams from the current data with concatenation, then performing supervised fine-tuning. S-AA is implemented by performing Direct Preference Optimization, using the supervised fine-tuned policy as the expert, and formulating the loss based on when the most likely actions between the expert and reference policy diverge. Experiments are conducted on a real world set up, and on Libero to analyze and ablate Action Streams."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Action Streams is well motivated and clearly detailed.\n- The Action Streams results on long horizons are interesting and promising, as shown in Table 1, and show that S-AA has the potential to combat error accumulation on longer horizons."}, "weaknesses": {"value": "- The novelty of Action Streams is unclear. How is L-BC different from instruction tuning? Is the S-AA component the main contribution?\n- The experiments lack other long-horizon VLA methods (such as those mentioned in the related works section) as baselines."}, "questions": {"value": "- Other methods such as $\\pi_{0}$ [1] generate actions over multiple timesteps and perform temporal aggregation to generate the final action. How does Action Streams improve over this method for long-horizon action generation?\n- Is the loss in equation 8 novel?\n- Did OpenVLA and Action Streams see the same data in Table 2?\n[1] $\\pi_0$: A Vision-Language-Action Flow Model for General Robot Control. Black et al. arxiv preprint arXiv:2410.24164 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mcRivc34xP", "forum": "ztBF43TsTg", "replyto": "ztBF43TsTg", "signatures": ["ICLR.cc/2026/Conference/Submission9151/Reviewer_TZL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9151/Reviewer_TZL1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965132082, "cdate": 1761965132082, "tmdate": 1762920833773, "mdate": 1762920833773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Action Stream paradigm for Vision-Language-Action (VLA) models, aiming to extend the policy generation horizon by enabling LLMs to generate multi-step action sequences autoregressively. The authors introduce a two-stage training scheme: (1) Long-Horizon Behavior Cloning (L-BC) to learn coherent action streams, and (2) Step-wise Action Alignment (S-AA) using Direct Preference Optimization to mitigate exposure bias. Experiments are conducted on the LIBERO benchmark and a Kinova Jaco2 robot."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The incorporation of DPO-based alignment is interesting.\n2. The evaluation is done in both simulation and real-world setups."}, "weaknesses": {"value": "1. The proposed “multi-step prediction” largely corresponds to modifying OpenVLA to generate multiple actions in one forward pass, which is something prior work[5] already implemented. The contribution is therefore incremental.\n2. The method is built based on OpenVLA but OpenVLA is no longer the SOTA. Most of the SOTA VLAs such as Pi[3,4] do multiple step prediction (e.g. pi0[3] do a 50 step prediction), combined with receding horizon control/ temporal assemble, they can achieve much better performance (e.g. pi0.5[4] has an average of 0.97 success rate on libero tasks with multiple steps prediction and execution [2]). The proposed method yields lower task success at longer horizons (Tables 1 and 3). The core idea of autoregressive long-horizon prediction is neither novel nor shown to improve real performance. The incremental beam-search decoding further limits its originality.\n3. The contribution of the proposed method is questionable.  It seems the authors want to do long horizon prediction but the shown performance actually decreases with longer horizon. Then what’s the argument of doing long horizon with the proposed method? And what’s the contribution of the proposed method? Furthermore, the paper didn't compare with other SOTA VLAs doing long horizon predictions as aforementioned.\n4. “current implementations underutilize the LLM’s full generation potential, confining action prediction to fixed-length, single-step token sequences and limiting the policy’s generation horizon. ” This is not true, for example, FAST[1] can tokenize a sequence of actions into one token, there are also other action tokenizers works that predicts varying length multi step token sequences.\n5. What is the expert data at the online exploration process? If it's the same training dataset, this doesn’t make sense, because the action can be multi-mode and deviating from the expert doesn’t mean failure at test time.\n\n[1] Pertsch, Karl, et al. \"Fast: Efficient action tokenization for vision-language-action models.\" arXiv preprint arXiv:2501.09747 (2025).\n\n[2]  Zhou, Xueyang, et al. \"LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization.\" arXiv preprint arXiv:2510.03827 (2025).\n\n[3] Black, Kevin, et al. \"$\\pi_0 $: A Vision-Language-Action Flow Model for General Robot Control.\" arXiv preprint arXiv:2410.24164 (2024).\n\n[4] Intelligence, Physical, et al. \"π0. 5: a vision-language-action model with open-world generalization, 2025.\" URL https://arxiv. org/abs/2504.16054 1.2: 3.\n\n[5]  Huang, Huang, et al. \"Otter: A vision-language-action model with text-aware visual feature extraction.\" arXiv preprint arXiv:2503.03734 (2025)."}, "questions": {"value": "see weakness 5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xPwi2GtuYw", "forum": "ztBF43TsTg", "replyto": "ztBF43TsTg", "signatures": ["ICLR.cc/2026/Conference/Submission9151/Reviewer_KQrT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9151/Reviewer_KQrT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968673650, "cdate": 1761968673650, "tmdate": 1762920833468, "mdate": 1762920833468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}