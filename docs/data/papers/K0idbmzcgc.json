{"id": "K0idbmzcgc", "number": 14647, "cdate": 1758240791766, "mdate": 1763698247263, "content": {"title": "OS-W2S: An Automatic Labeling Engine for Language-Guided Open-Set Aerial Object Detection", "abstract": "In recent years, language-guided open-set aerial object detection has gained significant attention due to its better alignment with real-world application needs. However, due to limited datasets, most existing language-guided methods primarily focus on vocabulary-level descriptions, which fail to meet the demands of fine-grained open-world detection. To address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. Centered around an open-source large vision-language model and integrating image-operation-based preprocessing with BERT-based postprocessing, we present the $\\textbf{OS-W2S Label Engine}$, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. Using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called Multi-instance Open-set Aerial Dataset ($\\textbf{MI-OAD}$), addressing the limitations of current remote sensing grounding data and enabling effective language-guided open-set aerial detection. Specifically, MI-OAD contains 163,023 images and 2 million image-caption pairs, with multiple instances per caption, approximately 40 times larger than comparable datasets.\nTo demonstrate the effectiveness and quality of MI-OAD, we evaluate three representative tasks: language-guided open-set aerial detection, open-vocabulary aerial detection (OVAD), and remote sensing visual grounding (RSVG). On language-guided open-set aerial detection, training on MI-OAD lifts Grounding DINO by +31.1 AP$_{50}$ and +34.7 Recall@10 with sentence-level inputs under zero-shot transfer. Moreover, using MI-OAD for pre-training yields state-of-the-art performance on multiple existing OVAD and RSVG benchmarks, validating both the effectiveness of the dataset and the high quality of its OS-W2S annotations. More details are available at \\url{https://anonymous.4open.science/r/MI-OAD}.", "tldr": "", "keywords": ["Open-Set Aerial Object Detection", "Automatic Label Engine", "Multi-instance Open-set Aerial Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e6d41257253a1cb147c280f46a0109369e09c4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents OS-W2S Label Engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. Using this label engine, the authors also construct a benchmark dataset MI-OAD for language-guided open-set aerial detection. Experiments on open-vocabulary aerial detection and remote sensing visual grounding are carried by training and evaluating the model with MI-OAD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MI-OAD contains 163K images and 2 million image-caption pairs, with multiple instances per caption, which is large-scale compared to existing datasets. \n2. The experiments show that using the proposed MI-OAD as training corpus results in a better performance in both open-vocabulary detection and visual grounding tasks."}, "weaknesses": {"value": "1. While MI-OAD contains 163,023 images and 2 million image-caption pairs, it is difficult to evaluate a dataset simply based on its size. For example, if we split each image into four, then the number will be 4x larger. And for MI-OAD, its quality is not guaranteed. The captions are generated by VLMs, but there is not enough experiments involving quality assessment. \n2. The dataset is essentially generated by integrating available datasets. The problem is, after reviewing some of the data, I find that most of them keep the original categories, without unification. From what I understand, most of the data are labeled with only a small portion of the 100 categories mentioned in the paper, and the other objects are just ignored.\n3. Some minor writing issues: For example, in Page 6, Fig.3b, Section5.1: Missing space. ( 1,765 instances): Redundant space. In Fig. 2, InterVL should be InternVL. In Fig. 3, (e) is not aligned, and some labels in (a)-(d) are too small."}, "questions": {"value": "1. Images in RS show high variation. Some of them contain only a few objects, some may contain more than 100 objects. How do you control the captions in these two different situations? If there are many objects in the image, what will the caption be like? \n2. In figure 3, \"instances per caption\" shows 98% images have <= 20 instances. But many datasets used by the authors (such as DOTA) are originally very dense. Why is this discrepancy? Do you crop the image to decrease the instance count? If yes, is the preprocessing rule detailed in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2iBGEPo5xL", "forum": "K0idbmzcgc", "replyto": "K0idbmzcgc", "signatures": ["ICLR.cc/2026/Conference/Submission14647/Reviewer_zebP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14647/Reviewer_zebP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769099140, "cdate": 1761769099140, "tmdate": 1762925021463, "mdate": 1762925021463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an automatic labeling engine, OS-W2S, which leverages large vision-language models (e.g., InternVL) with structured preprocessing and BERT-based postprocessing to generate word-, phrase-, and sentence-level annotations. Using this system, the authors construct MI-OAD, a large-scale language-guided open-set aerial detection dataset with 163K images and 2M image-caption pairs. The paper demonstrates substantial performance gains on three tasks (language-guided open-set detection, open-vocabulary detection, and remote-sensing visual grounding) achieving +31.1 AP50 and +34.7 Recall@10 improvements for Grounding DINO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: While the core methodology reuses existing components (e.g., InternVL, BERT, Grounding DINO), the paper exhibits a creative integration of large vision-language models for automated annotation in the aerial domain, which is a relatively unexplored application area. The introduction of a multi-granularity labeling strategy also shows originality in problem formulation. The OS-W2S pipeline effectively extends the scope of open-set detection from object categories to natural-language-level semantics.\n\n2. Quality: The technical quality of the work is strong. The pipeline is methodologically sound, carefully engineered, and empirically validated. The experiments are thorough, covering multiple downstream tasks, and results consistently support the paper’s claims. The reproducibility is high, with sufficient implementation details and dataset statistics provided.\n\n3. Clarity: The paper is very clearly written and well-structured. The figures and diagrams are of high quality, particularly those explaining the annotation pipeline and dataset structure. \n\n4. Significance: The proposed MI-OAD dataset and OS-W2S labeling engine have strong practical and community significance. They address a major bottleneck in open-set remote sensing(data scarcity)and could serve as a foundation for future multimodal research in aerial imagery. Although the theoretical innovation is limited, the impact on benchmark construction and large-scale data automation is substantial and could influence related domains.\n\nThis paper’s strength lies in clarity, execution quality, and domain-level contribution. It delivers a solid, reproducible, and impactful system that will benefit the community, even though it does not introduce fundamentally new learning mechanisms."}, "weaknesses": {"value": "1.The core innovation of OS-W2S lies in the integration of existing components (InternVL, BERT, Grounding DINO) rather than the introduction of new learning mechanisms or optimization objectives. Unlike recent ICLR papers that advance representation learning, this work primarily presents an engineering system for data generation.\n\n2.The paper reports aggregate metrics (AP50, Recall@10) but does not investigate failure modes or error patterns in generated annotations. This omission weakens the empirical rigor, especially for a system that depends heavily on VLM predictions.\n\n3.Although 10K samples were manually verified, this represents only 0.5% of the dataset. The authors should consider statistical sampling or cross-annotation consistency tests to better quantify the overall label reliability.\n\n4.The related work section does not clearly differentiate OS-W2S from prior automatic labeling frameworks, such as LabelAnything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts (ECAI 2025) , which also leverage LLM/VLM pipelines.\nThe work’s weaknesses are not in execution but in research framing and analytical depth. With more rigorous evaluation of label reliability, stronger theoretical motivation for learning representations, and detailed analysis of model behavior, the paper could evolve from a engineering contribution into a research study."}, "questions": {"value": "1.Currently, OS-W2S is described as a deterministic pipeline integrating pretrained models. Could the authors clarify whether any learnable components or adaptive mechanisms are included in the labeling process (e.g., fine-tuning InternVL or learning confidence thresholds)?\n\n2.You mention that 10K samples were manually checked. Could you please describe how these samples were selected (random, stratified by scene type, or class-balanced)? A more rigorous sampling or reliability metric could help validate the robustness of your dataset.\n\n3.Aerial imagery often involves dense and overlapping objects, where language-based models may confuse spatial relationships. How does OS-W2S handle such ambiguity? For instance, when multiple small vehicles are present, how is phrase-level annotation disambiguated? Could you provide examples of typical failure cases and mitigation strategies?\n\n4.Since OS-W2S relies on large VLMs, it would be helpful to know how hallucinated or semantically incorrect captions are detected or filtered out. How do you filter out noisy subtitles?\n\n5.How does OS-W2S differ from other automatic labeling systems that combine LLMs and VLMs, such as LabelAnything ?\n\n6.Could the authors discuss how OS-W2S might influence learned feature representations? For example, does the multi-granularity annotation improve feature disentanglement or visual-textual alignment in downstream models?\n\n7.Can OS-W2S be easily generalized to other domains (e.g., medical imaging, autonomous driving) where labeling cost is also high? If so, what adaptations would be needed, for example, domain-specific vocabulary or hierarchical label templates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ERIr7M5PbQ", "forum": "K0idbmzcgc", "replyto": "K0idbmzcgc", "signatures": ["ICLR.cc/2026/Conference/Submission14647/Reviewer_X2to"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14647/Reviewer_X2to"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811341575, "cdate": 1761811341575, "tmdate": 1762925021131, "mdate": 1762925021131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an automatic labeling tool (OS-W2S) and uses it to create MI-OAD, a large-scale aerial imagery dataset designed to advance language-guided aerial open-set object detection research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well organized.\n\nThe proposed MI-OAD dataset is a valuable, large-scale dataset for the community.\n\nExperiments on YOLO-World and Grounding DINO demonstrated that MI-OAD can improve the model's performance in aerial object detection."}, "weaknesses": {"value": "The core of the paper is to use VLM to generate annotations, but VLM itself may have biases (such as preferences for specific colors and shapes) and the risk of creating \"illusions\". Although the paper validates this with a stronger model in Section 4, \"Quality Control Analysis\", it does not fundamentally avoid the problem.\n\nIn section 5.4， Key terms like \"OPT-RSVG\" and \"DIOR-RSVG\" are not defined.\n\nWhy is the “Grounding DINO (+MI-PAD)” configuration in Table 2 much lower than the “LPVA” baseline under the cmuIoU metric?"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "L4mIsDaIgr", "forum": "K0idbmzcgc", "replyto": "K0idbmzcgc", "signatures": ["ICLR.cc/2026/Conference/Submission14647/Reviewer_eWjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14647/Reviewer_eWjL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818443690, "cdate": 1761818443690, "tmdate": 1762925020771, "mdate": 1762925020771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an automatic labeling pipeline for generating language-guided annotations in aerial imagery, and uses it to build a new large-scale dataset. The dataset integrates data from existing aerial detection sources and adds multiple types of text descriptions for each instance.  The authors show that models trained or adapted on MI-OAD achieve better performance on several aerial detection and grounding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the lack of large-scale language-grounded datasets in the aerial domain, which is a real bottleneck for open-set detection research. MI-OAD is significantly larger and more diverse than existing aerial grounding datasets.\n- Experiments are extensive and show clear improvements across several downstream benchmarks. The dataset and code are publicly released, making the work reproducible and potentially useful to the community."}, "weaknesses": {"value": "- The discussion of related work focuses almost entirely on model architectures rather than dataset construction. Since this paper’s main contribution is a dataset and annotation pipeline, it should instead position the work within the context of existing dataset-building methodologies. A detailed quantitative comparison with prior aerial or language-grounded datasets is missing. The paper should explicitly articulate what is new about the proposed pipeline beyond scale, and how its annotation strategy differs from existing automatic labeling systems.\n\n- The overall novelty is limited. The paper reads more like a detailed technical report that consolidates known components into a large dataset pipeline. While the engineering execution is solid, the scientific contribution is unclear. The paper lacks abstraction or theoretical framing that would justify it as a research advance suitable for ICLR. It would fit better as a data or resource paper for a vision-oriented venue such as CVPR / ICCV.\n\n- An ablation analysis is missing. Since the annotation pipeline contains multiple preprocessing and postprocessing components, it would be important to remove or modify individual steps and evaluate model performance on the resulting datasets. This would verify that each design choice meaningfully contributes to dataset quality; otherwise, the complexity of the pipeline is not well justified.\n\n- In the README.md source code (https://anonymous.4open.science/api/repo/MI-OAD/file/README.md)  of the provided code repository, a GitHub badge link (https://img.shields.io/github/stars/GT-Wei/MI-OAD) reveals the authors' identity. This appears to be unintentional, so I think maybe it is fine."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tgxiTgz5xD", "forum": "K0idbmzcgc", "replyto": "K0idbmzcgc", "signatures": ["ICLR.cc/2026/Conference/Submission14647/Reviewer_RRL5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14647/Reviewer_RRL5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926304856, "cdate": 1761926304856, "tmdate": 1762925020171, "mdate": 1762925020171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes  an automatic labeling engine that generates fine-grained textual annotations for aerial images using VLMs, and introduces MI-OAD, a large-scale dataset for language-guided openset aerial object detection. MI-OAD contains 163k images and 2 million image caption pairs at word, phrase, and sentence levels. Experiments show that pretraining on MI-OAD substantially boosts performance across open-vocabulary aerial detection, remote-sensing visual grounding, and zero-shot open-set detection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Visual grounding has significant value and wide applications. Yet, existing dataset is not large enough to support the task. This paper proposed an automated way to generate grounding dataset using VLMs. The dataset will advance the research in this direction.\n\n- The labeling pipeline is novel for aerial domains, combining structured preprocessing, VLM interaction, and BERT-based postprocessing. MI-OAD’s scale and multi-granularity annotation approach make it a comprehensive dataset for open-set aerial detection.\n\n- Training or pretraining on MI-OAD significantly improves multiple benchmarks, indicating clear practical value and potential to establish a standard benchmark for language-guided aerial detection."}, "weaknesses": {"value": "- Although sourced from eight aerial datasets, details about geographic, environmental, or temporal diversity are sparse. It is unclear whether MI-OAD adequately represents different regions, seasons, or sensor modalities.\n\n- The label engine relies heavily on a single chosen VLM (InternVL-2.5-38B-AWQ), and the paper does not assess how dataset quality varies across models, e.g. evaluating usinng other VLMs.\n\n- Only a very small portion of dataset is manually reviewed (0.5% of data).  The generalization of annotation quality to the remaining dataset is assumed, but not empirically proven.\n\n- The paper emphasizes overall improvements but provides little qualitative or quantitative discussion of where MI-OAD annotations or trained models fail."}, "questions": {"value": "- A more thorough analysis of the dataset, e.g. regions, sensor types, is encouraged.\n\n- Alternative VLMs are encouraged to be analyzed for comparison.\n\n- Additional human review is encouraged."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jv63TR5pJc", "forum": "K0idbmzcgc", "replyto": "K0idbmzcgc", "signatures": ["ICLR.cc/2026/Conference/Submission14647/Reviewer_6Dr8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14647/Reviewer_6Dr8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095317379, "cdate": 1762095317379, "tmdate": 1762925019810, "mdate": 1762925019810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}