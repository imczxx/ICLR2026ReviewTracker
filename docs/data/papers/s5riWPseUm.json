{"id": "s5riWPseUm", "number": 24833, "cdate": 1758360851421, "mdate": 1759896746206, "content": {"title": "Refining Bias and Reward in LLM Recommender Agents through Meta-Controlled Tool Invocation", "abstract": "Large language model (LLM) agents have recently been brought to recommender systems given their flexible capability of tool use. Although existing approaches adopt the reasoning and acting paradigms for profiling, planning, and memory augmentation, they remain ad hoc and overlook core recommendation challenges in agent-environment interactions, including debiasing and reward estimation in offline learning scenarios. In this paper, we introduce BARO (Bias And Reward Optimization), a meta-controlled, tool-augmented LLM agent framework that explicitly addresses these challenges. BARO employs a two-stage recommendation process: a coarse recommender generates a candidate slate based on user history, and a meta-controller adaptively invokes three specialized tools to refine the recommendation results: a bias detector assesses and mitigates bias in the candidate set, a reward estimator calibrates noisy offline rewards, and an action grounder selects final recommendations from the candidate pool. This design injects bias correction and reward refinement directly into the agent’s decision loop in the recommendations. Empirical results on two benchmark datasets demonstrate that BARO achieves consistent improvements over state-of-the-art methods in metrics such as accuracy, diversity, and fairness. The code will be made publicly available upon acceptance.", "tldr": "We design a tool-augmented LLM agent for offline recommendation, where a meta-controller adaptively invokes bias detection, reward refinement, and action grounding to improve both accuracy and fairness.", "keywords": ["LLM Agent", "Offline Reinforcement Learning for Recommender Systems"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/043a4988d93bcfe676a44f3c62c911d6386a342e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes BARO (Bias And Reward Optimization), a meta-controlled, tool-augmented LLM agent framework for recommender systems. The framework consists of a two-stage pipeline: a frozen LLM acts as a coarse recommender to generate a candidate slate, while a meta-controller adaptively invokes three tools—bias detector, reward estimator, and action grounder—to refine recommendations. The goal is to address bias mitigation and reward calibration in offline learning scenarios. Experiments on Steam and Amazon Book datasets are provided, showing improvements in some metrics over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important problem — how to mitigate bias and improve reward reliability in LLM-based recommendation.\n\n2. The general idea of combining LLM agents and meta-control for recommendation is interesting and in line with recent trends of using agentic reasoning for RecSys."}, "weaknesses": {"value": "1. The paper criticizes post-training as “computationally costly,” but its own framework introduces both SFT and RL stages, which are even more expensive. There is no analysis or discussion showing that BARO reduces cost compared to prior post-training methods. Hence, the motivation is not convincingly addressed.\n\n2. The design choice—using a frozen LLM for coarse recommendation and a downstream collaborative recommender to generate reward signals—is questionable. Typically, LLMs lack global-level item ranking capability, while collaborative recommenders are more effective in retrieval. The proposed division of labor thus appears reversed and unintuitive, weakening the conceptual justification.\n\n3. There are several critical missing details in the method and experimental settings.\n(1) It is unclear how the coarse recommender prompt is constructed, and how item candidates are ensured to belong to the dataset’s valid item set. \n(2) How are LLMs prompted when the item pool exceeds tens of thousands? Are items pre-filtered or indexed? How is fairness ensured when comparing LLM-based models with sequential baselines that have different candidate pools?\n(3) Metrics such as IoI/IoR lack calculation details within this framework.\n(4) It is unclear how sequential recommendation baselines are implemented in the current setup.\n\n4. Both bias and reward checkers improve results, but the paper also emphasizes that the meta-controller can decide whether to invoke them. Without experiments comparing “always-check” versus “meta-controlled” usage, the proposed meta-decision mechanism seems unnecessary and unvalidated.\n\n5. The reported results show SASRec performing drastically worse than Caser and GRU4Rec, which contradicts prior literature and suggests improper baseline setup or evaluation. The authors do not analyze or justify this anomaly.\n\n6. The reported accuracy values are extremely high (e.g., Caser reaching 0.977), which raises concerns about whether the dataset or task setup is meaningful.\n\n7. The refined reward estimation modules are under-explained. It remains unclear how refined rewards are obtained or why this additional deisgn is necessary."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2scASy755m", "forum": "s5riWPseUm", "replyto": "s5riWPseUm", "signatures": ["ICLR.cc/2026/Conference/Submission24833/Reviewer_B5zs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24833/Reviewer_B5zs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674420910, "cdate": 1761674420910, "tmdate": 1762943213123, "mdate": 1762943213123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a BARO method to solve the problems of LLM-based recommender systems: debiasing and reward estimation. Empirical results show the method works well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-motivated.\n\nThe paper is readable."}, "weaknesses": {"value": "1. One of the motivations of the paper is that LLM-based recommender systems suffer from *interaction bias*, but the paper does not give examples, quantitative analysis, a definition or qualitative explanation, nor does it explain the harms caused by interaction bias.\n2. Similarly, *reward inaccuracies* are not illustrated with examples or quantitative analysis, and their harms are not shown.\n3. What is the role of the “world model” in Section 4?\n4. Figure 1 needs improvement. Many arrows in the figure will confuse readers — I suggest adding explanatory labels on the arrows.\n5. The abstract claims “BARO achieves consistent improvements over state-of-the-art methods in metrics such as accuracy, diversity, and fairness,” but I do not see diversity or fairness metrics in the experiments.\n6. The paper aims to solve “interaction bias” and “reward inaccuracies” in LLM-based recommender systems, but I find no analysis of these problems in Experiments. Therefore, I am skeptical that the proposed method actually addresses these two issues."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O1uswnI0UY", "forum": "s5riWPseUm", "replyto": "s5riWPseUm", "signatures": ["ICLR.cc/2026/Conference/Submission24833/Reviewer_bLB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24833/Reviewer_bLB2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896972802, "cdate": 1761896972802, "tmdate": 1762943212914, "mdate": 1762943212914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BARO (Bias And Reward Optimization), an agentic recommendation framework for addressing the debiasing and reward estimation in recommender systems. Specifically, a meta-controller dynamically calls three tools (i.e., bias detector, reward estimator, and action executor) to correct bias and stabilize reward estimation. Experiments on extensive datasets verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Timely and well-motivated study for introducing agentic AI in recommender systems.** \nThe paper addresses a pertinent issue at the intersection of *agentic AI* and recommender systems. Moreover, the unique question of bias is an important issue in recommender systems, which is a reasonable motivation for adopting agents in recommendation.\n\n**Novel modular design.** \n\nThe two tools (i.e., bias detector and reward estimator) proposed in this work are practical for addressing the debiasing and reward estimation issues in recommendation. \n\n**Solid experiments and clear presentation.** \n\nExperiments on multiple datasets, together with detailed ablation studies, convincingly show the effectiveness of each module. The writing is also clear and coherent, making the whole paper easy to understand and easy to follow."}, "weaknesses": {"value": "**Lack of discussion about the user simulator.**\n\nIt would be important to give more details about how the user simulator is trained. Additionally, it would be important to add more details about the reliability of the user simulator.\n\n**Lack of case study and demonstration.**\n\nIt would be better to illustrate how the proposed method works in practice, especially with some demonstration and examples. For example, how does this method call the two tools under which condiction?"}, "questions": {"value": "Refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kls335Bu3O", "forum": "s5riWPseUm", "replyto": "s5riWPseUm", "signatures": ["ICLR.cc/2026/Conference/Submission24833/Reviewer_6Dei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24833/Reviewer_6Dei"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980582227, "cdate": 1761980582227, "tmdate": 1762943212738, "mdate": 1762943212738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BARO, a meta-controlled tool-invocation framework that aims to reduce recommendation bias and calibrate reward learning for LLM-driven recommender agents. The system adds a controller that chooses when and how the agent calls tools that (i) diversify or rebalance candidate sets and (ii) reshape reward signals used for agent tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presentation is clear and easy to follow.\n\n2. The paper focuses on two pain points for LLM recommender agents: exposure/popularity bias and reward misspecification. The former is widely documented to degrade user experience and diversity, while the latter can lead to unstable offline-to-online transfer."}, "weaknesses": {"value": "1. The paper states that “most agentic frameworks were not designed specifically for RecSys,” yet the proposed method mainly adds two recommendation-specific tools while keeping a generic ReAct-style agent loop otherwise unchanged. It is not very convincing that the proposed agentic framework is recommendation-specific as replacing the tools with other domain tools seems to not affect the framework's functionality.\n\n2. The design appears to host and orchestrate multiple models/modules (planner LLM, bias tool, reward tool, possible rerankers), but there is no cost/latency analysis and comparison with baseline methods. Therefore it is not clear how much performance gain comes from the agentic framework design instead of the use of more test-time compute.\n\n3. The bias handling module lacks novelty. The so-called “bias detector” is just a frozen LLM used as a tool without any new learning or calibration mechanism. The evaluation focuses narrowly on exposure/popularity bias and does not cover other key aspects in RecSys such as fairness, calibration, or long-tail coverage. Relying on a frozen LLM for bias detection also introduces its own biases and subjectivity."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8cGbxsRM3r", "forum": "s5riWPseUm", "replyto": "s5riWPseUm", "signatures": ["ICLR.cc/2026/Conference/Submission24833/Reviewer_uTkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24833/Reviewer_uTkx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990362786, "cdate": 1761990362786, "tmdate": 1762943212480, "mdate": 1762943212480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}