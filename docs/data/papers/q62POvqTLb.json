{"id": "q62POvqTLb", "number": 8602, "cdate": 1758092344077, "mdate": 1759897773846, "content": {"title": "Towards a Physics Foundation Model", "abstract": "Foundation models have revolutionized natural language processing through a \"train once, deploy anywhere\" paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative---democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.", "tldr": "We present GPhyT, a transformer trained on 1.8TB of diverse physics simulations that can zero-shot generalize to entirely new physical systems by inferring governing dynamics from context alone.", "keywords": ["Physics Foundation Model", "Multi-physics Learning", "In-context Learning", "Zero-shot Generalization", "Scientific Machine Learning", "Physics-Aware Machine Learning", "Spatiotemporal Transformers"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22e9bc9a32204ae402bf73ac5e32ae77e5d3e40a.pdf", "supplementary_material": "/attachment/bb7557fc3f8da9f1e6e373f392add528e405a51d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new foundation model for 2D physical emulation. The core architecture uses full space-timeattention with space-time patching and absolute position embeddings with fixed size inputs and augments the input data with finite-difference approximations of the spatial and temporal derivatives. The authors evaluate this model on a mixture of public and newly contributed data and find it outperforms several standard baselines on one-step prediction for data it was trained on. The authors also evaluate their model in zero-shot settings and longer rollout settings."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This is a really well presented and written paper and dataset contributions are extremely valuable. Strengths included:\n1. The paper is well written and engaging. Figures are easy to read and convey the appropriate information to support the text. \n2. The motivation is clear. The problem and some previous attempts at addressing them are laid out as solid motivation.\n3. The dataset contributions are excellent. They highlight a weakness of a public dataset and provide well-reasoned data to fill in that gap along with details of the solvers used to produce it."}, "weaknesses": {"value": "While the paper shows significant promise and enginuity, the current draft requires heavy re-writes and additional experiments. The paper makes strong claims, but often provides no supporting evidence. There appears to be the core of a strong paper here, but the submission isn't at a point I can recommend acceptance right now. \n\nCritical\n\n1. The critical issue right now is that the claims and experimental evidence are far apart. The most prominent claim in the submission is excellent zero shot performance and that the model has achieved a “train once, deploy anywhere” state for the first time, but there's next to no evidence for this. Section 4.2 is just listing results. There's no indication that these are good result, bad results, or whether other models in the space are unable to achieve these. This is similarly true in 4.3. Bluntly, with no baselines, 1-step RMSE provides no information, especially given this model is predicting the difference between steps so low error can often be achieved simply by predicting the input or a smoothed variant of the input. Some recommended baselines apart from the ones already cited like MPP that have gotten wide adoption and have been frequently used in benchmarking are Poseidon [1] and DPOT [2], though VICON [3] is probably the most similar to your work in terms of claiming strong in-context performance and OmniArch [4] also claims strong zero-shot performance. I'd strongly suggest at least running Poseidon and DPOT as baselines since they're relatively inexpensive and including dedicated or finetuned models in the comparison for 4.2 so there is evidence indicating that the zero-shot performance is non-trivial if that's a claim you want to make. \n2. Similarly, the text claims that the model shows excellent scaling and seems to rely on metric cherry picking. In table 2, while GPhyT seems to improve on the median loss, it seems like it's performance is quite unreliable given the significant mean-median gap and that this issue does not seem to be impacted by scaling since gains from 112M to 798M are very tiny relative to the significant increase in cost and seems to have similar mean performance to the UNet model. The mean-median gap indicates a problem in the model, but from a paper perspective this can be addressed by analysis. Are there particular reasons the model struggles with parts of the data? What does this tell us about the approach. \n3. The non-data contributions are largely either already commonly and those that are novel are not validated. Most models in this space use per-dataset normalization with a few using per-sample normalization. The variable time striding is used by both VICON and Poseidon. Using a numerical integrator would be novel, but the code appears to just be using one step of forward Euler which is the same as just predicting a difference term which is something already done by many neural surrogates. However, the neural differentiator and the use of space-time tubelet tokens appear to be novel within the realm of foundation models for PDEs. It would greatly strengthen the paper if the contributions were validated in ablations and tied to advantages GPhyT may have when compared to baselines. This doesn't have to be done at full scale to make the point. \n\nOverall, my recommendation here is figure out what problem GPhyT is solving that previous models were unable to address. Emphasize the aspects where your model beats the competitors - it's OK if the submission isn't beating them across the board, but there needs to be clear evidence that your contributions are leading to improvement on some problem. If once there are broader baselines, it does appear that this advantage is zero-shot in-context learning, then show that given the same training data, these prior models are underperforming yours. If it's that the forward Euler integration is improving stability, then show both that it's had better long term loss than baselines and a version of your model without this change. \n\nMajor\n\n1. The use of absolute metrics across datasets makes it significantly harder to interpret. Absolute metrics, particularly when averaged across multiple fields or explored over multiple datasets, tend to favor the the largest magnitude fields or at least the fields with largest variation. MSE is very informative when comparing short-horizon predictions for one variable of interest in one dataset, but it's mostly meaningless in large cross-dataset comparisons. This can be addressed with robust baselines, but it is still very typical to use relative error or normalized metrics so that all fields are put on roughly similar scales.\n2. Once the baselines are addressed, it still feels that most of 4.2 is quite trivial. This is one step prediction for a model that predicts the difference between two states, so for slowly changing fields, even predicting zero is going to produce a reasonable one-step forecast and given the lack of baselines here, it hasn't been shown that the model is even outperforming the persistence prediction. To make the claim of train once, deploy anywhere it is important to show at least some version of the real problem which requires at least short autoregressive predictions. \n3. There's very little engagement with the limitations in the main text. Fixed coarse resolution in 2D is a large limitation and it's important for this to be up front and clear to the reader rather than mentioned only in the appendix.\n\nMinor:\n1. 2D Euler equations are not generally thought of as chaotic, though proving this is an open question. Either way, I'd recommend avoiding that claim. They don't have a viscous cutoff so they're interesting from the perspective of never being fully resolved no matter how much resolution you throw at them, but the Euler quadrants problem is specifically used in benchmarking Riemann solvers because the behavior can be very well classified. [5] \n2. Similar to Major (2), while the comments are clear, when showing a field evolution, readers are going to expect the model predictions to be autoregressive rollouts which makes the figures in Appendix 6.7 misleading. The comments are clear enough that I consider this minor, but I'd recommend only using the sequential format for true multi-step prediction. \n\n[1] https://arxiv.org/abs/2405.19101 \n[2] https://arxiv.org/html/2403.03542v1\n[3] https://arxiv.org/pdf/2411.16063v2\n[4] https://arxiv.org/pdf/2402.16014\n[5] https://epubs.siam.org/doi/10.1137/S1064827595291819"}, "questions": {"value": "1. What issues in the space of multiple physics models has this approach successfully addressed and what evidence shows this?\n2. How does the model perform when compared to competitive baselines like DPOT and Poseidon?\n3. How do the specific contributions of the paper lead to improvement on some element of the problem. \n4. How do long-run predictions compare to other models and simple baselines like persistence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AeCAVGw43I", "forum": "q62POvqTLb", "replyto": "q62POvqTLb", "signatures": ["ICLR.cc/2026/Conference/Submission8602/Reviewer_vnz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8602/Reviewer_vnz8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700013106, "cdate": 1761700013106, "tmdate": 1762920445880, "mdate": 1762920445880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a transformer model for spatiotemporal prediction of fluid systems. After being trained on several different datasets, the model outperforms FNO and UNet models at predicting the next frame in seen systems. GPhyT is also capable of performing in-context learning, where the model is evaluated zero-shot on unseen systems with novel physics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The evaluations (at least those provided in the appendix) are detailed. \n\nThe ablation for the direct next-step prediction in appendix 6.4 is nice and addresses a question that I would have asked, given that many other models do this. It is interesting to see that the separation of differentiation and integration provides a substantial improvement on predictive performance. \n\nIt is encouraging to see that GPhyT at 9M parameters outperforms 100M+ parameter UNet and FNO; this indicates that the GPhyT architecture is quite well suited for this task."}, "weaknesses": {"value": "It would be really nice to see one of Tables 3, 4, or 5 in the main text, to get a better understanding of the performance of all the models for each of the different datasets, rather than just an average. \n\nI would like to see stronger baselines, and in particular ones which are designed for multiple physics like McCabe (2024), which is mentioned a few times in this paper as a relevant work."}, "questions": {"value": "There don't seem to be any baselines for the zero-shot prediction task or for the rollout stability analysis. Do the authors have any information about how well FNO and UNet do at zero-shot prediction of new physical systems? Ideally I would also like to see baselines trained for multiple physics included in this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RdIit7tL8Y", "forum": "q62POvqTLb", "replyto": "q62POvqTLb", "signatures": ["ICLR.cc/2026/Conference/Submission8602/Reviewer_xBLV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8602/Reviewer_xBLV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885311656, "cdate": 1761885311656, "tmdate": 1762920445535, "mdate": 1762920445535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the General Physics Transformer (GPhyT) , a large-scale transformer model trained on a diverse 1.8 TB dataset of physics simulations. The authors' goal is to create a \"Physics Foundation Model\" (PFM) that, analogous to LLMs, follows a \"train once, deploy anywhere\" paradigm. The core idea is that a single model can learn to infer the governing physical dynamics (e.g., fluid-solid interaction, shock waves, convection) from a \"prompt\" of prior states, enabling in-context learning. The paper presents three key results: (1) superior performance against specialized architectures on seen physics; (2) zero-shot generalization to unseen boundary conditions and entirely new physical systems ; and (3) stable autoregressive rollouts for 50 timesteps."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. A Compelling New Paradigm: The primary strength is the successful application of the in-context learning paradigm to physics simulation. The idea that a model can infer dynamics from a prompt of prior states , rather than being explicitly told the equations, is a significant shift from specialized solvers like PINNs or most Neural Operators.\n2. Strong Generalization Results: The paper's most convincing result is the zero-shot generalization to unseen boundary conditions (Table 2). The model achieves nearly identical MSE on \"open\" boundary variants as it does on \"known\" periodic/symmetric ones , demonstrating true generalization. The qualitative generalization to entirely new physics (Fig 4), while less accurate, is a powerful proof of concept.\n3. Large-Scale Data Curation: The 1.8 TB dataset is a significant contribution. More importantly, the data augmentation strategies—specifically variable time incrementsand per-dataset normalization—are intelligently designed to force the model to learn in-context inference rather than memorize specific scales or dynamics.\n4. Thoroughness and Transparency: The paper is rigorous, with detailed ablations on prompt size and integrator schemes . The authors are also very honest about the significant limitations , which grounds the paper's ambitious claims in reality."}, "weaknesses": {"value": "1. Weak Baseline Comparisons: The \"up to 29x\" performance gain is against a standard FNO (from 2020) and a standard UNet. These architectures were not designed for the multi-physics, in-context inference task this paper proposes. A stronger comparison would involve more recent, advanced operator architectures. This overstates the \"breakthrough\" on known physics (Q1).\n2. Major Unsolved Limitations: The authors correctly identify the key limitations, but they are severe. The model is 2D onlyand, most critically, fixed-resolution. This is a significant step backward from the Neural Operator paradigm, which is built on discretization-invariance. A true PFM must be able to handle variable resolutions and 3D systems.\n3. Long-Term Stability is Still Low: While 50-timestep rollouts are demonstrated (Q3) , the error accumulation is non-trivial (Fig 5). The authors admit this \"falls considerably short of the precision exhibited by numerical solvers\" and is not ready for \"practical engineering applications\". This remains a key barrier.\n4. \"New Physics\" Generalization is Only Qualitative: The generalization to unseen physics (supersonic flow, turbulent layers) is visually impressive but shows very high quantitative error (Table 2). This is an exciting proof-of-concept for \"plausibility\" but not yet a robust, accurate generalization.\n5. Reliance on Explicit Derivatives: The model is given numerically computed first-order spatial and temporal derivatives (dx, dy, dt) as input channels . This is a strong inductive bias that likely helps significantly. It slightly undermines the narrative of learning from \"data alone\"  and feels more like a hybrid \"physics-informed\" input feature than a pure in-context inference from raw states."}, "questions": {"value": "1. The baseline comparison (Q1) uses FNO and UNet. Could the authors comment on why these were chosen over more modern neural operators? Do they believe GPhyT would maintain its performance gap against architectures specifically designed for greater generalization?\n2. The model is fixed-resolution , which is a major limitation compared to neural operators. What are the authors' thoughts on achieving resolution-invariance? Does the \"tubelet\" patching approach  fundamentally prevent this?\n3. How critical is the inclusion of explicit derivatives (dx, dy, dt) in the input ? Have the authors ablated this feature? How much does performance degrade if the model only receives the raw state fields (pressure, velocity, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9AFDsKMIS", "forum": "q62POvqTLb", "replyto": "q62POvqTLb", "signatures": ["ICLR.cc/2026/Conference/Submission8602/Reviewer_PPrL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8602/Reviewer_PPrL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998235582, "cdate": 1761998235582, "tmdate": 1762920445165, "mdate": 1762920445165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}