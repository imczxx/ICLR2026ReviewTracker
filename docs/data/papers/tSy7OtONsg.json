{"id": "tSy7OtONsg", "number": 23613, "cdate": 1758346349501, "mdate": 1759896804602, "content": {"title": "Moving Beyond Medical Exams: A Clinician-Annotated Fairness Dataset of Real-World Tasks and Ambiguity in Mental Healthcare", "abstract": "Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. \nIn psychiatry especially, these challenges are worsened by fairness and bias issues, since models can be swayed by patient demographics even when those factors should not influence clinical decisions. \nThus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. \nThis dataset — created without any LM assistance — is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. \nAlmost all base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables, e.g., for age or ethnicity, and are available for male, female, or non-binary-coded patients. \nThis design enables systematic evaluations of model performance and bias by studying how demographic factors affect decision-making. \nFor question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations.\nWe outline a series of intended use cases and demonstrate the usability of our dataset by evaluating eleven off-the-shelf and four mental health fine-tuned LMs on category-specific task accuracy, on the fairness impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human-annotated samples.", "tldr": "", "keywords": ["AI for Healthcare", "mental health", "fairness", "bias", "dataset", "language models", "decision-making", "uncertainty", "expert annotation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b1e63397c92f65cb00901b097786dcb44372582.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel evaluation MENTAT designed to assess the performance on LMs for psychiatry. The authors introduce five dimensions \"diagnosis\", \"treatment\", \"triage\", \"monitoring\", and \"documentation\" to cover a more realistic range of challenges faced by psychiatrists in real-world clinical practice. The dataset is annotated by experts with a sound disagreement resolution method. This led to a dataset mainly used a multiple choice question dataset. The paper then evaluates the performance of a variety of models and unsurprisingly demonstrate the state-of-the-art models achieve impressive the best results. The five dimensions show discrepancy in terms of capabilities with every model achieving low scores on documentation performance while SOTA models achieve near perfect scores on treatment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper fills a gap in current evaluations by addressing psychiatry and especially given the 5 dimensions. The involvements of clinical experts strengthens the relevance and reliability of the evaluation compared to common approaches which are sourced from old exams or synthetic data generated by LLMs.\n\nAs said in the summary, the disagreement resolution method is solid and well motivated with a sufficient number of annotations.\n\nThe results are clearly communicated with appropriate statistical analysis.\n\nIntroducing variations in age, gender, and ethnicity is a welcome addition with a clear discussion of the impact and reinforces previously identified biases in LLMs."}, "weaknesses": {"value": "The framing of the evaluation in regards to related work is ambivalent. On the one hand the authors make the case that current board style methods are insufficient but they then proceed to build a board style exam using domain specific cases. The limitations of MCQs especially for LLMs are not addressed appropriately in the introduction/related work.\n\nThe experiment tests small models and SOTA models but do not include mid-sized models or specialized medical models that have demonstrated measurable performance increase over the base models, for example PMC Llama was not instruct tuned where as other models like GPT-4o are instruction tuned. In addition, the absence of a human baseline makes interpretation of the results difficult. For USMLE based benchmarks for example we know that the passing score is around 60% and that students score around 70%.\n\nPresentation could be improved, Figure 3 especially is difficult to read due to the usage of colors that are too similar. \n\nAs the authors explicitly address, this benchmark is limited by being US centric and having a relatively small number of samples. However, the authors appropriately justify that evaluations specific to local practices is important for clinical relevance and that curating evaluations with experts is a difficult and expensive process. While I agree with both arguments, my concern is with the choice of venue, it appears to me that the dataset's primary goal is to be clinically informative for US clinical settings. I wonder if a medical journal such as JAMA Psychiatry would be more appropriate and find the correct audience."}, "questions": {"value": "# Major concers\n1) There seems to be a contradiction in the introduction, board-style exams are said to be limited because they have only one correct answer but then the MENTAT dataset is transformed into board-style questions with a MCQ format with 5 options with only one correct. Can the author clarify their position on the place of MCQs? Maybe also include a discussion of different techniques such as LLM-as-a-judge and key features.\n\n2) The related work section could be expanded to include previous work on biases found in models [1] as well as the limitations of the multiple choice method for evaluating LMs which the paper also describes when comparing the performance on open-ended questions [2].\n\n3) There is a major gap in size and performance between the open models and the closed models evaluated. Most open models included are either outdated (PMC Llama) or small. I would have liked to see high performing open models like Qwen3, Gemma3 in different sizes (from small 3b to larger 200b+ models) and specialized models like MedGemma3, Internist.ai, Meditron3.\n\n4) While the dataset is annotated by experts, a baseline score for 1 or 2 psych residents/attendings would allow readers to better understand what a score of 50% on documentation represents. Are the models failing on this task compared to humans? Or would humans get similar results?\n\n# Minor suggestions\n\n1) Line 168: \"are practitioners and M.D.s\", could you detail the distribution? Including seniority level (residents vs attendings, MD/APP/NP)\n2) The 95% CI is included in every graph but I could not find the exact calculation used to compute them. Could you clarify?\n3) Figure 3 and Tables 1 and 2 arrive a bit too early in the document giving it a less polish feel and impeding comprehension, I suggest moving them down a page or two.\n4) In Figure 3, the color code and shapes are difficult to differentiate in the figure, for instance I am unable to tell if pmc_llama is better than mmeds in the figure because I can't tell one from the other. This issue is present for multiple pairs (o1/o1-mini for example). \n\n# References:\n[1] Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study (Zack et al. The Lancet Digital Health 2024)\n\n[2] Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine (Griot et al., ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FBXqwyELKi", "forum": "tSy7OtONsg", "replyto": "tSy7OtONsg", "signatures": ["ICLR.cc/2026/Conference/Submission23613/Reviewer_snwZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23613/Reviewer_snwZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761338452291, "cdate": 1761338452291, "tmdate": 1762942735486, "mdate": 1762942735486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper pioneers a new evaluation dataset for LLMs in mental health care: MENTAT. The authors curate a fully human written and annotated set of questions with multiple choice answers across five categories in mental health. Novelly, answer choices are not strictly correct and incorrect. Annotator preference is used as a proxy for simulating multiple correct answers and evaluated against. The authors present results of state-of-the-art LLMs on their dataset demonstrating specific, actionable performance gaps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tA fully human generated dataset is an exciting contribution.\n-\tThe dataset’s multiple category framework provides an informative lens into which aspects of mental health are falling behind in current LLMs, which is very informative and actionable. Models perform less well in triage and documentation domains, which is an important insight.\n-\tComparison of questions across demographic variables is clever, informative, and comprehensive.\n-\tThe inclusion of preference annotations and multiple correct answers is nice technique that is likely applicable in LLM evaluation in medicine more broadly."}, "weaknesses": {"value": "-\tThe description of the related work in the mental health space is confusing. It would be helpful to contrast which of these studies introduce benchmarks and to state an overall understanding of the state of LLM performance on mental health tasks. What are the generally accepted specific gaps or are they unknown?\n-\tAppendix plots of model performance across demographic variables do not suggest significance for the differences across them within groups. This is confusing given the claims in the text of better performance in some groups. It is also confusing how narrow the confidence intervals are in Table 1, 2 compared to in the Figures (ie 3, 19).\n-\tThe overlap of methods and results in text in section 3.2 makes the section dense and challenging to read. It remains unclear whether evaluation occurs against modeled probabilities or raw annotation preferences.\n-\tIt would be worth further explanation in the text of why comparisons with the Shrivastava et al BERTScore metric for comparing the free form text is valid as this is a core suggested contribution."}, "questions": {"value": "-\tOn language, how is MENTAT not a benchmark? I see how the dataset is a novel benchmark but not the contrast between “evaluation dataset” and benchmark.\n-\tHow are questions written by experts? Do experts create cases from real patient cases, study texts, or are scenarios entirely contrived?\n-\tHow did you get 11.5 annotations per question with 8 annotators? \n-\tWhich accuracy comparisons are computed using one of the Bradley Terry models versus the raw preference probabilities or annotation scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rUZKyZ93lc", "forum": "tSy7OtONsg", "replyto": "tSy7OtONsg", "signatures": ["ICLR.cc/2026/Conference/Submission23613/Reviewer_3LdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23613/Reviewer_3LdC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498492051, "cdate": 1761498492051, "tmdate": 1762942734940, "mdate": 1762942734940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MENTAT, a clinician-authored evaluation dataset for psychiatric decision-making that moves beyond exam-style QA. It covers five domains (diagnosis, treatment, monitoring, triage, and documentation) with 203 base vignettes expanded via demographic variables (age, gender coding, ethnicity) to probe fairness. Ambiguous categories (triage, documentation) receive expert preference labels derived with a hierarchical Bradley-Terry (HBT) model. The authors evaluate 11 general LMs and several health-tuned LMs; models score high on diagnosis/treatment, but struggle on triage/documentation, and show sensitivity to demographic attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clinician-first design with verification and a thoughtful ambiguity pipeline that avoids LM-generated contamination during creation.\n\n2. Methodological clarity: transparent HBT formulation (annotator effects), bootstrap CIs, and explicit prompt templates.\n\n3. Writing is clear and is generally easy to follow."}, "weaknesses": {"value": "1. Post-hoc zeroing of 'objectively wrong' answers (then renormalizing) encodes expert priors that bypass the HBT inference. The paper states most zeroed options had p ≤ 0.2 anyway, but no sensitivity analysis quantifies how many questions' top-ranked answer changes under alternative thresholds or no clamping. This risks conflating empirical disagreement with ex-post correctness.\n\n\n2. While Section C justifies jurisdiction-specificity, the abstract/intro should state this limitation upfront."}, "questions": {"value": "1. 203 items may suffice for ranking models but are too few for fine-tuning or error analysis by symptom subtype. Did the authors consider releasing raw clinician rationales (sanitized) to enable richer qualitative studies??\n\n2. Could you include ablations for answer-order randomization and few-shot vs zero-shot?\n\n3. Beyond average accuracy, did you analyze error types by demographic condition? That would better ground fairness risks.\n\n4. The paper states \"diverse clinicians\" but doesn't report demographics of the 9 annotators themselves. Could annotator bias (e.g., age, geography, years in practice) confound the HBT parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aSZnBcrwdB", "forum": "tSy7OtONsg", "replyto": "tSy7OtONsg", "signatures": ["ICLR.cc/2026/Conference/Submission23613/Reviewer_W1d3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23613/Reviewer_W1d3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762562068183, "cdate": 1762562068183, "tmdate": 1762942733908, "mdate": 1762942733908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MENTAT, a clinician-authored dataset and evaluation pipeline for psychiatric decision-making that aims to reflect routine practice rather than exam-style fact recall. The dataset targets five domains: diagnosis, treatment, monitoring, triage, and documentation, and it is designed to study ambiguity and disagreement that arise in real care. The authors collect expert ratings and convert them into preference probabilities through a hierarchical Bradley-Terry model, which provides soft labels for ambiguous tasks. They evaluate eleven general-purpose instruction-tuned models and four mental-health-focused models across multiple-choice and free-form settings, with dedicated analyses on performance shifts caused by patient age, gender coding, and ethnicity. The results show strong performance on diagnosis and treatment, weaker results on triage and documentation, and measurable fairness gaps linked to demographic variables."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The use of multiple clinicians on the same items would be valuable to show inter-rater agreement through statistics such as kappa or intra-class correlation. This would document label consistency, separate real clinical ambiguity from noise, and build confidence in the reliability of the resource. This shows label consistency, highlights true clinical ambiguity, and increases confidence in the dataset’s reliability.\n\n2. The focus on everyday, ambiguous decision-making in psychiatry is a clear strength. MENTAT moves away from board-style recall and targets tasks that practicing psychiatrists actually perform, including triage, documentation, and longitudinal monitoring. \n\n3. The analysis of demographic sensitivity across gender coding, age, and ethnicity is careful and informative. By constructing DG, DA, and DN variants, the authors show statistically significant accuracy differences, for example higher accuracy for non-binary-coded patients than for female-coded patients, lower accuracy for items with Asian or African American ethnicity than for other groups, and higher accuracy for the youngest age band. This investigation of fairness in clinical AI is highly relevant."}, "weaknesses": {"value": "1. A key weakness is the free-form evaluation relies on a single semantic similarity signal. The study measures inconsistency as one minus BERTScore using a DeBERTa xlarge model fine-tuned on MNLI, then aggregates with bootstrap resampling. This narrow view can miss clinically reasonable phrasings and may not reflect calibration. Reporting multiple signals, such as ROUGE or BLEU, would give a more stable picture of free-form behavior.\n\n2. The core dataset is small for the breadth of claims. The paper states there are 203 base questions across the five categories, and scale-ups for fairness come from demographic templating rather than new clinical scenarios. This limits statistical power for fine-grained comparisons and constrains analysis of rare but important edge cases.\n\n3. The paper would benefit from short qualitative examples of free-form outputs alongside the preference-based labels. Concrete examples of model answers that are close to or far from expert choices would help readers understand error modes in triage, documentation, and treatment adjustments."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vkf8iSnWtC", "forum": "tSy7OtONsg", "replyto": "tSy7OtONsg", "signatures": ["ICLR.cc/2026/Conference/Submission23613/Reviewer_zwDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23613/Reviewer_zwDT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762764373854, "cdate": 1762764373854, "tmdate": 1762942733719, "mdate": 1762942733719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}