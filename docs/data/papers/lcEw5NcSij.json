{"id": "lcEw5NcSij", "number": 18569, "cdate": 1758289173606, "mdate": 1759897095369, "content": {"title": "A Schrödinger Eigenfunction Method for Long-Horizon Stochastic Optimal Control", "abstract": "High-dimensional stochastic optimal control (SOC) becomes harder with longer planning horizons: existing methods scale linearly in the horizon $T$, with performance often deteriorating exponentially. We overcome these limitations for a subclass of linearly-solvable SOC problems—those whose uncontrolled drift is the gradient of a potential. In this setting, the Hamilton-Jacobi-Bellman equation reduces to a linear PDE governed by an operator $\\mathcal{L}$. We prove that, under the gradient drift assumption, $\\mathcal{L}$ is unitarily equivalent to a Schrödinger operator $\\mathcal{S} = -\\Delta + \\mathcal{V}$ with purely discrete spectrum, allowing the long-horizon control to be efficiently described via the eigensystem of $\\mathcal{L}$. This connection provides two key results: first, for a symmetric linear-quadratic regulator (LQR), $\\mathcal{S}$ matches the Hamiltonian of a quantum harmonic oscillator, whose closed-form eigensystem yields an analytic solution to the symmetric LQR with arbitrary terminal cost. Second, in a more general setting, we learn the eigensystem of $\\mathcal{L}$ using neural networks. We identify implicit reweighting issues with existing eigenfunction learning losses that degrade performance in control tasks, and propose a novel loss function to mitigate this. We evaluate our method on several long-horizon benchmarks, achieving an order-of-magnitude improvement in control accuracy compared to state-of-the-art methods, while reducing memory usage and runtime complexity from $\\mathcal{O}(Td)$ to $\\mathcal{O}(d)$.", "tldr": "", "keywords": ["Stochastic Optimal Control", "Schrödinger operator", "eigenfunction learning", "long-horizon control"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd05d5476905cf77b4c983020da697c5f837e9ef.pdf", "supplementary_material": "/attachment/f651960bc19844ae3844e9346811fd088e5b16d4.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the numerical resolution of continuous control PDEs (a.k.a. HJB equations) in high-dimensional state spaces using neural networks (as classical PDE solvers are prohibited by the curse of dimensionality). Contrary to most works, which focus on the computational impact of the number of samples of the trajectories, here the concern is the computational impact of the length of the trajectories. In this undiscounted, finite, but large horizon problem, the authors propose expanding the evolution operator into its eigenfunctions and keeping only the dominant eigenfunction as a numerical solution. This raises three sub-problems: 1) identifying conditions under which this approach is principled, 2) identifying or approximating the principal eigenfunction, 3) recovering an effective control from the principal eigenfunction. \n\nThe authors tackle 1) by working within a classical, moderate-complexity setting of affine control, whose (thus simplified) evolution operator can be related through a classical change of function to a linear PDE and associated Schrödinger operator. These theoretical derivations follow classical methodologies in control and PDE theory, but their highly qualitative and rigourous application yields novel results, and in particular Theorem 4. They also naturally yield an expression of the optimal control in terms of the eigenbasis (Theorem 3), which is truncated to provide a solution to sub-problem 3). This analytical method is the main body of the work, an architecture, if you will, which supports the use of neural network methods for learning eigenfunctions to solve sub-problem 2). The authors provide an extensive overview of existing methods for learning eigenfunctions, but also propose their own loss, which is expressed in units corresponding to the control problem (a reweighting which counteracts the change of functions of step 1))."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well-written, well-structured, and highly pedagogical, despite the difficulty of the topic and the distance between the communities of PDE theory and general ML. It is also very complete both in terms of the references in control and PDE theories and in terms of the rigour of the statements and argumentation. The theoretical contributions are impressive and support a novel multi-disciplinary approach fusing control/PDEs with ML, while being backed up by very rigourous mathematics and good experiments."}, "weaknesses": {"value": "The assumptions required to obtain the theoretical results are more general than, say, linear quadratic, but they remain a far cry from many practical situations. This is highlighted in the experiments, which are restricted to toy problems. However, in my opinion, while it is a weakness, it is difficult to criticise the quality of the work on these grounds in good faith, as the step up in complexity required by weaker assumptions would be so vast.\n\n_Minor feedbacks_:\n- The later appendices (C and beyond) contain a few typos and overloaded notations; there are also some less-than-correct phrases in the main text.\n- In equation (104), shouldn’t the integral over $t$ be a classical integral and not an expectation?"}, "questions": {"value": "There is one important question that, for me, casts a shadow over the phenomenon at play here, which I think ought to be answered: \n- If the horizon is becoming longer and longer, one would expect turnpike properties to kick in and the ergodic regime to become a good approximation for the behaviour of the system. This connection is suggested in Remarks 1 and 2. To what degree is this actually responsible for the improvement in computational performance? Wouldn’t a good benchmark for Figure $1$ be a solver which solves the ergodic version of the problem (though you would have to change the problem to one of the other LQRs for this to exist), which one would expect to improve with $T$ just by the law of large numbers? \n\n\nThis work has made me wonder about several research questions, which is something I like about it, but these wouldn’t change my evaluation of the work. I leave them without expecting an answer for the authors’ benefit: \n- You use only the first eigenvalue here, but you could consider higher-order approximations, which might improve performance. This performance would be inversely exponential in the spectral gap, of course, so it would be an interesting question to study the gaps in the spectra of different evolution operators to identify problems where different orders of approximation are interesting. \n- If you could study the full HJB equation (beyond control-affine assumptions, I mean) and show that the evolution operator has a spectral gap and eigenbasis, you could apply the learning components as well, no? Perhaps using hypocoercivity theory and some non-linear PDE analysis, it would be possible to obtain similar results under very general assumptions, though it would be very complicated. \n- I understand the problem with end-of-the-world effects, which cause the need to switch off the eigenfunction solver near $T$, but the performance degradation is very painful (Figure 5). Perhaps something clever can be done about it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NUfe9OL88X", "forum": "lcEw5NcSij", "replyto": "lcEw5NcSij", "signatures": ["ICLR.cc/2026/Conference/Submission18569/Reviewer_Y3as"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18569/Reviewer_Y3as"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649309548, "cdate": 1761649309548, "tmdate": 1762928280846, "mdate": 1762928280846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a **hybrid framework** for solving **stochastic optimal control (SOC)** problems under the **gradient-drift assumption**, combining **eigenfunctions** with **short-horizon solvers**. Using the **Cole–Hopf transform**, the **Hamilton–Jacobi–Bellman (HJB)** equation is converted into a **linear PDE**, enabling efficient solution through **eigenvalue and spectral analysis**. The framework reparameterizes the value function to learn the **top eigenfunction**, which determines the long-horizon control. Empirically, the method achieves strong performance for **long time horizons**, where existing approaches typically suffer from rapid error and computational cost growth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Rigorous theoretical treatment with clear assumptions, theorems, and proofs.\n2. Empirical results demonstrate improved performance over baselines as the horizon T increases."}, "weaknesses": {"value": "1. The paper could better **motivate the relevance and generality** of the gradient-drift assumption, clarifying how broad and practically applicable the class of SOC problems is where the HJB can be reduced to a linear PDE.\n2. The claim that “the exponential decay of the correction term in Eq. (2) suggests that the optimal control ($u^*$) can be approximated by the gradient of the logarithm of the top eigenfunction” holds only **asymptotically**; the bounded correction term may remain **non-negligible** over ([0, T]), especially at earlier times.\n3. The modification of the loss from Eq. (15) to Eq. (20) is said to be equivalent when ($\\phi$) is unit-norm, but since the regularization coefficient ($\\alpha$) is **chosen heuristically**, there is **no guarantee** that both objectives share the same optimum or yield a normalized eigenfunction.\n4. The authors state that the loss in Eq. (21) remains sensitive when ($\\exp(-\\beta V_0)$) is small; however, this may also cause **instability**, as the loss could **diverge** when ($V_0$) becomes large."}, "questions": {"value": "1. How is the cutoff time ($T_{\\text{cut}}$) selected or tuned in practice?\n2. Does the learned eigenfunction ($\\phi_\\theta$) correspond to the true eigenfunction of the operator (L), or is it only an approximation under the chosen loss?\n3. Is the eigenvalue ($\\lambda_0$) learned during training (fine-tuning)? If so, is its computation **differentiable** and integrated into the optimization process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M6ZiZTtpX2", "forum": "lcEw5NcSij", "replyto": "lcEw5NcSij", "signatures": ["ICLR.cc/2026/Conference/Submission18569/Reviewer_KCj8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18569/Reviewer_KCj8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884425143, "cdate": 1761884425143, "tmdate": 1762928279191, "mdate": 1762928279191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Within the context of stochastic optimal control, this paper shows that optimal control can be obtained from the eigensystem of a Schrödinger operator S=−\\Delta + V with purely discrete spectrum. The paper also states that for a symmetric linear-quadratic regulator (LQR), the S matches the Hamiltonian of a quantum harmonic oscillator, whose closed-form eigensystem yields an analytic solution to the symmetric LQR with arbitrary terminal cost. For the general version, the paper presents a learning strategy of the eigensystem of the operator L via physics informed neural  networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- An interesting connection between a version of stochastic optimal control and the Schrodinger operator. \n- A deep learning strategy based on PINN and several loss functions to learn the eigensystem of the operator L."}, "weaknesses": {"value": "-  The presentation of the theoretical results is a bit unclear. For example, it is not clear what is the different between the presented Theorem 2 and the (Reed & Simon, 1978, Theorem XIII.67, XIII.47. Appendix B.5 announced to provide a proof of Theorem 2, but that is hardly the case, it is at best a sketch and a restatement of the results in (Reed & Simon, 1978, Theorem XIII.67, XIII.47.\n- In the numerical results, how is the check that the eigenvalues do not have a finite accumulation point? \n- The paper states that Instead of learning V_0 and \\lambda_0 jointly, it first performs training\nwith a variational loss in equation (17) to obtain an estimate for the eigenvalue \\lambda_0 and an initialization of V_0, and then ‘fine-tune’ using equation (20). Can the authors comment on the convergence rate? What affects the convergence as a function of dimensionality d?\n- Another issue is related to how effective is the proposed eigensystem learning approach when addressing the long-horizon challenge? Although the paper states under the limitations that method for determining an appropriate cutoff time exist, can computational approaches shade some light on the T_cut?\n- When discussing order-of-magnitude, what sustains the improvement in L^2 error compared to standard IDO and FBSDE methods? Some additional explanations could have help me understand also the computational complexity analysis.\n- Although this is a minor issue, when considering the cases without gradient drift (where L is non-symmetric), what are the major technical problems?"}, "questions": {"value": "-  The presentation of the theoretical results is a bit unclear. For example, it is not clear what is the different between the presented Theorem 2 and the (Reed & Simon, 1978, Theorem XIII.67, XIII.47. Appendix B.5 announced to provide a proof of Theorem 2, but that is hardly the case, it is at best a sketch and a restatement of the results in (Reed & Simon, 1978, Theorem XIII.67, XIII.47.\n- In the numerical results, how is the check that the eigenvalues do not have a finite accumulation point? \n- The paper states that Instead of learning V_0 and \\lambda_0 jointly, it first performs training\nwith a variational loss in equation (17) to obtain an estimate for the eigenvalue \\lambda_0 and an initialization of V_0, and then ‘fine-tune’ using equation (20). Can the authors comment on the convergence rate? What affects the convergence as a function of dimensionality d?\n- Another issue is related to how effective is the proposed eigensystem learning approach when addressing the long-horizon challenge? Although the paper states under the limitations that method for determining an appropriate cutoff time exist, can computational approaches shade some light on the T_cut?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None detected"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2WXpIgvH10", "forum": "lcEw5NcSij", "replyto": "lcEw5NcSij", "signatures": ["ICLR.cc/2026/Conference/Submission18569/Reviewer_AKir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18569/Reviewer_AKir"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762724666672, "cdate": 1762724666672, "tmdate": 1762928278220, "mdate": 1762928278220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}