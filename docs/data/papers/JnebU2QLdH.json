{"id": "JnebU2QLdH", "number": 10612, "cdate": 1758177410820, "mdate": 1763702804411, "content": {"title": "Pre-training LLM without Learning Rate Decay Enhances Supervised Fine-Tuning", "abstract": "We investigate the role of learning rate scheduling in the large-scale pre-training of large language models, focusing on its influence on downstream performance after supervised fine-tuning (SFT).\nDecay-based learning rate schedulers are widely used to minimize pre-training loss.\nHowever, despite their widespread use, how these schedulers affect performance after SFT remains underexplored.\nIn this paper, we examine Warmup-Stable-Only (WSO), which maintains a constant learning rate after warmup without any decay.\nThrough experiments with 1B and 8B parameter models, we show that WSO consistently outperforms decay-based schedulers in terms of performance after SFT, even though decay-based schedulers may exhibit better performance after pre-training.\nThe result also holds across different regimes with mid-training and over-training.\nLoss landscape analysis further reveals that decay-based schedulers lead models into sharper minima, whereas WSO preserves flatter minima that support adaptability.\nThese findings indicate that applying LR decay to improve pre-training metrics may compromise downstream adaptability.\nOur work also provides practical guidance for training and model release strategies, highlighting that pre-training models with WSO enhances their adaptability for downstream tasks.", "tldr": "", "keywords": ["Learning rate schedules", "Large language models (LLMs)"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12af4c69452470d6838d26f4067a83af67d47dbe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates learning rate scheduling in large language model pre-training, specifically examining how different schedulers affect downstream performance after supervised fine-tuning (SFT). The authors propose Warmup-Stable-Only (WSO), which maintains a constant learning rate after warmup without decay, and demonstrate through experiments on 1B and 8B parameter models that WSO consistently outperforms decay-based schedulers (WSD, Cosine, Linear) on post-SFT tasks, despite achieving worse pre-training metrics. The authors attribute this to WSO preserving flatter loss landscape minima that support better adaptability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors rigorously demonstrate that WSO outperforms traditional decays across multiple scales (1B and 8B), stages (pre-, mid-, and post-training), and regimes (standard, mid-training, over-training). The consistency of results is convincing — the performance inversion between pre-training and SFT is robust.\n2. Loss landscape analysis connects WSO’s success to flatter minima — a strong explanatory narrative consistent with sharpness-aware generalization theory (Foret et al., 2021; Wen et al., 2025). Figure 3’s curvature dynamics clearly support this interpretation.\n3. The exposition is systematic, well-cited, and transparent. Appendices include hyperparameters, datasets, and evaluation details."}, "weaknesses": {"value": "1. The explanation of “flatter minima = better adaptability” is qualitative. The paper would benefit from formalizing how curvature interacts with SFT gradient flow (e.g., via a transferability Jacobian or Hessian spectrum analysis across tasks). Without this, the claim remains an empirical observation.\n2. SFT evaluation focuses on AlpacaEval, TruthfulQA, and MMLU. These are instruction-following benchmarks but do not fully probe reasoning or alignment generalization. \n3. WSO maintains a higher effective learning rate longer — potentially increasing training instability or wasted compute in late phases. The authors should quantify total compute efficiency (e.g., perplexity vs wall-clock time) to assess tradeoffs."}, "questions": {"value": "1. You attribute WSO’s superior SFT performance to flatter minima (lower sharpness). Could you quantify how much this flatness contributes to downstream adaptability? For example, is there a measurable correlation coefficient between sharpness values and SFT task scores?\n2. Have you examined whether the flatter minima correspond to wider basins of equivalent loss or simply slower convergence zones? This distinction matters for transfer dynamics.\n3. How sensitive are your conclusions to the warmup length? Since WSO keeps the LR constant after warmup, a longer warmup could emulate partial decay.\n4. The study is based on Llama-like architectures. Do you expect the same effect for mixture-of-experts (MoE) or sparse transformer setups where parameter utilization patterns differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7LUAgCCnye", "forum": "JnebU2QLdH", "replyto": "JnebU2QLdH", "signatures": ["ICLR.cc/2026/Conference/Submission10612/Reviewer_68iN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10612/Reviewer_68iN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811373614, "cdate": 1760811373614, "tmdate": 1762921874944, "mdate": 1762921874944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of learning rate (LR) scheduling during LLM pre-training on downstream supervised fine-tuning (SFT) performance. The authors challenge the standard practice of using decay-based schedulers (like Cosine or WSD) which are optimized for pre-training loss. The paper introduces \"Warmup-Stable-Only\" (WSO), a simple scheduler that maintains a constant LR after warmup without any decay. Through comprehensive experiments on 1B and 8B models, the authors demonstrate a consistent \"inversion\": while decay-based schedulers achieve better pre-training metrics, models trained with WSO consistently achieve superior performance after SFT. This finding is shown to be robust across standard pre-training, mid-training, and over-training regimes. The paper provides a mechanistic explanation, analyzing the loss landscape and showing that WSO guides models to flatter minima, which enhances adaptability, whereas decay-based schedulers converge to sharper minima that may compromise downstream performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is exceptionally clear, well-written, and easy to follow.\n\nThe central conclusion—that pre-training without LR decay enhances SFT performance—is simple, impactful, and supported by extensive evidence. The experiments are comprehensive, covering multiple model scales (1B and 8B), different training pipelines (two-stage and three-stage with mid-training), and modern training regimes (over-training).\n\nThis work has significant practical implications for the industry. The WSO scheduler is simple to implement and could provide real economic benefits by producing base models that are more adaptable and performant for downstream tasks.\n\nThe mechanistic explanation provided via loss landscape sharpness is insightful. The analysis linking the constant LR of WSO to flatter minima, and in turn, to better adaptability, offers a compelling hypothesis for *why* WSO outperforms decay-based methods in the post-SFT stage."}, "weaknesses": {"value": "The primary weakness, though minor, is that the investigation of downstream performance is limited to SFT. The paper does not explore other critical post-training stages, such as preference tuning (e.g., DPO) or reinforcement learning-based alignment. It remains an open question whether the significant benefits of WSO pre-training persist or behave differently in these other alignment scenarios. I don't think this would be an issue as the title also constrains the scope to SFT."}, "questions": {"value": "The paper compellingly argues that WSO leads to flatter minima (lower sharpness) and that WSO models perform better on SFT. The link is made by showing these two facts separately. To make the justification more persuasive, have the authors considered plotting a direct correlation between the measured sharpness of the pre-trained checkpoints and their final SFT benchmark scores? This would provide a more direct piece of evidence that sharpness is indeed the key indicator for downstream adaptability as hypothesized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jdeKgAcTfv", "forum": "JnebU2QLdH", "replyto": "JnebU2QLdH", "signatures": ["ICLR.cc/2026/Conference/Submission10612/Reviewer_NMCt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10612/Reviewer_NMCt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665265256, "cdate": 1761665265256, "tmdate": 1762921874519, "mdate": 1762921874519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate the role of learning rate scheduling in the large-scale pre-training of large language models, focusing on its influence on downstream performance after supervised fine-tuning (SFT). Specifically, this paper proposes Warmup-Stable-Only (WSO) learning rate schedule for pertaining, which is found to achieve better downstream tasks. Some experiments and intuitive understandings are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper presents WSO, a very simple and intuitive LR schedule to improve SFT on downstream performance.\n2. This paper provides a practical reference for LLM pre-training community to design LR schedule from a global training perspective."}, "weaknesses": {"value": "The primary concern with this paper is that the proposed approach—while effective—has been extensively discussed, implemented, and validated in prior work, without introducing significant novelty. Furthermore, the absence of references to these existing studies raises questions about the thoroughness of the literature review.\n\n1. In the original WSD paper [(https://arxiv.org/pdf/2404.06395)](https://arxiv.org/pdf/2404.06395), the authors already demonstrated the benefits of switching to high-quality datasets (including SFT data) during the learning rate decay phase, yielding intuitive and positive outcomes.\n\n2. The paper \"Scaling Law with Learning Rate Annealing\" [(https://arxiv.org/pdf/2408.11029)](https://arxiv.org/pdf/2408.11029) introduces a scaling law describing loss dynamics in relation to learning rates, of which the current work appears to be a specific instance.\n\n3. The paper \"Learning Dynamics in Continual Pre-Training for Large Language Models\" [(https://arxiv.org/pdf/2505.07796)](https://arxiv.org/pdf/2505.07796) provides comprehensive analyses, and the findings here seem to represent only a minor subset of their paper. Notably, their Finding 3 states: \"*PT models with higher loss potential consistently achieve lower D_cpt validation losses. Hence, we advocate that when releasing open-source models, it is beneficial to release a high loss potential version to facilitate downstream tasks.*\"\n\n   I strongly encourage authors to read this paper.\n\n4. The paper \"A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\" [(https://arxiv.org/pdf/2410.04103v1)](https://arxiv.org/pdf/2410.04103v1) applies a similar concept to LLM pre-training.\n\nIn essence, the core idea (**Let LR decay happen in the most important stage**) has already been well-established in the field. This paper just translates this idea into a superficial learning rate schedule.\n\nAdditionally, the paper lacks rigorous theoretical analysis, and the evaluation is insufficient. For example, Table 2 reports only loss variations and average SFT performance. To strengthen the claims, the authors should address deeper questions such as:\n\n1. How do the results vary if the pre-training duration is extended or shortened?\n2. Are certain SFT tasks more or less affected by the proposed schedule? If so, what underlying reasons might explain this?\n3. Given that WSO outperforms WSD, why not slightly increase the learning rate during pre-training to further boost downstream SFT performance, even at the expense of higher pre-training loss?\n4. Have the authors considered quantifying this process more formally, such as by deriving a scaling law?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6a64OU6yAH", "forum": "JnebU2QLdH", "replyto": "JnebU2QLdH", "signatures": ["ICLR.cc/2026/Conference/Submission10612/Reviewer_jBxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10612/Reviewer_jBxS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926340665, "cdate": 1761926340665, "tmdate": 1762921874122, "mdate": 1762921874122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents extensive experimentation devoted to understanding learning-rate scheduling in LLM training. This work provides empirical insights that directly suggest how learning rate scheduling should be selected during pre-training to better support downstream model adaptability. The study recommends adopting Warmup-Stable-Only (WSO) as an alternative learning-rate strategy and releasing WSO-trained models to encourage wider use and adaptability in future LLM development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper examines the common practice of using learning-rate decay in LLM pre-training. The paper provides empirical evidence that keeping a constant learning rate after warmup improves performance. This approach, called the Warmup-Stable-Only (WSO) scheduler, outperforms conventional decay-based schedulers in supervised fine-tuning. Their finding highlights practical effectiveness for optimizing the entire LLM training pipeline.\n2. The paper demonstrates the inversion effect between pre-training and supervised fine-tuning performance across a wide range of settings. Decay-based learning rate schedulers consistently achieve stronger pre-training metrics, whereas the WSO configuration achieves superior results after SFT. This phenomenon is validated across 1B and 8B model scales, in multi-stage training pipelines.\n3. The paper challenges the standard assumption that stronger pre-training performance leads to a better final model. It shows that decay-based learning rate schedules achieve superior pre-training metrics, yet consistently result in worse performance after supervised fine-tuning. Their evidence suggests a need to rethink optimization goals in LLM development. They also emphasize prioritizing downstream adaptability over pre-training loss."}, "weaknesses": {"value": "1. The experiments are restricted to 1B and 8B parameters, which are relatively small compared to state-of-the-art deployed LLMs (often 30B~70B+). The absence of results at larger scales limits confidence in whether the observed advantages of WSO would extend to all situations.\n2. The study evaluates WSO against only three decay-based schedulers (Cosine, Linear, and Warmup-Stable-Decay). Other commonly used or recently explored learning rate strategies, such as polynomial decay or cyclic policies, are not explored. This limited comparison makes it unclear whether WSO’s benefits extend to other learning-rate policies. The paper would benefit from a brief stability assessment that checks whether WSO remains reliable under different environments.\n3. The experiments tune SFT hyperparameters separately for each pre-trained model. However, they always use selective learning-rate policies during SFT. This choice gives WSO an inherent advantage in downstream evaluation. The learning-rate policy should instead be maintained consistently across all training phases to enable a fair comparison. The work also lacks theoretical significance, relying mainly on empirical observations.\n4. The paper primarily evaluates instruction-following and general reasoning tasks, without testing multilingual ability, coding, or robustness under distribution shift. This narrow benchmark scope limits confidence in how widely WSO’s performance would translate to real-world deployment scenarios.\n5. Important related studies are missing from the references, such as [1].\n\n[1] Jin, Hongpeng, et al. \"Rethinking learning rate tuning in the era of large language models.\" 2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI). IEEE, 2023."}, "questions": {"value": "Please check the detailed comments for weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JTQuqM3AJo", "forum": "JnebU2QLdH", "replyto": "JnebU2QLdH", "signatures": ["ICLR.cc/2026/Conference/Submission10612/Reviewer_m1D7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10612/Reviewer_m1D7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949513964, "cdate": 1761949513964, "tmdate": 1762921873566, "mdate": 1762921873566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "I was surprised to see that I was the only reviewer who gave a positive overall recommendation for this paper, which may simply reflect our diverse backgrounds as reviewers. I carefully read the other reviews and the authors’ rebuttal, and I would like to share my thoughts to help move the discussion forward. More broadly, I am somewhat disappointed by what feels like an increasingly harsh reviewing culture in current ML venues, and I would like to help ensure that high-quality work is properly acknowledged.\n\nI do not agree with several of the weaknesses raised by Reviewer m1D7, especially the first two:\n\n**Model size.** I think it is too demanding to expect an ICLR paper to conduct pre-training with 30B–70B+ models. This would be an enormous resource requirement and, in many cases, an unnecessary waste. Even if the conclusions are drawn from smaller models, they are still valuable, especially for understanding and improving small-model training, which is highly relevant for domain-specific LLMs.\n\n**Other LR policies.** I do not think the lack of experiments with additional LR policies (e.g., cyclic, polynomial decay) is a severe limitation of WSO. In industry, WSD is very popular and efficient, so it is still important and meaningful to investigate WSO under this widely used setting and demonstrate its practical value.\n\nOverall, I believe the paper already carries out experiments at a sufficiently large scale for the conclusions to be reasonably generalizable.\n\nI partially agree with Reviewer jBxS that the idea that a “decayed model is not suitable for further tuning” is already relatively well accepted in the community. However, I still find the quantitative analysis and controlled experiments in this paper interesting and informative. I am convinced by the authors’ rebuttal regarding their three premises. Given that many practitioners still apply LR decay during pre-training, I think this work is meaningful and timely.\n\nRegarding Reviewer 68iN’s comments, I find the concerns about the correlation between SFT performance and sharpness reasonable, and I share this concern. I agree that this is an important point to be addressed, and I appreciate that the authors have committed to adding a more detailed discussion in the paper. For the other points about additional benchmarks and more compute, I feel the authors have addressed these adequately.\n\nOverall, I find this paper interesting, clearly written, and relevant for the development of LLMs. I have reviewed more than six papers for ICLR this year, and while many of them have received quite harsh reviews, this is, in my opinion, the strongest paper in my batch. I hope we can collectively reconsider the value of this work.\n\nP.S. I have no conflicts of interest or prior connection with the authors, and I do not know who they are; I am writing these comments purely out of my genuine appreciation of the work."}}, "id": "gUSA3q7I6E", "forum": "JnebU2QLdH", "replyto": "JnebU2QLdH", "signatures": ["ICLR.cc/2026/Conference/Submission10612/Reviewer_NMCt"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10612/Reviewer_NMCt"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission10612/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763703174536, "cdate": 1763703174536, "tmdate": 1763703174536, "mdate": 1763703174536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}