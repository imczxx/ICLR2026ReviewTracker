{"id": "bj0dcKp9t6", "number": 19486, "cdate": 1758296638109, "mdate": 1763748531822, "content": {"title": "Multifidelity Simulation-based Inference for Computationally Expensive Simulators", "abstract": "Across many domains of science, stochastic models are an essential tool to understand the mechanisms underlying empirically observed data. Models can be of different levels of detail and accuracy, with models of high-fidelity (i.e., high accuracy) to the phenomena under study being often preferable. However, inferring parameters of high-fidelity models via simulation-based inference is challenging, especially when the simulator is computationally expensive. We introduce MF-(TS)NPE, a multifidelity approach to neural posterior estimation that uses transfer learning to leverage inexpensive low-fidelity simulations to efficiently infer parameters of high-fidelity simulators. MF-(TS)NPE applies the multifidelity scheme to both amortized and non-amortized neural posterior estimation. We further improve simulation efficiency by introducing A-MF-TSNPE, a sequential variant that uses an acquisition function targeting the predictive uncertainty of the density estimator to adaptively select high-fidelity parameters. On established benchmark and neuroscience tasks, our approaches require up to two orders of magnitude fewer high-fidelity simulations than current methods, while showing comparable performance. Overall, our approaches open new opportunities to perform efficient Bayesian inference on computationally expensive simulators.", "tldr": "", "keywords": ["simulation-based inference", "likelihood-free inference", "Bayesian inference", "transfer learning", "multifidelity", "neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daf9faeef776c9f08aeb64dbc099654e3f2f7fe5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MF-(TS)NPE, a multi-fidelity recipe for SBI that pre-trains a neural posterior estimator on cheap low-fidelity simulations and fine-tunes it using a small budget of high-fidelity simulations. Then they propose two enhanced variants:  a sequential variant that further performs sequential SBI starting from the fine-tuned model and using the high-fidelity simulator and an active variant (A-MF-TSNPE) that adds an acquisition rule based on the variance of an ensemble of fine-tuned models. \nAcross SBI benchmarks (SIR, SLCP, OU, Gaussian Blob) and two computational neuroscience tasks (multicompartment neuron; large recurrent spiking network), the method matches or improves accuracy while using orders of magnitude fewer HF simulations; wall-clock accounting shows substantial real compute savings when HF simulations dominate cost (Table 4). The paper also investigates when transfer helps via controlled OU perturbations, highlighting the roles of mutual information and representational coherence between fidelities (Fig. 4)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "-**Clarity and motivation**: The paper is clearly written and well motivated: leveraging multiple simulators with varying levels of fidelity seems like a natural way to improve efficiency of SBI. While it was considered in the ABC literature, it received little attention when it comes to deep networks for SBI.\n\n-**Simplicity**: The proposed approach based on fine-tuning is quite simple to implement and seems to yield effective improvements. This simplicity makes it broadly applicable accross tasks, as shown in the exeperiments, and easily adaptable to other settings: sequential and active settings.  \n\n-**Soundness**: The paper proposes an empirical characterization for when such approaches should be effective based on mutual information and feature alignment. This supports the soundness of the approach although precise theoretical characterization remains unclear, but could be the subject of future work."}, "weaknesses": {"value": "The limitations of this work are well discussed in section 5. \n**Reliance on LF-HF alignment**: The method relies on a-priori alignment between the low-fidelity and high-fidelity simulators, but in general it is hard to quantify such alignment. While the paper studies the effect of this alignment, it is done on a toy example/controlled setting. Currently, the approach relies on domain experts that can know beforehand whether simulators are aligned, but it would be could to have a procedure or some guidelines for non-experts. Perhaps a better theoretical understanding could achieve that."}, "questions": {"value": "Is there a way to know in advance (without evaluation on real data), if the low-fidelity simulator can be helpful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7kk5D9KFwg", "forum": "bj0dcKp9t6", "replyto": "bj0dcKp9t6", "signatures": ["ICLR.cc/2026/Conference/Submission19486/Reviewer_Sge5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19486/Reviewer_Sge5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656685608, "cdate": 1761656685608, "tmdate": 1762931392903, "mdate": 1762931392903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General reply to reviewers"}, "comment": {"value": "We thank the reviewers for their thorough and insightful comments, and for acknowledging the strengths of the work. We particularly appreciate the positive feedback regarding the paper's clarity, relevance, and empirical results. Below, we directly address the key concerns raised and highlight how we have improved the manuscript in response:\n\n# Verbosity of the introduction/background and results in appendix \nFollowing the suggestions of reviewers G9kC and CW6v, we shortened the introduction and background section, allocating more space  to the benchmarking results and discussion on the effectiveness of transfer learning.\n\n# Additional experimental results\nWe added two new experiments. First, we tested MF-NPE in the case where the  high-fidelity model has a smaller number of parameters than the low-fidelity model. We also tested, in the Spiking network model, how the performance of MF-NPE is impacted as the discrepancy in the number of parameters between the low- and high-fidelity models is systematically increased. We also provided TARP evaluations as an alternative calibration check for all the tasks.\n\n# Clarified theoretical vs empirical contributions\nWhile theoretical guarantees on transfer learning performance would be valuable, obtaining these results requires several substantial advances in neural posterior estimation and transfer learning. We therefore highlight this as an important direction of future work and focus the present paper on flexibility, scalability, and broad empirical validation. In lieu of theoretical guarantees, we highlight the conceptual and empirical analysis in identifying mutual information and representational coherence as key determinants of effective transfer learning.\n\n# Naming\nFollowing the suggestion of the reviewer G9kC, we renamed the algorithm A-MF-TSNPE to MF-TSNPE-AF, to avoid confusion with active learning or amortized inference.\n\nWe thank the reviewers again for their valuable feedback, which has greatly strengthened our manuscript. Their suggestions have helped us address important gaps, clarify key contributions, and enhance the paper's overall rigor and quality."}}, "id": "t0xrEsu5FU", "forum": "bj0dcKp9t6", "replyto": "bj0dcKp9t6", "signatures": ["ICLR.cc/2026/Conference/Submission19486/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19486/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19486/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763748758449, "cdate": 1763748758449, "tmdate": 1763748758449, "mdate": 1763748758449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MF-(TS)NPE, a novel and efficient approach to simulation-based inference (SBI) designed to tackle the computational burden of parameter estimation in expensive high-fidelity simulators. The method leverages multifidelity modeling – combining computationally cheap, low-fidelity simulations with a limited number of high-fidelity simulations – and transfer learning to accelerate Bayesian inference. An active learning extension, A-MF-TSNPE, further improves efficiency by strategically selecting high-fidelity evaluations. Demonstrated across benchmark tasks and challenging neuroscience applications, MF-(TS)NPE achieves up to two orders of magnitude reduction in required high-fidelity simulations while maintaining comparable accuracy to existing methods like NPE and TSNPE. The work provides a valuable tool for efficient Bayesian analysis in a wide range of scientific domains reliant on complex simulations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper demonstrates a strong commitment to rigorous and accessible research in simulation-based inference. A key strength lies in its exceptionally clear and well-motivated introduction to both the challenges of SBI and the foundational techniques like TSNPE, significantly enhancing the paper’s accessibility to a broad audience. The experimental design is particularly commendable, building logically from established benchmarks to increasingly complex, real-world simulations (including L5PC and spiking neural networks), providing robust validation of the proposed method. \n\nThe authors showcase a thorough analysis, supported by a diverse suite of metrics – even extending to appendix materials to accommodate the breadth of results – and consistently present findings with appropriate uncertainty estimates, empowering readers to critically evaluate the conclusions. Further contributing to clarity, the writing is commendably free of unnecessary jargon and complex simulators are described in sufficient detail to facilitate a basic level of reproducibility. Finally, the authors adopt a refreshingly honest and nuanced approach, presenting their work as a solid incremental improvement within the field rather than an unsubstantiated claim of state-of-the-art performance. This measured presentation strengthens the credibility and impact of the research."}, "weaknesses": {"value": "This paper, while strong overall, exhibits a few areas for improvement. The introduction occasionally suffers from verbosity, dedicating excessive space to establishing the context of simulation-based inference – particularly in the opening paragraphs – which could be streamlined for greater impact. More critically, the authors sometimes present conclusions that exceed the support provided by the empirical results, necessitating more cautious interpretation. Finally, the quantification of improvements, specifically regarding computational cost, sometimes relies on relative gains rather than absolute metrics, hindering a full appreciation of the practical benefits achieved by the proposed method. Addressing these issues would enhance the clarity, rigor, and impact of the presented work."}, "questions": {"value": "- lines 30-53: nice introduction to the field, could be cut shorter perhaps\n- lines 200-202: \"Overall, MF-NPE shows competitive performance compared ...\" this sentence confused me as no strong results in favor of this claim was presented up to here to a reader, perhaps reconsider the formulation or remove this passage\n- page 4, Algorithm 1: it might be helpful to clearly identify low-fidelity and high-fidelity related parts of the algorithm (perhaps hints to the right of the algorithm)\n- page 4, Algorithm 1: line numbers would be helpful to communicate with peers about this\n- line 240-241: clearly state that you are interested in the epistemic uncertainty (makes the text more approachable to people experienced in UQ)\n- line 246-248: \"Alternatively, one could ... Griesmer 2024)\". MCdropout is a simple but often unstable UQ method, I personally would remove this sentence\n- on section 3.1.3: comment briefly on the fact that any UQ method at use here needs to be battle proven in the OOD regime (this is a hard problem within its own right, hence only comment)\n- line 268: \"In the limit of a large number of pairs, ...\" the high fidely simulation dataset comprises at minimum 50 samples. Would we expect convergence of NLTP to the KL here? \n- line 295-300: I guess, you do not use TARP or methods alike because you are working in a low data regime?\n- figure 2: do NOT include an interpretation in bold letters as the caption of the figure. Please leave it to the text (subsequently the reader) to make such assertions.\n- line 320: \"Across experiments, we observed a consistent performance increase with MF-NPE compared to NPE\" This is not what I deduce from the figure. From all 12 cases under study, by eye, I can tell that in only 4 MF-NPE is below the NPE (black dashed line) with some distance larger than the error bars. Please adapt this statement, if my interpretation is correct.\n- line 326: I can hardly compare Appendix F.1.1 to e.g. Fig 2. Please create a plot where this is possible and put it in F.1.1\n- line 362: \"approx. 4 times\" please be precise; also provide an absolute scale for reference on what 4 times higher means\n- line 373: perhaps add a second model critique methdo like TARP etc\n- section 4.3: all citations lack hyperrefs in my PDF reader, please check"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RnKn4DbqCV", "forum": "bj0dcKp9t6", "replyto": "bj0dcKp9t6", "signatures": ["ICLR.cc/2026/Conference/Submission19486/Reviewer_CW6v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19486/Reviewer_CW6v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847916986, "cdate": 1761847916986, "tmdate": 1762931392467, "mdate": 1762931392467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper takes a few simple existing techniques and applies them to SBI. Transfer learning, active learning, and (already often applied) sequential training. They are applied specifically to multi fidelity simulators, which are plausibly useful to practitioners. The paper shows benchmark results and application-specific results in neuroscience. There are also many empirical attempts to characterize the effects of their contributions which are confusingly put in the appendix."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "# results\n- Figure 3 looks compelling. It shows the typical result that active learning decrease NLTP and also reduces calibration. It would be nice to know if it's problematically uncalibrated, but maybe an expert in the area can understand that.\n- lines 439-448 are also super interesting. Why is this not in the main text? There is ~1.5 pages of application specific info about neuroscience, but this ML-general information is left out. Please include general info in the main text."}, "weaknesses": {"value": "# writing\n- It would be helpful if you stated your contribution more clearly at the beginning. There is a ton of background that obfuscates what algorithmic pieces you are introducing. It seems like the idea mainly exists in lines 282-232 and 234 - 248. I think a summary of that info could come significantly earlier.\n- \"A-MF-TSNPE\" is super confusing because A often means amortized in this taxonomy while S implies sequential (aka active learning). There is already \"active learning\" going on in TSNPE. Usually amortized methods do not use active learning. \n\n# results\n- In Figure 2 it seems like you add a lot of algorithmic complexity for small gains. Meanwhile Figure 8 seems super novel and interesting. Please consider including Figure 8 in the main text.\n- Where is an example of different numbers of parameters in the high fidelity and lower fidelity settings? That could be made more clear in the text. I know it's in the appendix.\n- In general I think you can spend much more space on the benchmark results rather than all the background.\n\n# citations\n- Since you compare to and extend TSNPE, I would strongly recommend citing TMNRE https://arxiv.org/abs/2107.01214\n- Additionally there is a huge literature about multi-fidelity bayesian optimization. Maybe it's worth citing that as well?"}, "questions": {"value": "- What happens when your cheap simulator has a narrow posterior compared to your high-fidelity simulator? It seems you are implicitly assuming that the cheap posterior is absolutely continuous w.r.t. the high fidelity simulator, but this doesn't have to be the case. \n- You mention that the method is \"applicable in situations where the low-fidelity model has fewer parameters than the high-fidelity model.\" What about the other way around?\n- Why do you spend so much main text time on background rather than getting into the method more quickly and showing all your cool results? So many interesting experiments are in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1th3LiamGK", "forum": "bj0dcKp9t6", "replyto": "bj0dcKp9t6", "signatures": ["ICLR.cc/2026/Conference/Submission19486/Reviewer_G9kC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19486/Reviewer_G9kC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902548870, "cdate": 1761902548870, "tmdate": 1762931391785, "mdate": 1762931391785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an approach that uses transfer learning from cheap simulations to enable efficient posterior estimation for expensive higher-fidelity simulators. The method is evaluated on benchmark tasks and neuroscience applications, showing up to 100x reduction in required high-fidelity simulations. The basic idea is to pre-train a neural density estimator on cheap low-fidelity simulations, then fine-tunes on expensive high-fidelity simulations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a real bottleneck in SBI where simulators can take minutes to hours per run. Relevant to lots of domains, in addition to those explored in the paper also in the physics sciences.\n2. Exploration along the full SBI \"stack\" -- provides both amortized (MF-NPE) and sequential (MF-TSNPE) variants plus active learning (A-MF-TSNPE). Algorithms clearly specified.\n3. Empirical results of when transfer works are strong -- beyond just demonstrating improvements, authors look at the underlying mechanisms (mutual information, representational coherence). The controlled experiments manipulating the OU process parameters provide insights into transfer learning limits."}, "weaknesses": {"value": "1. https://arxiv.org/abs/2507.00514 and https://arxiv.org/abs/2505.21215 seem very similar in technique to the present paper (with a different application focus). While mentioned as concurrent work, this takes away from some of the methodological novelty somewhat. \n2. Along the same lines, overall, a fairly straightforward, empirical application paper. Even toy theoretical analyses on transfer learning bounds -- perhaps relating low/high-fidelity similarity to sample complexity reduction -- would significantly strengthen the contribution. The empirical analysis in Section 4.4 is valuable but doesn't substitute for formal guarantees.\n3. Possible minor notation inconsistencies, e.g. switches between p(x|\\theta) and p_H(x|\\theta)"}, "questions": {"value": "1. Table 4 shows A-MF-TSNPE training takes ~10x longer than MF-NPE -- when is this tradeoff worthwhile?\n2. What are the major differentiators wrt https://arxiv.org/abs/2507.00514 and https://arxiv.org/abs/2505.21215 (e.g. the sequential, active learning parts)?\n3. The spiking network has 12/24 parameters for low/high-fidelity, curious how performance degrades as this gap increases (guessing this is strongly domain dependent, but curious if there are general principles that could help guide applications)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "byg6nMr1Wj", "forum": "bj0dcKp9t6", "replyto": "bj0dcKp9t6", "signatures": ["ICLR.cc/2026/Conference/Submission19486/Reviewer_Eu4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19486/Reviewer_Eu4Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969991494, "cdate": 1761969991494, "tmdate": 1762931391312, "mdate": 1762931391312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}