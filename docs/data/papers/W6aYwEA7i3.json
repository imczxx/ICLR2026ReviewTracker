{"id": "W6aYwEA7i3", "number": 16403, "cdate": 1758264245772, "mdate": 1759897242553, "content": {"title": "Mitigating the Echo Chamber Effect in KV Cache Compression via Coverage Optimization", "abstract": "Large language models (LLMs) have achieved strong performance on complex tasks ranging from multi-document reasoning to long-dependency question answering. To enable efficient inference, these models rely on key-value (KV) caching, which stores and reuses KV pairs to avoid redundant computation. As the sequence length grows, the KV cache increases linearly, creating a severe GPU memory bottleneck. This issue is commonly addressed by compressing the KV cache using a top-k selection based on attention scores. However, this strategy induces a homogeneity bias, the tendency to repeatedly select similar tokens, which creates an Echo Chamber Effect where the compressed KV cache is dominated by redundant information. This results in low effective coverage, causing crucial information to be lost and leading to verbose and logically broken answers under constrained token budgets. To address this, we propose ApertureKV, a KV cache compression method that employs coverage optimizing strategies to mitigate the Echo Chamber Effect. ApertureKV addresses two distinct sources of redundancy through two core components: Query Diversification (QD), which adjusts queries to encourage the retention of a more diverse set of tokens, and Redundancy-Aware Budget Allocation (RABA), which allocates more budget to heads that capture distinct information. By achieving highly effective coverage, ApertureKV enables robust KV cache compression under tight memory constraints, yielding more accurate responses. Evaluations on long-context benchmarks such as LongBench and LooGLE, including Needle-in-a-Haystack tasks, show that ApertureKV consistently outperforms state-of-the-art methods under tight budgets. In particular, on one LongBench sub-task with Mistral-7B-Instruct, ApertureKV retains 92.6\\% of FullKV performance while using only 0.2\\% of the KV cache budget.", "tldr": "We propose ApertureKV, a coverage-optimizing KV cache compression method that mitigates the Echo Chamber Effect and enables accurate long-context inference under tight memory budgets.", "keywords": ["Large Language Models", "KV Cache Compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b629b4e762a15eb6ec608684c006e962562c8252.pdf", "supplementary_material": "/attachment/3a0a383a462074bed906545e2a76d97a3d11bad0.zip"}, "replies": [{"content": {"summary": {"value": "This paper targets the problem of efficient and high-quality reasoning in LLMs. To address the Echo Chamber Effect caused by homogenized bias during inference, it proposes a novel KV cache compression algorithm that utilizes coverage-optimizing strategies to alleviate the Echo Chamber Effect in LLM reasoning. At the query level, it employs Query Diversification (QD), which modifies the queries by removing shared components and amplifying residual differences, enabling each head to focus on a broader and less overlapping set of tokens. At the head level, it applies the Redundancy-Aware Budget Allocation (RABA) algorithm, which reallocates the token budget across heads in proportion to their distinctiveness, granting larger budgets to heads that capture more unique information. Experiments show that the proposed method achieves performance close to the full KV cache in long-context tasks while greatly reducing the required cache budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel KV cache compression algorithm based on coverage-optimizing strategies, achieving notable improvements in long-context scenarios.\n2. It conducts comprehensive comparisons against multiple baselines and achieves consistent gains across most settings.\n3. The work includes thorough ablation studies and efficiency analyses, demonstrating that the approach maintains strong performance while significantly reducing memory cost."}, "weaknesses": {"value": "1. The experiments are conducted primarily on long-context and needle-in-a-haystack tasks. Additional evaluations on broader benchmarks and diverse application domains would help confirm the generalization capability of the proposed method.\n2. The explanation of the RABA algorithm for budget reallocation is somewhat complex. Including a step-by-step illustration or concrete example of how token budgets are adjusted across heads would make the method easier to follow."}, "questions": {"value": "1. Would it be possible to evaluate the proposed method on more general benchmarks and across different model scales (e.g., small, medium, and large models) to better demonstrate its generalization and robustness?\n2. Could the authors provide a detailed comparison of how the proposed budget allocation differs from prior methods under identical instruction–response pairs, for example by visualizing the distribution of KV cache usage across heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GhOjnzEk2j", "forum": "W6aYwEA7i3", "replyto": "W6aYwEA7i3", "signatures": ["ICLR.cc/2026/Conference/Submission16403/Reviewer_ucRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16403/Reviewer_ucRM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750821743, "cdate": 1761750821743, "tmdate": 1762926524194, "mdate": 1762926524194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ApertureKV, a KV cache compression method that reduces redundancy between tokens retained in the cache. ApertureKV makes diversified query vectors by shifting them away from their centroid, then uses these diversified queries to compute attention scores for selecting tokens for the cache. In addition, ApertureKV saves more tokens for heads that have more distinct token score distributions compared to other heads. With these 2 techniques, ApertureKV achieves state-the-art performance on the LongBench, LooGLE, and Needle-in-a-Haystack benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. ApertureKV has state-of-the-art performance on 3 benchmarks with 2 different models, with the best F1 score on every evaluated KV cache size except 2 cases.\n2. The paper demonstrates empirically that query redundancy and head redundancy occurs in existing methods, forming a strong basis for its motivation."}, "weaknesses": {"value": "1. The paper claims that baselines produce logically inconsistent answers, but some examples in Figure 7 show that the baseline answers are verbose but correct. The baselines may unnecessarily repeat the answer or the question, but it should not affect task accuracy. If the goal is simply to shorten model outputs, there may be simpler effective methods than ApertureKV. This undercuts the motivation of ApertureKV.\n2. While the paper shows that query redundancy and head redundancy occur in baselines, it does not show that they are responsible for the verbose outputs. This causation is assumed by the paper and weakens the motivating logic."}, "questions": {"value": "Questions:\n1. In Table 4, why does performance start dropping when the diversification coefficient drops from 0.45 -> 0.50? Does it continue dropping at 0.55? Why was 0.45 tested when all other values were chosen at intervals of 0.1?\n\nSuggestions:\n1. Appendix B should include direct empirical evidence that ApertureKV addresses redundancy in the KV cache, by showing correlations between KV values in baseline caches vs. ApertureKV’s cache. ApertureKV’s token score distribution should be shown in Figure 5 to demonstrate that ApertureKV resolves the redundancy issue shown.\n2. Using 6.5% as the max KV cache size seems to focus the evaluation only on very small KV cache sizes. The advantage of ApertureKV in Table 1 is highest for the smallest KV sizes, becoming more comparable at KV size 2048 (6.5%). It would be good to add evaluation at bigger KV caches like 4096 (13%), to demonstrate that ApertureKV’s advantage does not disappear at more moderate cache sizes.\n3. Can all baselines be included in Table 3? Especially HeadKV, which is the highest performing baseline for the chosen setting. The efficiency comparison would be valuable to show that computing query diversification and redundancy-aware budget allocation does not add substantial overhead. Alternatively, the overhead of QD and RABA could be shown in a separate evaluation that turns them off.\n4. The related work on KV Cache Compression should include KVZip[1], a recent KV cache compression method that outperforms previous methods.\n\nMinor comments:\n1. Figure 1 does not show much information. It only shows that the baseline contains similar things in its KV cache whereas ApertureKV contains different “things”, but in a way that is too abstract to convey anything deeper. Perhaps it could be replaced with a figure that visualizes the actual KV caches (showing higher correlations within the baseline KV cache and lower correlations in ApertureKV), or the results shown in (C) should just be made bigger.\n2. Figure 3 should include a color legend to make clear what red vs green represents.\n3. The y-axis label in Figure 6 should say “Pairwise cosine similarity” instead of “Value”, which provides no information.\n4. “Echo Chamber Effect” is a cool name but perhaps oversells the issue. “Echo chamber” seems to imply a cycle of compounding error, but the issue at hand is simple redundancy.\n\n[1] Kim et al. “KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction.” NeurIPS 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DCMsYPQF5A", "forum": "W6aYwEA7i3", "replyto": "W6aYwEA7i3", "signatures": ["ICLR.cc/2026/Conference/Submission16403/Reviewer_rWAv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16403/Reviewer_rWAv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788930547, "cdate": 1761788930547, "tmdate": 1762926523474, "mdate": 1762926523474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a new pathology in top-K KV-cache compression, the “Echo-Chamber Effect”: successive attention heads and queries repeatedly select near-identical tokens, so the compressed cache suffers low coverage and long-range reasoning degrades.\nTo counter this the authors propose ApertureKV, a prefill-only method with two synergistic modules:\n(1) Query Diversification (QD) that subtracts a shared centroid from windowed queries, and\n(2) Redundancy-Aware Budget Allocation (RABA) that re-allocates the per-head token budget proportionally to the Jensen-Shannon divergence between head score distributions.\nExtensive experiments on LongBench, LooGLE and Needle-in-a-Haystack (Llama-3-8B, Mistral-7B) show that ApertureKV retains ≥ 92 % of full-cache F1 with only 0.2 % of the KV memory, outperforming SnapKV, PyramidKV, HeadKV and Ada-KV while keeping the same runtime and VRAM. Ablation studies and qualitative examples confirm that coverage optimization, not just higher cache size, drives the gains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It ntroduces a new problem, echo-chamber effect, and two complementary, easy-to-implement fixes.\n2. It clearly separates query-level vs head-level redundancy and shows both must be addressed.\n3. Evaluation on 11 long-context tasks, two models, four cache sizes, plus ablations and significance tests."}, "weaknesses": {"value": "1.\tThe paper lacks an analysis of the echo-chamber effect and does not provide solid evidence that the proposed method can effectively address this issue. \n2.\tExperiments are restricted to Llama-3-8B-Instruct and Mistral-7B-Instruct, two models that supports a finite context length of only 7,950 and 31,500 separately. I doubt whether conducting experiments on long context benchmarks with the two models can yield accurate results."}, "questions": {"value": "1.\tHow does ApertureKV behave with grouped-query attention (GQA) or multi-query attention (MQA) where head redundancy is structurally reduced?\n2.\tAccording to the authors, QD can eliminate the Echo-Chamber Effect on full KV. Do other existing compression methods (e.g., adaKV) also have this effect? If so, is further analysis needed on the causes of this phenomenon? \n3.\tCould the authors list the text lengths of the tasks in the benchmarks and compare them with the model's context window length? If the window length is insufficient, should extra experiments be conducted on models with longer context support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "idZz32rLiC", "forum": "W6aYwEA7i3", "replyto": "W6aYwEA7i3", "signatures": ["ICLR.cc/2026/Conference/Submission16403/Reviewer_YpMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16403/Reviewer_YpMQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929227850, "cdate": 1761929227850, "tmdate": 1762926523070, "mdate": 1762926523070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a limitation in existing KV cache compression methods, which typically assess cache importance using a unary importance score. This approach results in a homogeneous compressed KV cache and consequently reduces diversity. To address this issue, the authors propose ApertureKV, a method that preserves KV cache entries using diversified queries and employs a redundancy-aware budget allocation strategy to improve compression effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear motivation and a well-defined solution, supported by clear writing and presentation. The proposed method outperforms existing baselines across a diverse set of benchmarks."}, "weaknesses": {"value": "1. The paper lacks a discussion on the compression cost, raising concerns about the practicality of the proposed method in deployment scenarios. Since it utilizes much more queries than baselines such as SnapKV, the end-to-end inference time may increase substantially, potentially making the method impractical for real-world use. It would strengthen the paper to include an analysis of the end-to-end inference speed, varying the prefill and decode sizes, to better assess the actual runtime efficiency.\n\n2. How does adding the small Gaussian Random noise to the query vectors for diversification perform? Does the proposed orthogonalization performs better than this simple baseline?"}, "questions": {"value": "See the above weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not have ethics concern regarding this paper."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dXetIbzWIR", "forum": "W6aYwEA7i3", "replyto": "W6aYwEA7i3", "signatures": ["ICLR.cc/2026/Conference/Submission16403/Reviewer_ZxzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16403/Reviewer_ZxzQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999254162, "cdate": 1761999254162, "tmdate": 1762934647837, "mdate": 1762934647837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}