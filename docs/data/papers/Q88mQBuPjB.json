{"id": "Q88mQBuPjB", "number": 5449, "cdate": 1757911130497, "mdate": 1759897974320, "content": {"title": "Robust LLM-Based Scoring via Reference-Anchored ELO Estimation", "abstract": "LLM-based evaluation by direct (absolute) scoring suffers from systemic instabilities; ceiling compression constrains headroom, heavy-tailed score distributions inflate variance, and inconsistent agreement across independently trained judges induces scale drift that destabilizes rankings.\nWe present *Reference-Anchored Elo Estimation* (RAEE), a principled framework that anchors all model comparisons to a fixed reference and expresses outcomes as win probabilities on a relative scale as an alternative to absolute scoring.\nWe prove that, by design, RAEE minimizes judge-specific scale drift, suppresses between-judge variation, and yields analytic uncertainty estimates without costly resampling.\nExperimental results show that RAEE reduces per-run standard error by $\\approx 44$\\% and across-judge coefficient of variation by $\\approx 72$\\% relative to direct scoring, while preserving ranking stability even under reference changes.\nRobustness is observed across multiple domains, with RAEE sustaining low dispersion and consistent rankings despite task-specific difficulty shifts.\nOur analytic uncertainty bounds, which incorporate finite-population and reliability adjustments, predict observed variance within $\\pm 12$\\% on tested datasets.\nThese results position RAEE as a statistically efficient, reproducible, and readily deployable alternative to conventional LLM-based evaluation.", "tldr": "We introduce Reference-Anchored Elo Estimation (RAEE), a framework that converts simple pairwise comparisons against a fixed reference into stable, reproducible scores with built-in uncertainty, overcoming the instabilities of direct LLM evaluation.", "keywords": ["Large Language Model", "LLM Evaluation", "LLM Judge", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d045a9462d1978945b5f25c914301941ac9c3b5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present Reference-Anchored Elo-Estimation (RAEE), a principled framework that anchors all model comparisons\nto a fixed reference and expresses outcomes as win probabilities on a relative scale as an alternative to absolute scoring. The authors compare to absolute scoring and find that RAEE reduces per-run standard error by ≈ 44% and across-judge coefficient of variation by ≈ 72% relative to direct scoring, while preserving ranking stability even under reference changes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper's presentation and styling are strong.\n* The paper is reasonably well organized."}, "weaknesses": {"value": "* The main contribution of this work is mostly anticipated by Arena-Hard Auto (https://arxiv.org/abs/2406.11939), an extremely popular LLM-judged alignment benchmark from the creators of Chatbot Arena published in 2024, making this a largely derivative work unsuitable for publication.\n* Even were this not the case, ELO scoring introduces its own set of biases, including the enforcement of transitive preference where, in many cases, none exists, and the hyperparameter choice of baseline model for comparison, which can be highly biasing (https://arxiv.org/abs/2509.20293, https://arxiv.org/abs/2409.15268). The authors' mechanism is largely standard and does not remediate this.\n* The paper cites some highly relevant related works, e.g. Khurana et al., 2024, Zhang et al., 2024, Jung et al.,\n2025, which would have served as reasonable baselines to demonstrate the purported improvements of RAEE, but then fails to actually compare to them. This undermines the authors' claim that their contribution is a meaningful improvement on existing work. The only baseline presented in the main paper, in fact, is direct scoring without any attempt to calibrate or clean noisy judge responses, an unrealistically weak baseline.\n* The authors provide no reason to believe that their process of \"screening for high-reliability judges\" does not bias the judgment process; in other words, inter-rater disagreement, or even self-disagreement across runs, can reflect legitimate underlying uncertainty driven by benchmark design, model panel selection, et cetera. It is not possible, prima facie, to be sure that a judge is unreliable because it is inconsistent, and discarding uncertain judges will bias the panel in favor of judges who are more consistent, not necessarily more correct."}, "questions": {"value": "* Why did the authors largely reproduce the scoring mechanism of one of the most popular LLM-judged benchmarks and then claim it as a novel contribution in their paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SeS3AVlSC2", "forum": "Q88mQBuPjB", "replyto": "Q88mQBuPjB", "signatures": ["ICLR.cc/2026/Conference/Submission5449/Reviewer_sUeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5449/Reviewer_sUeC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760554458299, "cdate": 1760554458299, "tmdate": 1762918068566, "mdate": 1762918068566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Reference-Anchored Elo Estimation (RAEE), a new method for LLM benchmark scoring. Each prompt is paired with a reference answer, and a LLM judge is used to compare each of the evaluated model’s outputs with the corresponding reference answer and grade it as a win, loss, or tie. The grades are used to estimate an Elo score and a confidence interval. Compared to other methods, this method is more stable and does not suffer from scale drift and ceiling effects."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The problem area addressed is important; LLM judges have been used for benchmarking for a couple of years, yet the community has not converged on standardized scoring methods. This paper has the potential to be impactful by proposing a good \"default\" method for the community.\n- The proposed method has a number of desirable properties:\n  - The method is robust to the choice of model used for generating the reference answers.\n  - The method is robust to the choice of LLM judge model, as long as the model is “strong”.\n  - The method can be used with LLM juries, but the benefit of additional judges is small so a single judge can be used for cost efficiency.\n  - Compared to other methods, this method is more stable and does not suffer from scale drift and ceiling effects.\n  - The method is relatively simple to implement.\n- The experimental results are convincing and presented clearly.\n- The experiments address a number of interesting questions, such as the effect of adding additional LLM judges."}, "weaknesses": {"value": "- The authors should consider adding a reference to WildBench’s WB-Reward metric. WB-Reward is also based on LLM judged comparisons against fixed baselines, but does not use Elo.\n- The MT-Bench results only used three different models as the model under evaluation or the model for generating reference answers (o4-mini, gpt-4o, gemini-2.5-flash). Additionally, the difference in strength between these models are large, which makes stability easier. This makes the results somewhat less convincing.\n- The models to be used as judge models do not seem to be listed in the paper. There is a list of direct scoring models in Table 7, but it is not clear if this is the same as the pairwise scoring models.\n- Although the authors discuss problems such as the comparative trap (in the introduction), self-preference and verbosity/position biases (in Section 4 Related Work), the method does not address these problems."}, "questions": {"value": "- Did you analyze how your method addresses the comparative trap, self-preference, verbosity/position biases, etc., or do you consider this out of scope?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5g2D7NOGNZ", "forum": "Q88mQBuPjB", "replyto": "Q88mQBuPjB", "signatures": ["ICLR.cc/2026/Conference/Submission5449/Reviewer_TNg3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5449/Reviewer_TNg3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774213315, "cdate": 1761774213315, "tmdate": 1762918068240, "mdate": 1762918068240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the systemic instabilities of direct absolute scoring and the inherent biases of pairwise comparison in LLM-based evaluation. It proposes Reference-Anchored Elo Estimation (RAEE), a principled framework that anchors all model comparisons to a fixed reference (either human-annotated or model-generated responses) and transforms ternary win/tie/loss outcomes into interpretable Elo scores with analytic uncertainty quantification.\n\nRAEE features three core components: Jeffreys smoothing to stabilize win probability estimates, a monotone logit transformation to map probabilities to the Elo scale, and finite-population correction (FPC) for robust uncertainty bounds. The framework screens for \"strong judges\" using an ICC(3, k) threshold of ≥0.90 to ensure inter-judge consistency, and theoretically guarantees minimized scale drift, suppressed between-judge variation, and closed-form uncertainty estimates without costly resampling.\n\nExperimental evaluations across MT-Bench (dialogue), FloRES (machine translation), and TL;DR (abstractive summarization) demonstrate RAEE’s superiority: it reduces per-run standard error by ~44% and across-judge coefficient of variation by ~72% compared to direct 1-10 scoring. Notably, RAEE maintains stable model rankings even when switching reference anchors (Kendall’s τ ≈ 0.89) and exhibits robust performance across domains, with its analytic uncertainty bounds predicting observed variance within ±12% after FPC and reliability adjustments.\n\nThis work makes significant contributions by reconciling the discriminative power of pairwise comparisons with the interpretability of absolute scoring, providing a statistically efficient, reproducible, and readily deployable solution for LLM evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a meaningful and pressing problem in LLM evaluation: effectively resolves systemic instabilities of direct absolute scoring (e.g., scale drift, ceiling compression) and inherent biases of pairwise comparisons, filling a critical methodological gap for reliable, interpretable, and statistically grounded assessment.\n\nRigorous methodological design: Built on solid statistical foundations (Jeffreys-Beta posterior, monotone transformation, ICC-based strong judge screening) with transparent, modular workflows. It enables closed-form uncertainty quantification without costly resampling, balancing theoretical rigor with practical deployability.\n\nComprehensive and robust empirical validation: Evaluated across diverse tasks (dialogue, translation, summarization) using standardized benchmarks, with comparisons to multiple baselines and reference anchors. Demonstrates significant, quantifiable improvements (≈44% SE reduction, ≈72% CV reduction) and stable rankings, confirming cross-domain generalizability.\n\nStrong integration of theoretical guarantees and practical utility: Formally proves key properties (inter-judge correlation bounds, variance decomposition) while ensuring results are interpretable (Elo scores) and resource-efficient (single strong judge suffices), making it valuable for both academic research and real-world LLM evaluation pipelines."}, "weaknesses": {"value": "Incomplete theoretical underpinnings: The justification for Jeffreys smoothing is insufficient, with no clarification on its statistical consistency under extreme conditions (e.g., small sample sizes, highly skewed win probabilities) or comparisons with alternative smoothing approaches, leading to potential arbitrariness in method selection. Additionally, the key assumption of approximate variance equality across strong judges lacks empirical validation and relevant literature support.\n\nLimited validation scope: Boundary scenarios (e.g., extremely strong/weak reference anchors) are not tested, leaving uncertainty about potential ceiling/floor effects. The empirical reliability adjustment factor k=0.85 lacks verification of cross-scenario (e.g., diverse tasks, low-resource languages) adaptability, and no general calibration method is provided.\n\nModest innovativeness: The core idea of anchoring pairwise comparisons is simplistic and does not break free from the inherent limitations of pairwise frameworks. It fails to quantify the degree of model superiority over the anchor, relying heavily on anchor appropriateness—overly strong or weak anchors may compromise discriminability.\n\nFailure to address the comparative trap: The paper acknowledges the \"comparative trap\" (preferring fluent but incorrect responses over less coherent but correct ones) as a flaw of pairwise comparisons, yet the proposed RAEE framework does not include mechanisms to mitigate this issue. It may still inappropriately score responses that are more fluent than the anchor but factually incorrect higher.\n\nSuboptimal presentation: Insufficient transitions between sections hinder readability. Method modules lack unification, making it hard to intuitively grasp the relationships among different components. Non-standard expressions (e.g., \"entral estimand 1\") and unrefined details (e.g., \"Reference\" in figure 2) affect the paper’s professionalism."}, "questions": {"value": "1. Could you clarify the statistical consistency of Jeffreys smoothing under extreme conditions (e.g., small sample sizes, highly skewed win probabilities) and provide comparative analysis with alternative smoothing methods (e.g., Laplace smoothing) to justify its selection?\n\n2. Is there empirical evidence or relevant literature support for the key assumption of approximate variance equality across different strong judges in the variance decomposition derivation?\n\n3. Have you tested RAEE’s performance with extremely strong or weak reference anchors, and if so, what results were observed regarding potential ceiling/floor effects on Elo score discriminability?\n\n4. How adaptable is the empirical reliability adjustment factor across diverse tasks (e.g., code generation, logical reasoning) and low-resource languages, and is there a general calibration method for k?\n﻿\n5. Does the framework have plans to extend beyond pairwise comparison limitations, such as quantifying the degree of model superiority over the anchor rather than just the probability?\n\n6. Could you explain how the appropriateness of the reference anchor is assessed, and what mitigation strategies are available if the anchor is overly strong or weak?\n\n7. Since the paper acknowledges the \"comparative trap\" (preferring fluent but incorrect responses over less coherent but correct ones) as a limitation of pairwise comparisons, does the RAEE framework have any specific mechanisms to mitigate this issue? If not, how do you address the risk of inappropriately high scores for responses that are more fluent than the anchor but factually incorrect?\n\n8. Can you standardize non-conforming expressions (e.g., \"entral estimand 1\") and improve transitions between sections to enhance readability?\n\n9. Could you clarify the unrefined details mentioned (e.g., the specific ambiguity or inconsistency of \"Reference\" in Figure 2) and explain how you plan to revise them to enhance the paper’s professionalism?\n\n10. Could you provide more cohesive integration of method modules to clarify the relationships between different components of the RAEE framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TDvgIgKzGa", "forum": "Q88mQBuPjB", "replyto": "Q88mQBuPjB", "signatures": ["ICLR.cc/2026/Conference/Submission5449/Reviewer_Q8hk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5449/Reviewer_Q8hk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928294256, "cdate": 1761928294256, "tmdate": 1762918067908, "mdate": 1762918067908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}