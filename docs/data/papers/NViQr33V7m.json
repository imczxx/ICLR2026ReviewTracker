{"id": "NViQr33V7m", "number": 17372, "cdate": 1758275167019, "mdate": 1759897179518, "content": {"title": "Enhancing Generalization via Sharpness-Aware Trajectory Matching for Dataset Condensation", "abstract": "Dataset condensation aims to synthesize datasets with a few representative samples that can effectively represent the original datasets. This enables efficient training and produces models with performance close to those trained on the original sets. Most existing dataset condensation methods conduct dataset learning under the bilevel (inner- and outer-loop) based optimization. However, the preceding methods perform with limited dataset generalization due to the notoriously complicated loss landscape and expensive time-space complexity of the inner-loop unrolling of bilevel optimization. These issues deteriorate when the datasets are learned via matching the trajectories of networks trained on the real and synthetic datasets with a long horizon inner-loop. To address these issues, we introduce Sharpness-Aware Trajectory Matching (SATM), which enhances the generalization capability of learned synthetic datasets by optimizing the sharpness of the loss landscape and objective simultaneously. Moreover, our approach is coupled with an efficient hypergradient approximation that is mathematically well-supported and straightforward to implement, along with controllable computational overhead. Empirical evaluations of SATM demonstrate its effectiveness across various applications, including in-domain benchmarks and out-of-domain settings. Moreover, its easy-to-implement properties afford flexibility, allowing it to integrate with other advanced sharpness-aware minimizers.", "tldr": "", "keywords": ["bilevel optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/210a25b9ba3035e81bce2f7245c6ba188e76c3a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduced the sharpness-aware minimizer into the bilevel-optimization-based dataset condensation to address the optimization issue from the complicated loss landscape. To reduce the computational overhead of the added optimization, two strategies are proposed to approximate the hypergradient calculation. Experiments across various settings show the effectiveness of the proposed solution to optimizes the bilevel objective function and sharpness simultaneously."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The optimization issue of the bilevel problem in dataset condensation is very important. This work studied this problem from the less-explored loss landscape view. \n2. The proposed strategies for mitigating the doubled computational cost of SAM are mathematically sound and practical.\n3. The proposed solution achieves consistent improvements over state-of-the-art trajectory-matching competitors across standard benchmarks."}, "weaknesses": {"value": "1. The novelty of the proposed solution is not large. The introduction of SAM into dataset condensation  in Section 4.1 seems a simple application of SAM. The truncated unrolling hypergradient is similar to k-step SGD in [1]\n2. Although the proposed method is effective, the improvement compared with the SOTA methods is incremental.\n3. Some latest SOTA methods for dfficient dataset distillation are missing [2,3,4].\n\n[1] Meta Label Correction for Noisy Label Learning. AAAI 2021\n\n[2] Provable and Efficient Dataset Distillation for Kernel Ridge Regression. NeurIPS 2024\n\n[3] M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. AAAI 2024\n\n[4] Accelerating Dataset Distillation via Model Augmentation. CVPR 2023"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1bE4DInmdd", "forum": "NViQr33V7m", "replyto": "NViQr33V7m", "signatures": ["ICLR.cc/2026/Conference/Submission17372/Reviewer_TTrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17372/Reviewer_TTrg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760782815715, "cdate": 1760782815715, "tmdate": 1762927283032, "mdate": 1762927283032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on dataset condensation (also known as dataset distillation), where the goal is to learn a small synthetic dataset such that models trained on it perform well on the real data distribution. The authors target methods that use trajectory matching (e.g. MTT) – i.e. they optimize synthetic data by matching the training trajectory of a network on synthetic data to the trajectory on real data. While these methods can produce high-fidelity condensed sets, they suffer from two major issues: the bilevel optimization with long unrolled training trajectories is extremely costly (time- and memory-intensive), and the outer-loop loss landscape becomes highly non-smooth and “sharp,” leading to poor generalization (e.g. the synthetic set overfits to the specific model or training path used in optimization).\n\nTo tackle these challenges, the paper introduces Sharpness-Aware Trajectory Matching (SATM). SATM integrates the concept of Sharpness-Aware Minimization into the trajectory matching paradigm, aiming to simultaneously minimize the trajectory-matching loss and the sharpness of the outer-loop loss.\n\nA straightforward application of SAM in this setting would double the already heavy computation (since it requires computing gradients at a perturbed point), so the authors propose two efficiency strategies: (i) a truncated unrolling of the inner loop to approximate the hypergradient (reducing memory and computation by not backpropagating through the entire long trajectory at once), and (ii) trajectory reusing with gradual perturbations, which allows re-utilizing segments of the network’s training trajectory and the corresponding gradients for the sharpness calculation without starting from scratch each time.\n\nThey also derive a closed-form solution for adjusting the learning rate in the sharpness computation, which simplifies implementation and offers theoretical guarantees by bounding the error introduced by these approximations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Experiments demonstrate that SATM yields significant improvements in generalization of condensed datasets. Models trained on SATM-produced synthetic sets not only perform well in-distribution but also transfer better to different architectures and out-of-domain tests, alleviating the overfitting to the training network that plagues other methods.\n\nNotably, SATM outperforms prior trajectory matching approaches on standard condensation benchmarks and achieves noticeable gains on ImageNet-1K condensation, a challenging task where most existing methods fail to produce viable condensed sets.\n\nIt also runs faster than or on par with the most efficient recent method (TESLA) while maintaining similar memory usage, indicating that the sharpness-aware additions are cost-effective.\n\nIn summary, this work is novel in bringing flat-minima concepts to dataset condensation and demonstrates that doing so markedly boosts the versatility of the distilled data."}, "weaknesses": {"value": "One concern is that the method introduces several hyperparameters (for the sharpness term, truncation length, perturbation scale, etc.) and added complexity, which may require careful tuning. Maybe i have missed, but i would appreciate if the authors provide complexity on the setup of hyper-parameters, and transferability of these values to other datasets and architectures.\n\nOne other concern is lack of extensive discussions with existing methods. \nFor example, \n- https://arxiv.org/abs/2303.04449 discusses 1) sharpness and 2) generality of dataset distillation to other architectures. But, authors does not mention about this paper.\n- https://arxiv.org/abs/2403.16028 discusses the relationship between dataset bias and dataset distillation. This is related to out-of-distribution generality of dataset distillation."}, "questions": {"value": "Discussed in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4UkVfKNPG8", "forum": "NViQr33V7m", "replyto": "NViQr33V7m", "signatures": ["ICLR.cc/2026/Conference/Submission17372/Reviewer_mngL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17372/Reviewer_mngL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636289693, "cdate": 1761636289693, "tmdate": 1762927282646, "mdate": 1762927282646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel dataset condensation technique that employs two hypergradient approximation strategies to address the significant computational overhead caused by sharpness minimization. SATM outperforms trajectory-matching-based competitors on various dataset condensation benchmarks under both in-domain and out-of-domain settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. Dataset distillation/condensation is an important topic but still faces many challenges to be addressed."}, "weaknesses": {"value": "1. The novelty of the method is limited. It primarily combines sharpness-aware optimization with MTT, without introducing a new data compression technique, only some modifications to the optimization steps.\n2. It would be better if the authors compared their method with other approaches that also incorporate sharpness-aware minimization (SAM).\n3. The paper lacks comparisons with several recent state-of-the-art methods, such as RDED [1], NRR-DD [2], and SRE2L [3], among others.\n\n[1] On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. CVPR 2024\n\n[2] Enhancing Dataset Distillation via Non-Critical Region Refinement. CVPR 2025.\n\n[3] Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. NeurIPS 2023."}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "feF5fbjnkH", "forum": "NViQr33V7m", "replyto": "NViQr33V7m", "signatures": ["ICLR.cc/2026/Conference/Submission17372/Reviewer_fBqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17372/Reviewer_fBqz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908729320, "cdate": 1761908729320, "tmdate": 1762927281976, "mdate": 1762927281976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper introduces “Sharpness-Aware Trajectory Matching (SATM)” , which focuses on optimization of the sharpness of the loss landscape and the objective simultaneously.\n- This paper aims to address limitations of trajectory matching methods for dataset distillation.\n- The paper has combined ideas from trajectory matching dataset distillation method such as MTT and minimization of sharpness of loss landscape.\n- Two computational approximations to replace the costly calculations are introduced, namely “Truncated Unrolling Hypergradient (TUH)” and “Trajectory Reusing (TR)”."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Specific methods to deal with computational complexity are provided. Both methods are supported with theoretical bounds and practical implementation.\n2. The paper has provided experimental results on various standard benchmark datasets such as CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet-1K (along with various subsets of ImageNet-1K)."}, "weaknesses": {"value": "**1. Limited baselines**\n\nRecent advances in dataset distillation methods, including decoupled distillation, diffusion-based, and optimization-free dataset distillation methods, are not considered.\nBelow are some of the SOTA dataset distillation works, with which comparison should have been made to be more insightful.\n\n- \"Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective\" (NeurIPS, 2023)\n- Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment (NeurIPS, 2024)\n- On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm (CVPR, 2024) \n- Efficient dataset distillation via minimax diffusion (CVPR,2024)\n- DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation (CVPR, 2025)\n\n**2. The experiments are limited to (relatively simple) ConvNet architecture only**\n\nThe true impact of the proposed method cannot be analysed without considering more complex and standard architectures such as VGG, ResNet, ViT to name a few.\n\n3. The improvements in generalization accuracy as shown in Table 2 are marginal (often < 1%).\n\n4. Ablation studies are not provided regarding the impact of trajectory reusing and TUH. It would have been very useful to study how these approximations impact the generalization accuracy. \n\n5. The runtime cost as compared to the TESLA method is quite high despite the modest increment in accuracy, which weakens the argument of efficiency."}, "questions": {"value": "- Please refer to the weaknesses  section of the review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i7U1bXKicb", "forum": "NViQr33V7m", "replyto": "NViQr33V7m", "signatures": ["ICLR.cc/2026/Conference/Submission17372/Reviewer_XJ2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17372/Reviewer_XJ2n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011051605, "cdate": 1762011051605, "tmdate": 1762927281397, "mdate": 1762927281397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}