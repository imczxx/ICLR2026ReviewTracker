{"id": "s4jpvJv6I8", "number": 7468, "cdate": 1758023428287, "mdate": 1759897851139, "content": {"title": "Is Delayed Robustness Really Grokking?", "abstract": "We analyze the phenomenon of delayed robustness, where a neural network trained beyond overfitting becomes robust to adversarial attacks. This phenomenon was first observed by Humayun et al. (2024), and characterized as grokking behavior. We reproduce delayed robustness to PGD attacks in multiple set-ups and, using stronger attacks, show that this robustness is actually overestimated. We then demonstrate that delayed robustness is not grokking, but instead the result of two unintended side effects during overtraining: softmax collapse in the cross-entropy loss function and a too large effective learning rate caused by gradient scaling in the Adam optimizer. We provide experimental evidence that these issues indeed create networks that resist PGD attacks without actually becoming as robust to the stronger attacks. We also point out a relation with dying neurons and the slingshot effect. Using simple interventions to solve these issues, we show that no delayed robustness appears.", "tldr": "", "keywords": ["neural networks", "delayed robustness", "grokking", "adversarial attacks", "deep learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7140e5a049e627fa1d23ff358c12e399be22ec6.pdf", "supplementary_material": "/attachment/3956a088cbf6023ef12d620b1418e9e34efe6871.zip"}, "replies": [{"content": {"summary": {"value": "This paper offers a critical re-evaluation of the \"delayed robustness\" phenomenon, previously equated with Grokking, demonstrating compellingly that the observed robustness is significantly overestimated. The paper analyzes the causes from two perspectives—Softmax collapse and the Adam optimizer—and subsequently eliminates delayed robustness through gradient clipping and optimizer manipulations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper intuitively validates through experimental observation that delayed robustness is susceptible to more powerful adversarial attacks, and the phenomenon is made to vanish by modifications addressing both Softmax collapse and the optimizer.\n2. The paper is generally well-structured and easy to follow. The arguments are presented with adequate clarity."}, "weaknesses": {"value": "1. Insufficient discussion of the deep mechanism of the Grokking phenomenon. The explanation lacks a formal theoretical framework or analytical evidence linking these mechanisms to the observed robustness behavior. \n2. The selection and comparison of adversarial attack methods could be further enhanced. While the paper uses AutoAttack+ to demonstrate the inadequacy of PGD, it may not have fully explored other advanced black-box or white-box attack methods."}, "questions": {"value": "1. The connection between these numerical effects and the phase-transition characteristics typically associated with grokking is not clearly established. Additional theoretical modeling or quantitative diagnostics would be necessary to strengthen the argument.\n2. The authors might consider providing experimental results covering more adversarial attack methods. This expanded evaluation would help ensure the analysis is more comprehensive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3PeCTogVPc", "forum": "s4jpvJv6I8", "replyto": "s4jpvJv6I8", "signatures": ["ICLR.cc/2026/Conference/Submission7468/Reviewer_sBXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7468/Reviewer_sBXh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658297939, "cdate": 1761658297939, "tmdate": 1762919584094, "mdate": 1762919584094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a phenomenon called delayed robustness, where an over-trained neural network becomes robust to adversarial attacks.\nThis phenomenon was introduced by Humayun et al. (2024).\n\nThis paper presents evidences that delayed robustness is an artifact of improper evaluation.\nSpecifically, the authors show that the true robustness does not increase for over-trained under stronger attacks.\nIn addition, the authors also present alternative hypothesis for why delayed robustness was observed in the first place."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper refutes the claim by Humayun et al. (2024).\nThus, publishing this paper could potentially fix incorrect statements in the existing literature.\n1. Overall, I think the claims in this paper makes sense.\nIn the past, there were a lot of work in the adversarial robustness literature that turned out to be ineffective (or less effective) under stronger attacks.\nAnd it seems that delayed robustness also falls into this category."}, "weaknesses": {"value": "1. Experiments are only done on MNIST and CIFAR10.\nIdeally, it would to great to validate the claims in the paper on larger datasets like ImageNet.\n1. The impact of this paper is somewhat limited.\nThis paper seems to be written in a way that specifically refutes the delay robustness claim by Humayun et al. (2024).\n1. There are a lot of stuff in the experiments with different settings, and it would be better to consolidate a bit.\nFor example, a better way to present the results is to follow the same training procedure (with 32-bit floating point numbers, cross entropy loss, and Adam) in Humayun et al. (2024), and show that delayed robustness does not emerge under stronger attacks (e.g., auto-PGD and PGD with scaled softmax.)\n\nMinor.\n1. I am assuming experiments in Section 4.1 are trained with the mean squared error loss.\n1. Typo in line 473 \"\\\\(\\perp\\\\)Grad\"?\nIt would be better to explicitly mention this in the text, instead of letting readers infer it from the section title."}, "questions": {"value": "1. Why do some experiments use the MSE loss to train the neural network even though MNIST and CIFAR10 are classification datasets?\nThe use of MSE as a training loss does not seem to relate to any of the hypothesis mentioned in Section 3.\nNote that the cross entropy loss, as described in Section 4.2, affects PGD attack in the test time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lDlR6Nt720", "forum": "s4jpvJv6I8", "replyto": "s4jpvJv6I8", "signatures": ["ICLR.cc/2026/Conference/Submission7468/Reviewer_8wjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7468/Reviewer_8wjS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885003697, "cdate": 1761885003697, "tmdate": 1762919583744, "mdate": 1762919583744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Statement"}, "comment": {"value": "First of all, thank you all for your reviews. There is valuable feedback that we will work on. We hope to come back to each of you with individual responses soon. Given the nature of the reviews, however, we felt it necessary to make a general comment about the purpose of our paper. \n\nOur paper shows that the delayed robustness phenomenon is not grokking, but is either false robustness or an artifact of unintended training behaviors. This finding is indeed a direct refutation of the paper of Humayun et al. (2024), as some of the reviews have correctly noted. This ICML paper has been widely cited in subsequent literature as an example of grokking. \n\nOur refutation of the claim of Humayun et al. is based on the experimental setting of the original paper. Therefore,  in the spirit of the self-correction of science, we do not understand why reviewers want us to extend our findings to larger datasets or why they have doubts about our work providing a worthwhile contribution.\n\nWe may not have communicated our goal and scope clearly enough, and we will improve our paper in that respect. Under the assumption that we clarify our paper satisfactorily, we ask the reviewers if they still believe our contribution is not up to par for a presentation at ICLR."}}, "id": "17x0I5m8Kp", "forum": "s4jpvJv6I8", "replyto": "s4jpvJv6I8", "signatures": ["ICLR.cc/2026/Conference/Submission7468/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7468/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7468/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641705647, "cdate": 1763641705647, "tmdate": 1763641705647, "mdate": 1763641705647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that the “delayed robustness” reported by Humayun et al. (2024)—i.e., a sudden rise in adversarial (PGD) accuracy well after overfitting—does not reflect true robustness or grokking. Instead, it is largely an artifact of (i) softmax collapse under cross‑entropy (CE), which makes loss gradients uninformative for PGD, and (ii) Adam’s gradient‑scaling pathology that inflates the effective learning rate when running‑average second moments get tiny; both effects emerge late in training and can mislead PGD‑style attacks. The authors reproduce delayed robustness on MNIST MLPs and CIFAR‑10 ResNet‑18s, then (a) apply stronger or numerically stabilized attacks (AutoAttack+ or a proposed ScaledPGD) and (b) introduce stabilizing interventions (larger Adam δ or AMSGrad; higher‑precision loss/gradients), showing that the PGD gains largely vanish. They further connect the phenomenon to dying neurons, reduced local complexity, and the optimizer “slingshot” effect."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1- Clear negative result against a popular hypothesis: The paper demonstrates that the late PGD gains do not persist under stronger or stabilized attacks (AutoAttack+ / Auto‑PGD; ScaledPGD), undermining the interpretation of delayed robustness as “grokking‑induced robust partitions.” The side‑by‑side PGD vs AA+/ScaledPGD gap in Figure 1 and Figure 5 is compelling.\n\n2- Mechanistic decomposition. Two concrete mechanisms are analyzed:\nA) Adam gradient scaling: equations (2)–(7) explain how small vt inflates the effective step size, making training non‑convergent late and causing neuron death and local‑complexity drops; AMSGrad/δ↑ mitigations support the causal story (Figure 2, 3, 4).\n\nB) Softmax collapse: a careful numerical account (underflow vs absorption) using the stable logsumexp form (eq. 8–12) and explicit collapse counters that align with loss spikes and PGD rises (Figure 6). The ScaledPGD proposal (eq. 11–12) is a practical attacker‑side fix.\n\n3) Causal interventions rather than correlations. Changing the optimizer (AMSGrad or Adam δ), numerical precision (32→64 bit), and the attack (ScaledPGD/AA+) moves the phenomenon in the predicted direction. This triangulation is well‑executed. Figures 2, 5, 7.\n\n4) Useful diagnostics and instrumentation. The paper quantifies dead neurons, local complexity near data, and collapse counts through training (Figures 3–4, 6, 8), and shows slingshot‑like loss spikes in linear‑time plots (Figures 18–19), producing a coherent diagnostic suite other groups can reuse"}, "weaknesses": {"value": "1- Attack coverage and settings are narrow in places.\nProtocol rigidity. PGD always uses 10 steps and a fixed step size (0.0156) at ε=0.06 with [0,1] clipping (Appendix A.1). While the paper’s thesis is about PGD’s gradient informativeness, a broader sweep (steps, step‑size schedule, random restarts, ε grid) is needed to fully rule out “insufficient PGD tuning” as the sole culprit in some setups.\n\nAutoAttack usage. In several experiments only AA+’s apgd‑ce component is used; other components (e.g., FAB, Square) are omitted, which could matter especially around gradient obfuscation. This is disclosed but weakens the universal claim.\n\n2- Causality vs correlation for the optimizer story. The Adam‑induced effective step‑size blow‑up is plausible, and AMSGrad/δ↑ help (Figure 2), but there is no direct measurement of the problematic scale factor (e.g., histograms of $\\gamma / \\sqrt{v_t+\\delta}$ over layers/time). Without those, the link from scaling → slingshot → dead neurons → PGD failure is still partly inferential.\n\n3- 64‑bit precision as a “fix” is impractical and only delays problems in ResNets. Double precision removes MNIST collapse (Figure 5), yet for ResNet‑18 it mostly delays the onset (Figure 8). Since 64‑bit training is rarely used at scale, the paper should emphasize optimizer/loss‑level fixes over precision and demonstrate collapse‑free training with realistic toolchains.\n\n4- Limited normative space and datasets. All attacks are ℓ∞ with ε centered at 0.06; there is no exploration of ℓ2/ℓ1 nor datasets beyond MNIST/CIFAR‑10. The claim that “delayed robustness is not grokking” would be stronger with at least one non‑vision or larger‑scale dataset. Figure 10 varies ε only within MNIST.\n\n5- Softmax‑collapse detection could be quantified more tightly. We see the fraction of train samples with collapse (Fig. 6), but not the joint statistics that link collapse severity (max‑min logit gap, counts) to PGD failure per‑example and to AA+/ScaledPGD success. That mapping would directly substantiate the mechanism.\n\n6- Interplay between the two mechanisms is not fully isolated. The paper treats MSE/Adam to diagnose optimizer effects and CE to diagnose softmax collapse. It would be helpful to cross the factors—e.g., CE + AMSGrad/δ↑ and MSE in 64‑bit—to demonstrate additivity or independence. (Some CE 64‑bit results exist, but optimizer cross‑checks for CE are missing.)\n\n7- Attack–defense diagnostics for gradient obfuscation are incomplete. The community “red flags” (transfer attacks, black‑box vs white‑box gap, stepsize sensitivity, EOT for randomness) could be run systematically to show that the late‑phase PGD robustness satisfies classic obfuscation patterns. The current evidence strongly suggests it, but the standard checklist would close the loop"}, "questions": {"value": "1- PGD sensitivity: If you vary PGD steps (e.g., 10→50→200) and step sizes (fixed / backtracking), how do late‑phase adversarial accuracies change on MNIST MLP and ResNet‑18? Please plot per‑example attack success vs. step schedule.\n\n2- Random restarts: With ≥20 PGD restarts, does the late‑phase PGD robustness persist? Report mean/variance over restarts.\n\n3- AA+ completeness: For experiments where you used only apgd‑ce, what changes when the full AA+/AutoAttack suite (including FAB and Square) is enabled? Provide both curves and final numbers\n\n4- Transfer & black‑box: Does black‑box transfer from independently trained surrogates attack the late‑phase models better than white‑box PGD? This would be another obfuscation indicator.\n\n5- Other norms: Do the conclusions hold for ℓ2 and ℓ1 (with matched perceptual budgets)? Include ε‑sweeps like Fig. 10 for those norms.\n\n6- Direct measurement: Please report the distribution over iterations/layers of the effective scale factor $\\gamma / \\sqrt{v_t+\\delta}$ and of $v_t$ correlate spikes with loss slingshots, dead‑neuron counts, and PGD failure.\n\n7- Hyperparameter sweep: Beyond δ, how do β₂, β₁, and base LR affect the onset of delayed PGD robustness? Include AdamW and Adagrad/Lion comparisons.\n\n8- Causal intervention: If you clamp $\\gamma / \\sqrt{v_t+\\delta}$ to a max value during late training, do slingshots and dead neurons disappear without otherwise changing the optimizer?\n\n9- Per‑example linkage: For CE models, can you show a scatter of (max‑min logit gap, collapse type/count) vs. PGD success and vs. ScaledPGD/AA+ success at the same points in training (e.g., before/after the PGD jump in Fig. 5)?\n\n10- Optimizer control for CE: Does AMSGrad or δ↑ affect the frequency of absorption collapse (not just underflow)? If not, this would reinforce optimizer‑independence of collapse.\n\n11- Loss‑side fixes: Beyond precision, how do StableMax or logit‑clipping during loss/grad computation compare to ScaledPGD in both attack success and training stability? (The paper cites these ideas in discussion but does not evaluate them.)\n\n12- EOT for numerics: If you inject small logit noise and run Expectation‑over‑Transformation attacks, do PGD results align with ScaledPGD—i.e., does EOT “unstick” PGD when collapse makes the gradient locally flat?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GlnvzAvQ6a", "forum": "s4jpvJv6I8", "replyto": "s4jpvJv6I8", "signatures": ["ICLR.cc/2026/Conference/Submission7468/Reviewer_uJgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7468/Reviewer_uJgs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995542772, "cdate": 1761995542772, "tmdate": 1762919583341, "mdate": 1762919583341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies delayed robustness of models that are trained beyond overfitting, and shows that, in contrary to previous literature, robustness does not obviously increase with increasing training time. The authors then explain the observed phenomenon and show by well crafted experiments, that it is linked to side effects in overtraining, which correlates with observations in the literature. They also confirm that PGD cannot be seen as an absolute metric for robustness, especially in settings where gradients become unreliable, as it may be the case in the observed settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Extensive experiments are proposed to study delayed robustness, confirm its existence, and explain the underlying phenomenon in overtraining. The experiments are generally well chosen, and quite clearly presented, and offer confirm observations of other papers, as well as offer new insights on the training dynamics and robustness performance in the overtraining regime."}, "weaknesses": {"value": "While the paper is quite extensive in experiments, and (sometimes new) insights on the overtraining regime, the main message of the paper is not really clear. What is the main motivation of studying delayed robustness? What would the results and insights in the paper, bring in terms of constructive solutions for further developments of models that have both good generalisation performance, and robustness?\n\nThe structure of the paper, and the definition of the problem should probably be strengthened, so that the reader is properly guided through the numerous experiments, and so that the actual contribution becomes clearer. That would certainly help valorise the extensive experimental work, and the expertise offered by the authors in this paper. \n\nIn addition, it may be important to clearly clarify the new insights offered in the paper, with respect to results that conform obsverations proposed in other paper. In the current version, things are a bit inter-twined, which makes it hard to truly appreciate the proposed contribution. \n\nIt is also known that PGD is not a perfect proxy for robustness, due to its sensitivity to gradient artifacts. Also, MNIST, that is used in the majority of experiments, is known for being a very poor proxy for developing strong insights on large-scale model training (same for CIFAR10). In order to validate general results and insights, it is important to confirm experimental observations on larger datasets. \n\nProbably, a stronger explanation, or well posed intuitions, about the existence, or the possibility for increased robustness in overtraining regime, would be beneficial for the community. It does not seem to be related to grokking, according to the paper development, but that path is only discussed very superficially, unfortunately. Maybe there could be solid arguments to get inspiration from, in the development of Humayun 2024, that could be transposed to delayed robustness analysis and its connection to grokking, if any?\n\nFormally, eq (1) is not fully correct, as it is missing a connection to the correct label y."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tlnspTILyI", "forum": "s4jpvJv6I8", "replyto": "s4jpvJv6I8", "signatures": ["ICLR.cc/2026/Conference/Submission7468/Reviewer_48FM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7468/Reviewer_48FM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016842015, "cdate": 1762016842015, "tmdate": 1762919582866, "mdate": 1762919582866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}