{"id": "6TwQVKNnYy", "number": 3518, "cdate": 1757456981667, "mdate": 1759898083454, "content": {"title": "Contrastive Vision-Language Learning with Paraphrasing and Negation", "abstract": "Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.", "tldr": "Adding paraphrase and negation loss terms and trained with LLM‑generated triples to CLIP aligns paraphrases closer to image embedding while repels negations, achieving state‑of‑the‑art semantic robustness without harming retrieval accuracy.", "keywords": ["Contrastive Learning", "Multimodal", "Semantic Alignment", "Language and Vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fab17dc4e534cdcf204d3104456f484c65ed532d.pdf", "supplementary_material": "/attachment/79723a9c4f25ddb546fa80411881ffd4ac665374.zip"}, "replies": [{"content": {"summary": {"value": "**Summary:**\n\nThis paper proposes SemCLIP, an extension of CLIP that jointly models paraphrasing and negation to improve semantic robustness in vision–language learning. It introduces new paraphrase and negation losses within a low-dimensional projection subspace to align equivalent captions and separate contradictory ones. Experiments on CC-Neg and Sugarcrepe++ show SemCLIP preserves CLIP’s retrieval accuracy while improving robustness to negation and linguistic variation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "**Strengths:**\n\nAuthors tackle an important problem of negation in multimodal retrieval."}, "weaknesses": {"value": "**Weaknesses:**\n\n- CLIP is now outdated and many new multimodal models perform much better than CLIP. See MMEB leaderboard (V1) and the models on it.\n\t- Most of these models are expected to be very robust to paraphrases.\n- Comparison with ConCLIP, NegCLIP and ParaCLIP missing.\n- Missing Ablations:\n\t- What is the need for extra projection layer? Ablations need to be performed.\n\t- Why not use a contrastive loss with the new (anchor, paraphrase, negative). Why use two seperate losses? Ablation needs to be performed.\n- Writing needs to be improved:\n\t- \"However, large multimodal models underperformed relative to LLMs\" - needs citation\n\t- Lines 70-72: Citation/Evaluation missing. Does clip underperform on these examples?\n\t- Lines 87-88: What above findings?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KiuIvN5iac", "forum": "6TwQVKNnYy", "replyto": "6TwQVKNnYy", "signatures": ["ICLR.cc/2026/Conference/Submission3518/Reviewer_yYBD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3518/Reviewer_yYBD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760581187471, "cdate": 1760581187471, "tmdate": 1762916778857, "mdate": 1762916778857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The problem of improving CLIP training is considered. The paper introduces SemCLIP incorporating a dedicated embedding projection space and a combined loss function​, that includes components for paraphrasing (L_paraphrase​) and negation (L_negation​) alongside the standard contrastive loss (L_contrastive​). SemCLIP aims to move paraphrased captions closer to the original image embeddings while pushing negated captions further away, leading to a more robust semantic alignment between text and image. Experimental results, particularly on the CC-Neg benchmark, show that SemCLIP preserves CLIP's original performance while increasing the distance to negated captions, and this robustness extends to downstream zero-shot classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "1. Lack of technical novelty. It is not a new idea to finetune CLIP with negation data or paraphrasing data.\n2. Lack of comprehensive evaluation. The proposed SemCLIP model was only evaluated on  two compositionality benchmarks and 5 classification benchmarks (CIFAR-10, CIFAR-100, FOODS101, FLOWERS102, OXFORD Pet). This is clearly insufficient to evaluate a CLIP model. Evaluation on more benchmarks (e.g. VTAB+ for classification, COCO/Flickr for text-image retrieval) is necessary for a solid paper."}, "questions": {"value": "What is the cost for collecting synthetic caption data by LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ituq49TMir", "forum": "6TwQVKNnYy", "replyto": "6TwQVKNnYy", "signatures": ["ICLR.cc/2026/Conference/Submission3518/Reviewer_3wgX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3518/Reviewer_3wgX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558791179, "cdate": 1761558791179, "tmdate": 1762916778702, "mdate": 1762916778702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, SemCLIP, proposes an extension to the CLIP framework to jointly address model robustness against two critical semantic transformations: paraphrasing (equivalence) and negation (contradiction). The approach uses a new combined contrastive loss incorporating $L_{paraphrase}$ and $L_{negation}$ terms applied to LLM-generated training triples."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes to jointly model the two opposing yet critical semantic transformations—equivalence (paraphrasing) and contradiction (negation)—within a single unified contrastive learning framework. This approach is intriguing, establishing a necessary research direction for exploring the holistic semantic robustness of multimodal models."}, "weaknesses": {"value": "* Despite the joint objective, paraphrase robustness does not improve: on SCPP, SemCLIP underperforms the CLIP baseline ($53.1\\\\%$ vs. $60.0\\\\%$), and on CC-Neg paraphrase it trails a \"Paraphrase-only\" variant ($21.0\\\\%$ vs. $23.0\\\\%$). This pattern suggests a practical tension between the attractive force of $L_{\\\\text{paraphrase}}$ and the repulsive force of $L_{\\\\text{negation}}$.\n\n* Although negation robustness improves, it remains far from CoN-CLIP ($\\text{CC-Neg Acc}_{\\\\text{neg}}$ $78.1\\\\%$ vs. $99.70\\\\%$), with substantial downstream zero-shot classification drops ($\\approx 20$-$30$ p on Foods-101, Flowers-102, etc.). This questions the competitiveness of the projection-based loss for contradiction.\n\n* The paper lacks a mechanistic account of how the low-dimensional projection reconciles opposing forces in the joint objective. Moreover, restricting loss weights $\\\\alpha, \\\\gamma$ to $\\\\{0, 1\\\\}$ precludes assessing trade-offs; a continuous search $0 < \\\\alpha, \\\\gamma < 1$ is necessary to demonstrate optimality and robustness of conclusions."}, "questions": {"value": "* The paper needs a mechanistic account (e.g., visual or mathematical analysis) demonstrating how the low-dimensional projection successfully disentangles the competing $L_{paraphrase}$ and $L_{negation}$ forces, as their conflict seems to cause performance degradation on paraphrasing.\n\n* Paraphrasing accuracy dropped below the CLIP baseline. What is the root cause of the conflict between the opposing $L_{paraphrase}$ and $L_{negation}$ forces, and how does the projection space explicitly mitigate this tension?\n\n* Since loss weights were restricted to $\\{0, 1\\}$, a continuous parameter grid search is warranted to find the optimal balance.\n\n* Given the vast gap to CoN-CLIP ($Acc_{neg}$ 78.1% vs. 99.70%), what is the fundamental limitation of the projection-based loss that prevents achieving competitive performance?\n\n* The substantial performance drop on challenging downstream tasks (e.g., Foods 101, Flowers 102) suggests a failure in generalization. Please provide insight into why the learned semantic robustness does not effectively transfer to more complex, fine-grained visual recognition and compositional reasoning tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AYN4KRf4wb", "forum": "6TwQVKNnYy", "replyto": "6TwQVKNnYy", "signatures": ["ICLR.cc/2026/Conference/Submission3518/Reviewer_84YA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3518/Reviewer_84YA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971670927, "cdate": 1761971670927, "tmdate": 1762916778268, "mdate": 1762916778268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}