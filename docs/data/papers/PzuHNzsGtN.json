{"id": "PzuHNzsGtN", "number": 11103, "cdate": 1758189371628, "mdate": 1759897608610, "content": {"title": "GaLe: memory-efficient Global Approximate and Local Exact features", "abstract": "Embedded devices and Microcontroller units (MCUs) generally offer only a fraction of the memory and computational power available on machines equipped with general-purpose GPUs. Existing approaches for memory-efficient inference on these devices rely either on patch-based inference, which causes significant computational overhead, or approximation-based methods, leading to substantial accuracy degradation.\nIn this work, we propose \\emph{GaLe}, a novel memory-efficient approximation technique that enables the deployment of pretrained deep neural networks on tiny, resource-constrained devices without the need for retraining. Our method introduces a feature map partitioning strategy that approximates layer outputs using two complementary representations: (i) a local exact ($L_E$) component that preserves fine-grained details and (ii) a global approximate ($G_A$) component that retains long-range dependencies. Differently from available tiling approaches, GaLe maintains compatibility with architectures with global receptive field operations and attention mechanisms, such as modern hybrid CNN-transformer models, while significantly reducing memory usage and computational overhead.\nWe validate our approach on ImageNet classification, demonstrating performance comparable to exact inference methods while drastically reducing memory consumption and compute costs, achieving up to $65$% speedup on a Cortex-M33 core for a $90$% RAM reduction compared to patch-based inference. Beyond efficient deployment, GaLe offers a general recipe for feature map decomposition, enabling the design of novel, resource-efficient convolutional and attention modules and potentially guiding memory-aware architecture search. We further demonstrate its versatility across classification, detection, and diffusion models, highlighting its potential as a foundation for future research on memory-efficient architectures. GaLe also benefits general-purpose GPUs, reducing the memory usage of diffusion models under 200MB (from 6GB) for high-resolution outputs.", "tldr": "GaLe is a memory-efficient approximation technique that decomposes feature maps into local and global components, enabling pretrained deep models to run on tiny devices with up to 90% less RAM and 65% faster inference without retraining.", "keywords": ["Memory-efficient deep learning", "Global-local feature map approximation", "TinyML deployment", "Resource-efficient CNN-transformer design", "Low-memory inference"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8289130f6e6faf4b8ebf299c165bcddaac32e8b.pdf", "supplementary_material": "/attachment/7fbac2c9cc861673c80a43c888170b357cc1e611.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the GaLe method to address the memory bottleneck in deep learning deployment on embedded devices. Its core innovation lies in the feature map partitioning strategy of \"Local Exact (LE) + Global Approximate (GA)\". The overall design is theoretically sound and practically applicable, yet there remain areas requiring supplementary verification or optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Directly targets the memory constraint issue of embedded devices (MCUs) while avoiding the shortcomings of existing methods. It overcomes the high computational overhead of patch-based inference (PPBI) and mitigates the sharp accuracy drop of pure approximation methods, achieving a balance among \"memory saving, accuracy preservation, and computational efficiency\".\n2. Supports direct deployment of pre-trained model and is compatible with CNNs, hybrid CNN-Transformer architectures, and operations with global receptive fields . It also adapts to mainstream embedded runtimes, lowering the threshold for industrial implementation.\n3.Covers multiple hardware platforms  and multi-task scenarios with impressive key data—for instance, 90% memory saving and 65% inference acceleration on the Cortex-M33 core, and reducing the memory usage of diffusion models from 6GB to below 200MB, which is highly convincing.\n4. Beyond deployment optimization, it provides a general solution for feature map decomposition, which can guide the design of resource-efficient convolution/attention modules and memory-aware architecture search, expanding the application scope of the method."}, "weaknesses": {"value": "1. GA is generated based on \"resolution scaling\". Although this operation is lightweight, there is no comparison with more advanced global feature extraction methods, making it impossible to verify whether this strategy is the optimal solution for the \"accuracy-efficiency\" trade-off.\n2. Under high memory compression ratios, the task-specificity of accuracy loss is not deeply analyzed—for example, in scenarios sensitive to details such as small object detection and high-resolution diffusion generation, whether the accuracy drop remains controllable.\n3. The calibration phase adjusts the slice overlap (O) and slice number (N) iteratively to control errors, but the relationship between \"calibration time\" and \"dataset size\" is not explained. Given the limited computing power of embedded devices, excessive calibration time may undermine its engineering practicality.\n4. While comparisons are made with PPBI, FlashAttention, and ToMe, the latest TinyML methods from 2024 to 2025 are not covered, making it impossible to clearly position GaLe in the current technical landscape."}, "questions": {"value": "1. How is the weight α for fusing GA and LE determined? Is it a fixed value, adaptive per layer, or dynamically adjusted with tasks? Is there a systematic optimization strategy?\n2.In CNN-Transformer hybrid architectures, how to automatically decide \"which layers use LE slicing and which use GA approximation\"? Does it rely on manual parameter tuning or an end-to-end layer selection mechanism?\n3. When GaLe is combined with ToMe, GA is generated via ToMe while LE remains unchanged—does this combination cause consistency conflicts between local and global features? Are there optimization designs for the fusion logic?\n4.After multiple rounds of inference on embedded devices, does the memory management of GaLe pose a leakage risk? Will accuracy drift occur under hardware interferences such as temperature and voltage fluctuations?\n5.Can the calibration module and feature fusion module of GaLe be further lightweighted? For example, on MCUs, whether the memory usage and computational time of these two modules will become new bottlenecks?\n\n**If the author can address my questions, I am willing to improve my rating.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqvgKhGMnp", "forum": "PzuHNzsGtN", "replyto": "PzuHNzsGtN", "signatures": ["ICLR.cc/2026/Conference/Submission11103/Reviewer_YTrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11103/Reviewer_YTrW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760941271003, "cdate": 1760941271003, "tmdate": 1762922279528, "mdate": 1762922279528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GaLe, a method designed to reduce the RAM consumption of deep neural network models for image processing during inference. The key idea is to approximate the outputs of trained network layers using two lightweight representations: a Local Exact (LE) map that preserves fine image details, and a Global Approximate (GA) map that retains global information. The paper demonstrates that GaLe can be applied to a wide range of architectures, including CNN-based models, transformer-based models, and hybrid CNN–transformer models. It also improves patch-based inference by adopting horizontal slicing for faster RAM access and further reduces computational overhead by choosing the minimal patch overlap needed to meet target accuracy. Experiment are done to show the efficiency and effectiveness of GaLe in inference for tasks including image classification, object detection, and image generation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a new method to reduce the RAM consumption of image classification, object detection and image generation models during inference.\n\n2. The proposed method is adaptable to a wide range of mainstream image processing models, such as CNN-based, and CNN-transformer based models."}, "weaknesses": {"value": "1. The whole writing is very cryptic. Many details are missing. \n\n\t- There is no equation / formula / diagram to indicate how LE or GA is computed, making it impossible to understand how the method works.  \n\t\n\t- The combination of LE and GA for hybrid models is not sufficiently motivated and not adequately justified. It remains unclear why the proposed LE and GA mappings can effectively approximate the outputs of the network layers.\n\t\n\t- There are many sentences like the following one in the text: \"GaLe dynamically determines the number of patches for each block during the calibration pass, adapting to the memory footprint of the intermediate tensors\" (Lines 245-246). However, there is no explanation on how it is done.\n\n\t- In Line 287, the paper claims that \"our method can offer superior performance than approximation-based techniques.\" However, as far as I understand, the proposed method GaLe is also an approximation-based technique, as there is no formal proof to demonstrate that it achieves certain optimal performance.\n\n\t- In Line 175, it is argued that \"accuracy is often more heavily impacted by other factors.\" However, performance degradation is resulted by the proposed method regardless of what other factors are. Explanations are needed.\n\n\t- In Line 323, the Id matrix and the unit matrix are essentially the same. Why do we need to use two different terms?\n\n\n2. Concerns about the experimental evaluation.\n\n\t- How is the computational overhead reported in Table 2 and Figure 6 defined and measured?\n\n\t- Line 465 states that the proposed method achieves a 74% reduction for RT-DETR-L and an 88% reduction for YOLOv11n. How are these reduction percentages computed?\n\n\t- There is no sensitivity analysis of the important hyperparameter $\\alpha$, which controls the proportion of the LE and GA mappings when approximating hybrid network outputs.\n\n\n3. Some typos:\n\n\t- In Line 103, \"the MobileNet Family...\" instead of \"te MobileNet Family...\".\n\t- In Line 107, \"and automated network...\" instead of \"an automated network...\""}, "questions": {"value": "Please kindly refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QEEOQSrBN5", "forum": "PzuHNzsGtN", "replyto": "PzuHNzsGtN", "signatures": ["ICLR.cc/2026/Conference/Submission11103/Reviewer_E4Fc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11103/Reviewer_E4Fc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488628627, "cdate": 1761488628627, "tmdate": 1762922278923, "mdate": 1762922278923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GaLe, a novel, memory-efficient approximation technique for deploying large, pretrained deep neural networks on resource-constrained devices like microcontrollers without retraining. GaLe addresses the limitations of existing methods by using two complementary representations: a \"Local Exact\" ($L_E$) representation to preserve fine-grained details, and a \"Global Approximate\" ($G_A$) component to retain long-range dependencies. The \"Local Exact\" representation partitions the feature maps into multiple small tiles, which can significantly reduce RAM usage and computational overhead, while the \"Global Approximate\" helps maintain compatibility with modern architectures that use global operations and attention mechanisms. The authors demonstrate GaLe's effectiveness across various tasks, including image classification, object detection, and diffusion models, achieving performance comparable to exact inference but with substantial reductions in memory and latency, such as a 65% speedup on a Cortex-M33 core for a 90% RAM reduction compared to patch-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper shows the real speedup and memory savings on different devices, demonstrating the effectiveness of the proposed methods.\n- Evaluations across different tasks and models demonstrate the generalization of the GaLe method.\n- GaLe can not only be applied to the convolutional neural networks, but also to the attention-based models without retraining."}, "weaknesses": {"value": "- The novelty of the partitioning methods is limited. Such methods have been explored in previous architecture design works, e.g. [1] and [2]\n- The technical details aren't clear enough. See the questions for more details.\n- The paper could be further strengthened by including an analysis of how the results are influenced by different parameter settings. This would provide valuable insights into the sensitivity of the proposed method.\n\n[1] Gang Li, et al, Block Convolution: Toward Memory-Efficient Inference of Large-Scale CNNs on FPGA, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022\n\n[2] Manoj Alwani, et al, Fused-Layer CNN Accelerators, MICRO, 2016"}, "questions": {"value": "- Does the attention-based model also need calibration? This paper only discusses the calibration for the convolutional layer to determine the overlap parameter. But for the attention layer, it seems that there is no overlap between different patches.\n- There are many parameters during partitioning the feature map into different patches, e.g., patch size, overlap parameters, slicing patterns, and the weighting factor $\\alpha$. For a given model, how to determine these parameters?\n- This paper only evaluates GaLe on those vision tasks. Since GaLe can be applied to attention-based models, it would be better to evaluate the proposed method with LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lQvJhNCyZw", "forum": "PzuHNzsGtN", "replyto": "PzuHNzsGtN", "signatures": ["ICLR.cc/2026/Conference/Submission11103/Reviewer_Tdm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11103/Reviewer_Tdm2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922582516, "cdate": 1761922582516, "tmdate": 1762922278122, "mdate": 1762922278122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a memory-efficient inference framework GaLe for deep neural networks, aimed at embedded and resource-constrained devices. GaLe decomposes feature maps into two complementary representations: Local Exact (LE) that preserves fine-grained details via full-resolution features, and Global Approximate (GA) that retains long-range dependencies via low-resolution features. GaLe maintains compatibility with global receptive field operations and attention mechanisms, including hybrid CNN–transformer architectures without retraining, requiring only a lightweight calibration phase."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured and presents the technical contributions in a clear and accessible manner. The proposed memory-aware local exact feature map slicing and global approximation techniques are straightforward and practical to implement.\n\n2. The algorithms are technically solid and can be naturally extended to attention mechanisms, enabling their integration into transformer-based architectures and offering comprehensive insights into memory-efficient inference. It possesses a unified theory and practical framework across CNNs, transformers, and hybrid models.\n\n3. Experimental results on ImageNet demonstrate that the proposed method significantly reduces RAM usage and improves inference efficiency compared to baseline approaches."}, "weaknesses": {"value": "1. Although the proposed method is technically sound, it is not fully convincing that the inference slicing strategy for local exact feature maps is optimal. The approximation–accuracy trade-off is empirically tuned through calibration, and the work would be strengthened by a more rigorous theoretical analysis or formal characterization of the associated error bounds.\n\n2. The comparison with prior work primarily focuses on methods such as PPBI and FPBI, which were proposed several years ago. Including more recent state-of-the-art approaches of training-free memory efficient methods, such as post-training quantization/pruning baselines in the evaluation would provide a stronger and more up-to-date demonstration of the effectiveness of the proposed method.\n\n3. The experimental evaluation is limited to ImageNet dataset. To better demonstrate the generalization capability of the proposed method, it would be beneficial to include results on additional datasets or domains."}, "questions": {"value": "1. Could you provide some theoretical analysis that why the inference slicing with learned padding is optimal for memory efficiency?\n\n2. Could you provide some comparison with post-training quantization / pruning state-of-the-art methods?\n\n3. Could you provide more comparison results on the other datasets, such as Places, iNaturalist, COCO, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VVVgr8qDOE", "forum": "PzuHNzsGtN", "replyto": "PzuHNzsGtN", "signatures": ["ICLR.cc/2026/Conference/Submission11103/Reviewer_Vkmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11103/Reviewer_Vkmd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979581443, "cdate": 1761979581443, "tmdate": 1762922277243, "mdate": 1762922277243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}