{"id": "snbY9Uj0Gx", "number": 5479, "cdate": 1757913963353, "mdate": 1762951773552, "content": {"title": "Plug-and-Play Retrieval-Augmented Active Test-Time Adaptation for VLMs", "abstract": "Pre-trained vision-language models (VLMs) have demonstrated remarkable performance across various real-world benchmarks. In particular, CLIP, one of the famous VLMs, has achieved satisfactory performance on vision-language tasks without fine-tuning (\\ie zero-shot setting). Nevertheless, it is well-known that effectively leveraging a pre-trained model requires adaptation to the test distribution. Since the test distribution is typically unknown, test-time adaptation (TTA) has emerged as one of the solutions. However, existing TTA algorithms rely not on expert-provided ground-truth knowledge but on pseudo-labels derived from the knowledge of the pre-trained model itself. This undesirable reliance can lead to a cascade of incorrect knowledge propagation. To address this issue, we propose a novel framework, active test-time adaptation, which selectively queries human experts for ground-truth labels of uncertain samples and incorporates them for answering future queries. Then, we develop a novel algorithm, **RE**trieval-augmented **ACT**ive TTA (**REACT**), which is designed to be plug-and-play with any TTA algorithms. Through extensive experiments on ten real-world benchmarks commonly used in CLIP evaluation as well as a domain transfer benchmark based on ImageNet, the proposed algorithm is shown to effectively identify and query informative samples, leveraging them to enhance test-time inference capabilities.", "tldr": "", "keywords": ["Active Test-Time Adaptation", "Vision-Language Model", "Retrieval-Augmented"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0c86051f3be687309b4d13789ff303066f58962f.pdf", "supplementary_material": "/attachment/029844bf5f602578881e3e7abe2013f9d670f73d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces extra information from an external database during test-time adaptation via retrieval. The method is designed as a plug-and-play module for various  TTA methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-organized."}, "weaknesses": {"value": "- This paper does not discuss the difference between some highly relevant papers.\n- No analysis of the influence of different external database choices.\n- Some arguments in the paper need further justification."}, "questions": {"value": "- Retrieval-augmented TTA is not something new. It is a pity that the paper ignores some highly relevant papers with similar ideas. Please see papers [1,2]. Considering these existing works, the reviewer thinks the contribution of this paper needs further justification. \n\n\n- In lines 074-076, the paper claims \"simATTA (Gui et al., 2024) was the first work that emphasized the necessity of extra information for TTA\". If we consider the existence of [1], should we modify the argument?\n\n- In lines 076-078, the paper claims \"However, this training-based approach accompanies high computational overhead and likely incurs catastrophic forgetting that undermines the knowledge from pre-training data.\" Can we directly reset the parameters of the TTA model to avoid catastrophic forgetting, as many TTA works, like TPT?\n\n- In Table 1, the improvement of REACT is relatively trivial compared to that in Table 2. What is the reason behind this? The nature of the used external database or something others.\n\n- The reviewer does not find the discussion about different external retrieval sources/databases. Any discussion on this point would be great.\n\n[1] Zancato et al. Train/Test-Time Adaptation with Retrieval. https://arxiv.org/pdf/2303.14333\n\n[2] Lee et al. RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models. ICLR 2025. https://openreview.net/pdf?id=V3zobHnS61."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XrI9fUvtL8", "forum": "snbY9Uj0Gx", "replyto": "snbY9Uj0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission5479/Reviewer_qyM8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5479/Reviewer_qyM8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465230403, "cdate": 1761465230403, "tmdate": 1762918086361, "mdate": 1762918086361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "After careful consideration of the reviewers' comments, we have decided to withdraw our paper from further consideration at this time."}}, "id": "d1XYwixCnB", "forum": "snbY9Uj0Gx", "replyto": "snbY9Uj0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission5479/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5479/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762951772410, "cdate": 1762951772410, "tmdate": 1762951772410, "mdate": 1762951772410, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes REACT, a plug-and-play retrieval-augmented active test-time adaptation framework for pre-trained vision-language models (VLMs) such as CLIP. REACT aims to improve robustness against distribution shift by selectively querying human labels for uncertain samples and correcting future predictions via retrieval from an incrementally constructed labeled database. The approach is training-free and broadly compatible with diverse TTA pipelines. The authors empirically demonstrate REACT’s effectiveness on both out-of-distribution and cross-domain benchmarks, showcasing consistent improvements over strong TTA baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generalizable Plug-and-play Design: The method is training-free and broadly compatible with existing TTA frameworks. \n\n2. Clear Motivation and Context: The paper provides a thorough explanantion of how pseudo labels can fail under distribution shift, and why selective active querying can alleviate the issue. Figures were used to effectively illustrate these concepts, showing scenarios where retrieval-based correction is likely to help or fail.\n\n3. Comprehensive Experiments and Solid Results: The method is rigorously evaluated on multiple VLM architectures (ResNet50, ViT-B/16, BLIP-2, SigLIP), broad OOD and cross-domain benchmarks, and compared to a range of baselines including TTA, active learning, and prompt learning methods. Additionally, the paper provides visual analysis of decision boundaries and correction effects, runtime inference cost analysis and hyperparameter sensitivity tests."}, "weaknesses": {"value": "1. Limited Empirical Scope in Failure Cases: The paper focuses on positive results and does not sufficiently analyze or visualize where REACT fails across diverse domain shifts, class imbalance, or when labeled examples are misleading. It would benefit from explicit breakdowns and more detailed diagnostic visualizations of such cases.\n\n2. Lack of Differentiation from Training-based Methods: The comparison with training-based methods such as simATTA and prompt learning (e.g., CoOp) is made, but it seems counter-intuitive that REACT’s consistent outperforms those methods, and the underlying reason is not deeply explored. Could the benefits come from specific tuning or implementation details?\n\n3. Heuristic Nature of Hyperparameters: The framework’s main metrics, consistency and reliability, relies on two thresholds ($\\tau_u$, $\\tau_r$). Ablations show that the method is somewhat robust, but there is little justification for the choice of these thresholds. The method could benefit from attempts to learn or adapt the hyperparameters for deployment."}, "questions": {"value": "1. How does REACT scale with the size of the labeled set, particularly when test streams are long?\n\n2. What is the impact if the oracle labeling budget is used up well before the stream ends? Are there mechanisms for budget allocation over time, like prioritizing late-stage samples?\n\n3. In practice, how should $\\tau_u$ and $\\tau_r$ be set, and can the system adapt or learn these thresholds in an online manner rather than choosing a fixed value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rxDLJNYeZf", "forum": "snbY9Uj0Gx", "replyto": "snbY9Uj0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission5479/Reviewer_8oXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5479/Reviewer_8oXY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836318233, "cdate": 1761836318233, "tmdate": 1762918085979, "mdate": 1762918085979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REACT, a plug-and-play framework for active test-time adaptation of vision-language models (VLMs) that reduces reliance on noisy pseudo-labels by selectively querying human experts for ground-truth labels on uncertain samples. Instead of retraining the model, REACT builds a growing database of labeled examples and uses retrieval-augmented correction—guided by consistency and reliability criteria between neighboring labeled samples—to improve predictions on future test inputs. The approach significantly boosts performance across multiple VLMs and benchmarks (like ImageNet variants and cross-domain datasets) with negligible computational overhead, offering an efficient, training-free alternative to existing test-time adaptation methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- REACT introduces a novel, training-free framework for active test-time adaptation of vision-language models that significantly improves robustness under distribution shifts without requiring any model retraining.\n- The plug-and-play design seamlessly integrates with any existing TTA method (e.g., TPT, TDA), delivering consistent performance gains across diverse benchmarks (OOD and cross-domain) with minimal computational overhead (<1 ms per sample).\n- The concept of actively seeking external knowledge is intriguing, reminiscent of how humans approach and solve complex questions."}, "weaknesses": {"value": "- The framework relies on access to ground-truth labels during test time, which fundamentally shifts the problem from zero-shot test-time adaptation to a few-shot setting, undermining claims of \"zero-shot\" generalization and making comparisons with purely unsupervised TTA methods unfair.\n- The retrieval-augmented correction mechanism closely resembles memory-augmented or nearest-neighbor approaches like Tip-Adapter, offering incremental improvements without introducing novel architectural or representational innovations.\n- The requirement for a labeled buffer built via active querying introduces practical deployment barriers in real-world scenarios where human-in-the-loop annotation is costly, slow, or infeasible, limiting real-world applicability despite claims of “plug-and-play” efficiency.\n- The evaluation focuses on benchmark datasets where class sets are predefined and closed, failing to address the more challenging case of truly unseen or open-ended classes — the very problem TTA aims to solve.\n- The paper conflates active learning with test-time adaptation; its core contribution is better framed as a streaming, semi-supervised refinement method rather than a true adaptation of the model’s internal representation under distribution shift."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "edUzzfU82L", "forum": "snbY9Uj0Gx", "replyto": "snbY9Uj0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission5479/Reviewer_FboZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5479/Reviewer_FboZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895571470, "cdate": 1761895571470, "tmdate": 1762918085328, "mdate": 1762918085328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}