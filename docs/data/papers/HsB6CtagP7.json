{"id": "HsB6CtagP7", "number": 169, "cdate": 1756730154009, "mdate": 1763657733458, "content": {"title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models", "abstract": "Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.", "tldr": "We find that diffusion language models hide useful answers mid-generation and introduce simple voting and reinforcement learning methods that exploit the temporal dynamics to boost accuracy.", "keywords": ["Diffusion Language Models", "Semantic Entropy", "Self-Consistency", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbec3310329e6d64fab23fbc47456e46694e843d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses temporal oscillation in dLLMs, where correct answers often appear during intermediate denoising steps but are later overwritten. Building on insights from self-consistency and semantic entropy, the authors propose two complementary approaches: (1) Temporal Self-Consistency Voting, a training-free decoding method that aggregates intermediate predictions, and (2) Temporal Consistency Reinforcement, a post-training method using GRPO with Temporal Semantic Entropy as a reward to promote stable generations. Experiments on mathematical reasoning benchmarks demonstrate consistent improvements, with particularly strong gains when TSE is combined with accuracy-based rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The framing of temporal oscillation as a central issue in dLLM decoding and devising a way of extracting signal from the intermediate outputs is both novel and timely given the emerging importance of dLLMs.\n- This work presents strong empirical evidence, including ablation studies on reward design and weighting strategies.\n- Newly introduced metrics, such as EverPass and TSE, are intuitive and well-motivated, with their significance demonstrated through clear and quantitative analysis."}, "weaknesses": {"value": "- While the evaluation spans four datasets (GSM8K, MATH500, SVAMP, Countdown), this focuses primarily on mathematical reasoning tasks. Extending the analysis to other domains (e.g., natural language QA) would strengthen the case for the generalizability of this approach.\n- The success of the introduced strategies, inference-time voting as well as RL fine-tuning, appears to rely heavily on the baseline model's accuracy, which may suggest limited gains for harder reasoning tasks to expand the applicability of dLLMs broadly (although this point is mentioned in Appendix D.1).\n\nI also note the existence of a concurrent work (https://arxiv.org/abs/2508.19982) on closely related problem and method, while it assumes early stabilization of the prediction and proposes stopping criteria for training-free approach. This is not considered in this review, since they are concurrent works."}, "questions": {"value": "- Since temporal oscillation arises from the combination of remasking and incorrect denoising, it could be in principle be mitigated by a modified diffusion schedule. How much of the observed oscillation is inherent to dLLMs (and thus requires methods like those proposed here), versus being reducible by scheduling or inference design choices?\n- The use of TSE in this work is empirically motivated, and it would be possible to consider other types of temporal consistency metrics or rewards, for example, averaging the pairwise agreement between consecutive timesteps of a (later) denoising trajectory. What is the empirical advantage of using entropy-based methods over such alternatives?\n\nMinor typos/comments\n- p. 8 Paragraph \"Ablations on Voting Strategies.\" Fig. 5b -> Fig. 5a\n- p. 4 Eq (1). The use of $x_0$ on the RHS is inconsistent with the LHS. Given the later definition of $x_0^t$, it would be more precise to write $\\sum_{x_0^t} q(x_{t-1}|x_0^t) p_\\theta (x_0^t | x_t)$? Also, the later decoding step would be $x_0^t \\sim p_\\theta(x_0^t | x_t)$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MTJftSQtnV", "forum": "HsB6CtagP7", "replyto": "HsB6CtagP7", "signatures": ["ICLR.cc/2026/Conference/Submission169/Reviewer_39aQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission169/Reviewer_39aQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445115570, "cdate": 1761445115570, "tmdate": 1762915461706, "mdate": 1762915461706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals a critical phenomenon for diffusion language models: temporal oscillation. The authors observed that when answering a question, diffusion language models may generate a correct answer in the middle but replace it with an incorrect answer in the final output. The authors also conducted an empirical study to quantify this phenomenon. Based on this observation, the authors proposed two methods to enhance performance for diffusion language models: temporal self-consistency voting, a training-free majority voting method with minimal computational overhead; and temporal consistency reinforcement, a GRPO reinforcement learning algorithm that improves semantic consistency during post-training. For both methods, the authors observe improvements in accuracy across four reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is well-written and easy to follow.\n* This paper handles an interesting phenomenon that is distinct for diffusion language models. The authors conducted a quantitative analysis of temporal oscillations and proposed practical guidelines for leveraging them to improve the reasoning performance of diffusion language models.\n* The experimental section of this paper is relatively abundant, with many ablations and extra analyses."}, "weaknesses": {"value": "* The mathematical notation system is not rigorous and self-consistent. In equation 1, the authors tried to demonstrate the sampling algorithm of LLaDA-like diffusion language models. Although I understand what the authors were trying to convey, Eq. 1 is incorrect and confusing from a probabilistic perspective, and I strongly advise the authors to revise its presentation. Throughout the paper, the authors made no distinction between scalar values and tensors. For example, the $x_t$ in Eq. 1 represents a token sequence and is neither bolded nor indicated as a tensor. In Eq.3, it is not clear what is meant by $p(x)$, despite its importance. In Eq. 5, a minus sign is missing when it means a loss, but not when it means the policy gradient.\n* In Figure 4, it was not indicated what the error bars meant. I do not understand why the error bar for the correct column for SVAMP dips below 0, since the entropy values should not be negative.\n* In Table 1, the improvements of temporal majority voting seem marginal. I think more justifications, such as error bars, are needed to prove that these improvements are real but not random noise."}, "questions": {"value": "* My biggest question is how the authors extract answers for intermediate token sequences. This is confusing because for LLaDA, once a token is decoded, it does not change throughout the sampling process, unless predictor-corrector samplers such as ReMDM are used. Thus, it is counterintuitive that the time oscillation has such a significant influence on the model's performance, as shown in Figure 1(a). Besides, it is not clear how to extract an answer from a partially masked token sequence such as \"[MASK] [MASK] the [MASK]\". Can the authors explain this in more detail, because it is clear that this is vital for both temporal self-consistency voting and the computation of TSE.\n* Can the authors explain why TSE is chosen as the metric to use? The current explanation is Figure 4, but such a phenomenon holds true for many other metrics. For temporal consistency reinforcement, how will the performance be if TSE is replaced with token entropy? If they perform similarly, it is not clear why we should choose TSE. If they perform differently, can the authors provide some justification for why TSE is a good metric?\n* What algorithms do the authors use for clustering when computing TSE? Is TSE robust to different clustering algorithms? \n* Does temporal consistency reinforcement improve ever pass as well? What are the results if we apply temporal self-consistency voting to both the original checkpoint and the checkpoint after consistency reinforcement?\n* In previous works such as d1, and wd1, Sudoku is also a commonly used benchmark. Can the authors provide results on the Sudoku benchmark to further justify the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PoxtMByb8U", "forum": "HsB6CtagP7", "replyto": "HsB6CtagP7", "signatures": ["ICLR.cc/2026/Conference/Submission169/Reviewer_B4qA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission169/Reviewer_B4qA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673076134, "cdate": 1761673076134, "tmdate": 1762915461541, "mdate": 1762915461541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates temporal oscillation in diffusion language models—where correct intermediate answers are later overwritten—and proposes two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, to leverage this phenomenon. The work is well-motivated and empirically strong, showing consistent improvements across benchmarks, with large gains on Countdown. However, methodological novelty is limited: TSCV mainly adapts prior self-consistency ideas, and the proposed TSE metric does not directly capture temporal dynamics but instead reflects semantic stability. The related work section could be more focused on studies addressing similar temporal behaviors. Overall, solid experiments and clear presentation justify a weak accept."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1 This paper proposes leveraging the rich intermediate hidden states generated during the diffusion process—not only for inference-time aggregation but also for post-training reinforcement. This breaks away from the conventional paradigm that focuses solely on the final denoised output and opens a new avenue for understanding and enhancing dLLMs.\nS2 The authors define Temporal Semantic Entropy to quantify semantic stability during the generation process and further incorporate it as a self-supervised reward signal in reinforcement learning. This method consistently improves performance across datasets, with especially notable results on the Countdown task, validating the effectiveness of temporal consistency signals in improving both stability and accuracy.\nS3 The experimental results are strong and convincing. The proposed methods deliver clear and consistent gains across multiple reasoning benchmarks (GSM8K, MATH500, SVAMP, Countdown) and across different model scales (LLaDA-8B, LLaDA-1.5). In particular, the Temporal Consistency Reinforcement achieves up to 25 percentage-point improvement on the Countdown dataset, substantially outperforming prior diffusion-based baselines.\nS4 The evaluation is comprehensive and well-controlled, including ablations over weighting schemes, reinforcement reward formulations, and entropy metrics. The improvements are not marginal but significant and stable across output lengths and datasets, demonstrating the general applicability of the proposed approach."}, "weaknesses": {"value": "W1 The proposed Temporal Self-Consistency Voting essentially adapts the self-consistency idea from Self-Consistency Improves Chain-of-Thought Reasoning in Language Models (Wang et al., 2022) to diffusion models. This adaptation lacks substantial algorithmic innovation, and the observed performance gains are smaller than those achieved in autoregressive models—raising doubts about the method’s unique contribution within the diffusion framework.\nW2 The proposed TSE metric is based on semantic clustering to measure semantic variation across denoising steps. However, this metric reflects the stability of latent semantic distributions rather than true temporal dynamics. Since the clustering process depends on embedding space similarity rather than the evolution of diffusion timesteps, TSE may not adequately represent temporal consistency.\nW3 In Section 4.1, the paper attributes the efficiency and scalability of Temporal Self-Consistency Voting to its design. However, these “efficient” properties largely arise from the inherent parallel generation architecture of diffusion models rather than the proposed method itself. In other words, the ability to obtain multiple intermediate outputs within a single sampling trajectory is a natural feature of diffusion mechanisms, not a unique contribution of this approach.\nW4 The Related Work section contains a substantial amount of content describing the general development of diffusion models and reinforcement learning variants, but these discussions are only tangentially related to the paper’s specific focus on temporal oscillation and consistency. This makes the literature review feel somewhat disconnected from the central contribution and could be tightened to highlight works directly addressing intermediate-step dynamics or temporal consistency.\nW5 While the experimental results are quantitatively strong, the paper provides limited qualitative or diagnostic analysis of why temporal oscillation occurs and under what conditions correct intermediate answers are lost. Without a clearer causal explanation or visualization of representative trajectories, it remains difficult to interpret whether the proposed improvements stem from genuine temporal stabilization or simply from additional ensembling and regularization effects."}, "questions": {"value": "Q1 The claimed “temporal oscillation” phenomenon requires more systematic experimental validation to support its research significance. Autoregressive models can also produce correct intermediate answers but incorrect final outputs, typically due to decoding uncertainty or local likelihood fluctuations. Similarly, oscillations observed in diffusion models may simply result from randomness or resampling noise during denoising. To justify the proposed methods, the authors should provide additional evidence showing that the phenomenon is structural and exploitable rather than a random artifact.\nQ2 The strong performance of TSE-based reinforcement is primarily observed on the Countdown dataset, while its effects on GSM8K, MATH500, and SVAMP are comparable to those of TSCV. If TSE is only effective for specific tasks or distributions, its generalizability remains questionable. The authors are encouraged to extend their experiments to a broader range of datasets and task types to better demonstrate the method’s robustness and applicability.\nQ3 The Related Work section could be revised to better connect with the paper’s central theme. In particular, the authors are encouraged to compare their study with more recent works that explicitly discuss similar temporal phenomena in diffusion models. For example, Diffusion Language Models Know the Answer Before Decoding also reports that correct answers often appear in intermediate denoising steps but are overwritten in later iterations—a phenomenon conceptually similar to the “temporal oscillation” described in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PhZ2bDSSZO", "forum": "HsB6CtagP7", "replyto": "HsB6CtagP7", "signatures": ["ICLR.cc/2026/Conference/Submission169/Reviewer_TojT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission169/Reviewer_TojT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992470706, "cdate": 1761992470706, "tmdate": 1762915461299, "mdate": 1762915461299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates temporal dynamics in diffusion language models (dLLMs), highlighting that correct intermediate predictions are often lost during later denoising steps. The authors identify a “temporal oscillation” phenomenon and propose two complementary methods: (1) Temporal Self-Consistency Voting, a test-time decoding strategy that aggregates predictions across steps, and (2) Temporal Consistency Reinforcement, a post-training method using Temporal Semantic Entropy (TSE) as a reward to encourage stable generations. Experiments show consistent improvements across reasoning benchmarks, suggesting that temporal signals within diffusion decoding are a valuable yet underexplored source of information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new metric, Temporal Semantic Entropy (TSE), to quantify semantic fluctuation across denoising steps, revealing meaningful patterns in dLLMs.\n\n2. Proposes both training-free (voting-based decoding) and training-based (RL with TSE reward) methods, offering complementary ways to leverage temporal consistency.\n\n3. Demonstrates that TSE can serve as a soft reward signal for RL even in unlabeled scenarios, expanding applicability to broader settings."}, "weaknesses": {"value": "1. In Table 2, combining accuracy reward and TSE sometimes leads to negative effects under RFT, indicating potential instability in multi-reward optimization.\n\n2. After RFT, the model’s everPass@1 performance decreases for some tasks (MATH500 and SVAMP), as shown in Table 1 and S4.\n\n3. Using TSE as an RL reward requires semantic clustering, which introduces extra computation overhead not discussed in the paper.\n\n4. The paper lacks clarity about which decoding strategy is used in the analysis experiments (e.g., random remarking, low-confidence remasking, or block decoding). It appears analysis assumes full diffusion (len=128, steps=64), whereas RL experiments adopt semi-autoregressive decoding—raising concerns about experimental consistency."}, "questions": {"value": "1. In Figure 3, is the average token-level entropy computed over the last-step tokens only or all tokens across trajectories?\n\n2. In Figure 4, why not separately report metrics for \"finally correct\" versus \"intermediate correct\" cases to better illustrate temporal oscillation behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cfQ1Ak1htO", "forum": "HsB6CtagP7", "replyto": "HsB6CtagP7", "signatures": ["ICLR.cc/2026/Conference/Submission169/Reviewer_CBgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission169/Reviewer_CBgE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993691801, "cdate": 1761993691801, "tmdate": 1762915461082, "mdate": 1762915461082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}