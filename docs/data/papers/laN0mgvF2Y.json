{"id": "laN0mgvF2Y", "number": 3204, "cdate": 1757367304913, "mdate": 1759898102162, "content": {"title": "Revisiting Mixture Policies in Entropy-Regularized Actor-Critic", "abstract": "Mixture policies in reinforcement learning offer greater flexibility compared to their base component policies. We demonstrate that this flexibility, in theory, enhances solution quality and improves robustness to the entropy scale. Despite these advantages, mixtures are rarely used in algorithms like Soft Actor-Critic, and the few empirical studies that are available do not show their effectiveness. One possible explanation is that base policies, like Gaussian policies, admit a reparameterization that enables low-variance gradient updates, whereas mixtures do not. To address this, we introduce a marginalized reparameterization (MRP) estimator for mixture policies that has provably lower variance than the standard likelihood-ratio (LR) estimator. We conduct extensive experiments across a large suite of synthetic bandits and environments from classic control, Gym MuJoCo, DeepMind Control Suite, MetaWorld, and MyoSuite. \nOur results show, for the first time, that mixture policies trained with our MRP estimator are more stable than the LR variant and are competitive compared to Gaussian policies across many benchmarks. In addition, our approach shows benefits when the critic surface is multimodal and in tasks with unshaped rewards.", "tldr": "We study the benefit and reparameterization of mixture policies in entropy-regularized reinforcement learning with continuous action spaces.", "keywords": ["policy parameterization", "reparameterization", "entropy regularization", "actor-critic", "policy optimization", "exploration", "continuous control", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/031b70a2e43756b143b084a2f42650c490b450c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematically analyzes the benefits of Mixture Policies over standard Gaussian Policies in the context of entropy-regularized Reinforcement Learning (RL), specifically addressing the major algorithmic barrier to their adoption.\n\nThe authors theoretically analyze the advantages of mixture policies over Gaussian policies, demonstrating their improved robustness to the entropy scale. Crucially, they introduce the Marginalized Reparameterization (MRP) estimator for Gaussian Mixture (GM) policies and provide a theoretical proof of its lower variance compared to the standard Likelihood-Ratio (LR) estimator. Finally, the work comprehensively validates the advantages of mixture policies across an extensive suite of benchmarks.\n\nWhile the application of multimodal policies, including Gaussian Mixture Policies, to enhance RL exploration capabilities and final performance has been explored in prior work, it often lacked systematic theoretical grounding and extensive empirical validation. This paper successfully fills this gap by providing a much-needed comprehensive study."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear Structure and Readability: The paper is very well-structured, making the theoretical analysis and empirical results clear and easy to follow. \n\n\nSystematic Analysis of Policy Flexibility: The work provides a systematic analysis of the theoretical benefits of mixture policies over unimodal Gaussian policies, especially concerning the non-existence of stationary points and superior objective values under high entropy regularization. \n\n\n\n\nComprehensive Experimental Validation: The authors conducted extensive and systematic experiments across a large and diverse set of synthetic bandits, classic control, Gym MuJoCo, DeepMind Control, MetaWorld, and MyoSuite environments."}, "weaknesses": {"value": "Limited Algorithmic Novelty of Mixture Policies:  The application of mixture policies in RL is a topic that has been explored, even briefly in early versions of Soft Actor-Critic (SAC). The core contribution lies in enabling this class via the MRP estimator, not the policy parameterization itself. \n\nInsufficient General Performance Gain: Despite the broad and systematic experiments, the performance improvement of mixture policies (SGM-MRP) over the standard Gaussian policy (SG-RP) is generally modest or only competitive on major benchmarks. The choice of \"unshaped reward\" examples to demonstrate superiority (Section 5.3) could be further strengthened. Consideration should be given to including simpler toy environments designed explicitly with known multi-modal optimal policies to provide a clearer and more persuasive visual demonstration of the policy's multi-modality advantage.\n\nLimited Sensitivity Analysis on Component Number: For Gaussian Mixture Policies, the number of components ($N$) is a crucial hyperparameter that dictates policy complexity. The sensitivity analysis for it is confined to a simple set of classic control environments with unshaped rewards.  A more convincing analysis involving at least one high-dimensional environment is needed to confirm the generality of the finding that $N=5$ (or small $N$) is sufficient."}, "questions": {"value": "Noticed that the SGM-MRP estimator failed to converge in environments such as disassemble-v2 and stick-pull-v2. Could you provide a plausible explanation for why the MRP estimator, which is proven to be stable and low-variance, exhibits non-convergence or poor performance in these specific environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yDFMYs2tMF", "forum": "laN0mgvF2Y", "replyto": "laN0mgvF2Y", "signatures": ["ICLR.cc/2026/Conference/Submission3204/Reviewer_UC5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3204/Reviewer_UC5u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919595019, "cdate": 1761919595019, "tmdate": 1762916600450, "mdate": 1762916600450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the use of mixture policies in entropy-regularized actor–critic reinforcement learning and provides both theoretical and empirical evidence for their benefits.\nThe authors show that mixture policies lead to better or more robust stationary solutions under entropy regularization and propose a Marginalized Reparameterization (MRP) gradient estimator that reduces variance in training.\nExperiments across a wide range of continuous-control benchmarks demonstrate consistent, though modest, improvements in performance and stability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical results are internally consistent and logically organized.\nIn particular, Proposition 3.3 provides a novel robustness argument showing that Gaussian base policies may lose stationary points when the entropy coefficient $\\alpha$ exceeds $\\tfrac{3}{2}r_{\\max}$, whereas Gaussian mixture (GM) policies continue to maintain valid stationary solutions.\nThis insight connects entropy regularization and multimodal policy landscapes in a clean way.\n\n2. The optimality results (Propositions 3.1–3.2) extend known properties of entropy-regularized optimization, while the robustness to entropy scaling (Propositions 3.3–3.4) and the variance-reduction guarantees for the MRP estimator (Theorem 4.3, Proposition 4.7) represent genuine theoretical value.\nTogether, they strengthen our understanding of mixture policies in a principled manner.\n\n3. The experiments cover a wide range of continuous-control benchmarks.\nThe results convincingly demonstrate that mixture policies improve exploration and stability, especially under high-entropy or multimodal reward settings."}, "weaknesses": {"value": "Some of the theoretical contributions are mainly extensions of established analysis rather than entirely new formulations.\nThe results build on well-known principles of entropy-regularized optimization and existing variance-reduction techniques, providing thoughtful refinements rather than foundational changes.\nThat said, the extensions are clearly presented and meaningfully deepen the understanding of mixture policies in entropy-regularized reinforcement learning."}, "questions": {"value": "1. How sensitive are the results to the number of mixture components $K$ and to entropy coefficient $\\alpha$?\n2. Could similar robustness hold for non-Gaussian or discrete mixture families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03ziMvvwdr", "forum": "laN0mgvF2Y", "replyto": "laN0mgvF2Y", "signatures": ["ICLR.cc/2026/Conference/Submission3204/Reviewer_o9kL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3204/Reviewer_o9kL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974119945, "cdate": 1761974119945, "tmdate": 1762916599970, "mdate": 1762916599970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes mixture policies in entropy-regularized actor-critic algorithms, addressing the long-standing issue that mixture policies have not been practically effective due to the lack of efficient reparameterization (RP) gradient estimators. This paper propose a Marginalized Reparameterization (MRP) estimator for mixture policies, which is proven to have lower variance than the standard likelihood-ratio (LR) estimator. Through extensive experiments across synthetic bandits and diverse continuous control benchmarks, the paper demonstrates that mixture policies trained with MRP are stable, competitive with Gaussian policies, and particularly useful in environments with multimodality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides rigorous theoretical analysis, including proofs that mixture policies achieve better or comparable objective values than base Gaussian policies and are more robust to high entropy regularization.\n\nThis paper covers a wide range of environments, from synthetic bandits to complex robotic control tasks to demonstrate effectiveness."}, "weaknesses": {"value": "Mixture policies require more parameters (e.g., 5-component policies have 15 outputs vs. 2 for base Gaussian policies) but this paper does not provide a detailed analysis of computational costs\n\nThis paper briefly contrasts mixture policies with implicit policies (e.g., diffusion models) but does not include empirical comparisons on benchmarks.\n\nWhile this paper theoretically proves that the MRP estimator has lower variance than the likelihood-ratio (LR) estimator, it overlooks practical constraints of MRP. For instance, marginalizing over mixture components may implicitly amplify the impact of outlier components (e.g., components with extremely low weights but large parameter deviations), which could introduce hidden instability in long-term training. Additionally, the paper does not test MRP's robustness to hyperparameter variations.\n\nThe ablation study on component numbers (2, 5, 8) shows noisy results but does not address the risk of component collapse. This paper does not report whether components retain distinct roles (e.g., specializing in different sub-policies) throughout training or if they degenerate into redundancy. This ambiguity undermines claims about the mixture policy's flexibility in exploring diverse action modes."}, "questions": {"value": "Could you provide a more detailed analysis of the computational overhead (training/inference time, memory usage) of mixture policies with MRP compared to standard Gaussian policies across different environments?\n\nGiven the noisy results on the effect of component numbers, do you have any heuristic or theoretical guidance for selecting the optimal number of components for a given task?\n\n Why the multimodal exploration of mixture policies is said to be more effective than the exploration of Gaussian policies in such settings? Can you given a more detailed explanation?\n\nThe MRP estimator relies on marginalizing over mixture components. Have you observed cases where outlier components (with low weights but extreme parameter values) distort the gradient signal, and if so, how might this be mitigated?\n\nDiffusion policies have shown stronger multimodal modeling capabilities than GMMs in robotic tasks , particularly in position-controlled systems. Could you compare the mixture policy’s performance with diffusion policies on more continuous control tasks?\n\nComponent collapse is a known issue in GMMs. Did you track the divergence of component parameters during training? If components converged to similar distributions, how does this affect the mixture policy’s ability to explore diverse actions, and what safeguards can be added?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3UVrtZNqyZ", "forum": "laN0mgvF2Y", "replyto": "laN0mgvF2Y", "signatures": ["ICLR.cc/2026/Conference/Submission3204/Reviewer_dsgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3204/Reviewer_dsgq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160161977, "cdate": 1762160161977, "tmdate": 1762916599460, "mdate": 1762916599460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the use of mixture policies in entropy-regularized reinforcement learning (RL), specifically within Soft Actor-Critic (SAC). The authors argue that mixture policies offer greater flexibility than unimodal policies (e.g., Gaussian) but have been underexplored due to the lack of effective reparameterization gradient estimators. The paper makes three main contributions: (1) theoretical analysis showing that mixture policies achieve better optimal stationary points and exhibit greater robustness to entropy regularization as compared to Gaussian policies; (2) proposing Marginalized Reparameterization (MRP) estimator, which marginalizes over mixture weights to provide an unbiased, low-variance gradient estimator; and (3) empirical validation across synthetic bandits, classic control, and large-scale benchmarks. The results demonstrate that mixture policies with the MRP estimator are competitive with or superior to Gaussian policies on standard benchmarks, with significant improvements in environments with unshaped rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n- The theoretical results for the marginalized reparameterization (MRP) estimator provide novel insights into how policy parameterization affects stationary points of the non-convex entropy-regularized objective.\n- Propositions 3.1-3.4 rigorously establish that mixture policies achieve at least as good or better stationary points compared to base policies and retain stationary points under higher entropy regularization, where Gaussian policies diverge.\n\nQuality\n-  The experimental evaluation is comprehensive and methodologically rigorous, spanning across diverse domains with appropriate statistical reporting, including 95% bootstrap confidence intervals. \n\nClarity\n- The paper is clearly written with logical progression from motivation through theory to empirical validation, along with a discussion on the limitations.\n\nSignificance\n-  The finding that mixture policies significantly outperform base policies in unshaped-reward environments provides valuable practical guidance about when mixture policies are helpful."}, "weaknesses": {"value": "1. Assumptions 4.5 and 4.6 in Proposition 4.7 require specific smoothness properties of the reward function and importance sampling variance relationships that are neither verified empirically nor characterized in terms of when they hold in practice. The variance reduction analysis focuses on multimodal bandits and univariate actions, with only a remark (Remark 4.9) suggesting multivariate extension is possible. The gap between the bandit theory and MDP experiments is substantial, and it remains unclear whether the variance reduction guarantees meaningfully apply to the complex high-dimensional control tasks tested.\n\n2. Figure 4 shows that SGM-MRP (mixture policy) is only marginally better on average across MuJoCo, DMC, MetaWorld, and MyoSuite, with the main benefits concentrated in specific MetaWorld tasks. Given that the experiments use hyperparameters from the SAC paper,  the gains might improve with proper tuning, yet the paper does not investigate this.\n\n3. The paper does not report the computational overhead of the MRP estimator compared to standard RP for Gaussian policies, which might be critical for practical adoption. The choice of five components appears arbitrary, and while Appendix F.3 provides limited ablation, there is no principled guidance on selecting the number of components for a given task. \n\n4. The paper does not compare against other flexible policy classes like beta policies, heavy-tailed policies, or recent implicit policy methods beyond a brief discussion in the introduction and related work. \n\n5. The claim that mixture policies enable \"mode-directed exploration\" is intuitive but not rigorously quantified through metrics such as state coverage or exploration efficiency, for instance, in toy gridworld domains."}, "questions": {"value": "1. Can the authors empirically validate Assumptions 4.5 and 4.6 on representative tasks from the considered benchmarks? Specifically, is it easy/difficult to verify whether the reward functions satisfy the required smoothness conditions and whether the importance sampling variance relationships hold during training?\n\n2. What is the computational overhead of the MRP estimator compared to the standard RP estimator for Gaussian policies? Please provide wall-clock time comparisons across benchmarks.\n\n3. Can the authors provide principled guidance on selecting the number of mixture components, perhaps based on task characteristics such as action dimensionality, reward structure, and/or state space complexity?\n\n4. Can the authors provide quantitative metrics for exploration efficiency, such as state coverage or diversity of trajectories, to rigorously validate the claim that mixture policies enable better mode-directed exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rWUKTtquRq", "forum": "laN0mgvF2Y", "replyto": "laN0mgvF2Y", "signatures": ["ICLR.cc/2026/Conference/Submission3204/Reviewer_3Vf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3204/Reviewer_3Vf3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762326254114, "cdate": 1762326254114, "tmdate": 1762916599162, "mdate": 1762916599162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}