{"id": "YKxiut90ju", "number": 19471, "cdate": 1758296522269, "mdate": 1763399913105, "content": {"title": "On the Relationship Between the Choice of Representation and In-Context Learning", "abstract": "In-context learning (ICL) is the ability of a large language model (LLM) to learn a new task from a few demonstrations presented as part of the context. Past studies have attributed a large portion of the success of ICL to the way these in-context demonstrations are represented, particularly to how labels are represented. On the other hand, observations of the learning capacity of ICL (i.e., the extent to which more in-context demonstrations can lead to higher performance) have been mixed, and ICL is often thought to occur only under specific conditions. Furthermore, the interaction between these two aspects in ICL, representation and learning, has not been studied in depth until now. We hypothesize that they are largely independent of one another, such that the representation of demonstrations determines the baseline accuracy of ICL, while learning from additional demonstrations improves only on top of this baseline. We validate this hypothesis by developing an optimization algorithm that can enumerate a spectrum of possible label sets (representations) varying in semantic relevance. We then perform ICL with varying numbers of in-context demonstrations for each of these label sets. We observed that learning happens regardless of the quality of the label set itself, although its efficiency, measured by the slope of improvement over in-context demonstrations, is conditioned on both the label set quality and the parameter count of the underlying language model. Despite the emergence of learning, the relative quality (accuracy) of the choice of a label set (representation) is largely maintained throughout learning, confirming our hypothesis and implying their orthogonality. Our work reveals a previously underexplored aspect of ICL: the independent effects of learning from demonstrations and their representations on ICL performance.", "tldr": "Learning from demonstrations and the choice of label set representation independently affect in-context learning performance.", "keywords": ["in-context learning", "large language models", "representations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0c02403cf2be69d2bfbf36b8ed1ad78992c85b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the interplay between label representation and in-context learning (ICL) in LLMs, positing their independence: representation establishes baseline accuracy, while learning builds upon it. It introduces an optimization algorithm to generate label sets of varying semantic relevance, tested on sentiment classification with Llama models (1B, 8B, 70B parameters). Results confirm consistent label rankings, representation-constrained accuracy, and learning influenced by model size and label quality. It's a valuable addition to ICL research with innovative methods, but restricted to classification and synthetic data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Rigorous hypothesis formulation. The independence of representation (zero-shot baseline) and demonstration learning resolves prior ICL inconsistencies (e.g., Pan et al., 2023; Kirsanov et al., 2025), offering a logical framework for prompt decomposition.\n\n2. Innovative methodological approach. Hill-climbing optimization enumerates label sets by semantic relevance, surpassing binary gold/abstract schemes for precise representation quantification in classification.\n\n3. Comprehensive empirical analysis. Varies demonstrations (up to 100), model scales, and tasks (3/5-way), using slopes, correlations, and bootstrapping to prove orthogonality and reveal model/label conditioning on efficiency."}, "weaknesses": {"value": "1. Limited dataset diversity. Reliance on a single synthetic sentiment dataset (1,000 sentences) may not capture real-world variability, potentially overstating the generalizability of findings.\n\n2. Optimization algorithm simplicity. The hill-climbing approach for label enumeration, while effective, lacks comparison to more advanced methods (e.g., genetic algorithms) and may miss global optima.\n\n3. Incomplete resource reporting. No details on computational costs, such as training epochs, hardware requirements, or runtime, hindering reproducibility for resource-constrained researchers.\n\n4. Narrow task scope. Experiments focus solely on 3-way and 5-way classification; no exploration of multi-label, regression, or non-text tasks to assess broader ICL implications.\n\n5. Underexplored edge cases. While overlap in label semantics is noted, the paper does not address or mitigate potential biases from prompt formatting variations or noisy demonstrations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uL8KZfeOg0", "forum": "YKxiut90ju", "replyto": "YKxiut90ju", "signatures": ["ICLR.cc/2026/Conference/Submission19471/Reviewer_sH7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19471/Reviewer_sH7g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053845201, "cdate": 1761053845201, "tmdate": 1762931382305, "mdate": 1762931382305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between representation and learning in ICL tasks. Through extensive experiments and detailed analyses, the authors demonstrate that representation and learning are largely independent (orthogonal) to each other."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The study covers LLMs of various sizes, from 1B to 70B parameters, which validates the scalability of the findings.\n\n2.The analysis is thorough and deep, incorporating correlation studies and discussing the effect of zero-shot performance on final results.\n\n3.Most methodological details and experimental settings are clearly described."}, "weaknesses": {"value": "My primary concern lies in the generalizability of the findings. The scope of experiments and the refined label settings raise questions about how broadly the conclusions can be applied. Specifically:\n\n1.Since the experiments are conducted solely on sentiment analysis, it remains unclear whether the findings extend to other ICL tasks.\n\n2.The label set refinement process (Algorithm 1) may limit generalizability. Because the label names were carefully selected and are not the original class names, the conclusions might only hold under these controlled conditions. In real-world ICL scenarios, where original class names are typically used, will the observed independence between representation and learning still persist?\n\n3.It would also be valuable to understand the relationship between representation and learning when randomly selected class names are used instead of refined ones.\n\nI am open to revising my evaluation if the authors can adequately address these concerns."}, "questions": {"value": "1.What specific prompt template was used in the experiments?\n\n2.Can the findings be applied to real-world use cases or algorithms, or are they mainly of theoretical interest?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGUDthZdBs", "forum": "YKxiut90ju", "replyto": "YKxiut90ju", "signatures": ["ICLR.cc/2026/Conference/Submission19471/Reviewer_cjXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19471/Reviewer_cjXy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837012299, "cdate": 1761837012299, "tmdate": 1762931381946, "mdate": 1762931381946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate how the choice of label representation influences in-context learning(ICL) in large language model(LLM), and whether representation and learning from given demonstrations are independent. The authors claim that prompt's label representation and learning from given demonstrations are independent of one another and the label representation determines the zero-shot performance while learning from given demonstrations improves the performance on top of the zero-shot performance. To validate author's claim, author propose optimization algorithm for label selection by exploiting Hill-climbing algorithm. With this algorithm, the authors control the quality of label by adjusting various number of example(K) of the labeling set as an input for label selection algorithm. The authors test their hypothesis with 3-way and 5-way sentiment classification task using Llama 3 models with different size.\n\nWhile the paper presents interesting analysis about the relationship between the choice of representation and in-context learning, most of core insight are evolutionary given from prior works (Min et al, Pan et al, Kirsanov et al., McCoy et al., Chen et al.) Nonetheless, the authors provide interesting and novel claim, which the choice of representation and in-context learning are independent of one another. Still, from the result, their claim seems overgeneralization. That is, the choice of representation and in-context learning are disentangle-able but it seems bit hard to say that they are independent of one another."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. *Novel and Important Research Questions*: The paper attempts to tackle an underexplored aspect of in-context learning(ICL) by disentangling two fundamental elements of ICL, representation and learning. Prior works had mainly examined these two elements separately, while this paper examine the relationship between these two aspects and how these two aspects influence and act in ICL.\n\n2. *Comprehensive Experimental Design*: The authors validate their hypothesis with rigor across multiple factors. Therefore, experimental evaluation is thorough and convincing. The paper covers different model size with two different classification scenarios. Also, the paper present the results with different representation quality based on labeling set and different number of demonstration. These comprehensive experiment provide multidimensional insights.\n\n3. *Useful Insight regarding ICL*: The paper provides useful insight regarding ICL such as ranking preservation across different number of demonstration which emphasizes the importance of label representation selection.\n\n4. *Insightful Conceptual Connection with traditional ML*: The paper provide insightful conceptual bridge with traditional ML by analogizing the choice of label representation in ICL as feature selection in traditional ML. Similar to neural network based classifier where good features enable efficient learning, a good representation provides high baseline performance in ICL."}, "weaknesses": {"value": "1. *Critical overgeneralization of \"Independence\" Claim*: The major concerns with this paper was the contradiction between the main claim of \"independence/orthogonality\" and the experiment result. If the learning and the choice of representation are independent of one another, then the efficiency of learning(Slope in Figure 2) should not depend on the quality of the label representation. Also, the author explicitly states that \"learning is conditioned by representation\" and \"independent but interwined effect\" which seems self-contradiction of paper's core claim. The paper has successfully shown that the choice of representation and the learning are disentangle-able while it seems to fail to show that they are independent of one another.\n\n2. *Model Size influence Claim*: From section 5.2, the paper claims that model size influences the learning rate. While the result supports its claim, if the result of using same label representation with different models is provided, the claim would have been more convincing.  From the result, we can notice that the model size and label representation influence the learning rate but we are not able to see how much model size influence and how much label representation influence the learning rate by itself. To support the claim powerfully, the result of using same label with different model should be presented.\n\n3. *Require more explanation with some result interpretations*: While the paper provides some useful insight regrading ICL, some of the result interpretation may require more details. In section 5.2, the paper claims that the representation with a medium zero-shot accuracy benefit the most from demonstration which seems crucial finding regarding ICL. Still, there should be more explanation with this claim such as why medium zero-shot accuracy representation benefit the most. Also, in the same section, the paper explain the phenomenon observed in 1B model. The paper briefly explain that the model was confused by Nepali world. This brief explanation seem bit insufficient and left me question that for the small model, high-prior label can interfere with ICL at small number of demonstrations(Fundamental Instability of small model). To clear the this question, there should be more detail explanation or other experiment result."}, "questions": {"value": "Q1. See weakness discussed above\n\nQ2. The paper present the result only using Llama 3 family. I was just wondering how the result would be present using different LLM such as GPT, Qwen, Deepseek etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E9Z4clLOUS", "forum": "YKxiut90ju", "replyto": "YKxiut90ju", "signatures": ["ICLR.cc/2026/Conference/Submission19471/Reviewer_cGKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19471/Reviewer_cGKP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977341428, "cdate": 1761977341428, "tmdate": 1762931381505, "mdate": 1762931381505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes that in-context learning performance is conditioned on both the ability to learn from demonstrations (\"learning\") and the naturalness of the representation of the candidate classes (\"representation\"). They propose an optimization approach for generating label sets by iteratively hill-climbing on the task of selecting a token to use as the label for one of the classes in a zeroshot classification setting. By performing this optimization with more or less examples, they construct better or worse label sets for the same underlying classification task, and show that (1) representations that have worse zeroshot performance are consistently worse even in 100-shot settings; (2) representations that have sufficiently poor zeroshot performance (depending on the task) inhibit learning from additional demonstrations; (3) different model sizes in the same family exhibit different learning behavior, and optimizing label sets for larger models results in more semantically meaningful representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "S1. This is a nice hypothesis and a really clear untangling of two previously conflated components of ICL; I really like the idea of varying the representation of the label set to study learning behavior on the same data. The findings are interesting and a meaningful contribution to the empirical understanding of ICL.\n\nS2. The paper takes care with validating claims, and makes many good choices in the experimental design-- e.g the averaging over 10 demonstration sets for each ICL data point, the confidence intervals, the design of the optimization with random restarts. ICL is very noisy, but these details give me much more confidence that these results are meaningful.\n\nS3. The presentation is generally quite strong, with meaningful engagement with prior work. The core ideas of the paper are clearly explained; I think it would be easy to follow even if I did not work in this area, and I enjoyed reading the paper!"}, "weaknesses": {"value": "W1. Many real-world label sets include multi-token labels and longer semantically meaningful labels may be better representations than any single token would allow (and allow for distinguishing between similar classes, e.g. two labels in Banking-77 are \"card payment wrong exchange rate\" and \"card payment not accepted\"). Allowing for multi-token labels clearly expands the optimization space to an absurd degree, so I'm not asking for you to incorporate this, but I think this should be discussed as a limitation-- you are only studying the space of single-token label representations. Similarly, the focus on labels prevents consideration of representational effects from other parts of the input formatting. Discussion of these limitations would make it clearer what evidence the paper can and cannot provide for the representation vs learning hypothesis. \n\nW2. The paper considers only one family of models; while I don't think repeating every experiment across many families with many model sizes is really worth the expense, different model families do behave differently in many ways. Performing a small sweep of models at similar size across different model families (e.g. Qwen 2.5 7B and Mistral 7B, or any other pair of roughly 8B models) would help validate that this is not a Llama-family-specific effect. This is the main reason my score for the paper is not higher; if you can demonstrate this holds across model families, I'd be happy to raise my score.\n\nW3. While the paper presents the optimization of label space mostly as a tool to study representations, it's an interesting idea on its own and would be much more impactful if there was a bit more detail. Specifically: (1) can you show on Figure 2 how these numbers compare to the baselines of semantically meaningless (e.g. numerical) labels and gold (from the original dataset) labels? (2) Figure 2 shows sample efficiency for the optimization pipeline, but can you also discuss cost in terms of compute efficiency (e.g. in average forward passes through the model during optimization?) Wall-clock time would also be a helpful metric to build intuition, though of course this depends strongly on your inference setting."}, "questions": {"value": "Q1. I'm really interested in the transferability of these learned label sets across models (in the same family, but especially across families). Are label sets learned by one model necessarily strong label sets for another? It seems that this might be a one-way transfer (e.g. the semantically meaningful label sets from the 70B might transfer well to the 1B, but some of the 1B sets could be reflections of training artifacts that might not be mirrored in the 70B). \n\nQ2. I could imagine a number of alternate ways of choosing different-quality label sets, including using earlier versions of the label set from the optimization process or choosing the bottom assignment out of the 10 runs at a fixed $K$. Why construct the different-quality label sets through using different-sized $K$?\n\nComment, not related to score: generally I like the way the results are structured into paragraphs, but I think within some of the paragraphs, the framing of \"1B did this; 7B did this; 70B did this\" is sometimes a little hard to follow. Reframing this to focus on trends in results first and then discussing where each result applied would make it easier to read: \"generally, either X or Y happened; X happened more for [some models over others]\". I noticed this most in lines 398-403. By contrast, I liked the presentation in lines 357-364."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nvMeEPQCnC", "forum": "YKxiut90ju", "replyto": "YKxiut90ju", "signatures": ["ICLR.cc/2026/Conference/Submission19471/Reviewer_jKgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19471/Reviewer_jKgr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116804447, "cdate": 1762116804447, "tmdate": 1762931381035, "mdate": 1762931381035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}