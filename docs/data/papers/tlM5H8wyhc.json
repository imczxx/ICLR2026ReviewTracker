{"id": "tlM5H8wyhc", "number": 17985, "cdate": 1758282653926, "mdate": 1763722459850, "content": {"title": "Attention Misalignment Attacks: Targeting Cross-Modal Attention in Multimodal Large Language Models for Adversarial Examples", "abstract": "Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of multimodal understanding tasks. However, their growing deployment raises concerns about robustness under adversarial conditions. Existing adversarial attacks on MLLMs predominantly focus on disrupting the global semantic alignment between image and text by optimizing over joint embeddings or globally aggregated image/text token representations. We observe that such methods often fail to generate effective adversarial examples for fine-grained tasks such as VQA (Visual Question Answering), especially when the questions aim at detailed understanding of particular regions in the image, which requires precise alignment between image regions and textual answers for MLLMs. To address this, we propose Attention Misalignment Attack (AMA), a novel plug-and-play attack method that is highly compatible with existing attack objectives—it can be easily integrated by combining its attention misalignment loss with other attack losses. AMA operates by extracting attention maps from each decoding step of the MLLM and optimizing the divergence between target and adversarial attention patterns, guided by semantic similarity. This forces the model to attend to irrelevant regions, effectively misguiding its answer generation process even towards fine-grained questions. To improve efficiency, we further introduce FastAMA, a lightweight variant that avoids autoregressive decoding and instead uses a single forward pass to extract self-attention from the input tokens. Experiments show that our method significantly enhances the performance of existing attack methods across multiple tasks, especially on the more challenging instances within VQA datasets.", "tldr": "A novel adversarial attack paradigm targeting cross-modal attention in multimodal large language models, improving attack performance towards detailed attacking target.", "keywords": ["Adversarial Attack", "Multimodal Large Language Model", "Attention Mechanism"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4dc5741592ad32f2c5c491766aa0b5dd4f3a6a6d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method called Attention Misalignment Attack (AMA) to create adversarial examples for Multimodal Large Language Models (MLLMs). The core idea is to add a loss term to existing attack frameworks that maximizes the divergence of attention maps between a clean and a perturbed input. This is intended to disrupt the model's ability to ground its textual responses in relevant visual information. The authors also present a more efficient version, FastAMA, and test the method's ability to boost the performance of several existing attacks, particularly on Visual Question Answering (VQA) tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Focused Investigation: The work provides a focused investigation into the role of attention maps as an adversarial attack surface, which is a plausible mechanism for disrupting MLLMs.\n\nPractical Modularity: The method is designed as a \"plug-and-play\" module, which offers some practical utility for researchers looking to marginally improve existing attack pipelines.\n\nEfficiency Variant: The development of FastAMA shows consideration for the computational overhead of adversarial attacks, which is a practical and necessary step."}, "weaknesses": {"value": "Limited Novelty and Contribution: The central weakness of this paper is its limited novelty. The idea of attacking a model's internal representations is not new. Targeting attention maps is an incremental and unsurprising extension of this general strategy, rather than a novel paradigm. The method is ultimately an auxiliary loss function that provides a slight boost to existing techniques, which calls into question the significance of its contribution.\n\nOverstated Claims: The paper frames this incremental improvement as a \"novel framework,\" which seems to overstate the contribution. The reported performance gains are most pronounced in the specific \"VQA-Hard\" setting defined by the authors, but are less impressive on other tasks, suggesting the method's utility may be narrow.\n\nWeaknesses in Evaluation: The evaluation feels one-sided. It focuses solely on attack success without considering the robustness or transferability of the generated examples in the presence of even simple defenses. The ablation studies, while present, are not entirely convincing; the justifications for certain design choices (e.g., weighting strategies) feel post-hoc and may not represent fundamental insights."}, "questions": {"value": "Given that targeting internal representations is a well-established adversarial strategy, could you clarify what makes targeting attention maps fundamentally different or more insightful than, for example, targeting the output of intermediate feature layers?\n\nThe proposed method directly manipulates a core architectural component (the attention mechanism). Does this specificity have a negative impact on the transferability of the adversarial examples to different MLLM architectures? A successful attack that is not transferable has limited practical security implications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mo1728njFm", "forum": "tlM5H8wyhc", "replyto": "tlM5H8wyhc", "signatures": ["ICLR.cc/2026/Conference/Submission17985/Reviewer_Qn2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17985/Reviewer_Qn2r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510484216, "cdate": 1761510484216, "tmdate": 1762927782409, "mdate": 1762927782409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "f0VVbfs324", "forum": "tlM5H8wyhc", "replyto": "tlM5H8wyhc", "signatures": ["ICLR.cc/2026/Conference/Submission17985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17985/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722459285, "cdate": 1763722459285, "tmdate": 1763722459285, "mdate": 1763722459285, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on adversarial attacks of MLLMs, and it proposes a mis-alignment adversarial attack to perturb MLLM hidden embeddings for generating better attack performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on an intriguing and important research question.\n2. AMA achieves better performance compared to traditional attacks."}, "weaknesses": {"value": "The paper mainly has problems in the following aspects:\n\n1. Writing issues. The writing is unclear. Starting from Section 3, the authors do not define their threat model—is this a white-box or a black-box attack? In Section 2, what is A_l^{\\prime} and how is it obtained, especially under different settings, e.g., untargeted vs. targeted attacks? Ambiguous notation also hurts readability: the formulas use w_l, while the text uses w_t. Are they the same thing? In addition, I don’t understand the motivation for using edit distance to measure token similarity—shouldn’t the similarity between the target word and the generated word be measured semantically?\n\n2. Insufficient related-work discussion. Injecting perturbations into intermediate model features is not a new topic; it has been widely studied in prior work [1,2]. The authors should discuss how this paper differs from those studies to help readers better understand the technical contribution.\n\n[1] A self-supervised approach for adversarial robustness. CVPR 2020\n\n[2] FDA: feature disruptive attack. ICCV 2019\n\n3. Experimental issues. The authors only compare against LLaVA, MiniGPT-4, and InternVL-2.5. The paper lacks experiments on more advanced MLLMs such as Qwen.25-VL, and LLaVA-1.5. In addition, comparisons with existing adversarial attack methods for MLLMs [3,4] are missing; these should be included to demonstrate the method’s effectiveness.\n\n[3] A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1. NeurIPS 2025\n\n[4] M-Attack-V2: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting. arXiv 2025"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CVOB1LaQT3", "forum": "tlM5H8wyhc", "replyto": "tlM5H8wyhc", "signatures": ["ICLR.cc/2026/Conference/Submission17985/Reviewer_r8rb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17985/Reviewer_r8rb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863495920, "cdate": 1761863495920, "tmdate": 1762927782066, "mdate": 1762927782066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed Attention Misalignment Attack (AMA) method primarily addresses the problem of adversarial attacks on MLLMs in the context of fine-grained understanding tasks. AMA operates by directly manipulating the model's internal cross-modal attention maps during the decoding process. This forces the model to attend to irrelevant image regions, thereby misguiding its fine-grained comprehension. Furthermore, the authors introduce a lightweight variant, FastAMA, which approximates the attack by using a single forward pass and extracting the self-attention map from the final layer, significantly enhancing efficiency. Experiments conducted on LLaVA, MiniGPT-4, and InternVL models demonstrate that AMA/FastAMA can significantly boost the attack success rate of existing methods on tasks like VQA (especially VQA-Hard)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Attack Perspective\n\nThe paper accurately identifies the reason for the failure of existing attacks on fine-grained tasks and, in response, shifts the attack target from the model's final output to its internal attention mechanism. This approach directly targets the core mechanism of MLLM visual reasoning, is logically sound, and the use of KL divergence to measure attention misalignment is a reasonable and effective choice.\n\n2. Efficient Variant (FastAMA)\n\nThe authors clearly recognize the computational bottleneck of the AMA method (i.e., extracting attention at each decoding step is too slow) and proactively propose FastAMA. It requires only a single forward pass. As shown in Table 5, FastAMA achieves an approximately 7x speedup and a 37% reduction in memory usage, with only a marginal drop in attack success rate. This significantly enhances the practical utility of the method."}, "weaknesses": {"value": "1. The paper fails to discuss the transferability of adversarial examples generated from one white-box model to another. Consequently, the method's effectiveness in black-box scenarios is highly questionable.\n\n2. The work is limited to targeted attacks. The \"attention misalignment\" concept seems inherently bound to guiding the model's attention toward a specific target pattern. It is unclear how this loss function would be defined or optimized in a simpler, untargeted attack setting (i.e., where the goal is simply to force any incorrect answer).\n\n3. The ablation study is limited. It primarily discusses the settings for the lambda parameter in the integration strategy and the w_l in attention weighting. It fails to explore the individual contributions of AMA's different components (e.g., the weighting strategy itself, or the integration with existing attack methods) to the overall attack success. A more comprehensive ablation study would be more persuasive."}, "questions": {"value": "1. As mentioned in Weakness 1, the method relies heavily on white-box access. Have the authors tested the transferability of adversarial examples generated by AMA/FastAMA in a black-box setting? For example, can samples generated on LLaVA successfully attack InternVL? What is the potential of this method against practical black-box models?\n\n2. In Section 4.2, the paper states that for VQA-Easy, Classification, and Captioning tasks, the attack target is uniformly set to the generic token \"Unknown\". However, in Section 4.5.2, \"edit distance\" is chosen as the optimal strategy for computing token weights w_l, citing its ability to provide precise token-level guidance. How is a meaningful w_l weight, capable of guiding gradient optimization, calculated when the target is such a single, generic token like \"Unknown\"?\n\n3. For FastAMA, the authors chose to use the self-attention map from the decoder's final layer. Did the authors experiment with using other layers or a fusion of multiple layers? Why is the final layer the optimal choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Eh5T0KxrJx", "forum": "tlM5H8wyhc", "replyto": "tlM5H8wyhc", "signatures": ["ICLR.cc/2026/Conference/Submission17985/Reviewer_Jvth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17985/Reviewer_Jvth"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916502601, "cdate": 1761916502601, "tmdate": 1762927781571, "mdate": 1762927781571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Attention Misalignment Attack (AMA) and its efficient version FastAMA, plug-and-play adversarial methods designed to disrupt cross-modal attention in MLLMs. By optimizing the divergence between target and adversarial attention pattern, the approach aims to misguide fine-grained visual understanding. The method is evaluated across multiple models and attack baselines, demonstrating significant improvements in attack success rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Experimental results: The method consistently improves attack success rates across models and tasks.  \n\n- Efficiency: FastAMA offers a compelling speedup with minimal performance drop, making it suitable for resource-constrained scenarios.  \n\n- Compatibility: The plug-and-play design allows easy integration with existing attack frameworks, enhancing reproducibility and applicability."}, "weaknesses": {"value": "- Limited attack scenario: The method is strictly limited to white-box settings, as it requires access to intermediate attention maps from the MLLM decoder. This restricts its practicality in real-world applications (e.g., cloud-based MLLM APIs) where model internals are inaccessible.  \n\n- Novelty concerns: While targeting attention mechanisms is valuable, the core idea of using intermediate features (e.g., attention maps) to enhance adversarial attacks has been widely explored in prior works (for instance, Kim et al. (2024).\n\n\n- Limited insight and broader impact: The experiments focus heavily on attack success rates but lack deeper analysis of how attention misalignment affects model robustness and interpretability. For example, does AMA reduce the transferability of adversarial examples compared to baseline attacks?  \n\n- Technical clarifications needed, e.g., the size of victim models."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2wcA3EMSD3", "forum": "tlM5H8wyhc", "replyto": "tlM5H8wyhc", "signatures": ["ICLR.cc/2026/Conference/Submission17985/Reviewer_UGpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17985/Reviewer_UGpH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001349132, "cdate": 1762001349132, "tmdate": 1762927780482, "mdate": 1762927780482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}