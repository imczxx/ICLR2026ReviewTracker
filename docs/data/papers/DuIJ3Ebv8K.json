{"id": "DuIJ3Ebv8K", "number": 23505, "cdate": 1758344680753, "mdate": 1759896811329, "content": {"title": "Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators", "abstract": "Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. \nTo address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-free inference. \nIn contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA is at-par or better in accuracy compared to previous methods across five benchmark PDEs especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.", "tldr": "We propose PRISMA, a conditional diffusion neural operator that learns the posterior distribution and solves PDEs under noisy observations 15x-250x faster by architecturally embedding physics in the Fourier space.", "keywords": ["Diffusion Model", "Partial Differential Equations", "Inverse Problem", "Noisy Observation", "Fourier Neural Operators"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19c11af6ae11d96b2634584db698c012285b02f7.pdf", "supplementary_material": "/attachment/b84a2e9c14b765773ebc1b952aa0c179dd687cb8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a diffusion-based PDE solver that can significantly accelerate the problem solving process. The new design is that the author introduces a conditional diffusion neural operator that embeds PDE residuals directly into the model’s architecture via attention mechanisms in the spectral domain, enabling gradient-free inference. The experimental results show PRISMA is at-par or better in accuracy compared to previous methods across five benchmark PDEs especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The novelty is somewhat good. Unified Framework for Diverse PDE Tasks: PRISMA’s conditional design, enabled by input masks unifies forward and inverse PDE solving under a single model, supporting full, sparse, and noisy observation regimes. Unlike baselines (e.g., FNO, PINO) that require task-specific models or inference pipelines, PRISMA seamlessly adapts to different problem settings (e.g., sparse Darcy flow inverse problems, noisy Navier-Stokes forward problems) without reconfiguration. This versatility is a significant advance for real-world applications where observation quality varies.\n\n(2) The experimental results. The paper demonstrates compelling empirical performance across five benchmark PDEs (Darcy Flow, Poisson, Helmholtz, Navier-Stokes with/without BCs). PRISMA achieves 15x–250x faster inference (0.18–0.8 seconds per sample) compared to diffusion-based baselines (e.g., DiffusionPDE: 213s, FunDPS: 11.8s) by using only 20–50 denoising steps (vs. 200–2000 steps for baselines). Crucially, this speedup does not compromise accuracy: PRISMA outperforms baselines in noisy settings (e.g., Darcy Flow forward error: 12.28% vs. FunDPS’s 55.09% and DiffusionPDE’s 49.18%) and matches top performers (e.g., PINO, FunDPS) in full/sparse observations."}, "weaknesses": {"value": "(1) Limited Discussion of SRA Block Mechanics: While the SRA block is core to PRISMA’s success, the paper provides insufficient detail on its internal workings. For example:\nThe calculation of the compatibility score \nS^l(k) (complex inner product of Fourier-transformed features and residuals) is described, but the intuition for why spectral-domain attention outperforms spatial-domain methods (e.g., residual concatenation) is underdeveloped.\nThe MLP that learns g_res (guidance strength) is not specified (e.g., architecture, input features beyond r_avg and c_σ), making it hard to replicate or extend.\nThe paper does not explain how SRA handles frequency modes with conflicting residual signals (e.g., high-frequency noise vs. low-frequency physical signals).\n\n\n(2) Spatio-Temporal and Irregular Mesh Limitations: PRISMA is evaluated exclusively on static (time-independent) PDEs with regular grid inputs. The authors acknowledge future work will extend to spatio-temporal problems and irregular meshes, but the current limitation narrows PRISMA’s applicability. Many real-world PDEs (e.g., time-dependent Navier-Stokes, heat equation on complex geometries) require these capabilities, and the paper provides no insight into how PRISMA’s architecture might adapt (e.g., integrating temporal attention, handling unstructured grids). Are there any insights for this problem.\n\n(3) Scalability to High Resolution: The paper evaluates PRISMA on 64×64 and 128×128 grids, but does not address scalability to larger resolutions (e.g., 256×256, 512×512). Diffusion models and neural operators often face computational bottlenecks at high resolution (e.g., Fourier transform costs, memory usage). The authors should clarify whether PRISMA’s 64M parameter count (similar to baselines) remains feasible at higher resolutions, or if modifications (e.g., sparse Fourier transforms) are needed."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PKCGusao4m", "forum": "DuIJ3Ebv8K", "replyto": "DuIJ3Ebv8K", "signatures": ["ICLR.cc/2026/Conference/Submission23505/Reviewer_hiiL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23505/Reviewer_hiiL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615004070, "cdate": 1761615004070, "tmdate": 1762942689090, "mdate": 1762942689090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a PDE solver framework called PRISMA. This framework is a diffusion model which encodes the physical information into the structure of the denoiser, but not reinforced through physical loss. The embedding of physical information is through a mechanism called Spectral Residual Attention (SRA), which includes the transformation of physical residual into frequency domain, and then attention with the observed information also in frequency domain. The performance of their proposed method is better than other models on PDE problems with 100% unit Gaussian noise corruption."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- (Originality) This work introduces a new framework that can incorporate physical information other than direct calculation and backpropagation of physical loss, which seems to reduce the training time and inference steps for diffusion-based frameworks.\n- (Clarity) Besides some minor issues with mathematical symbols (see weaknesses part), the general presentation of this work is easy to follow."}, "weaknesses": {"value": "- (Wrong Physical Residual Calculation of NS) One of the most serious problems with this paper is that, the calculation of physical residual for nonbounded NS equations, which was adopted from DiffusionPDE, is actually wrong. The vorticity $\\vec{\\omega}(x,y)=\\vec{\\nabla} \\times \\vec{v}(x, y)$ is an (axial-)vector which only has $z$ component and is a function of $x, y$. Therefore, its zero divergency, $\\vec{\\nabla} \\cdot \\vec{\\omega}(x,y) = \\frac{\\partial \\omega}{\\partial z} = 0$ cannot be regarded as a meaningful physical residual. The same thing happens to bounded NS. In that case, only the magnitude of $\\vec{v}(x, y)$ is recorded, and one cannot take divergence on a magnitude field. This serious problem would make all the arguments for physical embedding useless, as a comparison of performance without physical residual and with a wrong physical residual does not make any sense.\n- (Problems Tested) This paper compares the performance of models on a rather rare case of observations with 100% Gaussian noise. In reality if one observation is with 100% noise, it is considered a failed observation. One more serious problem is about the physical residual calculated on observation with 100% noise. This noise is high frequency and would hurt the physical residual calculated with finite difference. One cannot get convincing physical information from a physical residual that is not reliable.\n- (Performance and Baseline) On more common full observation and partial observation, the performance of this model is inferior to other models like PINO and FunDPS, as reported in the appendices. The baseline of DiffusionPDE on full observation is actually available in their paper, and is much better than the results reported in the Table 6 of this paper.\n- (Mathematical Symbol) In equation (1), $\\mathcal{F}$ stands for the operator of PDE, but in equation (2), $\\mathcal{F}$ stands for the Fourier transform operator. This reuse of symbol hurts the clarity of this paper."}, "questions": {"value": "- To show that the proposed method works for NS, maybe the authors can try out with vector form of velocity field. The zero divergency of velocity vector field can be implemented as: $\\vec{\\nabla}\\cdot \\vec{v}(x, y) = \\frac{\\partial v_x}{\\partial x} + \\frac{\\partial v_y}{\\partial y} = 0$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZzdkPcMdTD", "forum": "DuIJ3Ebv8K", "replyto": "DuIJ3Ebv8K", "signatures": ["ICLR.cc/2026/Conference/Submission23505/Reviewer_851Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23505/Reviewer_851Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720915590, "cdate": 1761720915590, "tmdate": 1762942688765, "mdate": 1762942688765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PRISMA (PDE Residual Informed Spectral Modulation with Attention). They modify the global spectral path of a UNO, via a residual guided attention mechanism; which depends on the phase alignment of said PDE residuals, as well as a gate dependent on the diffusion step/noise level. This speeds up the inference time of the models by a 15-250x, while maintaining or surpassing their compared models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Novel PDE-informed guidance mechanism\nUsing complex-valued attention in Fourier space to modulate frequencies based on PDE residuals is genuinely novel. The authors established via gating and normalization a well rounded method to perform this task.\n\n### Impressive speedup of the inference (for diffusion models)\nThe 20-step inference while achieving comparative accuracy in some tasks is impressive. Having a speedup of 15-250x is the strongest contribution of this work. This brings the neural PDE solvers a step closer to being practical in real-time applications.\n\n### Unified framework (forward-inverse problems)\nTraining on both forward and inverse problem simultaneously with task-specific probabilities is elegant.\n\n### Provided code\nThis made the review process easier to understand how the experiments were setup/run."}, "weaknesses": {"value": "### Method is only decent on full observations, compared to other models (Table 6).\nIn this case PINO and the other models seem to outperform the proposed method (in speed and accuracy)\n\n### Noise robustness claims (Table 3).\nOverall this table shows that diffusion models are better suited for noisy data. You should compare PINO/FNO trained with the same data augmentation (noise injection during training). The current comparison falls flat, as this setting would be out-of-distribution data for PINO/FNO, while it is in-distribution for most diffusion methods.\nA similar claim can be made for Table 7 results (sparse observations).\n\n### More ablation runs for Table 4.\nWithout confidence intervals, the differences in Table 4 are questionable. Some tasks show minimal improvment or even worse performance without PDE residuals. The paper should report mean and std over multiple seeds (code only shows seed:33).\n\n### Claim of multi-scale guidance is somewhat misleading\nThe paper claims to provide \"multi-scale guidance at every layer of UNO\", but the implementation uses interpolation by downsampling to the same resolution to match the feature map spatial dimensions. Unlike hierarchical feature extractors, this method appears to not learn different representations of PDE residuals at different scales (simple resize of the same signal). This means the multi-scale is rather a multi-resolution.\n\n### The caption of Figure 4 appears to be incorrect\nCaption does not match the figure content (mentions \"inference time vs accuracy\", but this is not shown)"}, "questions": {"value": "- Is there any intuition why FUNDPS outperforms the other methods in the sparse observation case? (Table 7)\n\n- Did you also try out to integrate magnitude alignment into the attention? Phase alignment and using w_gain appears to perform well, I wonder if magnitude information could be used to omit the w_gain term.\n\n- Can you provide results where PINO and other baselines are trained with the similar noise augmentation schedule?\n\n- Can you report mean and std for +5 random seeds? (Table 4)\n\n- Have you ablated using magnitude-only vs. phase-only alignment in frequency domain?\n\n- Why is complex-valued attention necessary, compared to for example attention in the spatial path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6rFWP7ZQ3R", "forum": "DuIJ3Ebv8K", "replyto": "DuIJ3Ebv8K", "signatures": ["ICLR.cc/2026/Conference/Submission23505/Reviewer_H4cn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23505/Reviewer_H4cn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851330083, "cdate": 1761851330083, "tmdate": 1762942688494, "mdate": 1762942688494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRISMA (PDE Residual Informed Spectral Modulation with Attention), a novel conditional diffusion neural operator for solving forward and inverse problems in the context of PDEs. To avoid the cost and potential instabilities of inference-time guidance, they propose to condition the operator directly on the (noisy and masked) PDE residuals using a spectral residual attention block. Similar to existing solvers, PRISMA operates on a joint space of observations and parameters to enable the solution of forward and inverse problems with sparse and noisy observations. Due to the guidance-free inference, PRISMA can outperform existing solvers in terms of accuracy vs. inference cost."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The spectral residual attention block seems to be a novel module for conditioning diffusion models/operators on residuals from sparse and noise observations. This can lead to improved performance, in particular for noisy observations (potentially due to the learned gating mechanism). Since no guidance is needed during inference, this further reduces the required number of steps and inference time compared to DiffusionPDE and FunDPS.\n2. The paper includes a comprehensive set of experiments (from DiffusionPDE) and provides ablation studies on the conditioning type and number of required diffusion steps (showing that performance saturates quickly at 20-50 steps and validating the use of a low number of steps). Moreover, it is shown that PRISMA can improve statistics of the PDE residuals over the considered two baselines."}, "weaknesses": {"value": "1. A major concern is that PRISMA considers a different setting than the two main baselines. The baselines only require paired data, but no prior knowledge of the PDE or the corruptions (type of masking/noise/etc.) during training. In other words, these guidance-based methods are *agnostic* to the corruptions and just rely on inference-time control. On the other hand, PRISMA assumes knowledge of the type of masks, noise, and PDE equations *during training*. This is a more restrictive setting and (unsurprisingly) also leads to better performance.\n2. The framework of jointly modeling observations and parameters (and thus being able to solve forward and inverse problems) is taken directly from DiffusionPDE (finite-dim.) and FunDPS (infinite-dim.).\n3. As mentioned in the paper, the performance is only evaluated on the dataset and baselines provided by DiffusionPDE which do not consider practically relevant problems on full time-intervals (instead of initial/terminal time problems). Due to its dependence on FFT, it seems also non-trivial to extend PRISMA to problems on irregular geometries.\n4. Some of the claims seem to be too strong:\n    - The proposed guidance is still not “pointwise” since the Fourier transform has a global dependency.\n    - The method is not fully \"gradient-free\" during inference, since the gradients appearing in the PDE still need to computed (e.g. using finite-differences or Fourier differentiation).\n    - While the methods has a better performance vs. inference cost trade-off, the statement that it is “at-par or better in accuracy” is not true given the sparse-observation results reported in Table 7.\n\n**Minor:**\n\n- The caption of Figure 6 seems to be wrong (and coincides with the caption of Figure 4)."}, "questions": {"value": "- It would be interesting to see the ablation when directly conditioning on the sparse and noisy observations instead of the PDE residual.\n- It seems that the guidance weight for DiffusionPDE/FunDPS is not tuned properly since the reported performance is sometimes decreasing when increasing the number of steps.\n- How much slower is the training due to the computations of residuals and the additional spectral residual attention block?\n- Does PRISMA train different models for different noise levels for the (sparse) observation (and if not, how is the noise level sampled during training). Moreover, how are the masks sampled during training?\n- It would be interesting to ablate the effect of the gating mechanism in the noisy setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bcaD4bj9MQ", "forum": "DuIJ3Ebv8K", "replyto": "DuIJ3Ebv8K", "signatures": ["ICLR.cc/2026/Conference/Submission23505/Reviewer_DsDE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23505/Reviewer_DsDE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871867383, "cdate": 1761871867383, "tmdate": 1762942688253, "mdate": 1762942688253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}