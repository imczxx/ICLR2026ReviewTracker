{"id": "qz3BkyyHWJ", "number": 17066, "cdate": 1758271757235, "mdate": 1759897200743, "content": {"title": "Knowledge Editing with Subspace-Aware Key-Value Mappings", "abstract": "Knowledge editing aims to efficiently correct factual errors in Language Models (LMs). The popular locate-then-edit approach modifies an MLP layer by finding an optimal mapping between its input vector (key) and output vector (value) that leads to the expression of the edited knowledge. However, existing methods without any constraints on the key and value vectors cause significant perturbations to the edited model. To address this, we propose Subspace Knowledge Edit (SUIT), a method that identifies and modifies only the subspace of critical features relevant to the edit. Our empirical results on LLaMA-3-8B, GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge preservation over strong baselines while maintaining high edit efficacy. This effectiveness confirms that SUIT successfully identifies the critical subspace for the edit. Further analyses provide additional validation for our approach. The source code and data will be released to the public upon publication of the paper.", "tldr": "Subspace Knowledge Edit (SUIT) identifies and modifies only the subspace of critical features relevant to the edit, dramatically improves knowledge preservation over strong baselines while maintaining high edit efficacy.", "keywords": ["Knowledge Editing", "Large Language Model", "Subspace"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92827ce82a34a7a589d2f10604392120b36dd8d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SUIT (Subspace Knowledge Edit), a new knowledge editing method aimed at solving the problem of poor \"specificity\" (i.e., corrupting unrelated knowledge) that is common in locate-then-edit approaches. The method claims to achieve its goal via two core constraints: 1) When computing the \"key vector\" $k$, it identifies and removes an \"entity-agnostic\" subspace derived from 10,000 samples, purportedly isolating the edit to \"entity-specific\" features. 2) When computing the \"residual vector\" $\\delta$, it forcibly constrains the optimization to a fixed two-dimensional subspace, defined by two vectors $w_1$ and $w_2$ intended to promote the new knowledge and suppress the old. Experimental results show the method improves specificity on the COUNTERFACT dataset for models like LLaMA-3."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper identifies the challenge of low specificity in knowledge editing.\n2. The method demonstrates a significant specificity boost on the COUNTERFACT dataset based on a generation-based criterion.\n3. The paper provides mechanistic evidence (e.g., token-level perturbations in residual streams) to validate the reduced perturbation."}, "weaknesses": {"value": "1. The paper **assumes** the residual vector (the knowledge update) can be constrained to a two-dimensional subspace which lacks theoretical justification.\n2. The subspace identification relies on an energy threshold hyperparameter $\\tau_{energy}$ to which the model performance is **sensitive** as shown in Figure 5. The optimal $\\tau_{energy}$ value obtained on COUNTERFACT may not be applicable to other datasets which adds an extra tuning burden for the method's application.\n3. The paper selects $N=10,000$ subjects for the SVD pre-computation step which has no justification for this specific number nor any sensitivity analysis. It is unclear how robust the identified \"entity-agnostic\" subspace is; a different $N$ could potentially yield a different subspace. Furthermore, the computational cost of this SVD step scales directly with $N$, yet the authors fail to demonstrate that $N=10,000$ represents a reasonable trade-off between computational cost and the stability of the resulting subspace.\n4. The analysis in $\\S 6.2.1$ does not independently substantiate the semantic claim that $K_s^{\\perp}$ is ‚Äúentity-agnostic.‚Äù The subspace is defined via an SVD criterion, and the subsequent variance comparison merely reflects that construction rather than testing semantic invariance.\n5. The second $\\lambda_{KL}$ that appears in Appendix B should be weight decay $\\lambda_{WD}$, which is likely a typo."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DGQuxcojEL", "forum": "qz3BkyyHWJ", "replyto": "qz3BkyyHWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17066/Reviewer_bGWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17066/Reviewer_bGWe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662203619, "cdate": 1761662203619, "tmdate": 1762927077866, "mdate": 1762927077866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel knowledge editing method, SUIT (Subspace Knowledge Edit), which constrains updates to task-relevant subspaces by decomposing the key vector ùëò into entity-specific features and restricting the residual vector ùõø to the feature directions most relevant to the new object. By operating only within these targeted subspaces, SUIT enables precise and localized modification of factual knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance of SUIT is good and surpasses other baselines across datasets.\n\n2. The idea of making a finer-grained distinction in the editing subspace is novel and also reminds me of SAE..\n\n3. The paper's presentation is clear and well-organized."}, "weaknesses": {"value": "1. It would be valuable to further evaluate the \"ripple effects\" metric of knowledge editing, which is mentioned in [1].\n\n2. As with other locate-then-edit approaches, it would be good to explore whether this method can generalize beyond the triple-based question.\n\n3. It would strengthen the work if experiments were extended to larger-scale models, such as 14B-parameter models.\n\n4. On the ZSRE dataset, the average performance (S score) on two models is still lower than that of AlphaEdit.\n\n5. SUIT generally underperforms AlphaEdit on the Generalization metric; it would be helpful to investigate the underlying reasons for this gap.\n\n6. It would also be insightful to compare SUIT with SAE-based knowledge editing approaches and examine performance differences.\n\nIf the above weaknesses are addressed well, I will reconsider my rating.\n\n---\n**References**:\n\n[1] Evaluating the Ripple Effects of Knowledge Editing in Language Models"}, "questions": {"value": "Please see the weaknesses list above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dzqvP8bBZf", "forum": "qz3BkyyHWJ", "replyto": "qz3BkyyHWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17066/Reviewer_U4G2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17066/Reviewer_U4G2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798498042, "cdate": 1761798498042, "tmdate": 1762927077354, "mdate": 1762927077354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the unintended side effects of LLM knowledge editing and aims to improve specificity. Building on the locate-then-edit paradigm that treats the MLP down-projection as a linear associative memory W mapping keys to values, the authors propose Subspace Knowledge Edit (SUIT). SUIT (i) removes entity-agnostic components from the key vector by projecting onto and subtracting a data-driven ‚Äúcommon‚Äù subspace obtained via SVD over many subject keys, and (ii) restricts the residual/value update to a low-dimensional subspace spanned by two optimized directions that respectively promote the new object and suppress the old one."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Subspace isolation via data-driven SVD. The paper presents a clean and principled subspace isolation method: collect key vectors for a large set of subjects, perform SVD, and define the entity-agnostic subspace as the span of top singular vectors up to an energy threshold \\tau; subtracting this projection yields an entity-specific key k. This is simple, effective, and empirically validated (variance analysis in Table 2; reduced interaction with the common subspace in Table 3)\n- The experimental results suggest that the proposed method is comparable to SOTA baselines."}, "weaknesses": {"value": "- Sensitivity to the energy threshold œÑenergy and selection cost. The core localization step relies on a manually chosen œÑenergy. Fig. 5 shows a nontrivial trade-off: as œÑenergy increases, generation tends to decrease while specificity increases, with a ‚Äúsweet spot‚Äù around 0.3‚Äì0.4. Although efficacy is fairly stable, the turning point is not sharply defined and may require cross-validation that is data-dependent and potentially costly. Please (a) provide a principled selection rule/heuristic (e.g., knee detection on cumulative energy, variance ratio targets, or stability-based criteria), (b) report sensitivity across datasets/models when fixing \\tau energy without extra tuning (you currently fix \\tau=0.4 for all models ), and (c) discuss compute overhead for building K subject and running SVD (N=10k subjects) and how often this needs to be repeated per model/layer.\n- Not uniformly state-of-the-art across all settings. While SUIT is very strong overall, it is not consistently the best on every model/dataset. For example, on ZSRE, SUIT‚Äôs harmonic mean S trails AlphaEdit slightly on GPT-J and Qwen (e.g., AlphaEdit S‚âà96.9 vs. SUIT S‚âà95.9 on GPT-J; AlphaEdit S‚âà89.6 vs. SUIT S‚âà88.2 on Qwen).\n- AlphaEdit‚Äôs results are slightly lower than their original paper."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HfPAGhRPCy", "forum": "qz3BkyyHWJ", "replyto": "qz3BkyyHWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17066/Reviewer_anrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17066/Reviewer_anrX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967154887, "cdate": 1761967154887, "tmdate": 1762927076822, "mdate": 1762927076822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SUIT (Subspace Knowledge Edit), a new framework for editing factual knowledge in large language models while minimizing disruption to unrelated information. Building on the Linear Representation Hypothesis, the authors propose subspace-aware editing that confines changes to semantic subspaces relevant to the target entity and relation. SUIT improves upon prior methods like ROME, MEMIT, and AlphaEdit by refining the computation of the key and residual vectors through subspace decomposition. This design enhances specificity and stability during edits. Experiments on LLaMA-3-8B, GPT-J-6B, and Qwen2.5-7B show large gains in specificity (e.g., +43.2 points on LLaMA-3-8B) while maintaining strong edit accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper makes a solid and coherent contribution to the field of knowledge editing in large language models. It stands out in three main aspects.\n(1) It provides a clear theoretical framing grounded in the Linear Representation Hypothesis, establishing a principled link between semantic subspaces and factual updates. This theoretical foundation not only motivates the method design but also enhances interpretability and conceptual rigor.\n(2) The paper offers comprehensive and convincing analyses, including ablation studies, perturbation comparisons, and examinations of key components such as subspace isolation and feature decomposition. These analyses provide both empirical depth and transparency, showing that the improvements are consistent and well-supported.\n(3) The overall structure and presentation are highly polished. The logical flow‚Äîfrom motivation to methodology, experiments, and analysis‚Äîis clear and well-balanced, and the exposition makes complex ideas accessible without oversimplifying them. Collectively, these qualities make the work both theoretically meaningful and practically persuasive."}, "weaknesses": {"value": "While the paper presents a clear and well-executed framework, several aspects limit its novelty and clarity.\n(1) The core technical innovation is not particularly strong. Although the subspace-based approach is conceptually sound, its methodological route appears closely aligned with the null-space perturbation ideas introduced in AlphaEdit. The distinction between the proposed subspace isolation and existing null-space constraints is not sufficiently emphasized, making the contribution seem more incremental than foundational.\n(2) There are issues in the main experiments that reduce confidence in the reported improvements. The primary evaluation table introduces the ‚ÄúS‚Äù metric as the harmonic mean of three scores, yet the performance gain seems largely driven by unusually high specificity, while the other two components remain modest. Moreover, the specificity scores for baseline methods such as MEMIT and AlphaEdit appear unexpectedly low compared to prior results. Additionally, the table includes a ‚ÄúGA‚Äù metric that lacks explanation‚Äîit is likely intended to be ‚ÄúGC,‚Äù but this ambiguity should be clarified.\n(3) Finally, although the appendix provides numerous additional analyses, their purpose and logical role within the overall argument are unclear. The paper does not clearly justify why these analyses are necessary or how they support the main claims. Some component analyses could also be presented more transparently to better highlight their individual contributions to the overall performance gains."}, "questions": {"value": "(1) The proposed subspace-based approach seems conceptually similar to the null-space technique used in AlphaEdit. Could the authors elaborate on the fundamental distinction between the proposed subspace isolation and the existing null-space constraint formulation? In particular, what new insights or advantages does SUIT provide beyond current research‚Äôs framework?\n(2) Regarding the main experimental results, the ‚ÄúS‚Äù metric‚Äîdescribed as the harmonic mean of three sub-metrics‚Äîappears to be driven primarily by the high specificity score. Could the authors provide a detailed breakdown of the three components and explain whether the improvements are consistent across all of them? Additionally, the baseline scores for MEMIT and AlphaEdit seem unexpectedly low compared to prior reports. Were there any differences in implementation or evaluation setup that could explain this? Finally, please clarify the meaning of the ‚ÄúGA‚Äù metric in the main table; should this instead refer to ‚ÄúGC‚Äù?\n(3) The additional analyses in the appendix are extensive, but their motivation is not entirely clear. Could the authors better articulate how each analysis (e.g., perturbation visualizations, subspace decomposition tests, or ablations) contributes to the main argument? For instance, which specific results directly support the claimed benefits of subspace isolation? More explicit linking between these analyses and the core findings would strengthen the logical flow of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "weGfkrpw2j", "forum": "qz3BkyyHWJ", "replyto": "qz3BkyyHWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17066/Reviewer_JqDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17066/Reviewer_JqDj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163266054, "cdate": 1762163266054, "tmdate": 1762927076362, "mdate": 1762927076362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}