{"id": "SjLo76SpoW", "number": 7082, "cdate": 1758007007298, "mdate": 1759897873671, "content": {"title": "MobileKGQA: On-Device KGQA System on Dynamic Mobile Environments", "abstract": "Developing a mobile system capable of generating responses based on stored user data is a crucial challenge. Since user data is stored in the form of Knowledge Graphs, the field of knowledge graph question answering (KGQA) presents a promising avenue towards addressing this problem. However, existing KGQA systems face two critical limitations that preclude their on-device deployment: resource constraints and the inability to handle data accumulation. Therefore, we propose MobileKGQA, the first on-device KGQA system capable of adapting to evolving databases with minimal resource demands. MobileKGQA significantly reduces computational overhead through embedding hashing. Moreover, it successfully adapts to evolving databases under resource constraints through a novel annotation generation method. Its mobile applicability is validated on the NVIDIA Jetson Orin Nano edge-device platform, achieving 20.3% higher performance while using only 30.4% of the energy consumed by the SOTA (state of the art). On standard KGQA benchmarks, using just 7.2% of the computation and 9% of the parameters, MobileKGQA demonstrates performance that is empirically indistinguishable from the SOTA and outperforms baselines under distribution shift scenarios.", "tldr": "", "keywords": ["Knowledge Graph Question Answering", "Large Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f819bf39a30e59a3c9a3e350743cffa2482204a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenges of deploying Knowledge Graph Question Answering (KGQA) systems on mobile devices, specifically the significant obstacles posed by resource constraints and continuous data accumulation. The authors propose MobileKGQA, the first on-device KGQA system specifically designed to adapt to evolving knowledge graphs with minimal resource requirements. The system employs embedding hashing to compress high-dimensional embeddings into compact binary codes, significantly reducing computational and storage overhead. It also introduces a novel annotation generation method based on sequential reasoning to handle distribution shifts caused by newly accumulated data. Extensive experiments on the NVIDIA Jetson Orin Nano platform demonstrate that MobileKGQA achieves 20.3% higher performance while consuming only 30.4% of the energy of the state-of-the-art (SOTA) model. On standard KGQA benchmarks, it uses only 7.2% of the computation and 9% of the parameters of the SOTA model while maintaining comparable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper demonstrates significant strengths across several key dimensions, presenting a compelling case for its contribution to the field of on-device AI and knowledge graph reasoning.\n- This work exhibits high originality by tackling the relatively unexplored problem of on-device training and adaptation for KGQA systems, not just on-device inference. It creatively combines ideas from different domains: Embedding Hashing for KGQA, Stepwise Annotation Generation for Adaptation\n- The study is executed with high quality and methodological rigor. The experimental evaluation is comprehensive and convincing.\n- The paper is generally well-structured and clearly written. The logical presentation of the three-phase pipeline (Hashing, Retrieval, Adaptation) is clear. Figures 1 and 2 effectively illustrate the core workflows. The use of tables to summarize comparative results and ablation studies is clear and facilitates comparison.\n- This work is highly important for both academic research and practical applications. It directly addresses the growing need for privacy-preserving, efficient, and adaptive AI on personal devices. By enabling on-device training, it avoids data privacy concerns associated with cloud-based model retraining. The proposed techniques pave the way for a new generation of mobile systems that can continuously learn from user data without compromising performance or battery life. The provided prompts and methodology serve as a valuable resource for subsequent research in the community."}, "weaknesses": {"value": "- Insufficient Analysis of Hashing Module Limitations: \n\nThe hashing module is central to MobileKGQA's efficiency claims, but the paper lacks a deeper analysis of its inherent limitations – particularly regarding hash collisions. Compressing high-dimensional embeddings into low-dimensional binary codes is inherently a lossy process. The paper does not discuss the possibility of different entities or relations being mapped to the same or very similar hash codes (i.e., hash collisions) and their potential impact. This weakens the claim of the system's robustness. If the collision rate increases as the knowledge graph expands, it could contaminate the answer candidate set, leading to degraded retrieval accuracy and final QA performance.\n\n- Limited Diversity in Experimental Datasets Affects Generality Claims:\n\nThe paper's experiments are validated only on two datasets (WebQSP and CWQ) derived from Freebase. To claim broad applicability, testing on knowledge graphs with different structural characteristics is necessary. For example, graphs like Wikidata are denser with more complex relations, while some domain-specific graphs might be sparser. The performance of the hashing module might vary with graph properties. In sparse graphs, with less semantic information, the discriminative power of hash codes might be insufficient; in dense graphs, with high semantic overlap, the risk of hash collisions increases. The current experimental setup cannot demonstrate the system's effectiveness across these diverse scenarios.\n\n- Lack of Comparison with Simpler Hashing or Dimensionality Reduction Baselines:\n\nThe paper fails to adequately compare its proposed mutual information maximization-based hashing method against simpler, more established baseline techniques. There is no comparison against classical dimensionality reduction/hashing techniques like Locality-Sensitive Hashing (LSH) or Principal Component Analysis (PCA). It is therefore difficult to ascertain to what extent the performance gains stem from the paper's sophisticated hashing objective function versus merely the act of \"using hashing\" itself. This makes it challenging to accurately assess the innovative contribution of this module.\n\n- Imprecisions in Result Presentation:\n\nIn Table 2a, GNN-RAG's disk usage is reported as 0.54GB, while MobileKGQA's is 0.6GB. However, the narrative in the main text might lead readers to perceive MobileKGQA as optimal across all dimensions."}, "questions": {"value": "- Regarding Hash Collisions: Did you evaluate the hash collision rate? If collisions exist, how does the system mitigate their potential negative impact on the retrieval of multi-hop reasoning paths? \n- Regarding Generalization Capability: Do you have plans, or could you provide results, extending your evaluation to knowledge graphs beyond Freebase (e.g., Wikidata)? For dense graphs where semantic relations highly overlap, does your method require specific adjustments or optimizations?\n- Regarding Method Comparison: Could you provide comparative results against simpler hashing baselines (e.g., LSH)? This would help the community better understand the added value brought by the \"maximizing mutual information\" design choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6BUVYuAD8H", "forum": "SjLo76SpoW", "replyto": "SjLo76SpoW", "signatures": ["ICLR.cc/2026/Conference/Submission7082/Reviewer_RLPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7082/Reviewer_RLPG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656782188, "cdate": 1761656782188, "tmdate": 1762919264090, "mdate": 1762919264090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MobileKGQA, the first KGQA system that supports both inference and on-device training under strict resource constraints. To reduce memory and computation, the authors introduce a hashing module that compresses high-dimensional embeddings into compact binary codes while preserving semantic similarity. To adapt to continuously evolving knowledge graphs without cloud retraining, the paper further introduces a stepwise annotation generation method that constructs question–answer pairs locally. Experiments on WebQSP and CWQ show that MobileKGQA achieves performance comparable to state-of-the-art lightweight KGQA systems while using only 7.2% of computation and 9% of parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper presents the first KGQA system that supports on-device training and adaptation to evolving knowledge graphs, demonstrating clear practical value for privacy-sensitive mobile environments.\n\nS2: The proposed hashing module achieves large reductions in computation and storage while preserving semantic similarity, leading to near-SOTA performance with only a small fraction of the parameters and energy consumption. Real-device experiments further strengthen the credibility of the claims."}, "weaknesses": {"value": "W1: The theoretical justification of the hashing module relies on simplifying assumptions (e.g., independence between embedding norm and orientation), and the paper lacks deeper empirical analysis of potential failure cases, such as hash collisions or cold-start relations with limited semantic context.\n\nW2: Although the system claims full on-device adaptation, it relies on pre-trained embedding models to encode newly added triples. The paper does not analyze whether these embedding models can run efficiently on mobile hardware, making it unclear if the entire pipeline is truly on-device in practical scenarios."}, "questions": {"value": "Q1: Can the proposed system also adapt when triples are removed from the KG, or is the current adaptation mechanism designed only for newly added knowledge?\n\nQ2: Is there any scenario where hashing leads to ambiguous collisions that harm reasoning? Are there failure cases or examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aguDq4k5DO", "forum": "SjLo76SpoW", "replyto": "SjLo76SpoW", "signatures": ["ICLR.cc/2026/Conference/Submission7082/Reviewer_i4Rc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7082/Reviewer_i4Rc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968333160, "cdate": 1761968333160, "tmdate": 1762919263737, "mdate": 1762919263737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MobileKGQA, an on-device KGQA system that trains and runs fully on mobile/edge hardware. It combines a hashing-based retriever (compressing LLM embeddings to binary codes) with a lightweight reasoning module, plus a stepwise on-device annotation generation to handle evolving KGs under tight compute/privacy constraints. On WebQSP/CWQ and a Jetson Orin Nano, it reports comparable SOTA accuracy while using much less params, and on-device tests beats baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is studying a practical and important problem, i.e., to make the on device KGQA. \n2. Although the reviewer does not think the proposed the algorithm is very novel, these algorithms are sound and supported by theoretical verification and solves the problem nicely. \n3. This paper has thorough and solid empirical evidence to support the effectiveness of the proposed method. Especially, this paper really runs the algorithm on a Jetson, which is more practical."}, "weaknesses": {"value": "1. For the hashing algorithm, this seems to work similar as quantization to binary with cosine constraints, so the reviewer is wondering if a simple kmeans quantization can work better? \n2. The privacy preserving is an important aspect the paper is asserting, is there any privacy leakage when the LLM are generating the final answer?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vnMN3HLy0s", "forum": "SjLo76SpoW", "replyto": "SjLo76SpoW", "signatures": ["ICLR.cc/2026/Conference/Submission7082/Reviewer_DB7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7082/Reviewer_DB7Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762389287448, "cdate": 1762389287448, "tmdate": 1762919263356, "mdate": 1762919263356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}