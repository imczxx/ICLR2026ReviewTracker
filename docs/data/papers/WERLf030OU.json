{"id": "WERLf030OU", "number": 9083, "cdate": 1758110007626, "mdate": 1759897744710, "content": {"title": "LogProber: Disentangling confidence from contamination in LLM responses", "abstract": "In machine learning, “contamination” refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical.In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.", "tldr": "Distinguishing true confidence from contamination is challenging. Logprober addresses this by measuring similarity to the question instead of the answer. We conclude by weighing the pros and cons of question-based versus answer-based approaches.", "keywords": ["contamination", "confidence", "LLM", "question answering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b261b0168b71ff6c84eee461bac9727d8e5e5bef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method, LogProber, to detect data contamination in LLMs. (1) The paper argues contamination cannot be detected reliably from local features such as a few high-probability tokens, since those may arise naturally from linguistic or contextual predictability. Instead, this paper frames contamination as a global property of the model’s familiarity with an entire sequence. It computes a cumulative log-probability curve over the question and then integrates the sorted values into a single 'safe score'. (2) Instead of analyzing answers, LogProber examines how confidently the model predicts the question text itself. (3) Experiments on CRT, MMLU, and BigCodeBench show LogProber effectively distinguishes contaminated and outperforms CDD in avoiding false positives."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Simple, efficient, and suitable for black-box LLMs (only one forward pass).\n* Shifts perspective from local token-level confidence to a global measure of sequence familiarity\n* Offers a complementary view to existing answer-based contamination detection methods."}, "weaknesses": {"value": "The main issue of this paper is its limited experimental coverage. The experiments are restricted to CRT, MMLU, and BigCodeBench, and only two model families are tested (llama 7B and qwen2.5 32b). For a method aimed at general-purpose contamination detection, it would be much stronger to evaluate across a wider variety of models and benchmarks (for example, standard QA tasks or additional reasoning datasets). Moreover, the comparison is limited to CDD. There are many other relevant approaches, such as exposure-based memorization metrics, or masked-completion probes. Including several additional black-box baselines would clarify where LogProber stands within the broader landscape."}, "questions": {"value": "(1) This paper sorts the log-probabilities before integration. This makes the safe score stable but removes positional and contextual information. Early high-confidence tokens could be more diagnostic of contamination than late ones, yet this distinction is lost. Future work could explore weighting earlier tokens or measuring how early the cumulative curve saturates to capture “recognition onset” without losing robustness.\n\n(2) There are several minor typos and formatting issues. (a) line 117 “adn” (b) inconsistent quotation marks, eg line 126 “A”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KSX2beci19", "forum": "WERLf030OU", "replyto": "WERLf030OU", "signatures": ["ICLR.cc/2026/Conference/Submission9083/Reviewer_5GiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9083/Reviewer_5GiR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672697425, "cdate": 1761672697425, "tmdate": 1762920790420, "mdate": 1762920790420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of contamination in large language models, via a logprober. They propose to check the question of the QA, rather than the answer to avoid false positives where the model actually is smart and knows the answer. They use the cumulative log-probability to devise a Safescore. Experiments are done to show the effectiveness of their approach on CRT / Llama1, and Qwen2.5-32B."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Important problem, data contamination is still a difficult and important problem to be solved.\n2. The paper explained their key ideas very clearly"}, "weaknesses": {"value": "1. Lack of innovation, there have been a wide range of confidence/logP based-scores  [1], and people have already figured that rephrasing would escape those detection methods [2, 3]. This method lacks merit in advancing the field.\n2. Model / dataset used are too simple. Only one set of experiments are done (CRT / Llama-1) to show effectiveness.\n3. Lack of analysis. Does question length play an effect here? What about CoT models?\n\n\n[1] Zhang, Huixuan, Yun Lin, and Xiaojun Wan. \"Pacost: Paired confidence significance testing for benchmark contamination detection in large language models.\" arXiv preprint arXiv:2406.18326 (2024).\n\n[2] Yang, Shuo, et al. \"Rethinking benchmark and contamination for language models with rephrased samples.\" arXiv preprint arXiv:2311.04850 (2023).\n\n[3] Yao, Feng, et al. \"Data contamination can cross language barriers.\" arXiv preprint arXiv:2406.13236 (2024)."}, "questions": {"value": "Minor concerns include:\n1. Writing isn't polished through out the paper, many details are lacking: ’Q-A’ -> 'Q-A' (quotes), Confidence refers to a property of the\nmodel’s response to a given input — namely, that the model’s answer is confident (AI written)\n2. Citation format is incorrect, in-context learning abilities Brown et al. (2020a) -> in-context learning abilities (Brown et al. 2020a), ..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e2GsJnFy1r", "forum": "WERLf030OU", "replyto": "WERLf030OU", "signatures": ["ICLR.cc/2026/Conference/Submission9083/Reviewer_amUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9083/Reviewer_amUu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686246540, "cdate": 1761686246540, "tmdate": 1762920789174, "mdate": 1762920789174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"LogProber,\" a novel algorithm designed to detect data contamination in Large Language Models (LLMs) by disentangling \"contamination\" from \"confidence\". The authors argue that existing methods, which often focus on the model's answer (A), can confuse a model's high confidence (due to strong generalization) with contamination (due to memorization) ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a fundamental, high-impact problem. As models become more powerful, their performance on standard benchmarks is increasingly scrutinized for contamination . This work provides a practical tool to help maintain the integrity of LLM evaluation."}, "weaknesses": {"value": "- The paper introduces a specific, non-trivial formula for the \"Safe Score\" based on the integral of the sorted cumulative log-probabilities (Equation 1). However, there is no justification provided for why this specific formulation is optimal, or even necessary, compared to simpler, more direct statistical measures of the \"plateness\" of the $log(p)$ curve. For instance, what about the simple variance of the $log(p)$ values? Or the 10th percentile of $log(p)$? A contaminated sequence should have many $log(p) \\approx 0$, which would heavily skew these simpler metrics. The paper asserts this metric works, but does not provide an ablation study or theoretical justification for this specific choice over more intuitive alternatives.\n\n- The paper's motivation is rooted in detecting contamination within the \"gargantuan... corpora\" used for pretraining. However, all the controlled experiments simulate contamination by fine-tuning an existing model , sometimes with significant repetition (e.g., seeing each item 30 times ). This is a very strong, explicit \"memorization\" signal. It is not self-evident that this signal is representative of true pretraining contamination, where a benchmark item might be seen only a handful of times within a multi-trillion token corpus. The experiments clearly show LogProber detects fine-tuning contamination, but its sensitivity to the (likely weaker) signal of pretraining contamination remains unproven.\n\n- The paper repeatedly uses a fixed threshold of safe_score < 1 to flag contamination. This seems practically brittle. It is highly plausible that the baseline Safe Score for uncontaminated text is model-dependent. For example, a larger, more capable model (e.g., Llama-70B) will inherently find most text less \"surprising\" than a smaller model (e.g., Llama-7B), which would likely result in a lower baseline Safe Score, even without contamination. Asserting a single universal threshold without exploring its scaling properties across different model sizes and families is a significant oversimplification. This suggests a model-specific calibration of the \"clean\" baseline score might be necessary, undermining the method's simplicity.\n\n- This framework also fails to account for contamination beyond the token level, such as contamination at the semantic level. There are doubts about whether this can truly enhance the integrity of the LLM evaluation."}, "questions": {"value": "1. Could the authors provide justification for the specific \"Safe Score\" formulation in Equation 1? What simpler statistical metrics were considered (e.g., variance of $log(p)$, skewness, or the ratio of tokens with $log(p) > -0.1$) and why was this complex integral-based approach chosen over them?\n\n2. The paper proposes a universal threshold of safe_score < 1. How does this threshold hold across different model scales (e.g., 7B vs. 70B vs. 180B) and architectures (e.g., Qwen vs. Llama vs. Mistral)? Does this threshold need to be re-calibrated for each model, and if so, how does that affect the method's practical utility?\n\n3. The experiments use fine-tuning to simulate contamination . How confident are the authors that this signal is a valid proxy for pretraining contamination , which is arguably a much weaker signal (e.g., a single pass over a test item) than the multi-epoch fine-tuning performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fql9ImvkC0", "forum": "WERLf030OU", "replyto": "WERLf030OU", "signatures": ["ICLR.cc/2026/Conference/Submission9083/Reviewer_zdDS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9083/Reviewer_zdDS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934452781, "cdate": 1761934452781, "tmdate": 1762920788462, "mdate": 1762920788462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogProber, an algorithm to detect data contamination by disentangling a model's true confidence from its memorization.\n\n\nThe key problem LogProber solves is that existing methods (like CDD) mistake high confidence for contamination . On tasks a model is good at (like MMLU), these methods falsely flag all answers as \"contaminated\" .\n\n\n\nLogProber's solution is to analyze the question text instead of the answer . It calculates a \"Safe Score\" by measuring if the question's text is \"familiar\" (contaminated) or \"surprising\" (clean) to the model"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Solves the \"Confidence\" Flaw: The paper's main strength is identifying that existing detectors mistake a model's high confidence for contamination. Its novel solution is to analyze the question text instead of the answer, successfully disentangling genuine skill from memorization.\n\n\n2. High Transparency: The authors are rigorous and transparent about the tool's limitations. They explicitly demonstrate that LogProber is blind to \"answer-only\" (-A) contamination, which is a common format for fine-tuning, making the tool's precise capabilities clear."}, "weaknesses": {"value": "1. The writing can be improved -- the introductory content is too long and the citation format can be further improved. Most importantly, the paper will benefit from adding a conclusion section and related work section. These two are clearly missing.\n\n2. The Llama-1-7B model used in the experiment is too old. And the baselines are too few and not strong enough. It only compared with CDD, while data contamination detection is not a new topic and there are a lot of existing work defining and addressing this problem like Shared Likelihood[1], Guided Prompting, N-Gram Accuracy, and Choice Confusion.\n\n3. The LogProber algorithm detects contamination involving *questions* (Q-type) but fails for *answer-only* (A-type) contamination, common in fine-tuning stages.\n\n[1] Proving test set contamination for black-box language models. ICLR 2023\n\n[2] Time travel in llms: Tracing data contamination in large language models. ICLR 2024\n\n[3] Benchmarking benchmark leakage in large language models. ArXiv\n\n[4] Data Contamination Can Cross Language Barriers. EMNLP 2024"}, "questions": {"value": "1. How does the algorithm get the semantic-equivalent sequence of origin question? If it's also generated by the target LLM, the correctness can't be guaranteed.\n\n2. Is there any theoretical prove of the 1 boundary of safe score? Otherwise the Safe Score cutoff (<1) lacks theoretical justification or adaptive calibration.\n\n3. How will your method perform on some open-source / private LLMs? Will there be any contamination detected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0KbxlD1zGY", "forum": "WERLf030OU", "replyto": "WERLf030OU", "signatures": ["ICLR.cc/2026/Conference/Submission9083/Reviewer_AyQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9083/Reviewer_AyQU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980097287, "cdate": 1761980097287, "tmdate": 1762920788003, "mdate": 1762920788003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}