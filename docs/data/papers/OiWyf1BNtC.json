{"id": "OiWyf1BNtC", "number": 324, "cdate": 1756735369261, "mdate": 1759898267569, "content": {"title": "Realtime Video Frame Interpolation using One-Step Diffusion Sampling", "abstract": "Recent research on video Frame Interpolation (VFI) shows that a pretrained  Video Diffusion Model (VDM) can solve many challenging scenarios, including large or complex motion. However, VDMs require tedious diffusion sampling, making the inference slow. One possible way to accelerate is to distill a multi-step model into a one-step model, but additional modules are often introduced during distillation, which significantly increase training overhead. Instead, we propose a Real-time Diffusion-based Video Frame Interpolation pipeline, \\method. \\method achieves efficient interpolation by disentangling this task into two subproblems: motion and appearance generation. Specifically, \\method first calculates pixel movements across frames with the continuous motion fields, only utilizing a few sparse key frames. As a result, \\method only forwards the diffusion model for these sparse key frames rather than for each intermediate frame, effectively reducing one-step training cost. In the second appearance estimation step, \\method then only needs to create intermediate frames by warping input frames with sampled optical flows from the estimated continuous motion field in the first step. Because our diffusion model creates motions only, it can work at a fixed and relatively small resolution, leading to superior training and inference efficiency. Extensive experiments show that our \\method generates comparable or superior interpolation quality compared with existing multi-step solutions. It also offers outstanding inference efficiency, interpolating 17FPS at $1024\\times 576$ resolution, achieving \\textbf{50$\\times$ acceleration} than the fastest diffusion-based generation by Wan.", "tldr": "", "keywords": ["Video Frame Interpolation; Diffusion Models; Realtime Processing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2ab911cb1b557a04a97da5ecbd6027faadc500a.pdf", "supplementary_material": "/attachment/379a35ff600545ea6e092502880d01e8da54eb82.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces an efficient video interpolation approach based on a video diffusion model, enabling real-time frame interpolation. The proposed pipeline consists of three main stages: (1) estimating sparse, low-resolution keyframes through a one-step video diffusion process; (2) extracting a complex motion field from these keyframes using a continuous motion estimator; and (3) synthesizing full-resolution intermediate frames from the motion field and the input frames ($I_0$ and $I_1$) using a frame synthesis network. In addition, the authors propose a two-stage training framework to address the unstable convergence may observed in simple end-to-end training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Traditional non-generative methods preserve the appearance of input frames well but struggle to interpolate dynamic motions. In contrast, diffusion-based methods effectively model and interpolate complex motions but are computationally expensive and often fail to maintain the appearance consistency of input frames.\n\nThis paper proposes a novel approach that disentangles motion from the video diffusion model (VDM) and applies the estimated sparse motion to achieve efficient and effective video interpolation.\nThe main strengths of the paper can be summarized as follows:\n\n1) Disentangling motion field from a one-step VDM is both novel and effective.\n\n2) Integrating the motion field extracted from the VDM into a frame synthesis framework improves efficiency compared to full VDM-based methods.\n\n3) The qualitative and quantitative results are impressive and demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1) Insufficient experimental analysis on efficiency:\nAlthough the proposed framework emphasizes efficiency as a key contribution, the paper lacks detailed quantitative evidence. The authors should include explicit efficiency metrics in Table 1, such as inference time (s/frame) and memory usage (VRAM in GB), to substantiate their claims.\n\n2) Lack of detailed baseline categorization:\nFor video interpolation methods based on video diffusion models (VDMs), it is crucial to clearly categorize baselines into zero-shot, fine-tuned, and fully trained settings. Without this distinction, the fairness and clarity of the comparison become ambiguous, potentially undermining the paper’s original motivation. Reporting the total training compute—e.g., approximate FLOPs or other comparable measures—would further improve the transparency and fairness of the evaluation.\n\n3) Limited ablation studies:\nThe initial estimation of $k$ keyframes using the VDM plays a central role in determining the motion field and thus in the overall effectiveness of the proposed method. However, the paper lacks sufficient analysis of how the number of keyframes ($k$) affects both efficiency and interpolation quality. A detailed ablation study on this factor would greatly enhance the understanding of the method’s underlying principles."}, "questions": {"value": "1) Please include explicit efficiency metrics — such as inference time (s/frame), memory usage (GB), and training compute (e.g., approximate FLOPs or other comparable measures) — in the main manuscript to better support the claimed efficiency of the proposed framework.\n\n2) Please clearly categorize the baselines into zero-shot, fine-tuned, and fully trained settings for a fair comparison. Additionally, consider including more zero-shot video diffusion interpolation works such as TRF [1] and ViBiDSampler [2], which are missing from the current submission.\n\n3) If possible, add a brief ablation study on the number of keyframes ($k$). This would significantly improve the clarity and completeness of the paper by illustrating how $k$ influences both interpolation quality and computational efficiency.\n\n4) It would be helpful to include failure case analysis. Presenting representative failure cases and describing in which situations such samples frequently occur would provide deeper insight into the limitations and behavior of the proposed method.\n\n[1] Explorative Inbetweening of Time and Space, ECCV 2024.\n\n[2] ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler, ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p3qnirnJAz", "forum": "OiWyf1BNtC", "replyto": "OiWyf1BNtC", "signatures": ["ICLR.cc/2026/Conference/Submission324/Reviewer_1Tgm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission324/Reviewer_1Tgm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528966947, "cdate": 1761528966947, "tmdate": 1762915493597, "mdate": 1762915493597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel one-step diffusion method for video frame interpolation. The authors present a two-stage strategy. Specifically, it first decomposes the interpolation process into motion modeling and frame synthesis. The motion modeling and frame synthesis module are trained together in an end-to-end format, with the ground truth frame latents as the additional input. In the second training stage, they train the diffusion model for denoising the pseudo ground truth frame latents for motion prediction. In their experiments, the proposed method outperforms the baselines with fast speed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a novel framework for video frame interpolation, achieving fast speed and competitive performance.\n- The proposed continuous motion field representation enables more flexible motion modeling and generates plausible flow samples, as evidenced by the visualizations."}, "weaknesses": {"value": "- The paper lacks visualizations on more challenging scenarios, such as the “breaking dance” case in the DAVIS dataset.\n- The flow sampling mechanism is not sufficiently explained; more details on how samples are generated and utilized would strengthen the paper.\n- The paper is missing discussions of closely related works that also integrate motion or optical flow modeling in video generation/interpolation, such as *VideoJAM* [1], *Motion-I2V* [2], and *GIMM-VFI* [3].\n- Continuous temporal results. More visualizations like Figure 5 to present the continuous motion modeling and interpolation ability.\n\n[1] VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models.\n\n[2] Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling.\n\n[3] Generalizable Implicit Motion Modeling for Video Frame Interpolation."}, "questions": {"value": "- Is it necessary to use the VAE encoder? Given the recent progress in the academy, it would be interesting to replace the VAE encoder with other encoders, such as DINOv2.\n- Please add more visualizations for the cases indicated in the weakness section, including more challenging scenarios and continuous temporal results, to support the claimed interpolation ability.\n- It is necessary to have discussions with previous closely related work to enhance the clarity of the paper's motivation and contribution. \n- Please add more detailed descriptions for the core motion modeling part, especially the flow sampling operation, for better presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wg0SxYEsOz", "forum": "OiWyf1BNtC", "replyto": "OiWyf1BNtC", "signatures": ["ICLR.cc/2026/Conference/Submission324/Reviewer_G3iW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission324/Reviewer_G3iW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710715752, "cdate": 1761710715752, "tmdate": 1762915493442, "mdate": 1762915493442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The core idea of RDVFI is to disentangle VFI into two stages: motion prediction and appearance generation. Generating new frames based on low-resolution motion information enables RDVFI to perform effectively in both inference speed and generation authenticity.\n2. This is the first diffusion-based VFI method with one-step inference, which achieves 50× acceleration compared to SOTA with also better results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In the diffusion stage, this method generates low-resolution optical flow as an intermediate result to both accelerate the diffusion process and improve the stability of interpolation.\n2. Bi-directional interpolation significantly improves generation authenticity, making RDVFI get SOTA performance on DAVIS and FCVG benchmarks.\n3. The video demo shows significantly better results than existing methods."}, "weaknesses": {"value": "1. Optical flow is the key information for the entire pipeline, as it is utilized for both image warping and feature warping in a bidirectional manner. However, in the first training stage, optical flow results are trained in an unsupervised way. The rationality of this setting requires further verification.\n2. The number of testsets is limited and lacks diversity in their sources. The authors evaluate their method on DAVIS-7 and FCVG, however FCVG is sampled from DAVIS and RealEstate10K.\n3. For the RDVFI-U and RDVFI-D models, the authors set different numbers of key frames, but did not provide a reasonable explanation and lacked corresponding ablation experiments."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nqiZOMBluc", "forum": "OiWyf1BNtC", "replyto": "OiWyf1BNtC", "signatures": ["ICLR.cc/2026/Conference/Submission324/Reviewer_R4Dp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission324/Reviewer_R4Dp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718486308, "cdate": 1761718486308, "tmdate": 1762915493314, "mdate": 1762915493314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a video frame interpolation method using a one-step diffusion model, aiming to improve inference efficiency by eliminating the need for multi-step denoising. The approach decomposes intermediate frame generation into two stages: the first estimates a continuous motion field between the input frames, and the second synthesizes intermediate frames by warping the inputs according to the predicted motion field. Experimental results are compared against conventional video frame interpolation methods as well as diffusion-based approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper aims to improve the efficiency of diffusion models for video frame interpolation by disentangling in-between frame synthesis into two stages: continuous motion prediction and warping. The continuous motion (which use a spline interpolation curve) prediction stage is lightweight, operating on latent features from spatially and temporally downsampled video representations, which reduces memory consumption. In addition, the method employs a one-step diffusion model to predict the latent features during inference time further increasing inference speed."}, "weaknesses": {"value": "The main weakness is the experiment does not convince me of the effectiveness of the method:\n1. There are only six qualitative video comparisons in the supplementary material, which are insufficient to demonstrate the superior quality of the proposed method. Moreover, I did not observe a clear visual difference between Wan and the proposed approach.\n2.  Lacks baselines on direct one/few-step distillation of video in-betweening diffusion models, rather than decomposing the process into two stages as done in the paper."}, "questions": {"value": "1. How does  the number of keyframes in the continuous motion representation affect the inbetweening results especially in 24x interpolation?\n2. Directly fine-tuning the original diffusion denoiser to perform full noise removal in a single step seems rather ambitious. Incorporating a distillation-based loss might help improve stability and performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ETH88oYLDL", "forum": "OiWyf1BNtC", "replyto": "OiWyf1BNtC", "signatures": ["ICLR.cc/2026/Conference/Submission324/Reviewer_Ba4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission324/Reviewer_Ba4o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720580936, "cdate": 1761720580936, "tmdate": 1762915493098, "mdate": 1762915493098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}