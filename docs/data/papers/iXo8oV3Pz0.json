{"id": "iXo8oV3Pz0", "number": 11871, "cdate": 1758204398829, "mdate": 1762969233396, "content": {"title": "Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts", "abstract": "Practical post-training pipelines involving on-policy generation typically produce multiple candidate responses per prompt. However, popular alignment methods like Direct Preference Optimization (DPO) are restricted to pairwise comparisons, discarding valuable supervisory signal. In this setting, we propose Multi-Preference Optimization (MPO), a generalization of DPO that optimizes over entire sets of selected and rejected responses. This set-level contrastive approach is theoretically grounded: we first prove that leveraging $n$ responses achieves a $\\mathcal{O}\\bigl(\\tfrac{1}{\\sqrt{n}}\\bigr)$ convergence in TV-distance to the true preference distribution. We then prove, under a formal model with spacing-scaled Gaussian noise ($\\Delta, \\sigma = \\mathcal{O}(1/n)$), that MPO's 2-bin partition reliability remains bounded away from zero, in contrast to full-ranking methods which degrade exponentially ($\\exp(-\\mathcal{O}(n))$). To further enhance learning, MPO employs a deviation-based weighting, which emphasizes outlier responses to induce an implicit curriculum. Empirically, as we show over multiple models and benchmarks,  MPO achieves state-of-the-art performance, with an improvement of up to $\\sim 17.5$\\% WR on AlpacaEval2 in the on-policy iterative setting, and state-of-the-art results in off-policy settings.", "tldr": "We introduce Multi-Preference Optimization (MPO), a novel method that generalizes DPO to efficiently learn from sets of preferred and dispreferred responses, achieving state-of-the-art LLM post-training optimization performance", "keywords": ["Preference Optimization", "Direct Preference Optimization", "DPO", "Multi-Preference Optimization", "MPO", "Policy Optimization", "Reinforcement Learning from Human Feedback", "RLHF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd11ed64109e83f79870f6e8329b4d94cbc2eb12.pdf", "supplementary_material": "/attachment/af22c1be3d3c3ce34211d211bd8ee9b7b65667ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a set-level, contrastive preference optimization framework that generalizes DPO with groupwise. \nEmpirically, MPO and W-MPO deliver state-of-the-art results on AlpacaEval2 (WR, LC-WR), Arena-Hard, and MT-Bench across both off-policy and on-policy training regimes, with performance in some cases approaching that of GPT-4o on AlpacaEval2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "MPO retains the simplicity and elegance of DPO while naturally extending it to handle multiple responses per prompt, without the extra cost of full ranking or reward calibration. \nIt achieves consistent state-of-the-art performance across model sizes and training regimes, scaling effectively with more responses per query and remaining competitive even under limited data or compute budgets."}, "weaknesses": {"value": "Like DPO, MPO’s objective relies on the log-ratio between the policy and the reference model. It would be useful to analyze how sensitive performance is to the choice or vintage of the reference model (e.g., Llama vs. Qwen families).\n\nW-MPO weights samples by their absolute deviation from the mean. Why use absolute deviation, and why the mean specifically? Including ablations with alternative robust statistics (e.g., median, trimmed mean, quantiles) could clarify stability under skewed or noisy reward distributions.\n\nIn on-policy settings, top-k and bottom-k responses depend on a particular reward model. How robust is MPO/W-MPO to miscalibration or domain shift in that scorer? Cross-reward or human-labeled validation would strengthen the reliability of the findings.\n\nThe main paper appears to be 10 pages long, while the conference strictly limits submissions to 9 pages. Please ensure the paper adheres to the page limit requirements."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J2T3a2AIUP", "forum": "iXo8oV3Pz0", "replyto": "iXo8oV3Pz0", "signatures": ["ICLR.cc/2026/Conference/Submission11871/Reviewer_mVRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11871/Reviewer_mVRu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760628342821, "cdate": 1760628342821, "tmdate": 1762922890360, "mdate": 1762922890360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a set-level, contrastive preference optimization framework that generalizes DPO with groupwise. \nEmpirically, MPO and W-MPO deliver state-of-the-art results on AlpacaEval2 (WR, LC-WR), Arena-Hard, and MT-Bench across both off-policy and on-policy training regimes, with performance in some cases approaching that of GPT-4o on AlpacaEval2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "MPO retains the simplicity and elegance of DPO while naturally extending it to handle multiple responses per prompt, without the extra cost of full ranking or reward calibration. \nIt achieves consistent state-of-the-art performance across model sizes and training regimes, scaling effectively with more responses per query and remaining competitive even under limited data or compute budgets."}, "weaknesses": {"value": "Like DPO, MPO’s objective relies on the log-ratio between the policy and the reference model. It would be useful to analyze how sensitive performance is to the choice or vintage of the reference model (e.g., Llama vs. Qwen families).\n\nW-MPO weights samples by their absolute deviation from the mean. Why use absolute deviation, and why the mean specifically? Including ablations with alternative robust statistics (e.g., median, trimmed mean, quantiles) could clarify stability under skewed or noisy reward distributions.\n\nIn on-policy settings, top-k and bottom-k responses depend on a particular reward model. How robust is MPO/W-MPO to miscalibration or domain shift in that scorer? Cross-reward or human-labeled validation would strengthen the reliability of the findings.\n\nThe main paper appears to be 10 pages long, while the conference strictly limits submissions to 9 pages. Please ensure the paper adheres to the page limit requirements."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J2T3a2AIUP", "forum": "iXo8oV3Pz0", "replyto": "iXo8oV3Pz0", "signatures": ["ICLR.cc/2026/Conference/Submission11871/Reviewer_mVRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11871/Reviewer_mVRu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760628342821, "cdate": 1760628342821, "tmdate": 1763301433494, "mdate": 1763301433494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main text of this paper is 10 pages, which exceeds the page limit for ICLR submissions. According to ICLR policy, this paper should be desk-rejected."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sYKrRM2qxI", "forum": "iXo8oV3Pz0", "replyto": "iXo8oV3Pz0", "signatures": ["ICLR.cc/2026/Conference/Submission11871/Reviewer_Z2fi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11871/Reviewer_Z2fi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858181440, "cdate": 1761858181440, "tmdate": 1762922889222, "mdate": 1762922889222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional DPO methods only allow a pair of preference data to be trained. In this paper, they propose MultiPreferenceOptimization(MPO), a generalization of DPO that optimizes over entire sets of selected and rejected responses. Within the paper, authtors provide some theoretical evident on why MPO works better than other DPO-style methods. Experiments are conducted on several open-ended benchmarks and the results show that MPO achieves better performance than other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides theoretical evidence on why MPO works better than other DPO-style methods.\n2. The experiments are conducted and MPO are compared with several strong baselines."}, "weaknesses": {"value": "1. What is the difference between the \"Off-policySetting\" and \"On-policySetting\"? It seems that they only differ in the initial model (off-policy uses a weaker sft model, while the on-policy uses a stronger open-sourced instruct model). If so, why they get this name? Based on my understanding, Off-policy and On-policy should be different in how they are trained (sample from base model or from current policy model).\n2. In the experiments, it seems some of strong baseline models are missing -- SimDPO [1]、BMC[2] etc. The authors should enrich their baseline comparsion.\n3. Except for performance improvement, authors should also conduct some analysis on the strength of their approaches (e.g., case study). What is the additional advantages of MPO? (the authors provide results on data efficiency)"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ksw3G1ch4F", "forum": "iXo8oV3Pz0", "replyto": "iXo8oV3Pz0", "signatures": ["ICLR.cc/2026/Conference/Submission11871/Reviewer_vGFT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11871/Reviewer_vGFT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027681654, "cdate": 1762027681654, "tmdate": 1762922888715, "mdate": 1762922888715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the important challenge of aligning LLMs in the post-training and introduces  Multi-Preference Optimization (MPO), a generalization of Direct Preference Optimization (DPO) that extends beyond pairwise comparisons. The approach optimizes over the entire sets of selected and rejected responses, potentially capturing valuable supervisory signal. The method is supported by a theoretical analysis proving that using n responses leads to a faster convergence rate (O(1/√n)) and that MPO's 2-bin partitioning is more robust to reward model noise than full-ranking methods like Plackett-Luce. The paper also introduces Weighted MPO (W-MPO), which uses reward score deviations to create an implicit curriculum by up-weighting informative outlier responses. The approach shows strong empirical results with an improvement of up to 17.5% win rate on AlpacaEval2 in the on-policy iterative setting, and state-of-the-art results in off-policy settings and mostly equivalent or better results on Arena-Hard and MT-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important challenge in the alignment process, and the presented method is a clean and intuitive generalization of DPO, moving from pairwise to set-wise comparisons.\n- The method is shown to achieve state-of-the-art results across a variety of models, benchmarks, and training paradigms (off-policy, on-policy, iterative), demonstrating its robustness and effectiveness, especially on AlpacaEval2.\n- The paper provides both theoretical motivation (Theorems 1 & 2) and strong empirical validation for its core claims, particularly the benefits of using more responses and the robustness of 2-bin partitioning over full-ranking.\n- Strong selection of benchmarks like AlpacaEval 2.0, Arena-Hard, and MT-Bench indicates generalizability of the approach."}, "weaknesses": {"value": "- The on-policy results depend on a single reward model. It is possible that MPO is particularly good at optimizing for the specific reward distribution of the Skywork RM, and its gains might be less pronounced with other RMs.\n-  The theoretical result on noise robustness (Theorem 2) relies on a specific \"spacing-scaled\" noise model. It is unclear how realistic the assumption is.\n- Relying on RM leaves actual alignment to be questionable; the study would benefit from human evaluation and more qualitative case studies."}, "questions": {"value": "- Could the authors elaborate on potential failure modes for MPO? For instance, how would the mean-based partitioning perform if the reward distribution for a prompt is strongly bimodal?\n- Regarding the On-Policy Data: In your on-policy experiments, you discard the median-reward response. What was the rationale for this decision?\n- You mentioned skipped sets. How common are these in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OLnTcEn0q0", "forum": "iXo8oV3Pz0", "replyto": "iXo8oV3Pz0", "signatures": ["ICLR.cc/2026/Conference/Submission11871/Reviewer_mSvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11871/Reviewer_mSvB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028167415, "cdate": 1762028167415, "tmdate": 1762922888245, "mdate": 1762922888245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}