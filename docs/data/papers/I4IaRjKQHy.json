{"id": "I4IaRjKQHy", "number": 16471, "cdate": 1758264913917, "mdate": 1759897238610, "content": {"title": "WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity", "abstract": "Large Language Models (LLMs) deliver strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, leveraging its data adaptation and low computational overhead. However, existing methods typically only rely on activation information and a uniform sparsity ratio, overlooking the critical interplay with weights and inter-block sensitivity variation, which leads to suboptimal performance. In this paper, we examine these limitations and identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. To address these issues, we propose a novel Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse) method that leverages both activation and weight information and enables adaptive sparsity allocation across different granularities. Specifically, we introduce a weight-aware activation sparsification mechanism that integrates activation magnitudes with precomputed weight norms to more accurately identify salient channels. This is combined with a mixed-granularity sparsity allocation scheme featuring a coarse-to-fine strategy: a global sparsity budget is first distributed across blocks via evolutionary search to protect sensitive regions, and subsequently refined at finer granularities within each block to minimize reconstruction error. We improve existing sparse kernels and demonstrate the effectiveness of the proposed method via extensive experiments conducted on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1’s dense model performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research contributes to advancing the performance limits of training-free approaches for efficient LLM inference, effectively pushing the boundaries of achievable speedup without training.", "tldr": "", "keywords": ["Large Language Model", "Sparsity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07cf9d567a297db110208a25971eb5f542b9f831.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a training-free activation sparsity framework called WiSparse. Unlike prior activation-only methods (e.g., CATS), WiSparse integrates weight information into activation saliency estimation and employs a mixed-granularity sparsity allocation strategy. It combines a weight-aware importance score with a two-stage evolutionary search to adapt sparsity ratios to model sensitivity. The method demonstrates notable improvements in accuracy retention at 50% sparsity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed weight-aware importance score addresses a well-motivated limitation in previous activation-only sparsity methods.\n\n- The method is tested on multiple LLM families using diverse benchmarks."}, "weaknesses": {"value": "- The reported accuracy improvements appear modest relative to the added algorithmic and implementation complexity.\n\n- The method seems over-engineered; it is unclear why the simple metric of \"weight × activation\" is not sufficient to guide pruning decisions. Furthermore, why is an evolutionary algorithm needed when simpler alternatives—such as an ILP-based allocation—could potentially achieve similar results with lower overhead.\n\n- The paper does not report how much time each component (e.g., evolutionary search, grid search etc.) contributes to the total cost (even though it may be offline), making it difficult to assess the practicality of the approach.\n\n- The claim that sparsity sensitivity varies across layers or blocks is not new and has been established in prior sparsity literature; thus, it should not be positioned as a key contribution.\n\n- The work does not directly compare end-to-end inference latency with existing baselines, leaving uncertainty about real-world runtime advantages beyond FLOP reductions.\n\n- The paper reports results on sparsity ratios of only up to 50%."}, "questions": {"value": "Concerns/Questions and Points to Address in Rebuttal:\n- Recent work on LLM sparsity should be cited. Such as [1], [2].\n\n- Experiments to demonstrate why a simple ILP is not sufficient and time breakdown of performing evolutionary algorithm and the different searches.\n\n- GSM8K is not a complex reasoning task. An example of a complex reasoning task is AIME. This statement should be corrected in results section.\n\n- The colors make the text in Fig. 4 hard to read.\n\n- Experiments must be conducted on higher sparsity ratios. I believe LLMs are able to handle up to 60/65% sparsity. This is also supported by recent work.\n\n- It should be clarified in introduction what kind of sparsity this work tackles, which is \"channel sparsity\".\n\n- Experiments must be conducted using the \"simple selection rule\". It is an important but overlooked baseline. The morale behind the current selection rule is not clear and it must be demonstrated why these specific choices were made and how it helps. It is not intuitive as to how the exponent term etc. have been arrived at.\n\n- Inconsistencies in notation, \n$s_i$ is initially the score i.e., the output of a function and later becomes a function.\n\n- Curious how this technique might work when compounded with the emerging area of sparsity compensation [3].\n\n- Why can't stage 1 of sparsity allocation be done in a greedy manner ?\n\n- Experiments on additional model sizes must be done, currently all models are within the 8B range.\n\nReferences:\n\n[1]  Ramachandran, A., Kundu, S., Raha, A., Kundu, S., Mathaikutty, D. K., & Krishna, T. (2025). Accelerating llm inference with flexible n: M sparsity via a fully digital compute-in-memory accelerator. arXiv preprint arXiv:2504.14365.\n\n[2] Yin, L., Wu, Y., Zhang, Z., Hsieh, C. Y., Wang, Y., Jia, Y., ... & Liu, S. (2023). Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175.\n\n[3] Lee, M., Ramachandran, A., & Krishna, T. RECAP: Training-Free Compensation for Coarse Activation Channel Pruning in Compressed LLMs. In Machine Learning for Computer Architecture and Systems 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WzKtgqd5XX", "forum": "I4IaRjKQHy", "replyto": "I4IaRjKQHy", "signatures": ["ICLR.cc/2026/Conference/Submission16471/Reviewer_4HmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16471/Reviewer_4HmV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716677948, "cdate": 1761716677948, "tmdate": 1762926577787, "mdate": 1762926577787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WiSparse, a training-free activation sparsity framework for large language models (LLMs).  WiSparse incorporates weight awareness sparse activation from WINA, further proposes mixed-granularity allocation.  Experiments on Llama-3.1-8B, Qwen-2.5-7B, and Mistral-7B demonstrate the efficacy of this approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is written and organized well and technically sound.\n- The mixed-granularity allocation is reasonable to bringing more performance gain"}, "weaknesses": {"value": "- Lack of proper discussion. The weight awareness sparsity activation (eq 4, Sec 4.2) is the same as the one proposed by WINA. Though WiSparse discussed WINA in the related works, it would be suggested to further refer in Sec 4.2 to clarify the real contributions of this work. \n\n- Lack of numerical comparison. Conducting a direct numerical comparison with WINA to present the gain of mixed-granularity allocation is a recommendation.\n\n- Lack of discussion with more pruning works regarding block sparsity allocation upon calibration datasets. Discussing with these works are also recommended.\n\n- Lack of clarity. The evolution search algorithm is unclear without sufficient description.\n\nWINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference.\n\nLoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery.\n\nShortGPT: Layers in Large Language Models are More Redundant Than You Expect."}, "questions": {"value": "See the weakness.\n\nI would consider increasing rating if the comments are properly resolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JDR1U7ztnX", "forum": "I4IaRjKQHy", "replyto": "I4IaRjKQHy", "signatures": ["ICLR.cc/2026/Conference/Submission16471/Reviewer_Um4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16471/Reviewer_Um4s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887692998, "cdate": 1761887692998, "tmdate": 1762926577252, "mdate": 1762926577252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a training-free activation sparsity scheme WiSparse, which scores the channel importance by a weight-aware criterion and adaptively assigns the sparsity ratio for different blocks and layers. Experiments are conducted on multiple benchmarks and models, demonstrating the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The two insights are reasonable and well-motivated for the method design.\n2. WiSparse conducted a more fine-grained sparsity design for the weight-activation-based sparsity paradigm, which makes it more robust.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The paper somehow lacks a significant novelty compared to WINA, which seems to be an incremental improvement for WINA.\n2. The experimental comparison is insufficient, as I think WINA should be an important baseline.\n3. Although the authors claimed that the static norm is inadequate, WiSparse still uses the L2 norm as the base, where the only difference is an exponential $\\alpha_i$. Are there any insights about $\\alpha_i$ across different layers?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pCDnbbi1lO", "forum": "I4IaRjKQHy", "replyto": "I4IaRjKQHy", "signatures": ["ICLR.cc/2026/Conference/Submission16471/Reviewer_gfy4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16471/Reviewer_gfy4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896669350, "cdate": 1761896669350, "tmdate": 1762926576410, "mdate": 1762926576410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free activation sparsity framework called WiSparse. Unlike prior activation-only methods (e.g., CATS, TEAL), WiSparse combines activation magnitudes with precomputed weight norms via a layer-wise exponent and uses a coarse-to-fine sparsity allocation scheme (evolutionary search over block sparsities and greedy intra-block allocation). The method aims to better preserve accuracy at high sparsity (up to 50%)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clearly-motivated problem: shows empirical evidence that activation-only criteria can prune channels with small activations but very large weight columns, and that block-wise sparsity sensitivity is highly non-uniform.\n- Mixed-granularity sparsity allocation (block-level evolutionary search + layer-level greedy search) is reasonable.\n- Comprehensive empirical evaluation on three different 7–8B LLMs (Llama-3.1, Mistral, Qwen2.5) across multiple benchmarks, with consistent gains over strong training-free baselines (TEAL, R-Sparse), especially at 50% sparsity.\n- Reports both FLOP reductions and real end-to-end throughput improvements on GPU, showing that sparsity translates into actual speedups."}, "weaknesses": {"value": "- Conceptual novelty is somewhat limited relative to prior weight-aware sparsity (e.g., WINA) and activation-based methods (TEAL/R-Sparse). \n- The calibration and search pipeline appears non-trivial, but the paper does not quantify its wall-clock overhead or resource requirements.\n- Experiments are restricted to ~7–8B models and a single hardware setup; it is unclear how well WiSparse scales to larger models (e.g., 30B+) or different batch sizes."}, "questions": {"value": "- How sensitive are the learned $α_ℓ$ and sparsity allocations to the choice and composition of the calibration set? Does performance degrade if evaluation tasks differ significantly from calibration tasks?\n- Have you explored sparsity levels beyond 50% (e.g., 60–70%)? If so, how does WiSparse compare to TEAL/R-Sparse at those points, and where does accuracy begin to collapse?\n- Can you quantify how much of the total inference time is spent computing scores/masks versus running sparse kernels, and how this scales with batch size and sequence length?\n- Do you foresee any practical issues applying WiSparse to larger LLMs (e.g., 30B, 70B)? Any preliminary results or observations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j6vUAZ8spB", "forum": "I4IaRjKQHy", "replyto": "I4IaRjKQHy", "signatures": ["ICLR.cc/2026/Conference/Submission16471/Reviewer_vQjd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16471/Reviewer_vQjd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16471/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763112632469, "cdate": 1763112632469, "tmdate": 1763112632469, "mdate": 1763112632469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}