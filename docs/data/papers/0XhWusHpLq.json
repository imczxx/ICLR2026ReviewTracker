{"id": "0XhWusHpLq", "number": 23538, "cdate": 1758345119282, "mdate": 1759896809699, "content": {"title": "Finding Symmetry in Neural Network Parameter Spaces", "abstract": "Parameter space symmetries, or loss-invariant transformations, are important for understanding neural networks' loss landscape, training dynamics, and generalization. However, identifying the full set of these symmetries remains a challenge. In this paper, we formalize data-dependent parameter symmetries and derive their infinitesimal form, which enables an automated approach to discover symmetry across different architectures. Our framework systematically uncovers parameter symmetries, including previously unknown ones. We also prove that symmetries in smaller subnetworks can extend to larger networks, enabling direct generalization of discovered symmetries to more complex models.", "tldr": "", "keywords": ["Parameter space symmetry"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ca48bebfdbed24ff1005e017cf4ba20bda83922.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors claim to make a few contributions to finding symmetries in neural network parameter spaces. They claim to introduce an automated approach for discovering symmetry groups (and their group actions) in these parameter spaces, by defining the search space and using neural networks to learn them. They also claim that they can use discoveries about the symmetries of parameter spaces for smaller neural networks to learn about those for larger ones where the smaller ones are a subnetwork of the larger ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Let me state clearly for the record that the topic area is not one I am overly familiar with (i.e. I do not know the prior literature, nor - prior to reading this paper - the central questions in this topic area). So I suggest that my review is taken with a pinch of salt.\n\nIn saying that, I thought that\n- the paper was very well written and I could get a sense of the problem that they were tackling, as well as why one would want to look at this problem\n- the proofs of their theorems are correct (to the best of my knowledge)\n- the authors demonstrate what they set out to achieve"}, "weaknesses": {"value": "I only have one particular thought on this, which is: \n\nHow useful is it just to prove the existence of certain symmetries? I understand that the authors have demonstrated the existence of certain ones that weren't known before using their framework, but how would one go on and use this knowledge in the potential applications that they list in their introduction (such as optimising neural networks)?"}, "questions": {"value": "Beyond the question posed in the weaknesses section: I would be interested in the authors commenting on the following:\n\nCan the results be extended beyond linear groups? If not, what are the key issues behind this?\n\n(Also I think there may be a mistake in line 325, do they not mean L_{invariance} instead of L_{Lie_deriv}?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MSRUANvglI", "forum": "0XhWusHpLq", "replyto": "0XhWusHpLq", "signatures": ["ICLR.cc/2026/Conference/Submission23538/Reviewer_Sfv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23538/Reviewer_Sfv7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298760555, "cdate": 1761298760555, "tmdate": 1762942703432, "mdate": 1762942703432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study and provide methods for discovering symmetries in neural networks parameter spaces. Focusing on infinite groups and their Lie generators, they have the following contributions. First, in Theorem 3.1 and 3.2 they prove the necessary conditions that the loss and group action derivatives have to obey for a given set of generators. This allows them to promote these quantities during training, yielding the proposed loss function in Equation 4. Moreover, in Section 4 they proposed a modular viewpoint of symmetry discovery in neural networks parameter spaces and show that symmetries of small subnetworks of a big model will eventually appear as symmetries of the whole model. This is characterized in Proposition 4.1 and is applied to neural networks in Corollaries 4.2 and 4.3. They complete this section by proving that data-dependent symmetries, if observed in a batch, are also valid symmetries for subsets of the batch. The paper is concluded with a number of experiments (small scale) to show that for both data-dependent and data-independent symmetries, their proposed loss provides non-trivial solutions. The experiments are mostly providing a proof of concept for the proposed loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Neural network parameter spaces and their symmetries are an active area of research, and results in this direction are potentially interesting to a large body of the community.\n\n- The paper is well written so that it needs minimal background to understand the method and results."}, "weaknesses": {"value": "- The experiments are just for the proof of the concept. However, the main contribution of this paper is the introduction of a loss that promotes symmetry discovery and allows to algorithmically achieve it. This work needs large-scale, comprehensive experiments before publication, in my opinion.\n\n- The applications of discovering data-dependent symmetries are missing. One expects at least some experiments and some toy theory on why we need such things."}, "questions": {"value": "This is a nice, well-written paper on how to discover symmetries in neural networks' parameter spaces. The paper's most important contribution is a new loss function in Equation 4, along with a number of theoretical results, which provide an algorithm for symmetry detection. The focus of the paper is on infinite groups, and data depedendant symmetries are also handled in this framework.\n\n\nHowever, I think this paper at this moment is not ready for publication. The main reason is that the main contribution is the introduction of a method, a loss function, which leads to an algorithm for symmetry discovery. But the authors just provided proof of concept, small-scale experiments, which are not enough to validate the method for real cases of deep learning. One needs large scale (or at least mid scale) experiments, with concrete applications. Indeed, why are such symmetries in data needed to be discovered? What kind of underlying task gets resolved in I learned data-dependent symmetries? Are data-dependent symmetries some kind of noisy signals discovered from the overparametrized models, or do they carry a meaningful signal? In my opinion, the paper needs to have an explicit experiment showing that the application of the data-dependent symmetry discovery leads to some performance improvement. One may find bio-related datasets useful in this direction.\n\nI feel my concerns need some time well beyond the rebuttal period to be applied. So at the moment I recommend rejection, while encouraging the authors to apply the comments above and have a completely nice paper. \n\nSome questions:\n\n - How does the objective in Section 5.1 enforce group law? Any proofs? Or this is just heuristics?\n\n - Section 5.2: How is the hyperparameter beta chosen? Grid search, or there is some formula that at least gives us some hint on where to search?\n\n - Why Equation (4) returns a good symmetry? Any proofs?\n\n - Figure 4: An interpretation of what those data-dependent symmetries mean is missing\n\n\n\n - What does Figure 2 mean? It's a bit unclear (I understand what it means, but it takes some time so it's not providing much help for the reader)\n\n\n - Line 296: typo: 'a effective'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LjQ7rPlYXJ", "forum": "0XhWusHpLq", "replyto": "0XhWusHpLq", "signatures": ["ICLR.cc/2026/Conference/Submission23538/Reviewer_vVw6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23538/Reviewer_vVw6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703537286, "cdate": 1761703537286, "tmdate": 1762942703123, "mdate": 1762942703123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of finding data-dependent symmetries in neural network parameters. Recall that parameter symmetries are typically defined as transformations that can be performed on a neural network of arbitrary weights which result in an equivalent function (different weights, same function). This paper considers the broader question of data-dependent transformations (that is, the transformations only result in the same function on a subset of datapoints). It starts out by describing a formal framework of definitions to make this precise. It then translates these definitions into an infinitesimal version that can be calculated at the level of the Lie algebra, providing a couple of examples along the way. This is important since all calculations in practice will be performed at this local level. Finally, the paper describes an empirical method for finding symmetries by learning Lie algebra element actions on network parameters. Several very small-scale experiments ($<25$ network parameters) are analyzed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**An interesting and important topic, the research direction seems promising:** It is this reviewer’s opinion that neural network symmetries remain an underappreciated topic in deep learning (though this seems to be steadily changing). Further, since many symmetries that are easy for a human to identify have probably already been found, turning to data-driven approaches seems like a promising direction. \n\n**The paper presents a decent foundation on which to build the proposed method:** While the reviewer has comments in the *Weaknesses* section about presentation and organization, he did appreciate the way in which the paper lays out all of the supporting  material before presenting the main method in the work. The reviewer thinks this is important. Too many works in this field rush to present new methods/results without establishing notation and definitions (which can shift from work to work), leading to confusion and misinterpretation. With some polishing this paper could read very well.\n\n**Focusing on some small-scale examples is helpful:** This reviewer anticipates that there may be some criticism of this paper for performing experiments on very small networks. While the reviewer agrees that including larger-scale experiments would ultimately make the paper stronger, he also thinks the small-scale experiments are an important component. Many readers of this type of work will likely want to understand new symmetries, not just know they are there. This understanding probably requires working with small examples."}, "weaknesses": {"value": "**Deeper analysis of the experiments would be valuable:** The reviewer is very interested in this direction of research and was disappointed that the analysis of the experimental results was cursory. One can imagine a very thorough analysis that could help point the way towards very specific, non-trivial symmetry discovery (that could be later verified formally). This would be a great way to validate the method and would add to the scientific contribution. It would also align with the current experimental set-up which focuses on small networks. \n\n**The motivation for data-dependence was not clear:** Reading the first half of this paper, it felt like the motivation was to look for symmetries that were specifically data dependent. At the end however, it seemed that data-dependence may have been included because the method for finding symmetries depends on evaluation on a finite set of datapoints (and hence is data-dependent by necessity). Which is it? If data-dependence is introduced because the method requires it, that is fine, but it would be helpful to state this up-front. \n\n**More experiments:** Along with deeper analysis, it would be helpful to have more experiments (even if they remain small scale). The reviewer is curious what kinds of groups end up showing up here? Is there an easy way to measure this? It seems that some of the familiar symmetries (e.g., permutations) don’t appear because they are coming from finite groups?\n\n\n**Nitpicks:**\n- Section 3.1: It might be helpful to put in a single example or two to help ground this section. Maybe a case the reader will be familiar with.\n- Line 139: May be worth reminding the reader you are only considering Lie groups here.\n- Line 354: It may just be this reviewer’s opinion, but underscores look better in code variables than in LaTeX.\n- Line 325: I don’t think $\\mathcal{L}_{Lie deriv}$ is defined?\n- Line 119: Is ‘associative’ the right word here? This seems to be essentially the homomorphism property?\n- Line 187: ‘To facilitates…’ $\\mapsto$ ‘To facilitate…’.\n- Line 336: “Therefore, in implementation…” $\\mapsto$ “Therefore, in our implementation…”"}, "questions": {"value": "- Line 112: The reviewer is a little confused about this definition. Informally, a set $X$ from the data space $\\mathcal{D}$ is sent to a map $a_X$ which takes pairs $(g,\\theta)$ and applies $g$ to $\\theta$ to get $\\theta’$. Where does $X$ come in? There is just some arbitrary dependence on $X$ that is specified case by case?\n- Line 362: How do you choose the number of $h$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MdLWbEPqyk", "forum": "0XhWusHpLq", "replyto": "0XhWusHpLq", "signatures": ["ICLR.cc/2026/Conference/Submission23538/Reviewer_rWxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23538/Reviewer_rWxR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853201795, "cdate": 1761853201795, "tmdate": 1762942702859, "mdate": 1762942702859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an automated framework for discovering parameter space symmetries that keep a neural network’s loss function unchanged. It formalizes data-dependent symmetries, derives their infinitesimal form, and enables efficient learning of nonlinear and data-dependent group actions that can generalize from small subnetworks to larger architectures. Empirically, the method recovers known symmetries and uncovers new data-dependent and cross-layer ones, providing a principled step toward systematic symmetry discovery in neural networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a clear and well-founded framework for discovering parameter symmetries in neural networks by defining data-dependent and infinitesimal forms. The theory is mathematically sound and shows how symmetries in small subnetworks can extend to larger ones. The experiments, though simple, support the main idea by recovering known symmetries and suggesting new ones."}, "weaknesses": {"value": "1. The paper proposes an automated discovery framework for finding parameter symmetries, but the experiments only visualize small toy models (2–3 layer MLPs with a few parameters). There are no quantitative metrics showing how well the learned symmetries actually preserve loss invariance or how robust they are compared to random directions. Without such quantitative evidence, it is unclear whether the discovered new symmetries are genuine or artifacts of optimization.\n\n2. The claimed goal is to find symmetries in large neural networks by extending from smaller subnetworks, but the experiments never go beyond extremely small MLPs. The paper does not test on modern architectures like Transformers or ResNets, where the approach’s scalability and generality would matter most. Thus, it remains unproven that the framework can meaningfully handle high-dimensional parameter spaces.\n\n3. While the authors claim to discover previously unknown data-dependent or non-contiguous symmetries, there is no rigorous verification or analytical characterization of these symmetries. The results are limited to visual matrices of learned generators, without mathematical confirmation that they correspond to valid group actions or new invariances."}, "questions": {"value": "1. How sensitive is the discovery process to the choice of architecture for the action network $a_X$? Does the method require specific inductive biases or regularization strategies to ensure meaningful, interpretable group actions rather than degenerate solutions?\n\n2. In the infinitesimal formulation, the framework assumes smoothness and differentiability of the loss with respect to parameters. How does this apply to piecewise-linear networks such as ReLU architectures, where the loss landscape is not globally smooth?\n\n3. One of the paper’s stated motivations is improving downstream applications such as optimization and generalization. Have the authors attempted to use the discovered symmetries to modify training dynamics, e.g., via symmetry-aware optimization or reparameterization, and if so, what were the outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zeB95JMGxm", "forum": "0XhWusHpLq", "replyto": "0XhWusHpLq", "signatures": ["ICLR.cc/2026/Conference/Submission23538/Reviewer_Uz1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23538/Reviewer_Uz1c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997747947, "cdate": 1761997747947, "tmdate": 1762942702568, "mdate": 1762942702568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a formalization of data-dependent parameter symmetries in NNs. It also proposes an automated framework to discover them, and runs experiments in some interesting toy settings. The discovery method relies on learning an infinitesimal form of the symmetry, optimizing an objective function (Eq. 4) that enforces loss invariance, the identity axiom, and associativity. They find some previously unknown data-dependent symmetries in architectures like sigmoid and tanh MLPs that e.g. act on non-contiguous layers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a novel formalization of data-dependent symmetries, and a discovery algorithm using Lie algebras.\n- The paper makes and validates several interesting testable predictions, like the existence of novel, data-dependent symmetries in non-homogeneous networks. The authors show that the loss remains nearly constant along the paths generated by these learned symmetries."}, "weaknesses": {"value": "- I am confused about Thm. 3.2. The group action's identity axiom seems to force the term D_{\\theta} a\\_{X}|\\_{I, \\theta} to be the identity map. This would cause the theorem to collapse to a simple statement about linearity, rendering the \\mathcal{L}\\_{\\text{assoc}} loss term \"vacuous\" and failing to impose any new constraint from associativity.\n- There is limited empirical scope. The results are on small 2–3 layer MLPs, and it’s not clear how scaling to data-dependent symmetries in in larger architectures will go. Some discussion here seems merited."}, "questions": {"value": "Some small issues with the loss term as it is written in the paper, that all should be easily patched:\n- Orthogonality: The regularizer is described as enforcing orthogonality but is incorrectly formulated. By minimizing the cosine similarity, it actively encourages generators to become anti-parallel (cosine = -1) rather than orthogonal (cosine = 0). \n- Regularization: The \\mathcal{L}\\_{\\text {reg_id }} term contains a redundant inner $\\min_{a,h}$, which would cause the term to collapse to a constant during optimization instead of acting as a regularizer. \n- Associativity: The main \\mathcal{L}\\_{assoc} loss is introduced as a vector but is used as a scalar in the final objective function (Eq. 4) without specifying the required norm (e.g., $L\\_2$ norm) needed to make it a scalar value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "brUidG1Pq5", "forum": "0XhWusHpLq", "replyto": "0XhWusHpLq", "signatures": ["ICLR.cc/2026/Conference/Submission23538/Reviewer_Cv5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23538/Reviewer_Cv5L"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762893636554, "cdate": 1762893636554, "tmdate": 1762942702190, "mdate": 1762942702190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}