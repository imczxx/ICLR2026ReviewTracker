{"id": "92PM2kSzK1", "number": 12757, "cdate": 1758210106834, "mdate": 1763147582020, "content": {"title": "Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data", "abstract": "Diffusion models have achieved remarkable success in various generative tasks, but training them remains highly resource-intensive, often with millions of images and GPU days of computation required. From a data-centric perspective addressing the limitation, we study diffusion dataset condensation as a new challenging problem setting that aims at constructing a \"synthetic\" sub-dataset with significantly fewer samples than the original dataset for training high-quality diffusion models significantly faster. To the best of our knowledge, we are the first to formally study the dataset condensation task for diffusion models, while conventional dataset condensation focused on training discriminative models. For this new challenge, we further propose a novel $D$iffusion $D$ataset $C$ondensation ($D^{2}C$    ) framework, that consists of two phases: Select and Attach. The Select phase identifies a compact and diverse subset via a diffusion difficulty score and interval sampling, upon which the Attach phase enhances conditional signals and information of the selected subset by attaching rich semantic and visual representations. Extensive experiments across dataset sizes, model architectures, and resolutions demonstrate that our $D^{2}C$    can train diffusion models significantly faster with dramatically fewer data while retaining high visual quality. Notably, for the SiT-XL/2 architecture, our $D^{2}C$  achieves a $100 \\times x$ acceleration, reaching a FID of 4.3 in just 40k steps using only 0.8% of the training data.", "tldr": "We propose the first dedicated framework for diffusion dataset condensation: training diffusion models significantly faster and with dramatically fewer data, while retaining high output quality.", "keywords": ["dataset condensation", "diffusion model", "image generation", "interval sampling", "double conditional embedding", "visual information injection"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a5ec47467148db1be355b426212ce65f01b46ced.pdf", "supplementary_material": "/attachment/eb40e2ff2dcb196119626e0d35e0bdf898ecfe36.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a dataset condensation method for diffusion model training. The method consists of two phases: select and attach. In the select phase, the paper ranks the difficulties of the given data using the score from the pretrained model, then selects at a fixed interval. In the attach phase, the paper attaches additional information such as text encoding and visual information injection. With the condensed small data, the paper trains a model with both a denoising loss and a semantic alignment loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "With only a small amount of data, the method achieves performance comparable to REPA and vanilla models while showing much faster convergence. In addition, the approach of constructing the dataset through interval sampling based on diffusion model difficulty is intuitive and easy to understand. This research topic has the potential to significantly improve the accessibility of diffusion studies, which have traditionally required substantial computational resources."}, "weaknesses": {"value": "- It seems that the performance improvement of this dataset may come not from the selection process itself, but rather from attaching additional information. As shown in Table 5, gFID score is already comparable even without the selection step. Adding additional descriptions appears to help the model learn faster and achieve quicker convergence, making this work more similar to studies that investigate enriching data with additional information, such as long captions.\n- Comparisons with other dataset cleaning, distillation, or augmentation methods are needed. Although many of these works have been explored primarily in the GAN domain (e.g., Instance Selection for GANs), it would be better to demonstrate that this selection approach is more effective than using datasets selected by those existing methods. Since the method already relies on pretrained models, it can be compared with works that use them to build distilled datasets (e.g., D4M: Dataset Distillation via Disentangled Diffusion Model, CVPR 2024).\n- While the paper argues the proposed method is data condensation, it seems there is no image optimization unlike other condensation methods that synthesize original data using gradient-based optimization."}, "questions": {"value": "- Would it not be sufficient to randomly select images and then attach additional information to them?\n- Since the images remain raw data, and only additional information is attached, so the work may be similar with data cleaning or pruning methods.\n- Could comparisons with other dataset augmentation or distillation methods be added?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wyn8gGy8pt", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Reviewer_wMP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12757/Reviewer_wMP5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141989661, "cdate": 1761141989661, "tmdate": 1762923572290, "mdate": 1762923572290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "T0TKfjRTyN", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763147581206, "cdate": 1763147581206, "tmdate": 1763147581206, "mdate": 1763147581206, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant computational and data cost associated with training high-quality diffusion models. The authors introduce \"diffusion dataset condensation\" as a new problem setting and propose a novel two-stage framework called Diffusion Dataset Condensation to tackle it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.⁠ ⁠The paper's primary strength is identifying and formalizing the problem of dataset condensation specifically for diffusion models. It points out that prior methods for discriminative tasks fail in the generative setting (as shown in Table 4), necessitating a new approach.\n\n2.⁠ ⁠⁠The ability to achieve a 4.3 FID on ImageNet 256*256 using only 10K samples (0.8% data) and 40K training steps is an excellent result. The massive speedups in both training steps (100x-233x) and wall-clock time (21x-101x) are highly significant.\n\n3.⁠ ⁠⁠The D_2C framework is well-designed. The two-stage Select and Attach process is logical. The Select phase's use of a difficulty score to balance learnability and diversity is more sophisticated than naive pruning. The Attach phase's fusion of semantic and visual priors is a proven way to boost data efficiency. The method is also modular; as shown in the ablations, each part contributes value.\n\n4.⁠ ⁠⁠Another key strength worth mentioning is that the paper is well written, all ablations and dimensions are experimented in detail."}, "weaknesses": {"value": "1. The Attach phase, while effective, has limited novelty. The visual information injection and the L_proj loss are explicitly adopted from REPA. The DC-Embedding's fusion of learnable and pre-trained text embeddings is a well-known and effective technique. This places the bulk of the method's novelty on the Select phase and the combination of these parts for this new task.\n\n2.⁠ ⁠The fixed interval sampling seems trivial. Why not try other ablations over the sampling that respect the true distribution of the data?"}, "questions": {"value": "1.⁠ ⁠The comparison in Table 5b shows a significant difference between the result achieved using a strong pretrained scorer (4.3 gFID) versus a weaker scorer trained from scratch (4.9 gFID). Does this suggest that the performance ceiling of D2C is fundamentally linked to the quality of the initial model, potentially transferring the heavy computational burden from the diffusion model training phase to the scorer training phase?\n\n2.⁠ ⁠Did you try other ablations over the sampling techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gbx0bsmnGO", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Reviewer_UBi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12757/Reviewer_UBi3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642443265, "cdate": 1761642443265, "tmdate": 1762923572027, "mdate": 1762923572027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage data condensation method for diffusion models. The first step is selection, based on the difficulty (negative conditional probability). The second step is attachment, based on the semantic information (Dual Conditional Embeddings and Visual Features). For experiments, D2C is compared with traditional data selection methods like Herding, Random sampling, and K-Center. Also, a distillation method SRe2L is compared with D2C. It shows acceleration on diffusion models empirically."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. It studies the problem of data condensation in the diffusion models.\n2. The paper is easy to follow."}, "weaknesses": {"value": "1. Positioning and Novelty: The paper's claim regarding the novelty of studying data efficiency in diffusion models appears to be overstated. For instance, recent work (e.g., [arXiv: 2409.19128]) also investigates this specific problem. The authors are strongly encouraged to thoroughly discuss this and other related prior art, situating their own contributions more accurately within the existing literature to avoid overclaiming.\n\n2. Methodological Validation: The methodological contributions require further justification and validation through more rigorous ablation studies:\n- Selection Mechanism: The paper employs conditional probability for data selection but does not benchmark this choice against other established metrics. To justify this design, an ablation study is needed, comparing the proposed method to alternatives such as Gradient Norm or Loss-based scores.\n- Attachment Mechanism: The effectiveness of the attachment step seems highly dependent on the external feature/embedding extraction model. It is currently unclear whether the performance gains stem from the proposed attachment mechanism itself or simply from the rich semantics provided by the external model. To isolate the mechanism's specific contribution, the authors should conduct an experiment that removes or replaces this external model (e.g., using simpler, non-external embeddings) and compares the performance.\n\n3. Empirical Evaluation and Baselines: The empirical validation for the D2C method is not sufficient to be convincing, as the experiments are missing comparisons against several key state-of-the-art (SOTA) methods.\n\n- Selection Baselines: The comparison is limited to methods like Herding, Random, and K-center. The evaluation would be substantially stronger if it included more recent and relevant data selection techniques, such as CCS [1] and TDDS [2].\n- Distillation Baselines: The distillation experiments rely solely on SRe2L, which is insufficient. This comparison omits numerous modern methods like DC [4], DM [5], RDED [6], and NCFM [7].\n\n\n\n[1] Zheng H, Liu R, Lai F, et al. Coverage-centric coreset selection for high pruning rates[J]. arXiv preprint arXiv:2210.15809, 2022.  \n[2] Zhang X, Du J, Li Y, et al. Spanning training progress: Temporal dual-depth scoring (tdds) for enhanced dataset pruning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 26223-26232.  \n[3] Zhao B, Mopuri K R, Bilen H. Dataset condensation with gradient matching[J]. arXiv preprint arXiv:2006.05929, 2020.  \n[4] Zhao B, Bilen H. Dataset condensation with distribution matching[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023: 6514-6523.  \n[6] Sun P, Shi B, Yu D, et al. On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 9390-9399.  \n[7] Wang S, Yang Y, Liu Z, et al. Dataset distillation with neural characteristic function: A minmax perspective[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 25570-25580."}, "questions": {"value": "Please see Weaknesses for details.\n\nI would consider update my scores if the authors conduct extensive results and add related discussion and references properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pHOZvFcNXX", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Reviewer_yCdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12757/Reviewer_yCdG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887989446, "cdate": 1761887989446, "tmdate": 1762923571714, "mdate": 1762923571714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Diffusion Dataset Condensation (D2C), a framework of dataset distillation for diffusion models. The key idea is to construct a small yet information-rich synthetic sub-dataset that enables high-quality diffusion model training with only a fraction of the original data. D2C contains two phases: Select phase and Attach phase. The experiments show that only using 0.8% data the diffusion model can be trained from scratch and achieve a good performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  This is the first paper to formally study dataset condensation for diffusion models, whereas prior works (e.g., SRe2L, MTT, CAFE) targeted discriminative tasks like classification.\n\n  2.  D2C achieves up to 233× faster training using only 0.8% of ImageNet data, while maintaining competitive FID (e.g., 4.3 at 40k steps)."}, "weaknesses": {"value": "1.  Although the diffusion model itself is trained from scratch, the method relies heavily on pretrained T5 and DINOv2 encoders in the Attach phase, and a pretrained DiT for scoring. It is uncertain that the performance gain is from the data or the pretrained models. \n\n  2. The paper is mainly empirical and there is no theoretical guarantee or formal analysis.\n\n  3. The performance of the CS divergence is sensitive to the choice of kernel and bandwidth, but the paper does not provide a robustness analysis to assess this sensitivity.\n\n  4. The effectiveness of the D2C is unclear. For example, the random selection reported in Table 1 can achieved 4.19  gFID-50K, while the D2C only achieves 4.13. I do not see any performance gain from the D2C.\n\n  5. Another point concerns the ablation study. Table 5(a) shows that the Attach phase contributes the most to the overall performance improvement. However, according to the random selection results in Table 1, even the random selection  achieves strong performance. This raises a question about the marginal contribution of the Select phase versus the Attach phase, and whether the observed gains primarily stem from the added semantic and visual augmentations rather than from the data selection itself."}, "questions": {"value": "1. The table 1 mentioned that the experiments are reporting various dataset condensation methods. However,  K-Center, Herding are coreset selection methods not dataset condensation methods. The authors should include the baselines of dataset condensation such as MTT. \n\n2. The performance advantage of the SRe2L baseline primarily arises from its use of soft labels in training discriminative models. However, diffusion models rely on discrete class labels during training, meaning that SRe2L’s mechanism cannot be directly applied in this setting. Without soft labels, SRe2L would likely perform poorly, making it an inappropriate or unfair baseline for comparison in the diffusion model context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b5n3GTUPnn", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Reviewer_mRdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12757/Reviewer_mRdu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898355445, "cdate": 1761898355445, "tmdate": 1762923571405, "mdate": 1762923571405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Diffusion Dataset Condensation (D$^2$C), a novel two-stage framework aimed at significantly reducing the data and computational requirements for training diffusion models. The method consists of a Select phase, which uses a diffusion difficulty score and interval sampling to curate a compact yet diverse subset of training images, and an Attach phase, which enriches the selected samples with semantic (text) and visual (patch-level) embeddings. Extensive experiments on ImageNet-1K at various resolutions and model architectures demonstrate that D$^2$C achieves substantial acceleration and maintains competitive FID scores with data compression ratios as low as 0.8%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper is well-written and easy to follow. The motivation is clearly articulated, and the experimental setup is presented transparently.\n\n(2) The proposed D$^2$C framework is technically sound and intuitive. The decomposition into \"Select\" and \"Attach\" stages seems logical: The \"Select\" stage's use of a \"diffusion difficulty score\" is for identifying informative samples and the subsequent interval sampling to ensure diversity is a simple yet effective addition. Furthermore, in the \"Attach\" stage, the authors effectively increase the information density of each sample, guiding the model to learn more efficiently by \"attaching\" richer conditioning signals (both semantic and visual).\n\n(3) Given the massive computational costs associated with training SOTA diffusion models, an efficient method in this domain would have a substantial practical impact on inspiring similar efficiencies in other generative frameworks like flow-based models."}, "weaknesses": {"value": "(1) The \"Attach\" stage's augmentation with semantic and visual representations is innovative but lacks theoretical justification. This could be improved by deriving bounds or analyses showing how attachments reduce information loss in condensed datasets.\n\n(2) The term \"Dataset Condensation\" (DC) traditionally implies creating a small set of synthesized samples (e.g., through gradient matching) that are optimized to train a model. This paper, however, selects existing samples from the original dataset. While the \"Attach\" stage enriches these samples, it doesn't synthesize the image pixels themselves. Therefore, the method is more accurately described as a form of \"Informed Data Selection and Enrichment\" rather than \"Condensation\" in the classical sense. This naming could be misleading in DC literature. A clarification of this distinction is necessary.\n\n(3) Experimental comparisons seem strong but lack diversity in datasets beyond ImageNet-1K; testing on more varied domains like CelebA (for faces) or LSUN (for scenes) would better demonstrate generalization, as ImageNet's class-balanced structure might bias results toward structured data."}, "questions": {"value": "(1) Could authors clarify on choice of the term \"Dataset Condensation\" given that D$^2$C selects, rather than synthesizes, data samples? A brief discussion in the paper clarifying this and positioning your work with respect to classic DC would be very helpful.\n\n(2) It should be noted that the proposed approach is designed for and limited to the ImageNet dataset. Consequently, applying D$^2$C to Stable Diffusion models trained on LAION data for text-to-image generation poses significant challenges, primarily due to the partial domain shift. \n\n(3) What are the potential advantages and emergent capabilities arising from scaling up image resolution to 1024, extending the model architecture to an XL variant, or even applying video datasets for training Video diffusion models?\n\n(4) The paper mainly emphasizes the acceleration of the training phase. However, it does not analyze the computational cost of the D$^2$C pipeline itself. This pre-processing includes: (i) a full inference pass over the entire original dataset with a large diffusion model, (ii) generating rich captions for the selected subset, and (iii) encoding the selected images.\nFor a fair comparison of overall efficiency, this pre-processing cost must be quantified and compared against the training time saved. When training on smaller datasets, the D$^2$C 's overhead could be substantial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2VaB3yWhvg", "forum": "92PM2kSzK1", "replyto": "92PM2kSzK1", "signatures": ["ICLR.cc/2026/Conference/Submission12757/Reviewer_B6cT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12757/Reviewer_B6cT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762460721766, "cdate": 1762460721766, "tmdate": 1762923570957, "mdate": 1762923570957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}