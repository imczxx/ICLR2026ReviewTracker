{"id": "Je2QqXrcQq", "number": 17677, "cdate": 1758279127132, "mdate": 1759897161229, "content": {"title": "R2-Dreamer: Redundancy-Reduced World Models without Decoders or Augmentation", "abstract": "A central challenge in image-based Model-Based Reinforcement Learning (MBRL) is to learn representations that distill task-essential information from irrelevant details. While promising, approaches that learn representations by reconstructing input images often waste capacity on spatially large but task-irrelevant visual information, such as backgrounds. Decoder-free methods address this issue by leveraging data augmentation (DA) to enforce robust representations, but the reliance on such external regularizers to prevent collapse severely limits their versatility. To address this, we propose R2-Dreamer, an MBRL framework that introduces a self-supervised objective acting as an internal regularizer, thus preventing collapse without resorting to DA. The core of our method is a feature redundancy reduction objective inspired by Barlow Twins, which can be easily integrated into existing frameworks. In evaluations on the standard continuous control benchmark, DMC Vision, R2-Dreamer achieves performance competitive with strong baselines, including the leading decoder-based agent DreamerV3 and its decoder-free counterpart that relies on DA. Furthermore, its effectiveness is highlighted on a challenging benchmark with tiny but task-relevant objects (DMC-Subtle), where our approach demonstrates substantial gains over all baselines. These results show that R2-Dreamer provides a versatile, high-performance framework for decoder-free MBRL by incorporating an effective internal regularizer.", "tldr": "R2-Dreamer is a decoder-free agent that replaces data augmentation with a self-supervised objective to excel on challenging visual tasks.", "keywords": ["model-based reinforcement learning", "world models", "representation learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f83dcd766c20308dfae8be43ba831f7061cb717.pdf", "supplementary_material": "/attachment/7fb5aae686d24c105773a7e7ad9c8550d0ba1073.zip"}, "replies": [{"content": {"summary": {"value": "This paper  presents a decoder-free agent that introduces a self-supervised objective acting as an internal regularizer to prevent collapse. Experiments on DMC and DMC-Subtle validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a decoder-free MBRL agent that adopt internal regularizer to avoid reconstructing observations from a latent state.\n- This paper introduces DMC-Subtle, a modified DMC benchmark where task-critical objects’ sizes are significantly reduced, demanding a higher level of representational precision.\n- The proposed approach outperforms baselines in DMC and DMC-Subtle."}, "weaknesses": {"value": "- There is no comparison with competitive baselines such as TD-MPC2 [1].\n- The proposed DMC-Subtle benchmark seems somewhat questionable or not well-justified.\n- More challenging benchmarks are needed to better validate the effectiveness of the proposed algorithm.\n- The DMC tasks used are relatively simple. How does the method perform on more complex tasks like *DMC Dog* or *DMC Humanoid*?\n- As shown in Figure 3, the improvement appears to be marginal.\n- There is no comparison with VAI [2], which is a baseline that adopts unsupervised visual attention.\n\nReferences:\n\n[1] Hansen et al. \"TD-MPC2: Scalable, Robust World Models for Continuous Control\", ICLR, 2024.\n\n[2] Wang et al. \"Unsupervised Visual Attention and Invariance for Reinforcement Learning\", CVPR, 2021."}, "questions": {"value": "Have the authors tried conducting experiments on environments with distractors, such as Distracting Control Suite [1]?\n\nReference:\n\n[1] Stone et al. \"The Distracting Control Suite — A Challenging Benchmark for Reinforcement Learning from Pixels\", arXiv, 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oeMywGMJEI", "forum": "Je2QqXrcQq", "replyto": "Je2QqXrcQq", "signatures": ["ICLR.cc/2026/Conference/Submission17677/Reviewer_zUBH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17677/Reviewer_zUBH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400949224, "cdate": 1761400949224, "tmdate": 1762927527084, "mdate": 1762927527084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes R2-Dreamer, an enhanced variant of DreamerV3 that introduces an internal regularizer to improve latent state representation learning. Specifically, the authors replace the traditional image reconstruction loss with a Barlow Twins loss, aiming to encourage more informative and less redundant latent features. Theoretical analysis shows that this new objective is equivalent to optimizing a variational bound on an extended Sequential Information Bottleneck. Experimental results demonstrate that the proposed method achieves more robust latent representations and improved computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies a key limitation of DreamerV3: its latent representations can be overly influenced by reconstructing input images, leading to a focus on irrelevant pixels rather than task-relevant features. By replacing the reconstruction loss with a self-supervised objective defined in the latent space, the proposed method encourages more compact and task-relevant representations.\n2. The paper provides solid theoretical analysis to explain the impact of the new learning objective and includes extensive experimental validation. Results on the challenging DMC-Subtle benchmark demonstrate the superiority of the proposed representation. Moreover, the ablation studies convincingly show both the necessity of the proposed objective and the redundancy of data augmentation under this framework.\n3. The proposed approach is conceptually simple yet empirically effective, making it a strong candidate for a new baseline in model-based reinforcement learning."}, "weaknesses": {"value": "1. The experimental environments are not sufficiently diverse. The paper evaluates only on DMC-Subtle, whereas DreamerV3 has also been tested on other benchmarks such as Atari and DMLab. Including experiments on these additional environments would make the evaluation more comprehensive and the conclusions more convincing."}, "questions": {"value": "1. Minor: Why are the ablation results of R2-Dreamer+DA different between Figure 7 and Figure 10? In Figure 7, the performance of the DA version shows a significant drop, while in Figure 10, the drop is much smaller. This discrepancy seems inconsistent with the claim that data augmentation is harmful for precision-demanding tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TfK15YbPMh", "forum": "Je2QqXrcQq", "replyto": "Je2QqXrcQq", "signatures": ["ICLR.cc/2026/Conference/Submission17677/Reviewer_Eb9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17677/Reviewer_Eb9R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570270217, "cdate": 1761570270217, "tmdate": 1762927526584, "mdate": 1762927526584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data-augmentation-free learning method for RSSM-based decoder-free MBRL. The core of this method is is a feature redundancy reduction objective inspired by Barlow Twins and therefore has no need for pixel-wise reconstruction or data augmentation. Emprical study shows the superior performance on standard DMC benchmark and more chanlleging DMC-Subtle benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, allowing readers to follow the main arguments.\n\n2. It provides a comprehensive experiments and ablations to demonstrate the effectiveness of their proposed new representation learning paradigm.\n\n3. Releasing codebase is good, which can facilitate future research."}, "weaknesses": {"value": "1. The authors didn't compare TD-MPC2 in their experiments, which is a strong state-of-the-art baseline for decoder-free methods.\n2. Though authors evaluated different methods on many tasks on the DMC benchmark, I think evaluating only on these locomotion tasks is kind of not comprehesive and it would be better to evaluate on other types tasks like Meta-World.\n3. One of claims in this paper is that their method doesn't need hand-engineered data augmentation like other decoder-free MBRL methods. But I question whether this is really a significant problem. For example, TD-MPC2 only uses very simple random shift augmentation, which I don't think it is troublesome for algo implementation. So I'm confused that why the data augmentation for decoder-free MBRL methods is a drawback."}, "questions": {"value": "1. Could the authors compare their method with state-of-the-art decoder-free MBRL methods like TD-MPC2?\n2. Could the authors evaluate different methods on other benchmarks like Meta-World?\n3. Could authors explain why data augmentation is a problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jvd6Q8O1J5", "forum": "Je2QqXrcQq", "replyto": "Je2QqXrcQq", "signatures": ["ICLR.cc/2026/Conference/Submission17677/Reviewer_z3Lo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17677/Reviewer_z3Lo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794244148, "cdate": 1761794244148, "tmdate": 1762927526257, "mdate": 1762927526257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes R2-Dreamer, a Model-Based Reinforcement Learning (MBRL) agent based on the DreamerV3 architecture. It addresses two main limitations in current image-based world models: the computational expense and potential for task-irrelevant overfitting in decoder-based models, and the reliance on brittle, task-specific Data Augmentation (DA) in existing decoder-free models. R2-Dreamer replaces the reconstruction objective with a redundancy-reduction objective inspired by Barlow Twins, applied between the image embeddings and the projected latent states of the RSSM."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Principled approach to removing DA: The paper successfully identifies a major pain point in current decoder-free methods—the reliance on heuristic DA. Proposing an information-theoretic internal regularizer (redundancy reduction) as a replacement is a sound and theoretically motivated direction, nicely grounded in the Sequential Information Bottleneck framework in Appendix A2.\n2. Strong empirical validation of the core hypothesis: The ablation study (Figure 6) provides compelling evidence. It shows that while DreamerPro collapses without DA, R2-Dreamer maintains performance, proving that the proposed $\\mathcal{L}_{BT}$ effectively prevents collapse without external heuristics\n3. Effective stress-testing (DMC-Subtle): DMC-Subtle cleanly isolates failure modes of reconstruction-based (wasted capacity on background) and DA-based (distortion of small features) methods, highlighting the specific advantages of R2-Dreamer in precision-demanding tasks\n4. Computational Efficiency: The reported speedups are significant and practically important for scaling MBRL."}, "weaknesses": {"value": "1. Batch size concerns for Barlow Twins: Redundancy reduction objectives often require large batch sizes for stable covariance estimation. The paper uses standard Dreamer batching ($B=16, T=64 \\implies N=1024$)8. While this appears sufficient for DMC, it raises concerns about stability in higher-dimensional or more diverse visual environments where 1024 samples might not sufficiently estimate the cross-correlation matrix.\n2. Ambiguity in Encoder Training: The pseudocode (Algorithm 1) indicates that the image embeddings $e$ are detached before entering the $\\mathcal{L}_{BT}$ loss9. If accurate, this means the image encoder does not receive gradients from the primary representation learning objective, relying solely on gradients flowing back from the RSSM via $KL$ terms, which is highly unusual for this class of SSL objectives."}, "questions": {"value": "1. Clarification on Gradients: In Algorithm 1, you have e = ...detach(). Does this mean your image encoder $f_\\phi(x_t)$ receives NO gradients from $\\mathcal{L}_{BT}$? If so, what is the primary learning signal for the encoder? Is it solely the $KL(q(z_t|h_t, e_t) || p(z_t|h_t))$ term?\n2. Batch Size Sensitivity: Have you evaluated the sensitivity of R2-Dreamer to the batch size (specifically the total samples $B \\times T$ used for the correlation matrix)? Barlow Twins often degrades rapidly with small batches.\n3. Complex Backgrounds: How does R2-Dreamer perform when the background is not just static (like DMC) but dynamic and irrelevant (e.g., \"distracting control\" suite)? This would be a stronger test of the claim that it avoids wasting capacity on irrelevant details compared to reconstruction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AizcEr5AeM", "forum": "Je2QqXrcQq", "replyto": "Je2QqXrcQq", "signatures": ["ICLR.cc/2026/Conference/Submission17677/Reviewer_Hody"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17677/Reviewer_Hody"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971905733, "cdate": 1761971905733, "tmdate": 1762927525807, "mdate": 1762927525807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}