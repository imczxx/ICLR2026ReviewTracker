{"id": "dDvH0UA0CZ", "number": 14331, "cdate": 1758232890584, "mdate": 1759897376742, "content": {"title": "Sampling from Your Language Model One Byte at a Time", "abstract": "Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. These models are typically invoked to autoregressively complete a text prompt by tokenizing the prompt, sampling more tokens to continue the tokenized prompt, and detokenizing the result. However, prior work has shown that this process can introduce distortion into the model's sampling distribution, leading to unexpected or undesirable generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. While this heuristic is effective in English, the underlying problem continues to affect languages such as Chinese as well as code generation, settings where word and syntactic boundaries may not line up with token boundaries. We present an optimal method to solve this \"Prompt Boundary Problem,\" which is based on an efficient online algorithm for Byte-Pair Encoding (BPE). This allows one to compute the next byte distribution conditioned on an arbitrary byte prefix, given only logit access to the original tokenizer-based model. This procedure can be applied iteratively to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing the generative distribution at the text level. We show that this significantly improves next-character prediction accuracy when computed on arbitrary prefixes. Moreover, our method is able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.", "tldr": "", "keywords": ["language models", "tokenization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/230b35e5ed3ad3a568ec2554f3c2c85449e28073.pdf", "supplementary_material": "/attachment/a75536002d37ddc6bec5d917e10793e24222b0e1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ByteSampler, a method of treating a distribution over token sequences (an LLM) as the corresponding distribution over character strings, assuming that the probability of non-canonical token sequences in the LLM is 0. It provides three capabilities given a prompt $S$ in the form of a string of characters/bytes, as summarized in Section 3.2: (1) compute the total probability that $S$ is a prefix of a canonical token sequence generated by the LLM; (2) sample a completion of $S$ while avoiding the prompt boundary problem; and (3) compute the probability distribution over next characters/bytes (rather than tokens) following $S$, providing the ability to convert a token-level LLM to a character/byte-level one. The method works by constructing a tree of canonical token sequences where the final token straddles the boundary of $S$. The experiments show that the byte-level interface results in comparable bits per character on naturalistic data to the original token-level LLM while not adding too much overhead in terms of calls to the underlying token-level LLM. Being able to convert a token-level LLM also adds the ability to ensemble LLMs with different tokenizers or proxy-tune models with different tokenizers; and the authors present fairly strong results on these applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method provides a sensible solution for the prompt boundary problem and incurs relatively little computational overhead, so it clearly has practical use. The paper is generally clear and easy to follow. The idea of converting token-level LLMs to character/byte-level ones to enable ensembling and proxy tuning of models with different tokenizers is exciting and opens up a lot of new possibilities. With the exception of TriviaQA and WikidataQA in Table 7, the experimental results in Section 4.3 and 4.4 are quite strong. The paper includes an excellent breadth of models and benchmarks."}, "weaknesses": {"value": "This paper has quite a bit of overlap with two recently published papers, which I will refer to as [1] and [2]:\n\n* [1] Vieira et al. (2025). [From Language Models over Tokens to Language Models over Characters](https://openreview.net/forum?id=sQS0roNQZR)\n* [2] Vieira et al. (2025). [Language Models over Canonical Byte-Pair Encodings](https://openreview.net/forum?id=eCVrfVDNSY)\n\n[1] also provides algorithms for converting a token-level LLM to a character/byte-level one, and [2] provides algorithms for enforcing the canonicality of BPE token sequences.\n\n1. The characterization that ByteSampler provides an \"exact\" solution to the prompt boundary problem is misleading. Like this paper, [1] also provided algorithms for converting a token-level LLM to a character/byte-level one. The beam summing algorithm of [1] (approximately) marginalizes over all tokenizations that match a given character sequence, including non-canonical ones. This corresponds closely to this paper's Eq (1) (incidentally, the equation seems to be missing the probability of EOS). Crucially, ByteSampler relies on the assumption that the probability of non-canonical token sequences in the LLM is negligible. However, [2] showed that, in practice, this is often not the case. For example, LLaMa 1B and LLaMa 8B have a log-canonicality rate of about -1.2, or roughly 30% -- so about 70% of sampled token sequences are non-canonical. This means that the distribution that ByteSampler induces is considerably distorted from Eq (1) and from the actual distribution over character strings that LLMs generate in practice. The distribution of ByteSampler corresponds more closely to the \"locally canonicalized\" method of [2] (Section 3.2.2).\n1. The most comparable prior work is the beam summing algorithm of [1], but this paper does not sufficiently compare against it. It compares ByteSampler against [1] only in terms of computational cost, not the quality of the distribution over character strings, and it only compares against the exponential-time version that does not use beam search. [1] includes a beam size hyperparameter $K$ that can be used to trade computational cost for fidelity to the true marginal distribution over character sequences, in which case the time complexity is linear in the length of the character string, not exponential. A good comparison would be to show the Pareto frontier of character-normalized cross-entropy on naturalistic data vs. computational cost. Perhaps ByteSampler outperforms [1] for some range of $K$.\n1. The pairwise validation for BPE in Proposition 3.1 was already proven and used by [2], as was the trick for checking the merge trajectory along the boundary between the tokens for conflicts. The paper does not acknowledge this.\n1. A minor point: In Table 3 and similar tables, the values in the loss per unit column are not comparable, so it doesn't make sense to bold the lowest score.\n1. Very minor: Table 6 is not referenced in the main text."}, "questions": {"value": "1. Is Eq (1) missing EOS?\n1. Table 3: Why is the bits per character score for ByteSampler bold?\n1. 322: How did you get this conversion rate?\n1. Table 3: Can you provide the formula for computing bits per character?\n1. 328: Are you including the probability of EOS when computing the probabilities of these documents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AVn3DwKO7l", "forum": "dDvH0UA0CZ", "replyto": "dDvH0UA0CZ", "signatures": ["ICLR.cc/2026/Conference/Submission14331/Reviewer_Qz8v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14331/Reviewer_Qz8v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829707465, "cdate": 1761829707465, "tmdate": 1762924758139, "mdate": 1762924758139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of compute next-byte probabilities for autoregressive sampling from a tokenized LLMs. The authors introduces a construction of valid covering tree that improves robustness and speed over prior works. For applications, they show the advantage of this byte-sampler procedure on fixing broken tokens, model ensemble and proxy tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The application of byte-level sampling to proxy tuning is interesting and should be further emphasized as a key contribution of the paper. \n\nThe proposed method demonstrates clear improvements over prior approaches in both speed and robustness. \n\nMoreover, the comparison between the byte-sampling strategy and the token-healing approach is well-presented and helps clarify the advantages of the proposed technique."}, "weaknesses": {"value": "There are notable overlaps between the proposed method and prior work such as [1], which also introduces a similar byte-level sampling procedure and targets applications like handling broken tokens in code and ensembling LLMs. Nevertheless, [1] also has certain limitations, particularly that its pre-tokenization handling is restricted to older BPE-based tokenizers (e.g., SentencePiece), as detailed in Section C.6 where this paper also provides solution to handle such problem. In my opinion, this distinction should be emphasized more clearly in the main paper rather than being overshadowed by the ensembling experiments, so that readers without a strong tokenization background can better appreciate the robustness advantage. For this reason, it might also strengthen the paper to revise the title to highlight this aspect, for example: “Robustly Sampling from Your Language Model One Byte at a Time.”\n\nThe writeup in Section 3.2 is not very well-organized. Perhaps providing an example such as on a simple Markov chain would improve the clarity. Also, equation (2) needs more explanation so that it lines up with prior works [1,2].\n\n[1] Buu Phan, Brandon Amos, Itai Gat, Marton Havasi, Matthew J Muckley, and Karen Ullrich. Exact byte-level probabilities from tokenized language models for fim-tasks and model ensembles. In The Thirteenth International Conference on Learning Representations, 2025.\n\n[2] Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J O’Donnell, and Ryan Cotterell. From language models over tokens to language models over characters. arXiv preprint arXiv:2412.03719, 2024."}, "questions": {"value": "Could you elaborate on the differences among the methods discussed in Section D.3? \n\nRegarding proxy tuning, would it be possible to incorporate additional experts or anti-experts to further enhance performance? It would be interesting to include more experimental results regarding this.\n\nIn Phan et al. [1], an O(1) procedure for next-byte sampling is also introduced. Setting aside robustness concerns such as pre-tokenization issues, could you clarify how your approach improves sampling speed compared to theirs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tj5kMzG2dY", "forum": "dDvH0UA0CZ", "replyto": "dDvH0UA0CZ", "signatures": ["ICLR.cc/2026/Conference/Submission14331/Reviewer_E5BB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14331/Reviewer_E5BB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888569422, "cdate": 1761888569422, "tmdate": 1762924757760, "mdate": 1762924757760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for converting token-level language models to character-level (byte-level) models. The paper claims several contributions. First, an O(1) overhead algorithm for conditioning token-level LMs on character strings, a novel Valid Covering Tree concept for enumerating valid tokenizations, an efficient bigram-based canonicality test for BPE, and empirical validation showing the method achieves low error and improved bits/byte compared to canonical tokenization baselines. ByteSampler does not add high overhead relative to plain BPE decoding while substantially reducing boundary artifacts, and it evades the worst-case exponential cost of unpruned prefix-cover enumeration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The empirical evaluation is comprehensive, testing on four modern LLMs (Llama-3.2-1B, Meta-Llama-3.1-8B, DeepSeek-R1-Distill-Llama-8B, and phi-4) with careful measurement of the speed-accuracy tradeoff. The implementation appears to be well-engineered, with the bundled beam summing approach using trie-based filtering (Appendix D) providing meaningful practical speedups. The pedagogical presentation is generally clear, with helpful visualizations like the Valid Covering Tree diagrams that make the concepts accessible. The paper demonstrates that even with modest computational budgets (small beam sizes), reasonable approximations to the character-level distribution can be achieved, which is useful for practitioners. The experimental design measuring both Jensen-Shannon distance to a reference model and bits/byte compression is informative."}, "weaknesses": {"value": "The core weakness of the paper is lack of novelty:\n\nThe algorithm is similar to a previous algorithm from Vieira et al. (2025b). The paper claims throughout to provide exact solutions, but this claim is not clear as the authors redefine what \"exact\" means to be \"modulo invalid token sequences\".  The method only ensures canonicality of tokens overlapping the prompt prefix, not the entire sequence, making it an approximation to the ideal distribution. This limitation is in Appendix D.3 rather than discussed in detail. The exactness claim relies on an assumption that noncanonical strings have zero probability, but this assumption doesn't hold according to Geh et al. (2024) and Vieira et al. (2025b).\n\nThe complexity comparison in Table 1 is also not clear. It compares ByteSampler's practical O(1) overhead against Vieira et al. (2024)'s theoretical 2^O(n) worst-case bound, while not comparing against the \"beam summing algorithm\" in the same paper with O(N·K·|∆|) complexity, which is similar.. \n\nAdditionally, the bigram test is similar to previous work by Antwerpen & Neubeck (2024) and Vieira et al. (2025b)."}, "questions": {"value": "Did you measure the probability mass on noncanonical strings in your experiments? If so, did you consider how does considering noncanonical tokenizations affect performance on downstream tasks?\n\nHow does the bigram test differ from previous papers on canonicality?\n\nIf possible, it would be helpful to test the method to other canonicalization papers to understand the difference in runtime and accuracy, to differentiate the method more from previous work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGuXepZXiS", "forum": "dDvH0UA0CZ", "replyto": "dDvH0UA0CZ", "signatures": ["ICLR.cc/2026/Conference/Submission14331/Reviewer_Nkha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14331/Reviewer_Nkha"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971891706, "cdate": 1761971891706, "tmdate": 1762924757353, "mdate": 1762924757353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an efficient algorithm to the prompt boundary problem or tokenization bias problem. \n\nLines of attack are execution speed, memory efficiency and possibly mixed generation, i.e. generating bytes only if there is a tokenization bias expected, otherwise generating tokens.\n\nNote that I do not want to score the paper at this moment I will assign scores after our discussion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Making more efficient versions of exact solution to the PBP is crucial for its adoption in all decoding libraries and ultimately for its broad appeal.\n\nI will ask more clarifying questions I find the writing somewhat unclear."}, "weaknesses": {"value": "I struggle a lot reading this paper. For one the contributions are not clear to me exactly i also think it overclaims here and there and the experiments are a bit poorly chosen to illustrate the contributiin. Let me get more specific ( to be clear nothing you couldnt fix in the camera ready ). \n\nIn no specific order\n\n1 in abstract be specific about what efficiecy you improve \n\n2 in contributions i would restrict my claim to the efficency bit bc as u say yourself other work has already provided exact solutiond to the problem + do not claim ensembeling that was already done in prior work or if u do how is yours better\n\n3 u claim exact solutions but never define what that means i assume u refer to phans definition of statistically equivalent u should define what u mean\n\n4 in that context u should discuss clearee what distingushes your work from phan, in line 145\n\n5 in 3 you talk about valid coverings again i must assume u take that inspired by phan that define valid encodings and covers but i dont know u need to define that, or do you mean vieira?\n\n6 proposition 3.1 is not clear the first part is a definition the second one was already shown i phan just cite theirs\n\n7 3.2 and 3.3 i will ask more clarifying questions in discussion\n\n8 experiments: from your method i come out believe you came up with a more efficient version vieira and phan hence experiments should focus on that \n\n9 what is overhead with bpe\n\n10 the experiments that do not focus on efficenty i find distracting they seem mere additional experiments illustrating prior work. If there is a result that the other two would not have produced you should compare\n\n11 4.3 in paricular i find misleading u should byte level ensemble introduced by phan, so compare to them"}, "questions": {"value": "Please help me understand exaclty how your algorithm is different from phan, phan also uses a tree data structure, and i am understanding u than use token level sampling but i find the sections in the paper hard to follow. Hence i find it hard to verify its correctness.\n\nSo to summarize my main concerns \n\nClarity of writing needs improvement\n\nClarity of claim ideally much less claim but more precition\n\nExperiments are too unfocussed on the problem you are trying to solve which to my understanding is the efficency part not the pbp"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zqhy52mIP6", "forum": "dDvH0UA0CZ", "replyto": "dDvH0UA0CZ", "signatures": ["ICLR.cc/2026/Conference/Submission14331/Reviewer_KhEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14331/Reviewer_KhEW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043718214, "cdate": 1762043718214, "tmdate": 1762924756902, "mdate": 1762924756902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}