{"id": "T1h5em349L", "number": 15241, "cdate": 1758249243776, "mdate": 1763734336999, "content": {"title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models", "abstract": "Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence length, reasoning constrained by maximum context boundaries, and performance degradation beyond pre-training context windows. Existing approaches primarily compress reasoning chains without addressing the fundamental scaling problem. To overcome these challenges, we introduce InftyThink, a paradigm that transforms monolithic reasoning into an iterative process with intermediate summarization. By interleaving short reasoning segments with concise progress summaries, our approach enables unbounded reasoning depth while maintaining bounded computational costs. This creates a characteristic sawtooth memory pattern that significantly reduces computational complexity compared to traditional approaches. Furthermore, we develop a methodology for reconstructing long-context reasoning datasets into our iterative format, transforming OpenR1-Math into 333K training instances. Experiments across multiple model architectures demonstrate that our approach reduces computational costs while improving performance, with Qwen2.5-Math-7B showing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks. Our work challenges the assumed trade-off between reasoning depth and computational efficiency, providing a more scalable approach to complex reasoning without architectural modifications.", "tldr": "", "keywords": ["LLM Reasoning", "Context Management", "Efficient Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98505555440397fc4982d051184f8be782e55161.pdf", "supplementary_material": "/attachment/45e4b9698ed2520c255cdad02a3f5b1f426f7b98.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes InftyThink, an iterative long-context reasoning method for LLMs. Instead of generating one long chain-of-thought, the model reasons in bounded segments of length η (e.g. 2K/4K/6K tokens), summarizes progress, and continues conditioning mostly on the summary. This is meant to (i) let models “think” for arbitrarily many steps, (ii) avoid quadratic attention over extremely long contexts, and (iii) improve reasoning accuracy at lower compute cost. The authors reconstruct a large supervised dataset (OpenR1-Math-Inf) by segmenting long math reasoning traces and adding summaries, and fine-tune various backbones (Qwen2.5-Math 1.5B/7B/etc., Llama-3.1-8B) on that data. They report higher accuracy on MATH500, AIME24, and GPQA_diamond, while claiming equal or lower latency compared to vanilla long-CoT decoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea (periodic summarize-and-continue instead of one monolithic chain) is simple, intuitive, and architecturally lightweight — no special long-context transformer, just inference protocol + supervised finetuning.\n\n2. The data pipeline that turns 220K long solutions into 333K iterative traces is clearly described and likely reusable for others training math/logic models.\n\n3. Reported gains are consistent across multiple model sizes and families, especially on harder benchmarks like AIME24 and GPQA_diamond where deep multi-step reasoning matters."}, "weaknesses": {"value": "1. Table 1 shows that in overall InftyThink emits more tokens per problem than vanilla (e.g. Qwen2.5-Math-1.5B on MATH500: 5.94K → 6.79K tokens), yet the reported latency drops (1.42s → 0.80s). This is confusing. How this translates to practical setup is unclear.\n\n2. The InftyThink models depend critically on two hyperparameters: segment size η (they denote it like 2K/4K/6K) and max_iterations. Those exact values for each reported model are not given in Table 1. This makes it hard to reproduce or even interpret the latency numbers and token budget, since they obviously depend on η and how many iterations you unroll.\n\n3. No variance / confidence intervals are provided in Table 1. For some benchmarks the absolute gain is only ~1–2%, which could easily fall within noise. The paper references “stability” in the appendix but does not surface it in the main results.\n\n4. Figure 4 compares η = 2K / 4K / 6K. Converted to “total tokens generated,” the 2K model matches 4K and even beats 6K at similar total token counts. In other words, shorter segments with more iterations can be as good or better than longer segments, at roughly the same total text budget. This is surprising and important: it hints that extremely short steps plus good summaries may dominate, which would strongly shape how people deploy the method. The paper surfaces the curve, but gives almost no analysis of why 2K can keep up with or beat 4K/6K at equal effective cost. That missing discussion leaves open whether η is actually a tuning knob or just an implementation artifact.\n\n\nI think that the pitch of longer reasoning with less latency and better accuracy, drop-in for any LLM is very strong, but the evidence for the latency/computation story is currently incomplete, and several presentation issues (Table 1 hyperparameters, missing variance, lack of explanation for Figure 4 behavior) are significant for an ICLR acceptance."}, "questions": {"value": "1. How exactly were the latency numbers in Table 1 measured for vanilla vs. InftyThink? Were both sides run under identical inference infrastructure?\n\n2. Why does InftyThink emit more total tokens but run faster? Please provide a step-by-step cost model and per-iteration timing.\n\n3. What η and max_iterations were used for each model in Table 1? Were they tuned per task?\n\n4. Please provide accuracy variance / CIs for at least the Table 1.\n\n5. Figure 4. Why does η=2K match or outperform η=4K/6K at the same effective token budget? Is this simply because shorter segments force more frequent summarization (which the model has been trained to do well), or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ehL94t6lgq", "forum": "T1h5em349L", "replyto": "T1h5em349L", "signatures": ["ICLR.cc/2026/Conference/Submission15241/Reviewer_xfMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15241/Reviewer_xfMt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759686110, "cdate": 1761759686110, "tmdate": 1762925537824, "mdate": 1762925537824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InftyThink, which is a paradigm for turning monolithic reasoning traces into a sequence of short reasoning segments interleaved with summaries of all previous reasoning segments, incrementally and iteratively generated. This idea of \"summarize\" and \"reason\" pairs can be used for reconstructing existing reasoning datasets in this iterative format.\n\nThe authors empirically investigate this approach for five base models of various sizes (1.5B to 32B parameters) they train.\n\nFor training they also reconstruct OpenR1-Math, using a larger model for the summarization (or comparable-sized ones: Appendix F) of the reasoning segments they partition the traces into.\n\nThey test their accuracy on MATH500, AIME 2024 and GPQA Diamond benchmarks (main text) and they report on performance benefits when compared to the vanilla models' case with performance gains most notable for smaller sizes. A discussion on the impact of eta (segment length) in evolving performance across summarizations further augments the analysis of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Excellent motivation and intuition.\n\n- The sawtooth memory pattern offers a bounded memory footprint which is highly desired in applications."}, "weaknesses": {"value": "- Requires a specialized data reconstruction pipeline for training, which in turn necessitates the use of an LLM (e.g. for summarization), so not a simple/lightweight process (even for the very small summarizers in Table 4 (Line 994))."}, "questions": {"value": "- Partitioning the reasoning process is very important: on the one hand skewed partition sizes can lead to variable \"compression\" factors in generated summaries or skewed summaries (that could fail to fit into fixed context window sizes (eta)); on the other hand these imbalanced partitions could well be those that are semantically most reasonable. How do you plan to approach this? Fixed larger contexts? Smaller contexts? Variable contexts driven by semantics and heuristics for deciding semantic boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DM9tKlrV8E", "forum": "T1h5em349L", "replyto": "T1h5em349L", "signatures": ["ICLR.cc/2026/Conference/Submission15241/Reviewer_39tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15241/Reviewer_39tg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978912589, "cdate": 1761978912589, "tmdate": 1762925537293, "mdate": 1762925537293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an iterative summarization strategy that compresses earlier reasoning traces in real time. This strategy keeps the live context short, cuts computing costs, and, as demonstrated by evaluation on three math (MATH500, AIME24, and GPQA) and other benchmarks, improves task accuracy compared to vanilla CoT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is very clear, well motivated, and easy to follow.\n- The proposed approach allows models to “think” longer while maintaining a limited context size, reducing GPU RAM memory requirements.\n- Informative appendix with many details, extra experiments, and analyses."}, "weaknesses": {"value": "- There is no prompting-only pipeline ablation: it’s unclear how much of the gain comes from training on specific data rather than the summarize-and-continue procedure itself. How would your trained method (InftyThink) compare to a prompting-only pipeline on the same base model: run CoT, pause at fixed steps, prompt the model to summarize, feed the summary back, and continue without any extra fine-tuning. This would help isolate the value added by training and highlight the general impact of the proposed iterative reasoning approach.\n- Source of metrics gains is unclear. Some baselines return no answer due to context limits (according to Appendix E, Qwen2.5-Math-1.5B hits context limit in 18% samples on MATH500 and AIME24 in 32% samples). It is not clear how much of the improvement comes from (a) solving previously unanswered cases vs. (b) better reasoning on cases vanilla CoT did answer incorrectly.\n- The paper does not clearly show the break-even point where the method becomes faster/cheaper than vanilla CoT. The method adds generation of summaries, so the total number of tokens for InftyThink are roughly len(question) + len(summaries) + len(CoTs), which exceeds vanilla len(question) + len(CoTs). Where is the breaking point at which InftyThink becomes faster in latency / tokens per second than vanilla CoT?\n- Using n^2 tokens as a proxy for inference cost can be misleading (Appendix Q). In practice, compute is often dominated by FFN layers at shorter contexts, with quadratic attention dominating only at larger contexts. A comparison in FLOPs or measured latency would be more convincing."}, "questions": {"value": "Please address weaknesses as questions.\n\n- Why is InftyThink's latency higher than that of vanilla CoT on AIME24 for Llama-3.1-8B and R1-distill-Qwen-7B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t1sAis8C1p", "forum": "T1h5em349L", "replyto": "T1h5em349L", "signatures": ["ICLR.cc/2026/Conference/Submission15241/Reviewer_xrFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15241/Reviewer_xrFD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984191212, "cdate": 1761984191212, "tmdate": 1762925536887, "mdate": 1762925536887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "InftyThink proposes an iterative reasoning paradigm for large language models that breaks long, monolithic reasoning chains into multiple short segments with intermediate summarizations, enabling theoretically unbounded reasoning depth while keeping computational costs bounded. Experiments on different benchmarks and multi model architectures show that this approach improves reasoning accuracy and reduces inference latency, effectively overcoming context-length and efficiency limitations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel paradigm that controls context length through iterative summarization, thereby overcoming the limitation of maximum context boundaries. The idea is both intuitive and reasonable, and the authors further construct corresponding datasets to enable models to acquire this capability.\n2. Comprehensive experiments across different datasets and models demonstrate that this method improves model accuracy while reducing inference time, and it consistently proves effective across various model architectures.\n3. The experiments on reasoning iteration rounds demonstrate that the proposed method can, to some extent, overcome the constraints of maximum context length."}, "weaknesses": {"value": "1. During inference, the method shortens the context through summarization, which in turn reduces reasoning time. Although Table 1 provides evidence of this effect, the reduction in inference time is not particularly significant, and the paper lacks further analysis (e.g., detailed statistics showing the actual number of tokens used per reasoning step compared to the vanilla approach).\n2. At each reasoning step, the model should only have access to \\(S_{i-1}\\). Why, when constructing the SFT data in Equation (2), is it based on \\(RP_1, \\ldots, RP_{i-1}\\) rather than \\(S_{i-1}\\)?\n3. In Figure 4, how are the different iterations obtained? My understanding is that the model generates multiple reasoning steps adaptively. Could you provide the inference prompt? In addition, the paper lacks some visualization analysis, such as a concrete example illustrating the model’s reasoning process."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bf9FjElHGs", "forum": "T1h5em349L", "replyto": "T1h5em349L", "signatures": ["ICLR.cc/2026/Conference/Submission15241/Reviewer_wn6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15241/Reviewer_wn6o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225813973, "cdate": 1762225813973, "tmdate": 1762925535717, "mdate": 1762925535717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}