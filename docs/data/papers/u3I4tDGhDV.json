{"id": "u3I4tDGhDV", "number": 9283, "cdate": 1758117378729, "mdate": 1763717431914, "content": {"title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "abstract": "Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose **KeyWorld**, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.", "tldr": "", "keywords": ["World model", "Key frames", "Robotics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4c22470d870d9872cc0646656714c73bb104d94.pdf", "supplementary_material": "/attachment/78dbbfca26e0fb6b4977a5f5b4fcc53b3f2288d2.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces KeyWorld, a key-frame–based framework for robotic world models. It (i) selects motion-aware key frames via Ramer–Douglas–Peucker (RDP) over robot pose trajectories, (ii) uses a DiT/CogVideoX-style model to generate only those key frames, and (iii) interpolates the in-betweens with a lightweight CNN. On LIBERO, KeyWorld reports ~5.7× faster inference than full frame-to-frame generation with comparable or improved semantic/physical plausibility (incl. an object-level accuracy metric). Ablations vary key-frame density and compare RDP to uniform selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important computational bottleneck in video-based robot world models by reducing _unnecessary_ frame generation.\n\n- The approach is simple and easy to reproduce: key-frame detection + generation + interpolation, with clear implementation details.\n\n- Empirical results show substantial inference speedups (~5–8×) while maintaining comparable or in some cases better physical plausibility.\n\n- The comparison against uniform frame selection highlights the value of motion-aware key-frame identification.\n\n- Clear writing and diagrams that make the method easy to follow."}, "weaknesses": {"value": "- Limited novelty: Key frame-based video generation is well-established (as acknowledged in Related Work). The main contribution is applying RDP to robotic trajectories to select such key frames, which is somewhat incremental.\n\n- Evaluation limitations: Only evaluated on single-robot, fixed-camera scenarios and, most importantly, no comparison with other efficient world model alternatives (e.g., latent models, hierarchical models) \n\n- Method limitations: (1)  Requires access to robot state information for RDP, limiting applicability to vision-only settings. (2) The interpolation model could fail for complex motions between key frames. \n\n- Incomplete analysis: \n---- No study of how performance degrades with interpolation distance. \n---- Missing analysis of what makes a good key frame beyond motion transitions. \n---- No discussion of failure modes\n\n- Minor Issue: Inconsistent notation (x vs s for states)"}, "questions": {"value": "- Vision-only applicability: Is there a more general path for training this model without assuming access to poses? If not scalability of the method could be limited to robot-only (action-annotated) data. Could learned embeddings or motion cues substitute?\n\n- Trade-off behavior:  Do you have curves or an analysis showing fidelity vs. key-frame sparsity? This would clarify how aggressively one can compress.\n\n- Failure modes: What conditions break interpolation (fast contacts, occlusions, moving camera)? How would you go about mitigating this?\n\n- Alternative salience signals: Did you try simple alternatives (velocity/jerk thresholds, embedding change magnitude, attention-based salience)? Any insights?\n\n- Downstream planning: Have you tested whether KeyWorld outputs improve policy rollout or model-based planning performance? Currently, the evaluation focuses on video quality and plausibility, and demonstrating any downstream control benefit would significantly strengthen the robotics motivation and provide stronger support for some of the claims.\n\n- Robustness / tuning: How sensitive is performance to the RDP tolerance? Any heuristic or automatic schedule for setting key-frame density?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Qt3TnRr6U", "forum": "u3I4tDGhDV", "replyto": "u3I4tDGhDV", "signatures": ["ICLR.cc/2026/Conference/Submission9283/Reviewer_jUW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9283/Reviewer_jUW1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971035732, "cdate": 1761971035732, "tmdate": 1762920925737, "mdate": 1762920925737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a keyframe-centric framework for text-conditioned robotic world models. It first extracts motion-aware key frames from robot pose trajectories, trains a DiT-based generator (finetuned from CogVideoX) to synthesize semantically critical frames, and then uses a lightweight CNN gap estimator plus FILM interpolator to inpaint intermediate frames. On LIBERO dataset, KeyWorld cuts end-to-end inference time to <25% of frame-to-frame generation while maintaining PSNR/SSIM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem motivation is strong and the work has clear decomposition. The work improves redundancy limits in per-frame rollouts and cleanly splits generation into key-frame reasoning + lightweight interpolation.\n\n- The total latency drops to 17–18% of baseline across splits, with comparable PSNR/SSIM and large gains in object-level correctness on some sets."}, "weaknesses": {"value": "- Authors note that large pose differences make intermediate motion “difficult to model,” yet the interpolator is a lightweight CNN-style module, potentially brittle under long horizons or contact-rich transitions.\n\n- The approach explicitly targets scenes where observations are captured by a fixed camera, limiting applicability to moving setups.\n\n- Results are only on LIBERO variants, generalization to other robotic datasets or real-world videos is unclear.\n\n- As a video world model paper, no qualitative video results are provided. It's hard to evaluate the performance without watching the videos.\n\n- The paper claims that the proposed method improves \"physical validity\" of the generated videos. However, the metrics contain only PSNR/SSIM and object-level correctness, these do not directly assess physics/contacts or human preference."}, "questions": {"value": "- How would the pipeline adapt to camera motion. By estimating and compensating ego-motion before RDP selection or conditioning the generator on camera trajectory?\n\n- Could the authors validate on additional benchmarks or real robot captures (varying viewpoints and lighting) to test generalization beyond LIBERO? \n\n- Could the author provide qualitative video results?\n\n- Could the author conduct human preference studies to substantiate “higher physical plausibility” claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tgqd0BBzNM", "forum": "u3I4tDGhDV", "replyto": "u3I4tDGhDV", "signatures": ["ICLR.cc/2026/Conference/Submission9283/Reviewer_hi5p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9283/Reviewer_hi5p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978153079, "cdate": 1761978153079, "tmdate": 1762920925303, "mdate": 1762920925303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a method KeyWorld. Instead of generating motion predictions frame by frame, the authors observe that there are certain frames that are \"key\" for prediction, while other intermediate frames can be inferenced by interpolating the key frames.\n\nThe method has two steps:\n1. a key frame generation model trained on key frames extracted using robot pose vectors, these frames are selected to be sensitive to pose instead of uniform.\n2. reconstruction of the whole video by interpolating the frames using the FILM method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper choose to use key frames to resolve the key speed limitation for robotic WM deployment. The time needed for generation is only about 15-20% of the total time (or 5 times speedup) but with improved video generation quality."}, "weaknesses": {"value": "**Reliance on pose data**: to extract the key frames data, this method requires the RDP algorithm with robot pose data, which is an extra requirement compared with WM training with mainly video only data, or prior key frame aware work that utilized task descriptions.\n\n**Several task specific assumptions\": this paper makes a few assumptions, which are valid to the tasks studied here, but the assumptions may be too strong.\n  - The gap estimator assumes \"robot generally moves at a relatively stable speed\". In the experiment of this paper, these estimators seems to be relatively ok, as termed \"acceptable\" in appendix E.\n  - The method assumes the kinematic changes are good estimators for key frames. This assumption work well with the experimental dataset used.  However, this assumption may need more proper discussion on its potential limitation, and maybe experiments on more domains to confirm this."}, "questions": {"value": "- The authors try to approach the problem of \"real-time\" robotic control, but the method still requires minutes to do the prediction. Further, to generate next frames, the model needs to generate the next key frame first, which may creates another obstacle for reaching real-time.\n- The robot pose vectors are currently key to the method. What is a proposed way to overcome this. If the \"smooth\" frame transition is true, could we use that as a way to generate good key frame data instead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bGH6xAuOu5", "forum": "u3I4tDGhDV", "replyto": "u3I4tDGhDV", "signatures": ["ICLR.cc/2026/Conference/Submission9283/Reviewer_AKto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9283/Reviewer_AKto"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116993201, "cdate": 1762116993201, "tmdate": 1762920924977, "mdate": 1762920924977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary\nThis paper proposes a key-frame reasoning + lightweight interpolation paradigm for world-model video generation, designed to efficiently produce physically consistent videos and robot motion trajectories. The overall generation pipeline consists of four main components:\n\t1.\tKey-frame selection (offline).\nStarting from the robot’s state sequence (poses or joint angles), the method applies the Ramer–Douglas–Peucker (RDP) algorithm to extract key-frame indices at kinematic turning points, which represent only a small fraction of the total frames.\n\t2.\tKey-frame generation (heavy model).\nConditioned on the first-frame image and a textual instruction, the system uses a diffusion/DiT model (based on CogVideoX) to synthesize or edit only the selected key frames. This concentrates computation on a small set of critical frames, significantly reducing latency.\n\t3.\tGap prediction (lightweight network).\nBy encoding the visual differences between adjacent key frames, a small CNN/MLP predicts the number of intermediate frames (the gap) that should be inserted between them.\n\t4.\tInterpolation and reconstruction (lightweight module).\nUsing FILM or similar lightweight interpolation modules, the system generates the intermediate frames based on the predicted gap and reconstructs the complete video or motion sequence.\n\n⸻\n\nContributions\n\t1.\tMotion-driven key-frame paradigm.\nKey frames are selected from robot states rather than pixels or uniform sampling, aligning with physical semantic transitions such as grasping, lifting, and placing, thereby reducing redundant computation.\n\t2.\tTwo-stage generation design.\nThe computationally expensive diffusion/DiT model is applied only to key frames, while the remaining frames are generated through lightweight interpolation. This results in approximately linear scaling between compute cost and the proportion of key frames.\n\t3.\tMinimal gap-prediction head.\nA small model estimates the temporal span between key frames, improving interpolation stability and temporal consistency.\n\t4.\tBalanced efficiency–quality trade-off.\nOn standard robotic benchmarks, the proposed method achieves multi-fold speedups over frame-by-frame generation while maintaining or improving image quality and object/action consistency. Extensive ablation studies analyze the effects of key-frame density and sampling strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\t1.\tMotion-driven key-frame paradigm.\nUnlike common approaches that use uniform key-frame sampling or language-step alignment, this work selects key frames directly from the robot’s state sequence using the Ramer–Douglas–Peucker (RDP) algorithm at kinematic turning points, and concentrates heavy computation on these frames. This design forms a two-stage pipeline combining key-frame reasoning with lightweight interpolation. For world-model video generation, it redefines the notion of “key frames” from semantic or time-uniform choices to physically aligned, state-driven selections—a clearly novel perspective.\n\t2.\tCompute–key-frame linear scaling.\nThe inference cost scales approximately linearly with the proportion of selected key frames, making computational expense a tunable parameter. Such an explicit and controllable trade-off between cost and performance is rarely seen in comparable generation methods.\n\n⸻\n\nQuality\n\t1.\tSignificant speedup.\nOn benchmark tasks, the overall inference latency is reduced to roughly 17–25% of that of full frame-by-frame generation. Depending on the key-frame density, the reported acceleration ranges from approximately 3× to 8×, representing a substantial efficiency gain.\n\t2.\tMaintained or improved visual and physical consistency.\nDespite handling far fewer frames with the heavy generator, the method achieves PSNR and SSIM values comparable to or better than strong baselines. Moreover, object-level accuracy on the KeyWorld benchmark surpasses uniform key-frame selection, demonstrating that focusing computation on physically meaningful turning points helps capture critical events such as contact, grasping, and lifting.\n\n⸻\n\nClarity\n\t1.\tClear modular pipeline and simple interfaces.\nThe approach is transparently structured as four modular components—key-frame selection (offline), key-frame generation (DiT), gap prediction (MLP), and interpolation (FILM). Each stage has well-defined inputs, outputs, and dependencies, which greatly facilitates reproducibility and extension."}, "weaknesses": {"value": "Weaknesses (Specific and Actionable)\n\t1.\tKey-frame idea is not entirely new, but rather a state-anchored variant.\nThe concept of sparse generation plus interpolation has prior art in both video generation and robotic visualization—such as uniform sampling, language-step decomposition, and key-frame selection based on optical flow or action boundaries. The main novelty here lies in using robot states with the Ramer–Douglas–Peucker (RDP) algorithm for motion alignment. However, the paper does not include direct comparisons against language-step or semantic subgoal baselines, nor against purely visual action-boundary detection methods.\n\t2.\tDependence on accurate robot states limits applicability to video-only settings.\nThe method assumes access to precise pose or joint-state information. In many real-world, third-person videos (e.g., YouTube clips, handheld viewpoints, or moving cameras), such state data are unavailable. Without additional estimation mechanisms, the RDP-based key-frame selection cannot be directly applied.\n\t3.\tRobustness under moving cameras, multi-view setups, and heavy occlusion is unverified.\nKinematic turning points in robot state space do not necessarily correspond to visually salient changes in pixel space when the camera is moving or when occlusions occur. Consequently, RDP-selected key frames may become misaligned with visually important moments, leading to large and potentially unstable interpolation gaps between key frames.\n\n⸻"}, "questions": {"value": "Questions\n\t1.\tAlternatives for video-only settings.\nThe current key-frame selection depends on robot states (poses and joints). What if only videos are available, such as those with moving cameras or third-party footage? Could you provide a substitute key-frame selector based on visual motion estimation, optical flow, or action-boundary detection, and report its performance gap against the state + RDP approach in terms of PSNR/SSIM, object-level consistency, and latency?\n\t2.\tCross-paradigm comparison: language steps vs. semantic subgoals.\nUnder the same training budget and resolution, can you include a direct comparison against language-step decomposition or semantic-subgoal key-frame selection? It would be helpful to quantify how the three paradigms—state-aligned, language-aligned, and visual-boundary—differ in object consistency, contact-event recall, and overall latency.\n\t3.\tGeneralization and robustness of key-frame selection.\nDo the RDP threshold and the resulting key-frame ratio require manual tuning across different tasks and camera setups? Could you propose an adaptive scheme based on uncertainty or motion magnitude for threshold or ratio selection, and include benefit curves showing its impact on performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V9TNjaSFpX", "forum": "u3I4tDGhDV", "replyto": "u3I4tDGhDV", "signatures": ["ICLR.cc/2026/Conference/Submission9283/Reviewer_EYHT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9283/Reviewer_EYHT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762296247289, "cdate": 1762296247289, "tmdate": 1762920924572, "mdate": 1762920924572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}