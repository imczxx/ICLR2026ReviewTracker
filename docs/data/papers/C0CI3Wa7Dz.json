{"id": "C0CI3Wa7Dz", "number": 20105, "cdate": 1758302519783, "mdate": 1763766279737, "content": {"title": "Natural Language Actor-Critic: Policy Iteration in Natural Language Space", "abstract": "Large language model (LLM) agents---LLMs that dynamically interact with an environment over long horizons---have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. \nIn the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function.\nHowever, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space.\nIn this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values.\nThis approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. \nFurthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods.\nWe present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "tldr": "We propose a novel actor-critic algorithm to train LLM agents, where policy evaluation and improvement happens in natural language space using textual evaluations.", "keywords": ["reinforcement learning", "actor-critic", "large language model agents", "dialogue", "tool-use"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/011ccffd77cbdf98d1b60865b7d4bf73d94030b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a way to train LLM agents using an actor-critic approach where the critic provides a natural language critique of the agent's action together with an action refinement proposal. The critique is generated by training a language successor model to predict the outcome of the forthcoming sequence of actions. The authors took care to have the whole process implemented as off-policy RL to improve sample efficiency, and used a prioritized replay buffer ranked on the prediction loss to improve learning efficiency. The approach is evaluated using two LLMs of the Qwen family against React, fine-tuning and RL baselines, and shown to perform well."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the point that scalar-based, standard RL relies on blind exploration and that LLM knowledge can be used to refine actions more efficiently is a good point\n- the proposed architecture is sound, and most of its formalization is clear. See below for a critique on this point\n- the results are good (although they rise a few questions, see below)\n- the paper is reasonably well-written and easy to follow"}, "weaknesses": {"value": "- I would like to see a comparison of performance against closer baselines such as NLRL (Feng et al. 2025, NeurIPS), PNLC (Hong et al. 2025) or ArCHer (Zhou et al. 2024b). In particular, the NLRL paper has a NLAC algorithm (see Algo. 3 p. 19 and Section 4.3) which is very close to this one, thus a detailed comparison of the methods and an experimental comparison are mandatory. This is the main weakness of this work.\n- it is not specified whether the results are obtained just from one run, so their significance should be questioned.\n- clarity would benefit from a figure showing the architecture with all the components: the policy, the language successor model, the language evaluator, the refinement policy, the replay buffer, ... and how they are connected.\n- though I'm not expert of this literature, it seems to me that the main difference to NLRL, which aslo has an NL actor-critic architecture, is in the refinement policy component. And to me this refinement policy component is the one whose formalization needs the most improvement"}, "questions": {"value": "- an argument in favor of NLAC is computational and sample efficiency. Could you provide training and inference times for your experiments, and compare them to the ones that would be required to use NLRL, PNLC or ArCHer?\n- could NLRL, PNLC or ArCHer be applied to your benchmarks and if not, why? If yes, can you give the results for at least one of them? If not, could you provide the performance of your work in the benchmarks used in the NLRL paper?\n\n- line 193: \"We believe that the key for a critic to derive these explanations is the prediction and analysis of future outcomes.\" -> Did you try alternative mechanisms and compared their performance to yours? Did some other papers do so? You should turn this \"belief\" into a scientifically established fact.\n\n- Eq. (1), the notation is unclear. Is Pr[] a probability. Rather than Pr[x], y, z, shouldn't you use P(x|y,z)? \n\n- About Assumption 4.1: why is it formally necessary to make the process sound? Besides, what happens if two critics correspond to the same scalar? \n\n- I'm not convinced by the formalization of the refinement process calling upon a target policy and projection to the base policy. What I understand is that the LLM is just asked once through a prompt to propose a refinement based on the critique.\n\n*Remarks*:\n- having \"policy iteration\" in the title is weird, as your work is truly an actor-critic (AC) approach. AC is the equivalent of PI when the transition function is unknown...\n- paragraph \"Reinforcement Learning\" in Section 3 suffers from two slight over-simplifications: First, policy gradient methods such as PPO do not just use the gradient $\\nabla_\\pi$, they use it mutiplied by a value, action-value or advantage function which is a kind of critic. So you need a critic in these methods too, even though they do not qualify as actor-critic. Second, your action space being discrete, rather than using $Q(s_{t+1}, \\pi(s_{t+1}))$ as in DDPG, TD3 and SAC, you may use $max_a Q(s_{t+1}, a))$. So the paragraph could be sligtly reformulated to highlight that there are other options.\n- it would be nice if hyper-params k and m could appear in Algo 1\n- in the experiments, you could say once and for all that success is rewarded with +1 and speak about success in task descriptions. BTW, you don't say anything about failure, I assume the reward is 0 but you should specify it.\n- line 408 I had to search what k was (I remembered for m). Please remind the reader...\n- line 454: \"The only task where NLAC matches other methods\": you mean the only task where the performance of NLAC is not superior to other methods (we may understand that it is worse on all others :)).\n- results on 20Q are given as percents (between 0 and 100) whereas in $\\tau$-bench they are given between 0 and 1 -> you could use the same normalization\n- acheiving a 30% improvement: given (win or success) rates, I would rather say that NLAC is +~9% over competitors on 20Q and +~10% over competitors on Retail (rather than take the percentage over the competitors performance).\n\n\ntypos and minor remarks:\n- line 148: use \\citep for Yao et al. 2022.\n- line 176: \"based off of evaluations\". You don't mean based on evaluations, right? So rather say what it is based on.\n- line 185: \"LLMs are better suited to process and generate natural language over scalars\" -> you mean \"than scalars\"?\n- line 186: \"Therefore, we believe evaluation that is in natural language space leverages prior text-based reasoning capabilities\nof LLMs, and thus will largely improve sample efficiency.\" -> you could refer to Fig. 1 again here.\n- line 191: we will discuss later -> we discuss later\n- line 192: ultimately incredibly -> avoid too strong adverbs. If this is incredible, should I believe you? :)\n- line 213: some semblance -> ressemblance?\n- line 223: will \"discount\" -> \"discounts\"\n- \"Equation 3\" -> Equation (3) (use \\ eqref { }). Do this everywhere.\n- line 290: using $\\mathcal{M}$ and M to denote two different things is not a good idea\n- line 333: interpretted -> interpreted\n- line 333: \"but using generations by the refinement policy rather (than?) a teacher policy that is a separate model.\" -> I don't understand what you mean\n- line 400: as well an -> and an (?) or as well as an (?)\n- line 406: on any airline scenario(s).\n- line 480: signficantly -> missing i\n- the Carta et al. paper is given two identical references"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CNIu3zNThx", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["ICLR.cc/2026/Conference/Submission20105/Reviewer_JPr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20105/Reviewer_JPr6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760941814905, "cdate": 1760941814905, "tmdate": 1762933001438, "mdate": 1762933001438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Concerns Regarding Substantial Overlap and Attribution of Prior Work NLRL (arXiv:2411.14251)"}, "comment": {"value": "We are the authors of the paper \"Natural Language Reinforcement Learning\" (NLRL), first posted on arXiv in Nov 2024 (https://arxiv.org/abs/2411.14251). We thank the reviewers for their diligent work and would like to raise serious concerns regarding the substantial overlap between this submission, \"NATURAL LANGUAGE ACTOR-CRITIC\" (NLAC), and our prior work.\n\nWe especially appreciate Reviewer JPr6 for correctly identifying our paper as a critical and missing baseline. We believe a closer examination shows that the similarities extend beyond a missing comparison and touch upon the core conceptual framework, methodology, and even textual phrasing presented in this submission.\n\nOur key concerns are as follows:\n\n**Identical Core Contribution:** The central idea of reformulating RL principles into a natural language space for LLMs is the foundational premise of both our work and this submission. NLAC's \"natural language critic\" is functionally identical to the \"Language Value Function\" that we introduced and formalized in NLRL.\n\n**Systematic Rebranding of Key Concepts:** The core theoretical components of NLAC appear to be direct counterparts to those in our work, but have been renamed. This includes the \"Language Bellman Backup\" (our Language Bellman Equation), \"Refinement Policy\" (our Language Policy Improvement), and \"Language Successor Model\" (part of our Language MC/TD formulation).\n\n**Strikingly Parallel Structure and Phrasing:** The similarities extend to the structure and specific phrasing of the methodology. For instance, the introduction to the \"Policy\" component in NLAC's Section 5.1 almost exactly mirrors how we introduce \"LLMs as language policy\" in our work (NLRL v1, Sec 4.1). Both sections begin by stating that LLMs are used as policies, immediately highlight the benefit of leveraging chain-of-thought, and cite the exact same foundational papers (Wei et al., Yao et al.) to support this point. This degree of structural and textual parallelism is highly unusual and suggests more than an independent derivation of ideas.\n\n**Mischaracterization of Our Work to Claim Novelty:** This is a critical point of concern. The NLAC paper (lines 122-125) incorrectly claims our work is limited to on-policy Monte Carlo methods. This is a factual misrepresentation. Our paper explicitly formulates and discusses both on-policy \"Language Monte Carlo\" and, crucially, \"Language Temporal-Difference\" updates (see Section 3.2.1 and Equation 6 in v3). NLAC then presents its \"language Bellman backup\" (an off-policy TD update) as a novel solution to a limitation that does not exist in our work.\n\n**Omission of Direct Experimental Comparison:**: As noted by Reviewer JPr6, a direct comparison is mandatory. Given the profound conceptual similarity, the absence of NLRL as a primary baseline makes it difficult to assess the true scientific contribution of NLAC.\n\nFor verification, our paper is publicly available on arXiv:\nv3 (latest): https://arxiv.org/abs/2411.14251v3\nv1 (original submission): https://arxiv.org/abs/2411.14251v1\n\nWe believe these points raise significant questions about the novelty and positioning of this submission. Proper attribution and contextualization of prior work are fundamental to scientific progress.\n\nTherefore, we urge the authors to clarify the extensive conceptual and textual overlaps and the specific points raised here. At the same time, we respectfully ask the program committee to carefully review these substantial similarities when evaluating the paper's scientific contribution and originality.\n\nWe have provided a more detailed analysis to the ICLR Program Chairs for their review.\n\nSincerely,\nXidong Feng and the authors of \"Natural Language Reinforcement Learning\""}}, "id": "TWZdNDijeV", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["~Xidong_Feng1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Xidong_Feng1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762970029507, "cdate": 1762970029507, "tmdate": 1762970029507, "mdate": 1762970029507, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"Natural Language Actor-Critic: Policy Iteration in Natural Language Space\" proposes a principled actor-critic method, that uses a finetuned LLM either as actor and critic. The novelty lies mainly in the use of a generative critic, that produces feedback predictions on the future of trajectories, rather than -- less-informative -- numerical rewards. The approach takes inspiration from fundamental works in RL, specifically works about the successor measure, and nicely transfer them in the agentic setting with natural language feedbacks. Experiments demonstrate the good behavior of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting paper that takes inspiration from strong fundations. While optimizing with LLMs is always a bit opaque and lacks guarantees, I feel the paper makes one useful step toward more principled LLM agents finetuning. \n\n- Good experimental results"}, "weaknesses": {"value": "- Presentation :  While writting is globally rather good, I was sometimes a bit lost during section 5 reading. The structure and discussion could be improved from my point of view. A starting point would be to specify more clearly that $\\cal M$ is a central finetune LLM that is used from various prompts to cover different goals building the various building blocks of the method. Better formally defining its inputs and outputs would beuseful. \n\n- Training objective : Finally, the model $\\mathcal{M}$ is used for all components of the method (with different prompts as input), but it is \\textit{fine-tuned} only for two tasks: (1) matching the successor measure, using $p_{\\text{pred}}$ as input, and (2) maximizing the likelihood of the refined action, using $p_{\\text{react}}$ as input. From what i understand, there is no additional training for other types of tasks --- these rely solely on the general capabilities of the LLM, elicited through well-crafted prompts. No regularization term is applied either, which would avoid deviating too far from the reference model, possibly inducing catastrophic forgetting of the model’s general abilities. From my point of view, this issue could occur if the model is fine-tuned too aggressively, making the training process difficult to stabilize. More details on this aspect would be valuable.\n\n- Performance analysis : The paper only reports final accuracies for each considered task, but does not analyze the training dynamics. Performance curves regarding various metrics during the training process would be useful to understand these dynamics. Is the model oscillating ? Is it deviating far from the reference model ? How the entropy of the model evolves regarding the different inputs ? How does training on for instance the successor measure prediction transfer for other tasks ? etc."}, "questions": {"value": "Beyond questions / concerns reported in the weaknesses section, I have the following additional questions : \n\n- Authors consider SAC as a baseline. But not enough is said about how it used. Are the actor and the critic still LLMs that you finetune ? If yes how. If no, how does it deal with textuel inputs and actions ? Also, it is known that SAC is not good for the discrete domain. Wouldn't it be preferable to consider more common RL methods for LLMs RL finetuning, such as PPO / GRPO / DPO ?\n\n- In many places in section 4, author say that the only network to be finetuned is the successor model. But in section 5 I understand that finally not, as the same model is used for many tasks (and is also finetuned for the policy task). Please clarify\n\n- $\\theta$ appears at different place in Equation (6). The gradient is only backpropagating through the policy (the first term) right ? Not through $Q^\\theta$ for instance ?\n\n- Prioritized replay is said using a alpha parameter. But nothing is said about is. It is a temperature temperatur regarding errors on transitions ? or a factor of Importance Sampling as done in the original paper ? what kinds of errors are used for this priorization (the successor measure divergence or the negloglikelihood of the selected action ?\n\n- An interesting abblation would be to directly use the refined action as the sampled action, rather than requiring a policy component, with its associated loss. Is it actually useful ?      \n\n- Eq (2) uses a D_f term without discussing it at any place. We understand that this a kind of f-divergence but it could be useful to precise it . \n- Defining 4.3 that tells that E takes sequences of rollouts associated with rewards as input is not fully consistent with its formalization in eq (3)\n \n\n\n- In section 5, formalization of the language bellman backup : P_L ==> B_L"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nBZgcwix0v", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["ICLR.cc/2026/Conference/Submission20105/Reviewer_MUW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20105/Reviewer_MUW2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775005112, "cdate": 1761775005112, "tmdate": 1762933000783, "mdate": 1762933000783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Natural Language Actor-Critic (NLAC), a new actor-critic algorithm designed to train Large Language Model (LLM) agents. The authors identify that standard policy gradient methods are often unstable and sample-inefficient for long-horizon tasks with sparse rewards. To address this, NLAC replaces the traditional scalar-value critic with a generative LLM critic that produces textual critiques of actions. This natural language feedback aims to provide a richer, more actionable training signal by explaining *why* an action is suboptimal. The critic is trained off-policy using a novel \"language Bellman backup\", and the actor (policy) is improved by using the critic's textual evaluation to refine its own actions, which are then used as targets for a distillation-style update. The authors demonstrate that NLAC outperforms existing methods like PPO, GRPO, and RFT on tasks involving reasoning, dialogue, and tool-use."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important Problem: The paper tackles a critical and timely problem. Improving the stability and sample efficiency of RL fine-tuning for LLM agents, especially in complex, long-horizon scenarios with sparse rewards, is a significant challenge in the field.\n\n- Well-Motivated Method: The central idea of using language-based evaluations is strongly motivated. Instead of a simple scalar value, a natural language critique can provide richer, more interpretable feedback, which intuitively leverages the inherent reasoning capabilities of LLMs for policy improvement.\n\n- Data Efficiency: The algorithm is designed to be trained off-policy, which offers a promising and more data-efficient alternative to the prevailing on-policy methods (like PPO) that require new trajectories at every step."}, "weaknesses": {"value": "- Soundness of Policy Improvement: The policy improvement step, defined in Equation 6, appears to have a potential flaw. The algorithm trains the base policy $\\pi_{\\theta}$ to mimic the refined action $a_t^r$. This update relies on the crucial assumption that the refined action $a_t^r$ is guaranteed to be better than the original action $a_t$. However, this improvement is not enforced; the refinement policy $\\pi^r$ is merely *prompted* to generate a better action.\n\n- Limited Empirical Evaluation: The experimental validation could be more robust.\n\n  1. Model Diversity: The paper only reports results on two models from the same family (Qwen2.5-7B-Instruct and QwQ-32B). Testing the method on other widely-used and structurally different open models (such as Llama, or DeepSeek-distilled models) would be necessary to demonstrate the generalizability of the approach.\n\n  2. Lack of Ablations: The paper introduces several novel components, chiefly the language Bellman backup objective ($\\mathcal{L}_1$) and the refinement-distillation objective ($\\mathcal{L}_2$). However, there are no ablation studies to isolate the contribution of each component. For instance, how does $\\mathcal{L}_1$ compare to a simpler critic trained on Monte Carlo rollouts? How crucial is the $\\mathcal{L}_2$ update? Without these ablations, it's difficult to attribute the performance gains to specific parts of the proposed algorithm."}, "questions": {"value": "- Regarding the primary weakness: What happens if the refinement policy $\\pi^r$ consistently fails and generates an action $a_t^r$ that is *worse* than $a_t$? Since the policy $\\pi_{\\theta}$ is trained to mimic $a_t^r$ regardless, how does the algorithm prevent or recover from this \"actor-critic-refiner\" misalignment?\n\n- The paper claims better data efficiency but provides results based on a fixed number of gradient steps. Could the authors provide comparisons based on the number of environment samples or wall-clock time? How does the significant computational overhead of generating critiques and refined actions compare to the standard rollouts of PPO?\n\n- The practical implementation relies on complex, multi-step prompting, especially for correcting the chain-of-thought in the Bellman backup and refinement policy. How sensitive is the algorithm's performance to the precise wording and structure of these prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u5ASSLsbo0", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["ICLR.cc/2026/Conference/Submission20105/Reviewer_6UAb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20105/Reviewer_6UAb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976929445, "cdate": 1761976929445, "tmdate": 1762933000176, "mdate": 1762933000176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Training LLM agents for long-horizon, multi-turn tasks with sparse rewards is unstable and sample-inefficient under on-policy policy-gradient methods (e.g., PPO/GRPO). \nWhile standard actor–critic could, in principle, be applied, the authors argue it fails to exploit pretrained LLMs’ text-based reasoning.\n\nThey propose Natural Language Actor-Critic (NLAC), which replaces a scalar critic with a generative, natural-language critic. \nThe critic produces textual evaluations of actions that the policy can read and use for self-refinement, enabling policy iteration in language space and training that can proceed off-policy.\n\nConcretely, NLAC defines a language successor model $M^\\pi$ and a language Bellman backup $\\mathcal{B}_L$ that constructs one-step, off-policy targets over distributions of future textual descriptions (and terminal reward). \nA language evaluator $E$ then aggregates one or more sampled futures from $M^\\pi$ in-context to produce a textual critique $Q^\\pi\\_L$.\nConditioned on this critique, a refinement policy proposes an improved action $a\\_t^r$.\nThe base policy is then distilled toward these refined actions via a likelihood/divergence objective (rather than enumerating actions or using scalar Q for a closed-form policy).\n\nExperiments on MATH500-Hard, 20 Questions (20Q), and τ-Bench show that, under a fixed training budget of 30,720 gradient steps, NLAC consistently outperforms RL fine-tuning baselines (e.g., PPO/GRPO) and surpasses GPT-4.1 prompting on the long-horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed critic can explain why/how an action is suboptimal, reducing reliance on random exploration in language action spaces.\n- The authors reuse a single LLM with role-specific prompts (policy, successor, evaluator, refiner), simplifying engineering.\n- NLAC beats PPO/GRPO and even GPT-4.1 prompting on multi-turn dialogue/tool-use benchmarks.\n- The paper extends actor–critic ideas into language space—an interesting perspective for PRMs and distributional RL–inspired methods."}, "weaknesses": {"value": "- Lacks contraction/monotonic-improvement or SAC-style guarantees; the language backup remains heuristic.\n- Limited ablations (e.g., **numbers of sampled futures $k$ and refinement rounds $m$**; **further gradient steps**; reverse-KL vs. forward-KL; the effect of prioritized replay; EMA rate; and smaller base model than 7B).\n- It is unclear how to apply the approach to general RLHF tasks with non-binary and/or dense rewards."}, "questions": {"value": "- (weakness 1) Since the proposed approach is fairly complex, do the authors provide any theoretical derivations to justify such a complex design?\n- (weakness 2) Do the authors have plans for additional ablation studies?\n- (weakness 3) Do the authors have plans to apply NLAC to general RLHF tasks with non-binary and/or dense rewards?\n- I believe a similar high-level concept paper [1] exists (with different details). Could the authors compare their method against [1]?\n- Are there any GRPO results with larger sample counts (e.g., 32 or 64)? Using only 4 seems small given GRPO’s motivation.\n- What is the exact role of the scalar reward in this paper? Is it important for performance? Are there any ablation studies on this as well?\n\n[1] Wang, Hanyang, et al. \"Text2Grad: Reinforcement Learning from Natural Language Feedback.\" arXiv preprint arXiv:2505.22338 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1grKonUPQG", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["ICLR.cc/2026/Conference/Submission20105/Reviewer_mvfc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20105/Reviewer_mvfc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008757958, "cdate": 1762008757958, "tmdate": 1762932999475, "mdate": 1762932999475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Joint Response Regarding Revisions to Paper"}, "comment": {"value": "We would like to thank all the reviewers for their helpful reviews. After aggregating feedback from all the reviews, we made several key revisions to the paper:\n\n**Theoretical Analysis of NLAC**\n\nWe added a section on theoretical analysis showing our method can converge to the optimal policy. Traditional analyses do so by showing convergence of learned Q-values to the ground-truth, then monotonic improvement of the policy by optimizing such Q-values. However, in our setting, we never explicitly learn Q-values that are scalars. The key insight we leverage to circumvent this difficulty is that when learning our successor function, we show that the underlying representations converge to the unique successor features of the MDP [1]. This gives us a way to connect our learned language values to the true scalar Q-values. \n\n**Improvements to algorithm description**\n\nMultiple reviewers raised some important concerns about the effectiveness of our method, namely that multiple components of our method are not explicitly trained, but rely on the base reasoning capabilities of the LLM via careful prompting. These are concerns that we also had during the development of our method. \n\nWe add two new subsections explaining modifications that we considered making to our algorithm to address these concerns, namely: (1) explicitly training the refinement policy using policy optimization on achieved reward after refinement, and (2) regularization to the initial LLM model to mitigate catastrophic forgetting. Overall, we excluded these modifications from the default implementation of our method because we found that though they improved stability (allowing for training for more iterations), they ultimately did not improve final performance by a significant enough margin. \n\nWe also added figures to our revised paper showing how the different components of our method interact, as we realized that the description on its own was dense and potentially difficult to parse.\n\n**Comparison to NLRL**\n\nThe most similar prior method to ours is NLRL [2], which (to our knowledge) is the first work to propose learning a critic for RL fine-tuning that outputs evaluations in natural language space. We discuss our contribution on top of NLRL in more detail in the revised paper, but on a high-level, our algorithm NLAC falls under the paradigm of algorithms introduced by NLRL, with the goal of being more scalable and generalizable to complex tasks. Specifically, our algorithm NLAC aims to improve upon two aspects of the NLRL that we believed made it difficult to scale to general LLM agent tasks:\n\n(1)  During policy evaluation, the critic in NLRL is trained by aggregating multiple on-policy samples in-context and generating a holistic evaluation using these samples. Such on-policy samples can simply consist of next state with future predicted value (like in standard temporal difference learning), but for complex tasks, we found enumerating over environment transitions in-context for one training sample to be too expensive. In NLAC, rather than training the critic to output aggregated evaluations, we train a successor model to probabilistically generate rollouts. We found that such training can be done using a single transition like in traditional RL, is theoretically motivated by learning successor features, and can later be used for action refinement.\n\n(2) During policy improvement, NLRL provides multiple candidate actions and their evaluations and performs an \"argmax\" over them in natural language space. In our settings, the action space is too large for such enumeration to be tractable, and we found empirically that simply sampling a tractable subset of actions does not usually adequately explore the action space. Hence, in NLAC, we consider iterative refinement of the policy. In order to refine the policy, the evaluation must explain how an action can be improved, which we provide by including a description of what will likely happen afterwards.\n\nBecause of those drawbacks, we initially believed NLRL would be intractable to implement for our experiments, but we have hence decided to evaluate against it, only limiting the number of samples that are fit in-context. Thus, in the revised paper, we compare against NLRL, where we limit to 8 in-context transitions and actions to keep training tractable; note that this can be interpreted as providing 8 times more samples during training. NLRL performs competitively in both math reasoning and 20Q, but falls short in \\tau-bench. We found that in \\tau-bench, which has the most complex dynamics and action space, the language values learned by NLRL were always positive in sentiment. We believe this could be due to not seeing enough in-context samples during training to overcome the implicit positive bias of instruction-fine-tuned LLMs.\n\n[1] https://arxiv.org/abs/1606.05312\n\n[2] https://arxiv.org/abs/2411.14251"}}, "id": "E5Pp7Ekb9S", "forum": "C0CI3Wa7Dz", "replyto": "C0CI3Wa7Dz", "signatures": ["ICLR.cc/2026/Conference/Submission20105/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20105/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission20105/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763766883285, "cdate": 1763766883285, "tmdate": 1763766883285, "mdate": 1763766883285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}