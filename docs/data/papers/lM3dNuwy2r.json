{"id": "lM3dNuwy2r", "number": 4235, "cdate": 1757644358239, "mdate": 1759898045102, "content": {"title": "What Is Preference Optimization Doing, How and Why?", "abstract": "Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending the causes of their differences.\nFirst, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. \nSecond, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other.\nHowever, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.", "tldr": "", "keywords": ["Large Language Models", "Preference Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f921c767b6c135bb63596bf6e488913905e66b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the training dynamics of preference optimisation when using a DPO and a PPO approach. To this end, they analyse the gradient alignment condition and look in more detail at the effect of the positive components, the negative components individually, as well as the impact of the gradient weight split into three tertiles. Moreover, they conduct ablation studies of dynamically adapting the positive and negative components, as well as the weighting, during preference optimisation training.\n\nThis paper gives a novel insight into the training dynamics of what is happening during PO in more detail, proposes some interesting hypotheses of the observed effects, and finally makes some initial proposals on how to improve the performance/gradient alignment when doing PO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In my opinion, this paper introduces some new and novel insights into what is actually happening during preference optimisation, with a particular focus on the positive and negative components during training. I found their insight that DPO seems to overfit on the positive components and later focuses on the negative components, particularly interesting. I also appreciate their follow-up experiments in the appendix, which further validate their hypothesis.\n\nIn general, I find the paper mostly well written, and it's clear to follow. The experiments that are provided make sense to me and support the claims that the authors are making."}, "weaknesses": {"value": "I think there are specific ways in which the paper could be further improved upon:\n- The experiments lack any confidence interval, standard error, or at least standard deviation, which could indicate that the results are actually statistically significant. Especially in the win-rate experiments, reported in Figures 4b, d, e, and f (basically all the PPO ablations), the values of the win-rates seem to be within $\\pm2$ pp, and I have a suspicion that they may not be statistically significant. Naturally, I understand that running the experiment multiple times is computationally expensive; therefore, the authors could consider, for example, bootstrapped confidence intervals of the test set. \n- slightly related to the previous weakness, the proposed improvements on DPO and PPO, displayed in section 4, do not really seem to make a strong (or any) improvement over normal DPO and PPO. While we gain valuable insight into the dynamics of learning, the proposed solutions based on these insights do not seem particularly compelling. (Yet I understand that this is only a subpart of the paper, and the primary focus is on the insights.)\n- I understand that the authors focus on one single model to demonstrate all the findings of the PO dynamics. I wonder how much these results are a result of the backbone model itself? Have you tested these experiments on a slightly newer backbone architecture, of similar size (e.g. Qwen25-3B, Llama3.2-3B, Gemma3-4B), and are the results still consistent?\n- In Figure 4, to me it is not immediately clear what Cases 1,2,3 are, even after trying to find them in the text. Maybe the legend can be renamed\n- Figures 1c and 2c are hard to distinguish between the different shades of green, and the curves often overlap."}, "questions": {"value": "- Why are the win-rates of the DPO so much higher than the PPO approach? I know this is not directly linked to the insights of your paper, but I feel like they should be in the same range to make comparable claims that apply to both.\n- What are cases 1,2, and 3 in Figure 4?\n- Are the results actually statistically significant?\n- Are the results consistent when using different LLM backbones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UqCCz0vF4M", "forum": "lM3dNuwy2r", "replyto": "lM3dNuwy2r", "signatures": ["ICLR.cc/2026/Conference/Submission4235/Reviewer_i8HD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4235/Reviewer_i8HD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760800230329, "cdate": 1760800230329, "tmdate": 1762917244111, "mdate": 1762917244111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a metric, *i.e.*, gradient alignment, to quantize the contribution of gradient descent to the log-probability of the final answer.\nThe optimization dynamics of two popular post-training algorithms, *i.e.*, DPO and PPO, are then analyzed.\nThe conclusions include\n\n* DPO behaves like supervised fine-tuning as it has relatively stable targets\n* As training progresses, negative learning dominates target shaping, while positive learning prevents collapse\n* The implicit reward is not reliable but primarily serves as a regularizer to mitigate over-fitting\n* PPO behaves like reinforcement learning as its exploration covers a broad range of conflicting responses\n* Positive learning encourages discovery for new targets while negative learning fosters further exploration\n* Loss re-weighting controls exploration\n\nMultiple variants, *i.e.*, cDPO, cPPO, hPPO, are also proposed to ablate the effect of negative learning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The components of preference learning, *e.g.*, positive and negative learning and loss re-weighting, are analyzed thoroughly.\n* Ablation study strengthens the persuasiveness of the conclusions and provides insights for future research."}, "weaknesses": {"value": "I deem that several logical flaws hinders the soundness of the conclusions, so I lean to reject the paper.\nI would like to raise my score if these concerns are well addressed.\n\n* L105: Why does the distinction between SFT and RL lie in whether they have relatively stable targets?\nI deem the difference between SFT and RL lies in whether they learn from demonstrations or rewards.\n* L134 (Minor): I do not think the objective is inherently non-differentiable.\n* L143: It is not very clear to me why the log-probability of final answer rather than the ground truth is considered.\nL108 claimed that SFT is expected to steadily progress toward the targets, while the final answer is not necessarily the target.\n* L157 (Minor): I deem the design of gradient alignment can be regarded as extension of [1], which may be cited and discussed.\n* L160: It is claimed that the difference between SFT and RL lies in whether they have stable objectives, and here it is classified based on the value of gradient alignment.\nI understand that positive gradient alignment indicates that the gradient descent increases the log-probability of the final answer.\nWhy does this also indicates a stable objective?\n* L168: Only a single setting is performed so that it is not clear how well the conclusions can be generalized.\n* L315 (Minor): I think $\\hat{A}$ inherently can be negative without estimation and normalization.\n\n[1] Estimating Training Data Influence by Tracing Gradient Descent, NeurIPS 2020."}, "questions": {"value": "* L255: What is the unbiasedness of learning objective of DPO?\nWhy does that only hold at the optimal parameter?\n* L285: Is there any evidence to support the proposal?\n* L335: Is there any reference to support such definition for positive and negative learning and loss reweighting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hXKNS3Sa4Z", "forum": "lM3dNuwy2r", "replyto": "lM3dNuwy2r", "signatures": ["ICLR.cc/2026/Conference/Submission4235/Reviewer_CKPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4235/Reviewer_CKPZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761067408353, "cdate": 1761067408353, "tmdate": 1762917243385, "mdate": 1762917243385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the optimization dynamics of Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) to elucidate the distinct roles of positive learning, negative learning, and loss reweighting. The authors introduce a \"gradient alignment\" metric to investigate how learning targets evolve during training. The analysis reveals that in DPO, positive/negative learning jointly shape targets while loss reweighting acts as a regularizer. In contrast, PPO uses negative learning to aid exploration, and its loss reweighting differentiates the roles of token groups in updating the policy. The authors substantiate these findings with ablation studies examining the practical performance implications of controlling these dynamic components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a deep, mechanistic explanation for the oft-discussed differences between DPO and PPO by skillfully analyzing their respective training dynamics.\n2. The introduction of the 'gradient alignment' metric is a notable contribution, offering an effective method to quantify and inspect the optimization dynamics of preference alignment algorithms.\n3. The findings are clear and insightful, providing actionable explanations for the distinct roles of positive learning, negative learning, and loss reweighting.\n4. The paper's analytical claims are well-supported by sufficient empirical validation, including targeted ablation studies that connect the observed dynamics to practical performance."}, "weaknesses": {"value": "1. The paper provides extensive empirical analysis, but it lacks a rigorous theoretical foundation to formally explain the underlying reasons for the observed phenomena.\n2. The analysis could be strengthened by incorporating the distribution of key data properties. For instance, analyzing the distributions of the DPO reweighting term ($\\omega$) and the PPO absolute advantage ($|\\hat A|$), both globally and within subgroups, would provide a more complete picture of their impact.\n3. The 'gradient alignment' metric is a first-order approximation that does not account for the adaptive, non-linear dynamics of optimizers like AdamW or the non-convex landscape. \n4. Minor Issues on Presentation:\n* The conclusion (Section 5) is somewhat lengthy and could be compressed. This would create space to either expand the main analysis or move valuable insights from the appendices (e.g., parts of Appendix D) into the main paper.\n* The experimental cases (e.g., \"Case 1-3\") in Figure 4 are not clearly explained in the text, making the results difficult to interpret fully."}, "questions": {"value": "1. To strengthen the analysis on reweighting, could the authors show the distributions of the DPO term ($\\omega$) and the PPO absolute advantage ($|\\hat A|$)? It would be insightful to see this for the entire dataset and within the 'top', 'middle', and 'bottom' subgroups. This might also help justify the current split into three equal-sized groups, or perhaps suggest a more natural, data-driven way to segment the data.\n2. The cDPO (controlled DPO) in Appendix E explores a gradual shift from positive to negative learning. As a clearer ablation to test the \"role-switching\" hypothesis, what would be the result of a hard switch? (e.g., training only with the positive learning component for the first half of training, and only with the negative component for the second half).\n3. In lines 349-351, the authors state that in PPO, 'positive learning is stable in shaping the learning targets.' This is a key finding. Could the authors provide any further mathematical derivation or theoretical intuition to explain why this is the case, while negative learning's role is relegated to exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2VFmg3BClp", "forum": "lM3dNuwy2r", "replyto": "lM3dNuwy2r", "signatures": ["ICLR.cc/2026/Conference/Submission4235/Reviewer_pfMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4235/Reviewer_pfMR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761190479158, "cdate": 1761190479158, "tmdate": 1762917242819, "mdate": 1762917242819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies what preference optimization (PO) does by analyzing optimization dynamics for DPO and PPO through a gradient-alignment metric that measures the dot product between the PO objective gradient and the gradient of expected NLL on final responses. It reports that DPO behaves like supervised learning with targets implicitly shaped by both positive and negative learning, while PPO behaves like reinforcement learning with exploration near orthogonal targets; loss reweighting acts more like regularization in DPO and carries token-level information in PPO. The authors further test behavior-control variants (cDPO, cPPO, hPPO) and show illustrative win-rate gains on AlpacaEval with Pythia-2.8B."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured decomposition of PO into positive/negative learning and reweights; which connects intuitively to training heuristics.\n- The *gradient alignment* tool is simple, and allows concrete insights.\n- Evaluates variants of PO (cDPO, cPPO, hPPO) from the insights acquired from the analysis."}, "weaknesses": {"value": "- Insufficient breadth and scale of experiments\n    - The paper uses a single base model (Pythia-2.8b) and narrow task sets. The claims in the paper about \"what PO is doing\" should be tested on larger models, multiple families, and varied domains.\n    - Although the motivation of the paper seems promising, empirical proof of PO tendency should be backed up with much more depth.\n- Lack of theoretical framing\n    - Tightening the theoretical relation between $G$ and the performance can strenghthen the motivation of the paper, when extensive experiments is infeasible."}, "questions": {"value": "- Refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8SxyKJ6kD", "forum": "lM3dNuwy2r", "replyto": "lM3dNuwy2r", "signatures": ["ICLR.cc/2026/Conference/Submission4235/Reviewer_wNW2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4235/Reviewer_wNW2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996336239, "cdate": 1761996336239, "tmdate": 1762917242436, "mdate": 1762917242436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}