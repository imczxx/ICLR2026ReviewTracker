{"id": "rmvO5WRb58", "number": 15016, "cdate": 1758246906110, "mdate": 1759897335315, "content": {"title": "Transductive Learning for Out-of-Distribution Molecular Property Prediction", "abstract": "Predicting molecular properties outside the training data distribution (Out-of-Distribution, OOD) is critical for accelerating drug discovery. This task requires models to extrapolate beyond known property ranges and generalize to novel chemical structures—a common failure point for standard machine learning models. While transductive analogical reasoning shows promise, prior methods are often constrained by fixed descriptors and single-anchor comparisons. To overcome these limitations, we introduce Multi-Anchor Latent Transduction (MALT) framework, which operates directly within a learned latent space. MALT can leverage embeddings from any powerful, pre-trained molecular encoder to select multiple relevant analogues of query molecule. It then integrates the query and anchor embeddings to generate a final prediction. On rigorous OOD benchmarks targeting shifts in both property values and chemical features, MALT consistently improves generalization over standard inductive baselines. Notably, our framework also matches or surpasses the in-distribution performance of these base models. These findings establish multi-anchor transduction in latent space as an effective strategy to augment existing molecular encoders, enabling robust and extrapolative predictions needed to solve challenging discovery tasks.", "tldr": "We propose a novel multi-anchor transductive framework operating in learned latent spaces that improves out-of-distribution generalization for molecular property prediction by leveraging multiple relevant analogues.", "keywords": ["Out-of-Distribution", "OOD", "Out-of-Distribution Generalization", "Molecular Property Prediction", "MPP", "Transductive Learning", "Multi-Anchor Reasoning", "Molecular Encoders", "Moelcule Representations", "Drug Discovery", "Materials Science", "Machine Learning", "Deep Learning", "Extrapolation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab9549e420e881f834b54e59d9d6b7fefce1cb91.pdf", "supplementary_material": "/attachment/ed273825769a4f0728dd2ebfc4c54ff8cd7d827f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a transductive method to address the out-of-distribution (OOD) problem in molecular representation learning. The authors employ a strategy that selects anchor samples from the training data to integrate features for input query molecules, thereby generating their representations. This approach leads to improved model performance under OOD data splits. The model was evaluated on multiple datasets for both classification and regression tasks, where it demonstrated superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear, making the paper easy to follow.\n2. The experimental analysis is comprehensive, thoroughly evaluating the method's effectiveness from various aspects."}, "weaknesses": {"value": "1.  The presented baseline comparisons are limited. The work does not compare against any existing Test-Time Adaptation (TTA) paradigms from related work. Furthermore, for invariant molecular representation learning under the inductive setting, only one baseline is compared, which is not the most recent. There exist more up-to-date baselines, for example [1]. For the regression task, no inductive invariant learning baselines are included at all; a comparison could be made by adapting the predictive loss $ \\mathcal{L}_{pred}$ from [2] for regression.\n\n2.  Some of the definitions and formulations in the paper are confusing. Please refer to the **Questions** section for details.\n\n3.  The proposed method is not strictly limited to molecular representation learning and could potentially be applied to broader graph OOD problems. It is a pity that the authors did not explore this with corresponding experiments.\n\n**Reference**\n\n[1] Aming, W. U., and Cheng Deng. \"CFD: Learning Generalized Molecular Representation via Concept-Enhanced Feedback Disentanglement.\" *The Thirteenth International Conference on Learning Representations*.\n\n[2] Zhuang, Xiang, et al. \"Learning invariant molecular representation in latent discrete space.\" *Advances in Neural Information Processing Systems* 36 (2023): 78435-78452."}, "questions": {"value": "1.  Equation 3 is puzzling. Even if the possible values of $\\Delta x_{tr}$ are constrained to the set $\\\\{x_{an}-x_{j} |  x_j \\in D_{X}^{tr} , y_{j} < y_{an}  \\\\}$,  the specific value of $\\Delta x_{tr}$ used in the equation is still not fixed and can vary within this set. Could you please clarify how this value is determined?\n2.  The main text describes the Transduction Module as using a fixed, rule-based algorithm for anchor selection. This appears to contradict the \"Update $\\mathcal{T}$\" step in Algorithm 1. Could you explain the necessity and purpose of this update step?\n3.  What was the policy for selecting the model checkpoint used for inference on the test sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5HSzwjIq4C", "forum": "rmvO5WRb58", "replyto": "rmvO5WRb58", "signatures": ["ICLR.cc/2026/Conference/Submission15016/Reviewer_u6hj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15016/Reviewer_u6hj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728377642, "cdate": 1761728377642, "tmdate": 1762925346450, "mdate": 1762925346450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a learning framework named Multi-Anchor Latent Transduction (MALT) for handling out-of-distribution (Out-of-Distribution, OOD) problems in molecular property prediction (Molecular Property Prediction, MPP). The main contributions of the paper include: (1) A model-agnostic transductive framework that can be integrated with any pre-trained molecular encoder (such as GIN); (2) A multi-anchor latent reasoning mechanism that overcomes the fragility of single-anchor methods; (3) Experimental validation on benchmarks such as MoleculeNet, DrugOOD, and Activity Cliffs, showing that MALT outperforms baselines in OOD generalization while matching or surpassing baselines on in-distribution (ID) tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The quality of the paper is high, with a clear structure and smooth logic. The authors selected diverse benchmark datasets (MoleculeNet, DrugOOD, Activity Cliffs, and Lo-Hi), covering regression and classification tasks, as well as various types of distribution shifts (such as scaffold-based covariate shifts and label shifts). The baseline selection is reasonable. The evaluation metrics (such as AUROC, MAE, RMSE) are consistent with domain standards, and comparisons of ID and OOD performance are reported."}, "weaknesses": {"value": "The novelty of the method is insufficient. Although MALT introduces a multi-anchor latent transductive mechanism and claims to overcome the limitations of previous single-anchor methods, the multi-anchor concept is not entirely original. For example, in semi-supervised learning, Halpern et al. (2016) used multiple anchors to combine features to enhance representations; in domain adaptation tasks, UniJDOT (arXiv:2503.11217, 2025) adopted multiple anchors in the feature space to align unknown samples, although applied to time series data, its dynamic anchor update and fusion mechanism is similar to MALT's multi-anchor attention fusion. Additionally, MALT is built on the basis of Bilinear Transduction (arXiv:2502.05970, 2025), which already uses single anchors in representation space for transductive extrapolation; MALT's main extension is from single-anchor to multi-anchor and operating in learned latent space, but this is more like an incremental improvement rather than a fundamental innovation. If it can be proven that the specific application in the field of molecular property prediction (such as handling activity cliffs) brings unique advantages, this will strengthen the novelty claim of the paper; otherwise, readers may view it as a combination of existing ideas rather than a pioneering framework."}, "questions": {"value": "1. The paper emphasizes that multi-anchors overcome the fragility of single-anchors, but how to ensure that the selected anchors are diverse rather than redundant? For example, in activity cliff scenarios, if multiple anchors come from similar scaffolds, will it reduce generalization?\n2. In Equation 5, is it possible to consider the labels of anchors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cVJ2atc4Pz", "forum": "rmvO5WRb58", "replyto": "rmvO5WRb58", "signatures": ["ICLR.cc/2026/Conference/Submission15016/Reviewer_mDRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15016/Reviewer_mDRD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750735188, "cdate": 1761750735188, "tmdate": 1762925345874, "mdate": 1762925345874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MALT, a multi-anchor latent transduction framework for robust molecular property prediction under out-of-distribution (OOD) conditions arising from covariate shift (novel structures) or label shift (novel property values). Given a query embedding, MALT retrieves the top-k nearest training embeddings from a memory bank $Z_{train}$. These anchors then serve as keys/values in a multi-head attention layer, producing z_attn. A learnable prediction head then takes $z_{query}$, $z_{attn}$, and optionally $W_{anchors}$ (query-anchor distances) as inputs before making its final prediction. Across multiple benchmarks, MALT shows consistent gains over the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MALT is conceptually intuitive and can be used with most molecular encoders in a plug-and-play manner by adding a retrieval and attention module.\n* Reported results show generally consistent gains on OOD scenarios in MoleculeNet, DrugOOD, and Activity Cliffs datasets."}, "weaknesses": {"value": "* Covariate-shift claim depends on the retrieval quality. Cross-attention over anchors intuitively helps only if the retrieved set is informative in OOD regimes. Using only $z_{anchors}$ for K/V (default configuiration) may not directly address covariate shift when $z_{query}$ is still far too different from what is seen in the training set. Since the model can optionally include $W_{anchors}$, an ablation isolating its effects might strengthen the claim.\n* The baselines do not cover recent large-scale foundation models (e.g., UniMol-style pretraining). Showing results against large-scale pretraining methods would better contextualize MALT.\n* There are multiple cases in the reported tables where a MALT variant underperforms its non-MALT backbone. A systematic failure analysis and a safeguard to fall back to the base model such that the prediction quality is at least as good as the backbone would improve the usability of MALT.\n* The encoder $E$ is trained end-to-end, so $E_{n}(x)$ ≠ $E_{m}(x)$ for training steps $n$ and $m$, and molecule $x$. Because $Z_{train}$ is updated only every $N$ epochs, $W_{anchors}$ may not reflect real-time latent distances, i.e., the memory bank becomes stale during training. A clarification on whether this mismatch degrades performance would be interesting.\n* The predictor uses $f(z_{query}, z_{attn})$ (optionally with $W_{anchors}$), but it does not directly use the labels $\\{y_i\\}_{i=1}^k$ of retrieved anchors during inference. A label-aware fusion/attention could be beneficial for OOD cases."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGvdw1woR3", "forum": "rmvO5WRb58", "replyto": "rmvO5WRb58", "signatures": ["ICLR.cc/2026/Conference/Submission15016/Reviewer_Psww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15016/Reviewer_Psww"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927749033, "cdate": 1761927749033, "tmdate": 1762925345410, "mdate": 1762925345410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MALT, a multi-anchor transductive learning in latent space designed for generalizability on molecular property. MALT uses a memory\nbank of embeding vectors corresponding to training samples. Given a query molecule, MALT useses multiple anchor points selector from the memory bank,\nand aggregates this information with the query to make predictions. MALT achieves improvement on multiple OOD benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- MALT hows good performance on OOD generalization. \n\n- It relies on multiple anchor points as opposed to a single anchor point, making it more robust.\n\n- Since MALT operates in latent space, it makes the method modular and can be used with any molecular encoder.\n\n- Shows utility by demonstrating performance on real world drug discovery benchmark."}, "weaknesses": {"value": "- The effectiveness of MALT relies on the quality of the chosen encoder hence if the encoder produces poor representations, the transduction merely\n  propagates this. Thus, the foundational problem of OOD generalization cant be mitigated by the method if the underlying foundation model isn't\n  robust enough.\n\n- The computation cost of maintaining a memory bank, latent embeddings for all samples in the training set, can be high. This can be prohibitive\n  compared to keeping a few anchor points or even learned anchor points."}, "questions": {"value": "- For extremely large databases, how will the memory bank approach scale? How will anchor selection work in such a case?\n\n- Does anchor selection correlate with molecular properties? And in regions where very few samples exist in the molecular space, how does the anchor\n  based approach hold up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yDvQ0Q8Jao", "forum": "rmvO5WRb58", "replyto": "rmvO5WRb58", "signatures": ["ICLR.cc/2026/Conference/Submission15016/Reviewer_mB71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15016/Reviewer_mB71"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055659334, "cdate": 1762055659334, "tmdate": 1762925344903, "mdate": 1762925344903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}