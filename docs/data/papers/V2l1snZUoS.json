{"id": "V2l1snZUoS", "number": 9220, "cdate": 1758115534487, "mdate": 1759897737166, "content": {"title": "DEX-AR: A Dynamic Explainability Method for Autoregressive Vision-Language Models", "abstract": "As Vision-Language Models (VLMs) become increasingly sophisticated and widely used, it becomes more and more crucial to understand their decision-making process. Traditional explainability methods, designed for classification tasks, struggle with modern autoregressive VLMs due to their complex token-by-token generation process and intricate interactions between visual and textual modalities. We present DEX-AR (Dynamic Explainability for AutoRegressive models), a novel explainability method designed to address these challenges by generating both per-token and sequence-level 2D heatmaps highlighting image regions crucial for the model's textual responses. The proposed method offers to interpret autoregressive VLMs—including varying importance of layers and generated tokens—by computing layer-wise gradients with respect to attention maps during the token-by-token generation process.\n    DEX-AR introduces two key innovations: a dynamic head filtering mechanism that identifies attention heads focused on visual information, and a sequence-level filtering approach that aggregates per-token explanations while distinguishing between visually-grounded and purely linguistic tokens. Our evaluation on ImageNet, VQAv2, and PascalVOC, shows  a consistent improvement in both perturbation-based metrics, using a novel normalized perplexity measure, as well as segmentation-based metrics.", "tldr": "", "keywords": ["VLMs", "heatmaps", "per-token heatmap"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/631e259eba467e4de13ead54415114f377e74229.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors are tackling the problem of sequence-level autoregressive VLM explainability, where the current XAI approaches should be extended to handle multiple outputs. The idea is to use the derivative of the LogitLens at each timestep $t$, on each of the output tokens generated one by one. Then, weighting the impact of the saliency maps according to the ratio of vision-impact with respect to text-impact, where tokens relying more on vision should be amplified more than text-based ones. This weighting approach is done both in head level granularity and token level. This yields a single saliency map of the entire VLM output."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The gap of pure explainability on autoregressive VLMs, i.e. for question-answering is indeed seems to be an issue. I also think that VLMs should be handled appropriately in terms of explainability, so it is a very important topic and at least for me it seems that it is underexplored, hence novel.\n* The results of the method seems to be better than other existing methods which they compare with, moreover they compare with several cutting-edge VLMs."}, "weaknesses": {"value": "* The convention of the citations embedded in the article is weird and super not convenient. I saw submissions with blue citations, some are still black, but I did not see missing parentheses. This make the citations blended within the flow of the sentence without clear separation. Very confusing, this must be fixed.\n* Lines 51-77 - the claim of the inability of current explainability methods to act on autoregressive VLMs is too decisive. There are a plethora of works, some of them are modality-specific ones, however there are many which aim at explaining CLIP ([1], [2], [3] and of course many more), which is obviously multimodal. I think that claim like this must be more backed with experiments. This comes with a very limited Related Work section for explaining methods dedicated for joint embeddings like CLIP (I already mention some, there are plenty more).\n* Equation 3 - it seems like the authors are not familiar with the concept of LogitLens which is widely known, it is exactly what explained in Eq. 3 ([4]).\n* I think that XAI on CLIP methods might be more relevant and efficient , so it it highly preferable to compare with.\n\n\nMinor Weaknesses:\n* Line 47 - correct spacing: language models( Zini & Awad (2022); Zhao et al. (2024a)) -> language models (Zini & Awad (2022); Zhao et al. (2024a))\n* Line 93 - incorrect sentence: We evaluate to proposed method of various downstream tasks and datasets  -> We evaluate the / our proposed method on various downstream tasks and datasets\n* Line 105: 2)We -> 2) We. \n* Line 107 3)We -> 3) We.\n* Lines 154-158 I think that there is a mistake of 1 shifted in the indexing. Say for $t=1$, which is the first token the LLM should output, then it has no other output tokens to digest, thus it process $N + T_c$, and in general: $T_t = N + T_c + t - 1$ and $T = N + T_c + T_a - 1$. \n* Line 188, it is obviously not restricted to be a word. It is better to stick to the term token here.\n* Line 207 - missing period at the end of the sentence, before the word \"Next\".  \n* Line 209: $[:, : N]$ ->  $[:, :N]$, remove redundant space.\n* Line 253, there is a difference between starting (\") and ending (\"). You used the same through all the paper. \n* Line 268: What \"resp.\" is? it seems like a mistake.\n\nrefs:\n[1] Interpreting CLIP's Image Representation via Text-Based Decomposition. Yossi Gandelsman et al. ICLR 2024.\n[2] Interpreting the Second-Order Effects of Neurons in CLIP. Yossi Gandelsman et al. ICLR 2025.\n[3] From Attention to Prediction Maps: Per-Class Gradient-Free Transformer Explanations. Ronen Schaffer et al. Preprint\n[4] Logit Lens Blog post: https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"}, "questions": {"value": "* Dynamic head filtering. Im not sure why you called it \"filtering\", since it does not filter out the impact of several heads, it is weighting them, so you might consider to denote it more accurately (Dynamic head weighting or something like that).  Regarding the method itself - I might agree with the claim that larger objects might affect more dramatically if you average across all tokens, however on the other hand relying on the maximal value is super sensitive to outliers which occur frequently in vision transformers. Specifically it has been already shown that ViTs are dedicating tokens to embed global information, where it look like outliers (present also in your examples in Fig. 3) [1].\n* I saw you have an ablation study for the filtering stage, nevertheless I think that if you want to understand what ratio of the attention is spread on the vision on top of the text, it makes more sense to normalize the gradients - say with softmax, and take only the vision values of it. Im not sure why subtracting as you do is explicitly computing the ratio between how much attention is spread on vision with respect to text. It is very specific, but it might be nice to check this if it is simple.\n* It is a general issue in VLMS, that as the output sequence get larger, the matrices (for example the attention matrix) get larger and larger. Nowadays the output of VLMS are of a large bunch of tokens, so how do you think your approach will handle in terms of timing and efficiency (and also performance) on a much larger examples?\n\n\nI think that the idea of dedicating explainability method specifically for autoregressive VLMS is super relevant and covering a very important gap, where your approach is taking a step further in this aspect. However, I think that the paper is not written fluently enough, and that your approach is currently only a small extension step of current vision XAI methods (the only innovative part currently is the filtering approach both in head and token levels, which raises quite a few concerns, as I asked and mentioned here). Moreover I didnt see any interesting examples of how your new method reveals interesting failure cases which highly relevant to the case of autoregressive VLMs and interpretability in general. I have mixed feeling about this paper, because I think that the topic is new, challenging and important, but I decided on minor negative rating because I still think that the academic gain here is too small, and the paper is written not up to top-tier standard. Nevertheless, I highly encourage you to improve it and Im sure it can be accepted to high impact conferences.\nrefs: \n\n[1] Vision Transformers Need Registers. Darcet et al. ICLR 24."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cffJVXvgKK", "forum": "V2l1snZUoS", "replyto": "V2l1snZUoS", "signatures": ["ICLR.cc/2026/Conference/Submission9220/Reviewer_A3ah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9220/Reviewer_A3ah"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761057812289, "cdate": 1761057812289, "tmdate": 1762920880467, "mdate": 1762920880467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The large scale use of VLMs in daily lives make the process to arrive at its decision critical. Issues usually in auto regressive models are the token by token generation and this doesn’t help if the modalities are dual (VLM). Authors present  DEX-AR (Dynamic Explainability for AutoRegressive models), a novel explainability method designed to address these challenges by generating both per-token and sequence-level 2D heatmaps highlighting image regions crucial for the model’s textual responses. DEXAR offers to interpret autoregressive VLMs—including varying importance of layers and generated tokens—by computing layer-wise gradients with respect to attention maps during the token-by-token generation process. The method involves two key innovations: a dynamic head filtering mechanism that identifies attention heads focused on visual information, and a sequence-level filtering approach that aggregates per-token explanations while distinguishing between visually-grounded and pure linguistic tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)The paper addresses a critical gap in explainability for autoregressive VLMs by providing per-token explanations during sequential generation. This is particularly valuable given the widespread deployment of VLMs where understanding decision-making processes is crucial for trust and debugging.\n2)Dynamic Head Filtering: The attention head filtering mechanism that identifies heads focused on visual information represents a meaningful contribution to understanding cross-modal attention patterns.\n3)Multi-level Analysis: The dual approach of per-token and sequence-level explanations provides comprehensive insight into both local and global decision-making processes.\n4)Layer-wise Gradient Analysis: Leveraging gradients with respect to attention maps offers a principled approach to attribution that respects the model's internal computations.\n5)Excellent experiment results \nThe experimental evaluation demonstrates robust performance across multiple dimensions:\n(i)Perturbation Analysis\n(ii)Cross-Architecture Validation\n(iii)Computational Efficiency\n(iv)Segmentation Performance Excellence\n6)Thorough Ablation Studies"}, "weaknesses": {"value": "1. Clarity Issues\nThe paper suffers from several theoretical gaps that undermine the rigor of the proposed method. Most critically, the intermediate logits computation in Section 3.2 lacks clear justification for why o^{l,t} should be conditioned only on the last generated token. While this conditioning may stem from the autoregressive structure, the authors fail to explicitly explain how causal masking affects this choice, why this specific conditioning is optimal for attribution, or whether alternative conditioning strategies were considered and their associated trade-offs. Furthermore, the transition from per-token computations to sequence-level aggregation requires stronger theoretical grounding, particularly regarding how information flows through the autoregressive generation process and how this affects the attribution quality.\n2. Methodological Concerns\nThe claimed \"dynamic filtering mechanism\" (L228) appears to be primarily a weighting scheme rather than true dynamic filtering, raising significant methodological concerns. From a computational efficiency perspective, this weighting mechanism may be substantially more expensive than simpler threshold-based pruning of non-contributing attention layers, yet no comparison is provided with such alternatives that could achieve similar results more efficiently. The terminology \"dynamic filtering\" may be misleading when describing what is essentially attention re-weighting, suggesting a need for more precise naming and clearer distinction between the proposed approach and existing attention manipulation techniques.\n3. Experimental and Design Limitations\nThe assumption underlying sequence-level filtering—that filler and grammatical words are less important for visual grounding—is problematic and potentially limits the method's applicability. In scenarios involving similar objects (e.g., \"two apples on a table\"), grammatical words and spatial prepositions become crucial for accurate localization, contradicting this assumption. The examples in Figure 1 focus on distinct objects (\"cat and dog\") which may not represent the full complexity of visual-linguistic grounding tasks where subtle linguistic cues matter significantly. Additionally, the choice of Signal-to-Noise Ratio (SNR) for filtering lacks both theoretical justification and empirical validation against alternative metrics.\nWhile the perturbation and segmentation experiments are comprehensive and demonstrate strong results, the evaluation could benefit from additional explainability-specific metrics to strengthen the claims. More systematic faithfulness assessment beyond the current perturbation analysis would better validate whether explanations truly reflect the model's decision process. The evaluation would also benefit from completeness analysis to determine whether explanations capture all relevant visual information used by the model, and robustness testing to assess explanation stability under minor input variations or model parameter changes.\n4. Technical Implementation Gaps\nSeveral scalability concerns remain unaddressed in the technical implementation. The layer-wise gradient computation for each token may not scale well to longer sequences, potentially limiting practical applicability. Memory requirements for storing attention maps across all layers and tokens are not discussed, raising questions about the method's feasibility for resource-constrained environments. Real-time applicability for interactive systems remains unclear, particularly given the computational overhead of the gradient computations and attention map storage requirements."}, "questions": {"value": "1)Could the authors provide justification for the intermediate logits conditioning scheme in Section 3.2 ? \n2)Justify the design choice of dynamic/reweighting filtering\n3)Explain why grammatical words are deemed unimportant and provide evidence supporting SNR-based filtering\n4)Include analysis of computational overhead and comparison with simpler alternatives\n5) Could such metrics faithfulness, completeness be checked with  perturbation-based validation metrics to demonstrate the causal relationship between explanations and model decisions ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kx2M3d086E", "forum": "V2l1snZUoS", "replyto": "V2l1snZUoS", "signatures": ["ICLR.cc/2026/Conference/Submission9220/Reviewer_M5cf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9220/Reviewer_M5cf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933083110, "cdate": 1761933083110, "tmdate": 1762920879843, "mdate": 1762920879843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DEX-AR (Dynamic Explainability for AutoRegressive models), an explainability method for VLMs to evaluate the vision-answer token relevance.  The key contributions include: Dynamic Head Filtering, highlights attention heads that focus on visual information, filtering out irrelevant ones. Sequence-Level Filtering: further filters answer-sequence-level irrelevant noises, distinguishing between visually-grounded and other answer tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear presentation and well writing.\n\n2. Good generalizability: Demonstrates consistent performance improvements across multiple VLM architectures, including \\textbf{decoder-only}, \\textbf{encoder-decoder}. Outperforms baselines on both perturbation and segmentation tasks.\n\n3. Comprehensive evaluation: Provides thorough analysis using diverse metrics like normalized perplexity, insertion/deletion tests, and segmentation IoU scores."}, "weaknesses": {"value": "1. This paper shares similarities with TAM[1], which uses forward logits and causal inference to assess correlations between visual inputs, prompt texts, and answer sequence. The key distinction is the use of gradient evaluations across layers and heads. This paper should systematically compare the two approaches on: algorithm complexity; \\textbf{technical differences and advantages}; test both methods on difficult scenarios (e.g., multi-object scenes, occlusions, ambiguous prompts). \n\n2. Gradient-based methods inherently require significant computational resources, especially for large transformer architectures.\n\n\n[1] Token Activation Map to Visually Explain Multimodal LLMs"}, "questions": {"value": "If possible,\n\n1, Show the necessary and effectiveness of filtering operations on different layers and heads;\n\n2. Demonstrate methods to address the biases revealed in failure cases (e.g., over-reliance on background features or spurious correlations).\n\n3. Qualitative and quantitative evaluation on hard cases (true useful for the community), for example, multiple similar objects in one picture,  occlusions and interactions among objects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8L9AYbMhY1", "forum": "V2l1snZUoS", "replyto": "V2l1snZUoS", "signatures": ["ICLR.cc/2026/Conference/Submission9220/Reviewer_CpcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9220/Reviewer_CpcH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976475690, "cdate": 1761976475690, "tmdate": 1762920879453, "mdate": 1762920879453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}