{"id": "yAC56BJXmE", "number": 11531, "cdate": 1758201001107, "mdate": 1759897569800, "content": {"title": "CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill", "abstract": "The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying significantly between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by aggregating attention from generated answers back to the prompt. Using this oracle, we find that existing heuristics (GemFilter and FastKV) exhibit substantial instability across layers, motivating our proposed heuristic of Cross-Layer Attention Aggregation (CLAA). CLAA robustly aggregates token scores across multiple consecutive layers, significantly improving ranking stability and achieving a new state-of-the-art performance. On LongBench, CLAA reduces Time-to-First-Token (TTFT) by up to 39\\% compared to the Full KV Cache baseline. At a similar level of task accuracy, CLAA provides this speedup while being over 10\\% faster than the prior state-of-the-art, FastKV, demonstrating a superior accuracy-speed tradeoff.", "tldr": "We use an answer-informed oracle to show that cross layer attention aggregation (CLAA) produces more stable token rankings for accelerating LLM prefill on long contexts.", "keywords": ["LLM inference acceleration", "prefill optimization", "token ranking", "attention aggregation", "long-context processing", "KV cache compression", "oracle"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67c81817863a5987d5297d155550b13032e3f37c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the prefill bottleneck in long-context LLM inference by proposing a novel token ranking heuristic called Cross-Layer Attention Aggregation (CLAA). The authors introduce an Answer-Informed Oracle that establishes ground-truth token importance by aggregating attention from generated answers back to the prompt. Using this oracle, they demonstrate that existing methods like GemFilter and FastKV suffer from unstable rankings across layers. CLAA addresses this by aggregating token scores across multiple consecutive layers, achieving up to 39% reduction in TTFT while maintaining accuracy. The method is evaluated on LongBench, Needle-in-a-Haystack, and RULER benchmarks using Llama-3.2-3B, Llama-3.1-8B, and Mistral-Nemo-12B models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel and principled evaluation framework: The Answer-Informed Oracle is a clever contribution that provides a ground-truth way to evaluate token ranking quality independent of architectural differences. The idea of using attention from the actual generated answer to score prompt tokens is intuitive and well-motivated. This makes it much easier to compare different heuristics fairly, which has been a real problem in this area.\n- Strong empirical results with comprehensive evaluation: The experiments are pretty thorough - covering multiple benchmarks (LongBench, Needle-in-a-Haystack, RULER) and showing consistent improvements. The accuracy-speed tradeoff in Figure 4 clearly shows CLAA tracking the oracle much more closely than other methods.\n- Well-motivated method design: The layer-wise analysis in Figure 2 really drives home why cross-layer aggregation makes sense. Seeing those sharp correlation drops at specific layers for single-layer methods makes the instability problem very concrete. The design choices of deferring compression for the first 4 layers and max aggregation across layers follow naturally from this analysis.\n- Good ablation studies: The ablations on the pruning layer (Figure 6 left) and aggregation window size (Figure 6 right) help understand when and why CLAA works. \n- Paper writing is good and reading-friendly."}, "weaknesses": {"value": "- Limited model coverage in experiments: The paper only evaluates on Llama-3.2-3B, Llama-3.1-8B, and Mistral-Nemo-12B. It would be more convincing to see results on other model families like Qwen3 or larger models (e.g., 30B+ scale). Different architectures might have different attention patterns, and larger models might show different layer-wise stability characteristics. The current evaluation leaves some doubt about whether these findings generalize broadly across the model landscape.\n- Missing discussion on failure cases and limitations: The paper doesn't really dig into when CLAA might struggle or perform poorly. Are there specific task types or prompt characteristics where cross-layer aggregation doesn't help? The Multi-News summarization results in Figure 2 show relatively flat correlations for all methods - what's happening there? Understanding failure modes would make the contribution more complete."}, "questions": {"value": "NA. Overall, I like the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DQLsovWute", "forum": "yAC56BJXmE", "replyto": "yAC56BJXmE", "signatures": ["ICLR.cc/2026/Conference/Submission11531/Reviewer_a9cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11531/Reviewer_a9cR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342512340, "cdate": 1761342512340, "tmdate": 1762922626641, "mdate": 1762922626641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author first proposes a new way of defining token importance as an Oracle metrics to understand the quality of different token importance estimation techniques, whose results will be used to skip prefill computations and KV cache. Then the author introduces a method that aggregates attention scores across the first several layers, which they show to be more aligned with the Oracle they proposed. This method, like prior works, successfully reduces the TTFT while ensuring enough quality to downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The author proposes a principle way of checking the validity of a token estimation method, upon which they design a new way of aggregating the attention mechanism to conduct token dropping. \n2. The work compares many important baselines both from a quality and efficiency perspective. \n3. LongBench, RULER, and Niah are used for long context evaluations, as well as both prefill TTFT and KV cache / memory profiling, which is very comprehensive."}, "weaknesses": {"value": "1. Prior works have shown that token pruning can become less effective for shorter, standard tasks, which should be included for completeness. \n2. The model focuses on the 8B model, and should ideally include other sizes to further prove its practicality. \n3. The method seems more like an incremental approach from prior works that use attention as the main metrics for token estimation, the main novel part is how to aggregate the attention scores more robustly compared to GemFilter.  \n4. RULER results should be better with spec prefill. \n5. For Speculative Prefill, the draft model needs to be large enough as tested in the original paper to have enough estimation capacity for preserving quality. Also, in Figure 5, the TPS as well as prefill TTFT seem too bad compared to what the paper mentions (probably due to not using their vllm implementation?) The reviewer recommends only mentioning CLAAâ€™s superiority under the specific setting with 8B and non-original implementation for fairness. \n6. Like all prior works, it will be great to mention if this method can handle multi-turn setting where token skipping methods could potentially fall short. \n7. The method requires calculating q*K^T for each q in the prompt and averaged over the observational window, which is not natively outputted for SOTA attention kernels such as flash and flex attention. In order for this to be maximally performant, it will be better to mention this as a potential future work. \n8. In Figure2, it is not obvious that CLAA is better than FastKV though."}, "questions": {"value": "Listed above in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Wt3qR1Mw4", "forum": "yAC56BJXmE", "replyto": "yAC56BJXmE", "signatures": ["ICLR.cc/2026/Conference/Submission11531/Reviewer_Ke2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11531/Reviewer_Ke2f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643522260, "cdate": 1761643522260, "tmdate": 1762922626187, "mdate": 1762922626187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLAA (Cross-Layer Attention Aggregation), a training-free token ranking heuristic designed to accelerate LLM prefill by selectively processing only the most important prompt tokens. The authors first propose an Answer-Informed Oracle that establishes ground-truth token importance by aggregating attention from generated answers back to the prompt. Using this oracle, they demonstrate that existing single-layer ranking methods (GemFilter, FastKV) exhibit significant instability across layers. CLAA addresses this by aggregating token importance scores across multiple consecutive layers using maximum aggregation, deferring KV cache compression for the first 4 layers to preserve foundational representations. Experiments on LongBench, Needle-in-a-Haystack, and RULER benchmarks show that CLAA achieves up to 39% TTFT reduction compared to full KV cache while maintaining accuracy closer to the oracle baseline than existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and empirically demonstrates that existing single-layer methods suffer from significant layer-wise instability (Figure 2), which is a common limitation that impacts their effectiveness. The Answer-Informed Oracle is an important contribution that enables principled comparison of token ranking heuristics independently from architectural details. This could become a valuable tool for future research.\n\n2. CLAA's design is straightforward - maximum aggregation across layers with delayed compression - making it easy to understand, implement, and integrate into existing systems.\n\n3. CLAA demonstrates consistent improvements over baselines across multiple benchmarks (LongBench, Needle-in-a-Haystack, RULER) and model scales, with particularly strong performance on retrieval and multi-hop reasoning tasks. Achieving 39% TTFT reduction while maintaining accuracy close to the oracle represents meaningful benefits."}, "weaknesses": {"value": "1. The paper is missing critical comparisons with several recent and relevant methods. Specifically, the following papers also focus on improving TTFT but are not shown in the baselines:\n* Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers\n* Compressing Context to Enhance Inference Efficiency of Large Language Models\n* LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\n\n2. The core technical contribution (max aggregation across layers) is relatively incremental. While effective, it's a straightforward engineering solution rather than a fundamental algorithmic innovation. The paper would benefit from deeper theoretical analysis of why cross-layer aggregation stabilizes rankings.\n\n3. Missing real-world deployment analysis, including:\n* No analysis of memory bandwidth implications when accessing KV caches across multiple layers for scoring\n* No comparison of actual wall-clock time including the overhead of cross-layer aggregation computation\n* Missing analysis of compatibility with optimized attention implementations (FlashAttention, which is critical for deployment)"}, "questions": {"value": "1. Can the authors provide comparisons with the baselines mentioned in the weaknesses? These are important recent methods that should be included to properly establish state-of-the-art performance.\n\n2. How does CLAA interact with FlashAttention and other optimized attention implementations? Does the need to access attention scores across multiple layers prevent using these optimizations, potentially negating efficiency gains?\n\n3. While ablations are provided, the method has several hyperparameters (n=4 layers, m=4 defer layers, W=8 window, kernel=7 pooling) that may need tuning for different models or tasks. Can you give the ablation of the parameters, and also analyze the sensitivity of performance from them?\n\n4. Can you provide theoretical analysis or intuition for why maximum aggregation across layers specifically stabilizes rankings? What are the properties of attention distributions across layers that make this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3KA2f8Jvs1", "forum": "yAC56BJXmE", "replyto": "yAC56BJXmE", "signatures": ["ICLR.cc/2026/Conference/Submission11531/Reviewer_iFDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11531/Reviewer_iFDh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977957053, "cdate": 1761977957053, "tmdate": 1762922625318, "mdate": 1762922625318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}