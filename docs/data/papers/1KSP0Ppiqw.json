{"id": "1KSP0Ppiqw", "number": 14611, "cdate": 1758239972340, "mdate": 1759897359516, "content": {"title": "A recipe for scalable attention-based ML Potentials: unlocking long-range accuracy with all-to-all node attention", "abstract": "Machine-learning interatomic potentials (MLIPs) have advanced rapidly, with many top models relying on strong physics-based inductive bias, including rotational equivariance, high-order directional features, and energy conservation. However, as these MLIP models are being trained and evaluated on larger and larger systems, such as biomolecules and electrolytes, it is increasingly clear that solutions are needed for scalable and accurate approaches to long-range (LR) interactions in large systems. The most common approaches in literature to address long-range interactions rely on adding explicit physics-based inductive biases into the model. In this work, we propose a conceptually straightforward, data-driven, attention-based, and energy conserving MLIP, AllScAIP, that addresses long-range interactions\nand scales to O(100 million) training set sizes: a stack of local neighborhood self-attention followed by all-to-all node attention for global interactions across an entire atomistic system. Extensive ablations across model and dataset scales reveal a consistent picture: in low-data/small-model regimes, inductive biases help with improving some sample efficiency, and the all-to-all node attention increases LR accuracy. As data and parameters scale, the marginal benefit of these inductive biases diminishes (and can even reverse), while the all-to-all node attention remains the most durable ingredient for learning LR interactions. Our model achieves state-of-the-art on both energy/force accuracy and relevant physics-based evaluations on a representative molecular dataset (OMol25), while being competitive on materials (OMat24), and catalyst (OC20) datasets.", "tldr": "", "keywords": ["machine learning potentials;AI for Science"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40852a3941da56f8c3ed39c24a29db2faee8c781.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper targets a critical challenge in machine-learning interatomic potentials (MLIPs): existing scalable models struggle with long-range (LR) interactions in large systems, while methods adding explicit physics-based inductive biases lack scalability for large heterogeneous datasets. To solve this, the authors propose AllScAIP, a data-driven, energy-conserving attention-based MLIP that scales to 100 million training samples and excels at LR interactions. Experiments were conducted on three datasets: OMol25, OMat24, and OC20. On OMol25, AllScAIP achieves state-of-the-art energy/force accuracy. The attention mechanism lack novelty and the experiments and compared models are insufficient. There are several severe concerns should be addressed."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The study delivers a valuable empirical finding: inductive biases (LAE, ERoPE) are context-dependent, not universally beneficial.\n2. The study tries to address the LR interaction gap in MLIPs through a conceptually simple yet effective design, avoiding the locality constraints of cutoff-based message-passing models."}, "weaknesses": {"value": "1. Scalability Limits for Large Atomic Systems​\nAllScAIP’s all-to-all node attention introduces O(N²) computational complexity, creating a bottleneck for large atomic systems. While it handles 510-atom systems (SAMD23) adequately, its throughput degrades as ~1/N once N exceeds a threshold (dotted line in Figure 4), making it impractical for million-atom biomolecules (e.g., proteins) or bulk materials—critical use cases in drug discovery and materials science. The paper mentions potential fixes (tiled attention, linear-time attention) but provides no experimental validation of these optimizations.\n2. Insufficient Experiments\nOMol25, OMat24, and OC20 are very similiar. The authors should conduct experiments on AIMD-Chig (Sci Data, 2023), MD22 (Sci Adv, 2022) and Protein Unit dataset (Nature, 2024) to examine the effectiveness for molecules, especially biomolecules.\n3. Lack Novelty\nThe design of all node attention is too simple and lack novelty. \n4. Lack long-range effect validation\nThe author should seriously examine the performance gains of their design on long-range effects, such as polarization effect-domint system\n5. Lack simulations\nLow MAE of energy and force does NOT mean robust simulations. The authors should perform simulations to examine the usefulness of their MLIP, especially in long-range effect dominant systems.\n6. Lack LR interaction interpretability\nAllScAIP attributes LR accuracy to all-to-all attention but provides no visualizations."}, "questions": {"value": "The authors should seriously address the concerns shown in Weakness point by point to improve the quality of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PbLUBDqrYy", "forum": "1KSP0Ppiqw", "replyto": "1KSP0Ppiqw", "signatures": ["ICLR.cc/2026/Conference/Submission14611/Reviewer_EXix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14611/Reviewer_EXix"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660514761, "cdate": 1761660514761, "tmdate": 1762924992921, "mdate": 1762924992921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Long-range interactions have a primary importance in the modeling of many materials and finite systems (such as proteins), and thus it is necessary to incorporate them into Machine Learning Interatomic Potentials (MLIPs). Many current approaches explicitly incorporate physical priors, e.g., by predicting local charges and then evaluating the Coulomb terms. \n\nThe reviewed paper argues that at scale these physical priors are less important, and introduces a flexible all-to-all attention mechanism to capture long-range interactions. Empirical evaluations confirm that the model, AllScAIP, is capable of successfully learn long-range interactions given a sufficient amount of data. The model achieves state-of-the-art accuracy on the OMol25 dataset, and performs well on the OMat24 and OC20 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The question studied by the paper—whether a flexible model can efficiently learn long-range interactions without incorporating physical priors —is timely and relevant to the current landscape of MLIP development, in light of previous studies on the relevance of, e.g., enforced rotational symmetry or conservativeness of the force fields, which have significantly moved the field forward. \n\nThe model proposed in the paper illustrates competitive performance on OMol25, OMat24, and OC20 datasets. Many diverse ablations are designed to systematically evaluate the effect of model size, physical priors, and the presence of all-to-all node attention. \n\nThe findings that one needs to prioritize scalable, expressive components to construct more efficient models at scale, when a sufficient amount of training data is available, are insightful and will likely further guide the development of more efficient MLIPs."}, "weaknesses": {"value": "While very explicitly acknowledged, the main weakness of the paper is still the quadratic, O(N^2), computational complexity of the model with respect to the number of atoms in the system. Sometimes, there is a need to simulate very large systems, with tens of thousands, or millions of atoms, and the proposed model is not very suitable for this purpose. \n\nAnother thing is that the status of extensivity constraints - PBC supercell doubling and Vacuum duplications is not fully clear. The paper indicates that these constraints are satisfied exactly (e.g., in 3.4), but if so, then what is the purpose of Table 2?"}, "questions": {"value": "Is the quadratic scaling of the model inevitable? Is it possible to somehow achieve the same expressivity in O(N log N) time by Fast Fourier Transform? \n\nIs the model rigorously extensive? In other words, is it correct that if, e.g., changing the data type from float32 (which, I assume, is currently used) to float64, then the numbers in table 2 would drop significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IpMAP6NTxb", "forum": "1KSP0Ppiqw", "replyto": "1KSP0Ppiqw", "signatures": ["ICLR.cc/2026/Conference/Submission14611/Reviewer_jgxN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14611/Reviewer_jgxN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988150532, "cdate": 1761988150532, "tmdate": 1762924992473, "mdate": 1762924992473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the AllScAIP model, which achieves state-of-the-art results on the OMol25 leaderboard. Additionally, scaling experiments on AllScAIP with respect to both data and model size on OMol25 are conducted. A key conclusion drawn from these experiments is that geometric encodings such as directional (LAE) and radial (ERoPE) provide benefits in small-scale settings, but their marginal utility diminishes to zero or even becomes negative in large-scale settings; on the other hand, the long-range modeling capability enabled by global node attention consistently leads to improvements across all scales. This finding offers a potential avenue for further exploration in the field of scaling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces AllScAIP, which achieves state-of-the-art performance on the OMol25 benchmark and delivers competitive results on OMat24 and OC20.\n2. Ablation experiments on OMol25 are conducted, exploring the effects of data size and model scale, and demonstrating how the utility of inductive biases varies with scale."}, "weaknesses": {"value": "1. The model configuration and training parameter settings have not been disclosed in the paper, leaving the specific details and sources of performance unclear.\n2. The generalizability of the conclusions is questionable. The ablation experiments are based on the OMol25 molecular dataset, with no exploration of other dataset types, such as OMat24 or OMC25."}, "questions": {"value": "1. How does the differentiable kNN graph differ from traditional kNN graph construction algorithms? Could you explain how it is constructed?\n2. In Table 1, the ablation experiments are only validated on the OMol25 dataset, which may not be representative. Have you conducted ablations on other datasets (e.g., OMat24, OMC25)? Do the conclusions still hold?\n3. In Table 3, eSEN-md-d generally performs better in terms of force prediction compared to AllScAIP. Could this be due to AllScAIP focuses more on energy during training?\n4. If methods such as Latent Ewald Summation (LES) or Ewald were used to replace all-to-all node attention, would there be any changes in the model’s accuracy or scalability?\n5. This paper attempts to explore the scaling behavior of machine learning interatomic potentials (MLIPs). The comparison on OC20 and Omat is biased, because the datasets are not properly aligned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rutnPvpYHP", "forum": "1KSP0Ppiqw", "replyto": "1KSP0Ppiqw", "signatures": ["ICLR.cc/2026/Conference/Submission14611/Reviewer_nZ5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14611/Reviewer_nZ5V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993187914, "cdate": 1761993187914, "tmdate": 1762924992072, "mdate": 1762924992072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces AllSCAIP, a machine-learning interatomic potential (MLIP) that combines local neighborhood self-attention with an all-to-all node self-attention mechanism to capture long-range interactions. The central thesis is that, given sufficiently large data and model capacity, many inductive biases (e.g., rotational equivariance) can be learned, and that architectures providing a global receptive field (via all-to-all node self-attention) are the most robust way to capture long-range effects. The model attains state-of-the-art or competitive performance across large-scale benchmarks in molecules (OMol25), crystalline materials (OMat24), and catalysis (OC20)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Addresses a key limitation of many MLIPs, long-range interaction modeling, via a simple, general architectural choice.\n\n2.  Strong empirical results on three diverse, large-scale benchmarks, including OC20 where long-range effects matter.\n\n3.  The paper presents thorough ablation studies (Table 1) dissecting the contributions of geometric encodings (LAE, EROPE) and the global attention module (NodeAtt), and further evaluates performance across data scale and model size.\n\n4. In the distance-scaling evaluation (Figure 6), AllSCAIP maintains stable errors under molecular stretching, whereas baselines such as eSEN and UMA deteriorate sharply. This qualitative and quantitative advantage on LR-sensitive regimes addresses a known weakness of many MLIPs"}, "weaknesses": {"value": "- Computational scalability vs. global attention bottleneck: While AllSCAIP achieves strong accuracy with a clean implementation, its reliance on all-to-all node self-attention introduces O(N^2) time and memory complexity. This conflicts with the paper’s “scalability” positioning. \n\n\n- Limited architectural novelty: The core components—global attention, local attention, angular encodings (LAE), and radial encodings (ERoPE)—have prior art in Graph Transformers and MLIPs. Although the authors brand the approach as a “prior-light scalable recipe,” the main contribution is an integration of existing modules combined with data/model scaling.\n\n- The central claim is that long-range effects are learnable at scale and that all-to-all global attention is the most durable ingredient for capturing them. To substantiate this, comparisons should include methods that explicitly encode long-range physics or global receptive fields, not only local message-passing baselines like eSEN.\n\n- Materials benchmarks and SOTA comparisons are not standardized: On OMat24 and OC20, the presented results are limited and sometimes not directly comparable to public leaderboards or standard evaluation protocols. For example, the reported OMat24 energy error does not align with the MatBench Discovery settings and omits key metrics (e.g., F1 score),"}, "questions": {"value": "- Can you provide detailed scaling curves (time, memory, throughput) versus number of atoms N, separated by component (neighbor building, local attention, global attention, readout)? Please include wall-clock on standardized hardware and batch sizes.\n  - What is the maximum N you can handle at training and inference under 24/48 GB GPU memory, and how does this compare to strong neighbor-based baselines?\n  - Have you tested sparse/low-rank or blockwise global attention (e.g., Nyström, Performer, clustered attention) to mitigate O(N^2) costs? \n\n  - Which design elements are unique relative to prior Graph Transformers/MLIPs (e.g., GemNet-OC, Equiformer-v2, Allegro/MACE variants, SpookyNet/SchNet++)? Please clarify what is new beyond module composition and scaling.\n\n  - Which long-range–aware baselines did you include (e.g., explicit electrostatics/dispersion terms, PME/FMM-augmented MLIPs, equivariant transformers with global mixing, SchNet++/SpookyNet, Equiformer-v2 LR extensions)? \n\n  - Do you evaluate on charged systems, dipole/polarizability predictions, dielectric responses, or stretched/fragmented configurations ?\n\n  - For OMat24 and OC20, can you align with standard splits and metrics used by public leaderboards (e.g., Matbench Discovery, OC20 official metrics)? Please report full metric suites (including F1 for classification tasks) with confidence intervals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pK4qCEjcQg", "forum": "1KSP0Ppiqw", "replyto": "1KSP0Ppiqw", "signatures": ["ICLR.cc/2026/Conference/Submission14611/Reviewer_7cGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14611/Reviewer_7cGG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006234315, "cdate": 1762006234315, "tmdate": 1762924991749, "mdate": 1762924991749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}