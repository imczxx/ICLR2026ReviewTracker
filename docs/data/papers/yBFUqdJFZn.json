{"id": "yBFUqdJFZn", "number": 1362, "cdate": 1756875651583, "mdate": 1763558423736, "content": {"title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data", "abstract": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms.  Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research.", "tldr": "", "keywords": ["GUI Agent", "GUI Data Pipeline", "Computer Use", "Open Source"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2b3f215e3f56296960054d821d0de8d9ce24ac8.pdf", "supplementary_material": "/attachment/642010d0d68c1c2e2efc7d387db62b50bc412e26.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a dataset and agent models for computer agents. Provides a rich source of open source computer use data that is curated across various platform to fine tune the models. They show that fine tuning with this data curated across various platforms creates a strong model that performs well across various computer use benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The comprehensive data set and model that the paper proposes have the potential to further the research in this field.\n2. Experimental results showing the overall approach and how well it generalizes across various tasks. \n3. Method presented for data curation, which can be further extended to create more data assets"}, "weaknesses": {"value": "no comment"}, "questions": {"value": "Is the fin-tuning used SFT in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3XTyUGoC3L", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Reviewer_56qp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Reviewer_56qp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932535899, "cdate": 1761932535899, "tmdate": 1762915748276, "mdate": 1762915748276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a cross-platform computer use dataset, collected via an interactive data pipeline that\nintegrates automated agents with human experts. They also developed ScaleCUA, a family of base agent models achieving reported SOTA on several GUI-oriented benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The model achieves impressive results on several challenging benchmarks, including SOTA performance on MMBench-GUI L1-Hard (94.4%).\n- The paper presents a comprehensive suite of ablation studies exploring design choices, including data augmentation, trajectory weighting, coordinate formats, resolution impact, inference methods, and data scaling.\n- The dataset spans six OSs and multiple GUI domains, broader than previous open datasets (e.g., OSWorld, JEDI, WebArena). Openly releasing all data, models, and code is a significant contribution, particularly within this research area, where datasets are often proprietary."}, "weaknesses": {"value": "No major weaknesses, some suggestions:\n- The paper contains several minor typos and grammatical errors (e.g., \"expensively to annotate,\" capitalization issues) and inconsistencies in terminology. \n- They can add an analysis of the data quality, comparing the current approach and manual collection."}, "questions": {"value": "How does the model perform when faced with minor software changes (e.g., UI updates, version differences)? Is there a mechanism to adapt to these types of alterations without requiring retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TquCl48p6b", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Reviewer_TkSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Reviewer_TkSU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975196202, "cdate": 1761975196202, "tmdate": 1762915748157, "mdate": 1762915748157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We appreciate the reviewers' recognition of our work and their constructive feedback."}, "comment": {"value": "# We appreciate the reviewers' recognition of our work and their constructive feedback.\nWe sincerely thank all reviewers for their thoughtful and constructive feedback on our submission. The positive recognition of our work is encouraging, including **comprehensive experiments**, **interesting key insights**, **well written**, **potential to further the research**, etc.\n\nWe aim to address reviewers' concerns, which will further make the work more rigorous and complete. In the following, we respond to each comment in detail and revise the manuscript accordingly to improve both the technical soundness and clarity of our contributions.\n\nAccording to reviews, the submission is also updated with the following changes:\n1. **Human Expert Data Collection Details:** In Section 3.1, additional details have been provided regarding the data collection process involving human experts. Furthermore, the discussion in Section 3.2 now highlights the contributions of the data pipeline, particularly addressing concerns regarding the hybrid nature of data collection (human vs. autonomous agent).\n2. **Revised Structure of Appendix:** The section on \"Ablation on Data\" (previously in Appendix A.2) has been moved to the main text (Section 5.3). This addresses the reviewer's concern about data efficacy, weak semantic trajectories, and data augmentation by providing more visibility and depth on these issues.\n3. **Clarification on Experimental Analysis:** Based on reviewer suggestions, Section 5.2 has been revised to clarify aspects of the experimental analysis. This includes further elaboration on the impact of input resolution, different inference modes (Grounding, Direct Action, Reasoned Action), and the effects of data scaling, addressing concerns about the trade-offs in these factors.\n4. **Expanded Comparison of Methods:** Appendix A.2 (formerly A.3) now includes a more detailed comparison of methods, incorporating new benchmarks and reorganizing tables to facilitate a clearer and more comprehensive comparison. This change responds to the reviewer’s request for a more comprehensive comparison.\n5. **Proofreading:** We have thoroughly revised our paper to address grammar, typographical errors, and other writing issues.\n\nWe welcome continued discussion and feedback from the reviewers to help us further improve our work."}}, "id": "CoPc51GCoj", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763558447280, "cdate": 1763558447280, "tmdate": 1763558447280, "mdate": 1763558447280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ScaleCUA, a large-scale dataset with multiple OS and tasks supported and associated agents trained on the dataset. It begins by incorporating a dual-loop data collection pipeline consisting of agent-environment interaction loop and agent-human hybrid data acquisition loop. The authors then train Qwen2.5-VL on the collected dataset with three modes: grounding mode, direct action mode and reasoned action mode. Experiments are conducted on multiple datasets, including screenspot-pro, webarena-lite-v2, mmbench-gui, os-world-g etc. These evaluations cover multiple tasks and demonstrate a comprehensive & promising results for ScaleCUA. ScaleCUA shows the potential to scale up CUA performance with cross-platform data collected from automated agent data collection and human-in-the-loop data collection together."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ScaleCUA contributes a large-scale dataset to the GUI Agent community, with 15k weak-semantic trajectories from automated agent data collection and 4k expert-curated trajectories from human-in-the-loop data collection. Notably, the collected data cover 6 OS, including desktop and mobile ones. \n2. ScaleCUA contributes several interesting key insights to the community backed with supporting experiment results, including the finding that models trained with raw coordinates perform better than those trained with normalized coordinates, especially in cross-platform scenarios; reasoned action mode is  always better than direct action mode, the multi-format GUI grounding corpus, etc.\n3. This paper is well written, clearly structured, and provides comprehensive details and ablation studies in the appendix for future research."}, "weaknesses": {"value": "1. For the dual loop, currently it includes two components: agent-environment interaction loop and agent-human hybrid data acquisition loop, and these two components both contribute to the final data collection of ScaleCUA, with different properties, i.e., data collected from agent automated pipeline are of large scale but weak semantics, data collected from human-in-the-loop pipeline are of high quality but high cost. However, ScaleCUA did not comprehensively integrate these two components so that they can benefit each other. This is not a major drawback, but a point that the authors can improve further. The proposed dual loop is promising, but the current implementation of simply merging the two data sources is naive, which has been discussed in prior work。\n2. The dataset distribution is extreme, e.g., MacOS data takes only 0.6% of the total data, and grounding takes 97.6% of the total Windows data. As a cross-platform dataset with multi-task support, this distribution is not reasonable enough."}, "questions": {"value": "1. for Figure 5(a), I observed that the grounding accuracy keeps almost unchanged or dropping when the resolution is increased from 720p to 4k in screenspot-v2, also the grounding accuracy saturates at 1080p in osworld-g. Do these results demonstrate that \"Grounding accuracy rises steadily from 720p to 2K, with diminishing returns at 4K.\" is not a universal finding, but only occuring in screenspot-pro benchmark?\n2. It can be observed that scaling data has marginal effects on improving agent performance on WindowsAgentArena benchmark,  does this implicate that increasing train data amount cannot benefit navigation tasks? Can you also share the performance on OSWorld with different training data ratio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "orfBxffpNR", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Reviewer_ZN1J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Reviewer_ZN1J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762269590249, "cdate": 1762269590249, "tmdate": 1762915748027, "mdate": 1762915748027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of building computer use agents (CUAs)—AI systems that can autonomously operate GUIs across desktop, mobile, and web platforms. Current progress is limited by the scarcity of high-quality, open computer-use data and the reliance on closed-source models.\n\nTo address this, the authors introduce ScaleCUA, a framework that combines a large-scale cross-platform dataset with a family of open-source agent models. Their approach integrates automated exploration with human expert trajectories, creating a diverse and scalable training corpus. The resulting models unify perception, reasoning, and action, and support multiple inference modes for flexible deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a large-scale, open dataset spanning six platforms (Windows, macOS, Linux, Android, iOS, Web) and three task domains (GUI understanding, grounding, and task completion), collected through a hybrid pipeline of automated agents and human experts.\n\n2. Develops a suite of base agent models that unify perception, reasoning, and action, supporting three inference modes (Grounding, Direct Action, Reasoned Action) within a unified action space for seamless cross-platform interaction.\n\n3. Demonstrates strong performance across multiple benchmarks, achieving new state-of-the-art results and showing that scaling diverse GUI-specific data significantly improves general-purpose computer use agents."}, "weaknesses": {"value": "1. The dual-loop data collection pipeline (agent + human) is presented as a key innovation, but similar hybrid strategies have been explored in prior works such as OS-Genesis (Sun et al., 2024b) and AGUVIS (Xu et al., 2024). The paper risks being perceived as a scale-up rather than a conceptual breakthrough.\n\n2. A large fraction of trajectories come from random-walk or weakly semantic exploration, which may not reflect realistic user goals. This could bias the model toward superficial navigation patterns rather than meaningful task completion.\n\n3. The paper reports strong results but lacks fine-grained ablations. For instance, it is unclear how much each component (weak semantic trajectories, reasoning data, augmentation strategies) contributes to final performance. Similarly, the trade-offs between the three inference modes (Grounding, Direct, Reasoned) are not deeply analyzed."}, "questions": {"value": "While ScaleCUA spans six platforms, it is unclear how well it generalizes to unseen applications or domains. For example, can a model trained on productivity and web apps adapt to enterprise software or creative tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "7WIh1XBp2u", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Reviewer_ojRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Reviewer_ojRP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762343683152, "cdate": 1762343683152, "tmdate": 1762915747831, "mdate": 1762915747831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors build a large-scale training dataset for GUI agents, including 6 operating systems and 3 task domains, and develop the ScaleCUA series of models. The authors propose an automatic method to collect trajectories and grounding annotations and build a model that can perform well on GUI understanding, grounding, and planning. The authors benchmark ScaleCUA on a number of tasks to show its efficacy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors curate a large-scale dataset for GUI agents. This includes understanding, grounding, and planning data. The data consists of several software applications across diverse platforms. The authors claim to release the data openly, which will be very valuable.\n2. The authors conduct a comprehensive set of experiments. They evaluate on several benchmarks across different tasks. They also provide several analyses on data scaling, the effect of screenshot resolutions, and several other design decisions. These provide good insights for future model development. \n3. The authors provide practical guidance for collecting data from various platforms in the Appendix, which is very insightful."}, "weaknesses": {"value": "1. The data collection introduced by the authors consists of an automatic method where rule-based agents and heuristics are used, and another component that comprises human-created tasks and trajectories. The presentation of this section gives a misleading impression that the trajectories are created by both humans and agents collaboratively, but in actuality it is some trajectories created by agents and others created entirely by humans. I fail to see any new method or innovation here (besides the heuristics that the authors develop for rule-based agents). Could the authors please provide clarification for this?\n2. The authors do extensive evaluation, but compare with a limited set of models. Several recent models, like [1], [2], [3] use RL to train their models and show that they can achieve impressive performance using public data. I understand that the ScaleCUA models are only SFTed and not RL finetuned, but since models are also highlighted as one of the contributions of this work, comparing them more extensively would further show the effectiveness of their models.\n3. The authors should conduct a more controlled comparison between their data and open source datasets like Jedi. The authors currently compare the benefits of using their data over public data, but it is not clear if the quantity of data used for this comparison is the same or not. The authors could take a fixed random sample of data from different datasets like Jedi, OsAtlas etc., and fine tune the same model to show whether the data collected by the authors provides more gains than already available datasets.\n\n[1] Yang et al. Gta1: Gui test-time scaling agent\n\n[2] Liu et al. Infigui-g1: Advancing gui grounding with adaptive exploration policy optimization\n\n[3] Tang et al. Gui-g2: Gaussian reward modeling for gui grounding."}, "questions": {"value": "1. On online evaluation tasks, especially OSWorld, ScaleCUA underperforms compared to OpenCUA and a few other models despite being trained on large quantities of data. Could the authors comment on this? This is especially relevant because it raises questions about the effectiveness of the data collection method.\n\nPlease see the weakness for more questions and suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTPYNdG5Q4", "forum": "yBFUqdJFZn", "replyto": "yBFUqdJFZn", "signatures": ["ICLR.cc/2026/Conference/Submission1362/Reviewer_AxPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1362/Reviewer_AxPt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762578038632, "cdate": 1762578038632, "tmdate": 1762915747712, "mdate": 1762915747712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}