{"id": "73QNa7rAgm", "number": 17077, "cdate": 1758271881346, "mdate": 1759897199747, "content": {"title": "Unsupervised Anomaly Detection in Tabular Data with Test-time Contrastive Learning", "abstract": "Unsupervised anomaly detection methods typically learn the feature patterns of normal samples during training, subsequently identifying samples that deviate from the learned patterns as anomalies during testing. However, most existing methods assume that the normal patterns in the test set are similar to those in the training set, ignoring the fact that a limited number of training samples may not cover all possible normal patterns. As a result, when the normal patterns in the test set differ from those in the training set, the model may struggle to distinguish whether these samples are normal or anomalous, leading to incorrect predictions. To address this issue, we propose a novel Test-time Contrastive learning approach for unsupervised Anomaly Detection in tabular data (namely TCAD). Specifically, TCAD consists of two core stages: Collaborative Dual-task Training and Test-Time Contrastive Learning. In training, Collaborative Dual-task Training uses two self-supervised tasks to capture multi-level features of normal samples and model normal patterns. At test time, Test-Time Contrastive Learning assigns pseudo labels to high-confidence samples and updates the model in two ways: First, it facilitates model adaptation to pseudo-normal samples while preventing overfitting to pseudo-abnormal ones. Second, it employs a KNN-based contrastive strategy to align pseudo-normal samples with the training distribution while pushing pseudo-abnormal samples away. By combining robust normal pattern modeling with iterative test-time adaptation, TCAD improves anomaly discrimination, especially under distribution shifts between training and test sets. We construct distribution shifts on 15 widely used tabular datasets, and the results show that TCAD achieves state-of-the-art performance, outperforming the best baseline by 4.19% in AUC-ROC, 3.15% in AUC-PR, and 6.64% in F1 score.", "tldr": "", "keywords": ["Unsupervised Anomaly Detection", "Data shift", "Test-Time Training", "Contrastive Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac2e4299a7dd9e6428881b7f4889647bdf3a4682.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents TCAD, a test-time contrastive learning approach for unsupervised anomaly detection in tabular data, where normal patterns at test time deviate from those observed during training. The method has two stages. First, a collaborative dual-task training stage learns low-level and high-level features using a masked autoencoder as the primary task and an embedding reconstruction as the auxiliary task. Second, at test time, the method assigns pseudo labels to high-confidence samples, adapts to pseudo-normal examples while discouraging accurate modeling of pseudo-anomalous ones, and runs a K-nearest neighbors contrastive objective that pulls pseudo-normal embeddings toward the training distribution and pushes pseudo-anomalous ones away. The overall loop iterates until all test samples receive pseudo labels. A clear motivation and a helpful overview are provided. The authors construct distribution shifts on fifteen ODDS and ADBench datasets by clustering normals using K-means, training on the largest cluster, and testing on the remainder, which is mixed with anomalies.  The headline results show average improvements over the strongest baseline. The ablations indicate that both the auxiliary task and the test time contrastive component matter, while the cost table shows that TCAD has a noticeably higher time overhead than DRL and MCM on average. \n\nI appreciate the practical problem and the clean formulation.  At the same time, I have concerns about several choices that affect the strength of the claims. The shift construction with K-means on normals may bias training toward a single mode and therefore create an easier adaptation target than many real deployments where shifts arise from covariate drift, concept drift, or temporal regimes. Reliance on a known contamination rate is a strong assumption; in practice, this value is rarely known and often misspecified, yet the method uses it for high-confidence sample selection and for final thresholding. The test time loop selects extreme samples based on the model itself, which can amplify confirmation bias when early pseudo labels are wrong. The K nearest neighbors contrastive step uses a fixed k  and a fixed temperature and does not study sensitivity to these choices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on a realistic setting where normal behavior at test time shifts relative to training. The design is simple and implementable in standard toolchains, and the figures and equations are accessible. The empirical sweep across fifteen datasets is helpful, and the per-dataset tables make it easy to identify where the gains originate. The ablation study supports the role of the auxiliary task and the contrastive step, and the significance tests for F1 add credibility. I also found the visual and quantitative shift analysis useful for readers who may want to reproduce the construction on other corpora."}, "weaknesses": {"value": "The shift protocol may not reflect many real-world patterns. Training on the largest normal cluster and testing on the remainder can privilege cluster structure and does not cover temporal drift or label shift scenarios. The method assumes a known contamination rate during selection and final thresholding; this is a strong requirement, and the paper does not evaluate robustness when this value is wrong. The test time loop depends on the model to select extremes, which can create a feedback effect. The small pseudo-label audit in Table 1 is encouraging for four datasets, but a broader and more systematic analysis is lacking. Several hyperparameters are fixed globally, for example, k equal to three in the K nearest neighbors step, and there is no sensitivity study for these or for the balance between the adaptation and the contrastive losses. Baseline tuning appears uneven across families, and the search spaces are not aligned, which can inflate the advantage. The summary plots do not show confidence intervals for AUC metrics, and many experiments are averaged over only three seeds."}, "questions": {"value": "1) Could you report robustness to misspecified contamination rates and provide a variant that estimates or adapts this value from data rather than assuming it. \n\n2) How sensitive are results to k in the contrastive step, to the selection batch size per round, and to the trade-off between adaptation and contrastive losses?\n\n3) Would you consider a stronger shift protocol, for example, temporally stratified splits or the AnoShift design with time evolving normals, and report the same tables?\n\n4) Can you add confidence intervals for all main metrics and broaden the pseudo-label noise analysis beyond the four datasets with a simple noise control, like co-teaching or small disagreement filtering?\n\n5) Can you present an appropriate cost comparison that includes end-to-end latency per test sample through all adaptation rounds, and discuss memory growth of the neighbor pool as it accumulates known normals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LJOWnqrlHl", "forum": "73QNa7rAgm", "replyto": "73QNa7rAgm", "signatures": ["ICLR.cc/2026/Conference/Submission17077/Reviewer_jpNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17077/Reviewer_jpNR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413725317, "cdate": 1761413725317, "tmdate": 1762927086301, "mdate": 1762927086301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TCAD offers a novel and effective strategy for handling distribution shifts in unsupervised tabular anomaly detection. By integrating dual-task training with test-time contrastive learning, it enhances model robustness and sets a new state-of-the-art benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Tackles distribution shift between training and test normal samples—a common but overlooked issue in unsupervised tabular anomaly detection.\n\n2.Proposes TCAD, a test-time contrastive learning framework that safely adapts to pseudo-normal samples while repelling pseudo-anomalies. Outperforms SOTA baselines on 15 datasets with constructed distribution shifts ."}, "weaknesses": {"value": "1.Method relies on masked feature reconstruction, limiting applicability to images or time series.\n\n2.Requires prior knowledge of test-set anomaly proportion, which may not be available in practice.\n\n3.Test-time model updates increase latency vs. static inference in standard UAD methods."}, "questions": {"value": "1.Could TCAD be extended to image or multimodal data by replacing the masked autoencoder with a vision foundation model?\n\n2.How sensitive is performance to misspecification of α (e.g., true α=5% but set to 20%)? Is there a way to estimate α adaptively?\n\n3.Have you considered integrating pretrained tabular LLMs to improve initial normal pattern modeling?\n\n4.Is the method suitable for online/streaming detection, where test samples arrive sequentially?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yyYJwkqoK5", "forum": "73QNa7rAgm", "replyto": "73QNa7rAgm", "signatures": ["ICLR.cc/2026/Conference/Submission17077/Reviewer_yiGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17077/Reviewer_yiGT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908726921, "cdate": 1761908726921, "tmdate": 1762927085732, "mdate": 1762927085732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TCAD, an approach for unsupervised anomaly detection in tabular data designed to address distribution shifts where normal patterns in the test set differ from those in the training set. TCAD operates in two  stages: Collaborative Dual-task Training and Test-Time Contrastive Learning. During training, it uses two self-supervised tasks to capture features and model normal patterns. At test time, the model adapts by assigning pseudo-labels (normal or abnormal) to high-confidence samples. It then updates to adapt to these pseudo-normal samples while avoiding overfitting to pseudo-abnormal ones. A KNN-based contrastive strategy then pulls pseudo-normal samples toward the training distribution’s embeddings and pushes pseudo-abnormal samples away."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Designing robust anomaly detectors that generalize well to new domains is critical. \n\nThe paper states its goals and contributions clearly."}, "weaknesses": {"value": "W1) Anomaly detection under distribution shift has been explored in computer vision [A,B,C]; it is unclear why the authors did not cite or discuss this literature.\n\n\nW2) The pipeline’s technical novelty is the main issue. Contrastive loss and reconstruction loss are well known and widely used in the literature. Selecting samples with high confidence at test time is also a known technique [D]. Can the authors describe the components that genuinely belong to their method?\n\nW3) I believe anomaly detection under distribution shift is better defined in the vision domain, as foreground and background in images provide a well-defined approach for specifying shifted normal or abnormal data. The authors should evaluate their pipeline on those datasets as well. \n\n\nW4) The code is not available, making it challenging to reproduce the results.\n\n\n[A] Robust Novelty Detection through Style-Conscious Feature Ranking \n\n[B] A Contrastive Teacher-Student Framework for Novelty Detection under Style Shifts \n\n[C] Red PANDA: Disambiguating Anomaly Detection by Removing Nuisance Factors\n\n[D] SCAN: Learning to Classify Images without Labels"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O6j4PiMMqv", "forum": "73QNa7rAgm", "replyto": "73QNa7rAgm", "signatures": ["ICLR.cc/2026/Conference/Submission17077/Reviewer_fH6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17077/Reviewer_fH6Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181017042, "cdate": 1762181017042, "tmdate": 1762927085185, "mdate": 1762927085185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}