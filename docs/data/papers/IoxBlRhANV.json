{"id": "IoxBlRhANV", "number": 21923, "cdate": 1758323619262, "mdate": 1763713422555, "content": {"title": "Scalable Oversight via Partitioned Human Supervision", "abstract": "As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging.\nOur focus is on tasks that require deep knowledge and skills of multiple domains. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks.\nHowever, based on their narrow expertise, humans may provide a weak signal, i.e., a *complementary label* indicating an option that is incorrect. For example, a cardiologist could state that ``this is not related to cardiology,'' even if they cannot identify the true disease.\nBased on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth.\nWe derive an *unbiased* estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels.\nWe provide finite-sample deviation guarantees for both complementary-only and the mixed estimators.\nEmpirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels.\nWe further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can perform better by these partitioned human supervision.", "tldr": "", "keywords": ["LLM", "scalable oversight", "weak supervision", "agentic systems"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba80809cbec1c852bdf7ef31665125c10b3c497c.pdf", "supplementary_material": "/attachment/3e4057b7f735bca9c177998d343102eb0e262320.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the growing challenge of evaluating and supervising AI systems that exceed human expert capabilities. The authors propose Partitioned Human Supervision, a scalable oversight framework in which multiple experts each provide complementary labels, indicating which candidate answers are incorrect, rather than full ground-truth annotations. The paper derives unbiased estimators for model accuracy using only these partial signals, and further introduces mixture estimators that combine complementary and ordinary labels. Experiments in financial and medical domains demonstrate that such supervision can reliably estimate model performance and even serve as a weak training signal when full supervision is infeasible."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a timely and important problem: how to scale human oversight when AI systems operate beyond individual expertise. The framing of ``partitioned’’ expertise is both realistic and conceptually strong.\n- The derivation of unbiased estimators for accuracy under complementary labeling is clear and mathematically grounded. The analysis of variance and the proposed IVW and MLE-based estimators are rigorous and interpretable.\n- The approach can significantly reduce supervision cost by replacing full labeling with weaker but cheaper expert judgments. This has meaningful implications for high-expertise domains like medicine or law.\n- The authors evaluate their method on domain-specific benchmarks, showing that it remains reliable even when each expert only rules out incorrect answers. The results support the method’s robustness and real-world applicability."}, "weaknesses": {"value": "- The framework suits multi-choice or structured decision problems but may not extend naturally to open-ended or generative tasks where correctness is ill-defined.\n- The method assumes experts can accurately identify incorrect options. In practice, experts may err or have partial uncertainty, which could bias the estimators. The paper could better analyze the effect of noisy or correlated human feedback."}, "questions": {"value": "- The framework can involve experts who reliably provide negative feedback. How would it handle cases where experts are uncertain or only partially knowledgeable? Does the estimator remain valid under probabilistic or noisy complementary signals?\n- Does partitioned supervision assume disjoint expert domains? If experts’ competencies overlap, how are conflicting or redundant complementary labels managed, and what effect might this have on estimator consistency?\n- The method is tested on structured tasks. Could it extend to open-ended or generative settings by redefining complementary feedback (e.g., flagging logically inconsistent outputs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "g6hlVoaUSp", "forum": "IoxBlRhANV", "replyto": "IoxBlRhANV", "signatures": ["ICLR.cc/2026/Conference/Submission21923/Reviewer_oJ9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21923/Reviewer_oJ9E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761278527432, "cdate": 1761278527432, "tmdate": 1762941982718, "mdate": 1762941982718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to improve LLM evaluations in settings where we have limited access to expert annotations by using complementary labels (derived by asking experts to evaluate a model output as either confidently in or out of their expert class). These complementary labels are weaker than ordinary labels as they provide less information and have high variance as compared to ordinary labels, but this work formalizes the impact that these labels can have in evaluation by introducing 2 estimators for combining ordinary and complementary labels (IVW and ML), deriving associated Hoeffding/Bernstein bounds for the estimators, extensive empirical evaluation over traditional and more real-world datasets, and agentic training using the estimator as a fitness function. This work is answering a timely question of how do we leverage humans in LLM evaluations, especially in cases where human annotation is scarce, which is becoming more likely as LLM capabilities increase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work regarding how we can leverage human data in nontraditional ways via complementary labels is original and timely to the current dearth of adequate evaluation paradigms for LLMs. The paper leverages high-quality evaluation paradigms to support their claims, approaching the problem both empirically (across multiple benchmarks, tasks, and domains) and theoretically (unbiased estimator derivation, bounding the variance of the estimators, quantifying how many complementary labels equate to a singular ordinary label) and deriving strong and consistent results. The work is generally clear, although I discuss within the weakness/questions the parts that would benefit from additional detailing and motivation. The work also has potential for significance in the field of scalable oversight and evaluations, as more instances arise where obtaining explicit human feedback from a desired expert may be infeasible but alternative, weaker forms of feedback may be more scalable and readily available."}, "weaknesses": {"value": "One of my biggest qualms with the work is the general motivation framework. “...future models will tackle problems whose solutions are too technical or too crossdisciplinary for any single human to verify comprehensively. When we cannot produce ground truth or prepare automated verifiers, how should we evaluate and train such systems?” In theory there is value to this question and motivation, but how do we ensure that “not cardiology” is an informative label? Specifically, in this toy example, why is cardiology feedback available but the true specialist is not? Additional discussion of why we may expect this paradigm in medicine and in what instances (Is this a diagnosis problem? If it is, would we ever truly have a multiple choice set where we cannot define the true label but we know the complementary label? Then how did we get the set of choices?) . As such, if we are entering a paradigm beyond human expert understanding, is multiple choice the best setting in which to be evaluating this set-up? And if not, how would this system generalize to more open-ended evaluation tasks? It would be helpful to discuss these limitations in the conclusion as potential future work."}, "questions": {"value": "1. Please provide extended explanations for why MathMC results in Figure 3 are not as strong as on the other benchmarks. What is it about this benchmark that makes the complementary labels less effective?\n\n2. In Table 1, it is not clear what the estimator methods are. I see that the methods are outlined in the appendix, but there needs to be some discussion/legend present in the main text as well. Further, the presentation of the table generally can be improved.  Can you highlight how much mixture helps beyond just regular usage of fewer ordinary labels? You also could just report the delta directly between the oracle instance and the different methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "L97vPsntYH", "forum": "IoxBlRhANV", "replyto": "IoxBlRhANV", "signatures": ["ICLR.cc/2026/Conference/Submission21923/Reviewer_KfQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21923/Reviewer_KfQX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694697673, "cdate": 1761694697673, "tmdate": 1762941982523, "mdate": 1762941982523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a scalable oversight framework to evaluate and train advanced AI systems using \"complementary labels,\" which are weak signals from experts identifying incorrect options rather than the correct ground truth. The authors derive estimators to measure accuracy from these weak signals and show how to combine them with scarce ordinary labels. Empirically, the results demonstrate this method can successfully evaluate LLMs without ground truth and can even be used to train an agentic AI system to perform better."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work studies a very relevant topic that is fundamental for the enhancement of current AI systems, especially for their training and evaluation w.r.t. humans.\n- The paper provides mathematical details, motivation and theoretical analysis of its proposed framework. In this way, the contributions are concretely based on mathematical principles and characterization.\n- The result demonstrating that weak human feedback can provide useful learning signal is very promising."}, "weaknesses": {"value": "- Top-1 accuracy can be scarce or too limited to evaluate the performance of a foundation model.\n- The presentation is a bit difficult to follows as it is very technical. However, I don't consider this a proper weakness. \n- If I understand correctly, the number of complementary labels needed for a satisfactory training can be very large.\n- I didn't understand if it is a typo, but several paragraph titles are coloured, which is quite uncommon."}, "questions": {"value": "1) Can the authors comment about the limitations of their approach?\n2) Assuming full ground truth is scarce, what is an estimation of the number of weak supervision required to get the same performances?\n3) What the authors think is the reach of the different learning tasks that a LLMs can learn by using weak supervisions only, or weak supervisions + a few full ground truth supervisions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HoXKUOi2VU", "forum": "IoxBlRhANV", "replyto": "IoxBlRhANV", "signatures": ["ICLR.cc/2026/Conference/Submission21923/Reviewer_2oJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21923/Reviewer_2oJj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905072058, "cdate": 1761905072058, "tmdate": 1762941982275, "mdate": 1762941982275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how do we evaluate AI systems when direct verification is difficult (as the abilities of an AI may exceed that of any single human). The paper relies on the notion that ruling out an incorrect answer is easier than verifying that the answer is correct. It prposes a partitioned supervision protocol, where the human experts indicate when a particular answer is definitely incorrect rather than providing a correct answer (or actual verification). Using these complementary labels, with the additional assumption that these complementary labels are drawn uniformly at random from, the incorrect options, the paper proves that it is possible to construct an unbiased estimator for model top-1 accuracy using these labels, and variance estimators are also studied. Experimental results are convincing (but some questions remain)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I think the theoretical framing of complementary labels as sufficient to estimate model performance is clean and clearly motivated. I like that the paper focuses on the oversight bottleneck, which is timely.\n2. The derivation of the estimator and the variance analysis are correct to me. The decomposition and reasoning about how the uniform query mechanism leads to unbiased estimation is neatly done. I also found the combination with ordinary labels to reduce variance to be intuitive.\n3. The experimental evaluation is thorough within the context of multiple-choice settings. In that sense, the paper is rigorous in validating the correctness of the statistical estimator."}, "weaknesses": {"value": "1. While I follow the theoretical contribution, I am less convinced about the real-world applicability for the kind of motivation the paper has. The experiments are exclusively on closed-form multiple-choice tasks. However, the motivating context in the paper is open-ended and superhuman domains. In that sense, there is a bit of a mismatch: the paper does not really demonstrate that the method scales to the intended setting.\n2. I also think the paper assumes too much familiarity with the labeling protocol. The uniform selection of an index to query is crucial to guaranteeing unbiased complementary labels, but this assumption is not deeply discussed. In realistic scenarios, domain experts may not always be queried in such a uniform way.\n3. The paper claims agentic training benefits using this estimator as a reward signal, but the empirical improvements there are small and relatively difficult to interpret. It would help to clarify what practical training improvements this enables and whether there are examples where the method meaningfully changes agent behavior.\n\nOverall, I think the paper makes a meaningful and conceptually clean theoretical contribution. The estimator is sound and the controlled experiments support the theory. However, I think the paper could improve on motivating and validating how this method translates to settings beyond multiple-choice evaluation. I would benefit from stronger discussion on how the uniform querying assumption interacts with real-world annotation workflows."}, "questions": {"value": "1. I had bit difficulty understanding the paper due to its notation. I believe, using $X$ and $Y$ for random variables and small letters $x, y$ for their realisations could help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rP6fLCSuhv", "forum": "IoxBlRhANV", "replyto": "IoxBlRhANV", "signatures": ["ICLR.cc/2026/Conference/Submission21923/Reviewer_t5Fc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21923/Reviewer_t5Fc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21923/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990973928, "cdate": 1761990973928, "tmdate": 1762941982063, "mdate": 1762941982063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}