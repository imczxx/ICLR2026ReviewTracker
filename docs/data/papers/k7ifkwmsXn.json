{"id": "k7ifkwmsXn", "number": 12745, "cdate": 1758209990417, "mdate": 1759897489842, "content": {"title": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering", "abstract": "Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster’s enhanced outputs over other baselines. Demo samples are available through https://msonic793.github.io/SonicMaster/", "tldr": "We present SonicMaster, an all-in-one music restoration and mastering model controllable by text prompts which is a first of its kind and defines a new task in the field.", "keywords": ["Music Restoration", "Music Mastering", "Music Generation", "Audio Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4bb138c77a13fbdece4810e1c9e3004daf1a100.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a text-conditioned generative model for music restoration. It addresses diverse degradations using a single rectified-flow architecture that maps degraded inputs to restored versions guided by natural-language prompts. To train the model, the authors construct a dataset, where each data is degraded by simulated effects and paired with corresponding textual descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper’s attempt to combine multiple restoration tasks within a single controllable model is an interesting and ambitious direction, particularly when paired with natural-language conditioning.\n- The proposed dataset covers up to 19 degradation types across 5 categories, with textual prompts and metadata. If released, this dataset could become a valuable benchmark for controllable music-restoration research.\n- The multimodal aspect represents a step toward human-interpretable control over restoration on different degradations, which is a relevant trend for both research and creative workflows."}, "weaknesses": {"value": "**Ill-defined task formulation:** \n\nThe paper lacks a precise definition of what constitutes a “degraded” versus “clean” distribution. The chosen degradation types: EQ, dynamics, reverb, amplitude, stereo are actually very standard operations during music mastering procedure rather than authentic degradations. This conceptual framing blurs the line between restoration and stylized mastering.\n\n- For instance, Figures 7 & 8 waveforms resemble legitimate mastered outputs of modern music rather than degraded signals.\n- From the reviewer’s understanding, the target “clean” distribution appears to be the Audiobox-filtered Jamendo subset, but the assumption that this represents the true clean reference is not justified clearly in the manuscript.\n- The paper does not ensure non-overlap between degraded and clean distributions, which undermines the validity of “restoration” as the learning objective.\n- Many realistic degradations (e.g., environmental or background noise, bandwidth reduction, codec artifacts) are absent, further questioning whether the task genuinely represents *restoration*.\n\n**Choice and fairness of baselines:**\n\nWPE and HPSS (from 2010) are outdated and not meaningful as “recent” baselines. The paper omits several directly relevant systems, such as (but not limited to):\n\n- BABE-2 (for historical bandwidth extension),\n- Music Source Restoration (MSR) (Zang et al., 2025),\n- general purpose audio effects removal models or multi-effect restoration approaches [1]\n    \n    Consequently, the reported improvements may largely reflect baseline weakness rather than true superiority.\n    \n\n**Evaluation clarity and presentation:**\n\n- Tables 1 and 2 mix different metrics across degradations without explicitly listing metric definitions; equations or references should be provided for each metric.\n- Large tables are difficult to interpret—boldface patterns are inconsistent, and averages across metrics (from Table 1 and 2) are missing.\n- The comparison with Text2FX is conceptually unfair: Text2FX is a prompt-optimization framework, not a restoration model targeting a fixed “clean” reference. If the authors used the same text prompt as the proposed, I’d say this is not a fair comparison as the objective is different.\n\n**Insufficient analysis of results:**\n\nThe paper reports extensive quantitative results but little discussion or visualization to explain which aspects of the signal actually improve. The subjective results (≈ 8 listeners) are too small to draw strong conclusions. I would show only the results that you would highlight and provide more discussions. It’s hard to catch each ablation method’s purpose.\n\n**Overall impression:**\n\nThe direction is promising, but the core task definition, baseline choice, and experimental analysis require substantial clarification before the work meets ICLR standards.\n\n**Papers worth mentioning:**\n\n[1] Rice, Matthew, et al. \"General purpose audio effect removal.\" *2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)*. IEEE, 2023.\n\n[2] Han, Bing, et al. \"InstructME: An Instruction Guided Music Edit Framework with Latent Diffusion Models.” IJCAI, 2024\n\n**Minor comments:**\n\n- for some citations, instead use \\cite{} or \\citet{} than \\citep{}. this will remove the brackets and be more natural to read\n- line 136: correct the quotation marks: “ → `` (this also happens in other parts of the manuscript)\n- typo @line 182: Punchare → Punch\n- typo @line 200: layersrefine →layers refine\n- I’d recommend placing the tables on the top of the page for better presentation; on the proper page. The evaluation and results section especially lack significant readability.\n- Table 4 is never referred in the text\n- no bolden performance on Table 1: Bright and Vocals, Table 2: Dynamics and Clip, Table 3: Single deg’s PQ, and Table 5"}, "questions": {"value": "- why only use 1 type of effect for multi-degradation? This doesn’t make much sense for linear transformations (e.g., EQ, Stereo), since a single transformation could achieve the same result.\n    - have the authors tried on multi-degradations on different degradations?\n- line 183: “expert-written options” → explanations?\n- line 185: any validation on the task of “parameter prediction”?\n- line 186: what is “hidden clipping” and why is this added and needed?\n- line 187: what is the purpose of “peak-normalisation” and how is it computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Iu4VSuMPj7", "forum": "k7ifkwmsXn", "replyto": "k7ifkwmsXn", "signatures": ["ICLR.cc/2026/Conference/Submission12745/Reviewer_xvXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12745/Reviewer_xvXv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761196314059, "cdate": 1761196314059, "tmdate": 1762923565549, "mdate": 1762923565549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a neural network called SonicMaster which is an unified generative model for music restoration and mastering. The proposed architecture is conditioned on text with instructions.\nIn order to train the model, SonicMaster, they build a dataset which contain paired degraded and high-quality audio examples by simulating them with nineteen degradation functions types.\n\nThe proposed architecture use flow-matching generative model that maps the input and the text prompts into the wanted audio file. The proposed model show improved sound quality across categories which also evaluate with human listeners."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed SonicMaster is a single generative model that can handles a lots of artifact with guided text which is very practical solution \n\n2. The dataset design is well organised, with 19 degradations type across five groups - EQ, dynamics, reverb, amplitude, stereo and also instruction alignment. \n\n3. The paper used the well known Rectified Flow architecture with MM-DiT dual-stream that condition with FLAN-T5. \n\n4. The results of the proposed architecture is very good compare to pervious works which can be seen in the multiple metrics - FAD, KL, SSIM and PQ"}, "weaknesses": {"value": "1. The human evaluation has very low amount of participants - only 13, more participants can strength the results\n\n2. The results on EQ has the weakest performance among all categories which may be results of a problem of balancing \n\n3. The proposed network trained only on synthetic data which may limit the results on real world data."}, "questions": {"value": "1. Does the performance hold on real world scenario where the recording is more complex?\n\n2. How much the proposed model sensitive to wording in the text conditioning? \n\n3. What is the latency and the memory usage of the proposed model? Does the model size has trade off with the quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UJ0s6OJwd7", "forum": "k7ifkwmsXn", "replyto": "k7ifkwmsXn", "signatures": ["ICLR.cc/2026/Conference/Submission12745/Reviewer_Ej1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12745/Reviewer_Ej1n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830711553, "cdate": 1761830711553, "tmdate": 1762923565174, "mdate": 1762923565174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SonicMaster, a unified generative framework for controllable music restoration and mastering. The model addresses nineteen common degradation types (e.g., reverb, clipping, EQ imbalance) with a flow-matching architecture guided by natural-language prompts. It introduces a self-constructed text-conditioned dataset of degraded–clean audio pairs for training. Experiments show clear advantages over existing systems in both objective metrics and human listening tests."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces the first unified model for prompt-based multi-artifact music restoration, addressing 19 types of degradations via flow-matching and classifier-free guidance. This unified formulation represents a significant step toward scalable, controllable, and generalizable audio restoration."}, "weaknesses": {"value": "1.  Objective evaluation has some counter-intuitive metric behavior. For example, in Snippet single degradation in Table 3, SonicMaster's best FAD ($0.069$) and SSIM ($0.625$) scores are less favorable than the Degraded Input's FAD ($0.061$) and SSIM ($0.838$). \n\n2. The model exhibits a clear performance drop in full-song inference compared with snippet in most metrics across Table 2 and Table 3.\n\n3. Subjective evaluations (Table 5 and 6) do not include a quality rating for the Ground Truth (clean audio). The absolute quality rating in Table 5 does not include any SOTA baseline models for direct comparison."}, "questions": {"value": "1. Would it be possible to include a subjective evaluation of the ground truth for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b5WDhVafk5", "forum": "k7ifkwmsXn", "replyto": "k7ifkwmsXn", "signatures": ["ICLR.cc/2026/Conference/Submission12745/Reviewer_cmFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12745/Reviewer_cmFV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985216050, "cdate": 1761985216050, "tmdate": 1762923563930, "mdate": 1762923563930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified, text-guided generative system for controllable music restoration and mastering. The proposed model, SonicMaster, attempts to correct a broad set of music degradations in EQ, dynamics, reverb, amplitude, and stereo, using a rectified-flow model built with MM-DiT + DiT blocks operating in a VAE latent space. Evaluation includes degradation-aware metrics, global fidelity measures, and subjective listening tests, demonstrating that SonicMaster generally improves degradation inputs and is preferred by listeners against prior baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper lie in two parts.\n\n1. Task Novelty. The paper draws a clear line between music restoration/mastering and adjacent areas (e.g., source separation, generation, enhancement), and argues for a single controllable model that handles multiple artifact types prompted by text instruction in an editing mode. This framing is insightful and latest for Audio AI, which has recently focused on diffusion/RFM generation and instruction-guided editing.\n\n2. The analysis on the degradation taxonomy and the corresponding solution on simulation are well-specified. The dataset design is methodical: 19 degradations across five groups with explicit implementations, parameter ranges, and prompt templates. These definitions drive both training and evaluation, creating consistent interfaces between what’s degraded and what the model should fix. The model choice, conditional modules design, and experimental designs are well-organized. The rectified-flow-matching model is sensible for enhancement and editing in a generative manner. And the text conditioning via FLAN-T5 and audio pooling branch for segment stitching are practical, reproducible design decisions. \n\n3. The experimental design is comprehensive. Ablation studies cover audio-text conditioning, sequence length, and model scale variants. Beyond global fidelity (FAD, KL, SSIM, PQ), the authors compute targeted artifact metrics and run two subjective listening test. This breadth reflects the ambition of authors to address the task in multiple angles."}, "weaknesses": {"value": "However, the paper has three critical weaknesses that are worth discussing during the rebuttal process.\n\n1. Despite the task novelty, the methodological novelty at the architecture/learning level is thin for a top-tier AI conference. Specifically, the model uses known blocks (a.k.a., VAE + MM-DiT/DiT + text encoder; rectified-flow objective; and classifier-free guidance), but does not propose new conditional interfaces (e.g., operator-aware adapters, band-mask tokens, time-varying schedules) nor new inference-time optimization strategies that specifically target enhancement failure modes in music context.\n\n2. Despite the comprehensive experimental design, the comparisons to baselines miss close peers in instructional audio editing. Baselines emphasize classic signal-processing (WPE, HPSS) and a mel-spec enhancement pipeline (Mel2Mel+DiffWave) + Text2FX (EQ only). However, some advanced models such as AUDIT and MusicMagus, and more related instruction-guided models are only mentioned in passing without reporting results on implementations under the same data and prompts. This limits claims about data simulation effectiveness and model advantage over modern generative-based editing models. While category-specific metrics are valuable, several are proxy measures that can disagree with perception in dense mixes. Listening tests are helpful but small (8 for MOS; 13 for pairwise study). Generalization beyond the simulation (e.g., real recording faults, overall enhancement) remains under-explored to see if the model contains great generalization capabilities to understand \"what is a good music quality\" after learning millions of simulations in music degradation.\n\n3. The demos in the website are not attractive and reflect some mis-leading in the optimization target.In the paper’s own analysis, reverb metrics on full-length songs are mixed, and SSIM/FAD sometimes decrease relative to inputs. But stereo and some EQ categories do not consistently improve. The observations in the demo website reinforce this: (1) de-reverberation sometimes adds tail energy rather than removing it (4th, 6th, 12th examples); (2) stereo edits can devolve into delays rather than proper image widening (9th, 13th examples); and (3) EQ correction can miss the target on more complex mixes (3st examples). (4) only simple operations such as leveling and bandwidth extensions perform relatively effective, while prior generative works can also lead the performance and this should also require a comparison to them. These gaps suggest simulation-to-real mismatch and insufficient inductive bias in the conditioning to constrain physically plausible edits. \n\n4. Another minor comment on the data quality. Although the dataset is nominally sampled at 44.1 kHz, many training and evaluation tracks appear to have effective bandwidths only up to 32–36 kHz, showing striping artifacts that are typical of MP3 compression. This limits the model’s exposure to true full-band audio and its ability to restore or generate high-frequency details (e.g., brightness, air spot, and spatial shimmer). A controlled high-band test or uncompressed dataset evaluation is needed to confirm performance on the real full-band music."}, "questions": {"value": "1. Can you include AUDIT (might be very similar) and MusicMagus, or any other recent audio editing models as baselines re-trained under your data and prompts? Even if limited to key operators (EQ/reverb/stereo), this would ground the comparison in modern generative-base editing models.\n\n2. How sensitive is the system to wording? For example, if prompts underspecify targets (e.g., “reduce reverb”), how are some metrics like gain/RT60 change decided?\n\n3. Can you evaluate on real studio/home recordings with annotated fixes to demonstrate transfer beyond synthetic degradations? And Also can you provide some results of out-of-domain prompts (from the vague one \"make the music overall sounds better\" to a specific non-trained one \"remove the distortion\")?\n\n4. Is there any architecture-wise novelty you would like to emphasize compared to previous editing models? Or does the task definition novelty and the data simulation yield most contributions of the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3ZYuoYPTEr", "forum": "k7ifkwmsXn", "replyto": "k7ifkwmsXn", "signatures": ["ICLR.cc/2026/Conference/Submission12745/Reviewer_BVLP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12745/Reviewer_BVLP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240113127, "cdate": 1762240113127, "tmdate": 1762923563471, "mdate": 1762923563471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}