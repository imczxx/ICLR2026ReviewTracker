{"id": "IhuvSLIsUN", "number": 12041, "cdate": 1758205356758, "mdate": 1759897537603, "content": {"title": "D-AR: Diffusion via Autoregressive Models", "abstract": "This paper introduces Diffusion via Autoregressive (D-AR) models, a new paradigm recasting the pixel diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts an image into the sequence of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion property, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Then, we apply standard next-token prediction to these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step on pixels in a streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 and 2.00 FID using a 775M and 1.4B Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models.", "tldr": "", "keywords": ["visual generation", "diffusion models", "autoregressive models", "flow matching"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bda9a9085f93a72f2b24d2506d2a927103b455b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"D-AR\" (Diffusion via Autoregressive), a novel and elegant framework that reframes the image diffusion process as a standard autoregressive (AR) next-token prediction task.\n\nThe core of this work lies in a \"Sequential Diffusion Tokenizer.\" Unlike traditional tokenizers that linearize an image spatially (e.g., raster-scan), this tokenizer encodes an image into a 1D sequence of **discrete tokens** (e.g., 256 tokens) that are ordered according to the **diffusion process itself** (coarse-to-fine).\n\nThe generation process is as follows:\n\n1. A standard AR Transformer (e.g., Llama) uses standard next-token prediction to generate this discrete token sequence.\n\n2. The tokenizer's decoder (a pixel-space Diffusion Transformer) uses these AR-generated tokens as the condition for its own denoising steps.\n\n3. Critically, groups of tokens in the sequence correspond to specific diffusion steps. For example, the first 1-32 AR-generated tokens are used to condition step 1 of the diffusion; tokens 33-64 are added to condition step 2, and so on.\n\nThe authors claim SOTA results (2.00 FID) in the \"vanilla AR models\" category on the ImageNet 256x256 benchmark. The framework also naturally supports \"streaming previews\" (e.g., generating a 25% rough image after 25% of tokens are generated) and \"zero-shot layout control\" by fixing the initial (coarse-grained) prefix tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Conceptual Elegance and Novelty**: The main strength is conceptual. Reframing the entire multi-step diffusion process into a single standard AR sequence prediction problem is a highly elegant and novel \"unification\" of the two dominant generative paradigms (AR and Diffusion).\n\n**Coarse-to-Fine Tokenization**: This is a key insight: linearizing an image based on \"diffusion steps\" (temporal, coarse-to-fine) rather than \"space\" (raster-scan). The ablation study in Section 5.1 (2.44 FID for forward order vs. 4.17 FID for reverse order) provides strong evidence for this design choice.\n\n**SOTA Performance in \"Vanilla AR\" Category**: The paper achieves excellent results (2.00 FID with a 1.4B model, Table 3) within its defined category (\"vanilla AR models\"). It clearly and significantly outperforms the previous SOTA in this category, LlamaGen (2.34 FID).\n\n**Zero-Shot Controllability**: The demonstration of zero-shot layout control by simply fixing prefix tokens (which correspond to early, coarse diffusion steps, Fig 5) is a powerful emergent property that validates the design's self-consistency."}, "weaknesses": {"value": "**Potentially Misleading \"SOTA\" Classification**: The paper's headline result (2.00 FID) is only SOTA when compared to \"vanilla AR models.\" As Table 3 shows, this performance still lags significantly behind \"tailored AR models\" (e.g., RAR-XL @ 1.50 FID), \"mask-based models\" (e.g., MAR-H @ 1.55 FID), and hybrid models (e.g., CausalFusion-XL @ 1.77 FID). Claims of \"leading performance\" should be more precise.\n\n**Hidden Tokenizer Complexity**: The framing of the AR model as \"vanilla\" is debatable, as it outsources enormous complexity to the tokenizer, which is far from a simple VQ-GAN. The tokenizer is a 300M parameter model containing an encoder, a VQ, and a 185M parameter pixel-space Diffusion Transformer (DiT) decoder. This tokenizer is, in itself, a powerful generative model.\n\n**Unaddressed Latency & Scalability Concerns**: The framework presents a dual-latency bottleneck (N-step AR generation + K-step Diffusion decoding) and was only tested on low-resolution ImageNet. Furthermore, the reliance on pixel-space operations (to enable previews) suggests a fundamental scalability challenge for high-resolution or T2I tasks, which is not discussed.\n\n**Limited Evaluation Scope**: The entire paper is benchmarked only on ImageNet 256x256 class-conditional generation. This is a very constrained dataset that fails to test the model's ability to handle complex, open-ended text prompts (T2I) or scale to high resolutions (1024x1024), which are standard for SOTA diffusion models."}, "questions": {"value": "To strengthen the paper, it is highly recommended that the authors address the following key questions with experiments:\n\n**Question 1: Can a direct inference latency benchmark (e.g., sec/image) be provided?**\n\nThe paper claims \"KV cache-friendly\" inference but seems to have an inherent dual-latency challenge (N AR steps plus K Diffusion steps). How does the true wall-clock inference speed of D-AR (N=256, K=8) compare to (1) a standard 8-step DiT and (2) a standard VQ-GAN AR model (N=256)?\n\n**Question 2: How is this coarse-to-fine tokenization paradigm expected to scale to high-resolution (1024x1024) T2I tasks?**\n\nThe model was only tested on ImageNet 256. Does a 256-token sequence have sufficient bandwidth to capture both the semantics of a complex text prompt and the fine details of a 1024x1024 image? Or would this require a much longer token sequence (e.g., N=1024), further exacerbating the AR latency bottleneck?\n\n**Question 3: The ablation on K (number of groups) in Figure 7 seems counter-intuitive. Can this tradeoff be clarified?**\n\nFigure 7 shows that K=16 (strongest linearization) gives the best AR training loss, yet the worst rFID and gFID. This seems to contradict the argument that a stronger coarse-to-fine order is better for AR modeling. Conversely, K=1 (no linearization) has poor AR loss but better rFID. Please clarify this critical tradeoff.\n\n**Question 4: What is the distribution of \"work\" between the 1.4B AR model and the 300M Tokenizer?**\n\nThe 185M Diffusion decoder in the tokenizer is a powerful generator on its own. How much does the final image quality depend on the AR model's accuracy? For example, what is the FID if ground-truth tokens are provided for the first 50% of the sequence (z_1...z_128) and the AR model only generates the last 50% (z_129...z_256)?\n\nFurthermore: If the AR model only generates the first 50% of tokens (z_1...z_128), but the DiT decoder uses only these tokens as the condition for the entire K=8 diffusion process (i.e., no new tokens are provided for t>0.5) while t progresses normally, can the model still produce an acceptable image? This would help clarify how much critical information is in the last 50% of tokens.\n\n**Question 5: Is the \"streaming preview\" advantage overstated compared to standard diffusion?**\n\nThe paper claims its preview is a key advantage, but this is debatable. Standard diffusion models (pixel or latent) also produce coarse outlines from noise in early steps (e.g., step 4 of 20). The unique strength seems to be the link between \"AR sequence progress\" and \"image clarity,\" rather than the preview capability itself.\n\n**Question 6: Does fine-tuning this way waste the pre-trained LLM's capabilities?**\n\nA selling point is the use of a standard Llama architecture. However, the model is fully fine-tuned in Stage 2 to predict visual tokens. To what extent does this process \"destroy\" the LLM's original, powerful language understanding and reasoning abilities? If the linguistic capabilities are lost, what is the true advantage of using a pre-trained Llama over a Transformer trained from scratch? Would a unified multimodal model (e.g., training on a mix of text and image tokens) be a superior path?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bGASpUmm43", "forum": "IhuvSLIsUN", "replyto": "IhuvSLIsUN", "signatures": ["ICLR.cc/2026/Conference/Submission12041/Reviewer_iSt8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12041/Reviewer_iSt8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761406406460, "cdate": 1761406406460, "tmdate": 1762923019428, "mdate": 1762923019428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces D-AR, a framework bridging pixel-level diffusion and autoregressive modeling for image generation. The core idea is to transform 2D images into 1D discrete token sequences using a sequential diffusion tokenizer, where tokens are ordered in a coarse-to-fine manner corresponding to diffusion steps. A standard decoder-only Transformer is then used to perform autoregressive next-token prediction on these tokens. Sequential token generation directly mirrors diffusion denoising steps on pixels, enabling KV cache-friendly inference, streaming pixel decoding with consistent previews, and zero-shot layout-controlled synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel framework: Integrates diffusion and autoregressive modeling, providing a unified approach that preserves the strengths of both paradigms.\n2. Sequential diffusion tokenizer: Generates coarse-to-fine token sequences corresponding to diffusion steps, naturally suited for AR modeling.\n3. Preserves vanilla AR architecture: Works with standard decoder-only Transformers without modifications to causal masks, attention, or training schemes. Supports streaming pixel decoding, consistent previews, and zero-shot layout-controlled synthesis."}, "weaknesses": {"value": "1. Dataset and task limitation: Experiments are only on ImageNet 256×256 class-conditional generation. High-resolution images, other datasets (COCO, LSUN, FFHQ), or tasks (image repair, image edit) are not tested.\n2. Model complexity and inference speed: The sequential diffusion tokenizer adds 300M parameters, and AR generation requires predicting 256 tokens sequentially. Although KV caching and token grouping mitigate some overhead, more tokens still increase generation latency compared to smaller token setups like Titok-S (128 tokens), which may limit real-time applications. It would be interesting to know whether the authors have tried using Titok-S or a smaller token budget within the D-AR framework."}, "questions": {"value": "Have you compared the differences in reasoning speed or generation throughput between D-AR and other  AR methods ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSn4iOrPfE", "forum": "IhuvSLIsUN", "replyto": "IhuvSLIsUN", "signatures": ["ICLR.cc/2026/Conference/Submission12041/Reviewer_q2wL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12041/Reviewer_q2wL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908681272, "cdate": 1761908681272, "tmdate": 1762923018906, "mdate": 1762923018906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose D-AR, which reformulates the pixel diffusion process as a vanilla autoregressive procedure. They introduce a diffusion tokenizer that organizes discrete image tokens in a coarse-to-fine order. A conventional next-token prediction framework is then applied to sequentially generate these tokens, which are subsequently grouped and fed into the diffusion detokenizer as conditions to decode images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea to project diffusion process into a vanilla autoregressive procedure is interesting.\n2. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Although the idea is interesting, I am not entirely convinced that this approach truly demonstrates a synergistic effect between autoregressive modeling and diffusion. The autoregressive component still relies on discrete tokens, which are then used as conditions for the diffusion model. Consequently, the information loss inherent in the discrete tokens is propagated into the diffusion process. This is evidenced by the higher rFID of the diffusion tokenizer compared to its continuous VAE counterpart.\n2. The results in Table 3 are not a fair comparison with other methods in terms of parameter count. The diffusion tokenizer itself contains an additional 300M parameters, whereas the tokenizers in other approaches (whether continuous or discrete) are much smaller. For instance, D-AR-XL actually has around 1.1B parameters, yet achieves an FID of 2.09, which is worse than the approximately 700M SiT-XL diffusion model with an FID of 2.06. This raises concerns about the advantage of D-AR compared with a pure diffusion model under the same parameter budget.\n3. The authors claim that an intriguing property of their model is the ability to preview the diffusion generation process. However, this is not unique to D-AR, but rather a general characteristic of diffusion models in general."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tdq6KVG6tC", "forum": "IhuvSLIsUN", "replyto": "IhuvSLIsUN", "signatures": ["ICLR.cc/2026/Conference/Submission12041/Reviewer_MQZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12041/Reviewer_MQZQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942548642, "cdate": 1761942548642, "tmdate": 1762923018501, "mdate": 1762923018501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces D-AR (Diffusion via Autoregressive models), a new framework that reinterprets the image diffusion process as a standard autoregressive next-token prediction task by using a specially designed sequential diffusion tokenizer that maps coarse-to-fine diffusion steps into a linear sequence of discrete tokens. This approach enables high-quality image generation with vanilla autoregressive transformers (e.g., Llama backbones), achieving state-of-the-art FID scores (2.00 with 1.4B parameters) on ImageNet while supporting streaming previews and zero-shot layout control—without modifying core AR mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Its sequential diffusion tokenizer naturally imposes a coarse-to-fine token ordering aligned with diffusion denoising steps, which is highly suitable for autoregressive modeling. \n(2)  The framework supports streaming, consistent previews during generation at no extra cost by leveraging diffusion’s ability to jump-estimate final images from partial token sequences. \n(3) It enables zero-shot layout-controlled synthesis by fixing prefix tokens, all without finetuning. \n(4) D-AR benefits from the efficiency of AR inference (e.g., KV caching) and operates directly on raw pixels without VAEs, simplifying the pipeline while maintaining compatibility with existing LLM infrastructure."}, "weaknesses": {"value": "(1) I find D-AR to be a very interesting framework that combines two well-established paradigms. However, the motivation for introducing this new hybrid paradigm is unclear—current diffusion-based or purely autoregressive (AR) approaches already perform very well in image generation. Why is this fused paradigm necessary? What specific problem does it solve?\n(2) D-AR cannot be easily integrated with state-of-the-art step distillation methods, such as one-step image generation. If step distillation is applied to reduce the number of sampling steps to just 1 or 2, what is the purpose or benefit of the autoregressive component in D-AR?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lUu2aYptUX", "forum": "IhuvSLIsUN", "replyto": "IhuvSLIsUN", "signatures": ["ICLR.cc/2026/Conference/Submission12041/Reviewer_XBux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12041/Reviewer_XBux"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12041/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156838011, "cdate": 1762156838011, "tmdate": 1762923017858, "mdate": 1762923017858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}