{"id": "g1Aq2U1L3T", "number": 21912, "cdate": 1758323510465, "mdate": 1759896896961, "content": {"title": "Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization", "abstract": "Direct Preference Optimization (DPO) has been widely used for aligning language models with human preferences in a supervised manner. However, several key questions remain unresolved: the rationale behind its log-ratio reward, how the statistical structure of preference datasets shapes its training dynamics, and how those dynamics impact downstream capabilities. We approach these questions from a Bayesian perspective, interpreting the goal of preference optimization as learning the differential information required to update a reference policy into a target policy. To formalize this view, we introduce the Differential Information Distribution (DID), defined as the distribution over samples that carry the Bayesian evidence required to update policies. We introduce three complementary insights by viewing preference optimization through the DID. First, we find that DPO's log-ratio reward is uniquely justified when preferences encode the Differential Information needed to update a reference policy into the target policy. Second, we discuss how commonly observed training dynamics in DPO, including changes in log-likelihood and policy exploration, stem from a power-law DID relationship. Finally, we analyze how training dynamics influence downstream performance using the entropy of DID, a principled measure of uncertainty in the learned information. We observe that learning high-entropy DID improves open-ended instruction-following, while low-entropy DID benefits knowledge-intensive QA. Taken together, our results show that DPO’s reward design, training dynamics, and downstream capabilities all emerge as natural consequences of learning Differential Information, offering both a principled theoretical foundation and practical guidance for preference-based alignment.", "tldr": "We present a Bayesian perspective on Direct Preference Optimization, showing that its reward design, training dynamics, and downstream performance naturally follow from how preferences encode Differential Information.", "keywords": ["Direct Preference Optimization", "Alignment", "LLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b0e014f97ca912798f548ec4b2f2a4e281dd1cc.pdf", "supplementary_material": "/attachment/ec4cdeeca581fc3588cd6250f003cbf45e137534.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a Bayesian reinterpretation of Direct Preference Optimization (DPO). It introduces the concept of Differential Information Distribution (DID), which represents the Bayesian evidence required to update a reference policy `π_ref` into a target policy `π*`. The authors show that DPO’s log-ratio reward can be derived as the unique form consistent with a power-law DID structure, and that training dynamics, specifically changes in log-likelihood and exploration, arise naturally from the properties of DID. Furthermore, the paper introduces DID entropy as a principled measure of the uncertainty of alignment information, showing that low-entropy DIDs improve factual QA while high-entropy DIDs improve open-ended generation. Theoretical claims are supported by formal derivations (e.g., the Likelihood Ratio Representation) and experiments on preference-based LLM fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Establishes a Bayesian formulation of preference optimization, linking DPO to information-theoretic evidence accumulation.  \n2. Explains DPO’s reward structure, dynamics, and task-dependent behaviors (open-ended vs factual) under one consistent lens.  \n3. Provides closed-form derivations (e.g., Likelihood Ratio Representation, Entropy of DID) that connect policy updates to Bayesian ratios.  \n4. Offers a principled way to reason about why different `β` or entropy configurations produce distinct behaviors in alignment training."}, "weaknesses": {"value": "1. The conditional independence of `X` from the prior and the power-law DID assumption are not empirically testable or demonstrated to hold in real preference data.  \n2. DID “existence” is defined by construction, not derived, which weakens claims of theoretical generality.  \n3. DID entropy estimation uses a small sample (`K=32`) with potentially large variance; no confidence intervals or significance testing are reported.  \n4. Theorem 3.2’s “unique justification” of the log-ratio reward is contingent on strong assumptions and may not hold under alternative data generation processes.  \n5. Experiments lack ablations (e.g., sensitivity to `β`, prompt distribution, or entropy threshold) and multi-seed averages.  \n6. The analysis remains conceptual, real-world preference datasets are rarely power-law structured or independently sampled as assumed."}, "questions": {"value": "1. Can the authors empirically validate that preference data follow a power-law DID or that `P(X|Y)` is conditionally independent of the prior?  \n2. How sensitive are the DID entropy and corresponding trends (open-ended vs factual tasks) to sample size and importance weighting variance?  \n3. Does Theorem 3.2’s “uniqueness” still hold if the DID deviates from power-law structure or when sampling dependencies between `(y_w, y_l)` exist?  \n4. Can the authors clarify whether the entropy results persist under token-level DID instead of sequence-level ratios?  \n5. Would alternative estimators (e.g., bootstrapped or token-normalized entropy) yield the same qualitative findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F4G02xH3Fq", "forum": "g1Aq2U1L3T", "replyto": "g1Aq2U1L3T", "signatures": ["ICLR.cc/2026/Conference/Submission21912/Reviewer_TcgT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21912/Reviewer_TcgT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801634096, "cdate": 1761801634096, "tmdate": 1762941978391, "mdate": 1762941978391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Differential Information Distribution (DID), which represents the distribution over samples that carry the Bayesian evidence required to update policies. The paper provides theoretical insights to prove that the log-ratio reward of DPO is the only optimal solution when preferences encode the differential information. Through DID analysis of policy training dynamics, the DID entropy is empirically linked to the performance of the model on open-ended instruction-following and knowledge-intensive QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper introduces the Differential Information Distribution (DID), providing a deep understanding of how Bayesian evidence drives the updating of policies in DPO.\n\n+ The paper demonstrates that the reward parameterization, training dynamics, and learned capabilities in DPO emerge naturally by analyzing DID.\n\n+ By analyzing the Shannon entropy of the DID, this paper demonstrates how DID entropy influences the trade-off between factual accuracy and open-ended task performance via a real LLM experiment in Section 5."}, "weaknesses": {"value": "+ The controlled Energy-Based Model experiments and empirical tests (Figure 1, Figure 2, Figure 3) in Section 3 and 4 are built around strong assumptions of matched data generating processes and synthetic settings where DIDs align almost perfectly. While some results (Table 1) use real-world LLMs and datasets, they are limited. It's uncertain whether the findings from synthetic setups apply to LLMs on real tasks.\n\n+ There is the assumption that $\\pi_{w} = \\pi_{\\text{ref}}$ in Section 4, along with the assumption that $\\pi_{\\text{ref}}$ is the unoptimized policy. However, the initial policy is not necessarily fine-tuned on $y_w$, and there is research focusing on applying different $\\pi_{\\text{ref}}$ strategies, which limits the applicability of the theory.\n\n+ There are too many theorems, and many key proofs are in the appendix, which increases the difficulty of reading and hampers the flow."}, "questions": {"value": "+ Can the authors explain why SimPO's JS divergence exhibits a distinct behavior in Figure 1 (Right)?\n\n+ How about the performance of other baselines like SimPO in the experiment of Table 1? Can the same phenomenon be observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QmFdpVpAOg", "forum": "g1Aq2U1L3T", "replyto": "g1Aq2U1L3T", "signatures": ["ICLR.cc/2026/Conference/Submission21912/Reviewer_KUMF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21912/Reviewer_KUMF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878112456, "cdate": 1761878112456, "tmdate": 1762941978186, "mdate": 1762941978186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an idea of differential information which captures the information needed to update a reference policy to a target policy. They utilize this framework to show that when the differential information distribution between preferred and dispreferred is related to the DID between the target and reference policy by a power law, the preference data contains the information needed to learn the target policy. They also demonstrate how DPO learns the optimal policy under this framework and setting. They provide validation on synthetic data and show that high and low DID entropy is correlated with different model behavior."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Framework - The paper provides a thorough analysis under the DID framework and show conditions under which the DPO reward is optimal. The framework utilizes a Bayesian perspective and brings a new approach to analyzing the behavior of DPO.  The analysis leads to potential new insights on the effects of likelihood displacement and the structure of preference data."}, "weaknesses": {"value": "Justification - The framework while interesting and novel lacks justifications for key assumptions and definitions. For example, in definition 2.2, it is unclear why should conditional independence and a bayesian update model preference data and learning well. The framework relies on this definition, so it is important that justification and empirical support is provided. Furthermore, in Theorem 4.1, it is assumed that either preferred or dispreferred responses are sampled from the reference model which is in many settings not the case. Lastly, there is a lack of empirical evidence on real-world data that these assumptions hold or that the results do closely align with practice. \n\nExperimental results - Following up on previous comments, the experiments do not provide strong support for the claims. In particular, a demonstration of the power law on real-world data should be provided as well as a comparison across different rewards. Other results that would support claims are verifying Theorem 4.1 which can be directly verified on real-world data. Currently, the primary results on real-world data are a comparison between DPO and DPO-PG which was not a central part of the earlier results. \n\nDirect verification of results and justification of key assumptions would greatly improve the paper."}, "questions": {"value": "- Can you provide justification for Definition 2.2?\n- How might Theorem 4.1 generalize to other datasets?\n- Can you directly verify the power-law relationship, the optimality of DPO, or the likelihood change on real-world data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9cIhiOd0n", "forum": "g1Aq2U1L3T", "replyto": "g1Aq2U1L3T", "signatures": ["ICLR.cc/2026/Conference/Submission21912/Reviewer_9kxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21912/Reviewer_9kxT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954323555, "cdate": 1761954323555, "tmdate": 1762941977990, "mdate": 1762941977990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}