{"id": "KFrmUwP6Jx", "number": 11614, "cdate": 1758202504228, "mdate": 1759897564716, "content": {"title": "Mini-batch Submodular Maximization", "abstract": "We present the first mini-batch algorithm for maximizing a non-negative monotone decomposable submodular function, $F=\\sum_{i=1}^N f^i$, under a set of constraints. Our experiments demonstrate that a straightforward uniform mini-batch sampling approach significantly outperforms existing state-of-the-art sparsifier methods, requiring only a fraction of their running time. However, explaining this improvement via worst-case analysis is impossible.\n\nInstead, we employ smoothed analysis to provide a theoretical justification for our empirical findings. Under mild assumptions, we show uniform sampling is superior to weighted sampling for both the mini-batch and sparsifier approaches. We further verify empirically that these assumptions hold across various datasets. Unlike weighted sampling, uniform sampling is simple to implement and several orders of magnitude faster, making it ideal for handling massive real-world datasets.", "tldr": "", "keywords": ["smoothed analysis", "mini-batch methods"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/539ada1270be05f3d991b45b79e2d70a223e84c9.pdf", "supplementary_material": "/attachment/72834a7c8686bda3f3984fe93fe0a1bc5355512f.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of submodular maximization with a monotone decomposable objective function, i.e., $f(S)=\\sum_{i=1}^Nf^i(S)$. The authors propose an algorithm based on uniform mini-batch sampling, which empirically outperforms existing sparsifier-based methods. To explain this advantage, they conduct a smoothed analysis demonstrating that uniform sampling is theoretically superior to weighted sampling in both the mini-batch and sparsifier settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses a well-motivated problem with broad practical relevance and numerous applications.\n2. The theoretical analysis of the proposed submodular maximization algorithm is rigorous yet presented in a clear and accessible manner."}, "weaknesses": {"value": "1. A major concern lies in the significance of the proposed approach. The large-scale optimization of decomposable submodular functions can be viewed as a special case of stochastic submodular maximization, or more generally, submodular maximization with i.i.d. bandit feedback, where uniform sampling corresponds to drawing samples from the stochastic distribution. From this perspective, the authors should compare their results against existing works in this broader setting (e.g., [1–3]). Moreover, the use of uniform sampling to approximate the objective function is typically considered a baseline method in the literature, which limits the novelty of this contribution. As a result, the overall advancement provided by the paper appears marginal.\n\n2. The presentation of the smoothed analysis is unclear, making it difficult to follow. For instance, in the description of the smoothing model, the authors state that $f^i(e*)$  is a random variable, but the source of randomness is not explicitly explained. It remains ambiguous whether the randomness arises from the function $f^i$ itself or from the sampled element $e^*$?\n\n[1] Singla, Adish, Sebastian Tschiatschek, and Andreas Krause. \"Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization.\"\n\n[2] Wenjing Chen, Shuo Xing, and Victoria G Crawford. A threshold greedy algorithm for noisy submodular maximization. arXiv preprint arXiv:2312.00155, 2023.\n\n[3] Karimi, M., Lucic, M., Hassani, H., & Krause, A. Stochastic submodular maximization: The case of coverage functions. Advances in Neural Information Processing Systems, 30."}, "questions": {"value": "1. Could you provide a comparison between your algorithm and the one proposed in [1]? It appears that your method may be a straightforward adaptation of the algorithm in [1], obtained by replacing the TopK selection step with taking the same number of samples of the marginal gain for each element in the universe. \n2. Could you elaborate on the specific technical contributions of this work? The problem of submodular maximization under noise has been extensively studied in prior literature, where analyses typically focus on algorithms with additive or multiplicative approximation errors and employ concentration inequalities to estimate the objective value. It would be helpful if you could clarify how your analysis or techniques differ from these existing approaches.\n\n[1] Singla, Adish, Sebastian Tschiatschek, and Andreas Krause. \"Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h19jGhgL8y", "forum": "KFrmUwP6Jx", "replyto": "KFrmUwP6Jx", "signatures": ["ICLR.cc/2026/Conference/Submission11614/Reviewer_nDVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11614/Reviewer_nDVU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773263241, "cdate": 1761773263241, "tmdate": 1762922688749, "mdate": 1762922688749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a highly efficient mini-batch greedy algorithm for maximizing large-scale decomposable submodular functions ($F=\\sum_{i=1}^{N}f^{i}$), where evaluating the full function is prohibitively expensive. To theoretically justify the empirical superiority of a simple uniform sampling strategy over complex weighted methods, the authors move beyond worst-case analysis and employ a novel smoothed analysis framework. This framework allows them to prove that their uniform mini-batch algorithm achieves near-optimal approximation guarantees for both cardinality and p-system constraints, with improved query complexity. Specifically, for a cardinality constraint, the algorithm achieves a $(1-1/e-\\epsilon)$-approximation using only $\\tilde{O}(\\frac{k^{2}n}{\\epsilon^{2}\\phi})$ oracle queries. Extensive experiments on five real-world datasets substantiate these findings, demonstrating that the proposed method is orders of magnitude faster than the state-of-the-art while providing superior or comparable solution quality. The paper also empirically validates that the core assumptions of its theoretical model hold in these practical settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. A major strength lies in its comprehensiveness, as the authors rigorously validate their approach across three distinct settings: cardinality constraints, p-systems, and curvature. Moreover, the emphasis on mini-batching represents a timely and meaningful contribution to the machine learning community, addressing scalability in submodular optimization."}, "weaknesses": {"value": "The primary limitation stems from an overly strong assumption. Specifically, the authors assume that for any individual element $e$, each function $f_i$ satisfies both $f_i(e) \\in [0,1]$ and an expected value $E[f_i(e)] \\ge \\phi$. These strict upper and lower bounds on single-element contributions overly simplify the problem, making it plausible that a simple uniform sampling strategy could already achieve a comparable approximation of the overall objective function $F$ using standard concentration results such as the Chernoff bound."}, "questions": {"value": "1.\tIf we remove the normalization condition while keeping all other conditions unchanged, does the conclusion still hold?\n2.\tAlternatively, can you prove that without these assumptions, it is impossible to optimize using fewer than ( Nnk ) queries in terms of order? \n3.\tIt seems that the paper still contains some minor errors — for example, in proof of lemma 3.4, the Chernoff bound expressions are missing the $\\exp()$ wrapper.\n4.\tThe authors did not mention that their assumptions may oversimplify the theoretical analysis of the algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AKPgxMXseE", "forum": "KFrmUwP6Jx", "replyto": "KFrmUwP6Jx", "signatures": ["ICLR.cc/2026/Conference/Submission11614/Reviewer_yRVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11614/Reviewer_yRVE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910696136, "cdate": 1761910696136, "tmdate": 1762922688238, "mdate": 1762922688238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A decomposable submodual rmaximization, the sub-modular function $F$ is the sum of $N$ other sub-modular functions. The algorithms that make oracle calls to $F$ need to make $N$ oracle calls to the smaller/simpler submodular functions, thus the run-time has undesirable dependency on $N$.  To address this, prior work proposed sparsifier approach, that choses a subset of the smaller functions (and scales them) with certain probabilities. However the calucation of these probabilities takes $O(Nn)$ oracles, and this is treated as preprocessing step, in prior works.\n\nThe authors observe that uniform sampling probabilities work very well in practice and attempt to come with a theoretical explanation (though in the worst-case, uniform sampling is bad).  They resort to smoothed analysis, pioneered by Spielman and Tang, to address this."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. As far as I know, this is the first instance of applying smoothed analysis for this problem.\n2. It is known that many heuristic algorithms work well in practice, though their approximation guarantees are bad in the worst case. Thus, there is a need to go beyond the worst-case analysis, and this work is a step in that direction.\n3. Generally, when the exact oracles are replaced with approximate oracles that have a multiplicative error, the proofs go through, with multiplicative errors creeping into the approximation factors. This work relies on using additive-approximate oracles, where the additive approximation errors are based on the optimal values. They show that this type of approximation still yields good guarantees, which in my view is clever.\n4. Once the framework is set, the proofs are not hard to follow, which is a strength in my view."}, "weaknesses": {"value": "My biggest concern is the definition of the smoothing model. Where is the randomness coming from? The functions $f^i$ are all deterministic. So what does it mean to say $f^(e^*)$ is a random variable and so what does the Expectation of this reandom variable mean? Where is the underlying distribution? If I were to infer, my guess is that each $f^i$ is chosen with a certain probability, and that's where the randomness is coming from, but I am not sure. This must be addressed.\n\nI am willing to increase my score once I understand this."}, "questions": {"value": "1. Please see the weakness.\n2. Definition of Smoothing model: Can you formally define \"each RV depends on at most d others\". \n3. Similarly, what does bounded dependency mean in Theorem 3.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fvKgCHUQA0", "forum": "KFrmUwP6Jx", "replyto": "KFrmUwP6Jx", "signatures": ["ICLR.cc/2026/Conference/Submission11614/Reviewer_VhBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11614/Reviewer_VhBo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009418443, "cdate": 1762009418443, "tmdate": 1762922687664, "mdate": 1762922687664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies a uniform sampling algorithm for maximizing a nonnegative monotone *decomposable* subomdular function\n$F = \\sum_{i=1}^N f^i$, where each $f^i$ is a nonnegative monotone submodular function.\nSpecifically, it proposes a uniform sampling method to sparsify $F$ when $N$ is large by\nsampling a subset of function $f^i$ of size $N' << N$.\nThey offer a smoothed analysis inspired by [Spielman--Teng, JACM 2004] to go beyond worst-case inputs.\nMuch of this work is focused on removing a $O(N n)$-time bottleneck step in the\nsparsification scheme of [Kenneth--Krauthgamer, ICALP 2024] that computes the\nvalues $p_i = \\max_{e \\in E, F(e)\\ne 0} f^i(e) / F(e)$.\nFinally, the authors give an empirical study across a wide range of datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Builds on sparsification framework for decomposable submodular functions in [Rafiey--Yoshida, AAAI 2022].\n- Identifies a hard/pathological case where only a single $f^i$ takes nonzero\n  values, and proposes smoothed analysis to bridge this gap (beyond worst-case analysis)\n- Diverse set of experiments (e.g., several different interesting datasets, tasks, and objective functions)."}, "weaknesses": {"value": "- The writing is not as crisp as it could be: there are many technical ideas discussed that distract from the main contribution (e.g., the \"Approximate oracles\" paragraph in Section 2.1 doesn't add to the message and distracts the reader). The paper could benefit from presenting fewer ideas and making things more streamlined, without compromising the main message.\n- The theoretical contribution is novel but limited (Section 3)"}, "questions": {"value": "**Questions**\n- [049] What are practical examples of $N$ being extremely large? What are\n  reasonable assumptions for $f^i$ to all be different? If they are the same\n  function (e.g., functions for two students with the same preference), then\n  this is the same as increasing the coefficient of $f^i$ and reducing $N$.\n\n**Typos/suggestions**\n- [120] Suggestion: The paper could benefit from presenting a more compelling\n  running example than lunch menu optimization.\n- [528] Typo: Update the Kenneth--Krauthgamer reference to ICALP 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CY166iM3zJ", "forum": "KFrmUwP6Jx", "replyto": "KFrmUwP6Jx", "signatures": ["ICLR.cc/2026/Conference/Submission11614/Reviewer_9gZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11614/Reviewer_9gZU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195750270, "cdate": 1762195750270, "tmdate": 1762922687116, "mdate": 1762922687116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}