{"id": "sHn5rq6L0O", "number": 18373, "cdate": 1758286947754, "mdate": 1759897107841, "content": {"title": "On the Limits of Curriculum Learning for Post-Training Large Language Models", "abstract": "Large language models (LLMs) excel at many common-sense tasks, yet they remain brittle when required to perform consistent multi-step reasoning. \nEvaluations on benchmarks such as AMC or AIME25 are often affected by data contamination, motivating our focus on synthetic reasoning tasks with controllable difficulty. \nSynthetic datasets allow us to generate problems whose difficulty directly corresponds to the number of (verbalized) reasoning steps required.\nBy focusing on synthetic tasks with minimal natural language complexity, we ensure that our conclusions are driven by reasoning ability rather than sophisticated linguistic understanding.\nWe investigate generalization to higher difficulty levels at the granularity of individual difficulties, a setting that differs from the standard out-of-distribution evaluation, which typically tests on entirely different tasks. \nTo improve generalization to harder problems, we study curriculum learning (CL) as a mechanism to exploit difficulty during post-training. \nAcross multiple synthetic reasoning tasks and a family of medium-sized models, we find that CL has no significant impact under either supervised fine-tuning (SFT) or reinforcement learning (RL). \nMoreover, the optimal CL schedule varies across datasets and models, while standard random sampling performs competitively.\nWe identify response length as a key factor driving model performance, and observe that CL schedules do not significantly impact response length, explaining why SFT performance does not improve with CL.\nWhile pre-training commonly adopts data mixing strategies akin to curriculum learning, these findings call into question the usefulness of curriculum learning for post-training in mathematical reasoning tasks, and suggest that future work should explore alternative mechanisms for strengthening pure reasoning robustness in LLMs.", "tldr": "There is no single best curriculum strategy that leads to performance gains in synthetic data settings, where data contamination is not present. Standard random sampling often performs very competitively.", "keywords": ["Curriculum Learning", "SFT", "RL", "post-training", "finetuning", "synthetic datasets", "mathgap", "kk", "data contamination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d5adc6abbe853332f5db6c8e3e468a6e0f4a9d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether curriculum learning (CL) improves post-training performance of large language models on synthetic mathematical reasoning tasks. The authors systematically compare five curriculum schedules (including easy-to-hard, hard-to-easy, and random sampling) across multiple model families (LLaMA, Qwen, Gemma) and sizes (0.6B-9B parameters) using both supervised fine-tuning (SFT) and reinforcement learning (RL). \n\nThe main finding is that curriculum learning has no significant impact on performance—standard random sampling performs competitively with all curriculum schedules tested. The paper also observes that SFT can fail catastrophically on certain tasks (losing mathematical abilities), while RL maintains better performance, with response length identified as a key explanatory factor.\n\nWhile the experimental design is sound and the research question is important, the paper suffers from significant writing quality issues that impede comprehension. Additionally, the use of synthetic datasets with minimal linguistic complexity and the limitation to medium-sized models raise concerns about the generalizability of the findings to real-world applications."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Important and Timely Research Question\n\nThe paper addresses a highly relevant question: whether curriculum learning, commonly used in LLM pre-training and post-training, actually improves performance on reasoning tasks. Investigating this widely-adopted technique with rigorous empirical validation is valuable, and reporting negative results (that curriculum learning may not help) is particularly important for guiding future research.\n\n2. Sound Experimental Design\n\nDespite the writing issues, the experimental design is well-conceived:\n\n- Using synthetic datasets with controllable difficulty avoids data contamination issues that plague real-world benchmarks (AIME, GSM-8K, MATH)\n- Systematic comparison of five curriculum schedules provides comprehensive coverage of common strategies\n- Evaluation across multiple model families (LLaMA, Qwen, Gemma) and sizes (0.6B-9B) demonstrates generalizability\n- Comparing both RL (GRPO, PPO) and SFT provides insights into how training paradigms interact with curriculum learning\n\n3. Thorough Empirical Analysis\n\nThe paper provides comprehensive analysis including zero-shot baselines, in-distribution and out-of-distribution performance, response length and format-following behavior, and longitudinal tracking across training epochs. The identification of response length mismatch as a key factor in SFT failure is a concrete, actionable insight."}, "weaknesses": {"value": "1. Writing Quality and Clarity Issues\n\nSeveral sentences suffer from unclear or confusing constructions that hinder comprehension:\n\n- Line 63: \"performance typically degrades with reasoning length\" is ambiguous. The preposition \"with\" lacks directionality—does performance degrade as reasoning length increases, or simply in the presence of reasoning length? This should be rewritten as \"performance typically degrades as reasoning length increases.\"\n\n- Lines 64-65: \"Since CL acts on difficulty, there is reasonable expectation that it can improve performance on more difficult examples\" presents an incomplete causal chain. Why does \"acting on difficulty\" necessarily lead to improved performance on harder examples? The mechanism linking CL's operation on difficulty to performance improvement needs explicit explanation.\n\n- Lines 52-53: \"none of the schedules we consider consistently improves performance\" contains structural ambiguity. The placement of \"consistently\" between the relative clause \"we consider\" and the main verb \"improves\" creates confusion about what is being modified. Additionally, \"consider\" is inappropriate in an experimental context—\"test,\" \"evaluate,\" or \"examine\" would be more accurate. Suggested revision: \"none of the curriculum schedules we tested consistently improves performance.\"\n\n2. Incomplete Methodological Details\n\nSection 3.3 (lines 298-307) provides only a high-level description of the RL reward function and training objective. For a paper centered on comparing training methods, the reward design is crucial and should be fully specified in the main text, not relegated to the appendix. Specifically:\n\n- The exact reward function formulation with all components and their weights\n- The loss function for both GRPO and PPO\n- Justification for the chosen reward components and their relative importance\n\nWithout these details in the main text, it is difficult to assess whether differences between RL and SFT stem from the curriculum schedule or from the reward design itself.\n\n3. Limited Experimental Scope with Overgeneralized Claims\n\nThe paper tests only medium-sized models (0.6B-9B parameters) but draws broad conclusions about curriculum learning in LLM post-training (e.g., Abstract: \"CL has no significant impact\"; Conclusion line 485: \"CL consistently has no significant impact\"). While the conclusion (lines 540-544) briefly acknowledges that findings are limited to the tested model sizes, this is a critical limitation that deserves more prominent treatment because:\n\n- The paper does not discuss whether conclusions might differ for larger models (e.g., 14B, 32B) where curriculum effects might be more pronounced due to different capacity and training dynamics\n- The strong claims in the abstract and introduction are not sufficiently qualified given this limitation\n- For practitioners working with different model sizes, it is unclear how to interpret these results\n\nGiven that model scale is known to affect many aspects of LLM behavior, testing curriculum learning only at medium scales while making general claims about its effectiveness is a significant weakness. The abstract and introduction should be revised to explicitly state the scope, and the paper would benefit from at least preliminary experiments at one or two larger scales.\n\n4. Representativeness of Synthetic Datasets\n\nWhile using synthetic datasets to avoid contamination is laudable, the paper's emphasis on \"minimal natural language complexity\" (lines 58-59) raises serious concerns about generalizability. Although the conclusion (lines 540-544) acknowledges this limitation, we believe this is too important to dismiss with a brief mention in the conclusion:\n\n- Real-world mathematical reasoning tasks involve substantial linguistic complexity (parsing word problems, resolving ambiguities, mapping natural language to formal operations) that may interact critically with curriculum learning effects\n- The simplicity of the synthetic tasks might mask curriculum effects that would emerge in more naturalistic settings where models must learn both reasoning and language understanding simultaneously\n- The paper does not provide sufficient discussion of why findings on minimal-language synthetic tasks should generalize to natural language reasoning, nor does it cite evidence that curriculum effects are invariant to linguistic complexity\n- The design choice to minimize linguistic complexity, while methodologically convenient, may have inadvertently eliminated the very scenarios where curriculum learning provides benefits\n\nThe authors argue that synthetic tasks help \"focus on reasoning ability\" (line 59), but curriculum learning in real-world applications must address both reasoning and language complexity. At minimum, the paper should include experiments on at least one real-world dataset to validate that findings hold beyond the synthetic setting. Alternatively, a much more substantive theoretical or empirical discussion is needed to justify why the synthetic results should inform practice on natural language tasks.\n\n5. Presentation Quality\n\nTable 1 lacks a bottom horizontal rule (bottomrule in LaTeX booktabs), appearing unfinished and violating standard academic formatting conventions.\n\n**I found the paper challenging to follow due to several instances of unclear and ambiguous wording and expressions. I strongly recommend that the authors carefully revise the manuscript to enhance clarity and readability.** Although I believe the paper's writing requires substantial revision, I find the experimental design to be generally sound. I am currently inclined to give a score of 6, but I remain somewhat uncertain and will adjust my rating based on other reviewers' feedback and the authors' responses during the discussion phase."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7xxzwWKfTZ", "forum": "sHn5rq6L0O", "replyto": "sHn5rq6L0O", "signatures": ["ICLR.cc/2026/Conference/Submission18373/Reviewer_71Fw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18373/Reviewer_71Fw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795983007, "cdate": 1761795983007, "tmdate": 1762928081419, "mdate": 1762928081419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the effects of curriculum learning (CL) applied to SFT and RLVR. The authors consider synthetic datasets which can be generated procedurally to avoid any data contamination. Each dataset has a notion of difficulty which is tied to the number of reasoning steps required to solve the logical or mathematical reasoning task. The authors finetune a wide range of SLMs including Llama3.2, Qwen3 and Gemma2 and show that no CL setups that they propose outperform random sampling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The use of synthetic datasets to control for dataset contamination.\n* Thorough related works section."}, "weaknesses": {"value": "* The presentation is rough and requires some development. The abstract reads like a summary of the experiments. The motivation for performing CL in the introduction is not clear. In terms of the writing all figure captions are not descriptive enough. The SFT and RLVR implementation details are not included in the main text e.g. did you use PEFT or full finetuning, what learning rate schedules did you use? Did you optimize hyperparameters for each dataset model pair?\n* You make some claims which are not accompanied by experimental evidence or appropriate citations. For example:\n    * Lines 31-32: “... and suggest that future work should explore alternative mechanisms for strengthening pure reasoning robustness in LLMs”. As far as I understand the robustness of the reasoning is not measured, but rather only whether the predicted answers match the ground truth answers in Figures 2 and 3?\n    * Lines 38-40 “... consistent multi-step reasoning remains far from robust…”. I would argue that frontier LLMs are pretty good at multi-step reasoning. I can see from Figure 2 that only Qwen3-4b is robust to the problems under consideration, but to back up this claim you should also show the performance for frontier models e.g. GPT-4 or 5.\n* The definition of difficulty is limited. The difficulty corresponds to the number of $d$ participants in the LinearDepth and PartWhole datasets. While for Knights and Knaves it corresponds to the number of characters introduced in the puzzle-like questions. There are other common definitions of difficulty—from the active learning literature for instance—which are not considered like using a high loss or uncertainty, or a low reward from an external reward model or using LLM-as-a-judge to score the difficulty of a problem."}, "questions": {"value": "The main question I have is about the CL implementation. Every time you change the difficulty of the dataset you are introducing a distribution shift in the training process [1]. Did you have to make any particular changes to a regular training process to account for this? For instance, did you have a learning rate schedule per epoch since each epoch has data from a different difficulty [2]?\n\n[1] Ash, Jordan T., and Ryan P. Adams. \"On the difficulty of warm-starting neural network training.\" (2019).\n\n[2] Lialin, Vladislav, et al. \"Relora: High-rank training through low-rank updates.\" arXiv preprint arXiv:2307.05695 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8lqEUE0Pa", "forum": "sHn5rq6L0O", "replyto": "sHn5rq6L0O", "signatures": ["ICLR.cc/2026/Conference/Submission18373/Reviewer_Xhmm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18373/Reviewer_Xhmm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907652130, "cdate": 1761907652130, "tmdate": 1762928080854, "mdate": 1762928080854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies whether curriculum learning helps LLMs generalize in synthetic arithmetic and logical reasoning tasks, using carefully controlled difficulty levels. The answer is essentially \"no\": across SFT and RL or GRPO, curriculum schedules perform no better than standard random sampling. The authors identify response-length collapse as a key cause of SFT failure and note that RL preserves longer reasoning traces. Also notable is the effect of SFT on PartWhole and KK is significantly bad for the baseline models suggesting it is detrimental compared to zero shot reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Thorough literature review and scoping of the problem is appreciated.\n2. Example dataset instances are also appreciated.\n3. The authors study the performance of various small scale LLMs as a function of problem complexity and they find universal decline with problem complexity. What is this like for the SFT or curricula trained models?"}, "weaknesses": {"value": "1. Not enough novelty and depth of experimentation and insight. The work is a useful negative result, but the novelty is weak and the methodology is formulaic for this venue.\n2. The experimental scope is also constrained: small model sizes and synthetic/toy settings that might not correspond to the real-world settings where curriculum learning might be more beneficial due to the complex nature of that setting.\n3. The claim is framed as insightful, but most findings are rather unsurprising or task dependent: synthetic tasks with templated chain-of-thought offer limited room for nuanced curricula, and short SFT traces forcing unnatural brevity will obviously cripple reasoning and, at this point is quite clear from literature that longer CoTs help reasoning cf. deepseek R1 paper or from the theoretical viewpoint (takes tractable problem class from TC to NC cf. [this](https://arxiv.org/abs/2305.15408)."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3mCIUt2Ha4", "forum": "sHn5rq6L0O", "replyto": "sHn5rq6L0O", "signatures": ["ICLR.cc/2026/Conference/Submission18373/Reviewer_DZss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18373/Reviewer_DZss"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019216786, "cdate": 1762019216786, "tmdate": 1762928079328, "mdate": 1762928079328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}