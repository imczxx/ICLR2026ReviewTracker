{"id": "klRdsrja2h", "number": 6800, "cdate": 1757996275899, "mdate": 1759897892774, "content": {"title": "Perceive, Reflect and Understand Long Video: Progressive Multi-Granular Clue Exploration with Interactive Agents", "abstract": "Long videos, characterized by temporal complexity and sparse task-relevant information, pose significant reasoning challenges for AI systems. Although various Large Language Model (LLM)-based approaches have advanced long video understanding, they still struggle to achieve both completeness and efficiency in capturing task-critical information. Inspired by human progressive visual cognition, we propose CogniGPT, a framework that leverages an interactive loop between Multi-Granular Perception Agent (MGPA) and Verification-Enhanced Reflection Agent (VERA) for efficient and reliable long video understanding. Specifically, MGPA mimics human visual divergent and focused attention to capture task-related information, while VERA verifies perceived key clues to mitigate hallucination and optimize subsequent perception strategies. Through this interactive process, CogniGPT explores a minimal set of informative and reliable task-related clues.\nExtensive experiments on EgoSchema, Video-MME, NExT-QA, and MovieChat datasets demonstrate CogniGPT's superiority in both accuracy and efficiency. Notably, on EgoSchema, it surpasses existing training-free methods using only 11.2 frames and achieves performance comparable to Gemini 1.5-Pro.", "tldr": "", "keywords": ["Long Video Understanding", "LLM Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61ac679c4f6428ea68a078d69eaad10707ed80f8.pdf", "supplementary_material": "/attachment/9ea01fdfe779771cbf51b0c64958a076e87a83ed.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a training-free agent for long-form video QA that alternates between perception and verification in a loop manner. For perception, it gathers the information via 1) a diversity-promoting keyframe selector; 2) temporal clustering within segments; 3) spatial VQA on selected frames. For verification, it evaluates key hypotheses using targeted VQA to decide whether to continue the search or provide an answer. On EgoSchema, Video-MME, NExT-QA, and MovieChat, the method achieves improved accuracy with fewer processed frames than agentic baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Sound framework. A simple yet working training-free agent framework that alternates between perception and verification, reflecting a natural way for long-form video understanding.\n\n- Good results. It shows that the system, CogniGPT, achieves good results on multiple video QA benchmarks, including EgoSchema, Video-MME, NExT-QA, and MovieChat.\n\n- Useful ablations. The paper ablates how components like LLM and the verification stage affect the final performance."}, "weaknesses": {"value": "- Limited novelty. The training-free agentic framework in video understanding has been well explored, for example, in VideoAgent. Recent papers have already studied training-based agents that achieve better performance, such as VideoR1. This paper follows the VideoAgent, with perception using a retrieval system (already proposed by VideoAgent) and reflection and verification using an LLM (also suggested by VideoAgent), in a loop (again proposed by VideoAgent). Given the limited novelty, more insightful analysis, such as failure cases when applying the method to this setting, the reasons for the failures, and how to adapt them, is worth further study. \n\n- More ablations needed. One novelty from the paper is the \"diversity-promoting\" keyframe selector. However, there is no ablation showing how this diversity helps the final performance. Meanwhile, it needs more analysis of where the errors occur, both at the perception stage and at the verification stage.\n\n- Statistical significance. The error bars are only provided for NExT-QA; please add confidence intervals/seed variance for main tables or at least EgoSchema and Video-MME."}, "questions": {"value": "See weaknesses.\n\nTypos: many \\cite should be replaced as \\citep."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4NlibgBMRP", "forum": "klRdsrja2h", "replyto": "klRdsrja2h", "signatures": ["ICLR.cc/2026/Conference/Submission6800/Reviewer_ha9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6800/Reviewer_ha9r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342515419, "cdate": 1761342515419, "tmdate": 1762919071321, "mdate": 1762919071321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CogniGPT presents a training-free agentic framework for long-video question answering that addresses the computational inefficiency of dense-captioning approaches through progressive multi-granular exploration. The system comprises two coordinated agents operating iteratively with a maximum of three rounds. The Multi-Granular Perception Agent selectively employs three complementary tools that capture evidence at different granularities: Divergent Search uses EVA-CLIP similarity with watershed-style peak detection to identify salient keyframes across the full video, Temporal Focus applies K-means clustering within relevant spans to capture sub-event structures, and Spatial Focus conducts targeted frame-level visual question answering for fine-grained attributes and spatial relationships that captions may miss. The Verification-Enhanced Reflection Agent mitigates hallucination through explicit cross-validation of key observations by auto-generating verification questions tied to specific timestamps, while also evaluating information sufficiency and planning subsequent exploration steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Modular, training-free stack with clear swap points. The MGPA/VERA split isolates retrieval, captioning, and VQA so backbones (EVA-CLIP, captioner, VLM) can be upgraded without controller retraining—useful for rapid iteration and ablations.\n* Strong accuracy-per-frame trade-off. Consistently competitive results while processing only ~10–18 frames/sample on long-video suites, indicating the coarse-to-fine tool cascade is compute-efficient relative to dense captioning."}, "weaknesses": {"value": "* Limited conceptual novelty. The method mainly recombines established pieces—agentic plan-act loops (VideoAgent family), hierarchical/multi-granular retrieval (e.g., VideoTree-style coarse→fine), dense-caption→LLM aggregation (LLoVi/DrVideo), and Chain-of-Verification—plus an engineering tweak (watershed peak picking)."}, "questions": {"value": "Could the authors provide evidence that the verification loop truly improves grounding accuracy, rather than merely promoting self-consistency among correlated VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tpXNSZ0UpD", "forum": "klRdsrja2h", "replyto": "klRdsrja2h", "signatures": ["ICLR.cc/2026/Conference/Submission6800/Reviewer_mkrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6800/Reviewer_mkrX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449988829, "cdate": 1761449988829, "tmdate": 1762919071002, "mdate": 1762919071002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose CogniGPT, a framework that employs an interactive Perception-Reflection loop between two LLM agents: the Multi-Granular Perception Agent (MGPA) and the Verification-Enhanced Reflection Agent (VERA). MGPA uses tools like divergent and focused search to selectively extract minimal, multi-granular clues from the video, bypassing the need for computationally expensive global video processing. VERA then verifies the reliability of these clues to mitigate hallucinations and guides MGPA's next perceptual step, ensuring an iterative and progressive understanding process. The authors perform experiments across several benchmarks and demonstrate CogniGPT’s superior accuracy and efficiency compared to existing training-free LLM agent methods and competitive performance against proprietary multimodal LLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The main strength of this paper's approach is its ability to achieve high accuracy and significant efficiency for understanding long videos.\n- Another strength is the method's improved reliability in reasoning, especially concerning hallucinations introduced by VLMs, thanks to VERA.\n- The introduced MGPA captures information at different levels of detail, ensuring that both the broad context and fine details are acquired\n- The proposed framework achieves SOTA results across various long video understanding tasks and datasets (EgoSchema, Video-MME, NExT-QA, and MovieChat)"}, "weaknesses": {"value": "- A minor complaint I have is that CogniGPT falls into the category of LLM Agent-based methods, and its entire architecture is built around LLM agents, which is the paradigm used by Llovi, VideoTree, and others which might hurt its perceived novelty. \n- Minor typos: Figure 2, within the Spatial Focus box, “**Vison**-Language Model”"}, "questions": {"value": "- What’s the difference between Multi-Granular Perception Toolkit and Multi-Granular Perception Agent? Should we stick to one terminology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HOQFPVVNFQ", "forum": "klRdsrja2h", "replyto": "klRdsrja2h", "signatures": ["ICLR.cc/2026/Conference/Submission6800/Reviewer_j8ZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6800/Reviewer_j8ZP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465796598, "cdate": 1761465796598, "tmdate": 1762919069837, "mdate": 1762919069837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of understanding complex long videos with sparse task-relevant information. To improve AI reasoning, the paper propose CogniGPT, a framework inspired by human visual cognition. CogniGPT combines a perception agent (MPGA) to focus on task-relevant details, and a reflection agent (VERA)  to verify key information, reducing errors and improving efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow\n\n- The figures are intuitive."}, "weaknesses": {"value": "- The proposed method is straightforward and intuitive. \n\n - As an LLM-based agent, its performance is inferior to single models like Qwen2.5-VL and significantly worse than VideoChat-A1[1], which uses VL models. This raises the question of why such a complex LLM agent is needed when simpler and more effective alternatives exist.\n\n- While it uses fewer frames, the claim of \"selecting key information\" is not convincingly supported by the results. \n\n- The table formatting in the paper is poorly organized.\n\n- The paper should use \\citep instead of \\cite for citation\n\n\n[1] Wang Z, Chen B, Yue Z, et al. VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning[J]. arXiv preprint arXiv:2506.06097, 2025."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pfb90Sr2QQ", "forum": "klRdsrja2h", "replyto": "klRdsrja2h", "signatures": ["ICLR.cc/2026/Conference/Submission6800/Reviewer_m7vg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6800/Reviewer_m7vg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925524598, "cdate": 1761925524598, "tmdate": 1762919069318, "mdate": 1762919069318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}